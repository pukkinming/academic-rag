{
  "paper_id": "2107.04192v1",
  "title": "Emotion Recognition With Incomplete Labels Using Modified Multi-Task Learning Technique",
  "published": "2021-07-09T03:43:53Z",
  "authors": [
    "Phan Tran Dac Thinh",
    "Hoang Manh Hung",
    "Hyung-Jeong Yang",
    "Soo-Hyung Kim",
    "Guee-Sang Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The task of predicting affective information in the wild such as seven basic emotions or action units from human faces has gradually become more interesting due to the accessibility and availability of massive annotated datasets. In this study, we propose a method that utilizes the association between seven basic emotions and twelve action units from the AffWild2 dataset. The method based on the architecture of ResNet50 involves the multi-task learning technique for the incomplete labels of the two tasks. By combining the knowledge for two correlated tasks, both performances are improved by a large margin compared to those with the model employing only one kind of label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing aims to transfer the understanding of human feelings to computers, so they could recognize humans' emotional states and be applied to multiple advanced areas such as education or health service. Affective states can be decided by a wide range of sources in three main categories, namely visual, auditory and biological signals. Visual information, especially facial clues, is the most important and most adopted data due to high availability, interpretability and strong pertinence to emotional states. To analyze the affective states, Ekman  [1]  introduced the six basic emotions, i.e., anger, disgust, fear, happiness, sadness and surprise. Those categorical values are extensively applicable to human beings but this is not the only way to perceive the emotional states. In terms of dimensional model, they can be represented as continuous values, namely valence and arousal. Valence shows how positive or negative the emotion is while arousal measures the agitation level which is from non-active to ready to act. Furthermore, according to the Facial Action Coding System (FACS)  [2] , facial movements which are defined as Action Units (AUs) are recorded to interpret emotions. Affective Behavior Analysis In-The-Wild (ABAW) 2021  [3]  is a competition with the primary goal of improving the machines' capability of understanding human feelings, emotions 1 Contributed equally * Corresponding author and behaviors. The competition provides the massive dataset called AffWild2  [4] [5] [6] [7] [8] [9] [10]  about emotions in the wild with annotations for seven basic emotions, AUs and valence/arousal. There are three tasks corresponding to the three types of annotations and the dataset contains the videos and also the cropped and aligned images extracted from those videos. In this paper, since the dataset does not have complete labels for seven basic expression classification and facial action unit detection, we propose a modified multi-task learning technique for ResNet50 as the main model for implementing both tasks. Moreover, the data for seven emotions is highly imbalanced towards more common emotions such as neutral and happy, so we employ the Focal Loss to counter this effect. The detail of our work will be described in the next section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preprocessing",
      "text": "The AffWild2 dataset provides the cropped and aligned images that are extracted from the videos. We use them for both training and validation stages and did not use other tools to acquire the images from the videos. The input size for the model is 112 x 112 and RGB color space is applied. The images are normalized before inputting to the model and no augmentation techniques are used to enlarge the dataset. The audios are not adopted for training in our method since not all videos contain sounds and the sounds in some cases are noise from the environment or human activities. Without proper processing or an adequate mechanism to analyze the audios, they probably cause ambiguity and drop of performance to the main model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Structure",
      "text": "ResNet50  [11]  is the backbone of our deep learning network. Commonly, the pretrained weights on the ImageNet  [12]  are used to accelerate and enhance the training performance. In the field of emotion recognition, thanks to the existing works on emotion recognition, we opt for the pretrained weights on the VGGFace2 dataset  [13] . The VGGFace2 dataset is not only large in the amount of images but also varied in the number of subjects and covers a large range of pose, age and arXiv:2107.04192v1 [cs.CV] 9 Jul 2021 ethnicity too. This coincides with the concept of AffWild2 dataset which is not dependent on the context nor the age, gender, ethnicity, social status, etc. The fully connected layer is cut and then the pretrained weights are loaded on the main backbone. A new dense layer with 512 and two dense layers of 7 or 12 neurons are sequentially added according to the specific task. The model takes the input as static images. Because a lot of images do not have both labels for seven emotions and AUs, we need to have a particular training scheme to better learn the shared knowledge between two tasks. The training scheme is shown as below:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Loss Function",
      "text": "For seven basic emotion classification, we use Focal loss  [14] . The dataset for expression classification is imbalanced so we apply this loss to deal with this problem. For facial action unit detection, we use binary cross entropy loss for each action unit. The total loss is the summation of the two above losses with the same weight.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Setup",
      "text": "The training process is optimized by the Adam optimizer. We use GPU RTX 2080Ti as the hardware and Pytorch framework as the software. The mini-batch has size 256. To regularize the training process and accelerate the convergence of the model, we use the Cosine Annealing as the learning rate scheduler with the starting learning rate of 0.001. The testing models for the most time achieve the best results only after 10 epochs on the validation set.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "Table  1  shows the results from our experiment on the validation set of emotion classification. By using the pretrained weight from the EmotionNet  [15]  dataset, the model when only trains with seven emotion label gets the performance metric of 0.462. After apply the multi-task learning technique and using the pretrained weight from the VGGFace2 dataset, the metric improves by nearly 0.1. We also implement the shared backbone architecture between the two models of two tasks and get the performance metric of 0.713. The Focal loss which is used to counter the effect of imbalance dataset of seven emotions achieved the best results of 0.757. Table  2  displays the results from our experiments on the validation set of action unit detection. The application of shared backbone architecture does not improve the performance from using only annotation from the twelve action units in this case. However, the multi-task learning technique helps us attain the performance metric of 0.731.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present our experiments on the two tasks of emotion classification and action unit detection. ResNet50",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview system of proposed method",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "and behaviors. The competition provides the massive dataset"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "called AffWild2 [4–10] about emotions in the wild with an-"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "notations for seven basic emotions, AUs and valence/arousal."
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "There are three tasks corresponding to the three types of an-"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "notations and the dataset contains\nthe videos and also the"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "cropped and aligned images extracted from those videos."
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "In this paper,\nsince the dataset does not have complete la-"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "bels for seven basic expression classiﬁcation and facial ac-"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "tion unit detection, we propose a modiﬁed multi-task learning"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "technique for ResNet50 as the main model for implementing"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "both tasks. Moreover,\nthe data for seven emotions is highly"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "imbalanced towards more common emotions such as neutral"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "and happy, so we employ the Focal Loss to counter this effect."
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "The detail of our work will be described in the next section."
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "2. PROPOSED METHOD"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "2.1. Preprocessing"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "The AffWild2 dataset provides the cropped and aligned im-"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "ages that are extracted from the videos. We use them for both"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "training and validation stages and did not use other\ntools to"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "acquire the images from the videos.\nThe input size for\nthe"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "model\nis 112 x 112 and RGB color\nspace is applied.\nThe"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "images are normalized before inputting to the model and no"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "augmentation techniques are used to enlarge the dataset. The"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "audios are not adopted for training in our method since not all"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "videos contain sounds and the sounds in some cases are noise"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "from the environment or human activities. Without proper"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "processing or an adequate mechanism to analyze the audios,"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "they probably cause ambiguity and drop of performance to"
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": ""
        },
        {
          "Department of Artiﬁcial Intelligence Convergence, Chonnam National University, South Korea": "the main model."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: shows the results from our experiment on the vali-",
      "data": [
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "dataset which is not dependent on the context nor\nthe age,"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "gender, ethnicity, social status, etc. The fully connected layer"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "is cut and then the pretrained weights are loaded on the main"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "backbone. A new dense layer with 512 and two dense lay-"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "ers of 7 or 12 neurons are sequentially added according to the"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "speciﬁc task."
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "The model\ntakes the input as static images. Because a lot of"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "images do not have both labels for seven emotions and AUs,"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "we need to have a particular training scheme to better learn the"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "shared knowledge between two tasks. The training scheme is"
        },
        {
          "ethnicity too.\nThis coincides with the concept of AffWild2": "shown as below:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: shows the results from our experiment on the vali-",
      "data": [
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": "epochs on the validation set."
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": "3.2. Results"
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": "weight\nfrom the EmotionNet\n[15] dataset,"
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": "only trains with seven emotion label gets"
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        },
        {
          "3.1. Training Setup": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: shows the results from our experiment on the vali-",
      "data": [
        {
          "Algorithm 1: The\ntraining strategy for proposed": "method"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Input:"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Set images which has 7 expression as target E"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Set images which has 12 action units as target A"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Set images which has both types of labels B"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Number of epochs T"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Output:"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "One label for 7 expression e"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Multi-labels for 12 action units a"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "for t = 1, t ≤ T do"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "for i = 1, i ≤ len(E) do"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "e = M odel(Ei)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Loss = F (e, θi)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Update weight θi"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "end"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "for i = 1, i ≤ len(A) do"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "a = M odel(Ai)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Loss = F (a, θi)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Update weight θi"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "end"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "for i = 1, i ≤ len(B) do"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "e, a = M odel(Ai)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Loss = F (e, a, θi)"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "Update weight θi"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "end"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "end"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "2.3. Loss Function"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": ""
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "For seven basic emotion classiﬁcation, we use Focal loss [14]."
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "The dataset for expression classiﬁcation is imbalanced so we"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "apply this loss to deal with this problem. For facial action unit"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "detection, we use binary cross entropy loss for each action"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "unit. The total loss is the summation of the two above losses"
        },
        {
          "Algorithm 1: The\ntraining strategy for proposed": "with the same weight."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "F1 Score"
        },
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "0.3"
        },
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "0.395"
        },
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "0.494"
        },
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "0.675"
        },
        {
          "Table 1. Seven basic emotion classiﬁcation result on the validation set": "0.724"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "ResNet50 (VGG-Face2) (Multitasking)",
          "0.427\n0.883\n0.655": "0.566\n0.895\n0.731"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "with pretrained weight on the VGGFace2 dataset produces",
          "0.427\n0.883\n0.655": "[7] Dimitrios Kollias and Stefanos Zafeiriou.\nExpression,"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "good results on the AffWild dataset and proposed training",
          "0.427\n0.883\n0.655": "affect,\naction unit\nrecognition:\nAff-wild2, multi-task"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "scheme with the application of multi-task learning enhances",
          "0.427\n0.883\n0.655": "learning and arcface. arXiv preprint arXiv:1910.04855,"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "the performance by a\nconsiderable margin on both tasks.",
          "0.427\n0.883\n0.655": "2019."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "Moreover, Focal\nloss\nis\nsuitable for\nsolving the imbalance",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "[8] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "problem on the dataset of seven emotions.",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "Zafeiriou.\nFace behavior a la carte: Expressions, af-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "fect and action units in a single network. arXiv preprint"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "5. REFERENCES",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "arXiv:1910.11111, 2019."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[1] Paul Ekman and Wallace V Friesen. Constants across",
          "0.427\n0.883\n0.655": "[9] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nico-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "cultures in the face and emotion. Journal of personality",
          "0.427\n0.883\n0.655": "laou, Athanasios Papaioannou, Guoying Zhao, Bj¨orn"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "and social psychology, 17(2):124, 1971.",
          "0.427\n0.883\n0.655": "Schuller, Irene Kotsia, and Stefanos Zafeiriou. Deep af-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "fect prediction in-the-wild: Aff-wild database and chal-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[2] E Friesen and Paul Ekman. Facial action coding system:",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "International\nlenge, deep architectures,\nand beyond."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "a technique for\nthe measurement of\nfacial movement.",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "Journal of Computer Vision, 127(6):907–929, 2019."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "Palo Alto, 3(2):5, 1978.",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "[10] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nico-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[3] Dimitrios Kollias,\nIrene Kotsia,\nElnar Hajiyev,\nand",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "laou, Athanasios\nPapaioannou, Guoying Zhao,\nand"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "Stefanos\nZafeiriou.\nAnalysing\naffective\nbehavior",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "Irene Kotsia.\nAff-wild:\nvalence\nand arousal’in-the-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "arXiv\npreprint\nin\nthe\nsecond\nabaw2\ncompetition.",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "wild’challenge.\nIn Proceedings of the IEEE conference"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "arXiv:2106.15318, 2021.",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "on computer vision and pattern recognition workshops,"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "pages 34–41, 2017."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[4] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Ste-",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "fanos Zafeiriou.\nAnalysing affective behavior\nin the",
          "0.427\n0.883\n0.655": "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "ﬁrst abaw 2020 competition.\nIn 2020 15th IEEE In-",
          "0.427\n0.883\n0.655": "Sun. Deep residual\nlearning for image recognition.\nIn"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "ternational Conference on Automatic Face and Gesture",
          "0.427\n0.883\n0.655": "Proceedings of the IEEE conference on computer vision"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "Recognition (FG 2020), pages 637–643. IEEE, 2020.",
          "0.427\n0.883\n0.655": "and pattern recognition, pages 770–778, 2016."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[5] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos",
          "0.427\n0.883\n0.655": "[12] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "Zafeiriou.\nDistribution matching\nfor\nheterogeneous",
          "0.427\n0.883\n0.655": "Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "arXiv\nmulti-task learning:\na\nlarge-scale\nface\nstudy.",
          "0.427\n0.883\n0.655": "Karpathy, Aditya Khosla, Michael Bernstein, et al.\nIm-"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "preprint arXiv:2105.03790, 2021.",
          "0.427\n0.883\n0.655": "Inter-\nagenet\nlarge scale visual\nrecognition challenge."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "national\njournal of computer vision, 115(3):211–252,"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "[6] Dimitrios Kollias\nand\nStefanos\nZafeiriou.\nAffect",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "",
          "0.427\n0.883\n0.655": "2015."
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "analysis in-the-wild: Valence-arousal, expressions, ac-",
          "0.427\n0.883\n0.655": ""
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "arXiv preprint\ntion units\nand a uniﬁed framework.",
          "0.427\n0.883\n0.655": "[13] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and"
        },
        {
          "ResNet50 (VGG-Face2) (Shared backbone)": "arXiv:2103.15792, 2021.",
          "0.427\n0.883\n0.655": "Andrew Zisserman. Vggface2: A dataset for recognis-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "national conference on automatic face & gesture recog-"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "nition (FG 2018), pages 67–74. IEEE, 2018."
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "[14] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "and Piotr Doll´ar. Focal\nloss for dense object detection."
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "In Proceedings of the IEEE international conference on"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "computer vision, pages 2980–2988, 2017."
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "[15] Zijun Wei, Jianming Zhang, Zhe Lin, Joon-Young Lee,"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "Niranjan Balasubramanian, Minh Hoai,\nand Dimitris"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "Samaras. Learning visual emotion representations from"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "web data.\nIn Proceedings of the IEEE/CVF Conference"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "on Computer Vision and Pattern Recognition,\npages"
        },
        {
          "ing faces across pose and age.\nIn 2018 13th IEEE inter-": "13106–13115, 2020."
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "3",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "Paul Ekman"
      ],
      "year": "1978",
      "venue": "Facial action coding system: a technique for the measurement of facial movement"
    },
    {
      "citation_id": "4",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Irene Kotsia",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition",
      "arxiv": "arXiv:2106.15318"
    },
    {
      "citation_id": "5",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "6",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "7",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "8",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "9",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "10",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Aff-wild: valence and arousal'in-thewild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Imagenet large scale visual recognition challenge. International journal of computer vision",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "Imagenet large scale visual recognition challenge. International journal of computer vision"
    },
    {
      "citation_id": "14",
      "title": "Vggface2: A dataset for recognis-ing faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "15",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "16",
      "title": "Learning visual emotion representations from web data",
      "authors": [
        "Zijun Wei",
        "Jianming Zhang",
        "Zhe Lin",
        "Joon-Young Lee",
        "Niranjan Balasubramanian",
        "Minh Hoai",
        "Dimitris Samaras"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}