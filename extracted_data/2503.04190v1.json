{
  "paper_id": "2503.04190v1",
  "title": "Personalized Emotion Detection From Floor Vibrations Induced By Footsteps",
  "published": "2025-03-06T08:04:43Z",
  "authors": [
    "Yuyan Wu",
    "Yiwen Dong",
    "Sumer Vaid",
    "Gabriella M. Harari",
    "Hae Young Noh"
  ],
  "keywords": [
    "Emotion Recognition",
    "Footstep-Induced Floor Vibrations",
    "Mental Health Monitoring"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is critical for various applications, including the early detection of mental health disorders and emotion-based smart home systems. Previous studies utilized various sensing methods for emotion recognition, such as wearable sensors, cameras, and microphones. However, these methods have limitations in long-term domestic use because of the inherent limitations, including intrusiveness and privacy concerns. To overcome these limitations, this paper introduces a non-intrusive and privacy-friendly personalized emotion recognition system, EmotionVibe, which leverages footstep-induced floor vibrations for emotion recognition. The main idea of EmotionVibe is that individuals' emotional states influence their gait patterns, subsequently affecting the floor vibrations induced by their footsteps. However, there are two main research challenges: 1) the complex and indirect relationship between human emotions and footstep-induced floor vibrations and 2) the large betweenperson variations within the relationship between emotions and gait patterns. To address these challenges, we first empirically characterize this complex relationship and develop an emotionsensitive feature set including gait-related and vibration-related features from footstep-induced floor vibrations. Furthermore, we personalize the emotion recognition system for each user by calculating gait similarities between the target person (i.e., the person whose emotions we aim to recognize) and those in the training dataset and assigning greater weights to training people with similar gait patterns in the loss function. We evaluated our system in a real-world walking experiment with 20 participants, summing up to 37,001 footstep samples. EmotionVibe achieved the mean absolute error (MAE) of 1.11 and 1.07 for valence (unpleasant to pleasant) and arousal (calm to excited) score estimations, respectively, reflecting 19.0% and 25.7% error reduction compared to the baseline method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is crucial for various applications, such as mental health monitoring and emotion-based smart home devices  [1] -  [3] . According to the National Institute of Mental Health (NIMH), 23.1% U.S. adults, approximately 59.3 million individuals live with a mental illness in 2022  [4] . On average, individuals with anxiety or depression have a lifespan of 7.9 years shorter than those without these conditions  [5] . Since mental health disorders are often characterized by increased emotional instability and fluctuations  [6] , monitoring changes in individuals' emotions can facilitate the early detection and intervention of severe mental illnesses  [7] . In addition, integrating emotion recognition systems into smart home devices enables adaptive recommendation systems that enhance user interaction. For example, emotional data can inform music recommendations, adjust lighting and temperature for comfort or relaxation  [8] ,  [9] , and provide personalized content suggestions for smart TVs and video games  [10] ,  [11] .\n\nIn previous work, various sensing methods have been used for emotion recognition, including wearable sensors  [12] ,  [13] , cameras  [14] ,  [15] , microphones  [16] ,  [17] , and multimodal sensing methods  [18] ,  [19] . However, these methods are limited in long-term domestic usage because of intrusiveness or privacy concerns. Wearable sensors are intrusive and may cause discomfort. Cameras and microphones, which rely on capturing facial expressions, body postures, or speaking voices, often raise privacy concerns. Multi-modal sensing requires increased hardware deployment and data processing complexity. These limitations restrict the widespread use of emotion recognition systems for indoor applications.\n\nThis paper introduces EmotionVibe, a novel personalized emotion recognition system that infers human emotions through footstep-induced floor vibrations (see Fig.  1 ). The main intuition of EmotionVibe is that people's emotion influences their gait patterns, which in turn affect the floor vibrations induced by their footsteps. To this end, by analyzing footstep-induced floor vibration patterns captured by the vibration sensors attached to the floor, the emotional state of the pedestrian can be inferred. We showed a proof of concept for this intuition through preliminary small-scale laboratory experiments in our previous work  [20] . Compared to other sensing methods, EmotionVibe offers a non-intrusive and privacy-friendly personalized emotion recognition method.\n\nHowever, recognizing emotions through footstep-induced floor vibrations is challenging due to the complex, indirect, and personalized relationship between human emotions and footstep-induced floor vibrations. The main challenges are:\n\n1) The complex and indirect relationship between human emotions and footstep-induced floor vibrations. Emotions influence human gait patterns in various aspects, including kinematics, kinetics, and spatiotemporal parameters  [21] -  [25] . Furthermore, the relationship between gait patterns and footstepinduced floor vibrations is also challenging to model with the complex mechanisms of foot-floor interaction  [26] .\n\n2) Large between-person variations in the relationship between human emotions and footstep-induced floor vibrations. Each person has a distinct walking style and responds uniquely under varying emotional conditions  [27] ,  [28] . These variations result in highly diverse gait characteristics and footstepinduced floor vibration patterns. Consequently, vibration data from people whose gait patterns significantly differ from the target person can reduce the performance of the emotion recognition model.\n\nTo capture the complex relationship between human emotions and footstep-induced floor vibrations, we develop emotion-sensitive features from two aspects: gait-related and vibration-related features. The selection of these features is inspired by an analysis of how emotions influence gait pattern parameters including kinematic, kinetic, and spatiotemporal parameters, and how these gait patterns impact the characteristics of footstep-induced floor vibrations.\n\nTo address the large between-person variations in the relationship between emotions and gait patterns, we personalize the emotion recognition model by assigning higher weights to training samples from people whose gait patterns are similar to the target person in the loss function. To achieve this, we first estimate the gait similarity indices between the target person and people in the training dataset based on the distance between the features embedded in a lower dimensional space representing their gait patterns. We then utilize these gait similarity indices as sample weights in the loss function to personalize the emotion recognition model. Consequently, the performance of the emotion recognition model improves for the target person, as it assigns greater importance to the training data that are more similar to the target person.\n\nThe main contributions of this work are:\n\n• We introduce EmotionVibe, a novel personalized emotion recognition system based on footstep-induced floor vibrations. EmotionVibe provides a non-intrusive and privacyfriendly approach to emotion recognition, making it wellsuited for in-home applications. • We develop two sets of emotion-sensitive features to capture the complex relationship between emotion and footstep-induced floor vibrations. In addition, we personalize the emotion recognition model for each target user based on the gait similarity between people. • We evaluated EmotionVibe in real-world experiments with 20 participants, achieving promising results for emotion score estimation. The rest of the paper is organized as follows: Section II discusses related works and provides a comparison between the related works and our system. Section III introduces the basic physical insights of our system, including the description of emotion, the effect of emotions on gait patterns, and the effect of gait patterns on footstep-induced floor vibrations. Section IV details the EmotionVibe system design. Section V shows the real-world experiments and the evaluation results of EmotionVibe. Section VI evaluates the effectiveness of each module in EmotionVibe, assesses the robustness of our system performance, and explains the rationale behind participant selection criteria. Finally, Section VII concludes our work and explores future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "This section provides an overview of current research on various emotion recognition sensing methods as well as vibration analysis and modeling methods.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Recognition Methods",
      "text": "Previous studies on emotion recognition can be divided into: physiological signal-based methods, facial indicatorbased methods, body behavior-based methods, and linguisticbased methods.\n\nPhysiological signals such as Electroencephalography (EEG)  [29] -  [31] , Electrocardiography (ECG)  [32] -  [34] , Galvanic Skin Response (GSR)  [35] ,  [36] , Heart Rate Variability (HRV)  [37] ,  [38]  are effective indicators of human emotions. These physiological signals are difficult to mimic and provide accurate results in emotion recognition. However, measuring these physiological signals typically requires direct sensorskin contact, which can cause discomfort and limit daily usage. Moreover, their effectiveness is often reduced in mobile settings due to motion artifacts and signal instability.\n\nOther methods use visual cues from the face, including expressions  [39] -  [41] , eye movements  [35] ,  [42] , and gaze patterns  [43] ,  [44] . These methods are usually camera-based, thus enabling contactless emotion recognition and overcoming the limitations of wearable sensors. However, these methods are limited by lighting conditions, visual obstacles, and camera shooting angles. In addition, cameras usually raise privacy concerns, thus limiting their application in domestic settings.\n\nBody behavior is another important indicator of human emotions. Previous research has utilized cameras  [45] ,  [46] , motion capture systems  [47] ,  [48] , smartwatches  [49] ,  [50] , smartphones  [51] -  [53] , and force platforms  [54]  to analyze human gait behavior, posture, and other body movements to infer the user's emotional state. Camera and motion-capturingbased methods are contactless methods that capture a broad range of movements and can extract skeletal and joint locations for emotion recognition  [55] -  [59] . Compared to facial expression recognition-based emotion recognition, these methods are effective when the subject is away from the camera or when facial features are obscured. However, they can still cause privacy concerns for in-home scenarios and are limited by visual obstacles. The wearable or mobile device-based methods, use body-attached or embedded sensors in mobile devices to record movements directly for emotion recognition  [49] ,  [60] -  [63] . These devices, equipped with accelerometers and gyroscopes, realize emotion recognition by analyzing the occupants' movement information. Although more convenient than wired sensors, they still require users to carry or wear the device which limits their everyday applicability. The forcebased methods, involve measuring ground reaction forces using force platforms during walking to recognize emotions  [54] . However, the limited coverage of force platforms necessitates dense deployment, restricting their suitability for in-home use.\n\nEmotion recognition through linguistic analysis mainly includes speech-based and text-based methods. Speech-based methods analyze vocal attributes such as pitch, tone, speech rate, and intensity, and variations in these features for emotion recognition  [64] ,  [65] . For instance, a higher pitch and faster speech indicate excitement, while a slower rate and softer tone suggest sadness. Text-based methods use Natural Language Processing (NLP) to analyze word choice, sentence structure, and contextual semantics for emotion detection. Advances in Large Language Models have enhanced sensitivity to linguistic nuances and context  [66] ,  [67] . However, speech-based methods require high-quality audio, limiting effectiveness in uncontrolled environments and raising privacy concerns due to microphone usage. Text-based methods depend on userprovided textual data, which may not always be available.\n\nCompared to existing methods, EmotionVibe provides a non-intrusive and privacy-friendly approach to emotion recognition by utilizing footstep-induced floor vibrations. Unlike physiological, facial, or body behavior-based techniques that require wearables or cameras, EmotionVibe captures emotional cues without direct skin contact or compromising privacy. Moreover, it does not rely on verbal or text input from users. Consequently, EmotionVibe presents a promising solution for real-world, long-term domestic applications.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Vibration Analysis And Modeling",
      "text": "In this subsection, we review papers related to vibration and audio signal processing, modeling, and effective machinelearning model simplification methods that inspire the development of emotion-sensitive feature sets and the emotion recognition model.\n\nPrevious studies proposed various feature extraction methods for vibration and audio signal processing to capture signal features, which can be categorized into classical and deep learning-based methods. Classical methods extract features based on interpretable physical principles of signal processing research, including time-domain features (mean, kurtosis, zero-crossing rate, envelope analysis), frequencydomain features (Fourier transform, power spectral density), and time-frequency-domain spectral features (mel-frequency cepstral coefficients, short-time Fourier transform, constant Q transform, and wavelet transform)  [68] -  [72] . These features can handle transient and non-stationary signals and are used for pattern analysis in speech recognition  [73] , speaker identification  [74] , structural health monitoring  [72] , and humanbuilding interaction studies  [75] . These classical features are interpretable and computationally efficient but often rely on domain expertise to design effective feature sets. On the other hand, deep learning-based methods, including convolutional neural networks and autoencoders, can automatically extract complex hierarchical features from raw signals or spectrograms, enabling robust performance in tasks such as fault detection and sound classification  [72] ,  [76] -  [78] . However, they are computationally intensive and require a large number of high-quality datasets for learning the implicit relationships. EmotionVibe is a combination of these two approaches. We first analyze the complex relationship between emotion and footstep-induced floor vibrations and develop an emotionsensitive feature set based on this relationship, including gaitrelated and vibration-related features. Then, we model these features using a deep learning framework to learn implicit relationships that cannot be directly given by classical features.\n\nDue to the limited dataset resulting from the high cost of human experiments, EmotionVibe employs an iterative pruning approach, thereby mitigating overfitting and enhancing model effectiveness in emotion recognition. Previous model simplification methods mainly consist of pruning and knowledge distillation. Pruning methods remove unnecessary parameters or structures in the model to reduce the model size and computational cost. It can be categorized into unstructured pruning that removes individual weights  [79] ,  [80]  and structured pruning that prunes network layers  [81] -  [83] . Unstructured pruning targets individual weights and can remove redundant parameters in a more fine-grained manner. However, the inference speed for the unstructured pruned models is limited by the sparse weight matrices caused by such fine-grained pruning. In contrast, structured pruning supports faster inference. However, it is coarse-grained pruning, with each pruning cutting out an entire unit. This may cause important information loss  [84] . The knowledge distillation method transfers knowledge from a larger, pre-trained teacher model to a smaller student model, effectively simplifying the model while maintaining performance  [85] -  [87] . However, this method first requires a well-trained teacher model, which relies on a large dataset. This does not apply to our study because of the limited data we collected in the laboratory experiments. EmotionVibe simplifies the network structure by iteratively cutting the least important parameters in the network during training to reduce overfitting. Since reducing computation time is not our top priority, we chose the unstructured pruning method, which is more fine-grained and has a lower probability of cutting important parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Characterizing The Relationship Between Emotions And Footstep-Induced Floor Vibrations",
      "text": "EmotionVibe is based on the insight that human emotions affect their gait patterns, subsequently influencing the footstepinduced floor vibration patterns (see Fig.  2 ). To this end, we can infer people's emotional states by analyzing the footstepinduced floor vibration patterns captured by the vibration sensors attached to the floor. In this section, we analyze and characterize the relationship between human emotions and footstep-induced floor vibrations through both analytical and empirical studies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Description Of Emotions",
      "text": "In the field of psychology, emotions are often described by the two-dimensional valence-arousal model, as shown in     [88] . Emotions map onto a 2D space formed by the valence and the arousal axes. Valence describes the extent to which an emotion is positive or negative. In other words, it captures the degree to which an emotion is pleasant or unpleasant  [89] . High-valence emotions include happiness, joy, or excitement, while low-valence emotions include sadness, anger, or fear. Arousal refers to the physiological and psychological state of being awake or reactive to stimuli. It ranges from calmness and sleepiness at the low end to increased excitement and heightened alertness at the high end. High arousal is characterized by emotions of being energized, alert, or excited, while low arousal is associated with emotional states such as calmness, relaxation, or lethargy. These two dimensions are assumed to be independent of each other. Based on this 2D model, human emotions can be basically classified into four classes: high-valence, higharousal (HVHA), high-valence, low-arousal (HVLA), lowvalence, high-arousal (LVHA), and low-valence, low-arousal (LVLA). Each type of emotion maps to a specific area in this 2D space. For instance, excitement is a high-valence, high-arousal emotion; anger is a low-valence, high-arousal emotion; relaxation is a high-valence, low-arousal emotion; and depression is a low-valence, low-arousal emotion.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Effect Of Emotion On Gait Patterns",
      "text": "Emotions impact gait patterns from various aspects, including kinetics, kinematics, and spatiotemporal gait parameters (see Fig.  2 )  [21] -  [25] . Kinetics represents the forces and moments that cause the motion. This includes the ground reaction forces, joint forces, and moments that influence the movement of limbs. Kinematics focuses on the geometric movement patterns of the body without considering the forces related to these movements. For example, it involves how the angle of the knee changes throughout a stride, the trajectory of the foot during the swing phase, and the range of rotation in the hip joint. Spatial-temporal gait parameters measure timerelated and distance-related gait characteristics, such as stride length, step length, stride width, gait speed, cadence, stance time, and swing time.\n\nEmotions affect the kinetic aspects of gait, i.e., the forces that cause or result from motion, by influencing the force amplitudes and pressure distributions applied during walking  [54] ,  [90] ,  [91] . High-arousal emotions, such as anger and happiness, are associated with heightened forcefulness and greater stride intensity, whereas low-arousal emotions like sadness are related to a less energetic gait  [22] ,  [90] ,  [91] . In addition, variations in plantar pressure distribution and center of pressure shifts across emotional states reflect changes in foot loading and balance  [54] .\n\nThe kinematics of locomotion are another important indicator of emotions in gaits  [22] ,  [57] ,  [90] -  [93] . Kinematics of locomotion mainly include the movements of joint angles and body parts. It is a description of geometric motion without considering the footstep forces. Sadness is characterized by reduced amplitudes of pelvic rotation, hip flexion, shoulder flexion, torso rotation, and elbow flexion, leading to a more slumped posture and decreased arm swing amplitude  [22] ,  [91] -  [93] . Angry individuals exhibit more flexed trunks, elevated shoulders, and a forward-inclined spine, along with increased amplitudes in shoulder, elbow, hip, and knee movements, indicating a more dynamic and aggressive gait pattern compared to neutral or sad states  [22] ,  [90] . Joyful and content lead to an upright torso posture, with increased neck, trunk, thigh elevation angles, and thoracic extension, and elevated amplitudes in shoulder, elbow, trunk, pelvis, and hip move-ments, suggesting a more open, energetic, and coordinated gait compared to sadness  [22] ,  [57] .\n\nFurthermore, spatiotemporal gait parameters, which refer to time-related and distance-related gait characteristics described by gait parameters, are also influenced by human emotions  [21] ,  [90] -  [92] ,  [94] -  [97] . These parameters include gait velocity, stride, step lengths, step width, durations of single and double support, swing periods, phases, step frequency, and other parameters within a gait cycle. For example, happiness leads to increased stride length, pace, and walking speed, suggesting a more energetic state  [91] ,  [94] ,  [95] . Sadness reduces gait velocity and arm movement, shortens stride length, and increases double limb support, cycle duration, step time, stance time, and swing time  [21] ,  [92] ,  [96] ,  [97] . Anger usually results in faster walking speed and larger stride lengths, reflecting increased movement energy and expansiveness in body language  [90] ,  [95] . In addition, fear and excitement lead to reduced step time compared to neutral conditions  [97] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Effect Of Gaits On Footstep-Induced Floor Vibrations",
      "text": "Previous works have shown that footstep-induced floor vibrations contain valuable information about pedestrians' gait patterns  [20] ,  [27] ,  [98] -  [104] . Footstep-induced floor vibration signals can be used for person identification  [27] ,  [99] ,  [103] -  [105] , gait balance detection  [98] ,  [106] , and inference of gait parameters, foot-floor contact types, and ground reaction force  [100] ,  [107] ,  [108] . These findings provide a foundation for exploring the effects of emotional states on gait changes. The analysis of footstep-induced floor vibrations mainly incorporates time-domain and frequencydomain features, along with other task-specific characteristics related to the vibration signals. The spatio-temporal gait parameters directly affect the temporal and spectral components of the floor vibrations. For example, step frequency affects the time difference between footstep vibration pulses, and stride length affects the energy difference between adjacent footstep vibration signals in the vibration signal. The gait kinetic information affects the energy profile of the floor vibration.\n\nWe characterize the effect of emotional state on the footstepinduced floor vibration patterns based on empirical data collected in laboratory experiments. For Person A, the footstep amplitude is significantly larger during states of high valence emotions, and the step frequency notably increases in high arousal situations (see Fig.  4 (a, b )). These observations are consistent with previous conclusions in the literature that high arousal emotions are always associated with faster walking speeds, and that happiness elicits more forceful footsteps  [91] ,  [94] ,  [95] . However, this pattern does not apply to Person B (see Fig.  4 (c, d )). Relying solely on features identified in previous studies would yield inadequate emotion recognition results, as they fail to capture the complex relationship between human emotions and footstep-induced vibrations. Other gait behaviors related to emotions, such as leaning backward when relaxed or reduced foot-lift height when depressed, are also important for emotion recognition. Consequently, developing a comprehensive emotion-sensitive feature set encompassing various aspects of footstep-induced vibrations is essential for accurate emotion recognition.  To this end, we develop an emotion-sensitive feature set including gait-related and vibration-related features to capture the complex relationship between human emotions and floor vibrations. The feature set is detailed in Section IV-B. Due to the large number of features, we show representative characterization examples in Fig.  5 . The heatmap illustrates the deviations across the four emotion classes. The distributions of features vary depending on emotional states. For example, under high-arousal conditions, the step frequency increases, indicating faster walking; the peak height ratio between heelstrike and toe-off increases during the high valence low arousal situations, indicating a tendency to lean backward; the average energy of the footstep-induced floor vibration signals increases during high-valence emotional situations, indicating a more heavy-footed walking pattern. This finding aligns with conclusions from other research in gait emotion analysis  [90] ,  [91] . This variation in feature distributions shows the feasibility of using these features for emotion recognition. Furthermore, each feature exhibits distinct efficiency in differentiating specific emotions, which shows the need to combine these features for effective emotion recognition. For example, step frequency and average energy are effective in distinguishing between high and low arousal emotions, but less effective for differentiating high and low valence. Conversely, the peak ratio between heel strike (HS) and toeoff (TO) separates high valence-low arousal emotions (HVLA) but shows minimal variation for other emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lvla Hvla Lvha Hvha Emotion Classes",
      "text": "Step Frequency FWHM of 10 to  35",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Emotionvibe: Emotion Recognition System Using Footstep-Induced Floor Vibrations",
      "text": "The EmotionVibe system collects and analyzes floor vibration signals induced by footsteps to recognize the pedestrian's emotions with four modules: 1) Vibration signal collection and preprocessing, 2) Emotion-sensitive feature extraction, 3) General emotion recognition modeling and simplification, and 4) Personalized emotion recognition modeling (see Fig.  6 ). After collecting and preprocessing footstep-induced floor vibration data (Module 1), we capture the complex relationship between human emotion and floor vibrations by developing emotion-sensitive feature sets to address the first challenge (Module 2). These features, inspired by the relationship between human emotions and floor vibrations discussed in Section III, include gait-related and vibrationrelated features. Next, a general emotion recognition model is developed for preliminary emotion recognition and data-driven emotion information extraction from features (Module 3). This model is subsequently personalized to the target person (i.e., the person whose emotions we aim to recognize) to tackle the second challenge (Module 4). The personalization is achieved by first comparing the gait similarities between the vibration data from the target person and the training people. These gait similarities are then employed as weighting factors in the loss function during model fine-tuning, resulting in a personalized emotion recognition model. Finally, EmotionVibe outputs the target person's emotion estimation results, represented by valence and arousal scores.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Module 1: Vibration Signal Collection And Preprocessing",
      "text": "The vibration signal collection and preprocessing module includes three steps: 1) footstep-induced floor vibration signal collection, 2) single footstep signal segmentation, and 3) signal clipping calibration. First, we collect vertical floor vibrations using geophone sensors. Geophones are selected for their low cost and high sensitivity to vertical floor velocity in the lowfrequency range (0-200 Hz), which aligns with the range of footstep-induced floor vibrations  [27] . The collected vibrations are then sent to an amplifier board to enhance their signalto-noise ratio. Next, we segment the vibration signals representing each footstep (defined as a single footstep signal) for gait analysis by identifying the prominent peak in the wavelet transform coefficients within the frequency band of dominant structural natural frequencies (selected based on the floor type and specified in Section V-C). This peak corresponds to the impulse force induced by a single footstep. The signal window of the average footstep duration (chosen as 0.35 s based on our observation) is then extracted around this peak to segment the single footstep signal. Finally, to enhance signal quality, we identify the clipped signal sections and then use polynomial interpolation to reconstruct the clipped signal section using the neighboring data points of the clipped sections. The clipped sections are detected when the signal reaches the upper or lower limit that the sensor can measure and remains at the limit for at least 3 samples, representing the section where the signal exceeds the range of the vibration sensor. Assuming the signal remains continuous, the signal is reconstructed by fitting a polynomial function with the neighboring points of the clipped section and then using this function to interpolate the data within the clipped sections. After interpolation, we obtain a continuous signal representation for the clipped sections with enhanced signal quality.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Module 2: Emotion-Sensitive Feature Extraction",
      "text": "To capture the complex relationship between human emotions and floor vibrations, we develop gait-related and vibration-related features from the preprocessed single footstep signals. Gait-related features capture gait parameters influenced by emotions during the gait cycle, reflecting footstep characteristics. Vibration-based features, on the other hand, capture the detailed characteristics of vibration signals induced by footsteps. The selection of these features is inspired by our investigation of the relationship between human emotions and footstep-induced floor vibrations (see Section III).\n\n1) Gait-Related Features: Gait-related features are based on gait parameters in the gait cycle and provide basic information about footstep patterns influenced by emotions. Gaitrelated features include a variety of gait parameters: step frequency, double support time, peak height ratio of heelstrike and toe-off, full width at half maximum (FWHM) of heel-strike and toe-off, and energy contours (including the raw, smoothed, and logarithm energy contour).\n\nThe gait-related features are chosen based on the influence of human emotions on gait patterns and the corresponding affected gait parameters (see Section III). For example, step frequency usually increases under emotions like anxiety or happiness, while a slower step frequency might reflect sadness or fatigue  [21] ,  [22] . Double support time can help identify cautious and confident walking  [109] . The peak-to-height ratio between heel-strike and toe-off indicates the angle between the foot and floor during the stance phase and is related to the degree to which a person's center of gravity is tilted back (larger during relax states when leaning backward)  [110] . In addition, the FWHM indicates the duration of the heel-strike and toe-off processes, and the energy contours indicate the footstep strength and temporal changes of the footstep force. For energy features, we collectively include raw, smoothed, and logarithmic energy contours in our feature set. Based on empirical data analysis, using all three types of energy contours is superior to using any single energy contour. The likely reason for this is that raw energy reveals fine details, smoothed energy reveals overall trends, and logarithmic energy highlights changes associated with perception. To this end, combining these three types of energy features makes the analysis more robust and not limited by any single representation.\n\n2) Vibration-Related Features: We extract vibration-related features that characterize structural vibration signals correlated to footstep excitations. The vibration-related features are categorized into statistical features, time-domain features, frequency-domain features, time-frequency domain features, and compact signal representation features.\n\nStatistical features summarize the statistical properties of the overall amplitude distribution of the signal. Statistical features include the mean, median, standard deviation, maximum, range, skewness, kurtosis, number of peaks, number of valleys, autocorrelation, the slope of signal value increases or decreases in vibration signals, and spectral shape descriptors (spectrum/delta centroid, spectrum/delta crest, spectrum/delta decrease, spectrum/delta entropy, spectrum/delta flatness, spectrum/delta flux, spectrum/delta kurtosis, spectrum/delta skewness, spectrum/delta roll-off point, and spectrum/delta slope). Metrics such as mean, median, standard deviation, and range provide insights into the signal's overall shape and variability  [111] , offering a foundation for understanding general footstep properties. For instance, more forceful and inconsistent footsteps result in higher standard deviation, range, and maximum values. The number of peaks and valleys reflects the consistency of footstep forces, while slope features indicate the rate of vibration amplitude changes. Autocorrelation reveals the periodicity of the signal, which is essential for recognizing patterns and regularities, reflecting deliberate pacing or hesitation in walking. Steeper slopes correspond to harder, sharper foot-floor contacts, whereas more gradual slopes represent softer or rolling footsteps. Spectral shape descriptors indicate the overall statistical properties of energy distribution in the frequency spectrum and its temporal changes. For example, the spectral/delta centroid represents the weighted average frequency and energy-concentrated frequency band  [112] , with higher values reflecting sharper footstep impacts. Spectral/delta entropy quantifies randomness in energy distribution  [113] ,  [114] . Uneven or irregular footsteps exhibit higher entropy, while consistent, rhythmic steps correspond to lower values. Time domain features describe the behavior of vibration signals over time, corresponding to the footsteps' temporal dynamics, including jitter, shimmer, jitter relative average perturbation (jitter rap), and zero crossing rate. Jitter, shimmer, and jitter rap quantify the variability in signal frequency and amplitude  [115]  which can show footstep stability and force duration, similar to its ability to detect voice anomalies  [116] . The zero crossing rate is commonly used to distinguish between voiced and unvoiced speech  [117]  and can also help distinguish between smooth and erratic walking patterns.\n\nFrequency domain features describe the distribution of signal energy across frequencies and the shape of the frequency spectrum. They represent structural responses to footstep force excitations. Frequency domain features include Fourier transform coefficients, harmonic ratios, and cepstral features. The Fourier transform coefficients capture the distribution of vibration energy over each frequency band, which helps identify the specific structural vibration mode patterns excited by footstep forces  [108] . The harmonic ratio indicates the degree of harmonics in the signal with a larger value for a more consistent and stable gait pattern  [118] . Cepstral features reflect the rate of change in spectral components  [119] , which helps to identify the damping characteristics of structural vibration associated with foot-floor contact patterns and is robust across different structures  [120] .\n\nTime-frequency domain features capture both temporal and frequency information from vibrations, representing the coupling between the footstep force dynamics over time and the structural response patterns induced by the footstep forces. Time-frequency domain features include wavelet spectra, Hilbert-Huang transform spectra, and fundamental frequency contours. The wavelet spectra show amplitude changes in the frequency components over time  [121] , which is useful for distinguishing footstep events with similar frequencies at different gait phases. The Hilbert-Huang transform spectrum captures the signal envelope and phase information over time, making it effective for analyzing nonlinear and non-stationary vibration signals  [122] ,  [123] . This capability helps detect subtle footstep rhythm patterns. The fundamental frequency contour shows the variation of the dominant frequency over time. It reflects the excitation dynamics of the foot-floor interaction, revealing patterns such as sharp heel strike (a rising contour) or softer toe-off (a lowing contour).\n\nCompact signal representation features encode signal structure and intrinsic patterns through data compression approaches, including linear prediction coefficients and Legendre polynomial coefficients. Linear prediction coefficients capture the temporal dependencies within a signal by modeling it as a linear combination of its past values  [124] , which helps to understand the underlying signal dynamics of footstep-induced floor vibrations. The Legendre polynomial coefficients represent information efficiently with good compression, capturing the essence of the signal with fewer parameters  [125] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Module 3: General Emotion Recognition Modeling And Simplification",
      "text": "We develop a general emotion recognition model to build a preliminary relationship between emotions and footstepinduced floor vibrations before inputting specific information about the target person's footstep-induced floor vibrations. The model architecture is illustrated in Fig.  7 . The extracted emotion-sensitive features are regrouped into three types according to their data format: singular value features, onedimensional sequential features, and two-dimensional imagelike features. Distinct types of neural network layers are employed based on how each feature's data format influences information representation and extraction. For singular value features, we use fully connected layers to decode information because these layers contain separate parameters for each input feature value, enabling direct modeling between the feature and the output. Sequential features have dependencies across time steps. So, they are processed through long shortterm memory (LSTM) layers, which excel at capturing the temporal dependencies and deciding which data to retain or omit over sequences via the gating mechanism  [126] . Twodimensional time-frequency spectrum image features, such as those derived from continuous wavelet transforms (CWT), are analyzed with convolutional layers. The convolutional layers are chosen because they can preserve the spatial and temporal relationships within the time-frequency spectrum image features. The output from each processing layer is combined and input into a multilayer perceptron with two fully connected layers, which outputs valence and arousal scores for the emotion score estimation task. In addition, dropout layers are incorporated after concatenation to reduce the overfitting caused by the noise and fluctuations within the training data  [127] . The mean absolute error (MAE) is chosen as the loss function for the emotion score estimation due to its linear quantification of prediction errors, making it well-suited for regression tasks  [128] .\n\nHowever, this emotion recognition model is easily overfitted with the large number of features and the limited size of the dataset due to the high cost of human experiments. To address this problem, we simplify the emotion recognition model by cutting down the least important parameters within the model through iterative pruning. The original feature set spans over 10,000 dimensions, suggesting a requirement for a dataset size at least ten times this number, or over 100,000 footstep samples, to train the network, as indicated by  [129] ,  [130] . This requires 40 hours of continuous walking, not including the time allocated for emotion elicitation. The limited dataset can lead to model overfitting. On the other hand, conducting human experiments to gather such a vast dataset is prohibitively expensive and logistically challenging.\n\nTo this end, we simplify the model using the iterative pruning method which iteratively cuts down the least important parameters from the neural network during training. The iterative pruning method has been proven effective in reducing the computational time while keeping the model performance  [131] . In this work, iterative pruning is used for model simplification to mitigate overfitting which improves the model performance in the situation with a limited dataset. The iterative pruning algorithm includes three phases: The initial phase involves training the network to obtain baseline parameter values without any reduction, providing a foundation for evaluating parameter importance. Subsequently, the least important parameters are pruned after each training epoch, systematically eliminating non-essential parameters until this phase concludes. Finally, pruning stops and the network is further trained with the pruned architecture to improve performance. The number of epochs of each phase and the pruning rate are hyperparameters selected through the grid search method. After iterative pruning, we obtain a general emotion recognition model that can estimate emotion from the input features extracted from the footstep-induced floor vibrations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Module 4: Personalized Emotion Recognition Modeling",
      "text": "To address the large between-person variations among single footstep signals, the emotion recognition model is personalized with the incorporated footstep-induced floor vibrations from the target person (the individual whose emotions we aim to recognize). The personalization process contains two steps. First, we assess the gait similarity between the target person and the people in the training dataset and quantify the similarity using the gait similarity indices. Subsequently, we personalize the emotion recognition model by fine-tuning with these gait similarity indices as weights assigned to the training data in the loss function. During the fine-tuning, the model multiplies the loss by the sample's weight instead of treating each sample's loss equally. This allows the samples with higher weights to contribute more to the fine-tuning process. As a result, the model parameter updates prioritize minimizing loss for samples with higher weights, which represent samples that are more similar to the gait samples of the target person.\n\nWe first compare gait similarity (quantified as gait similarity indices) between the target person and people in the training set based on their footstep-induced floor vibrations. The steps for computing gait similarity indices are detailed as follows:\n\n1) Sample-wise Similarity Calculation: We first calculate the similarity between all sample pairs comprising one footstep sample from the target person (P T ) and the other from the training person (P i , where i = 1, 2, ..., n) through Euclidean distance. The distances are calculated using the samples' embedded features corresponding to the output values of the concatenated layer obtained in the general emotion recognition model. We assume that these concatenated layers effectively represent essential gait patterns, as they integrate information across all feature types. The Euclidean distance between embedded features of the k 1 -th sample (E k1 T ) from the target person and embedded features of the k 2 -th sample (E k2 i ) from training person P i is calculated as:\n\nwhere ||•|| 2 is the L2 norm representing the dissimilarity of two embedding vectors  [132] . The Euclidean distances indicate the inverse of the similarity between pairs of footstep samples, with shorter distances corresponding to greater similarity.\n\n2) Person-wise Similarity Calculation: Considering the intra-person gait variability, for each training person, we average the Euclidean distances for all samples from the target and the training person to reduce the impact of any outlier footsteps or noise, leading to a more reliable and stable similarity representation. This averaging approach mitigates the potential distortion caused by outlier samples, ensuring a more stable weight normalization process. The average Euclidean distance (D i ) is calculated as follows:\n\nIn this formula, K 1 denotes the number of gait samples from the target person (P T ), and K 2 represents the number of training people, P i . The average Euclidean distances (D i ) indicate the overall similarity of the target person's (P T ) gait patterns and the training person's (P i ) gait patterns, with smaller D i values corresponding to greater similarity.\n\n3) Gait Similarity Normalization: In order to quantify the gait similarities, the Gait Similarity Index (GSI) is established as the normalized inverse of this average distance (D i ):\n\n, where\n\nFor normalization, we divide each GSI by the maximum GSI value observed across the training people, constraining similarity indices to a range between 0 and 1, thereby facilitating weight assignment. Similar gait patterns usually have smaller Euclidean distances, thus corresponding to larger GSI values. After assessing the gait similarity indices (GSI) between the target person and the individuals in the training set, we personalize the general emotion recognition model through fine-tuning. During this process, the parameters from the pruned general model are utilized as the initial model settings. Training samples are weighted by the training people's GSI values. Consequently, data from individuals with gait patterns similar to the target person receive higher weights, thus improving model performance for the target person by concentrating training on samples of similar gait patterns.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Real-World Human Walking Evaluation",
      "text": "To evaluate the performance of EmotionVibe, we conducted real-world walking experiments, collecting a dataset from 30 participants in the laboratory setting. Data from 20 participants were used for system evaluation, with data selection criteria detailed in Section VI-C. EmotionVibe achieved mean absolute errors of 1.11 for valence score estimation and 1.07 for arousal score estimation, within a score range of 1 to 9, representing 19.0% and 25.7 % error reductions respectively compared to the baseline method, which uses only gait-related features without personalization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Human Walking Experiment Setup",
      "text": "We conducted the real-world walking experiment on a wooden platform in our lab at Stanford University. The wooden platform, with dimensions of 7.31 meters in length and 2 meters in width, was equipped with four SM-24 geophone sensors  [133]  attached to the edge of the floor using wax and glue (see Fig.  8 ). Following the Nyquist Sampling Theorem  [134] , the sampling rate was set at 500 Hz, considering that the structural natural frequency is below 200 Hz. All experiments were carried out in compliance with approved Institutional Review Board (IRB) protocols (Stanford IRB Protocol: 54912).\n\nDuring the experiment, we used a variety of music clips and light stimuli, which have been proven effective methods for emotion elicitations, as detailed in Table  I . Eight pairs of emotion elicitation were designed to elicitate four target emotional states: high valence high arousal, high valence low arousal, low valence high arousal, and low valence low arousal. The selected music clips were obtained from the PUMS database because of their efficacy in influencing human emotions  [135] . Light stimuli included a shining light for high arousal scenarios, bright white for high valence low arousal, colorful shining colors for high valence high arousal, red and yellow for low valence high arousal, and dark blue for low valence low arousal situations. The effectiveness of light stimuli in eliciting emotions is supported by  [136] -  [138] . The Govee RGB LED Strip was used to provide varying lighting conditions while music clips were played through AirPods.\n\nThe experimental procedure was as follows:\n\n1) The participant was guided to walk back and forth on the platform for 2 to 3 minutes with their initial emotions. 2) The participant was guided to complete the Self-Assessment Manikin (SAM) survey scale (see Appendix A)  [139] , rating their emotional valence and arousal on a scale from 1 to 9.\n\n3) The participant was exposed to a randomly selected emotional elicitation from the eight categories outlined in Table  I  for 1 minute. 4) The participant was guided to continue walking on the platform for 2 to 3 minutes, with music and lighting corresponding to the presented emotional elicitation. 5) The participant was guided to complete the SAM survey scale again to assess their emotional state during the walk and evaluate the effectiveness of the emotion elicitations (see Appendix A). 6) Steps 3 to 5 were repeated with alternative emotion elicitations. This was done 8 times, each with a different set of elicitations from the specified emotion categories.\n\nDuring the experiment, each participant completed nine walking trials, with each trial lasting 2 to 3 minutes. Finally, we got 37,001 footstep samples from the four sensors in the dataset for evaluation.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Dataset Overview And Effectiveness Validation",
      "text": "We evaluated the effectiveness of emotion elicitation and the variability of emotions in the dataset through their feedback on emotion elicitation and the distribution of emotion scores in the survey results. Each participant was asked to report an impact score from 1 to 9 for each set of emotional elicitation to assess the extent to which the emotional elicitation affected his or her emotion (bottom question in Appendix A). The overall average of this impact score is 5.11, suggesting a moderately strong influence of our emotion elicitation on participants' emotions (see Fig.  9 (a) ). Fig.  9  (b) shows the distribution of emotion scores within the 2D valence-arousal space. For valence, the measured values range from a minimum of 2 to a maximum of 9, while for arousal, the range spans from 1 to 9. This shows the wide variability in emotional states.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Hyper-Parameter Selection",
      "text": "Hyperparameter selection for EmotionVibe was guided by a priori knowledge of human walking patterns, empirical data observations, and a grid search for optimal neural network hyperparameters. For single footstep signal segmentation in Module 1, we identified the highest peaks of wavelet transform coefficients within the 30-70 Hz range, corresponding to the fundamental frequency band of the wooden structure used in our experiment. The footstep signal was extracted as a 0.35 s segment spanning -0.15 s to 0.2 s around each peak. To calculate the full width at half maximum (FWHM), we analyzed the signal contour in the 100-250 Hz range for higher frequencies and 10-35 Hz for lower frequencies. Double support time was determined by the time difference between peaks in the  [100, 200]  Hz and  [20, 30]  Hz bands. This is based on the observation that the higher frequency part (  [100, 200]  Hz) mainly represents heel strike (double support initiation), and the lower frequency part (  [20, 30]  Hz) represents toeoff (double support termination). This is slightly narrower than in the previous calculation because when calculating the double support time, we want to reduce the noisy peaks and keep the main peak, so narrowing down the frequency band selection shows better performance. In the wavelet choosing part, the Morse wavelet was used. When calculating the energy contour of the signal, the window size was set to 0.05 s with a smooth span of 0.5 s for the smoothed energy. The neural network architecture and hyperparameter selection were optimized via grid search. The convolutional block to process the 2D image-like features consisted of four convolutional layers, each followed by an average pooling layer with dropout regularization (dropout rate = 0.5). The 1D sequential features were processed using an LSTM block with four units.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Emotion Recognition Results",
      "text": "Our evaluation included two scenarios: Scenario A) none of the target person's data was used for model training and Scenario B) 10 minutes of walking data from the target person was used for training and the remaining part of the data (around 13 minutes of walking data) from the target person was used for testing (see  Fig 10) . In both scenarios, we calculated the average emotion recognition results from 20 tests as the evaluation result, each involving a different individual as the target person. In Scenario A, data from the 19 non-target participants were shuffled and split in a 9:1 ratio for training and validation. In Scenario B, the training and validation sets consisted of data from 19 non-target participants and 10-minute walk data from the target individuals, which were randomly assigned for training and validation in a 9:1 ratio. The test set consisted of approximately 13 additional minutes of walking data from the target individuals. The sets of tests, training, and validation were mutually exclusive. In addition, to prevent data leakage, we ensured that test data were not selected from the same walking trajectory (walking from one side of the platform to the other side once for about 10 seconds) as the training or validation set. The emotion recognition results for a trajectory were calculated based on the median scores of the emotion recognition results for each footstep in this trajectory. In both scenarios, EmotionVibe achieved accurate results for emotion score estimation. The mean absolute errors (MAEs) are 1.54 (19.5% error rate) for both valence and arousal score estimation even without any data from the target person in the training data (Scenario A). When incorporating 90 walking trajectories (approximately 10 minutes of walking) from the target person into the training data, the MAEs are reduced to 1.11 (13.9% error rate) and 1.07 (13.4% error rate) for valence and arousal score estimation, respectively, representing a 19.0% and 25.7% error reduction compared to the baseline method (see Fig.  11 ). The baseline model utilizes only gait-related features, which represent parameters derived from previous related works on gait patterns and emotions, without personalization. Fig.  12  shows the plot comparing estimated scores against ground truth for valence and arousal in Scenario B, showing moderate linear correlations with the Pearson correlation score of 0.612 and 0.695, respectively. As a reference, the state-of-the-art method leveraging gait information from videos achieves 87.5% accuracy in emotion recognition  [140] , which is comparable to our results but more intrusive and coarse-grained. EEG-based emotion recognition approaches reported an MAE of 0.48 (when scaled to the range of 1 to 9 scoring to match our scale) and a Pearson correlation of 0.8 between EEG-derived and self-reported emotion scores  [141] . However, EEG requires scalp electrodes, making it intrusive and potentially uncomfortable.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Discussion",
      "text": "In this section, we discuss the effectiveness of each module in EmotionVibe through ablation tests, evaluate the robustness of EmotionVibe's performance, and justify the participant selection criteria for model training.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Ablation Test",
      "text": "We conducted ablation tests to demonstrate the effectiveness of emotion-sensitive feature sets and the personalization method. Emotion estimation error increased in both the case of removing part of the feature set and the case of removing the personalization procedure. To demonstrate the effectiveness of the emotion-sensitive feature set, we used Scenario B (defined in Section V-D) as an example. As shown in Fig.  13  (a), using any one of the two features (gait-related and vibration-related features) gave relatively good results but not as good as using a combination of these features. This finding indicates that both features are effective in capturing the relationship between human emotions and floor vibrations. Fig.  13 (b)  shows the ablation results of the personalization method for Scenario B. Eliminating the personalization process and training the model with uniform weights across all individuals resulted in a 12.8% increase in estimation error, proving the effectiveness of personalization in improving emotion recognition performance.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. System Robustness Test",
      "text": "In this subsection, we evaluated the robustness of Emo-tionVibe's performance across individuals and its performance improvement as it progressively collected footstep samples with reported emotions from the target person.\n\n1) Robustness of System Performance Across Individuals: We evaluated the robustness of our system performance across different individuals and observed a consistent error reduction through personalization for most participants (see Fig.  14 ). EmotionVibe achieved the best performance (lowest estimation error) for Person 18, with an MAE of 0.52 (valence score estimation error: 0.48, arousal score estimation error: 0.56). In contrast, the model performed worst (highest estimation error) on Person 4's data, with an MAE of 1.9 (valence score estimation error: 2.18, arousal score estimation error: 1.68).\n\nBased on our observation, the variance in the test accuracy of emotion estimation between individuals is influenced by two key factors: the intrinsic variability of the target person's emotions and the general similarity in gait between the target person and those in the training set. Generally, a higher variance in emotional states in test data leads to a larger estimation error for the target person, as increased fluctuation in emotions makes it more challenging for the model to learn stable patterns. Additionally, when an individual's gait pattern is less similar to those in the training set, the model struggles to generalize effectively, resulting in a higher estimation error. Person 18's emotional variability is relatively low, as indicated by a low variance of 0.25 for the valence score and 0.48 for the arousal score. A smaller emotional range allows the model to learn more stable patterns, reducing estimation errors. Additionally, Person 18's gait similarity to other individuals in the training set is higher than the average, enabling the model to generalize more effectively based on learned gait-emotion relationships. In contrast, Person 4 exhibits a highly variable emotional range, with valence and arousal score variances of 7.07 and 9.16, respectively. This wide distribution increases the difficulty of accurately mapping gait features to emotional states. In addition, Person 4's gait pattern is distinctively different from the majority of individuals in the training set, as indicated by a median gait similarity that is 11.3% lower than the overall average before normalization. The reduced similarity limits the model's ability to generalize effectively, leading to a higher estimation error.\n\nNotably, even in the worst case, the MAE of our method remained below 2 within a range of 1 to 9, indicating a consistently high level of performance in emotion estimation tasks. Furthermore, the system exhibited significant error reduction for most people, with the most notable improvement observed for Person 6, where the MAE decreased from 2.07 to 1.39. However, the system did not reduce the error for Persons 4 and 16, potentially due to their unique gait patterns, which lack similar samples in the training dataset. Specifically, we observed that Person 4 wore boots with a hard heel, and Person 16 was the only participant wearing high heels, distinguishing their gait patterns from the rest of the participants. These outlier gait patterns make it challenging for the model to accurately estimate emotions without similar samples in the training dataset for reference. A potential solution could involve enriching the dataset with more diverse data, including examples of gait patterns of high heels and hard-heeled boots.\n\n2) System Performance with Varying Target Person Samples: The performance of EmotionVibe gradually improves when footstep samples are obtained from the target person. We noted a decrease in the emotion estimation error as the quantity of input data from the target person increased (see Fig.  15 ). When no footstep sample from the target person was incorporated into the training set, EmotionVibe achieved mean absolute errors (MAE) of 1.54 and 1.54 for valence and arousal estimation, respectively. When adding about 90 walking trajectories of the target person to the training set, the system performance improved, with reduced MAEs of 1.11 and 1.07 for valence and arousal score estimation, respectively.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "C. Explanation For Participant Selection Criteria",
      "text": "To ensure the reliability and validity of our analysis, we selected 20 participants from 30 participants for training data, excluding those who: (1) kicked the sensors while walking, (2) misunderstood the valence-arousal scale, leading to inconsistent or unreliable emotion reports, (3) left early without completing the experiment, and (4) exhibited small emotional response to elicitation, reporting an average impact score of emotional elicitation (Bottom question in Appendix A) of less than 2 out of 9. The first three cases introduce erroneous or incomplete data. Data from participants who exhibited small emotional responses to emotional elicitation is excluded because their low emotional variance can lead to model bias during training. Ideally, the model should learn the intended relationship: \"Changes in gait reflect changes in emotion.\" However, when certain participants display little to no emotional variation, the model risks learning an unintended shortcut: \"If I recognize this person's footsteps, I can predict their emotion as always being the same.\" While this shortcut may reduce training error, it does not accurately capture the relationship between gait and emotional changes, thereby undermining the model's validity. Although including these data in the training set may introduce bias, EmotionVibe still handles them well when they are in the test set. EmotionVibe resulted in a mean absolute error on the test set from these participants for valence and arousal estimations of approximately 0.5, corresponding to an error rate of around 6.25%.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Vii. Conclusions And Future Work",
      "text": "This paper introduces EmotionVibe, a novel emotion recognition system using footstep-induced floor vibrations. The physical insight of our system is that human emotions affect their gait patterns, which in turn influence the footstep-induced floor vibrations. Our main innovation lies in two aspects. Firstly, we develop two emotion-sensitive features, including gait-related and vibration-related features to capture the complex and indirect relationship between emotion and floor vibrations. Secondly, we personalize the emotion recognition system by assigning higher weights to people with similar gait patterns to the target person. To evaluate the effectiveness of EmotionVibe, we conducted a real-world walking experiment involving 20 participants, with a dataset of 37,001 footstep samples. Our system achieved mean absolute errors of 1.11 and 1.07 for valence and arousal score estimation respectively, achieving 19.0% and 25.7% error reductions compared to the baseline method. Our study provides a non-intrusive and privacy-friendly emotion recognition system that expands the possibilities of deploying emotion recognition systems in smart home environments for mental health monitoring and emotion-based recommendations.\n\nIn the future, we aim to extend EmotionVibe to realworld applications to recognize mixed emotions, adapt to more diverse structural environments, and take various types of human activities into account:\n\nExpanding to Include Mixed Emotions: Human emotions are often mixed, with individuals experiencing multiple emotions simultaneously  [142] -  [144] . These mixed emotions can lead to subtle and nuanced variations in gait and other behaviors, making it challenging to categorize them into distinct emotional states. Future research could explore techniques such as dynamic pattern recognition to better identify and interpret these mixed emotional states.\n\nExpanding to Diverse Structures: In the future, we aim to adapt EmotionVibe to various architectural structures. This requires the consideration of unique acoustics and vibration characteristics of different building materials such as wood, concrete, and steel. Developing a structure-invariant system can broaden its practical use in real-world environments.\n\nExpanding to Detect Various Activities: Emotions are reflected in various daily activities, including speaking, gaming, and typing. Each type of activity corresponds to a unique behavioral pattern and physical interaction with the structure. Future research will explore emotion recognition using structural vibrations induced by various activities.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EmotionVibe recognizes human emotions by analyzing the footstep-",
      "page": 1
    },
    {
      "caption": "Figure 2: ). To this end, we",
      "page": 3
    },
    {
      "caption": "Figure 2: Main Intuition of EmotionVibe. Emotion impacts gait pattern, which in turn affects footstep-induced floor vibrations.",
      "page": 4
    },
    {
      "caption": "Figure 3: Description of Emotion Using Valence-Arousal Model [88].",
      "page": 4
    },
    {
      "caption": "Figure 3: [88]. Emotions map onto a 2D space formed by the",
      "page": 4
    },
    {
      "caption": "Figure 2: ) [21]–[25]. Kinetics represents the forces and",
      "page": 4
    },
    {
      "caption": "Figure 4: (a, b)). These observations are",
      "page": 5
    },
    {
      "caption": "Figure 4: (c, d)). Relying solely on features identified in pre-",
      "page": 5
    },
    {
      "caption": "Figure 4: Representative example for footstep-induced floor vibration patterns",
      "page": 5
    },
    {
      "caption": "Figure 5: The heatmap illustrates the",
      "page": 5
    },
    {
      "caption": "Figure 5: Feature variance across the four emotion classes. Each cell indicates",
      "page": 6
    },
    {
      "caption": "Figure 6: ). After collecting and preprocessing footstep-induced",
      "page": 6
    },
    {
      "caption": "Figure 6: EmotionVibe System Overview.",
      "page": 6
    },
    {
      "caption": "Figure 7: The extracted",
      "page": 8
    },
    {
      "caption": "Figure 7: General Emotion Recognition Neural Network Architecture.",
      "page": 8
    },
    {
      "caption": "Figure 8: ). Following the Nyquist",
      "page": 9
    },
    {
      "caption": "Figure 9: (a)). Fig. 9 (b) shows the distribution",
      "page": 10
    },
    {
      "caption": "Figure 8: Real-World Walking Experiment Setup.",
      "page": 10
    },
    {
      "caption": "Figure 9: (a) The impact scores of emotional elicitation reported by participants,",
      "page": 10
    },
    {
      "caption": "Figure 10: Evaluation scenarios: (a) Scenario A: EmotionVibe has no training",
      "page": 11
    },
    {
      "caption": "Figure 10: ). In both scenarios,",
      "page": 11
    },
    {
      "caption": "Figure 11: Emotion estimation error of EmotionVibe in (a) Scenario A (where",
      "page": 11
    },
    {
      "caption": "Figure 12: Scatterplots of the estimated and ground truth emotion scores for",
      "page": 11
    },
    {
      "caption": "Figure 11: ). The baseline model utilizes",
      "page": 11
    },
    {
      "caption": "Figure 12: shows the plot comparing",
      "page": 11
    },
    {
      "caption": "Figure 13: (a) Validating the Effectiveness of the Emotion-Sensitive Feature Set:",
      "page": 12
    },
    {
      "caption": "Figure 13: (a), using",
      "page": 12
    },
    {
      "caption": "Figure 13: (b) shows the",
      "page": 12
    },
    {
      "caption": "Figure 15: ). When no footstep sample from the target person",
      "page": 12
    },
    {
      "caption": "Figure 14: Mean Absolute Error (MAE) of EmotionVibe for 20 participants",
      "page": 13
    },
    {
      "caption": "Figure 15: The performance of EmotionVibe improved as more data was",
      "page": 13
    },
    {
      "caption": "Figure 16: Emotion Survey Scale for Real-World Human Walking Experiments",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Type": "Valence",
          "Music Clips": "",
          "Light Type": ""
        },
        {
          "Emotion Type": "High",
          "Music Clips": "Peer Gynt Suite No. 1,\nOp. 46, Mvt 4,\nIn the Hall\nof\nthe Mountain King",
          "Light Type": "Colorful,\nShining"
        },
        {
          "Emotion Type": "",
          "Music Clips": "Prelude and Fugue\nNo. 15 BWV 860,\nI. Prelude in G Major",
          "Light Type": ""
        },
        {
          "Emotion Type": "High",
          "Music Clips": "Nocturne No. 2,\nOp. 9 in E Flat Major",
          "Light Type": "White,\nSteady"
        },
        {
          "Emotion Type": "",
          "Music Clips": "Blue in Green",
          "Light Type": ""
        },
        {
          "Emotion Type": "Low",
          "Music Clips": "Dracula, Vampire Hunters",
          "Light Type": "Red/Yellow,\nShining"
        },
        {
          "Emotion Type": "",
          "Music Clips": "High-Wire Stunts",
          "Light Type": ""
        },
        {
          "Emotion Type": "Low",
          "Music Clips": "Adagio in G Minor",
          "Light Type": "Dark Blue,\nSteady"
        },
        {
          "Emotion Type": "",
          "Music Clips": "The Seven Last Words of\nJesus Christ, Op 51, Mvt 3",
          "Light Type": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition for mental health prediction using ai techniques: An overview",
      "authors": [
        "S Singh",
        "N Srivastava"
      ],
      "year": "2023",
      "venue": "International Journal of Advanced Research in Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Energy-efficient home automation via emotion aware predictive modeling",
      "authors": [
        "A Rajuroy",
        "M Adetunji"
      ],
      "venue": "Energy-efficient home automation via emotion aware predictive modeling"
    },
    {
      "citation_id": "3",
      "title": "Real-time emotion recognition in smart homes",
      "authors": [
        "K Fodor",
        "Z Balogh",
        "G Molnár"
      ],
      "year": "2023",
      "venue": "2023 IEEE 17th International Symposium on Applied Computational Intelligence and Informatics (SACI)"
    },
    {
      "citation_id": "4",
      "title": "Mental illness",
      "year": "2024",
      "venue": "Mental illness"
    },
    {
      "citation_id": "5",
      "title": "Excess mortality due to depression and anxiety in the united states: results from a nationally representative survey",
      "authors": [
        "L Pratt",
        "B Druss",
        "R Manderscheid",
        "E Walker"
      ],
      "year": "2016",
      "venue": "General hospital psychiatry"
    },
    {
      "citation_id": "6",
      "title": "Everyday emotional dynamics in major depression",
      "authors": [
        "J Nelson",
        "A Klumparendt",
        "P Doebler",
        "T Ehring"
      ],
      "year": "2020",
      "venue": "Emotion"
    },
    {
      "citation_id": "7",
      "title": "Early identification of psychiatric disorders",
      "authors": [
        "T Lee",
        "M Kim",
        "J Kwon"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychiatry: Artificial Intelligence, Precision Medicine, and Other Paradigm Shifts"
    },
    {
      "citation_id": "8",
      "title": "Contextual music information retrieval and recommendation: State of the art and challenges",
      "authors": [
        "M Kaminskas",
        "F Ricci"
      ],
      "year": "2012",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "9",
      "title": "Effects of illuminance and correlated color temperature on emotional responses and lighting adjustment behaviors",
      "authors": [
        "A Mostafavi",
        "T Xu",
        "S Kalantari"
      ],
      "year": "2024",
      "venue": "Journal of Building Engineering"
    },
    {
      "citation_id": "10",
      "title": "The relationship between human and smart tvs based on emotion recognition in hci",
      "authors": [
        "J.-S Lee",
        "D.-H Shin"
      ],
      "year": "2014",
      "venue": "Computational Science and Its Applications-ICCSA 2014: 14th International Conference"
    },
    {
      "citation_id": "11",
      "title": "Towards emotion-based adaptive games: Emotion recognition input and performance features",
      "authors": [
        "J Frommel",
        "C Schrader",
        "M Weber"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "12",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia",
      "authors": [
        "A Raheel",
        "M Majid",
        "M Alnowami",
        "S Anwar"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "15",
      "title": "A study on computer vision for facial emotion recognition",
      "authors": [
        "Z.-Y Huang",
        "C.-C Chiang",
        "J.-H Chen",
        "Y.-C Chen",
        "H.-L Chung",
        "Y.-P Cai",
        "H.-C Hsu"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in sound",
      "authors": [
        "A Popova",
        "A Rassadin",
        "A Ponomarenko"
      ],
      "year": "2017",
      "venue": "Advances in Neural Computation, Machine Learning, and Cognitive Research: Selected Papers from the XIX International Conference on Neuroinformatics"
    },
    {
      "citation_id": "17",
      "title": "A new approach of audio emotion recognition",
      "authors": [
        "C Ooi",
        "K Seng",
        "L.-M Ang",
        "L Chew"
      ],
      "year": "2014",
      "venue": "Expert systems with applications"
    },
    {
      "citation_id": "18",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "N Ahmed",
        "Z Aghbari",
        "S Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "19",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition using footstep-induced floor vibration signals",
      "authors": [
        "Y Wu",
        "Y Dong",
        "S Vaid",
        "G Harari",
        "H Noh"
      ],
      "year": "2023",
      "venue": "Structural Health Monitoring 2023"
    },
    {
      "citation_id": "21",
      "title": "Emotional states affect steady state walking performance",
      "authors": [
        "A Homagain",
        "K Martens"
      ],
      "year": "2023",
      "venue": "Plos one"
    },
    {
      "citation_id": "22",
      "title": "Critical features for the perception of emotion from gait",
      "authors": [
        "C Roether",
        "L Omlor",
        "A Christensen",
        "M Giese"
      ],
      "year": "2009",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition using kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "W Wang",
        "Y Guo",
        "V Leung"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "25",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "26",
      "title": "Modeling foot-floor interactions during walking for normal and abnormal gaits",
      "authors": [
        "Y Dong",
        "Y Wu",
        "S Kim",
        "K Schadl",
        "J Rose",
        "H Noh"
      ],
      "year": "2025",
      "venue": "Journal of Engineering Mechanics"
    },
    {
      "citation_id": "27",
      "title": "Footprintid: Indoor pedestrian identification through ambient structural vibration sensing",
      "authors": [
        "S Pan",
        "T Yu",
        "M Mirshekari",
        "J Fagert",
        "A Bonde",
        "O Mengshoel",
        "H Noh",
        "P Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "28",
      "title": "In-home gait abnormality detection through footstep-induced floor vibration sensing and person-invariant contrastive learning",
      "authors": [
        "Y Dong",
        "S Kim",
        "K Schadl",
        "P Huang",
        "W Ding",
        "J Rose",
        "H Noh"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "29",
      "title": "Review on emotion recognition based on electroencephalography",
      "authors": [
        "H Liu",
        "Y Zhang",
        "Y Li",
        "X Kong"
      ],
      "year": "2021",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "30",
      "title": "Eeg-based emotion recognition",
      "authors": [
        "D Bos"
      ],
      "year": "2006",
      "venue": "The influence of visual and auditory stimuli"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition from eeg using higher order crossings",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on information Technology in Biomedicine"
    },
    {
      "citation_id": "32",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-a review",
      "authors": [
        "M Hasnul",
        "N Aziz",
        "S Alelyani",
        "M Mohana",
        "A Aziz"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "An ensemble learning approach for electrocardiogram sensor based human emotion recognition",
      "authors": [
        "T Dissanayake",
        "Y Rajapaksha",
        "R Ragel",
        "I Nawinne"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "34",
      "title": "The research on emotion recognition from ecg signal",
      "authors": [
        "C Jing",
        "G Liu",
        "M Hao"
      ],
      "year": "2009",
      "venue": "2009 international conference on information technology and computer science"
    },
    {
      "citation_id": "35",
      "title": "Recognizing emotions in human computer interaction: studying stress using skin conductance",
      "authors": [
        "A Liapis",
        "C Katsanos",
        "D Sotiropoulos",
        "M Xenos",
        "N Karousos"
      ],
      "year": "2015",
      "venue": "Human-Computer Interaction-INTERACT 2015: 15th IFIP TC 13 International Conference"
    },
    {
      "citation_id": "36",
      "title": "Skin admittance measurement for emotion recognition: A study over frequency sweep",
      "authors": [
        "A Greco",
        "A Lanata",
        "L Citi",
        "N Vanello",
        "G Valenza",
        "E Scilingo"
      ],
      "year": "2016",
      "venue": "Electronics"
    },
    {
      "citation_id": "37",
      "title": "Heart rate variability is associated with emotion recognition: Direct evidence for a relationship between the autonomic nervous sys-tem and social cognition",
      "authors": [
        "D Quintana",
        "A Guastella",
        "T Outhred",
        "I Hickie",
        "A Kemp"
      ],
      "year": "2012",
      "venue": "International journal of psychophysiology"
    },
    {
      "citation_id": "38",
      "title": "Heart rate variability as an index of regulated emotional responding",
      "authors": [
        "B Appelhans",
        "L Luecken"
      ],
      "year": "2006",
      "venue": "Review of general psychology"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition through facial expression analysis based on a neurofuzzy network",
      "authors": [
        "S Ioannou",
        "A Raouzaiou",
        "V Tzouvaras",
        "T Mailis",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "41",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Z Liu",
        "M Wu",
        "W Cao",
        "L Chen",
        "J Xu",
        "R Zhang",
        "M Zhou",
        "J Mao"
      ],
      "year": "2017",
      "venue": "IEEE CAA J. Autom. Sinica"
    },
    {
      "citation_id": "42",
      "title": "Simplifying multimodal emotion recognition single eye movement modality",
      "authors": [
        "X Yan",
        "L.-M Zhao",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition in children with autism spectrum disorders: Relations to eye gaze and autonomic state",
      "authors": [
        "E Bal",
        "E Harden",
        "D Lamb",
        "A Van Hecke",
        "J Denver",
        "S Porges"
      ],
      "year": "2010",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "44",
      "title": "Eye-gaze analysis of facial emotion recognition and expression in adolescents with asd",
      "authors": [
        "A Wieckowski",
        "S White"
      ],
      "year": "2017",
      "venue": "Journal of Clinical Child & Adolescent Psychology"
    },
    {
      "citation_id": "45",
      "title": "Emotion recognition of human body's posture in open environment",
      "authors": [
        "C Mingming",
        "F Jiandong",
        "Z Yudong"
      ],
      "year": "2020",
      "venue": "2020 Chinese Control And Decision Conference (CCDC)"
    },
    {
      "citation_id": "46",
      "title": "Adaptive real-time emotion recognition from body movements",
      "authors": [
        "W Wang",
        "V Enescu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "47",
      "title": "Motion capture sensor-based emotion recognition using a bi-modular sequential neural network",
      "authors": [
        "Y Bhatia",
        "A Bari",
        "G.-S Hsu",
        "M Gavrilova"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "48",
      "title": "Gesture-based affective computing on motion capture data",
      "authors": [
        "A Kapur",
        "A Kapur",
        "N Virji-Babul",
        "G Tzanetakis",
        "P Driessen"
      ],
      "year": "2005",
      "venue": "Affective Computing and Intelligent Interaction: First International Conference, ACII 2005"
    },
    {
      "citation_id": "49",
      "title": "Emotion-recognition using smart watch accelerometer data: Preliminary findings",
      "authors": [
        "J Quiroz",
        "M Yong",
        "E Geangu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "50",
      "title": "Using machine learning and smartphone and smartwatch data to detect emotional states and transitions: exploratory study",
      "authors": [
        "M Sultana",
        "M Al-Jefri",
        "J Lee"
      ],
      "year": "2020",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "51",
      "title": "A review of emotion recognition methods based on data acquired via smartphone sensors",
      "authors": [
        "A Kołakowska",
        "W Szwoch",
        "M Szwoch"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "52",
      "title": "Smartphone accelerometer data used for detecting human emotions",
      "authors": [
        "A Olsen",
        "J Torresen"
      ],
      "year": "2016",
      "venue": "2016 3rd International Conference on Systems and Informatics (ICSAI"
    },
    {
      "citation_id": "53",
      "title": "Emotion detection through smartphone's accelerometer and gyroscope sensors",
      "authors": [
        "O Piskioulis",
        "K Tzafilkou",
        "A Economides"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM conference on user modeling, adaptation and personalization"
    },
    {
      "citation_id": "54",
      "title": "Recognition of emotions in gait patterns by means of artificial neural nets",
      "authors": [
        "D Janssen",
        "W Schöllhorn",
        "J Lubienetzki",
        "K Fölling",
        "H Kokenge",
        "K Davids"
      ],
      "year": "2008",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition by skeleton-based spatial and temporal analysis",
      "authors": [
        "A Oguz",
        "Ö Ertugrul"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "56",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Del Bimbo"
      ],
      "year": "2017",
      "venue": "Image Analysis and Processing-ICIAP 2017: 19th International Conference"
    },
    {
      "citation_id": "57",
      "title": "Expression of emotion in the kinematics of locomotion",
      "authors": [
        "A Barliya",
        "L Omlor",
        "M Giese",
        "A Berthoz",
        "T Flash"
      ],
      "year": "2013",
      "venue": "Experimental brain research"
    },
    {
      "citation_id": "58",
      "title": "Identifying emotions from noncontact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "60",
      "title": "Emotion recognition using smart watch sensor data: Mixed-design study",
      "authors": [
        "J Quiroz",
        "E Geangu",
        "M Yong"
      ],
      "year": "2018",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "61",
      "title": "Emotion recognition based on customized smart bracelet with built-in accelerometer",
      "authors": [
        "Z Zhang",
        "Y Song",
        "L Cui",
        "X Liu",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "62",
      "title": "Emotion detection from natural walking",
      "authors": [
        "L Cui",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "Human Centered Computing: Second International Conference, HCC 2016"
    },
    {
      "citation_id": "63",
      "title": "Automatical emotion recognition based on daily gait",
      "authors": [
        "X Liu",
        "T Zhu"
      ],
      "year": "2019",
      "venue": "Analyzing Human Behavior in Cyberspace"
    },
    {
      "citation_id": "64",
      "title": "The relation between vocal pitch and vocal emotion recognition abilities in people with autism spectrum disorder and typical development",
      "authors": [
        "S Schelinski",
        "K Von Kriegstein"
      ],
      "year": "2019",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "65",
      "title": "The sound of emotional prosody: Nearly 3 decades of research and future directions",
      "authors": [
        "P Larrouy-Maestri",
        "D Poeppel",
        "M Pell"
      ],
      "year": "2024",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "66",
      "title": "Large language models on finegrained emotion detection dataset with data augmentation and transfer learning",
      "authors": [
        "K Wang",
        "Z Jing",
        "Y Su",
        "Y Han"
      ],
      "year": "2024",
      "venue": "Large language models on finegrained emotion detection dataset with data augmentation and transfer learning",
      "arxiv": "arXiv:2403.06108"
    },
    {
      "citation_id": "67",
      "title": "Exploring text-generating large language models (llms) for emotion recognition in affective intelligent agents",
      "authors": [
        "A Pico",
        "E Vivancos",
        "A García-Fornes",
        "V Botti"
      ],
      "year": "2024",
      "venue": "ICAART"
    },
    {
      "citation_id": "68",
      "title": "Trends in audio signal feature extraction methods",
      "authors": [
        "G Sharma",
        "K Umapathy",
        "S Krishnan"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "69",
      "title": "A matlab toolbox for musical feature extraction from audio",
      "authors": [
        "O Lartillot",
        "P Toiviainen"
      ],
      "year": "2007",
      "venue": "International conference on digital audio effects"
    },
    {
      "citation_id": "70",
      "title": "Feature extraction, recognition, and classification of acoustic emission waveform signal of coal rock sample under uniaxial compression",
      "authors": [
        "Z Ding",
        "X Li",
        "X Huang",
        "M Wang",
        "Q Tang",
        "J Jia"
      ],
      "year": "2022",
      "venue": "International Journal of Rock Mechanics and Mining Sciences"
    },
    {
      "citation_id": "71",
      "title": "Vibration feature extraction techniques for fault diagnosis of rotating machinery: a literature survey",
      "authors": [
        "H Yang",
        "J Mathew",
        "L Ma"
      ],
      "year": "2003",
      "venue": "Asia-pacific vibration conference"
    },
    {
      "citation_id": "72",
      "title": "Vibration feature extraction using signal processing techniques for structural health monitoring: A review",
      "authors": [
        "C Zhang",
        "A Mousavi",
        "S Masri",
        "G Gholipour",
        "K Yan",
        "X Li"
      ],
      "year": "2022",
      "venue": "Mechanical Systems and Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "Automatic speech recognition",
      "authors": [
        "D Yu",
        "L Deng"
      ],
      "year": "2016",
      "venue": "Automatic speech recognition"
    },
    {
      "citation_id": "74",
      "title": "Speaker recognition: A tutorial",
      "authors": [
        "J Campbell"
      ],
      "year": "1997",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "75",
      "title": "Human-structure interaction in vertical vibrations",
      "authors": [
        "B Ellis",
        "T Ji",
        "Bre"
      ],
      "year": "1997",
      "venue": "Proceedings of the Institution of Civil Engineers-Structures and Buildings"
    },
    {
      "citation_id": "76",
      "title": "Deep learning for audio signal classification",
      "authors": [
        "A Bose",
        "B Tripathy"
      ],
      "year": "2020",
      "venue": "Deep learning research and applications"
    },
    {
      "citation_id": "77",
      "title": "Automated damage-sensitive feature extraction using unsupervised convolutional neural networks",
      "authors": [
        "Z Wang",
        "Y.-J Cha"
      ],
      "year": "2018",
      "venue": "Sensors and Smart Structures Technologies for Civil, Mechanical, and Aerospace Systems"
    },
    {
      "citation_id": "78",
      "title": "Structural damage detection with automatic feature-extraction through deep learning",
      "authors": [
        "Z Y.-Z. Lin",
        "H.-W -H. Nie",
        "Ma"
      ],
      "year": "2017",
      "venue": "Computer-Aided Civil and Infrastructure Engineering"
    },
    {
      "citation_id": "79",
      "title": "Learning both weights and connections for efficient neural network",
      "authors": [
        "S Han",
        "J Pool",
        "J Tran",
        "W Dally"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "80",
      "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "authors": [
        "J Frankle",
        "M Carbin"
      ],
      "year": "2018",
      "venue": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "arxiv": "arXiv:1803.03635"
    },
    {
      "citation_id": "81",
      "title": "Structured pruning of deep convolutional neural networks",
      "authors": [
        "S Anwar",
        "K Hwang",
        "W Sung"
      ],
      "year": "2017",
      "venue": "ACM Journal on Emerging Technologies in Computing Systems (JETC)"
    },
    {
      "citation_id": "82",
      "title": "Structured pruning of neural networks with budget-aware regularization",
      "authors": [
        "C Lemaire",
        "A Achkar",
        "P.-M Jodoin"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "83",
      "title": "Data-efficient structured pruning via submodular optimization",
      "authors": [
        "M Halabi",
        "S Srinivas",
        "S Lacoste-Julien"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "84",
      "title": "Comparative analysis of structured pruning and unstructured pruning",
      "authors": [
        "Z Yang",
        "H Zhang"
      ],
      "year": "2021",
      "venue": "International Conference on Frontier Computing"
    },
    {
      "citation_id": "85",
      "title": "Knowledge distillation: A survey",
      "authors": [
        "J Gou",
        "B Yu",
        "S Maybank",
        "D Tao"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "86",
      "title": "On the efficacy of knowledge distillation",
      "authors": [
        "J Cho",
        "B Hariharan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "87",
      "title": "Relational knowledge distillation",
      "authors": [
        "W Park",
        "D Kim",
        "Y Lu",
        "M Cho"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "88",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "89",
      "title": "The emotions",
      "authors": [
        "N Frijda"
      ],
      "year": "1986",
      "venue": "The emotions"
    },
    {
      "citation_id": "90",
      "title": "Effort-shape and kinematic assessment of bodily expression of emotion during gait",
      "authors": [
        "M Gross",
        "E Crane",
        "B Fredrickson"
      ],
      "year": "2012",
      "venue": "Human movement science"
    },
    {
      "citation_id": "91",
      "title": "Emotion recognition from gait analyses: Current research and future directions",
      "authors": [
        "S Xu",
        "J Fang",
        "X Hu",
        "E Ngai",
        "Y Guo",
        "V Leung",
        "J Cheng",
        "B Hu"
      ],
      "venue": "Emotion recognition from gait analyses: Current research and future directions",
      "arxiv": "arXiv:2003.11461"
    },
    {
      "citation_id": "92",
      "title": "Gait, balance and posture in major mental illnesses: depression, anxiety and schizophrenia",
      "authors": [
        "R Feldman",
        "S Schreiber",
        "C Pick",
        "E Been"
      ],
      "year": "2020",
      "venue": "Austin Medical Sciences"
    },
    {
      "citation_id": "93",
      "title": "Embodiment of sadness and depression-gait patterns associated with dysphoric mood",
      "authors": [
        "J Michalak",
        "N Troje",
        "J Fischer",
        "P Vollmar",
        "T Heidenreich",
        "D Schulte"
      ],
      "year": "2009",
      "venue": "Psychosomatic medicine"
    },
    {
      "citation_id": "94",
      "title": "Emotional state affects the initiation of forward gait",
      "authors": [
        "K Naugle",
        "C Hass",
        "J Joyner",
        "S Coombes",
        "C Janelle"
      ],
      "year": "2011",
      "venue": "Emotion"
    },
    {
      "citation_id": "95",
      "title": "The effect of emotion on movement smoothness during gait in healthy young adults",
      "authors": [
        "G Kang",
        "M Gross"
      ],
      "year": "2016",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "96",
      "title": "Spatiotemporal gait parameter fluctuations in older adults affected by mild cognitive impairment: comparisons among three cognitive dual-task tests",
      "authors": [
        "S Du",
        "X Ma",
        "J Wang",
        "Y Mi",
        "J Zhang",
        "C Du",
        "X Li",
        "H Tan",
        "C Liang",
        "T Yang"
      ],
      "year": "2023",
      "venue": "Spatiotemporal gait parameter fluctuations in older adults affected by mild cognitive impairment: comparisons among three cognitive dual-task tests"
    },
    {
      "citation_id": "97",
      "title": "Spatiotemporal gait patterns during over ground locomotion in major depression compared with healthy controls",
      "authors": [
        "M Lemke",
        "T Wendorff",
        "B Mieth",
        "K Buhl",
        "M Linnemann"
      ],
      "year": "2000",
      "venue": "Journal of psychiatric research"
    },
    {
      "citation_id": "98",
      "title": "Md-vibe: physics-informed analysis of patient-induced structural vibration data for monitoring gait health in individuals with muscular dystrophy",
      "authors": [
        "Y Dong",
        "J Zou",
        "J Liu",
        "J Fagert",
        "M Mirshekari",
        "L Lowes",
        "M Iammarino",
        "P Zhang",
        "H Noh"
      ],
      "year": "2020",
      "venue": "Adjunct proceedings of the 2020 ACM international joint conference on pervasive and ubiquitous computing and proceedings of the 2020 ACM international symposium on wearable computers"
    },
    {
      "citation_id": "99",
      "title": "Characterizing the variability of footstep-induced structural vibrations for open-world person identification",
      "authors": [
        "Y Dong",
        "J Fagert",
        "H Noh"
      ],
      "year": "2023",
      "venue": "Mechanical Systems and Signal Processing"
    },
    {
      "citation_id": "100",
      "title": "Detecting gait abnormalities in foot-floor contacts during walking through footstep-induced structural vibrations",
      "authors": [
        "Y Dong",
        "Y Wu",
        "H Noh"
      ],
      "year": "2023",
      "venue": "STRUCTURAL HEALTH MONITORING 2023"
    },
    {
      "citation_id": "101",
      "title": "Occupant localization using footstep-induced structural vibration",
      "authors": [
        "M Mirshekari",
        "S Pan",
        "J Fagert",
        "E Schooler",
        "P Zhang",
        "H Noh"
      ],
      "year": "2018",
      "venue": "Mechanical Systems and Signal Processing"
    },
    {
      "citation_id": "102",
      "title": "Gait health monitoring through footstep-induced floor vibrations",
      "authors": [
        "J Fagert",
        "M Mirshekari",
        "S Pan",
        "P Zhang",
        "H Noh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 18th international conference on information processing in sensor networks"
    },
    {
      "citation_id": "103",
      "title": "Occupant-detection strategy using footstep-induced floor vibrations",
      "authors": [
        "S Drira",
        "Y Reuland",
        "N Olsen",
        "S Pai",
        "I Smith"
      ],
      "year": "2019",
      "venue": "Proceedings of the 1st ACM international workshop on device-free human sensing"
    },
    {
      "citation_id": "104",
      "title": "Using footstep-induced vibrations for occupant detection and recognition in buildings",
      "authors": [
        "S Drira",
        "S Pai",
        "Y Reuland",
        "N Olsen",
        "I Smith"
      ],
      "year": "2021",
      "venue": "Advanced Engineering Informatics"
    },
    {
      "citation_id": "105",
      "title": "Indoor person identification through footstep induced structural vibration",
      "authors": [
        "S Pan",
        "N Wang",
        "Y Qian",
        "I Velibeyoglu",
        "H Noh",
        "P Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 16th International Workshop on Mobile Computing Systems and Applications"
    },
    {
      "citation_id": "106",
      "title": "Characterizing left-right gait balance using footstep-induced structural vibrations",
      "authors": [
        "J Fagert",
        "M Mirshekari",
        "S Pan",
        "P Zhang",
        "H Noh"
      ],
      "year": "2017",
      "venue": "Sensors and Smart Structures Technologies for Civil, Mechanical, and Aerospace Systems"
    },
    {
      "citation_id": "107",
      "title": "Ubiquitous gait analysis through footstepinduced floor vibrations",
      "authors": [
        "Y Dong",
        "H Noh"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "108",
      "title": "Structure-and sampling-adaptive gait balance symmetry estimation using footstep-induced structural floor vibrations",
      "authors": [
        "J Fagert",
        "M Mirshekari",
        "S Pan",
        "L Lowes",
        "M Iammarino",
        "P Zhang",
        "H Noh"
      ],
      "year": "2021",
      "venue": "Journal of Engineering Mechanics"
    },
    {
      "citation_id": "109",
      "title": "Does falls efficacy influence the relationship between forward and backward walking speed after stroke?",
      "authors": [
        "K Bansal",
        "D Clark",
        "E Fox",
        "D Rose"
      ],
      "year": "2021",
      "venue": "Physical therapy"
    },
    {
      "citation_id": "110",
      "title": "Perception of emotion in torso and arm movements on humanoid robot quori",
      "authors": [
        "R Kaushik",
        "R Simmons"
      ],
      "year": "2021",
      "venue": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "111",
      "title": "Descriptive statistics",
      "authors": [
        "P Kaur",
        "J Stoltzfus",
        "V Yellapu"
      ],
      "year": "2018",
      "venue": "International Journal of Academic Medicine"
    },
    {
      "citation_id": "112",
      "title": "A large set of audio features for sound description (similarity and classification) in the cuidado project",
      "authors": [
        "G Peeters"
      ],
      "year": "2004",
      "venue": "CUIDADO Ist Project Report"
    },
    {
      "citation_id": "113",
      "title": "Spectral entropy based feature for robust asr",
      "authors": [
        "H Misra",
        "S Ikbal",
        "H Bourlard",
        "H Hermansky"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "114",
      "title": "A speech/music discriminator of radio recordings based on dynamic programming and bayesian networks",
      "authors": [
        "A Pikrakis",
        "T Giannakopoulos",
        "S Theodoridis"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "115",
      "title": "Jitter and shimmer measurements for speaker recognition",
      "authors": [
        "M Farrús",
        "J Hernando",
        "P Ejarque"
      ],
      "year": "2007",
      "venue": "Jitter and shimmer measurements for speaker recognition"
    },
    {
      "citation_id": "116",
      "title": "International Speech Communication Association (ISCA)",
      "year": "2007",
      "venue": "International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "117",
      "title": "Acoustic parameters for the evaluation of voice quality in patients with voice disorders",
      "authors": [
        "G Li",
        "Q Hou",
        "C Zhang",
        "Z Jiang",
        "S Gong"
      ],
      "year": "2021",
      "venue": "Annals of Palliative Medicine"
    },
    {
      "citation_id": "118",
      "title": "Separation of voiced and unvoiced using zero crossing rate and energy of the speech signal",
      "authors": [
        "R Bachu",
        "S Kopparthi",
        "B Adapa",
        "B Barkana"
      ],
      "year": "2008",
      "venue": "American Society for Engineering Education (ASEE) zone conference proceedings"
    },
    {
      "citation_id": "119",
      "title": "Harmonic ratios: a quantification of step to step symmetry",
      "authors": [
        "J Bellanca",
        "K Lowry",
        "J Vanswearingen",
        "J Brach",
        "M Redfern"
      ],
      "year": "2013",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "120",
      "title": "Analysis and improvements of the cepstrum method for fundamental frequency estimation in music signals",
      "authors": [
        "J Gauer",
        "D Kleingarn",
        "R Martin"
      ],
      "year": "2021",
      "venue": "2021 29th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "121",
      "title": "Clean vibes: Hand washing monitoring using structural vibration sensing",
      "authors": [
        "J Fagert",
        "A Bonde",
        "S Srinidhi",
        "S Hamilton",
        "P Zhang",
        "H Noh"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Computing for Healthcare (HEALTH)"
    },
    {
      "citation_id": "122",
      "title": "A practical guide to wavelet analysis",
      "authors": [
        "C Torrence",
        "G Compo"
      ],
      "year": "1998",
      "venue": "Bulletin of the American Meteorological society"
    },
    {
      "citation_id": "123",
      "title": "Hilbert-Huang transform and its applications",
      "authors": [
        "N Huang"
      ],
      "year": "2014",
      "venue": "Hilbert-Huang transform and its applications"
    },
    {
      "citation_id": "124",
      "title": "The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis",
      "authors": [
        "N Huang",
        "Z Shen",
        "S Long",
        "M Wu",
        "H Shih",
        "Q Zheng",
        "N.-C Yen",
        "C Tung",
        "H Liu"
      ],
      "year": "1971",
      "venue": "Proceedings of the Royal Society of London. Series A: mathematical, physical and engineering sciences"
    },
    {
      "citation_id": "125",
      "title": "Linear prediction: A tutorial review",
      "authors": [
        "J Makhoul"
      ],
      "year": "1975",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "126",
      "title": "Audio codec using legendre functions for simplified digital telephony",
      "authors": [
        "S Ananthi",
        "A Arun",
        "S Subramani",
        "K Padmanabhan"
      ],
      "year": "2020",
      "venue": "Proceedings of the First International Conference on Advanced Scientific Innovation in Science, Engineering and Technology"
    },
    {
      "citation_id": "127",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "128",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "129",
      "title": "Advantages of the mean absolute error (mae) over the root mean square error (rmse) in assessing average model performance",
      "authors": [
        "C Willmott",
        "K Matsuura"
      ],
      "year": "2005",
      "venue": "Climate research"
    },
    {
      "citation_id": "130",
      "title": "Is your dataset big enough? sample size requirements when using artificial neural networks for discrete choice analysis",
      "authors": [
        "A Alwosheel",
        "S Van Cranenburgh",
        "C Chorus"
      ],
      "year": "2018",
      "venue": "Journal of choice modelling"
    },
    {
      "citation_id": "131",
      "title": "What size net gives valid generalization?",
      "authors": [
        "E Baum",
        "D Haussler"
      ],
      "year": "1988",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "132",
      "title": "Compressed neural network equalization based on iterative pruning algorithm for 112-gbps vcsel-enabled optical interconnects",
      "authors": [
        "L Ge",
        "W Zhang",
        "C Liang",
        "Z He"
      ],
      "year": "2020",
      "venue": "Journal of Lightwave Technology"
    },
    {
      "citation_id": "133",
      "title": "Euclidean distance matrices: essential theory, algorithms, and applications",
      "authors": [
        "I Dokmanic",
        "R Parhizkar",
        "J Ranieri",
        "M Vetterli"
      ],
      "year": "2015",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "134",
      "title": "Sm-24 geophone element",
      "year": "2006",
      "venue": "Sm-24 geophone element"
    },
    {
      "citation_id": "135",
      "title": "A mathematical theory of communication",
      "authors": [
        "C Shannon"
      ],
      "year": "1948",
      "venue": "The Bell System Technical Journal"
    },
    {
      "citation_id": "136",
      "title": "The pums database: A corpus of previously-used musical stimuli in 306 studies of music and emotion",
      "authors": [
        "L Warrenburg"
      ],
      "year": "2021",
      "venue": "Empirical Musicology Review"
    },
    {
      "citation_id": "137",
      "title": "Arousal properties of red versus green",
      "authors": [
        "G Wilson"
      ],
      "year": "1966",
      "venue": "Perceptual and motor skills"
    },
    {
      "citation_id": "138",
      "title": "Color and psychological functioning: a review of theoretical and empirical work",
      "authors": [
        "A Elliot"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "139",
      "title": "Effects of color on emotions",
      "authors": [
        "P Valdez",
        "A Mehrabian"
      ],
      "year": "1994",
      "venue": "Journal of experimental psychology: General"
    },
    {
      "citation_id": "140",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "141",
      "title": "St-gait++: Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos",
      "authors": [
        "M Lima",
        "W Lima Costa",
        "E Martínez",
        "V Teichrieb"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "142",
      "title": "Predicting exact valence and arousal values from eeg",
      "authors": [
        "F Galvão",
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "143",
      "title": "The case for mixed emotions",
      "authors": [
        "J Larsen",
        "A Mcgraw"
      ],
      "year": "2014",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "144",
      "title": "Can mixed emotions peacefully coexist?",
      "authors": [
        "P Williams",
        "J Aaker"
      ],
      "year": "2002",
      "venue": "Journal of consumer research"
    },
    {
      "citation_id": "145",
      "title": "Further evidence for mixed emotions",
      "authors": [
        "J Larsen",
        "A Mcgraw"
      ],
      "year": "2011",
      "venue": "Journal of personality and social psychology"
    }
  ]
}