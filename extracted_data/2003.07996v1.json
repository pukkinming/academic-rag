{
  "paper_id": "2003.07996v1",
  "title": "Cross-Lingual Cross-Corpus Speech Emotion Recognition",
  "published": "2020-03-18T00:23:08Z",
  "authors": [
    "Shivali Goel",
    "Homayoon Beigi"
  ],
  "keywords": [
    "speech emotion recognition",
    "cross-corpus",
    "cross-lingual"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The majority of existing speech emotion recognition models are trained and evaluated on a single corpus and a single language setting. These systems do not perform as well when applied in a cross-corpus and crosslanguage scenario. This paper presents results for speech emotion recognition for 4 languages in both single corpus and cross corpus setting. Additionally, since multi-task learning (MTL) with gender, naturalness and arousal as auxiliary tasks has shown to enhance the generalisation capabilities of the emotion models, this paper introduces language ID as another auxiliary task in MTL framework to explore the role of spoken language on emotion recognition which has not been studied yet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech conveys human emotions most naturally. In recent years there has been an increased research interest in speech emotion recognition domain. The first step in a typical SER system is extracting linguistic and acoustic features from speech signal. Some para-linguistic studies find Low-Level Descriptor (LLD) features of the speech signal to be most relevant to studying emotions in speech. These features include frequency related parameters like pitch and jitter, energy parameters like shimmer and loudness, spectral parameters like alpha ratio and other parameters that convey cepstral and dynamic information. Feature extraction is followed with a classification task to predict the emotions of the speaker.\n\nData scarcity or lack of free speech corpus is a problem for research in speech domain in general. This also means that there are even fewer resources for studying emotion in speech. For those that are available are dissimilar in terms of the spoken language, type of emotion (i.e. naturalistic, elicited, or acted) and labelling scheme (i.e. dimensional or categorical).\n\nAcross various studies involving SER we observe that performance of model depends heavily on whether training and testing is performed from the same corpus or not. Performance is best when focus is on a single corpus at a time, without considering the performance of model in crosslanguage and cross-corpus scenarios. In this work, we work with diverse SER datasets i.e. tackle the problem in both cross-language and cross-corpus setting. We use transfer learning across SER datasets and investigate the effects of language spoken on the accuracy of the emotion recognition system using our Multi-Task Learning framework.\n\nThe paper is organized as follows: Section 2 reviewed related work on SER, cross-lingual and cross-corpus SER and the recent studies on role of language identification in speech emotion recognition system, Section 3 describes the datasets that have been used, Section 4 presents detailed descriptions of three types of SER experiments we conduct in this paper. In Section 5, we present our results and evaluations of our models. Section 6 presents some additional experiments to draw a direct comparison with previously published research. Finally, we discuss future work and conclude the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Over the last two decades there have been considerable research work on speech emotion recognition. However, all these differ in terms of the training corpora, test conditions, evaluation strategies and more which create difficulty in reproducing exact results. In  (Schuller et al., 2009a) , the authors give an overview of types of features, classi- Speech emotion recognition has evolved over time with regards to both the type of features and models used for classifiers. Different types of features that can be used can involve simple features like pitch and intensity  (Rychlicki-Kicior and Stasiak, 2014; Noroozi et al., 2017) . Some studies use low-level descriptor features(LLDs) like jitter, shimmer, HNR and spectral/cepstral parameters like alpha ratio  (Lugger and Yang, 2007; Vlasenko et al., 2007) . Other features include rhythm and sentence duration  (Jin et al., 2009)  and non-uniform perceptual linear predictive (UN-PLP) features  (Zhou et al., 2009) . Sometimes, linear predictive cepstral coefficients(LPCCs)  (Mao et al., 2009)  are used in conjunction with melfrequency cepstral coefficients (MFCCs).\n\nThere have been studies on SER in languages other than english. For example,  (Zhou et al., 2016)  propose a deep learning model consisting of stacked auto-encoders and deep belief networks for SER on the famous German dataset EMODB.  (Shaukat and Chen, 2008)  were the first to study SER work on the GEES, a Serbian emotional speech corpus. The authors developed a multistage strategy with SVMs for emotion recognition on a single dataset.\n\nRelatively fewer studies address the problem of cross-language and cross-corpus speech emotion recognition.  (Schuller et al., 2011 (Schuller et al., , 2010)) . Recent work by  (Latif et al., 2018 (Latif et al., , 2019) )  studies SER for languages belonging to different language fami-lies like Urdu vs. Italian or German. Other work involving cross-language emotion recognition includes  (Xiao et al., 2016)  which studies speech emotion recognition for for mandarin language vs. western languages like German and Danish. (Albornoz and Milone, 2017) developed an ensemble SVM for emotion detection with a focus on emotion recognition in unseen languages.\n\nAlthough there are a lot of psychological case studies on the effect of language and culture in SER, there are very few computational linguistic studies in the same domain. In  (Rajoo and Aun, 2016) , the authors support the fact that SER is language independent, however also reveal that there are language specific differences in emotion recognition in which English shows a higher recognition rate compared to Malay and Mandarin. In (Heracleous and Yoneyama, 2019) the authors proposed two-pass method based on language identification and then emotion recognition. It showed significant improvement in performance. They used English IEMOCAP, the German Emo-DB, and a Japanese corpus to recognize four emotions based on the proposed twopass method.\n\nIn  (Sagha et al., 2016) , the authors also use language identification to enhance cross-lingual SER. They concluded that in order to recognize the emotions of a speaker whose language is unknown, it is beneficial to use a language identifier followed by model selection instead of using a model which is trained based on all available languages. This work is to the best of our knowledge the first work that jointly tries to learn the language and emotion in speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "EMO-DB This dataset was introduced by  (Burkhardt et al., 2005) . Language of recordings is German and consists of acted speech with 7 categorical labels. The semantic content in this data is pre-defined in 10 emotionally neutral German short sentences. It contains 494 emotionally labeled phrases collected from 5 male and 5 female actors in age range of 21-35 years.\n\nSAVEE Surrey Audio-Visual Expressed Emotion (SAVEE) database  (Jackson and ul haq, 2011 ) is a famous acted-speech multimodal corpus. It consists of 480 British English utterances from 4 male actors in 7 different emotion categories.\n\nThe text material consisted of 15 TIMIT  (Garofolo et al., 1993)  sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phoneticallybalanced.\n\nEMOVO This  (Costantini et al., 2014 ) is an Italian language acted speech emotional corpus that contains recordings of 6 actors who acted 14 emotionally neutral short sentences sentences to simulate 7 emotional states. It consists of 588 utterances and annotated by two different groups of 24 annotators.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Masc: Mandarin Affective Speech Corpus",
      "text": "This is an Mandarin language acted speech emotional corpus that consist of 68 speakers (23 females, 45 males) each reading out read that consisted of five phrases, fifteen sentences and two paragraphs to simulate 5 emotional states. Altogether this database  (Wu et al., 2006)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser On Individual Datasets",
      "text": "The first set of experiments focused on performing speech emotion recognition for the 5 datasets individually. We perform a 5-way classification by choosing 5 emotions common in all datasets i.e. happy, sad, fear, anger and neutral. For each dataset, we experiment with different types of features and classifiers. To generate Melfrequency Cepstral Coefficients (MFCC) features we used the Kaldi-toolkit. We created spk2utt, utt2spk and wav.scp files for each dataset and generated MFCC features in .ark format. We leveraged kaldiio python library to convert .ark files to numpy arrays. Apart from MFCC's we also computed pitch features using the same toolkit. We keep a maximum of 120 frames of the input, and To compare emotion classification performance using MFCC's as input features we also tried a different feature set i.e. IS09 emotion feature set  (Schuller et al., 2009b)  which has in previous research shown good performance on SER tasks. The IS09 feature set contains 384 features that result from a systematic combination of 16 Low-Level Descriptors (LLDs) and corresponding first order delta coefficients with 12 functionals. The 16 LLDs consist of zero-crossing-rate (ZCR), root mean square (RMS) frame energy, pitch frequency (normalized to 500 Hz), harmonics-to-noise ra-tio (HNR) by autocorrelation function, and melfrequency cepstral coefficients (MFCC) 112 (in full accordance to HTK-based computation). The 12 functionals used are mean, standard deviation, kurtosis, skewness, minimum, maximum, relative position, range, and offset and slope of linear regression of segment contours, as well as its two regression coefficients with their mean square error (MSE) applied on a chunk. To get these features we had to install OpenSmile toolkit. Script to get these features after installation is included in code submitted (refer IS09 directory).\n\nOnce we had our input features ready we created test datasets from each of the 5 datasets by leaving one speaker out for small datasets (EMOVO, EMODB, SAVEE) and 2 speakers out for the larger datasets (IEMOCAP, MASC). Thus, for all corpora, the speakers in the test sets do not appear in the training set. We then performed SER using both classical machine learning and deep learning models. We used Support Vector onevs-rest classifier and Logistic Regression Classifier for classical ML models and a stacked LSTM model for the deep learning based classifier. The LSTM network comprised of 2 hidden layers with 128 LSTM cells, followed by a dense layer of size 5 with softmax activation.\n\nWe present a comparative study across all datasets, feature sets and classifiers in table 2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ser Using Transfer Learning For Small Sized Datasets",
      "text": "In the next step of experiments we tried to improve on the results we got for individual datasets by trying to leverage the technique of transfer learning. While we had relatively large support for languages like English and Chinese, speech emotion datasets for other languages like Italian and German were very small i.e. only had a total of around 500 labeled utterances. Such small amount of training data is not sufficient specially when training a deep learning based model. We used the same LSTM classifier as detailed in section 4.1. with an additional dense layer before the final dense layer with softmax. We train this base model using the large IEMOCAP English dataset. We then freeze the weights of LSTM layers i.e. only trainable weights in the classifier remain those of the penultimate dense layer. We fine tune the weights of this layer using the small datasets(eg. SAVEE, EMODB, EMOVO) and test performance on the same test sets we created in section 4.1.\n\nTable  3  shows the results of transfer learning experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multitask Learning For Ser",
      "text": "Last set of experiments focus on studying the role of language being spoken on emotion recognition. Due to the lack of adequately sized emotion corpus in many languages, researchers have previously tried training emotion recognition models on cross-corpus data i.e. training with data in one or more language and testing on another. This approach sounds valid only if we consider that expression of emotion is same in all languages i.e. no matter which language you speak, the way you convey your happiness, anger, sadness etc will remain the same. One example can be that low pitch signals are generally associated with sadness and high pitch and amplitude with anger. If expression of emotion is indeed language agnostic we could train emotion recognition models with high resource languages and use the same models for low resource languages.\n\nTo verify this hypothesis, we came up with a multi-task framework that jointly learns to predict emotion and the language in which the emotion is being expressed. The framework is illustrated in figure 2. The parameters of the LSTM model remain the same as mentioned in section 4.1. The SER performance of using training data from all languages and training a single classifier(same as shown in figure  1 ) vs. using training data from all languages in a multi-task setting is mentioned in table  4 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis",
      "text": "We will discuss the results of each experiment in detail in this section:\n\n1. For SER experiments on individual dataset we see from Table  2  that SVC classifier with IS09 input features gave the best performance for four out of 5 datasets. We also note a huge difference in accuracy scores when using the same LSTM classifier and only changing the input features i.e. MFCC and IS09. LSTM model with IS09 input features gives better emotion recognition performance for four out of 5 datasets. These experiments suggest the superiority of IS09 features as compared to MFCC's for SER tasks.\n\n2. As expected the second set of experiments show that transfer learning is beneficial for SER task for small datasets. In table  3  we observe that training on IEMOCAP and then fine-tuning on train set of small dataset improves performance for german dataset EMODB and smaller english dataset SAVEE. However, we also note a small drop in performance for Italian dataset EMOVO.\n\n3. Results in table  4  do not show improvement with using language as an auxiliary task in speech emotion recognition. While a improvement would have suggested that language spoken does affect the way people express emotions in speech, the current results are more suggestive of the fact that emotion in speech are universal i.e. language agnostic. People speaking different languages express emotions in the same way and SER models could be jointly trained across various SER corpus we have for different languages.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With Previous Research",
      "text": "In this section we present comparative study of two previous research papers with our work. We keep this report in a separate section because in order to give a direct comparison with these two papers we had to follow their train-test split, number of emotion classes etc.  Results are shown in Table  5 .\n\n2. In multi modal emotion recognition on IEMOCAP with neural networks  (Tripathi and Beigi, 2018) , the authors present three deep learning based speech emotion recognition models. We follow the exact same data pre-processing steps for obtaining same traintest split. We also use the same LSTM model as their best performing model to verify we get the same result i.e. accuracy of 55.65%. However, we could improve this performance to 56.45% by using IS09 features for input and a simple SVC classifier. This experiment suggested we could get equal or better performance in much less training time with classical machine learning models given the right input features as compared to sophisticated deep learning classifiers.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Future Work",
      "text": "In future we would like to experiment with more architectures and feature sets. We would also like to extend this study to include other languages, specially low resource languages. Since all datasets in this study were acted speech, another interesting study would be to note the differences that arise when dealing with natural speech.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "Some of the main conclusions that can be drawn from this study are that classical machine learning models may perform as well as deep learning models for SER tasks given we choose the right input features. IS09 features consistently perform well for SER tasks across datasets in different languages. Transfer learning proved to be an effective technique for performing SER for small datasets and multi-task learning experiments shed light on the language agnostic nature of speech emotion recognition task.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Transfer learning for small datasets",
      "page": 4
    },
    {
      "caption": "Figure 2: Multi-task learning for learning emotion and",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "EMO-DB",
          "Language": "German",
          "Utterances": "494",
          "#Emotion\ncategories": "7",
          "Emotion labels": "Anger, Sadness, Fear, Disgust, Boredom, Neutral, Happiness"
        },
        {
          "Dataset": "SAVEE",
          "Language": "English",
          "Utterances": "480",
          "#Emotion\ncategories": "7",
          "Emotion labels": "Anger, Sadness, Fear, Disgust, Neutral, Happiness, Surprise"
        },
        {
          "Dataset": "EMOVO",
          "Language": "Italian",
          "Utterances": "588",
          "#Emotion\ncategories": "7",
          "Emotion labels": "Anger, Sadness, Fear, Disgust, Neutral, Joy, Surprise"
        },
        {
          "Dataset": "MASC",
          "Language": "Chinese",
          "Utterances": "25636",
          "#Emotion\ncategories": "5",
          "Emotion labels": "Anger, Sadness, Panic, Neutral, Elation"
        },
        {
          "Dataset": "IEMOCAP",
          "Language": "English",
          "Utterances": "scripted: 5255 turns;\nspontaneous: 4784 turns",
          "#Emotion\ncategories": "9",
          "Emotion labels": "Anger, Happiness, Excitement, Sadness, Frustration, Fear,\nSurprise, Other and Neutral"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature Set": "",
          "Classiﬁer": "",
          "Dataset": "EMODB"
        },
        {
          "Feature Set": "MFCC",
          "Classiﬁer": "LSTM",
          "Dataset": "44.19"
        },
        {
          "Feature Set": "IS09 Emotion",
          "Classiﬁer": "Logistic Regression",
          "Dataset": "85"
        },
        {
          "Feature Set": "",
          "Classiﬁer": "SVC",
          "Dataset": "88.37"
        },
        {
          "Feature Set": "",
          "Classiﬁer": "LSTM",
          "Dataset": "86.05"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature\nSet": "",
          "Classiﬁer": "",
          "Test": "EMODB\nEMOVO\nSAVEE\nIEMOCAP\nMANDARIN"
        },
        {
          "Feature\nSet": "MFCC",
          "Classiﬁer": "LSTM (only predict emotion)",
          "Test": "58.14\n21.43\n34.44\n50.80\n43.37"
        },
        {
          "Feature\nSet": "",
          "Classiﬁer": "Multi-task LSTM (predict\nboth emotion and language ID)",
          "Test": "53.48\n28.00\n33.30\n50.69\n43.10"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: periments. we observe that training on IEMOCAP and",
      "data": [
        {
          "Train": "IEMOCAP",
          "Test": "EMOVO"
        },
        {
          "Train": "51.45",
          "Test": "33.33"
        },
        {
          "Train": "61.00",
          "Test": "32.00"
        },
        {
          "Train": "55.20",
          "Test": "31.43"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in never-seen languages using a novel ensemble method with emotion profiles",
      "authors": [
        "M Albornoz",
        "D Milone"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2503757"
    },
    {
      "citation_id": "2",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of Interspeech, Lissabon"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "4",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "5",
      "title": "A comprehensive study on bilingual and multilingual speech emotion recognition using a two-pass classification scheme",
      "authors": [
        "J Garofolo",
        "L Lamel",
        "W Fisher",
        "J Fiscus",
        "D Pallett",
        "N Dahlgren"
      ],
      "year": "1993",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0220386"
    },
    {
      "citation_id": "6",
      "title": "Surrey audiovisual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sana Ul Haq"
      ],
      "year": "2011",
      "venue": "Surrey audiovisual expressed emotion (savee) database"
    },
    {
      "citation_id": "7",
      "title": "Study on the emotion recognition of whispered speech",
      "authors": [
        "Y Jin",
        "Y Zhao",
        "C Huang",
        "L Zhao"
      ],
      "year": "2009",
      "venue": "WRI Global Congress on Intelligent Systems",
      "doi": "10.1109/GCIS.2009.175"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Junaid Qadir",
        "Muhammad Bilal"
      ],
      "year": "2019",
      "venue": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "Cross lingual speech emotion recognition: Urdu vs. western languages"
    },
    {
      "citation_id": "10",
      "title": "An incremental analysis of different feature groups in speaker independent emotion recognition",
      "authors": [
        "Marko Lugger",
        "Bin Yang"
      ],
      "year": "2007",
      "venue": "An incremental analysis of different feature groups in speaker independent emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Multi-level speech emotion recognition based on hmm and ann",
      "authors": [
        "X Mao",
        "L Chen",
        "L Fu"
      ],
      "year": "2009",
      "venue": "WRI World Congress on Computer Science and Information Engineering",
      "doi": "10.1109/CSIE.2009.113"
    },
    {
      "citation_id": "12",
      "title": "Supervised vocal-based emotion recognition using multiclass support vector machine, random forests, and adaboost",
      "authors": [
        "Fatemeh Noroozi",
        "Dorota Kamiska",
        "Tomasz Sapiski",
        "Gholamreza Anbarjafari"
      ],
      "year": "2017",
      "venue": "Journal of the Audio Engineering Society",
      "doi": "10.17743/jaes.2017.0022"
    },
    {
      "citation_id": "13",
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2019-2753"
    },
    {
      "citation_id": "14",
      "title": "Influences of languages in speech emotion recognition: A comparative study using malay, english and mandarin languages",
      "authors": [
        "R Rajoo",
        "C Aun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium on Computer Applications Industrial Electronics (ISCAIE)",
      "doi": "10.1109/ISCAIE.2016.7575033"
    },
    {
      "citation_id": "15",
      "title": "Multipitch estimation using judge-based model",
      "authors": [
        "K Rychlicki-Kicior",
        "B Stasiak"
      ],
      "year": "2014",
      "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences",
      "doi": "10.2478/bpasts-2014-0081"
    },
    {
      "citation_id": "16",
      "title": "Enhancing multilingual recognition of emotion in speech by language identification",
      "authors": [
        "Hesam Sagha",
        "Pavel Matejka",
        "Maryna Gavryukova",
        "Filip Povolny",
        "Erik Marchi",
        "Bjrn Schuller"
      ],
      "year": "2016",
      "venue": "Enhancing multilingual recognition of emotion in speech by language identification",
      "doi": "10.21437/Interspeech.2016-333"
    },
    {
      "citation_id": "17",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "Bjorn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Martin Wollmer",
        "Andre Stuhlsatz",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2010.8"
    },
    {
      "citation_id": "18",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "Bjrn Schuller",
        "Anton Batliner",
        "Stefan Steidl",
        "Dino Seppi"
      ],
      "year": "2011",
      "venue": "Sensing Emotion and Affect -Facing Realism in Speech Processing",
      "doi": "10.1016/j.specom.2011.01.011"
    },
    {
      "citation_id": "19",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "Bjrn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "20",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "Bjrn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "21",
      "title": "Towards automatic emotional state categorization from speech signals",
      "authors": [
        "Arslan Shaukat",
        "Ke Chen"
      ],
      "year": "2008",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH-Proc. Annu. Conf. Int. Speech"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multimodal emotion recognition on iemocap dataset using deep learning"
    },
    {
      "citation_id": "23",
      "title": "Frame vs. turnlevel: Emotion recognition from speech considering static and dynamic processing",
      "authors": [
        "Bogdan Vlasenko",
        "Björn Schuller",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2007",
      "venue": "Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "24",
      "title": "Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition",
      "authors": [
        "T Wu",
        "Y Yang",
        "Z Wu",
        "D Li"
      ],
      "year": "2006",
      "venue": "2006 IEEE Odyssey -The Speaker and Language Recognition Workshop",
      "doi": "10.1109/ODYSSEY.2006.248084"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition cross language families: Mandarin vs. western languages",
      "authors": [
        "Z Xiao",
        "D Wu",
        "X Zhang",
        "Z Tao"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Progress in Informatics and Computing (PIC)",
      "doi": "10.1109/PIC.2016.7949505"
    },
    {
      "citation_id": "26",
      "title": "Deep learning based affective model for speech emotion recognition",
      "authors": [
        "X Zhou",
        "J Guo",
        "R Bie"
      ],
      "year": "2016",
      "venue": "2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress",
      "doi": "10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0133"
    },
    {
      "citation_id": "27",
      "title": "Applying articulatory features to speech emotion recognition",
      "authors": [
        "Y Zhou",
        "Y Sun",
        "L Yang",
        "Y Yan"
      ],
      "year": "2009",
      "venue": "2009 International Conference on Research Challenges in Computer Science",
      "doi": "10.1109/ICRCCS.2009.26"
    }
  ]
}