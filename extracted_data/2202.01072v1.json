{
  "paper_id": "2202.01072v1",
  "title": "Interpretability For Multimodal Emotion Recognition Using Concept Activation Vectors",
  "published": "2022-02-02T15:02:42Z",
  "authors": [
    "Ashish Ramayee Asokan",
    "Nidarshan Kumar",
    "Anirudh Venkata Ragam",
    "Shylaja S Sharath"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Interpretability",
    "Concept Activation Vectors",
    "Emotion AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition refers to the classification of input video sequences into emotion labels based on multiple input modalities (usually video, audio and text). In recent years, Deep Neural networks have shown remarkable performance in recognizing human emotions, and are on par with human-level performance on this task. Despite the recent advancements in this field, emotion recognition systems are yet to be accepted for real world setups due to the obscure nature of their reasoning and decision-making process. Most of the research in this field deals with novel architectures to improve the performance for this task, with a few attempts at providing explanations for these models' decisions. In this paper, we address the issue of interpretability for neural networks in the context of emotion recognition using Concept Activation Vectors (CAVs). To analyse the model's latent space, we define humanunderstandable concepts specific to Emotion AI and map them to the widely-used IEMOCAP multimodal database. We then evaluate the influence of our proposed concepts at multiple layers of the Bi-directional Contextual LSTM (BC-LSTM) network to show that the reasoning process of neural networks for emotion recognition can be represented using human-understandable concepts. Finally, we perform hypothesis testing on our proposed concepts to show that they are significant for interpretability of this task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The research of Machine Learning (ML) systems has witnessed a rapid growth in recent years, with their presence in diverse fields ranging from day-to-day use cases such as personal assistants and search engines to highly regulated domains involving high-risk decision-making such as medical diagnosis and autonomous driving. The increasing availability of large databases and hardware resources to train such complex ML systems have resulted in state-of-the-art performance across a wide range of tasks. However, despite these advancements, ML systems still lack transparency, i.e, the internal reasoning process of these models are hidden from the user, which can prove to be a pitfall that prevents humans from verifying the decisions made by these black box models  [1] . Došilović et al.  [2]  highlight the fact that Deep Neural Networks (DNNs) are criticized for serving only as approximations of a decisionmaking system whose decisions cannot be trusted. Therefore, these black box models must satisfy several assurances such as justifiability, usability, reliability, etc., for a practicable ML system.\n\nInterpretability for Machine Learning can be defined as the extent to which a model's decisions can be consistently predicted or accounted for  [3] . According to Carvalho et al., the taxonomy of interpretability methods is based on -(i) when the methods are applicable (Pre-Model, In-Model, Post-Model (ii) whether the model is trained with a complexity constraint or analysed post-training (Intrinsic vs Post hoc) (iii) whether the interpretation is based on the model architecture (Model-Specific vs Model-Agnostic). There is often a tradeoff between model complexity and model interpretability, i.e, the more complex a model is, the harder it is to interpret the decisions made by the same. This is especially the case with Intrinsic methods where the model is trained with an additional complexity constraint to ensure effective interpretability, which affects model performance. However, Post Hoc and Post Model methods provide interpretability post-training, thereby ensuring no loss in performance.\n\nWith a better understanding of human emotions and the increasing availability of large emotion databases, emotion recognition has become an emerging research area in recent years. Emotions can be defined as a psycho-physiological process that is initiated by interaction with (or perception of) people or situations, with varying motivation and mood  [4] . Emotion Recognition can be done using various modalities such as speech, text, EEG signals and facial expressions, among which facial expressions are more widely adopted due to easier availability of these datasets. Even though a large amount of work has been done in this field, emotion recognition is often challenging due to intra-class variance, i.e, variations in emotions among different ethnicities, cultures and age groups. In practice, it is observed that multimodal approaches are more robust to intra-class variance and often adopted by clinicians and psychologists.  [5] .\n\nMultimodal Emotion Recognition finds its application in the healthcare industry to provide a preliminary assessment of a patient's emotional state. Such systems have been used in a clinical setting for the diagnosis of medical conditions such as arXiv:2202.01072v1 [cs.LG] 2 Feb 2022 Schizophrenia and Autism  [6] . Due to the limited exploration of interpretability for emotion recognition by prior work, we address this problem using Concept Activation Vectors (CAVs) to determine which concepts a model uses to recognize human emotions. Based on cues used by clinicians to recognize emotions, we define appropriate concepts with the publicly available IEMOCAP multimodal database and evaluate their significance on the Bi-Directional Contextual LSTM network. In summary, our contributions are as follows:\n\n• We extend the existing Testing with Concept Activation Vectors (TCAV) method to video, audio and text input, which is yet to be explored. • We propose novel human-understandable concepts for interpreting multimodal emotion recognition and evaluate the significance of the same.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "This section provides an overview of the recent literature on Interpretable AI and Emotion Recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Interpretable Ai",
      "text": "Interpretability aims to explain the reasoning process of DNNs through human-understandable terms to facilitate robustness and impartiality in decision-making. In addition to the broad classification of interpretability methods outlined in Sec. I, the sub-classes of methods also include Feature Attribution Methods, and Concept-based Methods, discussed in detail below.\n\n1) Feature Attribution Methods: Feature attribution methods attempt to explain each individual prediction by determining the effect (positive or negative) of each input feature on the prediction. Local Interpretable Model-Agnostic Explanations (LIME)  [7]  and SHapley Additive exPlanations (SHAP)  [8]  are some of the most well-known general feature attribution methods. LIME attempts to construct interpretable classifiers on a perturbed dataset to interpret a given model, and SHAP proposes a method to compute an additive feature attribution score with desirable properties. A special case of feature attribution is Pixel Attribution (Saliency Maps) that highlights relevant pixels for each individual prediction in image classification. Few of the methods discussed here are Grad-CAM, SmoothGrad and Integrated Gradients. Grad-CAM  [9]  highlights the important regions of an input image for an individual prediction using the gradients of the final convolutional layer to generate an activation map. SmoothGrad  [10]  attempts to improve the visual quality of gradient-based sensitivity maps by averaging those of noisy versions of the input image. Integrated Gradients  [11]  provides pixel-level attribution by computing the path integral of the gradients between a baseline input and the regular input.\n\n2) Concept-based methods: Concept-based methods aim to address interpretability in DNNs by extracting humanunderstandable concepts from a model's latent representations. Liu et al.  [12]  propose a model distillation method based on unsupervised clustering that produces an Intrinsic (interpretable by design) surrogate model. Kim et al.  [13]  introduce Concept Activation Vectors (CAVs) that use directional derivatives to represent human-understandable concepts from a model's activations and quantify the influence of a concept on the predictions of a single target class. Pfau et al.  [14]  build on TCAV by providing global and local conceptual sensitivities and accounting for the non-linear influence of concepts on a model's predictions. Lucieri et al.  [15]  explore TCAV in the context of skin lesions classification using an InceptionV4 model built by the REasoning for COmplex Data (RECOD) Lab. Ghorbani et al.  [16]  propose Automatic Concept-based Explanations (ACE) -a novel method that uses image segmentation and clustering to extract visual concepts used by a model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Ai",
      "text": "Emotion AI deals with the detection and interpretation of emotive channels involved in human communication. A considerable portion of Emotion AI research in recent years has dealt with performance improvements on the emotion recognition task through novel DNN architectures. Tripathi et al.  [4]  explore multimodal emotion recognition on the IEMOCAP database using speech, text and motion capture features. Mittal et al.  [17]  propose a novel fusion method to combine the face, text and speech modalities that is impervious to noise. Krishna et al.  [18]  propose a cross-modal attention mechanism that uses audio and text features for emotion recognition. Majumder et al.  [19]  and Poria et al.  [20]  explore hierarchical contextual feature extraction for emotion recognition, which we adopt in this work. A comprehensive list of the architectures for Multimodal Emotion Recognition (MER) and Emotion Recognition in Conversation (ERC) is provided here (ERC, 1 MER 2 ), but our main focus is the former task.\n\nInterpretability for multimodal emotion recognition has been explored with intrinsic and post-hoc methods primarily through EEG signals and speech input. Quing et al.  [21]  explore interpretable EEG-based emotion recognition using Emotional Activation Curves and evaluate their results on the DEAP and SEED dataset. Liu et al.  [22]  propose Gated Bi-directional Alignment Network that effectively captures speech-text relations, and an interpretable Group Gated Fusion (GGF) layer that determines the significance of each modality through contribution weights. Mayou et al.  [23]  propose a SincNet-based network for emotion classification with EEG signals that is interpreted by inspecting the filters learned by the model. Nguyen et al.  [24]  introduce a novel DNN architecture for multimodal emotion recognition and use nonlinear Gaussian Additive Models to interpret the same.\n\nA thorough survey of relevant literature showed that concept-based interpretation of multimodal emotion recognition is yet to be explored, and we attempt to address this gap by extending Concept Activation Vectors  [13]  to video, audio and textual data. We first define human-understandable concepts specific for emotion recognition based on inferences and observations from  [6] . The CAVs are then fitted to the BC-LSTM model's latent space at the chosen layers to compute the concept sensitivities and TCAV scores for each concept and for each target emotion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Methodology",
      "text": "In this section, we discuss the feature extraction method used for multimodal emotion recognition and introduce our human-understandable concepts for interpreting DNNs with Concept Activation Vectors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "In this work, we adopt the feature extraction method described by Poria et al.  [20] . It is carried out in 2 stages: Context-Independent Extraction that extracts the features for each input mode (audio, video and text) separately, and Context-dependent Extraction that learns features across utterances for both unimodal and multimodal emotion recognition.\n\n1) Context-Independent Feature Extraction for Unimodal Input: Feature extraction on the unimodal input is done independent of the surrounding utterances and without any contextual information (or dependency). The steps involved in feature extraction for each input mode are discussed in detail below: 2) Context-Dependent Feature Extraction: The contextual features are extracted using the Contextual-LSTM architecture proposed by Poria et al., which is a part of their Bidirectional Contextual LSTM network (Fig.  1b ). The intuition behind this architecture is that the surrounding utterances can provide essential information in the classification of the current utterance, thereby requiring a model that takes such dependencies into consideration. Let X k represent the input features for utterance k and L k represent the output of the LSTM network for utterance k. The output for the next utterance L k+1 depends on X k+1 as well as the output of the previous LSTM network L k , which represents the learning of contextual information. This contextual-LSTM module is used for unimodal and multimodal feature extraction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Interpretability Using Cavs 1) Testing With Concept Activation Vectors (Tcav):",
      "text": "To achieve interpretability in terms of human-understandable concepts, Kim et al.  [13]  proposed Concept Activation Vectors (CAVs) -a linear interpretability method that represents a concept with a vector in a neural network's activation space given a set of positive and negative examples representing the concept. Given a positive examples set X +ve and a negative examples set X -ve , a binary classifier v l C is trained to distinguish between the activations of the positive examples set f l (x), x ∈ X +ve and the negative examples set f l (x), x ∈ X -ve , where f l (x) represents the neural activation at a layer l of a network. This binary classifier v l C represents the CAV for the concept C at layer l. The Testing with CAV (TCAV) method introduces a metric known as the TCAV score that represents a ML model's sensitivity to a particular concept across all class labels. Given a concept C, the TCAV score at a layer l for examples belonging to class k (X k ) is given as:\n\nwhere S C,l,k represents the directional derivative at layer l for concept C and class k given by S C,l,k = f l (x).v l C (f l (x) is the derivative of the activation at layer l). TCAV provides a quantitative measure of conceptual sensitivity across entire input classes and can be extended to input modes other than images.\n\n2) TCAV for Emotion Recognition: Here, we delineate the concepts used to interpret multimodal emotion recognition models with TCAV. We define a single concept for each of video, audio and text input modes -Variations in Physiognomy, Voice Pitch and Utterance Polarity, which are discussed below:\n\n• Variations in Physiognomy (Deviation from Neutral Expressions). Emotions such as anger and excitement capture extreme changes in facial expressions compared to the neutral resting face. They are characterized by changes in the facial features such as eye contact, lip movement, etc. According to Grabowski et al.  [6] , analysis of emotions in a valence/arousal spectrum allows for Algorithm 1 TCAV for Emotion Recognition\n\nfor k = 1 to K do 6:\n\nend for 10: end for\n\nThe sequence of steps involved in computing conceptual sensitivities for emotion recognition are outlined in Algo. 1. Given the set of concepts C and the concept annotations set D, we wish to compute the CAV for each concept along with the TCAV scores for each concept C i , label k and layer l. Similar to the original TCAV method, the activations of the concept examples at layer l are extracted from the network and a binary classifier is fitted to these activations. However, there is an additional step while extending TCAV to emotion recognition, which is the conversion of video-level activations to utterancelevel activations (Steps 3,6 in Algo. 1). The raw activations have the dimensions (n, t, f ), where n is the number of videos, t is the sequence length and f is the number of features. These activations are reshaped to (n × t, f ) so that the first dimension represents the number of utterances. Therefore, the reshaped activations represent the utterance-level activations of the model at layer l. A binary classifier is then fitted to the concepts sets for each concept C i to obtain the CAV v Ci l . The conceptual sensitivities and TCAV scores are computed as explained in Sec. III.B.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "In this section, we cover the experimental setup used for training the multimodal emotion recognition model and interpreting the same using Concept Activation Vectors.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Model",
      "text": "To determine the influence of our concepts for emotion recognition, we make use of the Bi-directional Contextual LSTM network (Fig.  1b ) introduced by Poria et al.  [20]  trained on the IEMOCAP multimodal database (Sec. IV.B). The motivation behind choosing this architecture is the fact that this is one of the few simple and straightforward speakerindependent multimodal architectures for emotion recognition, which makes interpreting its decisions more convenient. The current state-of-the-art methods  [28]    [29]  for emotion recognition (in conversation) on IEMOCAP make use of speakerspecific components to enhance performance, which is outside the scope of our work. Contextual Hierarchical Fusion  [19]  extends the idea of contextual information to 3 hierarchical levels but provides only a marginal improvement over BC-LSTM, thereby making BC-LSTM the appropriate choice for our work. Bi-directional LSTMs are used here to account for contextual information from the preceding and following utterances for emotion classification. Fusion of the modalities is done in a hierarchical fashion consisting of 2 levels. Level 1 extracts context-sensitive information from the contextindependent features that are fed to the contextual-LSTM module. These context-sensitive unimodal features are then concatenated and fed to the final contextual-LSTM module to extract context-sensitive multimodal features. For all 6 emotion labels of the IEMOCAP database, the BC-LSTM network achieves 41.7% on video input, 47.4% on audio input, 53.7% on text input and 57.5% with all 3 inputs combined, which is in accordance to the results presented in PapersWithCode 3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Dataset",
      "text": "The dataset used to train the BC-LSTM network and define human-understandable concepts for TCAV is the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database  [30] . It consists of scripted acts and improvisations involving 10 speakers. Each video involves a conversation between 2 subjects divided into several utterances and each of these utterances is associated with one of the following 6 emotion labels: happy (0), sad (1), neutral (2), angry (3), excited (4) and frustrated  (5) . To train the BC-LSTM network, we use a 70-30 split for the training and testing sets, i.e, the training set contains 121 videos (4290 utterances) and the testing set contains 31 videos (1208 utterances).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "All the experiments are carried out on the BC-LSTM network with concept examples from the IEMOCAP database. The hierarchical components of the BC-LSTM architecture are trained sequentially and separately, i.e, the model is not trained in an \"end-to-end\" fashion. The unimodal contextual-LSTM modules are trained separately and frozen while training the multimodal contextual-LSTM module. The unimodal contextual-LSTM modules are trained separately with the Adam optimizer for 100 epochs at a learning rate of 1 × 10 -4 . Based on trials with all the layers of the model, we found that extracting the activations from the contextual-LSTM layer at the unimodal level and the Dense layer at the multimodal level gave the best results. The samples distribution for the 3 concepts are outlined in Table  1 . The concept examples are selected from the top n videos that have the maximum number of utterances from the emotion labels for positive and negative examples indicated in Table  1 .\n\nExamples for the VP concept are collected solely based on the assumption from Sec III.B. The positive examples set consists of preprocessed utterances belonging to the happy, excited and frustrated emotion classes based on manual inspection of a small subset of videos from the IEMOCAP database. For the 3 PapersWithCode -IEMOCAP Benchmark",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Table I: Sample Distribution For Our Proposed Concepts",
      "text": "PT concept, we use Self-supervised Pitch Estimation proposed by Gfeller et al.  [31]  to estimate the pitch for every utterance from the concept set and assign positive/negative labels based on a threshold pitch value of 250Hz, i.e, the preprocessed utterance belongs to the positive examples set for PT if the pitch of the utterance exceeds 250Hz. To compute the text polarity of utterances, we use the TextBlob Python library that assigns a text polarity of -1 to 1 for each utterance based on a weighted average sentiment score of the words in the utterance.\n\nTo account for variations in the binary classifiers' initialization and preprocessing of the concept examples  [15] , the training of the CAVs is repeated 30 times resulting in 30 different vectors. We evaluate the statistical significance of our concepts by training 50 random CAVs for each layer and assigning random labels. We then perform a 2-tailed t-test on the TCAV score distributions of the random concepts and the proposed concepts at a significance level α = 0.05.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Results",
      "text": "Here, we discuss the quantitative evaluation of the CAVs for our proposed concepts through the classifier accuracies, TCAV scores and hypothesis tests for concept significance. Since there is no quantitative method to compare interpretability methods and due to the lack of results for concept-based interpretation of emotion recognition, we evaluate our concepts without any comparison to prior work.\n\nFig.  2a  shows the test accuracies of the classifiers for the CAVs at the unimodal and multimodal levels of the BC-LSTM network trained on the IEMOCAP multimodal database. The overall accuracies are relatively low due to the fact that linear classifiers are used to define the concepts. The graph also shows that there is minimal variation in the classifiers' accuracies, indicating that the 30 different vectors are consistent for each of the concepts.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "1) Variations In Physiognomy (Vp):",
      "text": "The examples used to represent VP are collected based on the general assumption that emotions showing medium to high arousal such as excitement, joy, and anger  [6]  display a greater deviation from the neutral resting face and can be distinctly identified through variations in visual features. Fig.  2b  indicates the TCAV scores for VP at the unimodal and multimodal bottleneck layers. We see that the highest scores are observed for the happy and excited classes, while the scores for sad, angry and frustration classes are relatively lower, which confirms the assumption stated above. However, the scores for frustrated and neutral classes are not in line with this assumption. At the multimodal bottleneck, it is observed that VP has a strong influence on happy and sad classes but much lower influence on the rest of the classes. The consistently strong influence of VP on the happy class across the unimodal and multimodal bottlenecks is evidence for the fact that VP is essential for recognizing happiness. It is also observed that the scores for the sad, excited and frustration classes are exceptions for the general assumption stated earlier. Since the concept set is not created based on a quantitative measure for variations in facial expressions, it is possible that the inconsistencies in TCAV scores are due to the nature of the concept set given that it is only an approximation based on the general assumption. Another factor that could contribute to this discrepancy is the relatively poor performance of the BC-LSTM network on video input from IEMOCAP as mentioned in Sec. IV.A.\n\n2) Voice Pitch (PT): Pitch of an individual's voice can be used as a strong indicator of expression of specific emotions. Fig.  2b  shows the TCAV scores for PT at the unimodal and multimodal bottlenecks. It is observed that PT has the highest influence (0.865 and 0.726) on the anger and frustration classes at the unimodal bottleneck. This observation is consistent with  [27] , in that expression of such emotions are associated with high pitch and PT can be used as a distinguishing trait. This, however, is not true for the excitement class, which ideally is characterized by high pitch. At the multimodal level, it is observed that the happy, sad, neutral, and frustrated emotion classes have high scores for PT (0.843, 1, 0.988 and 0.875 respectively). This is a contradiction to the general presumption that only emotions with high arousal are associated with high pitch and that pitch can be used as a distinguishing factor. This discrepancy could be due to the nature of the utterances found in the IEMOCAP database. It is observed that some of the utterances for emotions with medium arousal (happy, sad, etc.) have higher pitch than some of the utterances for emotions with high arousal.\n\n3) Utterance Polarity (UP): Fig.  2d  shows the TCAV scores for UP at the unimodal and multimodal bottleneck layers. From the scores, it is evident that UP has a high influence on all the target emotions except the sad and neutral labels at the unimodal bottleneck, which is as expected. Phrases that depict emotions involving medium to high arousal (intensity) tend to have a high level of sentiment polarity compared to the neutral emotion. We observe that the influence of UP on the neutral and sad target emotions is relatively low, which is in line with common observations on emotion recognition using text. Emotions such as neutral and sadness are not as conveniently distinguishable as the other target emotions. Given a phrase from these emotion classes, the polarity is approximately 0, which makes it difficult to differentiate utterances of the neutral and sad classes. At the multimodal bottleneck layer, the scores are negligible for the neutral and angry classes and significant for the happy and anger classes. This signifies that UP is insignificant towards the classification of examples into the sad, neutral, excited and frustrated emotions labels and that VP, PT concepts play a more important role for these classes. Despite the high TCAV scores of UP for the sad and frustrated classes, hypothesis tests shows insignificance of the concept for these labels.\n\n4) Hypothesis Testing: To test the significance of the proposed concepts for emotion recognition, we perform a 2-tailed t-test. We first generate 50 random concept sets from the training set and assign positive and negative labels in a random fashion to the activations from the unimodal and multimodal bottleneck layers. This is followed by fitting binary classifiers to these random concept sets to generate 50 random CAVs. We then perform a hypothesis test by conducting a 2 tailed t-test for the distribution of the TCAV scores for the proposed concepts and the 50 random concepts at a significance level α = 0.05. The null and alternate hypotheses are defined as follows:\n\nNull Hypothesis H o : µ r = µ t\n\nAlternate Hypothesis H a : µ r = µ t\n\nHere, µ r represents the mean score of the random concepts distribution and µ t represents the mean score of the proposed concepts score distributions. We consider a concept to be significant if the null hypothesis is rejected for 40 of the 50 random TCAV score distributions. It is observed that (Fig.  2b, 2c, 2d ) the UP concept is significant for all emotion classes at the unimodal bottleneck and insignificant for the sad, neutral, excited and frustrated classes. The VP concept is significant for all emotion classes at both the unimodal and multimodal bottlenecks. The PT concept is significant for all emotion classes at the unimodal level but insignificant for the neutral and excited emotions at the multimodal level.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "Emotion AI has been widely used in critical domains such as medical diagnosis and security, and interpretability for emotion recognition will ensure the robustness and reliability of affective computing systems. To this end, we explore concept-based interpretation of emotion recognition through Concept Activation Vectors (CAVs) to quantify the influence of emotion-related concepts for a typical multimodal emotion recognition model. We define novel concepts based on existing Emotion AI literature, and analyse the relevance of the same. Through our results, it is evident that DNNs for emotion recognition make use of human-understandable concepts for classification, just like humans.\n\nWe further evaluate the significance of our concepts through hypothesis testing on the TCAV scores. The results show that the multimodal architecture makes use of specific concepts for specific emotion classes. There is no single concept that is significant for all the emotions, which is in line with preestablished notions for this task from the human perspective. Current literature on this task shows that most of the models trained on the IEMOCAP database tend to perform better on text input than the other two input modes, which could affect the interpretation of these models. Thus, one of the focuses for future work can be the interpretation of emotion recognition models that are independent of dataset bias.\n\nIn this work, we have explored the interpretability for emotion recognition on the BC-LSTM network, which is relatively simple compared to the state-of-the-art models. The TCAV method can be extended to more complex architectures to evaluate our concepts for these models. In addition to these improvements, the discovery of emotion-related concepts in an unsupervised setting can be a possible direction for future research. This can reduce human effort in annotating concept examples for emotion classification and enhance the interpretability of DNNs by allowing models to generate their own concepts.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: b). The intuition",
      "page": 3
    },
    {
      "caption": "Figure 1: (a) Overview of Methodology for TCAV on Emotion Recognition (b) Bottleneck layers chosen for TCAV on Emotion",
      "page": 4
    },
    {
      "caption": "Figure 1: b) introduced by Poria et al. [20]",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) Unimodal & Multimodal CAV accuracy for all 3 concepts. (b) TCAV scores for PT concept. (c) TCAV scores for",
      "page": 6
    },
    {
      "caption": "Figure 2: a shows the test accuracies of the classiﬁers for the",
      "page": 6
    },
    {
      "caption": "Figure 2: b indicates the TCAV scores",
      "page": 7
    },
    {
      "caption": "Figure 2: b shows the TCAV scores for PT at the unimodal",
      "page": 7
    },
    {
      "caption": "Figure 2: d shows the TCAV",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "Abstract—Multimodal Emotion Recognition refers to the clas-",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "are criticized for serving only as approximations of a decision-"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "siﬁcation\nof\ninput\nvideo\nsequences\ninto\nemotion\nlabels\nbased",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "making system whose decisions cannot be trusted. Therefore,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "on multiple\ninput modalities\n(usually\nvideo,\naudio\nand\ntext).",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "these black box models must\nsatisfy several assurances\nsuch"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "In recent years, Deep Neural networks have shown remarkable",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "as justiﬁability, usability, reliability, etc., for a practicable ML"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "performance\nin recognizing human emotions,\nand are\non par",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "with human-level performance on this\ntask. Despite\nthe\nrecent",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "system."
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "advancements\nin\nthis\nﬁeld,\nemotion\nrecognition\nsystems\nare",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Interpretability\nfor Machine Learning\ncan\nbe\ndeﬁned\nas"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "yet\nto be\naccepted for\nreal world setups due\nto\nthe\nobscure",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "the extent\nto which a model’s decisions can be consistently"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "nature\nof\ntheir\nreasoning\nand\ndecision-making\nprocess. Most",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "predicted or accounted for\n[3]. According to Carvalho et al.,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "of\nthe\nresearch in this ﬁeld deals with novel\narchitectures\nto",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "the\ntaxonomy\nof\ninterpretability methods\nis\nbased\non\n-\n(i)"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "improve the performance for this\ntask, with a few attempts at",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "providing explanations for these models’ decisions. In this paper,",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "when the methods are applicable (Pre-Model, In-Model, Post-"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "we address the issue of interpretability for neural networks in the",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Model\n(ii) whether\nthe model\nis\ntrained with a\ncomplexity"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "context of emotion recognition using Concept Activation Vectors",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "constraint or analysed post-training (Intrinsic vs Post hoc) (iii)"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "(CAVs). To analyse the model’s\nlatent\nspace, we deﬁne human-",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "whether\nthe interpretation is based on the model architecture"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "understandable concepts\nspeciﬁc to Emotion AI and map them",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "(Model-Speciﬁc vs Model-Agnostic). There\nis often a\ntrade-"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "to\nthe widely-used IEMOCAP multimodal database. We\nthen",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "evaluate the inﬂuence of our proposed concepts at multiple layers",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "off between model complexity and model\ninterpretability,\ni.e,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "of\nthe Bi-directional Contextual LSTM (BC-LSTM) network to",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "the more complex a model\nis,\nthe harder\nit\nis to interpret\nthe"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "show that\nthe reasoning process of neural networks for emotion",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "decisions made by the same. This is especially the case with"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "recognition\ncan\nbe\nrepresented\nusing\nhuman-understandable",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Intrinsic methods where\nthe model\nis\ntrained with an addi-"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "concepts. Finally, we perform hypothesis testing on our proposed",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "tional complexity constraint to ensure effective interpretability,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "concepts to show that\nthey are signiﬁcant\nfor interpretability of",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "this task.",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "which affects model performance. However, Post Hoc and Post"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "Index Terms—Multimodal Emotion Recognition, Interpretabil-",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Model methods provide interpretability post-training,\nthereby"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "ity, Concept Activation Vectors, Emotion AI",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "ensuring no loss in performance."
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "With a better understanding of human emotions\nand the"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "I.\nINTRODUCTION",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "increasing\navailability\nof\nlarge\nemotion\ndatabases,\nemotion"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "recognition has become an emerging research area in recent"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "The research of Machine Learning (ML) systems has wit-",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "years. Emotions\ncan\nbe\ndeﬁned\nas\na\npsycho-physiological"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "nessed a rapid growth in recent years, with their presence in di-",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "process that\nis initiated by interaction with (or perception of)"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "verse ﬁelds ranging from day-to-day use cases such as personal",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "people or\nsituations, with varying motivation and mood [4]."
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "assistants\nand\nsearch\nengines\nto\nhighly\nregulated\ndomains",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Emotion Recognition can be done using various modalities"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "involving high-risk decision-making such as medical diagnosis",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "such\nas\nspeech,\ntext, EEG signals\nand\nfacial\nexpressions,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "and autonomous driving. The increasing availability of\nlarge",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "among which\nfacial\nexpressions\nare more widely\nadopted"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "databases and hardware resources to train such complex ML",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "due\nto\neasier\navailability\nof\nthese\ndatasets. Even\nthough\na"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "systems have\nresulted in state-of-the-art performance\nacross",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "large\namount of work has been done\nin this ﬁeld,\nemotion"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "a wide range of\ntasks. However, despite these advancements,",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "recognition is often challenging due\nto intra-class variance,"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "ML systems still\nlack transparency,\ni.e,\nthe internal\nreasoning",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "i.e, variations in emotions among different ethnicities, cultures"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "process of\nthese models are hidden from the user, which can",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "and age groups.\nIn practice,\nit\nis observed that multimodal"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "prove to be a pitfall\nthat prevents humans from verifying the",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "approaches are more robust\nto intra-class variance and often"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "decisions made by these black box models\n[1]. Došilovi´c et",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "adopted by clinicians and psychologists.\n[5]."
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "al.\n[2] highlight\nthe fact\nthat Deep Neural Networks (DNNs)",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "Multimodal Emotion Recognition ﬁnds its application in the"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "healthcare industry to provide a preliminary assessment of a"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "This work\nhas\nbeen\nsubmitted\nto\nthe\nIEEE for\npossible\npublication.",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "patient’s emotional\nstate. Such systems have been used in a"
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "Copyright may be transferred without notice, after which this version may",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": ""
        },
        {
          "suryanidarshan@gmail.com\nashish.ramayee@gmail.com": "no longer be accessible.",
          "shylaja.sharath@pes.edu\nanirudhragam19@gmail.com": "clinical setting for the diagnosis of medical conditions such as"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "of\ninterpretability for emotion recognition by prior work, we",
          "on unsupervised clustering that produces an Intrinsic (inter-": "pretable by design)\nsurrogate model. Kim et al.\n[13]\nintro-"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "address this problem using Concept Activation Vectors (CAVs)",
          "on unsupervised clustering that produces an Intrinsic (inter-": "duce Concept Activation Vectors (CAVs)\nthat use directional"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "to determine which concepts a model uses to recognize human",
          "on unsupervised clustering that produces an Intrinsic (inter-": "derivatives to represent human-understandable concepts from"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "emotions. Based\non\ncues\nused\nby\nclinicians\nto\nrecognize",
          "on unsupervised clustering that produces an Intrinsic (inter-": "a model’s activations and quantify the inﬂuence of a concept"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "emotions, we deﬁne\nappropriate\nconcepts with the publicly",
          "on unsupervised clustering that produces an Intrinsic (inter-": "on the predictions of\na\nsingle\ntarget\nclass. Pfau et al.\n[14]"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "available IEMOCAP multimodal database and evaluate their",
          "on unsupervised clustering that produces an Intrinsic (inter-": "build\non TCAV by\nproviding\nglobal\nand\nlocal\nconceptual"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "signiﬁcance on the Bi-Directional Contextual LSTM network.",
          "on unsupervised clustering that produces an Intrinsic (inter-": "sensitivities\nand\naccounting\nfor\nthe\nnon-linear\ninﬂuence\nof"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "In summary, our contributions are as follows:",
          "on unsupervised clustering that produces an Intrinsic (inter-": "concepts on a model’s predictions. Lucieri et al.\n[15] explore"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "TCAV in the\ncontext of\nskin lesions\nclassiﬁcation using an"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "• We extend the existing Testing with Concept Activation",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "InceptionV4 model\nbuilt\nby\nthe REasoning\nfor COmplex"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "Vectors\n(TCAV) method to video, audio and text\ninput,",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "Data (RECOD) Lab. Ghorbani et al.\n[16] propose Automatic"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "which is yet\nto be explored.",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "Concept-based Explanations (ACE) - a novel method that uses"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "• We\npropose\nnovel\nhuman-understandable\nconcepts\nfor",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "image segmentation and clustering to extract visual concepts"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "interpreting multimodal emotion recognition and evaluate",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "used by a model."
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "the signiﬁcance of\nthe same.",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "B. Emotion AI"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "II. RELATED WORKS",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "detection\ninterpretation\nEmotion AI\ndeals with\nthe\nand"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "This\nsection provides an overview of\nthe recent\nliterature",
          "on unsupervised clustering that produces an Intrinsic (inter-": "of\nemotive\nchannels\ninvolved\nin\nhuman\ncommunication. A"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "on Interpretable AI and Emotion Recognition.",
          "on unsupervised clustering that produces an Intrinsic (inter-": "considerable portion of Emotion AI\nresearch in recent years"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "has\ndealt with\nperformance\nimprovements\non\nthe\nemotion"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "A.\nInterpretable AI",
          "on unsupervised clustering that produces an Intrinsic (inter-": "recognition\ntask\nthrough\nnovel DNN architectures. Tripathi"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "et\nal.\n[4]\nexplore multimodal\nemotion\nrecognition\non\nthe"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "Interpretability\naims\nto\nexplain\nthe\nreasoning\nprocess\nof",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "IEMOCAP database using speech,\ntext\nand motion capture"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "DNNs\nthrough human-understandable\nterms\nto facilitate\nro-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "et\nal.\nfeatures. Mittal\n[17]\npropose\na\nnovel\nfusion method"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "bustness\nand impartiality in decision-making.\nIn addition to",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "to combine\nthe\nface,\ntext\nand speech modalities\nthat\nis\nim-"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "the\nbroad\nclassiﬁcation\nof\ninterpretability methods\noutlined",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "pervious\nto noise. Krishna et al.\n[18] propose a cross-modal"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "Sec.\nin\nI,\nthe\nsub-classes\nof methods\nalso\ninclude Feature",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "attention mechanism that\nuses\naudio\nand\ntext\nfeatures\nfor"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "Attribution Methods, and Concept-based Methods, discussed",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "emotion recognition. Majumder et al. [19] and Poria et al. [20]"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "in detail below.",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "explore hierarchical contextual\nfeature extraction for emotion"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "1) Feature Attribution Methods:\nFeature attribution meth-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "recognition, which we adopt\nin this work. A comprehensive"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "ods\nattempt\nto explain each individual prediction by deter-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "list of\nthe architectures for Multimodal Emotion Recognition"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "mining\nthe\neffect\n(positive\nor\nnegative)\nof\neach\ninput\nfea-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "(MER)\nand Emotion Recognition in Conversation (ERC)\nis"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "ture\non\nthe\nprediction. Local\nInterpretable Model-Agnostic",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "provided here (ERC,1MER2), but our main focus is the former"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "Explanations (LIME)\n[7] and SHapley Additive exPlanations",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "task."
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "(SHAP)\n[8] are some of\nthe most well-known general\nfeature",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "Interpretability\nfor multimodal\nemotion\nrecognition\nhas"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "attribution methods. LIME attempts to construct\ninterpretable",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "been explored with intrinsic and post-hoc methods primarily"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "classiﬁers on a perturbed dataset\nto interpret a given model,",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "et\nal.\nthrough EEG signals\nand\nspeech\ninput. Quing\n[21]"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "and SHAP proposes a method to compute an additive feature",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "explore\ninterpretable EEG-based\nemotion\nrecognition\nusing"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "attribution\nscore with\ndesirable\nproperties. A special\ncase",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "Emotional Activation Curves\nand\nevaluate\ntheir\nresults\non"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "(Saliency Maps)\nof\nfeature\nattribution\nis Pixel Attribution",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "the DEAP and SEED dataset. Liu et al.\n[22] propose Gated"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "that highlights\nrelevant pixels\nfor each individual prediction",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "Bi-directional Alignment Network\nthat\neffectively\ncaptures"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "in image\nclassiﬁcation. Few of\nthe methods discussed here",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "speech-text relations, and an interpretable Group Gated Fusion"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "are Grad-CAM, SmoothGrad and Integrated Gradients. Grad-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "(GGF) layer that determines the signiﬁcance of each modality"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "CAM [9] highlights the important\nregions of an input\nimage",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "al.\nthrough\ncontribution weights. Mayou et\n[23]\npropose\na"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "for\nan individual prediction using the gradients of\nthe ﬁnal",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "SincNet-based network for\nemotion classiﬁcation with EEG"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "convolutional layer to generate an activation map. SmoothGrad",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "signals\nthat\nis\ninterpreted\nby\ninspecting\nthe ﬁlters\nlearned"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "[10] attempts to improve the visual quality of gradient-based",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "et\nal.\nby\nthe model. Nguyen\n[24]\nintroduce\na\nnovel DNN"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "sensitivity maps by averaging those of noisy versions of\nthe",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "architecture for multimodal emotion recognition and use non-"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "input\nimage.\nIntegrated Gradients\n[11]\nprovides\npixel-level",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "linear Gaussian Additive Models to interpret\nthe same."
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "attribution\nby\ncomputing\nthe\npath\nintegral\nof\nthe\ngradients",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "A\nthorough\nsurvey\nof\nrelevant\nliterature\nshowed\nthat"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "between a baseline input and the regular\ninput.",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "concept-based interpretation of multimodal emotion recogni-"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "2) Concept-based methods:\nConcept-based methods\naim",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "",
          "on unsupervised clustering that produces an Intrinsic (inter-": "tion is yet\nto be\nexplored,\nand we\nattempt\nto address\nthis"
        },
        {
          "Schizophrenia and Autism [6]. Due to the limited exploration": "to\naddress\ninterpretability\nin DNNs\nby\nextracting\nhuman-",
          "on unsupervised clustering that produces an Intrinsic (inter-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": ""
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "nomy, Voice Pitch and Utterance Polarity, which are discussed"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": ""
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "below:"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": ""
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "• Variations\nin Physiognomy (Deviation from Neutral"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "Expressions). Emotions\nsuch as\nanger\nand excitement"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "capture extreme changes in facial expressions compared"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "to\nthe\nneutral\nresting\nface. They\nare\ncharacterized\nby"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "changes\nin the\nfacial\nfeatures\nsuch as\neye\ncontact,\nlip"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "movement, etc. According to Grabowski et al.\n[6], anal-"
        },
        {
          "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations": "ysis of emotions in a valence/arousal spectrum allows for"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "audio and textual data. We ﬁrst deﬁne human-understandable",
          "2) Context-Dependent Feature Extraction: The contextual": "features\nare\nextracted\nusing\nthe Contextual-LSTM architec-"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "concepts speciﬁc for emotion recognition based on inferences",
          "2) Context-Dependent Feature Extraction: The contextual": "ture proposed by Poria\net al., which is\na part of\ntheir Bi-"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "and observations from [6]. The CAVs are then ﬁtted to the BC-",
          "2) Context-Dependent Feature Extraction: The contextual": "directional Contextual LSTM network (Fig. 1b). The intuition"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "LSTM model’s\nlatent\nspace at\nthe chosen layers\nto compute",
          "2) Context-Dependent Feature Extraction: The contextual": "behind\nthis\narchitecture\nis\nthat\nthe\nsurrounding\nutterances"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "the concept\nsensitivities and TCAV scores\nfor each concept",
          "2) Context-Dependent Feature Extraction: The contextual": "can provide essential\ninformation in the classiﬁcation of\nthe"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "and for each target emotion.",
          "2) Context-Dependent Feature Extraction: The contextual": "current utterance,\nthereby requiring a model\nthat\ntakes\nsuch"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "dependencies\nrepresent\nthe input\ninto consideration. Let Xk"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "III. PROPOSED METHODOLOGY",
          "2) Context-Dependent Feature Extraction: The contextual": "features\nrepresent\nthe output of\nthe\nfor utterance k and Lk"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "LSTM network\nfor\nutterance\nk. The\noutput\nfor\nthe\nnext"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "In this\nsection, we discuss\nthe\nfeature\nextraction method",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "utterance Lk+1 depends on Xk+1 as well as the output of the"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "used for multimodal\nemotion recognition and introduce our",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "previous LSTM network Lk, which represents the learning of"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "human-understandable\nconcepts\nfor\ninterpreting DNNs with",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "contextual information. This contextual-LSTM module is used"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "Concept Activation Vectors.",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "for unimodal and multimodal\nfeature extraction."
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "A. Feature Extraction",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "B.\nInterpretability using CAVs"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "In this work, we adopt\nthe feature extraction method de-",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "1) Testing with Concept Activation Vectors\n(TCAV):\nTo"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "et\nal.\nscribed\nby Poria\n[20].\nIt\nis\ncarried\nout\nin\n2\nstages:",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "achieve interpretability in terms of human-understandable con-"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "Context-Independent Extraction that extracts\nthe features\nfor",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "cepts, Kim et al.\n[13] proposed Concept Activation Vectors"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "each\ninput mode\n(audio,\nvideo\nand\ntext)\nseparately,\nand",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "(CAVs)\n-\na\nlinear\ninterpretability method\nthat\nrepresents\na"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "Context-dependent Extraction that\nlearns features across utter-",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "concept with a vector\nin a neural network’s activation space"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "ances for both unimodal and multimodal emotion recognition.",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "given\na\nset\nof\npositive\nand\nnegative\nexamples\nrepresenting"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "1) Context-Independent Feature Extraction\nfor Unimodal",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "the\nconcept. Given\na\npositive\nexamples\nset X +ve\nand\na"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "Input:\nFeature\nextraction\non\nthe\nunimodal\ninput\nis\ndone",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "negative examples set X −ve, a binary classiﬁer vl\nC is trained"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "independent of\nthe\nsurrounding utterances\nand without\nany",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "to distinguish between the activations of the positive examples"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "contextual\ninformation (or dependency). The steps involved in",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "set fl(x), x ∈ X +ve and the negative examples set fl(x), x ∈"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "feature extraction for each input mode are discussed in detail",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "X −ve, where fl(x) represents the neural activation at a layer"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "below:",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "l of a network. This binary classiﬁer vl\nC represents the CAV"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "• Text\nInput. The\ntext\ninputs\nused\nfor\ntextual\nfeature",
          "2) Context-Dependent Feature Extraction: The contextual": "for\nthe concept C at\nlayer\nl. The Testing with CAV (TCAV)"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "extraction are the transcripts\nfor each of\nthe utterances.",
          "2) Context-Dependent Feature Extraction: The contextual": "method introduces\na metric known as\nthe TCAV score\nthat"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "As stated by Poria et al., each of the utterances in a video",
          "2) Context-Dependent Feature Extraction: The contextual": "represents\na ML model’s\nsensitivity to a particular\nconcept"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "are\nrepresented\nas\na\ncombination\nof\n300\ndimensional",
          "2) Context-Dependent Feature Extraction: The contextual": "across all class labels. Given a concept C,\nthe TCAV score at"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "word2vec vectors [25] of each word in the utterances. The",
          "2) Context-Dependent Feature Extraction: The contextual": "l\na layer\nfor examples belonging to class k (Xk) is given as:"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "CNN used for feature extraction consists of 2 convolution",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "layers\nand\na\nsingle max-pool\nlayer. The\nresult\nof\nthe",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "|x ∈ Xk : SC,l,k(x) > 0|"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "(1)\nT CAVQC,k,l ="
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "pooling\noperation\nis\nprojected\nonto\na\ndt-dimensional",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "Xk"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "dense\nlayer whose\noutput\nserves\nas\nthe\ninput\ntextual",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "l\nlayer\nwhere SC,l,k represents the directional derivative at"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "features for emotion recognition.",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "• Video Input. The feature extraction for visual\ninput\nis",
          "2) Context-Dependent Feature Extraction: The contextual": "for concept C and class k given by SC,l,k = f (cid:48)\nC (f (cid:48)\nl (x)\nl (x).vl"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "is the derivative of\nthe activation at\nlayer\nl). TCAV provides"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "done\nusing\na\n3D-CNN, which\nis\ncapable\nof\nlearning",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "a quantitative measure of conceptual sensitivity across entire"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "features for each frame of\nthe video as well as temporal",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "input classes and can be extended to input modes other\nthan"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "features\nacross\nvideo\nframes. The\nvideo\ninput\nto\nthe",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "images."
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "3D-CNN has the dimensions (c, h, w, f ), where c is the",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "2) TCAV for Emotion Recognition: Here, we delineate the"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "h\nnumber\nof\nchannels\n(3\nfor RGB),\nand w are\nthe",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "concepts\nused\nto\ninterpret multimodal\nemotion\nrecognition"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "dimensions of each frame, and f is the total number of",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "models with TCAV. We deﬁne a single concept\nfor each of"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "frames. The 3D-CNN consists of a convolution layer with",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "in Physiog-\nvideo,\naudio and text\ninput modes\n- Variations"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "a 3D ﬁlter\nand a maxpool\nlayer,\nfollowed by a dense",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "nomy, Voice Pitch and Utterance Polarity, which are discussed"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "this 3D-CNN is a\nlayer of dimensions dv. The output of",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "",
          "2) Context-Dependent Feature Extraction: The contextual": "below:"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "that\nrepresents the utterance-level\ndv-dimensional vector",
          "2) Context-Dependent Feature Extraction: The contextual": ""
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "input visual\nfeatures.",
          "2) Context-Dependent Feature Extraction: The contextual": "• Variations\nin Physiognomy (Deviation from Neutral"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "• Audio Input. Audio feature extraction is done using the",
          "2) Context-Dependent Feature Extraction: The contextual": "Expressions). Emotions\nsuch as\nanger\nand excitement"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "openSMILE open-source software [26] that automatically",
          "2) Context-Dependent Feature Extraction: The contextual": "capture extreme changes in facial expressions compared"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "extracts\nessential\naudio\nfeatures. OpenSMILE extracts",
          "2) Context-Dependent Feature Extraction: The contextual": "to\nthe\nneutral\nresting\nface. They\nare\ncharacterized\nby"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "several\nlow-level features such as pitch,\nintensity, MFCC,",
          "2) Context-Dependent Feature Extraction: The contextual": "changes\nin the\nfacial\nfeatures\nsuch as\neye\ncontact,\nlip"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "etc. These features serve as the input audio features for",
          "2) Context-Dependent Feature Extraction: The contextual": "movement, etc. According to Grabowski et al.\n[6], anal-"
        },
        {
          "gap by extending Concept Activation Vectors\n[13]\nto video,": "the emotion recognition model.",
          "2) Context-Dependent Feature Extraction: The contextual": "ysis of emotions in a valence/arousal spectrum allows for"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b)": "(a) Overview of Methodology for TCAV on Emotion Recognition (b) Bottleneck layers chosen for TCAV on Emotion"
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        },
        {
          "(b)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: The concept examples are",
      "data": [
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "Input: Layer\nl, C = {C1, C2, . . . Cn}"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "Input: D = {(X C1, Y C1 ), (X C2, Y C2 ), . . . (X Cn, Y Cn )}"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "1:\nfor i = 1 to n do"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "2:\ni}\nDi = (fl(X Ci), Y Ci) {data for concept"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "DU"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "3:\ni = V ideoT oU tterance(Di)"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "4:\nvCi\n= BinaryClassif ier(DU\n) {CAV}"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "i\nl"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "5:\nfor k = 1 to K do"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "6:\nh = V ideoT oU tterance(X Ci\n)"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "k"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "7:\nSC,k,l = f (cid:48)\nl (h) × vCi"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "8:\nT CAVQC,k,l = T CAV Score(h, SC,k,l)"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "9:\nend for"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "10:\nend for"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "The\nsequence of\nsteps\ninvolved in computing conceptual"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "sensitivities\nfor emotion recognition are outlined in Algo. 1."
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "Given the set of concepts C and the concept annotations set D,"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "we wish to compute the CAV for each concept along with the"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "label k and layer l. Similar\nTCAV scores for each concept Ci,"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "to the original TCAV method,\nthe activations of\nthe concept"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "examples at layer l are extracted from the network and a binary"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "classiﬁer\nis ﬁtted to these\nactivations. However,\nthere\nis\nan"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "additional step while extending TCAV to emotion recognition,"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "which is the conversion of video-level activations to utterance-"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "level activations\n(Steps 3,6 in Algo. 1). The raw activations"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "have the dimensions (n, t, f ), where n is the number of videos,"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "t\nf\nis\nthe\nsequence\nlength\nand\nis\nthe\nnumber\nof\nfeatures."
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "These activations are reshaped to (n × t, f )\nso that\nthe ﬁrst"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "dimension represents the number of utterances. Therefore,\nthe"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "reshaped activations\nrepresent\nthe utterance-level\nactivations"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "of\nthe model at\nlayer\nl. A binary classiﬁer\nis\nthen ﬁtted to"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "to obtain the CAV vCi\n.\nthe concepts sets for each concept Ci"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "l"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "The conceptual\nsensitivities and TCAV scores are computed"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "as explained in Sec.\nIII.B.1."
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "IV. EXPERIMENTS AND RESULTS"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "In\nthis\nsection, we\ncover\nthe\nexperimental\nsetup\nused"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "for\ntraining the multimodal\nemotion recognition model\nand"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "interpreting the same using Concept Activation Vectors."
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "A. Model"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "To\ndetermine\nthe\ninﬂuence\nof\nour\nconcepts\nfor\nemotion"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "recognition, we make\nuse\nof\nthe Bi-directional Contextual"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "et\nal.\nLSTM network\n(Fig.\n1b)\nintroduced\nby\nPoria\n[20]"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "trained on the\nIEMOCAP multimodal database\n(Sec.\nIV.B)."
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "The motivation behind choosing this architecture is\nthe fact"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "that\nthis is one of the few simple and straightforward speaker-"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "independent multimodal architectures for emotion recognition,"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "which makes interpreting its decisions more convenient. The"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "current state-of-the-art methods [28]\n[29]\nfor emotion recog-"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "nition (in conversation) on IEMOCAP make use of\nspeaker-"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "speciﬁc components to enhance performance, which is outside"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": ""
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "the scope of our work. Contextual Hierarchical Fusion [19]"
        },
        {
          "Algorithm 1 TCAV for Emotion Recognition": "extends\nthe\nidea of\ncontextual\ninformation to 3 hierarchical"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(c)": "(d)"
        },
        {
          "(c)": "(a) Unimodal & Multimodal CAV accuracy for all 3 concepts.\n(b) TCAV scores for PT concept.\n(c) TCAV scores for"
        },
        {
          "(c)": "(d) TCAV scores for UP concept\n(Stars indicate insigniﬁcant concepts)"
        },
        {
          "(c)": "Concept\nNo. of Samples\nLabels of Samples\non a weighted average\nsentiment\nscore of\nthe words\nin the"
        },
        {
          "(c)": "+ve\n-ve\n+ve\n-ve\nutterance."
        },
        {
          "(c)": "To account\nfor variations\nin the binary classiﬁers’\ninitial-"
        },
        {
          "(c)": "Variations in"
        },
        {
          "(c)": "2200\n2200\n0,4,5\n2"
        },
        {
          "(c)": "ization and preprocessing of\nthe concept examples\n[15],\nthe"
        },
        {
          "(c)": "training\nof\nthe CAVs\nis\nrepeated\n30\ntimes\nresulting\nin\n30"
        },
        {
          "(c)": "Utterance"
        },
        {
          "(c)": "1361\n792\n0,2,4\n1,3,5"
        },
        {
          "(c)": "Polarity (UP)\ndifferent vectors. We\nevaluate\nthe\nstatistical\nsigniﬁcance of"
        },
        {
          "(c)": "our concepts by training 50 random CAVs for each layer and"
        },
        {
          "(c)": "Voice"
        },
        {
          "(c)": "620\n1706\n0 4 5\n2"
        },
        {
          "(c)": "assigning random labels. We then perform a 2-tailed t-test on\nPitch (PT)"
        },
        {
          "(c)": "the TCAV score distributions of\nthe random concepts and the"
        },
        {
          "(c)": "proposed concepts at a signiﬁcance level α = 0.05."
        },
        {
          "(c)": ""
        },
        {
          "(c)": "D. Results"
        },
        {
          "(c)": "Here, we discuss the quantitative evaluation of the CAVs for"
        },
        {
          "(c)": "our proposed concepts through the classiﬁer accuracies, TCAV"
        },
        {
          "(c)": "scores\nand\nhypothesis\ntests\nfor\nconcept\nsigniﬁcance. Since"
        },
        {
          "(c)": "there\nis\nno\nquantitative method\nto\ncompare\ninterpretability"
        },
        {
          "(c)": "threshold pitch value of 250Hz,\ni.e,\nthe preprocessed\nmethods\nand\ndue\nto\nthe\nlack\nof\nresults\nfor\nconcept-based"
        },
        {
          "(c)": "to the positive examples\nset\nfor PT if\nthe\ninterpretation of emotion recognition, we evaluate our concepts"
        },
        {
          "(c)": "the utterance exceeds 250Hz. To compute the text\nwithout any comparison to prior work."
        },
        {
          "(c)": "Fig. 2a shows\nthe test accuracies of\nthe classiﬁers\nfor\nthe"
        },
        {
          "(c)": "a\ntext polarity of\n-1 to 1 for\neach utterance based\nCAVs at the unimodal and multimodal levels of the BC-LSTM"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "network trained on the IEMOCAP multimodal database. The": "overall accuracies are relatively low due to the fact\nthat\nlinear",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "neutral\nlabels\nat\nthe\nunimodal\nbottleneck, which\nis\nas\nex-"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "classiﬁers\nare\nused\nto\ndeﬁne\nthe\nconcepts. The\ngraph\nalso",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "pected. Phrases\nthat\ndepict\nemotions\ninvolving medium to"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "shows\nthat\nthere\nis minimal variation in the\nclassiﬁers’\nac-",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "high arousal (intensity) tend to have a high level of sentiment"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "curacies,\nindicating that\nthe 30 different vectors are consistent",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "polarity compared to the neutral emotion. We observe that\nthe"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "for each of\nthe concepts.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "inﬂuence of UP on the neutral\nand sad\ntarget\nemotions\nis"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "1) Variations in Physiognomy (VP): The examples used to",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "relatively low, which is in line with common observations on"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "represent VP are collected based on the general assumption",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "emotion recognition using text. Emotions such as neutral and"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "that\nemotions\nshowing medium to high arousal\nsuch as\nex-",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "sadness are not as conveniently distinguishable as\nthe other"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "citement,\njoy, and anger\n[6] display a greater deviation from",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "target emotions. Given a phrase from these emotion classes,"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "the neutral resting face and can be distinctly identiﬁed through",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "the polarity is\napproximately 0, which makes\nit difﬁcult\nto"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "variations in visual features. Fig. 2b indicates the TCAV scores",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "differentiate utterances of\nthe neutral and sad classes. At\nthe"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "for VP at\nthe\nunimodal\nand multimodal\nbottleneck\nlayers.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "multimodal bottleneck layer,\nthe scores are negligible for\nthe"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "We\nsee\nthat\nthe highest\nscores\nare observed for\nthe happy",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "neutral and angry classes and signiﬁcant\nfor\nthe happy and"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "excited\nangry\nand\nclasses, while\nthe\nscores\nfor\nsad,\nand",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "anger classes. This\nsigniﬁes\nthat UP is\ninsigniﬁcant\ntowards"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "frustration\nclasses\nare\nrelatively\nlower, which\nconﬁrms\nthe",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "the classiﬁcation of examples into the sad, neutral, excited and"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "frustrated\nassumption stated above. However,\nthe scores\nfor",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "frustrated emotions\nlabels and that VP, PT concepts play a"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "and neutral classes are not\nin line with this assumption. At",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "more important role for these classes. Despite the high TCAV"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "the multimodal bottleneck,\nit\nis observed that VP has a strong",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "scores of UP for\nthe\nsad and frustrated classes, hypothesis"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "inﬂuence on happy and sad classes but much lower\ninﬂuence",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "tests shows insigniﬁcance of\nthe concept\nfor\nthese labels."
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "on the rest of the classes. The consistently strong inﬂuence of",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "4) Hypothesis Testing: To test\nthe signiﬁcance of\nthe pro-"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "VP on the happy class across\nthe unimodal and multimodal",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "posed concepts for emotion recognition, we perform a 2-tailed"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "bottlenecks\nis\nevidence\nfor\nthe\nfact\nthat VP is\nessential\nfor",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "t-test. We ﬁrst\ngenerate\n50\nrandom concept\nsets\nfrom the"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "recognizing happiness.\nIt\nis also observed that\nthe scores for",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "training set and assign positive and negative labels in a random"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "the sad, excited and frustration classes are exceptions for\nthe",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "fashion to the activations from the unimodal and multimodal"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "general assumption stated earlier. Since the concept set\nis not",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "bottleneck layers. This is followed by ﬁtting binary classiﬁers"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "created based on a quantitative measure for variations in facial",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "to these random concept\nsets\nto generate 50 random CAVs."
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "expressions,\nit\nis possible\nthat\nthe\ninconsistencies\nin TCAV",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "We then perform a hypothesis\ntest by conducting a 2 tailed"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "scores are due to the nature of\nthe concept\nset given that\nit",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "t-test for the distribution of the TCAV scores for the proposed"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "is only an approximation based on the general\nassumption.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "concepts and the 50 random concepts at a signiﬁcance level"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "Another\nfactor\nthat\ncould\ncontribute\nto\nthis\ndiscrepancy\nis",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "α = 0.05. The null and alternate hypotheses are deﬁned as"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "the relatively poor performance of\nthe BC-LSTM network on",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "follows:"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "video input\nfrom IEMOCAP as mentioned in Sec.\nIV.A.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "2) Voice Pitch (PT): Pitch of an individual’s voice can be",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "(2)\nNull Hypothesis Ho : µr = µt"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "used as a strong indicator of expression of speciﬁc emotions.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "(3)\nAlternate Hypothesis Ha : µr (cid:54)= µt"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "Fig.\n2b\nshows\nthe TCAV scores\nfor PT\nat\nthe\nunimodal",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "and multimodal bottlenecks.\nIt\nis observed that PT has\nthe",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "represents the mean score of the random concepts\nHere, µr"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "highest\ninﬂuence (0.865 and 0.726) on the anger and frustra-",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "represents the mean score of the proposed\ndistribution and µt"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "tion classes\nat\nthe unimodal bottleneck. This observation is",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "concepts\nscore\ndistributions. We\nconsider\na\nconcept\nto\nbe"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "consistent with [27],\nin that expression of such emotions are",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "signiﬁcant\nif\nthe null hypothesis\nis\nrejected for 40 of\nthe 50"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "associated with high pitch and PT can be used as a distin-",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "random TCAV score distributions. It\nis observed that (Fig. 2b,"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "excitement\nguishing trait. This, however,\nis not\ntrue\nfor\nthe",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "2c, 2d) the UP concept\nis signiﬁcant for all emotion classes at"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "class, which\nideally\nis\ncharacterized\nby\nhigh\npitch. At\nthe",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "the unimodal bottleneck and insigniﬁcant for the sad, neutral,"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "multimodal\nlevel,\nit\nis observed that\nthe happy, sad, neutral,",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "excited and frustrated classes. The VP concept\nis\nsigniﬁcant"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "and frustrated emotion classes have high scores for PT (0.843,",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "for all emotion classes at both the unimodal and multimodal"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "1, 0.988 and 0.875 respectively). This is a contradiction to the",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "bottlenecks. The PT\nconcept\nis\nsigniﬁcant\nfor\nall\nemotion"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "general presumption that only emotions with high arousal are",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "classes at\nthe unimodal\nlevel but\ninsigniﬁcant\nfor\nthe neutral"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "associated with high pitch and that pitch can be used as\na",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "and excited emotions at\nthe multimodal\nlevel."
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "distinguishing factor. This discrepancy could be due\nto the",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "V. CONCLUSIONS AND FUTURE WORK"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "nature\nof\nthe\nutterances\nfound\nin\nthe\nIEMOCAP database.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": ""
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "It\nis observed that\nsome of\nthe utterances\nfor emotions with",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "Emotion AI has been widely used in critical domains such"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "medium arousal (happy, sad, etc.) have higher pitch than some",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "as medical\ndiagnosis\nand\nsecurity,\nand\ninterpretability\nfor"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "of\nthe utterances for emotions with high arousal.",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "emotion recognition will ensure the robustness and reliability"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "3) Utterance\nPolarity\n(UP):\nFig.\n2d\nshows\nthe\nTCAV",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "of\naffective\ncomputing\nsystems.\nTo\nthis\nend, we\nexplore"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "scores\nfor UP at\nthe\nunimodal\nand multimodal\nbottleneck",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "concept-based interpretation of\nemotion recognition through"
        },
        {
          "network trained on the IEMOCAP multimodal database. The": "layers. From the\nscores,\nit\nis\nevident\nthat UP has\na\nhigh",
          "sad\ninﬂuence\non\nall\nthe\ntarget\nemotions\nexcept\nthe\nand": "Concept Activation Vectors (CAVs)\nto quantify the inﬂuence"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "networks,” in International Conference on Machine Learning, pp. 3319–"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "recognition model. We deﬁne novel concepts based on existing",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "3328, PMLR, 2017."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "Emotion AI\nliterature, and analyse the relevance of\nthe same.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[12] Y.-h. Liu\nand S. O. Arik,\n“Explaining\ndeep\nneural\nnetworks\nusing"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "Through\nour\nresults,\nit\nis\nevident\nthat DNNs\nfor\nemotion",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "unsupervised clustering,” arXiv preprint arXiv:2007.07477, 2020."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[13] B. Kim, M. Wattenberg,\nJ. Gilmer, C. Cai,\nJ. Wexler,\nF. Viegas,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "recognition make use of human-understandable concepts\nfor",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "et al., “Interpretability beyond feature attribution: Quantitative testing"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "classiﬁcation,\njust\nlike humans.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "with concept activation vectors\n(tcav),” in International conference on"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "We further evaluate the signiﬁcance of our concepts through",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "machine learning, pp. 2668–2677, PMLR, 2018."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[14]\nJ. Pfau, A. T. Young,\nJ. Wei, M. L. Wei,\nand M.\nJ. Keiser,\n“Robust"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "hypothesis testing on the TCAV scores. The results show that",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "semantic interpretability: Revisiting concept activation vectors,” arXiv"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "the multimodal\narchitecture makes use of\nspeciﬁc\nconcepts",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "preprint arXiv:2104.02768, 2021."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "for\nspeciﬁc emotion classes. There is no single concept\nthat",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[15] A. Lucieri, M. N. Bajwa, S. A. Braun, M.\nI. Malik, A. Dengel,\nand"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "S. Ahmed,\n“On\ninterpretability\nof\ndeep\nlearning\nbased\nskin\nlesion"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "is signiﬁcant\nfor all\nthe emotions, which is\nin line with pre-",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "classiﬁers using concept activation vectors,” in 2020 International Joint"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "established notions for\nthis task from the human perspective.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Conference on Neural Networks (IJCNN), pp. 1–10,\nIEEE, 2020."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "Current\nliterature on this task shows that most of\nthe models",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[16] A. Ghorbani,\nJ. Wexler,\nJ. Zou,\nand B. Kim,\n“Towards\nautomatic"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "concept-based explanations,” arXiv preprint arXiv:1902.03129, 2019."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "trained on the IEMOCAP database tend to perform better on",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[17]\nT. Mittal, U. Bhattacharya, R. Chandra, A. Bera,\nand D. Manocha,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "text\ninput\nthan the other\ntwo input modes, which could affect",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "“M3er: Multiplicative multimodal\nemotion\nrecognition\nusing\nfacial,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "the interpretation of these models. Thus, one of the focuses for",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "the AAAI Conference on\ntextual, and speech cues,” in Proceedings of"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Artiﬁcial\nIntelligence, vol. 34, pp. 1359–1367, 2020."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "future work can be the interpretation of emotion recognition",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[18] D. Krishna and A. Patil, “Multimodal emotion recognition using cross-"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "models that are independent of dataset bias.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "modal attention and 1d convolutional neural networks.,” in Interspeech,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "In\nthis work, we\nhave\nexplored\nthe\ninterpretability\nfor",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "pp. 4243–4247, 2020."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[19] N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria,\nand S. Poria,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "emotion\nrecognition\non\nthe BC-LSTM network, which\nis",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "“Multimodal\nsentiment analysis using hierarchical\nfusion with context"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "relatively simple compared to the state-of-the-art models. The",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "modeling,” Knowledge-based systems, vol. 161, pp. 124–133, 2018."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "TCAV method can be extended to more complex architectures",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[20]\nS. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh,\nand L.-"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "P. Morency,\n“Context-dependent\nsentiment\nanalysis\nin user-generated"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "to\nevaluate\nour\nconcepts\nfor\nthese models.\nIn\naddition\nto",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "the 55th annual meeting of\nthe association\nvideos,” in Proceedings of"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "these improvements, the discovery of emotion-related concepts",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "for\ncomputational\nlinguistics\n(volume 1: Long papers), pp. 873–883,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "in\nan\nunsupervised\nsetting\ncan\nbe\na\npossible\ndirection\nfor",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "2017."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[21] C. Qing, R. Qiao, X. Xu, and Y. Cheng, “Interpretable emotion recog-"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "future research. This can reduce human effort\nin annotating",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "nition using eeg signals,” Ieee Access, vol. 7, pp. 94160–94170, 2019."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "concept examples for emotion classiﬁcation and enhance the",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[22]\nP. Liu, K. Li,\nand H. Meng,\n“Group gated fusion on attention-based"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "interpretability of DNNs by allowing models to generate their",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "arXiv\nbidirectional\nalignment\nfor multimodal\nemotion\nrecognition,”"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "preprint arXiv:2201.06309, 2022."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "own concepts.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[23]\nJ. M. Mayor-Torres, M. Ravanelli, S. E. Medina-DeVilliers, M. D."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Lerner, and G. Riccardi, “Interpretable sincnet-based deep learning for"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "REFERENCES",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "2021\n43rd Annual\nemotion\nrecognition\nfrom eeg\nbrain\nactivity,”\nin"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "International Conference\nof\nthe\nIEEE\nEngineering\nin Medicine &"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[1] D. V. Carvalho, E. M. Pereira, and J. S. Cardoso, “Machine learning",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Biology Society (EMBC), pp. 412–415,\nIEEE, 2021."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "interpretability: A survey on methods and metrics,” Electronics, vol. 8,",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[24]\nT.-L. Nguyen, S. Kavuri,\nand M. Lee,\n“A multimodal\nconvolutional"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "no. 8, p. 832, 2019.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "neuro-fuzzy network for emotion understanding of movie clips,” Neural"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[2]\nF. K. Došilovi´c, M. Brˇci´c,\nand N. Hlupi´c,\n“Explainable\nartiﬁcial\nin-",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Networks, vol. 118, pp. 208–219, 2019."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "International convention on infor-\ntelligence: A survey,” in 2018 41st",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[25]\nT. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "mation and communication technology, electronics and microelectronics",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "word representations in vector space,” arXiv preprint arXiv:1301.3781,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "(MIPRO), pp. 0210–0215,\nIEEE, 2018.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "2013."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[3] C. Molnar,\nInterpretable Machine Learning. 2019.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[26]\nF.\nEyben, M. Wöllmer,\nand B.\nSchuller,\n“Opensmile:\nthe munich"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[4]\nS. Tripathi, S. Tripathi,\nand H. Beigi,\n“Multi-modal\nemotion\nrecog-",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "versatile and fast open-source audio feature extractor,” in Proceedings of"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "arXiv\npreprint\nnition\non\niemocap\ndataset\nusing\ndeep\nlearning,”",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "the 18th ACM international conference on Multimedia, pp. 1459–1462,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "arXiv:1804.05788, 2018.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "2010."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[5] M. Zhang, Y. Liang, and H. Ma, “Context-aware affective graph reason-",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[27]\nL. R. Quinto, W. F. Thompson, and F. L. Keating, “Emotional communi-"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "ing for emotion recognition,” in 2019 IEEE International Conference on",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "cation in speech and music: The role of melodic and rhythmic contrasts,”"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "Multimedia and Expo (ICME), pp. 151–156,\nIEEE, 2019.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Frontiers in psychology, vol. 4, p. 184, 2013."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[6] K. Grabowski, A. Rynkiewicz, A. Lassalle, S. Baron-Cohen, B. Schuller,",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[28]\nL. Yang, Y. Shen, Y. Mao, and L. Cai, “Hybrid curriculum learning for"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "N. Cummins, A. Baird,\nJ.\nPodgórska-Bednarz, A.\nPieni ˛a˙zek,\nand",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "emotion recognition in conversation,” arXiv preprint arXiv:2112.11718,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "I. Łucka, “Emotional expression in psychiatric conditions: New tech-",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "2021."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "nology for clinicians,” Psychiatry and clinical neurosciences, vol. 73,",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[29]\nT. Kim and P. Vossen, “Emoberta: Speaker-aware emotion recognition"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "no. 2, pp. 50–62, 2019.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "in conversation with roberta,” arXiv preprint arXiv:2108.12009, 2021."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[7] M. T. Ribeiro, S. Singh, and C. Guestrin, “\" why should i\ntrust you?\"",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[30] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "the 22nd\nexplaining the predictions of any classiﬁer,” in Proceedings of",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "ACM SIGKDD international\nconference on knowledge discovery and",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "data mining, pp. 1135–1144, 2016.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "vol. 42, no. 4, pp. 335–359, 2008."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[8]\nS. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting model",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "[31] B. Gfeller, C.\nFrank, D. Roblek, M.\nShariﬁ, M. Tagliasacchi,\nand"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "of\nthe\n31st\ninternational\nconference\non\npredictions,”\nin Proceedings",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "M. Velimirovi´c, “Spice: Self-supervised pitch estimation,” IEEE/ACM"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "neural\ninformation processing systems, pp. 4768–4777, 2017.",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "Transactions\non Audio,\nSpeech,\nand\nLanguage Processing,\nvol.\n28,"
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "[9] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh,\nand",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": "pp. 1118–1128, 2020."
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "D. Batra,\n“Grad-cam: Visual\nexplanations\nfrom deep\nnetworks\nvia",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        },
        {
          "of emotion-related concepts for a typical multimodal emotion": "the IEEE international\ngradient-based localization,” in Proceedings of",
          "[11] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Machine learning interpretability: A survey on methods and metrics",
      "authors": [
        "D Carvalho",
        "E Pereira",
        "J Cardoso"
      ],
      "year": "2019",
      "venue": "Electronics"
    },
    {
      "citation_id": "2",
      "title": "Explainable artificial intelligence: A survey",
      "authors": [
        "F Došilović",
        "M Brčić",
        "N Hlupić"
      ],
      "year": "2018",
      "venue": "2018 41st International convention on information and communication technology, electronics and microelectronics"
    },
    {
      "citation_id": "3",
      "title": "Interpretable Machine Learning",
      "authors": [
        "C Molnar"
      ],
      "year": "2019",
      "venue": "Interpretable Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "5",
      "title": "Context-aware affective graph reasoning for emotion recognition",
      "authors": [
        "M Zhang",
        "Y Liang",
        "H Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "6",
      "title": "Emotional expression in psychiatric conditions: New technology for clinicians",
      "authors": [
        "K Grabowski",
        "A Rynkiewicz",
        "A Lassalle",
        "S Baron-Cohen",
        "B Schuller",
        "N Cummins",
        "A Baird",
        "J Podgórska-Bednarz",
        "A Pieni",
        "I Łucka"
      ],
      "year": "2019",
      "venue": "Psychiatry and clinical neurosciences"
    },
    {
      "citation_id": "7",
      "title": "explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "8",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st international conference on neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "10",
      "title": "Smoothgrad: removing noise by adding noise",
      "authors": [
        "D Smilkov",
        "N Thorat",
        "B Kim",
        "F Viégas",
        "M Wattenberg"
      ],
      "year": "2017",
      "venue": "Smoothgrad: removing noise by adding noise",
      "arxiv": "arXiv:1706.03825"
    },
    {
      "citation_id": "11",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "Explaining deep neural networks using unsupervised clustering",
      "authors": [
        "Y.-H Liu",
        "S Arik"
      ],
      "year": "2020",
      "venue": "Explaining deep neural networks using unsupervised clustering",
      "arxiv": "arXiv:2007.07477"
    },
    {
      "citation_id": "13",
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "authors": [
        "B Kim",
        "M Wattenberg",
        "J Gilmer",
        "C Cai",
        "J Wexler",
        "F Viegas"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "14",
      "title": "Robust semantic interpretability: Revisiting concept activation vectors",
      "authors": [
        "J Pfau",
        "A Young",
        "J Wei",
        "M Wei",
        "M Keiser"
      ],
      "year": "2021",
      "venue": "Robust semantic interpretability: Revisiting concept activation vectors",
      "arxiv": "arXiv:2104.02768"
    },
    {
      "citation_id": "15",
      "title": "On interpretability of deep learning based skin lesion classifiers using concept activation vectors",
      "authors": [
        "A Lucieri",
        "M Bajwa",
        "S Braun",
        "M Malik",
        "A Dengel",
        "S Ahmed"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "16",
      "title": "Towards automatic concept-based explanations",
      "authors": [
        "A Ghorbani",
        "J Wexler",
        "J Zou",
        "B Kim"
      ],
      "year": "2019",
      "venue": "Towards automatic concept-based explanations",
      "arxiv": "arXiv:1902.03129"
    },
    {
      "citation_id": "17",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Multimodal emotion recognition using crossmodal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using crossmodal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "19",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "20",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "21",
      "title": "Interpretable emotion recognition using eeg signals",
      "authors": [
        "C Qing",
        "R Qiao",
        "X Xu",
        "Y Cheng"
      ],
      "year": "2019",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "22",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2022",
      "venue": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "arxiv": "arXiv:2201.06309"
    },
    {
      "citation_id": "23",
      "title": "Interpretable sincnet-based deep learning for emotion recognition from eeg brain activity",
      "authors": [
        "J Mayor-Torres",
        "M Ravanelli",
        "S Medina-Devilliers",
        "M Lerner",
        "G Riccardi"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "24",
      "title": "A multimodal convolutional neuro-fuzzy network for emotion understanding of movie clips",
      "authors": [
        "T.-L Nguyen",
        "S Kavuri",
        "M Lee"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "25",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "26",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Emotional communication in speech and music: The role of melodic and rhythmic contrasts",
      "authors": [
        "L Quinto",
        "W Thompson",
        "F Keating"
      ],
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "28",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2021",
      "venue": "Hybrid curriculum learning for emotion recognition in conversation",
      "arxiv": "arXiv:2112.11718"
    },
    {
      "citation_id": "29",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "30",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "31",
      "title": "Spice: Self-supervised pitch estimation",
      "authors": [
        "B Gfeller",
        "C Frank",
        "D Roblek",
        "M Sharifi",
        "M Tagliasacchi",
        "M Velimirović"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}