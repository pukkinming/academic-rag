{
  "paper_id": "2106.02556v2",
  "title": "Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal Of Emotions Through Machine Learning",
  "published": "2021-06-04T15:40:19Z",
  "authors": [
    "Nicholas Farris",
    "Brian Model",
    "Richard Savery",
    "Gil Weinberg"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The task of classifying emotions within a musical track has received widespread attention within the Music Information Retrieval (MIR) community. Music emotion recognition has traditionally relied on the use of acoustic features, verbal features, and metadata-based filtering. The role of musical prosody remains under-explored despite several studies demonstrating a strong connection between prosody and emotion. In this study, we restrict the input of traditional machine learning algorithms to the features of musical prosody. Furthermore, our proposed approach builds upon the prior by classifying emotions under an expanded emotional taxonomy, using the Geneva Wheel of Emotion. We utilize a methodology for individual data collection from vocalists, and personal ground truth labeling by the artist themselves. We found that traditional machine learning algorithms when limited to the features of musical prosody (1) achieve high accuracies for a single singer, (2) maintain high accuracy when the dataset is expanded to multiple singers, and (3) achieve high accuracies when trained on a reduced subset of the total features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The work presented in this paper is situated in the intersection between research on emotion for robotics  [1]  and emotional classification research in Music Information Retrieval  [2] . In particular, we focus on the under-explored domain of emotion-driven prosody for human-robot interaction  [3] . Verbal prosody is concerned with elements of speech that are not individual phonetic segments but rather pertain to linguistic functions such as intonation, tone, stress, and rhythm. Similarly, musical prosody is defined as the performer's manipulation of music for certain expressive and coordinating functions  [4] . It has been hypothesized that these expressive functions serve to communicate emotion  [5] .\n\nIn this paper, we explore the relationship between musical prosody and emotion through three research questions. First, are traditional machine learning algorithms able to accurately classify an individual's emotions when trained on only the features of musical prosody? Next, are these models able to generalize to a larger group of vocalists? Finally, which features of musical prosody contribute the most to the classification of emotion?\n\nThe paper is structured as follows, in Section 2, background and motivation are discussed. Section 3 describes the dataset collection, training and testing, the taxonomies used in classification, the feature extraction methodology and analysis of their relevance to emotion, feature aggregation, feature selection, and model generalization. Section 4 presents the experiments: Experiment 1 asks how well can traditional machine learning models classify emotion when limited to inputs of musical prosody, Experiment 2 explores our approach's ability to generalize to a larger population of singers, and Experiment 3 explores the individual contribution to accuracy of each feature via training on reduced subsets of the input vector. Section 5 provides discussion to these results, with particular attention paid to the relationships between emotions and potential future work. Finally, section 6 concludes the paper. A demo via python notebook with audio samples is available online.  1",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Emotion classification has been a major focus of research in recent years. Ekman created a discrete categorization that consists of fundamental basic emotions which are the root for more complex emotions  [6] . Another classification model is the Circumplex model proposed by Posner et al which plots emotions on a continuous, two-dimensional scale of valence and arousal  [7] . In this paper, we classify emotions using a model similar to the two-dimensional Circumplex model which is further described in section 3.1.\n\nThere has also been much work done in the field of analyzing emotion from text for tasks such as sentiment analysis. Research on classification of emotion in audio has taken many different approaches. Research into classifying emotions in knocking sounds has found that anger, happiness and sadness could be easily classified from audio alone  [8] . There have been multimodal approaches which use audio in combination with another feature, namely visual facial features  [9]  [10] or text lyrics  [11] . Furthermore, researchers have performed emotional classification from audio in the context of music by analyzing which musical features best convey emotions  [12] . Panda et al. have found a relationship between melodic and dynamic features to a number of specific emotions  [13] . Such features that were used to classify emotion in music, however, cannot be easily generalized to other domains. Prosody has been found by linguists to communicate emotion across various cultures, with patterns of pitch and loudness over time representing different emotions  [14] , and has shown the potential to improve human-robot interaction  [15] [16] [17] . Our approach aims to bridge this gap by analyzing these prosodic features which are fundamental to everyday speech and explore how they can be used to classify emotional driven prosody.\n\nKoo et al. have done work in speech emotion recognition using a combination of MFCC and prosodic features with a GRU model on the IEMOCAP dataset  [18] . We expand upon their work by performing an in-depth analysis of 11 different audio features and their effect on classifying emotion. We also classify emotion beyond spoken language by analyzing prosodic features which better generalize to how humans convey emotion using the new dataset collected, as described in section 3.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Taxonomy",
      "text": "One of the main challenges in emotional classification is the derivation of a taxonomy that accurately reflects the problem domain. The two common approaches to address this challenge are 1. Discrete emotional categorization; and 2. Continuous quantitative metrics of Valence and Arousal (sometimes called Control). We use both approaches with a categorical, as opposed to regression, approach to the latter.\n\nOur models classify emotion under two taxonomies: first we categorize each data point as belonging to one of the twenty emotions located around the Geneva Wheel of Emotion. Then we categorize each data point as belonging to one of the quadrants depicted by the intersection of valance and control by assigning each emotion from the Geneva Wheel of Emotion to its respective quadrant. We abbreviate each of these quadrants as follows: \"High Control Negative Valance\": \"HCN\", \"High Control Positive Valance\": \"HCP\", \"Low Control Negative Valance\": \"LCN\", and \"Low Control Positive Valance\": \"LCP\". See Table  1  and Figure  1",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "Due to a lack of data labeled with the appropriate taxonomy, we decided to collect and annotate a new dataset. To Figure  1 . The Geneva Wheel of Emotion achieve this goal, we asked professional singers to consciously sing each emotion. To generate our dataset, three professional singers were tasked to improvise as many phrases as possible for each emotion in the Geneva Wheel of Emotion. The singers were instructed to sing each phrase between 1 and 20 seconds, and to spend approximately 15 minutes on each emotion, resulting in 4 to 6 hours of recordings per singer annotated with ground-truth labels.\n\nAdditionally, the singers were given the following instructions during their recording session:\n\n1. Do not attempt to control for different intensities for each emotion 2. Sing anything for each phrase that you believe matches the emotion except use words.\n\n3. After recording, mark any phrase that you believe did not capture the intended emotion and it will be deleted",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "In the following section, we define the features selected for extraction from our dataset prior to model training. Furthermore, we discuss each feature's relevance to emotional classification through an analysis of prior works.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Zero Crossing Rate",
      "text": "Zero Crossing Rate, the rate of sign-changes across a signal, is key in classifying percussive sounds. Unvoiced regions of audio are known to have higher Zero Crossing Rates  [19] . One study analyzed ZCR for Anger, Fear, Neutral, and Happy signals and noted that higher peaks were found for Happy and Anger emotions  [20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Energy",
      "text": "Energy, the area under the squared magnitude of the considered signal, relates to the amount of spectral information in a signal  [21]  and previous studies have found energy is essential in distinguishing stressed and neutral speech  [22] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Entropy Of Energy",
      "text": "Entropy of Energy, the average level of \"information\" or \"uncertainty\" inherent within a signal's energy, has been shown in one study to have similar values for disgust and boredom  [23] . To accurately measure the entropy of the different emotions, we must make sure we are not including parts of the signal where the individual is not speaking.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectral Centroid",
      "text": "Spectral Centroid, the power spectrum's center of mass, perceptually has a connection with a sound's brightness. It follows, that this parameter serves as an indicator of musical timbre  [24] . Previous studies have shown spectral centroid is a significant component in music emotion  [25] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spectral Spread",
      "text": "Spectral Spread, the second central moment of the power spectrum, has shown to help the listener to differentiate noise-like and tone-like portions of a signal  [26] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectral Entropy",
      "text": "Spectral Entropy, the entropy of the power spectrum, when used with MFCC features has shown an improvement in speech recognition accuracy  [27] . Another study found spectral entropy to have the highest correlation to emotional valence of all features tested  [28] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectral Flux",
      "text": "Spectral Flux, a measure of the rate of change of the power spectrum calculated as the Euclidean distance between sequential frames, relates to how fast the pitch changes in time and has been shown to be dominant in cross-domain emotion recognition from speech and sound and from sound and music  [29] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectral Rolloff",
      "text": "Spectral Rolloff, the frequency under which some percentage of the total energy of the spectrum is contained, helps differentiate between harmonic content, characterized below the roll-off, and noisy sounds, characterized above the roll-off. Spectral rolloff has been shown to be one of the most important prosodic features in classifying emotion  [28] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mfccs",
      "text": "Mel-Frequency Cepstral Coefficients (MFCCs), a representation of the short-term power spectrum based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency, are used in speech recognition with their ability to represent the speech amplitude spectrum in a compact form  [30] . Many studies have linked the importance of MFCC analysis to emotion recognition  [20]  [31]  [32]  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Chroma Vector And Deviation",
      "text": "Chroma Vector, an approximation of the pitch class profiles present within a given frame and often used as the twelve tones, allows for the capture of harmonic and melodic characteristics while remaining robust toward   [33, 34] .   2  delineates the feature aggregation hyper parameters used in this study.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Aggregation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Short-Term Aggregation",
      "text": "The short-term aggregation of a 5-second clip, using a Short-term Window Step of .05 seconds and a Short-term Window Size of .05 seconds is defined as follows: Each of the 34 features discussed above are extracted for every 50ms, resulting in 100 feature vectors of size 34x1, represented as a 34x100 matrix. Next, the deltas between each time step are calculated according to the equation ùëëùëíùëôùë°ùëé = ùëì ùëíùëéùë°ùë¢ùëüùëí_ùë£ùëíùëêùë°ùëúùëü -ùëì ùëíùëéùë°ùë¢ùëüùëí_ùë£ùëíùëêùë°ùëúùëü_ùëùùëüùëíùë£. The first time stamp has all deltas set to 0. Each delta vector is concatenated onto its respective feature vector resulting in a size of 68x1, represented as a 68x100 matrix for the entire 5 second audio clip.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mid-Term Aggregation",
      "text": "Next, mid-term aggregation occurs with a Mid-term Window Size of 1.0 seconds and Mid-term Window Step of 1.0 seconds. The 68x100 matrix of Short-term features is split according to the ratio between the Mid-term and Short-term window size and step, resulting in 5 matrices of size 68x20. For each matrix, we calculate and flatten the mean and standard deviation for each row, resulting in 5 136x1 mid-term feature vectors, represented as a 136x5 matrix. Finally, we take the mean across the first axis resulting in a 136x1 feature vector representing our 5 second audio clip.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Classification",
      "text": "Prior work focused on musical classification has primarily found success in the implementation of k-nearest neighbor (K-NN) and support vector machines (SVM), finding the highest accuracies using SVMs  [35] . In exploration of the relationship between musical prosody and emotion, we will implement a variety of machine learning models, namely we will train and evaluate KNNs, Linear SVMs, Random Forests, Extra Trees, Gradient Boosting, and Feed Forward Neural Networks (FFNN). FFNNs are used in experiment 3 only.\n\nExperiment 1: we explore the base line accuracies, Fscores, and confusion matrices achieved by training each model with identical training, validation, and testing data from a single singer.\n\nExperiment 2: we explore our model architecture's ability to generalize by expanding the dataset to include all 3 singers from data collection.\n\nExperiment 3: we explore model performance on a reduced subset of the training feature, utilizing additive feature selection to compile a ranking of features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment 1",
      "text": "In experiment 1, we analyze the baseline accuracies, Fscores, and confusion matrices achieve by training KNNs, linear SVMs, Random Forests, Extra Trees, Gradient Boosting models on a single singer utilizing only the prosodic features outlined in the previous section. All models were trained with features extracted according to the parameters outlined in Table  2 . Additionally, each model is optimized with respect to its associated hyper parameter. We optimize KNN for the number nearest neighbors, SVM for the soft margin, random forest for number of trees, gradient boosting for the number of boosting stages, and extra trees for the number of trees. 2  Table  3  provides the best accuracy, F1-score, and selected hyper-parameter for each of our models trained on a Big 4 taxonomy for a single singer. All models perform better than twice the accuracy of random guessing, with the linear SVM and Gradient Boosting models achieving the highest accuracies. Further analysis of the confusion matrix of the Gradient Boosting model, shown in Figure  4 , provides information about the classes that are most often confused for one another. The model struggles in distinguishing between Low Control Positive Valance and High Control Positive Valance. This is to say the model can tell that an individual is in a positive mood, but has difficulties distinguishing the Control or Arousal of the emotion.\n\nNext, we examine classification under a single emotion taxonomy for a single singer. Table  4  shows the best accuracy, F1-score, and selected hyper-parameter for each of our models. Each model significantly outperforms random guessing. Even the worst model, the KNN, performs 6.5 times better than random chance (20 possible categories = 5% chance random guessing). Our best model, the linear Finally, our models perform extremely well when tasked with categorizing between two emotions, achieving accuracies as high as 98.9% with a f1 of 98.9 in the distinction between Love and Disgust using a SVM. This reinforces the intuition that by reducing the number of emotional categories we can achieve higher accuracies for identification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment 2",
      "text": "Within machine learning, model generalization poses many challenges as models tend to memorize data and perform worse when exposed to new datasets. In experiment 2, we generalized our model by training on 3 different singers as opposed to training on one singer. Tables  5  and 6  compare the accuracies achieved by the various model architectures for 3 singers vs 1 singer.\n\nWith the exception of linear SVM, all model architectures maintain similar accuracies when trained on the 3 singer datasets. This maintenance of accuracy demonstrates the ability for traditional machine learning models to generalize well to a larger population when trained on only the features of musical prosody. We are unsure of why linear SVMs perform worse during generalization as compared to other models, seeing a drop of 6% in Big 4 taxonomy and a drop of 13% in single emotion taxonomy. This drop could potentially be a limitation in our methodology of only applying a linear kernel to SVM training, as perhaps an RBF or polynomial kernel would be better able to generalize to a larger population.\n\nThe results of this experiment are encouraging to the development of a general model of emotional classification based on musical prosody as accuracy is maintained when  Table  5 . Big 4 Taxonomy, 1 Singer vs 3 Singer Accuracy the dataset is expanded to a larger portion of the overall population.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment 3",
      "text": "Experiment 3 analyzes model performance on a reduced subset of the feature vector for our single emotion taxonomy. Our implementation of Feature Selection follows an additive approach. We start with an empty permanent feature set and each feature is trained on its own. The feature with the highest f1 score is selected and added to our permanent feature set. This process is repeated until all fea-  Our feed forward neural net contained the input layer, two dense layers of 136 nodes with relu activation functions, and a dense 20 node output layer. We trained using a Sparse Categorical Cross entropy loss function optimized using an Adam optimizer with 5 epochs per model.\n\nFigure  7  shows the F1 score achieved vs the Feature included in the model pipeline. All feature on and to the right of any point in the x axis are included in training. An F1 of 45 is achieved within the first 25 features. Furthermore, the addition of the remaining 111 features only increases our F1 score to 52. This graph emphasizes the importance of spectral roll-off and MFCC 7 in the classification of emotion, as aggregations of these two features allow for an F1 score just below 20 with 4 total features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis",
      "text": "We demonstrate that prosodic features can be used to classify human emotions, achieving high accuracies on classifying emotions for a single singer dataset as seen in tables 3 and 4. Furthermore, we obtained encouraging results regarding the model's generalization between singers as demonstrated by tables 5 and 6. However, given our limited dataset, more research is needed to study how the models generalize for additional singers with different voices.\n\nOur feature selection aligns with prior research indicating that energy and MFCC were the most useful features for classifying emotion  [9] . However, we have been able to show that the results holds true not just for phonological speech, but in the more specific domain of musical prosody.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Relationships Between Emotions",
      "text": "The classification results give us new insights into the uniqueness and relationships between emotions. Looking at the individual classification data between all the singers in Figure  3 , we can see how the model was best able to classify fear, joy and relief. This is in contrast to emotions such pleasure or admiration which showed the lowest classification accuracy. These results demonstrate the manner in which different humans convey emotions, and what emotions are similarly expressed by different individuals. When conveying relief, all three singers expressed a diminuendo and exhale. Similarly, when conveying fear all three singers expressed a crescendo and more accented tones. On the other hand, there was a high level of variation when conveying pleasure, with many different tone ranges, mouth shapes, etc. being present in the data.\n\nFurthermore, from the confusion matrix in Figure  5 , we can see that the emotion pairs of Hate and Disgust as well as Pleasure and Contentment are the most common emo-tions to be misclassified as one another. We suggest that this is due to these emotions representing similar meanings, thus they would be conveyed using similar features. For instance, Hate and Disgust both tend to consist of lower tones while Pleasure and Contentment have higher tones.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Future Work",
      "text": "One of the major challenges we faced was the limited amount of data that was collected. We plan on expanding this dataset to a larger variety of singers and other instrumentalists so that we can better understand how the models can generalize to different sounds. Additional future work includes developing a more sophisticated deeplearning based model on the raw audio data for classifying emotion using the expanded dataset we will collect. This will allow the model to make predictions beyond what could be possible using the features we chose in our feature selection. It would open up the potential to achieve much higher accuracy and better model generalization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "Our novel dataset using an expanded emotion taxonomy provides opportunity for the development of a more articulate understanding of emotions. Previous attempts to correlate emotion to audio or music are based on fewer emotions, and often rely on lyrics or song metadata for classification. Our algorithms demonstrate a high level of accuracy on a 20 category taxonomy for emotions, utilizing only prosodic features. By restricting the type of input data to prosodic features and expanding the number of classified emotions, our models can be used for a wide range of research challenges within the domain of emotional classification. Furthermore, we have demonstrated that our approach is able to generalize to a larger subset of the overall population. Finally, the restriction of our feature vector via additive feature selection demonstrates the ability for prosodic features to achieve a high-level accuracy for emotional classification for a relatively small number of features.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: for a visualization of the domain‚Äôs",
      "page": 2
    },
    {
      "caption": "Figure 1: The Geneva Wheel of Emotion",
      "page": 2
    },
    {
      "caption": "Figure 2: Model of Feature Aggregation",
      "page": 3
    },
    {
      "caption": "Figure 3: Analysis of this confusion matrix yields a few",
      "page": 4
    },
    {
      "caption": "Figure 3: SVM, Individual Taxonomy, 1 Singers Confusion Matrix",
      "page": 5
    },
    {
      "caption": "Figure 4: Gradient Boosting, Big 4 Taxonomy, 1 Singer",
      "page": 5
    },
    {
      "caption": "Figure 7: shows the F1 score achieved vs the Feature in-",
      "page": 5
    },
    {
      "caption": "Figure 5: Gradient Boosting, Individual Taxonomy, 3 Singers Confusion Matrix",
      "page": 6
    },
    {
      "caption": "Figure 6: Gradient Boosting, Big 4 Taxonomy, 3 Singer",
      "page": 6
    },
    {
      "caption": "Figure 3: , we can see how the model was best able to",
      "page": 6
    },
    {
      "caption": "Figure 7: F1 score vs Features included in model pipeline",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and Figure 1 for a visualization of the domain‚Äôs classificationthroughananalysisofpriorworks.",
      "data": [
        {
          "HCN": "Anger\nContempt\nDisgust\nHate\nRegret",
          "HCP": "Amusement\nInterest\nJoy\nPleasure\nPride",
          "LCN": "Disappointment\nFear\nGuilt\nSadness\nShame",
          "LCP": "Admiration\nCompassion\nContentment\nLove\nRelief"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameter": "Mid-term Window Step\nMid-term Window Size\nShort-term Window Step\nShort-term Window Size",
          "Value": "1.0 seconds\n1.0 seconds\n0.05 seconds\n0.05 seconds"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Additionally, each theintuitionthatbyreducingthenumberofemotionalcat-",
      "data": [
        {
          "Model": "KNN\nSVM\nExtra Trees\nGradient Boosting\nRandom Forest",
          "Accuracy": "56.1\n66.5\n64.6\n67.0\n63.5",
          "F1": "56.2\n65.3\n64.3\n66.7\n63.2",
          "Hyperparam": "C=11\nC=1.0\nC=100\nC=500\nC=200"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Additionally, each theintuitionthatbyreducingthenumberofemotionalcat-",
      "data": [
        {
          "Model": "KNN\nSVM\nExtra Trees\nGradient Boosting\nRandom Forest",
          "Accuracy": "33.8\n49.1\n44.3\n47.2\n43.8",
          "F1": "32.1\n48.1\n42.8\n46.6\n42.3",
          "Hyperparam": "C=15\nC=5.0\nC=500\nC=200\nC=200"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "KNN\nSVM\nExtra Trees\nGradient Boosting\nRandom Forest",
          "1-S Accuracy": "56.1\n66.5\n64.6\n67.0\n63.5",
          "3-S Accuracy": "57.9\n60.6\n63.5\n68.8\n65.1"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "KNN\nSVM\nExtra Trees\nGradient Boosting\nRandom Forest",
          "1-S Accuracy": "33.8\n49.1\n44.3\n47.2\n43.8",
          "3-S Accuracy": "32.5\n36.9\n42.7\n43.8\n43.8"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A survey of robotics and emotion: Classifications and models of emotional interaction",
      "authors": [
        "R Savery",
        "G Weinberg"
      ],
      "year": "2020",
      "venue": "A survey of robotics and emotion: Classifications and models of emotional interaction"
    },
    {
      "citation_id": "3",
      "title": "Contentbased music audio recommendation",
      "authors": [
        "P Cano",
        "M Koppenberger",
        "N Wack"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th annual ACM international conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Establishing human-robot trust through music-driven robotic emotion prosody and gesture",
      "authors": [
        "R Savery",
        "R Rose",
        "G Weinberg"
      ],
      "year": "2019",
      "venue": "2019 28th IEEE International Conference on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "5",
      "title": "What is musical prosody?",
      "authors": [
        "C Palmer",
        "S Hutchins"
      ],
      "year": "2006",
      "venue": "Psychology of Learning and Motivation"
    },
    {
      "citation_id": "6",
      "title": "Music and emotion: Theory and research",
      "authors": [
        "P Juslin",
        "J Sloboda"
      ],
      "year": "2001",
      "venue": "Music and emotion: Theory and research"
    },
    {
      "citation_id": "7",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "8",
      "title": "The circumplex model of affect: an integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "9",
      "title": "Perception of emotions in knocking sounds : an evaluation study",
      "authors": [
        "M Houel",
        "A Arun",
        "A Berg",
        "A Iop",
        "A Barahona-Rios",
        "S Pauletto"
      ],
      "year": "2020",
      "venue": "Perception of emotions in knocking sounds : an evaluation study"
    },
    {
      "citation_id": "10",
      "title": "Audio-visual feature selection and reduction for emotion classification",
      "authors": [
        "P Jackson",
        "J Edge"
      ],
      "year": "2008",
      "venue": "The proceedings of international conference on auditory-visual speech processing"
    },
    {
      "citation_id": "11",
      "title": "Bimodal emotion recognition",
      "authors": [
        "L Silva",
        "Pei Ng"
      ],
      "year": "2000",
      "venue": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion analysis of songs based on lyrical and audio features",
      "authors": [
        "A Jamdar",
        "J Abraham",
        "K Khanna",
        "R Dubey"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "13",
      "title": "Evaluation of musical features for emotion classification",
      "authors": [
        "Y Song",
        "S Dixon",
        "M Pearce"
      ],
      "year": "2012",
      "venue": "Evaluation of musical features for emotion classification"
    },
    {
      "citation_id": "14",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Communicating emotion: The role of prosodic features",
      "authors": [
        "R Frick"
      ],
      "year": "1985",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "16",
      "title": "Finding shimi's voice: fostering human-robot communication with music and a nvidia jetson tx2",
      "authors": [
        "R Savery",
        "R Rose",
        "G Weinberg"
      ],
      "year": "2019",
      "venue": "Proceedings of the 17th Linux Audio Conference"
    },
    {
      "citation_id": "17",
      "title": "Emotional musical prosody for the enhancement of trust in robotic arm communication",
      "authors": [
        "R Savery",
        "L Zahray",
        "G Weinberg"
      ],
      "year": "2020",
      "venue": "Trust, Acceptance and Social Cues in Human-Robot Interaction: 29th IEEE International Conference on Robot & Human Interactive Communication"
    },
    {
      "citation_id": "18",
      "title": "Enriching robot communication surrounding collaborative creative activities",
      "authors": [
        "Before",
        "After"
      ],
      "year": "2021",
      "venue": "Frontiers in Robotics and AI",
      "doi": "10.3389/frobt.2021.662355"
    },
    {
      "citation_id": "19",
      "title": "Development of speech emotion recognition algorithm using mfcc and prosody",
      "authors": [
        "H Koo",
        "S Jeong",
        "S Yoon",
        "W Kim"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Electronics, Information, and Communication (ICEIC)"
    },
    {
      "citation_id": "20",
      "title": "Divergence detection in a speech-excited in-service non-intrusive measurement device",
      "authors": [
        "Wai Pang Ng",
        "J Elmirghani",
        "R Cryan",
        "Yoong Choon Chang",
        "S Broom"
      ],
      "year": "2000",
      "venue": "2000 IEEE International Conference on Communications. ICC 2000. Global Convergence Through Communications"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition from speech signal",
      "authors": [
        "E Ramdinmawii",
        "A Mohanta"
      ],
      "year": "2017",
      "venue": "TENCON 2017 -2017 IEEE Region 10 Conference"
    },
    {
      "citation_id": "22",
      "title": "A large set of audio features for sound description (similarity and classification) in the cuidado project",
      "authors": [
        "G Peeters"
      ],
      "year": "2004",
      "venue": "CUIDADO Ist Project Report"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition using dwt",
      "authors": [
        "S Lalitha",
        "A Mudupu",
        "B Nandyala",
        "R Munagala"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)"
    },
    {
      "citation_id": "25",
      "title": "Difference thresholds for timbre related to spectral centroid",
      "authors": [
        "R Kendall",
        "E Carterette"
      ],
      "year": "1996",
      "venue": "Proceedings of the 4th International Conference on Music Perception and Cognition"
    },
    {
      "citation_id": "26",
      "title": "Musical timbre and emotion: The identification of salient timbral features in sustained musical instrument tones equalized in attack time and spectral centroid",
      "authors": [
        "B Wu",
        "A Horner",
        "C Lee"
      ],
      "year": "2014",
      "venue": "Musical timbre and emotion: The identification of salient timbral features in sustained musical instrument tones equalized in attack time and spectral centroid"
    },
    {
      "citation_id": "27",
      "title": "Cubic svm classifier based feature extraction and emotion detection from speech signals",
      "authors": [
        "U Jain",
        "K Nathani",
        "N Ruban",
        "A Raj",
        "Z Zhuang",
        "V Mahesh"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Sensor Networks and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Spectral entropy as speech features for speech recognition",
      "authors": [
        "A Toh",
        "R Togneri",
        "S Nordholm"
      ],
      "year": "2005",
      "venue": "Proceedings of PEECS"
    },
    {
      "citation_id": "29",
      "title": "High-level analysis of audio features for identifying emotional valence in human singing",
      "authors": [
        "S Cunningham",
        "J Weinel",
        "R Picking"
      ],
      "year": "2018",
      "venue": "Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion, ser. AM'18",
      "doi": "10.1145/3243274.3243313"
    },
    {
      "citation_id": "30",
      "title": "On the acoustics of emotion in audio: what speech, music, and sound have in common",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "31",
      "title": "Mel frequency cepstral coefficients for music modeling",
      "authors": [
        "B Logan"
      ],
      "year": "2000",
      "venue": "Ismir"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition in speech using mfcc and wavelet features",
      "authors": [
        "K Krishna Kishore",
        "Krishna Satish"
      ],
      "year": "2013",
      "venue": "2013 3rd IEEE International Advance Computing Conference (IACC)"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition in spontaneous speech using gmms",
      "authors": [
        "D Neiberg",
        "K Elenius",
        "K Laskowski"
      ],
      "year": "2006",
      "venue": "Ninth international conference on spoken language processing"
    },
    {
      "citation_id": "34",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        "J Speck",
        "D Turnbull"
      ],
      "year": "2010",
      "venue": "Music emotion recognition: A state of the art review"
    },
    {
      "citation_id": "35",
      "title": "Learning emotionbased acoustic features with deep belief networks,\" in 2011 IEEE workshop on applications of signal processing to audio and acoustics (Waspaa)",
      "authors": [
        "E Schmidt",
        "Y Kim"
      ],
      "year": "2011",
      "venue": "Learning emotionbased acoustic features with deep belief networks,\" in 2011 IEEE workshop on applications of signal processing to audio and acoustics (Waspaa)"
    },
    {
      "citation_id": "36",
      "title": "Music mood and theme classificationa hybrid approach",
      "authors": [
        "K Bischoff",
        "S Claudiu",
        "R Paiu",
        "W Nejdl",
        "C Laurier",
        "M Sordo"
      ],
      "year": "2009",
      "venue": "Music mood and theme classificationa hybrid approach"
    }
  ]
}