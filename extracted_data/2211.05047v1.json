{
  "paper_id": "2211.05047v1",
  "title": "A Comparative Study Of Data Augmentation Techniques For Deep Learning Based Emotion Recognition",
  "published": "2022-11-09T17:27:03Z",
  "authors": [
    "Ravi Shankar",
    "Abdouh Harouna Kenfack",
    "Arjun Somayazulu",
    "Archana Venkataraman"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automated emotion recognition in speech is a long-standing problem. While early work on emotion recognition relied on hand-crafted features and simple classifiers, the field has now embraced end-to-end feature learning and classification using deep neural networks. In parallel to these models, researchers have proposed several data augmentation techniques to increase the size and variability of existing labeled datasets. Despite many seminal contributions in the field, we still have a poor understanding of the interplay between the network architecture and the choice of data augmentation. Moreover, only a handful of studies demonstrate the generalizability of a particular model across multiple datasets, which is a prerequisite for robust realworld performance. In this paper, we conduct a comprehensive evaluation of popular deep learning approaches for emotion recognition. To eliminate bias, we fix the model architectures and optimization hyperparameters using the VESUS dataset and then use repeated 5-fold cross validation to evaluate the performance on the IEMOCAP and CREMA-D datasets. Our results demonstrate that long-range dependencies in the speech signal are critical for emotion recognition and that speed/rate augmentation offers the most robust performance gain across models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human speech is a rich and varied medium that encapsulates not only semantic content, but also the speaker's mood and intent. While it is often easy for humans to decode the latter information, the same cannot be said for computers, particularly when it comes to parsing emotional cues from speech. At the same time, automated emotion recognition has the potential to greatly improve human-interactions  [1] . For example, it can be used in call centers for on-the-fly sentiment analysis to provide better customer support  [2] . Likewise, smart car systems and forensic analysis of speech rely heavily on being able to understand the emotional state of the person  [3, 4]  under study. There are also medical applications for automated emotion recognition, such as diagnosing depression and anxiety from the speaking style, and automated monitoring of patients with bipolar disorder  [5] .\n\nDue to these and other applications, the area of emotion recognition has seen a steady growth in the past few years. Deep neural networks, in particular, have allowed for a steady increase in performance on challenging acted and improvised emotional speech datasets  [6, 7, 8, 9, 10] . In the simplest case, hand-crafted features are fed into a deep network classifier to predict one of several emotional states. One such example is the work of  [9] , in which the authors use a multi-layer perceptron to identify five different emotional states. This hand-crafted approach has largely been superceded by models that learn task-specific feature representations across time. Convolutional neural neural network (CNN) models operate on time-frequency inputs (e.g., Mel-frequency cepstral coefficients, chromagram) and use either 1-D or 2-D convolutions to learn the relevant filters for emotion classification  [11, 12, 13, 14] . A 3-D convolutional model using the first and second order derivative of MFCCs has also been proposed in the literature by  [15] . Recurrent neural networks (RNNs) for emotion recognition  [16, 17, 18]  are also popular due to their ability to process temporal sequences. In fact, an attention based Bi-LSTM model proposed by  [19]  simultaneously learns not only the emotional label but which segments of an utterance are responsible for the corresponding prediction. Finally, the transformer architecture is often used for multi-modal emotion recognition, where the inputs are audio and text  [20]  or audio and video  [21] . Transformers and RNNs have also been combined with other deep network modules, such as CNNs, for improved emotion recognition.\n\nThe flip-side to the popularity of deep learning models is the need for larger and more varied training data. To mitigate this need, researchers have proposed data augmentation strategies that can expand the training dataset by few orders of magnitude. The most common data augmentation techniques for emotion recognition are to add random noise  [22] , to vary the sampling rate of the input speech signal  [23] , to manipulate the spectral content of the utterances  [24] , and a mixing between these approaches  [25] . Beyond signal manipulations, a recently proposed augmentation strategy known as copy-paste  [26]  randomly combines neutral and emotional utterances. This strategy allows the models to extract time-limited emotional cues that may not be present for the entire utterance duration. While all of these augmentation methods have led to a significant increase in emotion recognition accuracy, they are tested on a specific model and experimental dataset. Therefore, we have little understanding about which models and augmentation strategies provide robust and generalizable performance gains.\n\nIn this paper, we conduct a comprehensive evaluation of canonical deep network architectures and data augmentation strategies. Our architectures consist of (1) a convolutional model, (2) a fully-connected model similar to MLP, (3) a recurrent model, and (4) a transformer. For each model, we evaluate five augmentation strategies: original/no augmentation, adding noise at different SNRs, re-sampling the timescales of the training utterances, masking time/frequency blocks in the spectrogram, and concatenating neutral and emotional utterances to mitigate class imbalance. To avoid over-fitting, we fix the network architectures and hyperparameters based on the VESUS dataset  [9] . We then perform a repeated k-fold cross validation of each (model, augmentation) pair on the more challenging IEMOCAP  [6]  and CREMA-D  [7]  datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "In order to provide a comprehensive evaluation of the interplay between network architecture and data augmentation, we select one model from each of the following classes: (1) convolutional network, (2) fully-connected network, (3) recurrent network, and (4) transformer. We also query four distinct data augmentation strategies used in speech analysis tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Network Architectures",
      "text": "This section details the selected architectures. These networks are adapted from existing work in language modeling, speech recognition, and multimodal audio/video processing. The input to these models are the MFCCs feature vectors computed over a window of 5ms with the same temporal shift. These representations are commonly used for emotion recognition  [27] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gated Convolutional Network",
      "text": "The gated convolutional neural network (Gated-CNN) was proposed by  [28]  for language modeling using short contexts. The Gated-CNN extends a typical CNN by constructing two parallel convolution operations for each input, with the output of one branch passed through a sigmoid activation, which acts as a gating. The two branches are combined element-wise. Mathematically, let S ∈ R T ×F be the input signal, and ws and wg denote the convolutional kernels. The Gated-CNN output is:\n\nwhere is the element-wise Hadamard product, and LN denotes layer normalization. At a high level, the sigmoid gating serves as an information regulator for the downstream task.\n\nAs seen in Fig.  1 (a), we feed the output of the CNN cascade into one fully-connected layer, followed by a softmax operation to classify the correct emotion category.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mlp-Mixer",
      "text": "The multilayer perceptron mixer (MLP-mixer) is an innovative network architecture for processing 2D inputs. It was first proposed in the context of image processing  [29] , but here we adapt it to time-frequency representations of speech.\n\nOur MLP-mixer consists of a stack of sequential time and frequency mixing layers interleaved with layer normalization to address the vanishing gradient and covariant shift issue (Fig.  1(f) ). This operation facilitates information sharing across time and frequency bands in a computationally efficient manner.\n\nMathematically, let S R T ×F be the input time-frequency representation, where T is the number of frames in the speech signal, and F is the resolution of the MFCCs. The output at the first layer of MLP-mixer can be written as follows:\n\nfor f = 1, . . . , F and t = 1, . . . , T . Once again, LN denotes the layer normalization operation, and the variables {Wi} are the weight matrices. Subsequent layers of the MLP-mixer follow likewise. As seen in Fig.  1 (b), we use global average pooling and a linear layer for the final classification task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bi-Lstm With Attention",
      "text": "Our recurrent module consists of the Bi-LSTM with attention model proposed by  [19] . The input is a sequence of frequencybased feature vectors, such as the frame-wise spectrogram or MFCCs. In parallel to processing the inputs, an attention value for each time point is obtained by computing a dot-product with a learnable vector. At a high level, the attention captures how important a particular time point is for the downstream task. A final feed-forward layer combines the data representation and attention for the emotion classification (Fig.  1(d) ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transformer Network",
      "text": "Transformers have been also proposed for language and emotion modeling over longer contexts  [20, 30] . The central idea of this architecture is to exploit short and long-term dependencies via a dot product operation on the input sequence. This is combined with multiple self-attention operations, which act as feature extractors for the downstream emotion classification task. Mathematically, the self-attention can be written as follows:\n\nWe use the linear transformations of the input sequence to construct the query Q, the key K, and the value V in Eq. (  4 ).\n\nWe use sinusoidal position embedding to identify the relative positioning of a single frame in the input sequence. Further, each attention layer has a residual connection that combines the input directly with the output of the attention mechanism to facilitate gradient flow during backpropagation (Fig.  1   1 : Hyperparameters of the network architectures optimized using the VESUS dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation Strategies",
      "text": "As seen in Fig.  2 , we evaluate four data augmentation strategies that have been recently proposed for speech analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Additive Noise:",
      "text": "We add white Gaussian noise to the original audio signal at a specified signal-to-noise ratio (SNR). In this work, we consider three SNR levels of 10dB, 20dB and 30dB.\n\n2) Spectral Augmentation (SpecAug): This data augmentation strategy was proposed by  [24]  and has led to significant improvement in speech recognition. The algorithm involves masking a random interval of the spectrogram along the frequency axis. This masking is followed by a random control point registration of the time-frequency representation (see Fig.  2(b) ). In this work, we also run SpecAug for time/frequency masking and for time-frequency masking to compare their relative effects.\n\n3) Speed Perturbation: Introduced for speech recognition  [23] , this augmentation strategy changes the rhythm of the signal will preserving the original pitch. In this work, we augment the original input data with samples that have been slowed by 10% (i.e., speed perturbation of 0.9) and samples that have been sped up by 10% (i.e., speed perturbation of 1.1). We also evaluate the effect of mixing both perturbation types.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "4)",
      "text": "Copy-Paste: This algorithm was proposed by  [26]  to increase the diversity of training examples in the dataset for emotion recognition. The key idea in Copy-Paste technique is to concatenate neutral and emotional speech but maintain the emotion label for the augmented utterance (Fig.  2(d) ). This technique capitalizes on the fact that emotions are expressed in a segmental manner, meaning that only portions of the utterance carry emotion-specific attributes. In this work, we use Copy-Paste to balance the training examples across all classes.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "The input to each model consists of the standard 23-dimensional Mel filterbank features extracted from the speech utterance using a window of size 5ms and the same shift. We fix the hyperparameters for each architecture (e.g., depth, nodes, #filters, #attention heads, etc) using the VESUS dataset  [9]  to eliminate bias when evaluated on the testing datasets, IEMOCAP  [6]  and CREMA-D  [7] . Table  1  summarizes the hyperparameters optimized for each model. In addition, to account for the effects of random initialization, we train/test each model multiple times to produce a confidence interval for the emotion recognition. We train each model on a single A32 GPU in PyTorch  [31] .\n\nThe Gated-CNN, MLP mixer and Bi-LSTM are optimized using Adam optimizer with a fixed learning rate of 1e-3. The Transformer model starts from the same learning rate but has a step-wise learning rate decay schedule of 0.95 per step.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Speech Datasets",
      "text": "As noted above, we select the architectures and hyperparameters of each model via five-fold cross validation on the VESUS dataset  [9] . VESUS contains a total of 10 actors (5 males and 5 females) reading a script in multiple emotion categories. We create the five folds by pairing one male and one female actor. VESUS also contains crowd-sourced emotional ratings for each utterance. For robustness, we use only utterances that have been correctly labeled as emotional by at least half of the raters. Our evaluation datasets in this work are IEMOCAP  [6]  and CREMA-D  [7] . IEMOCAP contains emotional speech collected by from conversations between a male and a female actor. The dataset contains 10 actors in total, grouped into 5 sessions. Unlike VESUS, it also contains both scripted and improvised utterances. To remain consistent with the literature, we select four primary emotions (neutral, angry, sad, and happy) for our classification task. We conduct 5 fold session-independent experiments by training each model on 4 sessions and testing on the held-out session, cyclically. CREMA-D is a multimodal corpus with audio-video recording by 91 speakers. We select audio files corresponding to the four primary emotions used in the IEMOCAP experiment and randomly partition the speakers in five groups to perform a five-fold cross validation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition Performance",
      "text": "Table  2  summarizes the performance of each model and augmentation strategy on the IEMOCAP dataset. For convenience, we have bolded the best performance and underlined a close second place (e.g., within a standard deviation). As seen, the best model achieves a maximum accuracy score of 0.6 using the Gated-CNN architecture with the copy-paste augmentation strategy. The corresponding F1 score is high indicating a roughly balanced result across emotion classes. At a high level, convolutions facilitate the propagation of local information across multiple layers. This works well with the copy-paste algorithm, as only portions of the combined utterances are emotional. The Bi-LSTM and MLP-mixer models struggle to capture these local characteristics, in part due to the dyadic setup of IEMOCAP, where emotions are sporadic in nature. The transformer comes close to the best performance without any data augmentation, which suggests that the self-attention helps to improve the sensitivity for certain classes. Focusing on the aug-   mentation strategies, we note that additive noise and SpecAug are poor augmentation schemes for IEMOCAP, possibly due to the varied semantic and emotional nature of the recording sessions. On the other hand, speed perturbation provide a robust increase in performance across all model classes. Table  3  reports the performance of each model and augmentation strategy on the CREMA-D dataset. First, we note that the overall performance is significantly higher than IEMO-CAP. This is because CREMA-D is an acted emotional dataset collected in a noise free environment, whereas IEMOCAP contains \"messy\" dyadic interactions. Once again, we note that the Gated-CNN model performs the best, followed by the transformer. Across model architectures, we see that the MLP mixer has the worst performance, as it does not employ any type of temporal continuity or smoothness constraint in its formulation. With regards to the augmentation, the speed perturbation is the clear winner across model classes. Once again, noise augmentation does not improve the metrics. Masking of time/frequency bands is similar to no augmentation meaning that most deep neural networks can fill in the information gap, i.e, interpolate in the masked region of training examples. Taken together with the IEMOCAP results, we conclude that having sufficient variability in the speech rhythm is crucial for emotion recognition.\n\nFinally, Fig.  3  and Fig.  4  illustrate the confusion matrices for the best performing augmentation strategy by each model on IEMOCAP and CREMA-D. On IEMOCAP, happy is the worst performing class, likely due to the smaller number of samples. Angry and sad are often confused with neutral, perhaps due to variations in the utterance lengths and emotional expressions across actors. CREMA-D has a balanced number of samples across emotion categories, which leads to higher sensitivity. This can be seen in the diagonally-dominant confusion matrices across model classes. Once again, happy is the most challenging class, which suggests that future studies should focus on data augmentation schemes targeted at this class.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "This paper presents a comprehensive study of the interplay between model architectures and data augmentation in the context of speech emotion recognition. We evaluate a convolutional, fully-connected, recurrent and transformer architecture on two distinct datasets. We optimize the model hyper-parameters on VESUS and then evaluated them on IEMOCAP and CREMA-D for an unbiased comparison. We conclude that speed perturbation is a robust augmentation strategy across network architectures and datasets. The copy-paste augmentation is also useful for convolutional models. Ultimately, this study can be used as a helpful guide for future researcher when designing and implementing neural architectures for emotional speech recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Neural network architectures used for emotion classiﬁcation: (a) Gated CNN, (b) MLP mixer, (c) Bi-LSTM with attention,",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), we feed the output of the CNN cascade",
      "page": 2
    },
    {
      "caption": "Figure 1: (f)). This operation facilitates information sharing across",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), we use global average pool-",
      "page": 2
    },
    {
      "caption": "Figure 2: Augmentation schemes used for training emotion clas-",
      "page": 3
    },
    {
      "caption": "Figure 2: , we evaluate four data augmentation strategies",
      "page": 3
    },
    {
      "caption": "Figure 3: IEMOCAP Confusion matrices. Clockwise from top",
      "page": 4
    },
    {
      "caption": "Figure 3: and Fig. 4 illustrate the confusion matrices",
      "page": 4
    },
    {
      "caption": "Figure 4: CREMA-D Confusion matrices. Clockwise from top",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: summarizes the performance of each model and aug-",
      "data": [
        {
          "Model": "Gated-CNN",
          "#Layers": "",
          "#Nodes(ﬁnal)": "",
          "#Kernels": "",
          "#Attention Heads": "X",
          "#LSTM cells": "X",
          "Patch size": "X"
        },
        {
          "Model": "MLP-mixer",
          "#Layers": "",
          "#Nodes(ﬁnal)": "",
          "#Kernels": "X",
          "#Attention Heads": "X",
          "#LSTM cells": "X",
          "Patch size": ""
        },
        {
          "Model": "Bi-LSTM",
          "#Layers": "",
          "#Nodes(ﬁnal)": "",
          "#Kernels": "X",
          "#Attention Heads": "X",
          "#LSTM cells": "",
          "Patch size": "X"
        },
        {
          "Model": "Transformer",
          "#Layers": "",
          "#Nodes(ﬁnal)": "",
          "#Kernels": "X",
          "#Attention Heads": "",
          "#LSTM cells": "X",
          "Patch size": "X"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: summarizes the performance of each model and aug-",
      "data": [
        {
          "(a)": "(c)",
          "(b)": "(d)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "3",
      "title": "Two-stream emotion recognition for call center monitoring",
      "authors": [
        "P Gupta",
        "N Rajput"
      ],
      "year": "2007",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Emotion-awareness for intelligent vehicle assistants: A research agenda",
      "authors": [
        "H.-J Vögel",
        "C Süß",
        "T Hubregtsen",
        "E André",
        "B Schuller",
        "J Härri",
        "J Conradt",
        "A Adi",
        "A Zadorojniy",
        "J Terken",
        "J Beskow",
        "A Morrison",
        "K Eng",
        "F Eyben",
        "S Al Moubayed",
        "S Müller",
        "N Cummins",
        "V Ghaderi",
        "R Chadowitz",
        "R Troncy",
        "B Huet",
        "M Önen",
        "A Ksentini"
      ],
      "year": "2018",
      "venue": "2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS)"
    },
    {
      "citation_id": "5",
      "title": "A forensic phonetic study of the vocal responses of individuals in distress",
      "authors": [
        "L Roberts"
      ],
      "year": "2012",
      "venue": "A forensic phonetic study of the vocal responses of individuals in distress"
    },
    {
      "citation_id": "6",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah",
        "N Rahim",
        "N Ullah",
        "J Ahmad",
        "K Muhammad",
        "M Lee",
        "S Kwon",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "10",
      "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
      "authors": [
        "J Sager",
        "R Shankar",
        "J Reinhold",
        "A Venkataraman"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Machine Audition: Principles, Algorithms and Systems",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2010",
      "venue": "ch. Multimodal Emotion Recognition"
    },
    {
      "citation_id": "12",
      "title": "Continuous speech emotion recognition with convolutional neural networks",
      "authors": [
        "N Vryzas",
        "L Vrysis",
        "M Matsiola",
        "R Kotsakis",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2020",
      "venue": "Journal of the Audio Engineering Society. Audio Engineering Society"
    },
    {
      "citation_id": "13",
      "title": "Recognition of emotional speech with convolutional neural networks by means of spectral estimates",
      "authors": [
        "N Weißkirchen",
        "R Bock",
        "A Wendemuth"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "14",
      "title": "Mlt-dnet: Speech emotion recognition using 1d dilated cnn based on multi-learning trick approach",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "17",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Mustaqeem",
        "S Sajjad",
        "Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using recurrent neural networks with directional self-attention",
      "authors": [
        "D Li",
        "J Liu",
        "Z Yang",
        "L Sun",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "19",
      "title": "Longshort term memory for emotional recognition with variable length speech",
      "authors": [
        "Y Xie",
        "F Zhu",
        "J Wang",
        "R Liang",
        "L Zhao",
        "G Tang"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "20",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Automatic speech emotion recognition using recurrent neural networks with local attention"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "22",
      "title": "Transformer based multimodal speech emotion recognition with improved neural networks",
      "authors": [
        "R Patamia",
        "W Jin",
        "K Acheampong",
        "K Sarpong",
        "E Tenagyei"
      ],
      "year": "2021",
      "venue": "2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML)"
    },
    {
      "citation_id": "23",
      "title": "Best practices for noise-based augmentation to improve the performance of emotion recognition \"in the wild",
      "authors": [
        "E Provost"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "24",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "26",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cissé",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Platform Technology and Service"
    },
    {
      "citation_id": "29",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Y Dauphin",
        "A Fan",
        "M Auli",
        "D Grangier"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "Mlp-mixer: An all-mlp architecture for vision",
      "authors": [
        "I Tolstikhin",
        "N Houlsby",
        "A Kolesnikov",
        "L Beyer",
        "X Zhai",
        "T Unterthiner",
        "J Yung",
        "A Steiner",
        "D Keysers",
        "J Uszkoreit",
        "M Lucic",
        "A Dosovitskiy"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "31",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: closing the valence gap"
    },
    {
      "citation_id": "32",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "E Lerer"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}