{
  "paper_id": "2309.11911v6",
  "title": "Instructerc: Reforming Emotion Recognition In Conversation With Multi-Task Retrieval-Augmented Large Language Models",
  "published": "2023-09-21T09:22:07Z",
  "authors": [
    "Shanglin Lei",
    "Guanting Dong",
    "Xiaoping Wang",
    "Keheng Wang",
    "Runqi Qiao",
    "Sirui Wang"
  ],
  "keywords": [
    "Retrieval Module Task 1",
    "Task 2",
    "…",
    "Task n"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The field of emotion recognition of conversation (ERC) has been focusing on separating sentence feature encoding and context modeling, lacking exploration in generative paradigms based on unified designs. In this study, we propose a novel approach, InstructERC, to reformulate the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs). InstructERC makes three significant contributions: (1) it introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information. (2) We introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. (3) Pioneeringly, we unify emotion labels across benchmarks through the feeling wheel to fit real application scenarios. Instruc-tERC still perform impressively on this unified dataset. Our LLM-based plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provides empirical guidance for applying it in practical scenarios. You can find the offical realization in the Github link: https: //github.com/LIN-SHANG/InstructERC",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Equal Contribution †",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Corresponding Author",
      "text": "In contrast to conventional binary sentiment analysis tasks  (Pontiki et al., 2016)  , which only rely on text with explicit attitude tendencies, the emotion recognition in conversation (ERC) task aims to identify more fine-grained emotional tendencies in each sentence of a conversation. Specifically, for a given complete dialogue sequence input and a set of emotional labels, the model is required to accurately assign an emotional label to each sentence. Intuitively, the recognition of emotional tendencies in the target sentence is heavily influenced by its historical utterances  (Yingjian et al., 2023) , and there is significant variation in how different speakers perceive and express emotions  (Shen et al., 2021) . Therefore, it is imperative to meticulously model the speakers and dialogue context.\n\nFigure  1  illustrates that previous work based on Roberta  (Liu et al., 2019)  in ERC can be roughly divided into three categories: (1) Transformerbased methods  (Li et al., 2020; Song et al., 2022; Liu et al., 2023; Chudasama et al., 2022)  attempt to establish long-range emotional correlations in conversational scenarios by directly adopting or modifying the original transformer block. (2) Recurrent-based methods  (Hu et al., 2023; Lei et al., 2023; Majumder et al., 2019; Hazarika et al., 2018; Poria et al., 2017)  utilize various forms of RNNs, like LSTM and GRU, to model individual emotional states and global emotional impacts separately. (3) GNN-based methods  (Ghosal et al., 2019; Ishiwatari et al., 2020; Shen et al., 2021; Li et al., 2023a)  typically use nodes and edges to model characters and dialogue relationships in conversations. Above approaches have their strengths in modeling dialogue at the sentence level, but they still generally adhere to the paradigm of fine-tuning sentence features and separately modeling dialogue context. However, in realistic scenarios, end-to-end model designs are often more practical.  1  . Fortunately, the recent successful application  (OpenAI, 2023; Yang et al., 2024)  and emergence capabilities  (Zhao et al., 2023)  of pre-trained large language models (LLMs) have demonstrated remarkable performance in natural language reasoning tasks. By using a generative architecture, LLMs unify the output and input of different tasks and have shown significant performance improvements in all NLP tasks. Despite their powerful capabilities, enabling these abilities for specific sub-tasks requires high-quality prompts  (Wei et al., 2021; Chung et al., 2022)  and designs to fill the reasoning gap. Therefore, how to use LLMs framework to reconstruct ERC while considering context modeling, speaker modeling, and capturing conversation relationships poses a significant challenge in pushing this framework towards a realistic ERC application.\n\nIn this work, we reformulate the ERC task using LLMs. Specifically, we design a simple but efficient retrieval template module, which consists of instruction, historical utterance, label statements, and demonstration retrieval to explicitly integrate multi-granularity dialogue supervision information during reasoning. In addition, we separately design two auxiliary tasks for the ERC task: speaker identification task and emotion prediction task. The speaker identification task assists LLMs in modeling dialogue role relationships by predicting the speaker of each sentence, while the emotion prediction task models future emotional tendencies in conversations. Furthermore, due to biases in data distribution and labeling across different ERC domains, it's still challenging for discriminative ERC models to achieve multi-domain ERC capabilities, both in terms of engineering and performance. To dive deeper into this topic, we pioneeringly align labels for three benchmarks and conduct a series of unified dataset experiments. Looking ahead, we contend that IERC, as the first framework transitioning from single-domain to multi-domain ERC, offers us a glimpse into the prospective landscape of open-domain emotional artificial intelligence (Emotional AGI).\n\nIn conclusion, our work can be outlined as follows:\n\n• To the best of our knowledge, we are the first to reformulate the ERC task as a retrieval based Seq2Seq paradigm with LLMs and present an effective instruction template which can adapt to different dialog scenarios.\n\n• We propose two novel emotional auxiliary tasks to implicitly model the dialogue role relationships and future emotional tendencies in conversations.\n\n• Our InstructERC significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets.\n\n• To advance towards multi-domain ERC scenario, we pioneeringly align labels for three benchmark to form the UIME ERC dataset, a series of unified dataset experimental results provides empirical guidance for application in practical scenarios.\n\nIn this section, we present a comprehensive overview of the proposed InstructERC framework shown as Figure  3 . Firstly, we provide a brief introduction to the task definition of ERC. Next, we discuss the framework of InstructERC, which consists of two major parts: retrieval template module and emotional alignment tasks. Finally, we introduce training and inference process of our framework. 2\n\n2.1 Problem Definition\n\nof length n is given, which includes M speakers/parties p 1 , p 2 , ..., p M (M ≥ 2) in the dialogue, and each utterance u i spoken by the corresponding speaker p K(u i ) . Function K is employed to establish a mapping between each utterance and its corresponding speaker. o is the number of emotional categories, which varies with the number of emotional types in different evaluation datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Retrieval Template Module",
      "text": "To better transfer and utilize the inference ability of pre-trained large language models, we reconstruct the ERC task to the seq2seq form and solve it through fine-tuning LLMs. Therefore, we construct a efficient retrieval template module to bridge the gap when applying LLMs to specific NLP subtasks. As shown in Figure  2 , for ERC task, each input consists of four parts: instructions, historical content, label statement, and demonstration retrieval.\n\nInstruction. The instructions serve to provide the model with a well-defined role, precise details of the ERC task, and a standardized format for the input dialogue text. For the primary ERC task, our instruction u i,I is shown in Figure  2 .\n\nHistorical Content. To model the context in realistic ERC scenarios, We employ a hyperparameter, the historical window (denoted as w), to indicate the specific rounds (including current utterance) of historical dialogue along with the corresponding speaker information. For the emotion recognition of the target utterance u n , its historical content u i,H is shown in Figure  2 .\n\nLabel Statement. To confine the model's output within a finite range of labels and enable the model\n\nNow you are expert of sentiment and emotional analysis. The following is a conversation which involves several speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Historical Content ! !,$",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Label Statement ! !,%",
      "text": "Please select the emotional label of < Speaker_0:\"Oh my god, it was just last weekend\" > from < happy, sad, neutral, angry, excited, frustrated>.\n\nHere is a conversation : Speaker_0:\"Guess what?\" Speaker_1:\"what?\" Speaker_0:\"I did it, I asked her to marry me.\" … Speaker_0:\"Yes, I did it.\" Speaker_1:\"When?\" Speaker_0:\"Oh my god, it was just last weekend\". Please select the emotional label of < Speaker_0:\"Oh my god, it was just last weekend\" > from < happy, sad, neutral, angry, excited, frustrated>.\n\nHere is a conversation : to focus on the current utterance being recognized, our label statement u i,L is shown in Figure  2 .\n\nDemonstration Retrieval. In order to further integrate emotional information to assist reasoning, we have developed a domain demonstration recall module based on semantic similarity. In detail, we construct a domain base D domain from the training dataset that removes speaker identity information and balances the number of emotion labels, which ensures that the demonstrations is not influenced by the distribution of speakers or emotion labels in the dataset. For a given utterance u i to be identified, we retrieve the most relevant ERC example from D domain as the demonstration. To perform the retrieval, we use a bidirectional encoder SBERT  (Reimers and Gurevych, 2019)  to find the most semantically similar ERC example d rvl . SBERT generates independent CLS embeddings for the target utterance u i and each element d j in D domain . After sorting all target-demonstration pairs by cosine similarity, we select the pair with the highest score as the most relevant element d rvl . An abstract mathematical description of this process is as follows:\n\nThe textual input u i,D for the demonstration retrieval part is shown in Figure  2 . In summary, after constructing the Retrieval template, the simplified input x i for the main task is as follows:\n\nwhere [;] means the textual concatenation, u i,I , u i,H , u i,L , and u i,D indicate Instructions, Historical content, Label statement, demonstration retrieval for a given utterance u i .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotional Alignment Tasks",
      "text": "To better capture the dialogue role relationships and future emotional tendencies in conversations, we have incorporated two auxiliary tasks, namely speaker identification and emotion impact prediction, which constitute the fine-grained subtasks of the InstructERC framework. The model is jointly trained with these auxiliary tasks to improve its overall performance, illustrated in Figure  3 . Speaker Identification. Emotions are expressed differently among different speakers. Previous models have used techniques such as speaker-based masked attention modules or multiple GRUs to capture the emotional expression features of different characters. This modeling of emotional expression in the task can also be transformed into a generative task using our InstructERC. To enable the LLM to capture the speaking styles of different individuals, beyond  (Li et al., 2020) , the model is trained to identify the relevant speaker for a given utterance, without considering the historical context. For a given dataset, a predefined set of speaker labels is provided. Consistent with the main task, the Instruction text input x p i for this task is constructed as follows:\n\n\"Now you are an expert of sentiment and emotional analysis. Please select the Speaker label of the utterance <Speaker:u i > from <p 1 ,...,p M >\"\n\nThe loss function for the Speaker Identification is as follows:\n\nHere, µ i represents the token of the corresponding speaker label for the given speaker identification task input sample x p i . Unless otherwise specified, N stands for the total number of utterances in the dataset, while θ * represents the parameters of the LLM in different periods.\n\nEmotion Impact Prediction. In the daily conversations, the intricate relationships between individuals can have a significant impact on the emotional states of subsequent dialog. Prior research has attempted to address this issue by constructing a dialogue relationship graph and utilizing a complex graph neural network to model the emotional impacts of these relationships. However, these methods are often associated with a highly intricate data preprocessing pipeline and are susceptible to overfitting on certain datasets. To address these issues, we propose a generative framework for the emotion impact prediction task, which implicitly captures the interplay between dialogues and emotional impacts.\n\nSpecifically, the input for emotion impact prediction consists of three parts: instruction, historical content, and label statement. First, the instruction part of this task is kept consistent with the instruction part of the main task. Then, since the task requires predicting the impact of previous historical utterances on the current utterance, unlike the main task, the historical content u e i,H with a window of \"w\" will not include the current utterance. Correspondingly, to stay aligned with the original design intention of the task, the label statement of this task is modified as follows:\n\n\"Based on the above historical utterances, the next utterance is spoken by <P K(u i ) >, please predict the emotion states of <P K(u i ) >from <e 1 , e 2 , ..., e o >:\" Hence, the overall input for emotion impact prediction is:\n\nThe loss calculation for the emotion impact prediction task is as follows:\n\nHere, ϵ i represents the emotional label token of the text label e i corresponding to the formatted input utterance x i .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overview Of Instructerc",
      "text": "To sum up the instruction based generative framework for ERC, given an input utterance x i after concatenating the retrieval template d rvl and a LLM, the model returns the logits g i and the generated",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Inference",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Instructerc Framework (B) Demonstration Retrieval (A) Emotional Alignment",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Recurrent Neural Unit",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Llm Erc Dataset",
      "text": "Instruction ：",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Historical Content",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotional Alignment Demonstration Retrieval",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "+ Prediction Task",
      "text": "Step 1\n\nStep 2",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Label Statement Instruction",
      "text": "Figure  3 : The overview of InstructERC framework text y i for the entire sentence, including both input and output tokens. This is represented by the following equation:\n\nHere, θ is the same as mentioned. The LLM predicts the conditional probability p(γ i |x i , θ) of generating each token γ i of the generated text y i until the end symbol <eos>is outputted. As for logits g i ∈ R L×V , where L and V denote the length of the entire sentence and the size of the vocabulary used by the LLM, respectively.\n\nIn accordance with the original training method of LLMs, we adopt the next token prediction loss to measure the model's output error. Therefore, the loss calculation of the main task, denoted as L main , is defined as follows:\n\nTraining and Inference. During training and inference, our retrieval process, emotional alignment tasks and main tasks in InstructERC can be divided into two stages:\n\nIn the first stage of joint training, the characteristics of the speaker intuitively form the basis of emotional expression. Therefore, we use the speaker identification task for LLM pre-training to fine-tune speaker characteristics, which aims to preheat parameters for subsequent ERC tasks.\n\nIn the second stage, we fine-tune LLM using both the ERC main task and the emotion influence prediction task to improve overall performance.\n\nThe training loss at this stage is L main + α * L e , where α is a hyperparameter used to adjust the weight of the emotion influence prediction task loss in the second overall joint training loss.\n\nThe difference of demonstration retrieval on training and inference stage is shown in figure  3 , we limit the retrieved examples to those with the same emotion label as the current recognized speech, namely same label pairing ,in order to provide more diverse emotional understanding while avoiding excessive noise during training. During inference, there are no restrictions on the retrieved demonstrations due to the labels are unknown, namely All labels pairing. The retrieval results, simply referred as d rvl , are specialized as d t rvl and d i rvl in training and inference stage, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate the efficacy of InstructERC on three standard benchmark datasets: IEMOCAP, MELD, and EmoryNLP. The specifics of the datasets are outlined in Table  6 . The details of dataset can be refer to Appendix C.1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "Align with the related works, we select several only textual modality baselines to compare with our InstructERC. 1) Transformer-based: SPCL+CL  (Song et al., 2022)  and MPLP  (Zhang et al., 2023b)  , 2) Recurrent-based: Emo-tionIC  (Yingjian et al., 2023)  and SACL-LSTM  (Hu et al., 2023) , 3) GNN-based: DualGATs  (Zhang et al., 2023a)  and Skier  (Li et al., 2023b) . 4) LLM backbones: ChatGLM-6B & ChatGLM2-6B  (Du et al., 2022)     and LLaMA-7B  & LLaMA2-7B (Tou   vron et al., 2023) . More details of baselines and implementations can be refered to Appendix C.2 and D.1.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Main Results",
      "text": "Table  1  illustrates the results of comparing our In-structERC model with other models and backbones from different perspectives. Based on this, We make the following observations:\n\n(1) Our methods achieves significant improvements over the SOTA of discriminative models on all benchmarks. Specifically, we outperform EmotionIC, Skier, and DuaGATs by 1.73%, 1.76%, and 1.08% on IEMOCAP, MELD and EmoryNLP respectively. Notably, we completely outperformed commonsense knowledge models (Skier) on two benchmarks without any external knowl-edge, demonstrating the extreme utilization of our method for textual data.\n\n(2) To gain an insight into LLM models under different supervision scenarios for ERC task, we conduct experiments on Zero-shot + InstructERC and LoRA + InstructERC settings. It can be observed that even with carefully designed primary task instructions, LLMs still struggle in zero-shot scenarios, which further confirms the existence of a significant reasoning gap in their application to ERC sub-task. Furthermore, by utilizing the LoRA + InstructERC, the performance of the four LLMs has significantly improved, especially on the IEMOCAP dataset. This fully demonstrates the effectiveness and generalization ability of our InstructERC framework, which greatly enhances the emotion recognition capability of LLM in long texts.\n\n(3) InstructionERC is a plug-and-play method that can be adapted to multiple generative frameworks, such as prefix decoder or causal decoder. Although ChatGPT has a relevant competitive good performance on short length conversation scenrios(e.g. Meld,EmoryNLP), as can be seen, our results are far superior to the level of ChatGPT. Our unified alignment task and demonstration construction strategy are not tailored to any specific dataset design, highlighting the strong transferability and generalization capability of our approach.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablution Study",
      "text": "We conduct an ablation study to investigate the characteristics of the main components in Instruc-tERC. Table  2  shows the ablation results, and \"w/o\" denotes the model performance without a specific module. We have following observations:\n\n(1) The performance of InstructERC drops when removing any one component, which suggests that every part of the design is necessary.\n\n(2) Removing any one Emotional alignment task results in great performance degradation. This is consistent with our conjecture since speaker identification and emotion impact prediction provide relatively orthogonal semantic information from two perspectives.  4 (3) Taking away the domain retrieval module resulted in a steady decline on all three datasets, demonstrating the important role of domain information in dialogue modeling.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Unified Label Mapping",
      "text": "The Feeling Wheel  4) Removing joint alignment task tasks causes obvious performance degradation compared with removing one of them, which indicates that jointly pre-training objectives have a mutually reinforcing effect.  5 (5) Replacing LoRA with full-parameter finetuning results in a significant drop in performance, which indicates that the parameter-efficient approach is effective in preventing overfitting of LLMs on the ERC task. For detailed analysis, please refer to the \"All Parameters vs Parameter Efficiency\" section in Appendix E.4 . The further data scaling analysis of single dataset can be refer to Appendix E.5.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Emotional Label Mapping",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "One-Hot Speaker Label Mapping",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Unified Dataset Experiments",
      "text": "In real-world scenarios, the ideal ERC model should be able to address ERC challenges across multiple domains, and even carry out open-domain ERC tasks. However, biases in data distribution and labeling make it challenging for small ERC models to achieve multi-domain capabilities, To better simulate real-world scenarios, we first reconstruct three ERC datasets into a single ERC dataset (UIME) with unified labels based on the Emotion Wheel (Figure  4 ), to better suit more industrial scenarios.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Unified Dataset Experiment Setup",
      "text": "Within the settings of this experiment, all emotional labels across the datasets are standardized, and all speaker labels are also consolidated. The unification details of speaker labels and emotional labels can be refered to Appendix A. Subsequently, we conduct data scaling experiments on the UIME. To explore the impact of different sampling methods on the final performance, two data scaling approaches are experimented with: Total Mixing and Ratio Mixing.\n\nIn the \"Total Mixing\" approach, all subdatasets in UIME are first merged together, and then {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} amounts of data are randomly sampled separately from the merged data to fine-tune instructERC. Conversely, in the \"Ratio Mixing\" approach, {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} amounts of subdatasets are first randomly sampled separately, and then they are merged in The Unified Dataset Experiments of Llama2 on three benchmarks The details of results are shown in Table  5  in Appendix A, and a more intuitive presentation is shown in Figure  5 .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "The Robustness Of Instructerc",
      "text": "As depicted in the Figure  5 , Compared to the single dataset training setup, the performance of In-structERC, when fine-tuned on the UIME, has experienced a minor drop across three benchmarks. Specifically, there's a decrease of 2.4% in IEMO-CAP, 1.08% in MELD, and 1.1% in EmoryNLP. However, a relatively high Weighted F1 score (W-F1) can still be maintained simultaneously on these three benchmarks, particularly the performance of MELD(68.07%), which continues to surpass the SOTA level of all small models. The results exhibits InstructERC 's exceptional robustness, which is capable of concurrently acquiring emotional paradigms from a multitude of distinct distributions 6 .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "The Data Scaling Exploration",
      "text": "The data scaling experiments are conducted on the unified dataset from 1 to 1/64. As the scale of trainig data exponentially decreases from 1 to 1/32 within the range, the performance of the model on the three benchmarks exhibits a slight fluctuation in linear decline.\n\nWe are also surprised to discover that during the final stage of training data reduction from 1/32 to 1/64, the Total Mixing and Ratio Mixing strategies continue to exhibit a linear performance decline. However, the performance of the model trained un- 6 The statistics of scaling analysis can be found in Table  5  der the single method experiences a drastic drop, as depicted in Figure  5 . We posit that data from different scenarios endows the model with the capability to comprehend emotions from diverse perspectives. This, in turn, allows the model to achieve robust enhancements under various data conditions. Such mutual gain is particularly pronounced in low resource scenarios (1/64). This is consistent with the findings of some existing explorations in large models  (Dong et al., 2023) .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "The Discussion Of Mixing Strategies",
      "text": "We have further investigated the impact of different mixing strategies on data scaling. The results displayed by different datasets on various mixing strategies can be interpreted from the following two perspectives:\n\nData Representativeness: In Total Mixing sampling, where each dataset's samples are equally likely to be selected, the unique traits of smaller datasets like IEMOCAP may be obscured by larger ones like MELD. In contrast, Ratio Mixinging sampling, which represents each dataset proportionally to its original sample size, may better highlight the characteristics and influence of smaller datasets.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Effect Of Class Imbalance:",
      "text": "In smaller datasets with internal class imbalances, Total Mixing sampling could exacerbate these imbalances. For instance, if IEMOCAP has a relatively smaller number of samples in a certain category, Total Mixing sampling might further intensify this imbalance during model training. Ratio Mixing sampling, however, better preserves the original class proportions of the datasets, potentially mitigating class imbalance impacts to a degree.\n\nWe introduce InstructERC, a novel approach that transforms the ERC task from a discriminative framework to a generative framework using LLMs. InstructERC presents a simple and effective retrieval template adapting to different conversation lengths. Futhermore, we introduce two emotional alignment tasks to model speaker and complex conversation relationships. InstructERC outperforms all previous models and achieve comprehensive SOTA results on three benchmarks. We also pioneer in unifying label mapping and modeling across these datasets, demonstrating the InstructERC's robust generalization capabilities. Our extensive analysis provides practical insights for implementing InstructERC in real-world ERC scenarios.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Limitation",
      "text": "In this work, we focus solely on the textual aspects of these datasets. The exploration of multimodal aspects is reserved for future research. We have conducted our explorations specifically on two representative large model frameworks, ChatGLM and LLaMA. Due to limitations in our graphics card capacity, the maximum parameter size of the large models we used does not exceed 7 billion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A The Details Of Unified Dataset Experiment Setup",
      "text": "To further substantiate the efficacy and robustness of our framework, we conduct a compelling experiment involving a unified dataset. Within the settings of this experiment, all emotional labels across the datasets are standardized, and all speaker labels are also consolidated. Subsequently, we conduct data scaling experiments on the processed unified dataset. The evaluation method employed in the experimental results, utilizing the weighted F1 score, aligned with the evalution method delineated in Section Experiments. We continue to use the previous datasets IEMO-CAP, MELD, and EmoryNLP. According to The Feeling Wheel  (Willcox, 1982)  proposed in 1982, as shown in subfigure of Figure  4 , we align all emotional labels from three datasets with this standard, the details of which are shown in Tabel 3. After completion of label mapping, there are a total of 9 types of emotional labels, which are joyful, sad, neutral, mad, excited, powerful, fear, peaceful and disgust. Furthermore, due to the uniqueness of character labels in each dataset, we have renumbered them using a One-hot encoding approach, as demonstrated in the \"One-hot Speaker Label Mapping\" Table  4 , which also is shown in subfigure of Figure  4 .\n\nWe still utilize the LoRA method in PEFT to train InstructERC on the unified dataset, and the training results are evaluated on the three datasets respectively. As mentioned above, these datasets have significant variations in sample size and class imbalance within each dataset. To explore the impact of different sampling methods on the final performance, two data scaling approaches were experimented with: Total Mixing and Ratio Mixing.\n\nIn the Total Mixing approach, all datasets are combined for uniform sampling. Conversely, in the Ratio Mixing approach, datasets are sampled separately and then combined. Both approaches maintain the same quantity of training data, but due to the larger absolute number of training samples in MELD and EmoryNLP, the Total Mixing approach results in a higher proportion of samples from these two datasets when varying data scaling is applied.\n\nTotal Mixing and ratio Mixing modes are applied proportionally across the entire training set, while still segregating a validation set and a test set. The reported results are obtained after training on a unified training set and then testing on separate test sets. The Single mode, on the other hand, involves training on individual training sets and then testing on their respective test sets.\n\nMeanwhile, we design Total Mixing and Ratio Mixing experiments to explore the impact of different data mixing strategies and data quantities on the model. On the basis of the following, we further explore the impact of data sampling ratio on the model's performance.The details of results are shown in Table  5 , and a more intuitive presentation is shown in Figure  5 .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "B Related Works",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B.1 Emotion Recoginition In Conversation",
      "text": "After more than a decade of development, the field of Emotion Recognition in Conversation (ERC) has seen many outstanding works. These can be broadly classified into three categories: Transformer-based, GNN-based, Recurrent-based.\n\nSpecifically, Transformer-based works  (Li et al., 2020; Song et al., 2022; Liu et al., 2023; Yingjian et al., 2023; Chudasama et al., 2022)  attempt to establish long-range emotional correlations in conversational scenarios by directly adopting or modifying the original transformer block. These efforts have made significant contributions in this direction.\n\nGNN-based works  (Ghosal et al., 2019;   Ishi-   watari et al., 2020; Shen et al., 2021; Li et al., 2023a)  extensively use graphs and edges to model interactions between people in conversational scenarios and the influences between different modalities. They employ various forms of multi-layer graph neural networks to fit potential conversational relations, effectively exploring this direction.\n\nRecurrent-based works  (Hu et al., 2023; Lei et al., 2023; Majumder et al., 2019; Hazarika et al., 2018; Poria et al., 2017)  utilize various forms of RNNs, like LSTM and GRU, to model individual emotional states and global emotional impacts separately. They incorporate attention mechanisms or direct vector concatenation to represent personal and global emotional states collectively, marking effective exploration in this area.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B.2 Large Language Models",
      "text": "The emergence of large-scale language models (LLMs) have brought revolutionary transformation to the field of natural language processing (NLP)  (Shen et al., 2023) . LLMs, such as  GPT-3 (Brown et al., 2020) , LLaMA  (Touvron et al., 2023)  and  GPT-4 (OpenAI, 2023) , have demonstrated impressive abilities on various tasks, as well as the use of external techniques such as reinforcement learning from human feedback (RLHF)  (Ouyang et al., 2022) . LLMs based on generative framework even reformulate the multi modal perspective  (Lin et al., 2021; Zhang et al., 2023c; Qiao et al., 2024) . More recently, the NLP community has been exploring various application directions for LLMs. For instance, chain-of-thought prompting and RFT  (Wei et al., 2023; Yuan et al., 2023; Dong et al., 2024b; Li et al., 2024)  enables LLMs to generate problemsolving processes step-by-step, significantly enhancing the model's reasoning ability. Researchers have utilized the interactive capabilities of LLMs to generate commands that invoke external tools for handling of downstream tasks  (Shen et al., 2023) . a series of works  (Le et al., 2022; Qiao et al., 2023; Dong et al., 2024a)  utilize execution feedback from tools such as code executors to provide supervision for specific tasks. Other researchers have proposed parameter-efficient fine-tuining (PEFT) to address the issue of excessive computational resource without sacrificing performance  (Hu et al., 2021) .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "C Datasets & Baselines",
      "text": "C.1 Datasets IEMOCAP  (Busso et al., 2008 ) is a dataset recorded as dyadic conversational video clips with eight speaker participating in the training set while two speaker in testing set.\n\nMELD dataset  (Poria et al., 2018 ) is a multimodal dataset that has been expanded from the EmotionLines dataset. MELD is obtained from the popular TV show Friends and comprises over 1400 dialogues and 13000 utterances, each of which is labeled with emotion and sentiment classes.\n\nEmoryNLP (Zahiri and Choi, 2017) is a dataset also collected from the TV series Friends. The dataset comprises utterances that are categorized into seven distinct emotional classes.\n\nThis study exclusively focuses on the emotional classes and the text modality in these datasets. Moreover, we ensure consistency with COSMIC regarding the train/val/test splits.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C.2 Baselines",
      "text": "For discriminative ERC models, we selected several SOTA baseline for each method. For our reconstructed generative model, we chose four popular LLMs as backbones.\n\nRecurrent-based: 1) EmotionIC (Yingjian et al., 2023) uses IM-MHA and DialogGRU to capture contextual information in the dialogue, and SkipCRF to capture high-order dependencies between speakers for emotional flow simulation. 2) SACL-LSTM  (Hu et al., 2023)  extracts structured representations using contrast-aware adversarial training and joint class-spread contrastive learning, an additional contextual adversarial training strategy to enhance context robustness.\n\nTransformer-based: 1) MPLP (Lu et al., 2022) is a framework that unifies multimodal sentiment analysis and emotion recognition in conversation tasks. This framework achieves this by performing modality fusion at both the syntactic and semantic levels, and by introducing contrastive learning between modalities and samples. 2) SPCL  (Song et al., 2022)  is a method that addresses imbalanced classification issues using Prototypical Network and contrastive learning, without the need for large batch sizes, and incorporates a difficulty measure function and curriculum learning to mitigate the effects of extreme samples.\n\nGNN-based: 1) DualGATs  (Li et al., 2021 ) uses a connected graph to enhance the targeted utterance with information from the past and future context, and utilizes CommonSense Knowledge (CSK) to enrich edges with knowledge representations. 2) Skier  (Li et al., 2023a ) is a module that efficiently models contextual and interactive information for ERC task. It uses multiple extractors and PairCC strategy to address the heterogeneity gap in multimodal fusion.\n\nLLM backbones: 1) ChatGLM-6B & ChatGLM2-6B: ChatGLM-6B is an open-source conversational language model  (Du et al., 2022)  for Chinese and English. It has 6.2 billion parameters and is optimized for Chinese QA. It has been trained on 1 trillion Chinese and English identifiers and further improved through various techniques. ChatGLM2-6B is the second generation of the model, pre-trained on 1.4 trillion Chinese and English identifiers with human preference alignment training. It extends the context window to 32K and speeds up inference with Multi-Query Attention. 2) Llama-7B & Llama2-7B: Llama-7B is the 7B parameters' version of the a collection of foundation language models  (Touvron et al., 2023)  ranging from 7B to 65B parameters, which is trained on trillions of tokens. Llama2-7B pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1. Its fine-tuned models have been trained on over 1 mil-lion human annotations.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "D Implementation & Discussion",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "D.1 Implementation Details",
      "text": "We use ChatGLM and Llama as our backbone model. Considering the efficiency and effectiveness of Parameter-Efficient-Fine-Tuning (PEFT), we adopt LoRA  (Hu et al., 2021)  and insert lowrank adapters after self-attention layers. We set the dimension of adapters to 16 a nd the learning rate to 2e-4. The learning rate is set to 2e-5 for all parameters' finetune. The histoical window is set to 1, 5, 12, 20 for iemocap, meld and EmoryNLP respectively for all experiments. The retrieval parameter \"TopK\" is set to Top1 emprically. The hypermeter α is set to 0.1 during training. Greedy search is used during inference if not specified. Moreover, our experiments are conducted by taking the average of three runs with no hyperparameter searching. We train with FP16 precision on 4 × 80G Nvidia A100 GPUs.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "D.2 Discussion With Discriminative Erc Models",
      "text": "Problem definition.\n\nIn the discriminative framework, researchers first fine-tune an RoBERTA-style model with the context-free utterance, extract the feature vector at the CLS position as the input for the downstream ERC model. The aim is to map the feature vector of the given utterance to a scalar between 1 and o.\n\nIn the generative framework based on LLMs, for a given utterance, we process it into formatted text according to the pre-designed template and input it into LLMs. The aim is to enable LLMs generate the most reasonable text emotional label, which must belong to the predefined text emotional label set E = {e 1 , e 2 , ..., e o }.\n\nParameter Scales. As shown in Table  7 , we present the publicly available statistics for all trainable parameters across the models. Although the base architecture of our model is in the 6-7B parameter range, only 12.5M LoRA parameters are   7  and Figure  1 , taking the influential work such as COSMIC as an example, COSMIC fine-tuned RoBERTA on single-sentence dialogues, extracted its features, and encapsulated them into a dataset. Many works in the baseline are based on the feature dataset extracted from this work rather than the original text data for downstream model design. This means that these models (including but not limited to all the compared baselines which adopt this practice) need to use the single-sentence speech features fine-tuned with emotional labels during inference, which clearly does not conform to reality (the sentences that need to perform emotion recognition cannot access the gold emotional labels in advance). Furthermore, even if these single-sentence features do not need fine-tuning, it is still necessary to use Roberta to infer and obtain features.\n\nIn contrast, our InstructERC can directly input text and output emotional labels. Additionally, the InstructERC framework can be migrated to multiple datasets and combine datasets across multiple domains without modification, whereas discriminative models require manual changes to the architecture of the model, specifically the number of softmax classification neurons in the last layer, to perform multi-domain operations. In terms of scalability, the generative model InstructERC is clearly more practical than discriminative models.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E The Supplementary Experiments E.1 The Historical Window Exploration Study",
      "text": "In the historical window exploration shown as Table 8, we examine how different sizes of historical windows affect emotion recognition tasks. Due to token limitations, we set the upper limit for conversational turns to 20. This is an upgrade from earlier, smaller Pretrained Language Models (PLMs, e.g. Roberta  (Liu et al., 2019) ), which only support up to 5 turns. We find that a window of 12 turns is optimal for capturing the necessary historical context. In general, expanding the count of historical turns aids in enhancing the accuracy of emotion detection, a trend that is readily observable in the IEMO-CAP dataset featured long-term turns. However, there's a point where adding more historical turns doesn't lead to better results and might even harm performance, especially for datasets like MELD and EmoryNLP, which have an average length of 6 to 7 turns. However, these insights are beyond the reach of smaller PLMs that top out at 5 turns.   9 , the influence of alpha on In-structERC's performance varies across different datasets due to their unique characteristics. In general, as alpha increases, its contribution to model performance also increases, peaking at alpha=0.1. Specifically, in the IEMOCAP dataset, characterized by longer dialogues averaging 47 turns, even when alpha exceeds 0.1 significantly, there is no significant decrease in performance. However, in datasets like MELD and EmoryNLP, which have shorter dialogues averaging 7 turns, an alpha value of 0.2 can lead to a negative impact, particularly evident in MELD. Therefore, careful consideration is necessary when selecting alpha values for different datasets.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.2 The Exploration Experiments On Α",
      "text": "This phenomenon can be explained as follows: In the IEMOCAP dataset, with its longer dialogues, emotional changes occur relatively slowly. In contrast, datasets like MELD and EmoryNLP, sampled from the sitcom \"Friends\", feature many brief and intense emotional shifts. Excessive reduction in the weight of emotion impact prediction may cause the model to overly emphasize the influence of past utterances on current emotion judgment, which may not be suitable for MELD and EmoryNLP.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.3 Label Ablation Experiments",
      "text": "To further explore the impact of using the same or unrestricted emotional labels at different stages during the demonstration retrieval process on final performance, we designed experiments as shown in the table 10, where ×represents not using the same labels, and ✓represents using the same labels. Our conclusions are as follows:\n\nImpact of Label Restrictions: The performance consistently improves across all datasets when moving from unrestricted to restricted labels in both training and inference. This suggests that restricting labels helps the model learn more robust features that are better at generalizing during inference.\n\nComparison Across Datasets: IEMOCAP: Shows a steady increase in W-F1 scores as restrictions are applied first in training and then in both training and inference. The improvement from fully unrestricted to fully restricted is 1.54 points. MELD: Similar to IEMOCAP, restricted training and inference show a noticeable improvement. The gain from the least to the most restricted setup is 2.54 points, indicating a potentially more significant impact of label restriction in emotionally complex interactions, possibly due to MELD's diverse emotional content and real-life scenarios. EmoryNLP: This dataset shows the lowest overall scores but follows the same trend. The increase is 2.14 points from no restrictions to full restrictions. Given the smaller base score, this improvement is quite significant, emphasizing how crucial precise label handling is in models trained on this data.\n\nFairness and Performance Trade-offs: The best results obtained by using restricted labels in both phases might not be fair or realistic for realworld applications, where the model shouldn't have prior knowledge of the emotional context. This indicates a need for models that perform well under unrestricted conditions. The performance drop when moving to unrestricted labels in inference underscores the challenge in generalizing the learned emotional cues without specific hints, highlighting a potential area for further research in enhancing model robustness.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "E.4 All Parameters Vs Parameter Efficiency",
      "text": "In order to investigate the effect of different parameter fine-tuning methods on the ERC task, we conducted comparative experiments in Table  11 .\n\nWe have the following observations:\n\n(1) The all parameter fine-tuning performs weaker than LoRA's fine-tuning on all backbones on average performance (especially ChatGLM with a 9.32 % improvement). It is worth noting that the best performance of the full parameter method is often achieved in the first 1-3 epochs in the experiment. These findings demonstrate that parameterefficient methods are more suitable for LLMs in ERC tasks.\n\n(2) From the perspective of model structure, the average performance of full parameter ChatGLM even decreases compared to the zero-shot results in Table  1  (from 32.33% to 28.38%), while replacing it with LoRA brings a significant improvement (from 32.33% to 37.77%). Other decoder-only The Low-source Setting exploring of Llama2 on three benchmarks backbones do not show such drastic performance fluctuations, which further indicates that the prefixdecoder paradigm is unstable in ERC tasks compared to the casual decoder, and parameter-efficient frameworks can effectively alleviate this problem.\n\n(3) From the perspective of datasets, compared to full parameter fine-tuning, the performance gain of the LoRA method in MELD and EmoryNLP is significantly greater than that in IEMOCAP. We believe that this is related to the characteristics of thees datasets: IEMOCAP has long dialogue texts and multiple conversation rounds, these strong supervision signals lead to good performance in both settings. However, MELD and Emory have fewer dialogue rounds, diverse speakers, and imbalanced categories. Low-parameter methods can effectively prevent LLMs from overfitting to certain semantic patterns of dialogues format and speaker's habits, thereby enhancing the generalization ability of emotion recognition in conversation.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "E.5 Scaling Analysis In Low-Source Scenario",
      "text": "In this section, we gain an insight into the scaling analysis of data and performance for different parameter fine-tuning settings (LoRA & All Parameter), as shown in Figure  6 .\n\nParameter-efficient Scaling Analysis: On the IEMOCAP dataset, our scaling curve initially increases (from 1/16 to 1/4) and then stabilizes. This may be because the dataset has long dialogue texts and multiple dialogue rounds, leading to increased diversity with the addition of early data. However, as the supervision signal strengthens, the performance gain gradually weakens. For datasets with fewer dialogue rounds and imbalanced categories, such as MELD and EmoryNLP, our method only yields a small gain in extremely low-resource scenarios (from 1/16 to 1/4) and achieves a relatively stable performance improvement with the increase of data (from 1/2 to 1). This finding supports the idea that when a unit-scaling of data only provides weak supervision signals, the data size needs to exceed a certain threshold (1/4 -1/2) to achieve significant improvement.\n\nFull-Parameter Scaling Analysis: The scaling curves of full-parameter settings on the IEMOCAP and EmoryNLP datasets showed significant fluctuations and performance degradation in two intervals (from 1/16 to 1/8, 1/4 to 1/2) compared to LoRA. Fine-tuning large models with all parameters may cause redundant parameters to overfit the patterns in the current dialogue, which hinders the model's ability to generalize new supervised signals as data volume increases. The MELD dataset also exhibited performance degradation with data augmentation (from 1/4 to 1). These findings demonstrate the stability and robustness of parameter-efficient fine-tuning in the ERC task, providing empirical guidance for large models in industrial interfaces with ERC tasks of varying data characteristics.",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates that previous work based on",
      "page": 1
    },
    {
      "caption": "Figure 1: The illustration of different paradigms for ERC",
      "page": 2
    },
    {
      "caption": "Figure 3: Firstly, we provide a brief intro-",
      "page": 3
    },
    {
      "caption": "Figure 2: , for ERC task, each",
      "page": 3
    },
    {
      "caption": "Figure 2: Historical Content. To model the context in",
      "page": 3
    },
    {
      "caption": "Figure 2: Label Statement. To confine the model’s output",
      "page": 3
    },
    {
      "caption": "Figure 2: The Schematic of Retrieval Template Module.",
      "page": 3
    },
    {
      "caption": "Figure 2: Demonstration Retrieval. In order to further",
      "page": 3
    },
    {
      "caption": "Figure 2: In summary, after",
      "page": 3
    },
    {
      "caption": "Figure 3: Speaker Identification. Emotions are expressed",
      "page": 4
    },
    {
      "caption": "Figure 3: The overview of InstructERC framework",
      "page": 5
    },
    {
      "caption": "Figure 4: Unified Label Mapping Across three Open-source Benchmarks. The Feeling Wheel is proposed by",
      "page": 7
    },
    {
      "caption": "Figure 4: ), to better suit more industrial",
      "page": 7
    },
    {
      "caption": "Figure 5: The data scaling analysis demonstrated on three benchmarks using different data mixing strategies",
      "page": 8
    },
    {
      "caption": "Figure 5: , Compared to the sin-",
      "page": 8
    },
    {
      "caption": "Figure 5: We posit that data from differ-",
      "page": 8
    },
    {
      "caption": "Figure 4: , we align all emo-",
      "page": 12
    },
    {
      "caption": "Figure 4: We still utilize the LoRA method in PEFT to",
      "page": 12
    },
    {
      "caption": "Figure 1: , taking the influential work such",
      "page": 16
    },
    {
      "caption": "Figure 6: The scaling of data and performance for different parameter fine-tuning settings (LoRA & All Parameters)",
      "page": 18
    },
    {
      "caption": "Figure 6: Parameter-efficient Scaling Analysis: On the",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data Precent": "",
          "IEMOCAP W-F1": "Total Mixing\nRatio Mixing\nSingle",
          "MELD W-F1": "Total Mixing\nRatio Mixing\nSingle",
          "EmoryNLP W-F1": "Total Mixing\nRatio Mixing\nSingle"
        },
        {
          "Data Precent": "1\n1/2\n1/4\n1/8\n1/16\n1/32\n1/64",
          "IEMOCAP W-F1": "71.39\n68.99\n68.99\n69.13\n67.95\n68.96\n67.54\n63.02\n64.46\n64.13\n58.48\n60.06\n60.42\n57.77\n53.40\n54.76\n45.89\n48.50\n43.07\n38.42\n30.34",
          "MELD W-F1": "69.15\n68.07\n68.07\n67.54\n66.50\n66.42\n66.42\n66.41\n65.85\n65.14\n64.57\n62.94\n62.89\n61.15\n58.42\n57.76\n57.38\n57.72\n54.26\n53.29\n45.48",
          "EmoryNLP W-F1": "41.37\n40.27\n40.27\n39.65\n39.18\n39.33\n38.33\n38.26\n37.29\n39.24\n38.27\n38.24\n37.60\n37.19\n36.83\n37.09\n36.09\n34.03\n35.19\n34.65\n26.10"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 11: diverse emotional content and real-life scenarios. Wehavethefollowingobservations:",
      "data": [
        {
          "IEMOCAP\nIEMOCAP\nIEMOCAP": "MELD\nMELD\nMELD",
          "×\n✓\n✓": "×\n✓\n✓",
          "×\n×\n✓": "×\n×\n✓",
          "70.71\n71.39\n72.25": "68.52\n69.15\n71.06"
        },
        {
          "IEMOCAP\nIEMOCAP\nIEMOCAP": "EmoryNLP\nEmoryNLP\nEmoryNLP",
          "×\n✓\n✓": "×\n✓\n✓",
          "×\n×\n✓": "×\n×\n✓",
          "70.71\n71.39\n72.25": "40.54\n41.37\n42.68"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Gray"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani"
      ],
      "venue": "Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "5",
      "title": "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
      "authors": [
        "Guanting Dong",
        "Keming Lu",
        "Chengpeng Li",
        "Tingyu Xia",
        "Bowen Yu",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Self-play with execution feedback: Improving instruction-following capabilities of large language models",
      "arxiv": "arXiv:2406.13542"
    },
    {
      "citation_id": "6",
      "title": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "authors": [
        "Guanting Dong",
        "Hongyi Yuan",
        "Keming Lu",
        "Chengpeng Li",
        "Mingfeng Xue",
        "Dayiheng Liu",
        "Wei Wang",
        "Zheng Yuan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "How abilities in large language models are affected by supervised fine-tuning data composition",
      "arxiv": "arXiv:2310.05492"
    },
    {
      "citation_id": "7",
      "title": "Understand what llm needs: Dual preference alignment for retrieval-augmented generation",
      "authors": [
        "Guanting Dong",
        "Yutao Zhu",
        "Chenghao Zhang",
        "Zechen Wang",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "year": "2024",
      "venue": "Understand what llm needs: Dual preference alignment for retrieval-augmented generation",
      "arxiv": "arXiv:2406.18676"
    },
    {
      "citation_id": "8",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "10",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "11",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "12",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "Edward Hu",
        "Yelong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Lu Wang",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models"
    },
    {
      "citation_id": "13",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning",
      "authors": [
        "Hung Le",
        "Yue Wang",
        "Akhilesh Deepak Gotmare",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "Coderl: Mastering code generation through pretrained models and deep reinforcement learning"
    },
    {
      "citation_id": "15",
      "title": "Watch the speakers: A hybrid continuous attribution network for emotion recognition in conversation with emotion disentanglement",
      "authors": [
        "Shanglin Lei",
        "Xiaoping Wang",
        "Guanting Dong",
        "Jiang Li",
        "Yingjian Liu"
      ],
      "year": "2023",
      "venue": "Watch the speakers: A hybrid continuous attribution network for emotion recognition in conversation with emotion disentanglement",
      "arxiv": "arXiv:2309.09799"
    },
    {
      "citation_id": "16",
      "title": "Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning",
      "authors": [
        "Chengpeng Li",
        "Guanting Dong",
        "Mingfeng Xue",
        "Ru Peng",
        "Xiang Wang",
        "Dayiheng Liu"
      ],
      "year": "2024",
      "venue": "Dotamath: Decomposition of thought with code assistance and self-correction for mathematical reasoning",
      "arxiv": "arXiv:2407.04078"
    },
    {
      "citation_id": "17",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "19",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "arxiv": "arXiv:2003.01478"
    },
    {
      "citation_id": "20",
      "title": "Rui Mao, and Erik Cambria. 2023b. Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "Wei Li",
        "Luyao Zhu"
      ],
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Jie Tang, and Hongxia Yang. 2021. M6: Multi-modality-to-multimodality multitask mega-transformer for unified pretraining",
      "authors": [
        "Junyang Lin",
        "Rui Men",
        "An Yang",
        "Chang Zhou",
        "Yichang Zhang",
        "Peng Wang",
        "Jingren Zhou"
      ],
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, KDD '21",
      "doi": "10.1145/3447548.3467206"
    },
    {
      "citation_id": "22",
      "title": "Hierarchical dialogue understanding with special tokens and turn-level attention",
      "authors": [
        "Xiao Liu",
        "Jian Zhang",
        "Heng Zhang",
        "Fuzhao Xue",
        "Yang You"
      ],
      "year": "2023",
      "venue": "Hierarchical dialogue understanding with special tokens and turn-level attention",
      "arxiv": "arXiv:2305.00262"
    },
    {
      "citation_id": "23",
      "title": "Unified structure generation for universal information extraction",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "; Yaojie Lu",
        "Qing Liu",
        "Dai Dai",
        "Xinyan Xiao",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun",
        "Hua Wu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "24",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "25",
      "title": "Society of mind. Simon and Schuster. OpenAI. 2023. Gpt-4 technical report",
      "authors": [
        "Marvin Minsky"
      ],
      "year": "1988",
      "venue": "Society of mind. Simon and Schuster. OpenAI. 2023. Gpt-4 technical report"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "Long Ouyang",
        "Jeff Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray",
        "John Schulman",
        "Jacob Hilton",
        "Fraser Kelton",
        "Luke Miller",
        "Maddie Simens",
        "Amanda Askell",
        "Peter Welinder",
        "Paul Christiano",
        "Jan Leike",
        "Ryan Lowe"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "27",
      "title": "Semeval-2016 task 5: Aspect based sentiment analysis",
      "authors": [
        "Maria Pontiki",
        "Dimitris Galanis",
        "Haris Papageorgiou",
        "Ion Androutsopoulos",
        "Suresh Manandhar",
        "Al-Smadi Mohammed",
        "Mahmoud Al-Ayyoub",
        "Yanyan Zhao",
        "Bing Qin",
        "Orphée De Clercq"
      ],
      "year": "2016",
      "venue": "ProWorkshop on Semantic Evaluation (SemEval-2016)"
    },
    {
      "citation_id": "28",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "29",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "30",
      "title": "We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint",
      "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Guanting Dong",
        "Minhui Wu",
        "Chong Sun",
        "Xiaoshuai Song",
        "Zhuoma Gongque",
        "Shanglin Lei",
        "Zhe Wei",
        "Miaoxuan Zhang"
      ],
      "year": "2024",
      "venue": "We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint",
      "arxiv": "arXiv:2407.01284"
    },
    {
      "citation_id": "31",
      "title": "Making language models better tool learners with execution feedback",
      "authors": [
        "Shuofei Qiao",
        "Honghao Gui",
        "Chengfei Lv",
        "Qianghuai Jia",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "year": "2023",
      "venue": "Making language models better tool learners with execution feedback",
      "arxiv": "arXiv:2305.13068"
    },
    {
      "citation_id": "32",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "33",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "34",
      "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "authors": [
        "Yongliang Shen",
        "Kaitao Song",
        "Xu Tan",
        "Dongsheng Li",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "year": "2023",
      "venue": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface"
    },
    {
      "citation_id": "35",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "arxiv": "arXiv:2210.08713"
    },
    {
      "citation_id": "36",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin",
        "Edouard Grave",
        "Guillaume Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models"
    },
    {
      "citation_id": "37",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "arxiv": "arXiv:2109.01652"
    },
    {
      "citation_id": "38",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
      ],
      "year": "2023",
      "venue": "Chain-of-thought prompting elicits reasoning in large language models"
    },
    {
      "citation_id": "39",
      "title": "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy",
      "authors": [
        "Gloria Willcox"
      ],
      "year": "1982",
      "venue": "Transactional Analysis Journal"
    },
    {
      "citation_id": "40",
      "title": "Qwen2 technical report",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang",
        "Guanting Dong",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jialin Wang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Ma",
        "Jin Xu",
        "Jingren Zhou",
        "Jinze Bai",
        "Jinzheng He",
        "Junyang Lin",
        "Kai Dang",
        "Keming Lu",
        "Keqin Chen",
        "Kexin Yang",
        "Mei Li",
        "Mingfeng Xue",
        "Na Ni",
        "Pei Zhang",
        "Peng Wang",
        "Ru Peng",
        "Rui Men",
        "Ruize Gao",
        "Runji Lin",
        "Shijie Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Tianhang Zhu",
        "Tianhao Li",
        "Tianyu Liu",
        "Wenbin Ge",
        "Xiaodong Deng",
        "Xiaohuan Zhou",
        "Xingzhang Ren",
        "Xinyu Zhang",
        "Xipin Wei",
        "Xuancheng Ren",
        "Yang Fan",
        "Yang Yao",
        "Yichang Zhang",
        "Yu Wan",
        "Yunfei Chu",
        "Yuqiong Liu",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zhihao Fan"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report",
      "arxiv": "arXiv:2407.10671"
    },
    {
      "citation_id": "41",
      "title": "Emotionic: Emotional inertia and contagion-driven dependency modelling for emotion recognition in conversation",
      "authors": [
        "Li Liu Yingjian",
        "Wang Jiang",
        "Zeng Xiaoping",
        "Zhigang"
      ],
      "year": "2023",
      "venue": "Emotionic: Emotional inertia and contagion-driven dependency modelling for emotion recognition in conversation",
      "arxiv": "arXiv:2303.11117"
    },
    {
      "citation_id": "42",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "Zheng Yuan",
        "Hongyi Yuan",
        "Chengpeng Li",
        "Guanting Dong",
        "Chuanqi Tan",
        "Chang Zhou",
        "; Sayyed",
        "M Zahiri",
        "Jinho D Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "arxiv": "arXiv:2308.01825"
    },
    {
      "citation_id": "43",
      "title": "2023a. DualGATs: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.408"
    },
    {
      "citation_id": "44",
      "title": "2023b. Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "authors": [
        "Ting Zhang",
        "Zhuang Chen",
        "Ming Zhong",
        "Tieyun Qian"
      ],
      "venue": "2023b. Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "arxiv": "arXiv:2306.06601"
    },
    {
      "citation_id": "45",
      "title": "2023c. Pay attention to implicit attribute values: A multi-modal generative framework for AVE task",
      "authors": [
        "Yupeng Zhang",
        "Shensi Wang",
        "Peiguang Li",
        "Guanting Dong",
        "Sirui Wang",
        "Yunsen Xian",
        "Zhoujun Li",
        "Hongzhi Zhang"
      ],
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "46",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "47",
      "title": "LLM-based NOTE: The best-performing results of other models are highlighted in gold font, while SOTA results across all models are emphasized in red font. Models annotated with an * indicate results sourced from the model's paper, and a ( †) denotes results from reproductions conducted by the authors",
      "venue": "LLM-based NOTE: The best-performing results of other models are highlighted in gold font, while SOTA results across all models are emphasized in red font. Models annotated with an * indicate results sourced from the model's paper, and a ( †) denotes results from reproductions conducted by the authors"
    }
  ]
}