{
  "paper_id": "2503.13485v1",
  "title": "A Causal Inference Approach For Quantifying Research Impact",
  "published": "2025-03-07T10:06:42Z",
  "authors": [
    "Keiichi Ochiai",
    "Yutaka Matsuo"
  ],
  "keywords": [
    "Research Impact",
    "Causal Inference",
    "Bibliometric"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has had a great impact on various fields of computer science by enabling data-driven representation learning in a decade. Because science and technology policy decisions for a nation can be made on the impact of each technology, quantifying research impact is an important task. The number of citations and impact factor can be used to measure the impact for individual research. What would have happened without the research, however, is fundamentally a counterfactual phenomenon. Thus, we propose an approach based on causal inference to quantify the research impact of a specific technical topic. We leverage difference-in-difference to quantify the research impact by applying to bibliometric data. First, we identify papers of a specific technical topic using keywords or category tags from Microsoft Academic Graph, which is one of the largest academic publication dataset. Next, we build a paper citation network between each technical field. Then, we aggregate the crossfield citation count for each research field. Finally, the impact of a specific technical topic for each research field is estimated by applying difference-in-difference. Evaluation results show that deep learning significantly affects computer vision and natural language processing. Besides, deep learning significantly affects cross-field citation especially for speech recognition to computer vision and natural language processing to computer vision. Moreover, our method revealed that the impact of deep learning was 3.1 times of the impact of interpretability for ML models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning made a remarkable impact on many scientific disciplines especially in computer science domain such as image recognition and natural language processing  [23] . Deep Learning has been used in various fields since the success of image recognition at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012  [21] . As a result, Yoshua Bengio, Geoffrey Hinton, and Yann LeCun received the 2018 ACM A.M. Turing Award for breakthroughs of deep neural networks that revolutionalized in Artificial Intelligence (AI) 1 . Although deep learning made a qualitatively significant impact on AI, a quantitative impact remains unveiled. It is important to quantify the impact of research on a technical topic, not only deep learning, on the research field. Therefore, we aim to consider how to quantify the impact of research on a technical topic.\n\nQuantifying research impact is an important task for a national government and individual researchers. From the perspective of a national government, in national science and technology policy, it is necessary to measure the impact of research because the national government plans its budget and personnel based on the prediction of technological development. From the perspective of individual researchers, it is useful to understand the influence of past research when considering research themes.\n\nA naive method to quantify the research impact is to measure the number of citations of individual research. When measuring the effects of a technical topic, it is possible to aggregate citations by averages. Suppose that a specific technical topic occurred at year ùëÅ (e.g., Deep Learning at 2012), and we would like to quantify its impact. One simple method is to compare the number of papers whose topic is neural network before and after year ùëÅ . However, this method has two problems. First, what would have happened without the research is fundamentally a counterfactual phenomenon (counterfactual problem (P1)). Second, the difference between before and after a specific technical topic occurred could contain several confounding factors not only the effect of the technical topic occurred but also a temporal trend, etc. This is the well-known problem of pre-post study design (pre-post comparison problem (P2))  [7, 13] .\n\nIn this paper, we propose a framework based on causal inference to quantify the research impact of a specific technical topic using a large-scale citation dataset. The key idea of our approach is leveraging difference-in-difference  [22]  to bibliometric data. Figure  1  shows the conceptual overview of the proposed approach. First, we identify papers of a specific technical topic from the research paper dataset using keywords or category tags. The identified papers are referred to as treatment group papers, and the remained papers are referred to as control group papers. We use Microsoft Academic Graph (MAG)  [33, 38, 39]  which is one of the largest academic publication dataset as a research paper dataset. Second, we build a paper citation network using the treatment group papers, the control group papers, and papers cited by both groups of papers. Next, the number of citations related to a specific topic (treatment group) and those not related to a specific topic (control group) is counted for each research field. Finally, the impact of a research topic for each research field is estimated by applying difference-in-difference. Exploiting causal inference can solve the counterfactual problem (P1) and pre-post comparison problem (P2). We validate our framework using several technical topics (e.g., deep learning and interpretability for ML models) for AI top conferences including quantifying the research impact of deep learning, and comparing the research impact of deep learning with other topics. Evaluation results show that (1) deep learning significantly affects computer vision (+42.15% publication) and natural language processing (+35.31% publication), (2) data mining papers cite many papers in various fields, (3) deep learning significantly affect cross-field citation especially for speech recognition to computer vision (+17.4% p.p.), and natural language processing to computer vision (+7.1% p.p.), and (4) the quantified  The rest of this paper is organized as follows. In the next section, we reviews related work on the analysis of bibliometric data. We explain the dataset used in this study in Section 3. Section 4 describes the proposed approach. We evaluate our approach in Section 5. Finally, we conclude this study and discuss future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Quantifying Research Impact",
      "text": "Most basic approach to quantify a research impact is to use citation count, such as total number of citations and average number of citations per publication  [36] . However, it is necessary to calculate the number of citations by considering various factors (e.g., time window and research field) depending on the purpose of comparing studies. In the existing studies, several types of research impacts have been proposed such as long-term impact  [37] , impact of individual  [32]  and impact scores for individual research publications  [35] . In these studies, the situation of what would have happened without the study is not taken into account.\n\nThere are several studies which exploit causal inference for quantifying the research impact. Farys and Wolbring estimated the effect of the Nobel Prize for publication using matched control group  [16] . Bornmann et al. proposed to leverage the propensity score to consider the difference in the number of publications in each field (i.e., field-normalization)  [6] . Since DID is more suitable for the estimation considering the fixed effect than propensity score matching  [18] , we proposed to leverage the DID method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Citation Prediction And Trend Detection",
      "text": "Citation prediction and trend detection are widely studied research topics  [8, 27, 40] . For example, Abrishami et al. proposed a method to predict citation count using deep neural networks  [1] . Dong et al. predicted the effect for the author's h-index by publishing a paper  [15] . Asatani et al. proposed a framework that detects the research trend using network representation learning for a citation network  [5] . Recently, Shao et al. investigated the evolution and trend of artificial intelligence using an academic knowledge graph on AI, called AI 2000  [30] . Besides, Frank et al.  [17]  analyzed the evolution of AI. These studies focus on prediction and detection method which differs from our study.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "We use Microsoft Academic Graph (MAG)  [33, 38, 39]  which is one of the largest academic publication dataset provided by Microsoft 2  . MAG contains paper information such as paper title, author, published year, venue, field of study (technical topic). Comprehensive data schema is provided at the web page of MAG 3  . As of February 1, 2021, MAG consists of 251,493,799 papers and 53,529 venues. To focus on AI field, we filter the dataset by AI related venues. To select the venues, we follow the work of Shao et al.  [30]  which defines 20 sub-fields of AI such as Classical AI, Machine Learning (ML), Data Mining (DM), Computer Vision (CV), Natural Language Processing (NLP), Speech Recognition (SR) etc. They made these sub-field by interviewing experts in each sub-field. We slightly change the list of venues for each sub-field. For example, COLT (Conference on Learning Theory), which is originally assigned to NLP, is changed  to an AI conference rather than an NLP conference. The complete venue lists of each sub-field are shown in Table  1 .\n\nFigure  2  shows the number of published papers for each research field from 2007 to 2020. Because the transition of the number of papers has an uptrend, we have to consider the trend to compare the data of different time points. Finally, the number of selected venues is 39, and the number of papers is 318,061.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Framework",
      "text": "Our goal is to quantify the impact of the emergence of a specific technical topic, taking causality into account. The ideal method is to perform a so-called Randomized Controlled Trial (RCT), in which research topics in each paper are randomly assigned and compared the citation count. However, this is impractical, as researchers cannot be forced to write a paper regarding a specific topic. Thus, we adopt quasi-experimental design  [10]  which assesses a causality based purely on the observational data. Specifically, we conduct difference-in-differences (DID)  [22] , commonly used in economics  [14]  and healthcare  [12]  literature. Similar approach is used to quantify the effect of audio streaming consumption (i.e., music vs podcast)  [24] , the effect of online social actions to offline behavior  [3]  and the change of human needs during COVID-19 pandemic using web search logs  [34] . We treat the emergence of a novel specific technical topic as the intervention. Then, papers which are related to a specific technical topic are considered as treatment group, and papers which are not related to a specific technical topic are considered as control group.\n\nTo conduct DID, the proposed framework consists of four steps.\n\n(1) Identifying treatment group papers (paper related to a specific technical topic) from academic paper database. (2) Building a paper citation network using the treatment group papers and papers cited by the treatment group papers (3) Counting intra and inter sub-field citation using the paper citation network. (4) Conducting DID to estimate the effect of a specific technical topic emerged.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Identifying Treatment Group Papers",
      "text": "A naive method to identify a technical topic of a paper is keyword matching to paper title or abstract, whereas MAG dataset has more rich information regarding technical topic of a paper. MAG contains field of study field which represents the technical topic of each paper such as deep neural networks, gradient boosting etc. Thus, we exploit field of study to identify the technical topic of a paper. Field of study for each paper is labeled automatically  [33] , and the topical hierarchy is generated by applying hierarchical topic modeling  [31] .\n\nTo create the treatment group papers which are related to a specific topic, we extract the papers by keyword matching using list of keywords for field of study field. For example, if we would like to extract papers related to deep learning, the examples of keyword are neural network, deep learning, and long short term memory. In out experiment, we further filter the papers by AI related conferences defined in 3 to focus on AI related topics, and added sub-fields of AI (Classical AI, ML, DM, CV, NLP, SR). The remained papers, which are not matched to the keyword list in field of study field, are defined as control group papers. Finally, both the treatment group papers and the control group papers contain paper id, published year, venue, field of study, sub-fields of AI.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Building A Paper Citation Network",
      "text": "In this step, we first extract papers which are cited by the treatment group papers from MAG, called as cited papers for treatment. Then, a paper citation network is built using the treatment group papers and cited papers for treatment. Similar to treatment group, cited papers for control group papers are also extracted from MAG, and a paper citation network for control group papers is build using the control group papers and cited paper for control group. At the end of this step, the processed dataset contains original papers and cited papers information (paper id, venue, published year, subfields of AI for both original papers and cited papers, and reference relationship).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Calculating Paper Citation",
      "text": "Most simple method to calculate the citation measure is simply count the number of paper\n\nwhere ùëá = [ùë° ùë† , ùë° ùëí ] is target period (ùë° ùë† is start time and ùë° ùëí is end time), ùëú ùëîùëüùëúùë¢ùëù is the research field of the original paper, ùëîùëüùëúùë¢ùëù ‚àà ùë°ùëüùëíùëéùë°ùëöùëíùëõùë°, ùëêùëúùëõùë°ùëüùëúùëô, and ùëùùëéùëùùëíùëü _ùëõùë¢ùëö ùëá ,ùëú is the number of papers of field ùëú within time period ùëá . One of our objective is to quantify the effect of the emergence of a technical topic to other research field. Thus, we can also consider the inter-field citation. Because the number of papers varies depending on the research field, field-normalization  [36]  should be applied to remove a selection bias. Hence, we calculate the citation ratio for comparing the inter-field citation effect as follows.\n\nwhere",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Difference-In-Differences",
      "text": "Before conducting DID, we need to confirm that the parallel trends assumption is met. There are several method to verify the parallel trends assumption. We adopt the parallel trend test using slope of regression model before the intervention  [2, 26] . Specifically, we test the difference of the slopes for regression models of both treatment group and control group  [4] . As an alternative method, we can adopt propensity score matching  [24]  to validate the indistinguishable of two groups. Then, the average treatment effect (ATE) is estimated by DID by comparing the average difference of the before and after of each group. There are two methods to calculate ATE, i.e., absolute effect and relative effect. The absolute effect can be calculated as follows.\n\nOn the other hand, the relative effect can be calculated similar to the work of  [34]  as follows.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analyses And Results",
      "text": "In this section, we conduct a qualitative evaluation through case studies by considering research questions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Case 1: Deep Learning",
      "text": "Background: One of the important role of deep learning is representation learning that allows a machine to automatically generate the representations  [23] , not the hand-crafted representations, needed for classification or regression. While hand-crafted feature was designed by using domain knowledge in each research field before the rise of deep learning, the realization of representation learning using deep learning is thought to have accelerated research across fields. Therefore, we aim to quantify the impact of deep learning as a technical field in the research of computer science domain. Specifically, we solve the following research questions.\n\n‚Ä¢ RQ1: How much did deep learning affect each field of computer science? ‚Ä¢ RQ2: To what extent does deep learning impact the integration of computer science disciplines?\n\nTo answer these research questions, we used the following setting. To identify the research related to deep learning (which indicates that the paper is regarded as the treatment group), the paper which included the word of the neural network in the field of study was extracted. We set the time of intervention was 2012 because the drastic improvement of image recognition accuracy in ILSVRC 2012 was one of the opportunities of the popularization of deep learning  [21] . Thus, we set ùëá  (i.e., before and after 5 years). We count the number of paper citation by Eq.(  2 ). We estimated the ATE by Eq.(  4 ) for RQ1, and Eq.(3) for RQ2.\n\n5.1.1 RQ1: How much did deep learning affect each field of computer science? Table  2  shows the result of relative change in the number of publication in each research field for deep learning. Note that when we test the parallel trend for pre-intervention period from 2002 to 2011, most research fields except for Classical AI were not met the parallel trend. Although we need to adjust covariates from this results, we can see that the number of publication regarding deep learning has greatly increased after 2012, especially for computer vision and natural language processing.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Rq2:",
      "text": "To what extent did deep learning impact the integration of computer science disciplines? Table  3  shows the inter-field effect of deep learning. Original Field indicates the paper cite other field, Cited Field indicates the paper cited from other research fields, Change of p.p. represents the change of percentage points of citation (CoP for short), and * means that the parallel trend was not met. From this table, the following trends are observed by focusing on CoP increases of more than 10 points.\n\n(1) Data mining papers cite many papers in various fields.\n\n(2) Speech recognition papers cite computer vision papers.\n\nFor  (1) , it seems natural that data mining utilizes technologies in various fields. For (2), deep neural networks were used in speech recognition in 2012  [19] , and it seems to have been affected by deep learning from an early stage. Besides, because the paper of ICASSP 2013  [11]  4 by Deng, Hinton and Kingsbury noted that \"Convolutional neural networks have been widely used in computer vision where they have been very successful  [21] \", it may have triggered the citation of the image recognition field.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Case 2: Comparison Of Deep Learning With Other Technical Topics",
      "text": "Background: Deep learning is not the only technology at the top of the AI conference. For example, gradient boosting decision tree (GBDT) such as LightGBM  [20]  and XGBoost  [9] , and the interpretability of ML models such as LIME  [28]  and SHAP  [25]  are also attracting attention. Thus, we aim to quantitatively compare the impact of deep learning with other technical topics in computer science. Specifically, we solve the following research questions. 4 The number of citations is 1060 as of Oct, 17th, 2021 by Google scholar To answer this research question, we used the following setting. For deep learning, we used the same setting in the previous subsection. To identify the research related to GBDT, the paper which included the word of the gradient boost in the field of study was extracted. Similarly, to identify the research related to interpretability of ML models, the paper which included the word of the interpretability in the field of study was extracted.\n\nWe set the time of intervention for GBDT was 2016 because the paper of XGBoost was published at KDD 2016. Similarly, the time of intervention for interpretability of ML models was set at 2016 because the paper of LIME  [28]  was published at KDD 2016. Thus, we set ùëá 1 = [2010, 2015] and ùëá 2 = [2016, 2020] (i.e., before and after 5 years) for both GBDT and interpretability of ML models. We count the number of paper citation by Eq.  (2) . and the ATE is estimated using Eq.(3).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Rq3:",
      "text": "To what extent did deep learning affect other technical topics? Table  4  shows the result of different technical topics. DL indicates deep learning, GBDT indicates gradient boosting decision tree, and INTP indicates interpretability of ML models. Because the order of the research impact of each technology was deep learning, interpretability of ML models and GBDT by comparing the average of the relative change, the research impact of deep learning is the most influential. Here, we discuss these results in more depth. Notably, NLP and SR have a relatively high values of the relative change compared to other research field for GBDT. In NLP and SR, a tree-based classifier such as GBDT may be likely to be used because the structure of sentence modification is handled in a tree structure. Moreover, CV has a significantly high value of the relative change compared to other research field. In CV field, the relative change may be high because studies on the interpretability peculiar to images such as Grad-CAM  [29]  are being conducted.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Limitations And Implications",
      "text": "Limitation: Our work has the following limitations.\n\n(1) Method of causal inference: Although we used the DID method, if concurrent technical trends have significantly influenced the number of citation, the reliability of the estimated impact may be low. Besides, because the parallel trend was not met in several cases, if we adopt propensity score matching, the estimated results can be more reliable. Futhermore, there may be potentially spill-over effects between the treatment and control groups (e.g., the citations between treatment and control papers). In this case, the vanilla DID is not suitable for this analysis.\n\n(2) Identifying a technical topic: in our implementation, the identification of a technical topic relies on the field of study field automatically tagged by the work of  [33] . Thus, incorrect tagging leads to incorrect estimation. We can alternatively identify the topic of a paper by using keyword matching for the title or abstract. Then, the estimated impact may be affected. Implications: Our results lead to implications for the prediction of research impact. If we combine the proposed research impact measure and the paper citation prediction, we can obtain more precise research impact on each research field. Besides, if keywords",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We have proposed a conceptual framework based on causal inference to quantify the research impact of a specific technical topic using large-scale citation dataset. The proposed framework exploited the difference-in-difference to quantify the research impact. We validated our framework using several technical topic (e.g., deep learning and interpretability for ML models) for AI top conferences through two case studies. Our findings was (1) computer vision and natural language processing were the most influenced research field by deep learning (+42.15% publication for computer vision, and +35.31% publication for natural language processing), (2) data mining research exploited the research of various fields, (3) deep learning significantly promoted cross field citation especially for speech recognition to computer vision (+17.4% p.p.), and natural language processing to computer vision (+7.1% p.p.), and (4) the impact of deep learning was 3.1 times of the impact of interpretability for ML models. We believe that this study opens up a novel bibliometrics approach for quantifying the research impact, and hope that the approach will be helpful in real world.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the conceptual overview of the proposed approach. First, we",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the proposed approach.",
      "page": 2
    },
    {
      "caption": "Figure 2: The number of papers for each research field from",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the number of published papers for each research",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "NLP",
          "Target": "NLP",
          "Citation Ratio \n(before)": "0.933",
          "Citation Ratio \n(after)": "0.891"
        },
        {
          "Source": "NLP",
          "Target": "CV",
          "Citation Ratio \n(before)": "0.007",
          "Citation Ratio \n(after)": "0.265"
        },
        {
          "Source": "NLP",
          "Target": "SR",
          "Citation Ratio \n(before)": "0.066",
          "Citation Ratio \n(after)": "0.093"
        },
        {
          "Source": "CV",
          "Target": "CV",
          "Citation Ratio \n(before)": "0.933",
          "Citation Ratio \n(after)": "0.891"
        },
        {
          "Source": "CV",
          "Target": "NLP",
          "Citation Ratio \n(before)": "0.007",
          "Citation Ratio \n(after)": "0.265"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "NLP",
          "Target": "NLP",
          "Citation Ratio\n(before)": "0.933",
          "Citation Ratio\n(after)": "0.891"
        },
        {
          "Source": "NLP",
          "Target": "CV",
          "Citation Ratio\n(before)": "0.007",
          "Citation Ratio\n(after)": "0.265"
        },
        {
          "Source": "NLP",
          "Target": "SR",
          "Citation Ratio\n(before)": "0.066",
          "Citation Ratio\n(after)": "0.093"
        },
        {
          "Source": "CV",
          "Target": "CV",
          "Citation Ratio\n(before)": "0.933",
          "Citation Ratio\n(after)": "0.891"
        },
        {
          "Source": "CV",
          "Target": "NLP",
          "Citation Ratio\n(before)": "0.007",
          "Citation Ratio\n(after)": "0.265"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Predicting citation counts based on deep neural network learning techniques",
      "authors": [
        "Ali Abrishami",
        "Sadegh Aliakbary"
      ],
      "year": "2019",
      "venue": "Journal of Informetrics"
    },
    {
      "citation_id": "2",
      "title": "Effects of federal policy to insure young adults: evidence from the 2010 Affordable Care Act's dependent-coverage mandate",
      "authors": [
        "Yaa Akosa Antwi",
        "Asako Moriya",
        "Kosali Simon"
      ],
      "year": "2013",
      "venue": "American Economic Journal: Economic Policy"
    },
    {
      "citation_id": "3",
      "title": "Online Actions with Offline Impact: How Online Social Networks Influence Online and Offline User Behavior",
      "authors": [
        "Tim Althoff",
        "Pranav Jindal",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Proceedings of the Tenth ACM International Conference on Web Search and Data Mining",
      "doi": "10.1145/3018661.3018672"
    },
    {
      "citation_id": "4",
      "title": "Statistical methods in medical research",
      "authors": [
        "Peter Armitage",
        "Geoffrey Berry",
        "John Nigel",
        "Scott Matthews"
      ],
      "year": "2008",
      "venue": "Statistical methods in medical research"
    },
    {
      "citation_id": "5",
      "title": "Detecting trends in academic research from a citation network using network representation learning",
      "authors": [
        "Kimitaka Asatani",
        "Junichiro Mori",
        "Masanao Ochi",
        "Ichiro Sakata"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "6",
      "title": "Should citations be field-normalized in evaluative bibliometrics? An empirical analysis based on propensity score matching",
      "authors": [
        "Robin Lutz Bornmann",
        "R√ºdiger Haunschild",
        "Mutz"
      ],
      "year": "2020",
      "venue": "Journal of Informetrics"
    },
    {
      "citation_id": "7",
      "title": "Experimental and quasi-experimental designs for research",
      "authors": [
        "Donald Thomas Campbell",
        "Julian Stanley",
        "Nathaniel Gage"
      ],
      "year": "1963",
      "venue": "Experimental and quasi-experimental designs for research"
    },
    {
      "citation_id": "8",
      "title": "A data analytic approach to quantifying scientific impact",
      "authors": [
        "Xuanyu Cao",
        "Yan Chen",
        "Ray Liu"
      ],
      "year": "2016",
      "venue": "Journal of Informetrics"
    },
    {
      "citation_id": "9",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "Tianqi Chen",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "10",
      "title": "Experimental and quasi-experimental designs for generalized causal inference",
      "authors": [
        "Donald Thomas D Cook",
        "William Thomas Campbell",
        "Shadish"
      ],
      "year": "2002",
      "venue": "Experimental and quasi-experimental designs for generalized causal inference"
    },
    {
      "citation_id": "11",
      "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
      "authors": [
        "Li Deng",
        "Geoffrey Hinton",
        "Brian Kingsbury"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "12",
      "title": "Methods for evaluating changes in health care policy: the difference-in-differences approach",
      "authors": [
        "Justin Dimick",
        "Andrew Ryan"
      ],
      "year": "2014",
      "venue": "Jama"
    },
    {
      "citation_id": "13",
      "title": "Pretest-posttest designs and measurement of change",
      "authors": [
        "M Dimiter",
        "Dimitrov",
        "D Phillip",
        "Rumrill"
      ],
      "year": "2003",
      "venue": "Work"
    },
    {
      "citation_id": "14",
      "title": "Inference with difference-in-differences and other panel data",
      "authors": [
        "G Stephen",
        "Kevin Donald",
        "Lang"
      ],
      "year": "2007",
      "venue": "The review of Economics and Statistics"
    },
    {
      "citation_id": "15",
      "title": "Will this paper increase your h-index? Scientific impact prediction",
      "authors": [
        "Yuxiao Dong",
        "Reid Johnson",
        "Nitesh V Chawla"
      ],
      "year": "2015",
      "venue": "Proceedings of the eighth ACM international conference on web search and data mining"
    },
    {
      "citation_id": "16",
      "title": "Matched control groups for modeling events in citation data: An illustration of Nobel Prize effects in citation networks",
      "authors": [
        "Rudolf Farys",
        "Tobias Wolbring"
      ],
      "year": "2017",
      "venue": "Journal of the Association for Information Science and Technology"
    },
    {
      "citation_id": "17",
      "title": "The evolution of citation graphs in artificial intelligence research",
      "authors": [
        "Dashun Morgan R Frank",
        "Manuel Wang",
        "Iyad Cebrian",
        "Rahwan"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "The impact of employment transitions on health in Germany. A difference-in-differences propensity score matching approach",
      "authors": [
        "Michael Gebel",
        "Jonas Vo√üemer"
      ],
      "year": "2014",
      "venue": "Social science & medicine"
    },
    {
      "citation_id": "19",
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "authors": [
        "Geoffrey Hinton",
        "Li Deng",
        "Dong Yu",
        "George Dahl",
        "Abdel-Rahman Mohamed",
        "Navdeep Jaitly",
        "Andrew Senior",
        "Vincent Vanhoucke",
        "Patrick Nguyen",
        "Tara Sainath"
      ],
      "year": "2012",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "20",
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "authors": [
        "Guolin Ke",
        "Qi Meng",
        "Thomas Finley",
        "Taifeng Wang",
        "Wei Chen",
        "Weidong Ma",
        "Qiwei Ye",
        "Tie-Yan Liu"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "The Estimation of Causal Effects by Difference-in-Difference Methods",
      "authors": [
        "Michael Lechner"
      ],
      "year": "2011",
      "venue": "Foundations and Trends(R) in Econometrics",
      "doi": "10.1561/0800000014"
    },
    {
      "citation_id": "23",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "24",
      "title": "Do Podcasts and Music Compete with One Another? Understanding Users' Audio Streaming Habits",
      "authors": [
        "Ang Li",
        "Alice Wang",
        "Zahra Nazari",
        "Praveen Chandar",
        "Benjamin Carterette"
      ],
      "year": "2020",
      "venue": "Proceedings of The Web Conference",
      "doi": "10.1145/3366423.3380260"
    },
    {
      "citation_id": "25",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st international conference on neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Cycling to school: Increasing secondary school enrollment for girls in India",
      "authors": [
        "Karthik Muralidharan",
        "Nishith Prakash"
      ],
      "year": "2017",
      "venue": "American Economic Journal: Applied Economics"
    },
    {
      "citation_id": "27",
      "title": "Citation count prediction as a link prediction problem",
      "authors": [
        "Nataliia Pobiedina",
        "Ryutaro Ichise"
      ],
      "year": "2016",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Explaining the predictions of any classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "29",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "30",
      "title": "AI 2000: A Decade of Artificial Intelligence",
      "authors": [
        "Zhou Shao",
        "Zhenting Shen",
        "Sha Yuan",
        "Jie Tang",
        "Yongli Wang",
        "Lili Wu",
        "Wenjiang Zheng"
      ],
      "year": "2020",
      "venue": "12th ACM Conference on Web Science",
      "doi": "10.1145/3394231.3397925"
    },
    {
      "citation_id": "31",
      "title": "A Web-scale system for scientific knowledge exploration",
      "authors": [
        "Zhihong Shen",
        "Hao Ma",
        "Kuansan Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of ACL 2018, System Demonstrations"
    },
    {
      "citation_id": "32",
      "title": "Quantifying the evolution of individual scientific impact",
      "authors": [
        "Roberta Sinatra",
        "Dashun Wang",
        "Pierre Deville",
        "Chaoming Song",
        "Albert-L√°szl√≥ Barab√°si"
      ],
      "year": "2016",
      "venue": "Science"
    },
    {
      "citation_id": "33",
      "title": "An Overview of Microsoft Academic Service (MAS) and Applications",
      "authors": [
        "Arnab Sinha",
        "Zhihong Shen",
        "Yang Song",
        "Hao Ma",
        "Darrin Eide",
        "Bo-June (paul) Hsu",
        "Kuansan Wang"
      ],
      "year": "2015",
      "venue": "Proceedings of the 24th International Conference on World Wide Web",
      "doi": "10.1145/2740908.2742839"
    },
    {
      "citation_id": "34",
      "title": "Population-Scale Study of Human Needs During the COVID-19 Pandemic: Analysis and Implications",
      "authors": [
        "Jina Suh",
        "Eric Horvitz",
        "Ryen White",
        "Tim Althoff"
      ],
      "year": "2021",
      "venue": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM '21)",
      "doi": "10.1145/3437963.3441788"
    },
    {
      "citation_id": "35",
      "title": "Quantifying the impact and relevance of scientific research",
      "authors": [
        "J William",
        "David Sutherland",
        "Simon Goulson",
        "Lynn Potts",
        "Dicks"
      ],
      "year": "2011",
      "venue": "PloS one"
    },
    {
      "citation_id": "36",
      "title": "A review of the literature on citation impact indicators",
      "authors": [
        "Ludo Waltman"
      ],
      "year": "2016",
      "venue": "Journal of informetrics"
    },
    {
      "citation_id": "37",
      "title": "Quantifying long-term scientific impact",
      "authors": [
        "Dashun Wang",
        "Chaoming Song",
        "Albert-L√°szl√≥ Barab√°si"
      ],
      "year": "2013",
      "venue": "Science"
    },
    {
      "citation_id": "38",
      "title": "Microsoft academic graph: When experts are not enough",
      "authors": [
        "Kuansan Wang",
        "Zhihong Shen",
        "Chiyuan Huang",
        "Chieh-Han Wu",
        "Yuxiao Dong",
        "Anshul Kanakia"
      ],
      "year": "2020",
      "venue": "Quantitative Science Studies"
    },
    {
      "citation_id": "39",
      "title": "A review of microsoft academic services for science of science studies",
      "authors": [
        "Kuansan Wang",
        "Zhihong Shen",
        "Chiyuan Huang",
        "Chieh-Han Wu",
        "Darrin Eide",
        "Yuxiao Dong",
        "Junjie Qian",
        "Anshul Kanakia",
        "Alvin Chen",
        "Richard Rogahn"
      ],
      "year": "2019",
      "venue": "Frontiers in Big Data"
    },
    {
      "citation_id": "40",
      "title": "Citation prediction in heterogeneous bibliographic networks",
      "authors": [
        "Xiao Yu",
        "Quanquan Gu",
        "Mianwei Zhou",
        "Jiawei Han"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 SIAM international conference on data mining"
    }
  ]
}