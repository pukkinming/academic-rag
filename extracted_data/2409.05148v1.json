{
  "paper_id": "2409.05148v1",
  "title": "Better Spanish Emotion Recognition In-The-Wild: Bringing Attention To Deep Spectrum Voice Analysis",
  "published": "2024-09-08T16:25:38Z",
  "authors": [
    "Elena Ortega-Beltrán",
    "Josep Cabacas-Maso",
    "Ismael Benito-Altamirano",
    "Carles Ventura"
  ],
  "keywords": [
    "Emotion recognition",
    "Paralinguistic",
    "Spanish",
    "Deep Spectrum",
    "Attention mechanisms"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Within the context of creating new Socially Assistive Robots, emotion recognition has become a key development factor, as it allows the robot to adapt to the user's emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage, e. g. the vocal characteristics that go along with the message and clarifies the meaning. We proposed the use of the DeepSpectrum method, which consists of extracting a visual representation of the audio tracks and feeding them to a pretrained CNN model. For the classification task, DeepSpectrum is often paired with a Support Vector Classifier -DS-SVC-, or a Fully-Connected deep-learning classifier -DS-FC-. We compared the results of the DS-SVC and DS-FC architectures with the state-of-the-art (SOTA) for ELRA-S0329 and EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon Attention Mechanisms, namely DS-AM. We trained all models against both datasets, and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM model in one dataset and tested it in the other, to simulate real-world conditions on how biased is the model to the dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "During the last decades, an increasing of age population has been observed in many countries, i. e. the USA  [16]  or Western Europe countries  [19] , this has let to the proposal for Socially Assistive Robots (SAR) to help elder people with day-to-day problems, but also aiming to help in with mental struggles related to age  [23] . Within this context, human emotion recognition becomes a key factor in the successful adoption of such technologies, and proposals have came around using artificial intelligence to solve this problem  [1] .\n\nIn this work, we focused on the analysis of voice-recorded data from Spanish speakers, as Spanish is one of the most spoken languages in Europe, South and North America and often does not receive the same attention from researchers than English. We selected two databases: (1) ELRA-S0329  [13] , a stock dataset from the European Language Resources Association which contains recordings of professional speakers in six emotions (anger, disgust, fear, joy, sadness, surprise) plus a neutral style in fast, slow, soft, loud and normal style, and (2) Emo-MatchSpanishDB  [9] , a dataset from the Universidad Europea de Madrid which contains recordings of 50 individuals expressing six different emotions (anger, disgust, fear, happiness, sadness, surprise) and a neutral one.\n\nWe proposed to follow the work of Amiriparian et al., using DeepSpectrum toolkit  [2]  to extract paralinguistic features from the audio data using a pipeline that consisted of:  (1)  converting audio data into a spectrum representation of this data -a Mel spectrogram-; (2) using pretrained a VGG16 CNN  [20]  as an image feature extractor; and, (3) using a Support Vector Classifier (SVC) to classify the emotions for each of the datasets. We named this architecture DeepSpectrum-SVC or, simply, DS-SVC (see Figure  1 ). We trained this architecture under a 10-fold cross-validation process and compared the results with the state-of-theart models for each dataset  [9, 13] . Additionally, we proposed to use a well-known technique to solve the classification task, using a Fully-Connected deep-learning classifier, which we named DeepSpectrum-FC or, simply, DS-FC. Moreover, we proposed our own classifier based on Attention Mechanisms, following the work of Gorriz et al.  [10] . We named this architecture DeepSpectrum-AM or, simply, DS-AM. We trained all models against both datasets and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures.\n\nFinally, we trained our DS-AM model in one dataset and tested it in the other, in order to simulate in-the-wild conditions where the audio samples are not only from unknown speakers but also with other acoustic conditions and different input texts.\n\nAll in all, the DS-SVC architecture performed better than its counterparts for both datasets; the DS-FC only outperformed SOTA models that used an SVC to solve the classification task; and, the DS-AM model outperformed all SOTA models for both datasets and the DS-SVC and DS-FC architectures that we also studied. The DS-AM model was trained in one dataset and tested in the other showed that the model is biased to the dataset, as it performed worse than the DS-AM model trained and tested in the same dataset. And as expected, the EmoMatchSpanishDB dataset, having 50 speakers instead of the 2 in ELRA-S0329, is more appropriate for in-the-wild emotion recognition.",
      "page_start": 1,
      "page_end": 13
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Speech Databases",
      "text": "Emotion recognition is a widely studied field that comprises several disciplines, such as psychology or linguistics, and in recent decades, also computer science. Normally, speech databases introduce labels to classify or quantify the emotions expressed in the recordings. These labels can be discrete, such as the ones proposed by Ekman  [8] , which consists on several emotions, i. e. \"anger\" or \"joy\"; or continuous, such as the valance-arousal system  [5] , which consists on two dimensions that describe the emotional state of the speaker.\n\nIn addition to this, speech databases distinguish themselves by the way the recordings are obtained. We can divide them in three categories:\n\n-Acted or Simulated: These databases are recorded by professional actors in a recording studio. Although they usually have very good audio quality, they are not so realistic or useful as real ones. Examples are: EmoDB  [6] , IEMOCAP  [7]  Spanish Expressive Voices Corpus Description  [4] , ELRA-S0329  [13]  or EmoMatchSpanishDB  [9] . -Induced: There are created placing the actors in simulated situations to produce the requested emotion. They are more useful than the acted ones, as they are more close to real situations. An example is eNTERFACE'05  [15] . -Natural: They are obtained from real situations as talk-shows, social media or call centers. Although the more useful of all, they have legal and ethical issues to use them. Examples are RECOLA Speech Database.  [17]  or CMU-MOSEAS  [24] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Acoustic Characteristics",
      "text": "From a phonological point-of-view, we can distinguish between segmental and suprasegmental characteristics in the study of audio tracks. On one hand, segmental characteristics are tied to a limited window of time within an utterance (such as a phoneme, approximately 20-30ms). The most common characteristics are those associated with the cepstrum, which consists of a transformation of the spectrum that allows us to discover periodic features in the frequency domain.\n\nFor example, the MFCC (Mel Frequency Cepstral Coefficients) or the LPCC (Linear Prediction Cepstral Coefficients) belong to this category, and their definition can be found in the common literature  [21] .\n\nOn the other hand, the suprasegmentals characteristics are tied to the entire utterance as a whole, like speech rate, shimmer or jitter. Often, these characteristics are evaluated in much larger windows of time, these characteristics are called Low-Level Descriptors (LLD), a widely spread standard for these features is the ComParE  [18]  feature set, composed of 6373 static features derived from low-level descriptors (LLD). Also, another widespread feature set is the MSFs (Modulation Spectral Features  [22] ), which are processed through a set of filters that simulate the human auditory system.\n\nThe usual approach from authors to machine learning solutions is: first, to implement any sort of these above-mentioned classical feature extraction method and, then, use a classifier to solve the classification task. The classifier can be as simple as a Support Vector Classifier (SVC), or more complex, like a Recurrent Neural Network (RNN).\n\nFor the ELRA-S0329 dataset, Kerkeni et al.  [13]  used two different feature extractors: MSFs and MFCC; and they combined both, obtaining three different feature sets (MSF, MFCC and MSF+MFCC). Regarding the classifier task, they introduced three different classifiers, first a Multi-Linear Regression (MLR), then a Support Vector Machine classifier (SVC) and finally a Recurrent Neural Network (RNN). Table  1  shows the results obtained by these authors. For the EmoMatchSpanishDB dataset, García-Cuesta et al.  [9]  used the wellknown ComParE feature set for audio preprocessing, they also introduced the EgeMaps [14] feature set. Regarding the classifier task, they three algorithms: a Support Vector Machine Classifier (SVC), a XGBOOST classifier and a Feed-Foward Neural Network (FFNN). Table  2  shows the results obtained by these authors. It supports widely more than 10 different backbone models, such as VGG16, ResNet, DenseNet, Inception, etc. Plus, it allows to select from which convolutional layer the features will be extracted. Moreover, other parameters can be selected, like: window and hopsize for feature extraction, length of fft window used for creating the spectograms, frequency scales and limits, color map of the plots, etc.  [3] .\n\nIn the original deep spectrum analysis, Amiriparian et al. used a Supported Vector Machine classifier to solve the classification task after extracting the features with the CNN, this is depicted at Figure  2 . In other related works, such as in JAafar and Lachiri  [12] , were authors used a Fully-Connected deep-learning classifier to solve the classification task, this is depicted at Figure  3 . Note this was also considered in the original DeepSpectrum work but only as a future work possibility  [2] . 3 Materials and Methods",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "Two datasets were considered in this study, both of them are in Spanish language:\n\n-ELRA-S0329  [13]  was presented on 2018 and consists on the recordings of one female and one male professional speakers in six emotions (anger, disgust, fear, joy, sadness, surprise) plus a neutral style in fast, slow, soft, loud and normal style. It contains 6041 archives with a total recording time of 7 hours and 52 minutes. In the original work, authors trained the dataset using a 10-cross validation scheme, first they splitted up the folds and later, they created a 70% split for training and 30% for testing. In order to compare to the state-of-the-art, we will use the same methodology, for this dataset.\n\nThe best baseline obtained by Kerkeni et al. was a RNN with MFCC and MS features, with an accuracy of 90.05%  [13] , as is described in Table  1 . -EmoMatchSpanishDB  [9]  was presented on 2023 and contains the recordings of 50 individuals (31 male and 19 female) with a total of 2050 archives expressing six different emotions (anger, disgust, fear, happiness, sadness, surprise) and a neutral one. In the original work, authors introduced a cross validation scheme named \"Leave-One-Speaker-Out\" (LOSO), where they used 45 speakers for training and 5 speakers for testing. The best baseline obtained by García-Cuesta et al. was a SVC (or XGBOOST) classifier with the ComParE feature set, with an accuracy of 64.2%  [9] , as is described in Table  2 .\n\nThere exists a noticeable difference in the baseline accuracy despite the similarities in the feature extraction pipelines. This is somehow to be expected, as the EmoMatchSpanishDB dataset has 50 speakers instead of the 2 in ELRA-S0329. Nevertheless, we understood this distinction as an opportunity to test the robustness of our models in different conditions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Proposed Method",
      "text": "We proposed to use our own variant of a DeepSpectrum pipeline, this approximation consists on bringing attention mechanisms to the DeepSpectrum architecture. Instead of using the entire back-bone CNN to extract the image features from the Mel spectrogram, we modified a VGG-16 CNN to include two attention mechanisms, following the work of Gorriz et al.  [10] , who used this architecture to evaluate the severity of knee osteoarthritis processing X-Ray images. We named this architecture DeepSpectrum-AM or, simply, DS-AM. The attention mechanisms were added to the last two convolutional blocks of the VGG-16, as shown in Figure  4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Design",
      "text": "As the first experiment, we proposed a transfer learning solution to the classification task by using the pretrained VGG-16 CNN in the DeepSpectrum toolkit (from Keras stock library -where VGG-16 was trained over ImageNet [3]-) to extract the features from the Mel spectrograms. Then we used the well-known SVM classifier, as proposed in the original work of DeepSpectrum  [2] . We named the architecture for this experiment DeepSpectrum-SVC or, simply, DS-SVC (see Figure  2 ). Note that we used a VGG-16 CNN as the backbone, as it is the same architecture used in the further experiments, specially in the case of the addition of attention mechanisms.\n\nAs the second experiment, we proposed a fine-tuning solution for the feature extraction step paired with a fully-connected deep-learning. For the finetuning, we obtained the same pretrained stock network -VGG-16 trained on ImageNet from Keras-and we unfroze all layers with a low-learning rate. We expected this approximation to better handle the specificity of our datasets, as our images are Mel spectograms, not general images. We named this architecture DeepSpectrum-FC or, simply, DS-FC (see Figure  3 ).\n\nAs third experiment, we proposed a step further in network configuration, by taking a modified version of the VGG-16 network with two attention mechanisms, following the work of Gorriz et al.  [10] . As we took the VGG-16 from the one pretrained in Keras, we used all the pretrained weights for the firsts layers of the extractor, but start using new untrained values for the attention heads. We named this architecture DeepSpectrum-Attention Mechanisms or, simply, DS-AM.\n\nFor experiments 1, 2 and 3, for ELRA-S0329, we have grouped all the five neutral labelled emotions in only one category, as we consider that the neutral emotion contained in EmoMatchSpanishDB only represents the normal category and doesn't contain the wide range that ELRA-S0329 has. We trained over both datasets, ELRA-S0329  [13]  and EmoMatchSpanishDB  [9] , following the partitions that most resembled the original works. This means that for both datasets we used a 10-fold cross-validation process, as described in the original works. Despite this, notice that this means that the folds for the ELRA-S0329 dataset are constituted by different instances from the two same speakers, and the folds for the EmoMatchSpanishDB dataset are constituted by different instances from the 50 speakers. In this case, as we have a total of 50 speakers, we used 45 speakers for training and 5 speakers for validation.\n\nAs the fourth experiment, we trained the best model obtained with each of the datasets and we tested its performance in the other dataset, we did not use the cross-validation here. Doing so, we simulated an in-the-wild scenario where our task should be applied, being the audio samples not only from unknown speakers but also with other acoustic conditions and different input texts. For this experiment, for ELRA-S0329, we used only the normal-neutral labeled samples for testing purposes, although we maintained the whole set of five neutral emotions for training. We considered that the neutral emotion contained in Emo-MatchSpanishDB only represents the normal category and doesn't contain the wide range that ELRA-S0329 has.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "Table  3  shows the results of training DeepSpectrum-SVC over both datasets, ELRA-S0329 and EmoMatchSpanishDB, and compares them with the state-ofthe-art models for each dataset regarding machine-learning classic classifiers. It can be shown that our model performed similarly to the SOTA models for the ELRA-S0329 dataset, but it was outperformed by the SOTA models for the EmoMatchSpanishDB dataset.\n\nTable  3 : Comparison between the SOTA models for machine-learning classification solvers: MRL and SVC for ELRA-S0329 dataset  [13] ; and SVC and XGBOOST for EmoMatchSpanishDB dataset  [9] . Our model using a pretrained VGG-16 CNN as feature extractor and a SVC as classifier is also shown, which scores similar to SOTA for the ELRA-S0329 dataset. Table  4  show the results of training DeepSpectrum-FC and DeepSpectrum-AM over both datasets, ELRA-S0329 and EmoMatchSpanishDB -experiments 2 and 3-, and compares them with the state-of-the-art models for each dataset regarding deep-learning classifiers. It can be shown that our model outperformed the SOTA models for both datasets and the SOTA DeepSpectrum architectures. Specially for the EmoMatchSpanishDB dataset, can be seen that our deep-learning models surpassed the XGBOOST classifier, which was the best model in the original work, see also Table  3 .\n\nTable  4 : Comparison between the SOTA models for deep-leaning classification solvers: RNN for ELRA-S0329 dataset  [13] ; FFNN for EmoMatchSpanishDB dataset  [9] ; and our model using a pretrained VGG-16 CNN as feature extractor and: a Fully-Connected deep-learning classifier (DS-FC) and a CNN with two attention mechanisms (DS-AM). Additional results are shown in Table  5  where a detailed classification report is shown for the ELRA-S0329 dataset and DeepSpectrum-AM model. Table  6  shows the same for the EmoMatchSpanishDB dataset and DeepSpectrum-AM model. For this architecture, also a detailed view of the confusion matrix for both datasets is shown in Figure  5 . Results for experiment 4 are shown in Table  7 . It can be seen that the model trained in ELRA-S0329 and tested in EmoMatchSpanishDB performed poorly, while the model trained in EmoMatchSpanishDB and tested in ELRA-S0329 performed better. This is also shown in the confusion matrices in Figure  6 . This is considered a good result, and to be expected. As training in EmoMatchSpan-ishDB, the model has to learn to generalize the features of the emotions, and testing on in-the-wild scenario -ELRA-S0329-the model has to learn to generalize the features of the speakers.   Additional results for the in-the-wild experiment can be found in the Table  8  and Table  9 , where the confusion matrices for both crossed experiments can be found in Figure  6 . Finally, a detailed report for precision, recall and F1-score for both experiments can be found in Table  8  and Table  9 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "For ELRA-S0329 the three models proposed outperform the current state-ofthe-art models. If we compare DeepSpectrum-VGG16 MSF-SVC, we obtain an accuracy increase of 4,51%. and DeepSpectrum-FN and DeepSpectrum-AM obtain an accuracy increase of 7,43% and 8,33% versus RNN (MFCC + MS). As the performance is so outstanding, the classification report gives us not any new information.\n\nFor EmoMatchSpanishDB, only two of the models outperform the current state-of-the-art. DeepSpectrum-FN and DeepSpectrum-AM obtain an accuracy increase of 1,7% and 4,1% versus SVC. In the confusion matrix and the classification report we can observe that for emotions like anger or fear the system has both a high precision and recall, while for emotions like joy the performance is not so good.\n\nIn the cross-datasets experiments, we see that the model trained in Emo-MatchSpanishDB outperforms the model trained in ELRA-S0329 in a 5,88%. This was expected as EmoMatchSpanishDB, having 50 speakers instead of the 2 in ELRA-S0329, is more appropriate for in-the-wild emotion recognition.\n\nIf we analyze the confusion matrix in both cases, we see the model trained in ELRA-S039 has the highest performance for neutral labelled samples, not surprising considering the five subcategories present in the dataset. Instead, in the model trained in EmoMatchSpanishDB, this happens with emotions like Surprise and Anger.\n\nFinally, we can affirm that DeepSpectrum has proven its usefulness in the field of emotion recognition. It allows us to apply image recognition developments and to do transfer learning to an audio recognition problem, furthering the available ways of improvement in the research field.\n\nFuture lines of research should be directed to, in one hand, increasing the number of available datasets to do research, taking into account that languages like Spanish have a large number of speakers but a not so large number of datasets, compared to English. On the other hand, Generative Adversarial Networks (GANs)  [11]  and the addition of other audio parameters like MSFs  [22]  to the system could increase the performance of it.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). We trained this architecture under a",
      "page": 2
    },
    {
      "caption": "Figure 1: Schematic of a DeepSpectrum pipeline for audio classification. The audio signal",
      "page": 2
    },
    {
      "caption": "Figure 2: Fig. 2: A typical DeepSpectrum-SVC (DS-SVC) pipeline for audio classification. The",
      "page": 5
    },
    {
      "caption": "Figure 3: Note this was also considered in the original DeepSpectrum",
      "page": 5
    },
    {
      "caption": "Figure 3: Another configuration of DeepSpectrum, DeepSpectrum-FC (DS-FN) where the",
      "page": 6
    },
    {
      "caption": "Figure 4: The DeepSpectrum-Attention Mechanisms (DS-AM) architecture. The figure",
      "page": 7
    },
    {
      "caption": "Figure 2: ). Note that we used a VGG-16 CNN as the backbone, as it is the",
      "page": 8
    },
    {
      "caption": "Figure 5: Table 5: Classification report for DeepSpectrum-AM model trained over the ELRA-",
      "page": 10
    },
    {
      "caption": "Figure 5: Confusion matrix for DeepSpectrum-AM over a) ELRA-S0329 dataset; and, b)",
      "page": 11
    },
    {
      "caption": "Figure 6: Finally, a detailed report for precision, recall and F1-score for",
      "page": 11
    },
    {
      "caption": "Figure 6: Confusion matrix for DeepSpectrum-AM model: a) training in EmoMatchSpan-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the results obtained by these authors.",
      "page": 4
    },
    {
      "caption": "Table 1: Accuracy results obtained for MLR, SVC, and RNN at ELRA-S0329 (MSF,",
      "page": 4
    },
    {
      "caption": "Table 2: shows the results obtained by these",
      "page": 4
    },
    {
      "caption": "Table 2: F1-Score and accuracy results obtained for SVC, XGBOOST, and FFNN at",
      "page": 4
    },
    {
      "caption": "Table 1: – EmoMatchSpanishDB [9] was presented on 2023 and contains the record-",
      "page": 6
    },
    {
      "caption": "Table 2: There exists a noticeable difference in the baseline accuracy despite the sim-",
      "page": 6
    },
    {
      "caption": "Table 3: shows the results of training DeepSpectrum-SVC over both datasets,",
      "page": 9
    },
    {
      "caption": "Table 3: Comparison between the SOTA models for machine-learning classification",
      "page": 9
    },
    {
      "caption": "Table 4: show the results of training DeepSpectrum-FC and DeepSpectrum-",
      "page": 9
    },
    {
      "caption": "Table 4: Comparison between the SOTA models for deep-leaning classification solvers:",
      "page": 10
    },
    {
      "caption": "Table 5: where a detailed classification report",
      "page": 10
    },
    {
      "caption": "Table 6: shows the same for the EmoMatchSpanishDB dataset and DeepSpectrum-AM",
      "page": 10
    },
    {
      "caption": "Table 5: Classification report for DeepSpectrum-AM model trained over the ELRA-",
      "page": 10
    },
    {
      "caption": "Table 7: It can be seen that the model",
      "page": 10
    },
    {
      "caption": "Table 6: Classification report for DeepSpectrum-AM model trained over the Emo-",
      "page": 11
    },
    {
      "caption": "Table 7: Accuracy comparison between training and testing in ELRA-S0329 and Emo-",
      "page": 11
    },
    {
      "caption": "Table 8: and Table 9, where the confusion matrices for both crossed experiments can be",
      "page": 11
    },
    {
      "caption": "Table 8: and Table 9.",
      "page": 11
    },
    {
      "caption": "Table 8: Classification report training ELRA-S0329 and testing EmoMatchSpanishDB",
      "page": 12
    },
    {
      "caption": "Table 9: Classification report training EmoMatchSpanishDB and testing ELRA-S0329",
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Artificial emotional intelligence in socially assistive robots for older adults: A pilot study",
      "authors": [
        "H Abdollahi",
        "M Mahoor",
        "R Zandie",
        "J Siewierski",
        "S Qualls"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3143803"
    },
    {
      "citation_id": "2",
      "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "ISCA",
      "doi": "10.21437/Interspeech.2017-434"
    },
    {
      "citation_id": "3",
      "title": "Deepspectrum",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deepspectrum"
    },
    {
      "citation_id": "4",
      "title": "Spanish expressive voices: corpus for emotion research in spanish",
      "authors": [
        "R Barra Chicote",
        "J Montero Martínez",
        "J Macías Guarasa",
        "S Lutfi",
        "J Lucas Cuesta",
        "F Fernández Martínez",
        "D'haro",
        "L Enríquez",
        "R San Segundo Hernández",
        "J Ferreiros López",
        "R Córdoba Herralde",
        "J Pardo Muñoz"
      ],
      "year": "2008",
      "venue": "Spanish expressive voices: corpus for emotion research in spanish"
    },
    {
      "citation_id": "5",
      "title": "Effects of emotional valence and arousal on the voice perception network",
      "authors": [
        "P Bestelmeyer",
        "S Kotz",
        "P Belin"
      ],
      "year": "2017",
      "venue": "Social Cognitive and Affective Neuroscience",
      "doi": "10.1093/scan/nsx059"
    },
    {
      "citation_id": "6",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Emotional and conversational nonverbal signals",
      "authors": [
        "P Ekman"
      ],
      "year": "2004",
      "venue": "Language, knowledge, and representation: Proceedings of the sixth international colloquium on cognitive science (ICCS-99)"
    },
    {
      "citation_id": "9",
      "title": "EmoMatchSpanishDB: study of speech emotion recognition machine learning models in a new spanish elicited database",
      "authors": [
        "E Garcia-Cuesta",
        "A Salvador",
        "D Pãez"
      ],
      "venue": "EmoMatchSpanishDB: study of speech emotion recognition machine learning models in a new spanish elicited database",
      "doi": "10.1007/s11042-023-15959-w"
    },
    {
      "citation_id": "10",
      "title": "Assessing knee oa severity with cnn attention-based end-to-end architectures",
      "authors": [
        "M Górriz",
        "J Antony",
        "K Mcguinness",
        "X Giró-I Nieto",
        "N O'connor"
      ],
      "year": "2019",
      "venue": "International Conference on Medical Imaging with Deep Learning"
    },
    {
      "citation_id": "11",
      "title": "Gantron: Emotional speech synthesis with generative adversarial networks",
      "authors": [
        "E Hortal",
        "R Brechard Alarcia"
      ],
      "year": "2021",
      "venue": "Gantron: Emotional speech synthesis with generative adversarial networks"
    },
    {
      "citation_id": "12",
      "title": "Stress recognition from speech by combining image-based deep spectrum and text-based features",
      "authors": [
        "N Jaafar",
        "Z Lachiri"
      ],
      "year": "2022",
      "venue": "2022 IEEE Information Technologies & Smart Industrial Systems (ITSIS)"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition: Methods and cases study",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "M Mbarki",
        "K Raoof",
        "M Mahjoub"
      ],
      "venue": "Proceedings of the 10th International Conference on Agents and Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Emotion identification using extremely low frequency components of speech feature contours",
      "authors": [
        "C Lin",
        "W Liao",
        "W Hsieh",
        "W Liao",
        "J Wang"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "16",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Kotsia",
        "I Macq",
        "B Pitas"
      ],
      "year": "2006",
      "venue": "The enterface'05 audio-visual emotion database",
      "doi": "10.1109/ICDEW.2006.145"
    },
    {
      "citation_id": "17",
      "title": "The changing age distribution of the united states",
      "authors": [
        "S Preston",
        "Y Vierboom"
      ],
      "year": "2021",
      "venue": "Population and Development Review"
    },
    {
      "citation_id": "18",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "19",
      "title": "The acm multimedia 2023 computational paralinguistics challenge: Emotion share & requests",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Amiriparian",
        "A Barnhill",
        "M Gerczuk",
        "A Triantafyllopoulos",
        "A Baird",
        "P Tzirakis",
        "C Gagne",
        "A Cowen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "A retrospective and prospective view of current and future population ageing in the european union 28 countries",
      "authors": [
        "L Šídlo",
        "B Šprocha",
        "P Ďurček"
      ],
      "year": "2020",
      "venue": "Moravian geographical reports"
    },
    {
      "citation_id": "21",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "22",
      "title": "Digital Speech Transmission and Enhancement",
      "authors": [
        "P Vary",
        "R Martin"
      ],
      "year": "2023",
      "venue": "Digital Speech Transmission and Enhancement"
    },
    {
      "citation_id": "23",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "venue": "Automatic speech emotion recognition using modulation spectral features",
      "doi": "10.1016/j.specom.2010.08.013"
    },
    {
      "citation_id": "24",
      "title": "Socially assistive robots for people with dementia: systematic review and meta-analysis of feasibility, acceptability and the effect on cognition, neuropsychiatric symptoms and quality of life",
      "authors": [
        "C Yu",
        "A Sommerlad",
        "L Sakure",
        "G Livingston"
      ],
      "year": "2022",
      "venue": "Ageing research reviews"
    },
    {
      "citation_id": "25",
      "title": "Cmumoseas: A multimodal language dataset for spanish, portuguese, german and french",
      "authors": [
        "A Zadeh",
        "Y Cao",
        "S Hessner",
        "P Liang",
        "S Poria",
        "L Morency"
      ],
      "year": "2020",
      "venue": "Cmumoseas: A multimodal language dataset for spanish, portuguese, german and french",
      "doi": "10.18653/v1/2020.emnlp-main.141"
    }
  ]
}