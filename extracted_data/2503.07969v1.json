{
  "paper_id": "2503.07969v1",
  "title": "7Abaw-Compound Expression Recognition Via Curriculum Learning",
  "published": "2025-03-11T01:53:34Z",
  "authors": [
    "Chen Liu",
    "Feng Qiu",
    "Wei Zhang",
    "Lincheng Li",
    "Dadong Wang",
    "Xin Yu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the advent of deep learning, expression recognition has made significant advancements. However, due to the limited availability of annotated compound expression datasets and the subtle variations of compound expressions, Compound Emotion Recognition (CE) still holds considerable potential for exploration. To advance this task, the 7th Affective Behavior Analysis in-the-wild (ABAW) competition introduces the Compound Expression Challenge based on C-EXPR-DB, a limited dataset without labels. In this paper, we present a curriculum learningbased framework that initially trains the model on single-expression tasks and subsequently incorporates multi-expression data. This design ensures that our model first masters the fundamental features of basic expressions before being exposed to the complexities of compound emotions. Specifically, our designs can be summarized as follows: 1) Single-Expression Pre-training: The model is first trained on datasets containing single expressions to learn the foundational facial features associated with basic emotions. 2) Dynamic Compound Expression Generation: Given the scarcity of annotated compound expression datasets, we employ CutMix and Mixup techniques on the original single-expression images to create hybrid images exhibiting characteristics of multiple basic emotions. 3) Incremental Multi-Expression Integration: After performing well on single-expression tasks, the model is progressively exposed to multi-expression data, allowing the model to adapt to the complexity and variability of compound expressions. The official results indicate that our method achieves the best performance in this competition track with an F-score of 0.6063. Our code is released at this repository.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The Compound Expression (CE) Recognition task focuses on identifying complex emotional states conveyed by facial expressions that are combinations of ⋆ * Equal Contribution, † Corresponding authors. This work is done at Netease. arXiv:2503.07969v1 [cs.CV] 11 Mar 2025 basic emotions. Unlike recognizing single, basic emotions such as happiness, sadness, or anger, compound expressions involve more nuanced and mixed emotions, such as happily surprised, sadly angry, or fearfully disgusted. These compound expressions provide a richer and more accurate representation of human affective states  [10-18, 26, 31, 37, 45, 53, 56-58] .\n\nSingle-expression recognition has seen significant advancements with the advent of deep learning, which enables more accurate and efficient recognition of basic emotional states. However, understanding compound expressions still faces great challenges. We categorize several challenges inherently of this task as follows: a) Limited datasets. The annotated compound expression data is very rare, posing a challenge for training robust models. b) Subtle Differences of Compound Expressions. The compound expression usually contains subtle variations in facial features. Hence, they are more difficult to distinguish than basic emotions. To facilitate the development of this field, the 7th Affective Behavior Analysis competition (ABAW7)  [19]  set the Compound Expression (CE)  [3, 7, 42, 43]  Challenge based on the C-EXPR-DB  [9]  dataset. Participations are required to achieve compound expression recognition in videos with limited amounts and unknown labels  [3, 7, 42, 43] .\n\nTo solve the above challenges, we introduce a curriculum learning framework, which enhances the generalization ability of our model in compound expression recognition tasks by gradually transitioning from basic expression to multiexpression learning. More specifically, we first train our model on the datasets just containing single expressions. In this fashion, our model learns the foundational facial features associated with basic emotions. After achieving satisfactory performance on the single-expression recognition task, we expose the model to data with multiple expressions progressively. This stepwise introduction allows the model to adapt to the increased complexity and variability inherent in compound expressions.\n\nTo address the scarcity of annotated compound expression datasets, we apply CutMix and Mixup to the original single-expression images, creating hybrid images that exhibit characteristics of multiple basic emotions. The CutMix technique involves cutting and pasting patches from different images, while Mixup generates linear interpolations between pairs of images. These augmented images are then used to train the model, providing it with a rich and varied training set that includes a wide range of compound expressions. We conduct comprehensive evaluations on the officially provided dataset and our self-curated validation dataset. The results demonstrate the effectiveness of our method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Compared to compound expression recognition, basic expression recognition has achieved many advancements in recent years  [1, 4, 8, 51] . Single-expression recognition has been a foundational area of research in the field of affective computing and computer vision  [24, 25, 34, 35, 59] . Early methods rely heavily on handcrafted features such as Local Binary Patterns (LBP)  [32] , Histogram of Oriented Gra-dients (HOG)  [2] , and Gabor filters  [30]  to capture the distinct characteristics of facial expressions. These traditional techniques, though effective to some extent, faced limitations in handling variations in lighting, pose, and occlusion.\n\nWith the advent of deep learning, particularly convolutional neural networks (CNNs), the performance of single expression recognition systems has seen significant improvements  [46] [47] [48] [49] [50] . CNNs automatically learn hierarchical feature representations from raw pixel data, leading to more robust and accurate recognition. Hasani and Mahoor  [5]  employ deep residual networks for CER, while Kosti et al.  [20]  use LSTM networks to capture temporal dynamics in video sequences. Multi-task learning (MTL) frameworks, as explored by Zhang et al.  [55] , have also enhanced CER by leveraging shared feature extractors with task-specific heads.\n\nFurther advancements were made by incorporating transfer learning and finetuning pre-trained models on large-scale face datasets. This approach leveraged the generalization capabilities of models trained on extensive datasets, such as VGG-Face  [33]  and ResNet  [21] , to enhance the performance of expression recognition tasks. Additionally, researchers have explored the integration of attention mechanisms and ensemble learning to refine the focus on critical facial regions and combine the strengths of multiple models  [39, 41, 59] , respectively. However, given the complexity of human emotions in real-world situations, detecting a single expression is inadequate.\n\nCompound Expression Recognition (CER) extends traditional emotion recognition by identifying complex, blended expressions. Early efforts by  [3, 7]  systematically categorized compound emotions, laying the groundwork for further studies  [46] [47] [48] [49] [50] . Dimitrios  [9]  curate the Multi-Label Compound Expression dataset, C-EXPR, and introduced C-EXPR-NET, which simultaneously tackles Compound Expression Recognition (CER) and Action Unit (AU) detection tasks. Despite progress, challenges remain due to the subtlety and variability of compound expressions, highlighting the requirement for more sophisticated models and diverse datasets.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this work, we employ Masked Autoencoder (MAE)  [6]  as our feature extractor. In this section, we present our approach for the two competition tracks in three parts. First, we will describe the construction process of the feature extractor utilized in this challenge. Then we detail our framework flow (as depicted in Fig.  1 ) for the Compound Expression Recognition (CER) challenge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method Overview",
      "text": "In our approach, we begin by training the model on single expression recognition tasks. This involves learning to identify basic emotions such as Neutral, Anger, Disgust, Fear, Happiness, Sadness, and Surprise. Once the model achieves satisfactory performance on the single expression recognition task, we introduce",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Inference Stage",
      "text": "Fig.  1 : Illustration of our proposed frameworks for the compound recognition competition. We adopt a curriculum learning approach, transitioning from basic expression prediction to compound expression learning. We take the second stage training process as an example to illustrate the transition process. Different from the first training stage only utilizes the basic expression data to train the model, in this second stage, we randomly select the compound expression data from the natural compound expression datasets (i.e. RAD-BF and Fuxi-EXPR ) and the generated compound expression data. Specifically, there are 80% basic expression images and 20% compound expression images involved in the second training stage. Here, cls. Head refers to the classification head, which is composed of linear layers.\n\ncompound expression recognition. This progressive training strategy decreases the difficulty of tasks. It allows the model to adapt to the more complex task of identifying blended or compound expressions. Moreover, in the second learning stage, we utilize Cutmix  [52]  and Mixup  [54]  data augmentation techniques to generate compound expressions. This increases dataset diversity, enhancing the robustness and generalization of our model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extractor",
      "text": "In this competition, we employ Masked AutoEncoder as our feature extractor due to its powerful feature extraction capabilities. To obtain high-quality facial features, we first integrate a large-scale facial dataset, including AffectNet  [29] , CASIA-WebFace  [44] , CelebA  [28] , IMDB-WIKI  [38] , and WebFace260M  [60] .\n\nThe data undergoes further cleaning by the annotation platform to ensure its quality, and ultimately, 4.5 million images are used in the training process. Based on this dataset, we train the MAE in a self-supervised manner. Specifically, we randomly mask 75% of each facial image and then reconstruct the original face image. During the training stage, we utilize L2 Loss to calculate the difference between the reconstructed image and the original image for model optimization.\n\nTo better adapt the MAE model to the affective behavior analysis, we fine-tuned the model on the AffectNet and data from Aff-Wild provided by the competition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Single Expression Recognition",
      "text": "To attain the high-quality feature extractor, we first finetune the MAE model on RAF-DB and our curated Fuxi-EXPR dataset (including 25K images with seven basic expressions as RAF-DB  [22] ). Note that, in this stage, we just utilize the data with the single expression label to train the model, as shown in Fig.  1 . To increase the diversity of the training dataset, we adopt the cutout data augmentation technique as well as the basic data augmentation techniques such as random flipping, color jittering, and random crop in the initial learning stage. Note that considering the final compound expressions do not include the neutral label, we filter out all data with the neutral label from the entire dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Compound Expression Recognition",
      "text": "As the model's performance stabilizes, we progressively introduce more complex compound expressions to train our model. To reduce the risk of overwhelming the model with difficult data too early, we progressively increase the proportion of compound expressions in the training data. Specifically, we divide the overall training stage into four sub-stages, with the number of training epochs for each stage being 5, 5, 3, and 3. The proportion of compound expressions in each stage is 0, 0.2, 0.4, and 1, respectively. During the training phase, we treat the compound expression recognition task as a multi-class classification problem. To achieve better performance, during the inference phase, we constrain the final results into the officially provided compound expression categories. Specifically, we sum the classification probabilities of the individual expression categories involved in the compound expressions and utilize the argmax of these summed probabilities as the final result.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Objectives",
      "text": "In the whole training stage, we utilize Binary Cross-Entropy Multi-Label Loss as the optimization objective, mathematically expressed by:\n\nHere, N is the number of samples, C is the number of categories, y ic is the binary indicator (0 or 1) if class label c is the correct classification for sample i, and p ic is the predicted probability of sample i being of class c.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "Official Provided Data. For the CE track, the organizers provide 56 video data from the C-EXPR-DB  [9]  database. These videos are categorized into the following categories: Fearfully Surprised, Happily Surprised, Sadly Surprised, Disgustedly Surprised, Angrily Surprised, Sadly Fearful, and Sadly Angry. Note that we do not have the ground truth labels for these videos.\n\nExtra Data for CE. Except for the official provided data, we enhance the dataset with RAF-DB  [22]  and AffectNet  [29]  to increase the data of Anger, Disgust, and Fear expressions. Fuxi-EXPR contains 71,618 facial images collected from films, television works, and public video platforms. Data cleaning and management are handled by the Netease Fuxi Youling Crowdsourcing platform 2  . The dataset includes labels for six basic emotions (Surprised, Fear, Disgust, Happiness, Sadness, Anger) and an additional category for Others. To obtain stable performance, we retain only the six basic emotions for the two datasets.\n\nExtra Data for MAE. To attain a powerful expression feature extractor, we train MAE on our re-collected facial image dataset, which includes AffectNet  [29] , CASIA-WebFace  [44] , CelebA  [28] , IMDB-WIKI  [38] , WebFace260M  [60] , and the Fuxi-EXPR private dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Metrics",
      "text": "For the CE track, the performance P is measured by the average F1 score across all seven categories, expressed by:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "All training images are resized to 224 × 224 pixels. The MAE pertaining is conducted on 8 NVIDIA A30 GPUs with a batch size of 4096. We conduct the compound expression recognition experiments on 4 NVIDIA 4090 with a batch size of 256. The training configurations (including the batch size, optimizer, scheduler, learning rate, and so on) can be found in our released code. Moreover, we divide the training into three stages, with the number of epochs for each stage being 5, 5, 3, and 3. The proportion of compound expression images in each stage was 0, 0.2, 0.4, and 1, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "Analysis of Curriculum Learning. We evaluate our CE recognition model with different epoch distributions and compound data proportion distributions, and the results are presented in Table  2 . As indicated in Table  2 , the best performance is achieved under the Epoch distribution of  [5, 5, 3, 3]  and Compound Evaluation on Official Test Data. Table  3  presents the results of all participating teams' approaches on the official test set. The results suggested that our approach achieves a significant victory in the compound expression track, outperforming the second place by 0.282. This also demonstrates that our curriculum learning approach, progressing from simple to complex, facilitates the model to gradually understand and master basic facial features, thereby performing better in handling complex multi-expression tasks. Discussion. Although our curriculum learning-based method attains competitive results, there are still many challenges that remain to be addressed. One of the critical challenges in compound expression recognition is the scarcity of annotated datasets. To overcome this, our method includes dynamic compound expression generation techniques such as CutMix and Mixup, which synthesize new training data by combining single-expression images. This approach not only expands the training dataset but also introduces variability, which is crucial for training a model capable of handling the diversity of compound expressions. Moreover, integrating multimodal data, such as audio, physiological signals, and contextual information, could further enhance the accuracy and reliability of compound expression recognition systems. This multimodal approach would provide a more holistic understanding of human emotions, leading to applications that can better interpret and respond to human affective states in real time.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) for the Compound Expression Recognition (CER) challenge.",
      "page": 3
    },
    {
      "caption": "Figure 1: Illustration of our proposed frameworks for the compound recogni-",
      "page": 4
    },
    {
      "caption": "Figure 1: To increase the diversity of the training dataset, we adopt the cutout data",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 CSIRO, Data61, Sydney, Australia": "Dadong.Wang@data61.csiro.au ⋆"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "Abstract. With the advent of deep learning, expression recognition has"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "made significant advancements. However, due to the limited availability"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "of annotated compound expression datasets and the subtle variations of"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "compound expressions, Compound Emotion Recognition (CE) still holds"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "considerable potential\nfor exploration. To advance this task, the 7th Af-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "fective Behavior Analysis\nin-the-wild (ABAW)\ncompetition introduces"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "the Compound Expression Challenge based on C-EXPR-DB, a limited"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "dataset without labels. In this paper, we present a curriculum learning-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "based framework that initially trains the model on single-expression tasks"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "and subsequently incorporates multi-expression data. This design ensures"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "that our model first masters the fundamental features of basic expressions"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "before being exposed to the complexities of compound emotions. Specif-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "ically, our designs can be summarized as follows: 1) Single-Expression"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "Pre-training: The model\nis first\ntrained on datasets\ncontaining sin-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "gle expressions to learn the foundational\nfacial\nfeatures associated with"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "basic emotions. 2) Dynamic Compound Expression Generation:"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "Given the scarcity of annotated compound expression datasets, we em-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "ploy CutMix and Mixup techniques on the original single-expression im-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "ages\nto create hybrid images exhibiting characteristics of multiple ba-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "sic emotions. 3) Incremental Multi-Expression Integration: After"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "performing well on single-expression tasks,\nthe model\nis progressively"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "exposed to multi-expression data, allowing the model\nto adapt\nto the"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "complexity and variability of compound expressions. The official results"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "indicate that our method achieves\nthe best performance in this com-"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "petition track with an F-score of 0.6063. Our\ncode\nis\nreleased at\nthis"
        },
        {
          "3 CSIRO, Data61, Sydney, Australia": "repository."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nC. Liu et al.": "basic emotions. Unlike recognizing single, basic emotions such as happiness, sad-"
        },
        {
          "2\nC. Liu et al.": "ness, or anger, compound expressions involve more nuanced and mixed emotions,"
        },
        {
          "2\nC. Liu et al.": "such as happily surprised, sadly angry, or fearfully disgusted. These compound"
        },
        {
          "2\nC. Liu et al.": "expressions provide a richer and more accurate representation of human affective"
        },
        {
          "2\nC. Liu et al.": "states [10–18, 26, 31, 37, 45, 53, 56–58]."
        },
        {
          "2\nC. Liu et al.": "Single-expression recognition has seen significant advancements with the ad-"
        },
        {
          "2\nC. Liu et al.": "vent of deep learning, which enables more accurate and efficient recognition of"
        },
        {
          "2\nC. Liu et al.": "basic emotional states. However, understanding compound expressions still faces"
        },
        {
          "2\nC. Liu et al.": "great challenges. We categorize several challenges inherently of this task as fol-"
        },
        {
          "2\nC. Liu et al.": "lows: a) Limited datasets. The annotated compound expression data is very"
        },
        {
          "2\nC. Liu et al.": "rare, posing a challenge for training robust models. b) Subtle Differences of"
        },
        {
          "2\nC. Liu et al.": "Compound Expressions. The compound expression usually contains\nsubtle"
        },
        {
          "2\nC. Liu et al.": "variations in facial\nfeatures. Hence,\nthey are more difficult\nto distinguish than"
        },
        {
          "2\nC. Liu et al.": "basic\nemotions. To facilitate\nthe development of\nthis field,\nthe 7th Affective"
        },
        {
          "2\nC. Liu et al.": "Behavior Analysis\ncompetition (ABAW7)\n[19]\nset\nthe Compound Expression"
        },
        {
          "2\nC. Liu et al.": "(CE)\n[3, 7, 42, 43] Challenge based on the C-EXPR-DB [9] dataset. Participa-"
        },
        {
          "2\nC. Liu et al.": "tions are required to achieve compound expression recognition in videos with"
        },
        {
          "2\nC. Liu et al.": "limited amounts and unknown labels [3, 7, 42, 43]."
        },
        {
          "2\nC. Liu et al.": "To solve the above challenges, we introduce a curriculum learning framework,"
        },
        {
          "2\nC. Liu et al.": "which enhances\nthe generalization ability of our model\nin compound expres-"
        },
        {
          "2\nC. Liu et al.": "sion recognition tasks by gradually transitioning from basic expression to multi-"
        },
        {
          "2\nC. Liu et al.": "expression learning. More specifically, we first train our model on the datasets"
        },
        {
          "2\nC. Liu et al.": "just containing single expressions. In this fashion, our model\nlearns the founda-"
        },
        {
          "2\nC. Liu et al.": "tional facial features associated with basic emotions. After achieving satisfactory"
        },
        {
          "2\nC. Liu et al.": "performance on the single-expression recognition task, we expose the model to"
        },
        {
          "2\nC. Liu et al.": "data with multiple expressions progressively. This stepwise introduction allows"
        },
        {
          "2\nC. Liu et al.": "the model to adapt to the increased complexity and variability inherent in com-"
        },
        {
          "2\nC. Liu et al.": "pound expressions."
        },
        {
          "2\nC. Liu et al.": "To address the scarcity of annotated compound expression datasets, we ap-"
        },
        {
          "2\nC. Liu et al.": "ply CutMix and Mixup to the original single-expression images, creating hybrid"
        },
        {
          "2\nC. Liu et al.": "images that exhibit characteristics of multiple basic emotions. The CutMix tech-"
        },
        {
          "2\nC. Liu et al.": "nique involves cutting and pasting patches from different images, while Mixup"
        },
        {
          "2\nC. Liu et al.": "generates linear interpolations between pairs of images. These augmented images"
        },
        {
          "2\nC. Liu et al.": "are then used to train the model, providing it with a rich and varied training set"
        },
        {
          "2\nC. Liu et al.": "that\nincludes a wide range of compound expressions. We conduct comprehen-"
        },
        {
          "2\nC. Liu et al.": "sive evaluations on the officially provided dataset and our self-curated validation"
        },
        {
          "2\nC. Liu et al.": "dataset. The results demonstrate the effectiveness of our method."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "dients (HOG) [2], and Gabor filters [30] to capture the distinct characteristics of"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "facial expressions. These traditional techniques, though effective to some extent,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "faced limitations in handling variations in lighting, pose, and occlusion."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "With the advent of deep learning, particularly convolutional neural networks"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "(CNNs), the performance of single expression recognition systems has seen sig-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "nificant\nimprovements\n[46–50]. CNNs automatically learn hierarchical\nfeature"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "representations from raw pixel data, leading to more robust and accurate recogni-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "tion. Hasani and Mahoor [5] employ deep residual networks for CER, while Kosti"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "et al. [20] use LSTM networks to capture temporal dynamics in video sequences."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "Multi-task learning (MTL)\nframeworks, as explored by Zhang et al.\n[55], have"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "also enhanced CER by leveraging shared feature extractors with task-specific"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "heads."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "Further advancements were made by incorporating transfer learning and fine-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "tuning pre-trained models on large-scale face datasets. This approach leveraged"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "the generalization capabilities of models trained on extensive datasets, such as"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "VGG-Face [33] and ResNet [21], to enhance the performance of expression recog-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "nition tasks. Additionally, researchers have explored the integration of attention"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "mechanisms and ensemble learning to refine the focus on critical\nfacial regions"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "and combine the strengths of multiple models [39, 41, 59], respectively. However,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "given the complexity of human emotions\nin real-world situations, detecting a"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "single expression is inadequate."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "Compound Expression Recognition (CER) extends traditional emotion recog-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "nition by identifying complex, blended expressions. Early efforts by [3,7] system-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "atically categorized compound emotions, laying the groundwork for further stud-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "ies [46–50]. Dimitrios [9] curate the Multi-Label Compound Expression dataset,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "C-EXPR, and introduced C-EXPR-NET, which simultaneously tackles Com-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "pound Expression Recognition (CER) and Action Unit\n(AU) detection tasks."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "Despite progress, challenges remain due to the subtlety and variability of com-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "pound expressions, highlighting the requirement for more sophisticated models"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n3": "and diverse datasets."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expression": "Feature"
        },
        {
          "Expression": "Cls. Head\nExtractor"
        },
        {
          "Expression": "0.1"
        },
        {
          "Expression": "Fig. 1: Illustration of our proposed frameworks for the compound recogni-"
        },
        {
          "Expression": "tion competition. We adopt a curriculum learning approach, transitioning from basic"
        },
        {
          "Expression": "expression prediction to compound expression learning. We take the second stage train-"
        },
        {
          "Expression": "ing process as an example to illustrate the transition process. Different from the first"
        },
        {
          "Expression": "training stage only utilizes the basic expression data to train the model,\nin this second"
        },
        {
          "Expression": "stage, we randomly select the compound expression data from the natural compound"
        },
        {
          "Expression": "expression datasets (i.e. RAD-BF and Fuxi-EXPR ) and the generated compound ex-"
        },
        {
          "Expression": "pression data. Specifically, there are 80% basic expression images and 20% compound"
        },
        {
          "Expression": "expression images involved in the second training stage. Here, cls. Head refers to the"
        },
        {
          "Expression": "classification head, which is composed of\nlinear layers."
        },
        {
          "Expression": "compound expression recognition. This progressive training strategy decreases"
        },
        {
          "Expression": "the difficulty of tasks. It allows the model to adapt to the more complex task of"
        },
        {
          "Expression": "identifying blended or compound expressions. Moreover,\nin the second learning"
        },
        {
          "Expression": "stage, we utilize Cutmix [52] and Mixup [54] data augmentation techniques to"
        },
        {
          "Expression": "generate compound expressions. This increases dataset diversity, enhancing the"
        },
        {
          "Expression": "robustness and generalization of our model."
        },
        {
          "Expression": "3.2\nFeature Extractor"
        },
        {
          "Expression": "In this competition, we employ Masked AutoEncoder as our\nfeature extractor"
        },
        {
          "Expression": "due to its powerful\nfeature extraction capabilities. To obtain high-quality facial"
        },
        {
          "Expression": "features, we first integrate a large-scale facial dataset,\nincluding AffectNet [29],"
        },
        {
          "Expression": "CASIA-WebFace [44], CelebA [28],\nIMDB-WIKI\n[38], and WebFace260M [60]."
        },
        {
          "Expression": "The data undergoes\nfurther cleaning by the annotation platform to ensure its"
        },
        {
          "Expression": "quality, and ultimately, 4.5 million images are used in the training process. Based"
        },
        {
          "Expression": "on this dataset, we train the MAE in a self-supervised manner. Specifically, we"
        },
        {
          "Expression": "randomly mask 75% of each facial\nimage and then reconstruct the original\nface"
        },
        {
          "Expression": "image. During the training stage, we utilize L2 Loss to calculate the difference"
        },
        {
          "Expression": "between the reconstructed image and the original\nimage for model optimization."
        },
        {
          "Expression": "To better adapt the MAE model to the affective behavior analysis, we fine-tuned"
        },
        {
          "Expression": "the model on the AffectNet and data from Aff-Wild provided by the competition."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "3.3\nSingle Expression Recognition"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "To attain the high-quality feature extractor, we first finetune the MAE model"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "on RAF-DB and our curated Fuxi-EXPR dataset\n(including 25K images with"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "seven basic expressions as RAF-DB [22]). Note that,\nin this stage, we just utilize"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "the data with the single expression label\nto train the model, as\nshown in Fig."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "1. To increase the diversity of\nthe training dataset, we adopt\nthe cutout data"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "augmentation technique as well as the basic data augmentation techniques such"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "as random flipping, color jittering, and random crop in the initial\nlearning stage."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "Note that considering the final compound expressions do not include the neutral"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "label, we filter out all data with the neutral\nlabel\nfrom the entire dataset."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "3.4\nCompound Expression Recognition"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "As the model’s performance stabilizes, we progressively introduce more complex"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "compound expressions to train our model. To reduce the risk of overwhelming"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "the model with difficult data too early, we progressively increase the proportion"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "of compound expressions in the training data. Specifically, we divide the overall"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "training stage into four sub-stages, with the number of training epochs for each"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "stage being 5, 5, 3, and 3. The proportion of\ncompound expressions\nin each"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "stage is 0, 0.2, 0.4, and 1, respectively. During the training phase, we treat the"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "compound expression recognition task as a multi-class classification problem. To"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "achieve better performance, during the inference phase, we constrain the final"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "results into the officially provided compound expression categories. Specifically,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "we\nsum the\nclassification probabilities of\nthe\nindividual\nexpression categories"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "involved in the compound expressions and utilize the argmax of these summed"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "probabilities as the final result."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "3.5\nTraining Objectives"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "In the whole training stage, we utilize Binary Cross-Entropy Multi-Label Loss"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n5": "as the optimization objective, mathematically expressed by:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: As indicated in Table 2, the best per-",
      "data": [
        {
          "6\nC. Liu et al.": "following categories: Fearfully Surprised, Happily Surprised, Sadly Surprised,"
        },
        {
          "6\nC. Liu et al.": "Disgustedly Surprised, Angrily Surprised, Sadly Fearful, and Sadly Angry. Note"
        },
        {
          "6\nC. Liu et al.": "that we do not have the ground truth labels for these videos."
        },
        {
          "6\nC. Liu et al.": "Extra Data for CE. Except\nfor"
        },
        {
          "6\nC. Liu et al.": "dataset with RAF-DB [22] and AffectNet [29] to increase the data of Anger, Dis-"
        },
        {
          "6\nC. Liu et al.": "gust, and Fear expressions. Fuxi-EXPR contains 71,618 facial"
        },
        {
          "6\nC. Liu et al.": "from films, television works, and public video platforms. Data cleaning and man-"
        },
        {
          "6\nC. Liu et al.": "agement are handled by the Netease Fuxi Youling Crowdsourcing platform2. The"
        },
        {
          "6\nC. Liu et al.": "dataset includes labels for six basic emotions (Surprised, Fear, Disgust, Happi-"
        },
        {
          "6\nC. Liu et al.": "ness, Sadness, Anger) and an additional category for Others. To obtain stable"
        },
        {
          "6\nC. Liu et al.": "performance, we retain only the six basic emotions for the two datasets."
        },
        {
          "6\nC. Liu et al.": "Extra Data for MAE. To attain a powerful expression feature extractor, we"
        },
        {
          "6\nC. Liu et al.": "train MAE on our\nre-collected facial"
        },
        {
          "6\nC. Liu et al.": "[29], CASIA-WebFace [44], CelebA [28],\nIMDB-WIKI"
        },
        {
          "6\nC. Liu et al.": "and the Fuxi-EXPR private dataset."
        },
        {
          "6\nC. Liu et al.": "4.2\nMetrics"
        },
        {
          "6\nC. Liu et al.": "For the CE track, the performance P is measured by the average F1 score across"
        },
        {
          "6\nC. Liu et al.": "all seven categories, expressed by:"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Analysis of Data Augmentation. All experiments are conducted under",
      "data": [
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "Exp"
        },
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "1"
        },
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "2"
        },
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "3"
        },
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "4"
        },
        {
          "Data Proportion Distribution) is the proportion of the compound data in each training": "5"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "Netease Fuxi AI Lab) attains the best performance of the C-EXPR-DB dataset."
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "Teams"
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "AIPL-BME-SEU [23]"
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "HFUT-MAC2 [40]"
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "ETS-LIVIA [36]"
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "HSEmotion [27]"
        },
        {
          "Table 3: The final evaluation results on the official test dataset. Our team (i.e.": "Netease Fuxi AI Lab (Ours) [26]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "References"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "1. Canal, F.Z., Müller, T.R., Matias, J.C., Scotton, G.G., de Sa Junior, A.R., Pozze-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "bon, E., Sobieranski, A.C.: A survey on facial emotion recognition techniques: A"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "state-of-the-art literature review. Information Sciences 582, 593–617 (2022) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "2. Dalal, N., Triggs, B.: Histograms of oriented gradients\nfor human detection.\nIn:"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "2005 IEEE computer society conference on computer vision and pattern recognition"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "(CVPR’05). vol. 1, pp. 886–893. Ieee (2005) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "3. Dong, R., Lam, K.M.: Bi-center\nloss for compound facial expression recognition."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "IEEE Signal Processing Letters (2024) 2, 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "4. Guo, J., Lei, Z., Wan, J., Avots, E., Hajarolasvadi, N., Knyazev, B., Kuharenko,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "A., Junior, J.C.S.J., Baró, X., Demirel, H., et al.: Dominant and complementary"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "emotion recognition from still\nimages of faces. IEEE Access 6, 26391–26403 (2018)"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "5. Hasani, B., Mahoor, M.H.: Facial expression recognition using enhanced deep 3d"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "convolutional neural networks. In: Proceedings of the IEEE conference on computer"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "vision and pattern recognition workshops. pp. 30–40 (2017) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "6. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "vision and pattern recognition. pp. 16000–16009 (2022) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "7. He, S., Zhao, H., Yu, L., Xiang, J., Du, C., Jing, J.: Compound facial expression"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "recognition with multi-domain fusion expression based on adversarial\nlearning. In:"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "2022 IEEE International Conference on Systems, Man, and Cybernetics\n(SMC)."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "pp. 688–693. IEEE (2022) 2, 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "8. Houssein, E.H., Hammad, A., Ali, A.A.: Human emotion recognition from eeg-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "based brain–computer interface using machine learning: a comprehensive review."
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "Neural Computing and Applications 34(15), 12527–12557 (2022) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "9. Kollias, D.: Multi-label compound expression recognition: C-expr database & net-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "work. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "tern Recognition. pp. 5589–5598 (2023) 2, 3, 5"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "10. Kollias, D., Schulc, A., Hajiyev, E., Zafeiriou, S.: Analysing affective behavior in"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "the first abaw 2020 competition. In: 2020 15th IEEE International Conference on"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "Automatic Face and Gesture Recognition (FG 2020). pp. 637–643. IEEE (2020) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "11. Kollias, D., Sharmanska, V., Zafeiriou, S.: Face behavior a la carte: Expressions,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "affect and action units in a single network. arXiv preprint arXiv:1910.11111 (2019)"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "12. Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching for heteroge-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "neous multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "(2021) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "13. Kollias, D., Tzirakis, P., Baird, A., Cowen, A., Zafeiriou, S.: Abaw: Valence-arousal"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "estimation, expression recognition, action unit detection & emotional reaction in-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "tensity estimation challenges.\nIn: Proceedings of\nthe\nIEEE/CVF Conference on"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "Computer Vision and Pattern Recognition. pp. 5888–5897 (2023) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "14. Kollias, D., Tzirakis, P., Baird, A., Cowen, A., Zafeiriou, S.: Abaw: Valence-arousal"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "estimation, expression recognition, action unit detection & emotional reaction in-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "tensity estimation challenges.\nIn: Proceedings of\nthe\nIEEE/CVF Conference on"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "Computer Vision and Pattern Recognition. pp. 5888–5897 (2023) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "15. Kollias, D., Tzirakis, P., Cowen, A., Zafeiriou, S., Shao, C., Hu, G.: The 6th affective"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "behavior analysis in-the-wild (abaw) competition. arXiv preprint arXiv:2402.19344"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n9": "(2024) 2"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10\nC. Liu et al.": "16. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller,"
        },
        {
          "10\nC. Liu et al.": "B., Kotsia,\nI., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-wild database"
        },
        {
          "10\nC. Liu et al.": "and challenge, deep architectures, and beyond. International Journal of Computer"
        },
        {
          "10\nC. Liu et al.": "Vision pp. 1–23 (2019) 2"
        },
        {
          "10\nC. Liu et al.": "17. Kollias, D., Zafeiriou, S.: Expression, affect, action unit\nrecognition: Aff-wild2,"
        },
        {
          "10\nC. Liu et al.": "multi-task learning and arcface. arXiv preprint arXiv:1910.04855 (2019) 2"
        },
        {
          "10\nC. Liu et al.": "18. Kollias, D., Zafeiriou, S.: Analysing affective behavior in the second abaw2 compe-"
        },
        {
          "10\nC. Liu et al.": "tition.\nIn: Proceedings of the IEEE/CVF International Conference on Computer"
        },
        {
          "10\nC. Liu et al.": "Vision. pp. 3652–3660 (2021) 2"
        },
        {
          "10\nC. Liu et al.": "19. Kollias, D., Zafeiriou, S., Kotsia,\nI., Dhall, A., Ghosh, S., Shao, C., Hu, G.:"
        },
        {
          "10\nC. Liu et al.": "7th abaw competition: Multi-task learning and compound expression recognition."
        },
        {
          "10\nC. Liu et al.": "arXiv preprint arXiv:2407.03835 (2024) 2"
        },
        {
          "10\nC. Liu et al.": "20. Kosti, R., Alvarez, J.M., Recasens, A., Lapedriza, A.: Emotion recognition in con-"
        },
        {
          "10\nC. Liu et al.": "text. In: Proceedings of the IEEE conference on computer vision and pattern recog-"
        },
        {
          "10\nC. Liu et al.": "nition. pp. 1667–1675 (2017) 3"
        },
        {
          "10\nC. Liu et al.": "21. Li, B., Lima, D.: Facial expression recognition via resnet-50. International Journal"
        },
        {
          "10\nC. Liu et al.": "of Cognitive Computing in Engineering 2, 57–64 (2021) 3"
        },
        {
          "10\nC. Liu et al.": "22. Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving learn-"
        },
        {
          "10\nC. Liu et al.": "ing for expression recognition in the wild. In: 2017 IEEE Conference on Computer"
        },
        {
          "10\nC. Liu et al.": "Vision and Pattern Recognition (CVPR). pp. 2584–2593. IEEE (2017) 5, 6"
        },
        {
          "10\nC. Liu et al.": "23. Li,\nS., Lian, H., Lu, C., Zhao, Y., Qi, T., Yang, H., Zong, Y., Zheng, W.:"
        },
        {
          "10\nC. Liu et al.": "Temporal\nlabel hierachical network\nfor\ncompound emotion recognition (2024),"
        },
        {
          "10\nC. Liu et al.": "https://arxiv.org/abs/2407.12973 8"
        },
        {
          "10\nC. Liu et al.": "24. Liu, C., Li, P., Zhang, H., Li, L., Huang, Z., Wang, D., Yu, X.: Bavs: bootstrapping"
        },
        {
          "10\nC. Liu et al.": "audio-visual\nsegmentation by integrating foundation knowledge. arXiv preprint"
        },
        {
          "10\nC. Liu et al.": "arXiv:2308.10175 (2023) 2"
        },
        {
          "10\nC. Liu et al.": "25. Liu, C., Li, P.P., Qi, X., Zhang, H., Li, L., Wang, D., Yu, X.: Audio-visual\nseg-"
        },
        {
          "10\nC. Liu et al.": "mentation by exploring cross-modal mutual semantics. In: Proceedings of the 31st"
        },
        {
          "10\nC. Liu et al.": "ACM International Conference on Multimedia. pp. 7590–7598 (2023) 2"
        },
        {
          "10\nC. Liu et al.": "26. Liu, C., Zhang, W., Qiu, F., Li, L., Yu, X.: Affective behaviour analysis via pro-"
        },
        {
          "10\nC. Liu et al.": "gressive learning (2024), https://arxiv.org/abs/2407.16945 2, 8"
        },
        {
          "10\nC. Liu et al.": "27. Liu, X., Shen, K., Yao, J., Wang, B., Liu, M., An, L., Cui, Z., Feng, W., Sun,"
        },
        {
          "10\nC. Liu et al.": "X.: Compound expression recognition via multi model\nensemble\nfor\nthe abaw7"
        },
        {
          "10\nC. Liu et al.": "challenge (2024), https://arxiv.org/abs/2407.12257 8"
        },
        {
          "10\nC. Liu et al.": "28. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:"
        },
        {
          "10\nC. Liu et al.": "Proceedings of\nthe IEEE international conference on computer vision. pp. 3730–"
        },
        {
          "10\nC. Liu et al.": "3738 (2015) 4, 6"
        },
        {
          "10\nC. Liu et al.": "29. Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet: A database for facial ex-"
        },
        {
          "10\nC. Liu et al.": "pression, valence, and arousal computing in the wild. IEEE Transactions on Affec-"
        },
        {
          "10\nC. Liu et al.": "tive Computing 10(1), 18–31 (2017) 4, 6"
        },
        {
          "10\nC. Liu et al.": "30. Movellan, J.R.: Tutorial on gabor filters. Open source document 40, 1–23 (2002)"
        },
        {
          "10\nC. Liu et al.": "3"
        },
        {
          "10\nC. Liu et al.": "31. Nguyen, D.K., Ho, N.H., Pant, S., Yang, H.J.: A transformer-based approach"
        },
        {
          "10\nC. Liu et al.": "to video frame-level prediction in affective behaviour analysis\nin-the-wild. arXiv"
        },
        {
          "10\nC. Liu et al.": "preprint arXiv:2303.09293 (2023) 2"
        },
        {
          "10\nC. Liu et al.": "32. Pietikäinen, M.: Local binary patterns. Scholarpedia 5(3),\n9775 (2010) 2"
        },
        {
          "10\nC. Liu et al.": "33. Qawaqneh, Z., Mallouh, A.A., Barkana, B.D.: Deep convolutional neural network"
        },
        {
          "10\nC. Liu et al.": "for age estimation based on vgg-face model. arXiv preprint arXiv:1709.01664 (2017)"
        },
        {
          "10\nC. Liu et al.": "3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "34. Qi, X., Liu, C., Li, L., Hou, J., Xin, H., Yu, X.: Emotiongesture: Audio-driven"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "diverse emotional co-speech 3d gesture generation. arXiv preprint arXiv:2305.18891"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "(2023) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "35. Qi, X., Liu, C., Sun, M., Li, L., Fan, C., Yu, X.: Diverse 3d hand gesture prediction"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "from body dynamics by bilateral hand disentanglement.\nIn: Proceedings of\nthe"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4616–"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "4626 (2023) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "36. Richet, N., Belharbi, S., Aslam, H., Schadt, M.E., González-González, M., Cortal,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "G., Koerich, A.L., Pedersoli, M., Finkel, A., Bacon, S., Granger, E.: Text- and"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "feature-based models\nfor compound multimodal emotion recognition in the wild"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "(2024), https://arxiv.org/abs/2407.12927 8"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "37. Ritzhaupt, A.D., Huang, R., Sommer, M., Zhu, J., Stephen, A., Valle, N., Hampton,"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "J., Li, J.: A meta-analysis on the influence of gamification in formal educational"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "settings on affective and behavioral outcomes. Educational Technology Research"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "and Development 69(5), 2493–2522 (2021) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "38. Rothe, R., Timofte, R., Van Gool, L.: Deep expectation of real and apparent age"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "from a single image without facial\nlandmarks. International Journal of Computer"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "Vision 126(2), 144–157 (2018) 4, 6"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "39. Savchenko, A.V.: Facial expression and attributes recognition based on multi-task"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "learning of\nlightweight neural networks.\nIn: 2021 IEEE 19th International Sym-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "posium on Intelligent Systems and Informatics (SISY). pp. 119–124. IEEE (2021)"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "40. Savchenko, A.V.: Hsemotion team at\nthe 7th abaw challenge: Multi-task learn-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "ing and compound facial expression recognition (2024), https://arxiv.org/abs/"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "2407.13184 8"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "41. Savchenko, A.V., Savchenko, L.V., Makarov, I.: Classifying emotions and engage-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "ment in online learning based on a single facial expression recognition neural net-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "work. IEEE Transactions on Affective Computing 13(4), 2132–2143 (2022) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "42. She, J., Hu, Y., Shi, H., Wang, J., Shen, Q., Mei, T.: Dive into ambiguity: La-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "tent distribution mining and pairwise uncertainty estimation for facial expression"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "recognition. In: Proceedings of the IEEE/CVF conference on computer vision and"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "pattern recognition. pp. 6248–6257 (2021) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "43. Wang, K., Peng, X., Yang, J., Lu, S., Qiao, Y.: Suppressing uncertainties for large-"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "scale facial expression recognition.\nIn: Proceedings of\nthe IEEE/CVF conference"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "on computer vision and pattern recognition. pp. 6897–6906 (2020) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "44. Yi, D., Lei, Z., Liao, S., Li, S.Z.: Learning face representation from scratch. arXiv"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "preprint arXiv:1411.7923 (2014) 4, 6"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "45. Yin, Y., Tran, M., Chang, D., Wang, X., Soleymani, M.: Multi-modal facial action"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "unit detection with large pre-trained models for the 5th competition on affective"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "behavior analysis in-the-wild. arXiv preprint arXiv:2303.10590 (2023) 2"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "46. Yu, X., Fernando, B., Ghanem, B., Porikli, F., Hartley, R.: Face super-resolution"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "guided by facial component heatmaps. In: Proceedings of the European conference"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "on computer vision (ECCV). pp. 217–233 (2018) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "47. Yu, X., Fernando, B., Hartley, R., Porikli, F.: Super-resolving very low-resolution"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "face images with supplementary attributes. In: Proceedings of the IEEE conference"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "on computer vision and pattern recognition. pp. 908–917 (2018) 3"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "48. Yu, X., Porikli, F.: Face hallucination with tiny unaligned images by transformative"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "discriminative neural networks. In: Proceedings of the AAAI conference on artificial"
        },
        {
          "Compound Expression Recognition via Curriculum Learning\n11": "intelligence. vol. 31 (2017) 3"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nC. Liu et al.": "49. Yu, X., Porikli, F.: Hallucinating very low-resolution unaligned and noisy face"
        },
        {
          "12\nC. Liu et al.": "images by transformative discriminative autoencoders. In: Proceedings of the IEEE"
        },
        {
          "12\nC. Liu et al.": "conference on computer vision and pattern recognition. pp. 3760–3768 (2017) 3"
        },
        {
          "12\nC. Liu et al.": "50. Yu, X., Shiri, F., Ghanem, B., Porikli, F.: Can we see more? joint\nfrontalization"
        },
        {
          "12\nC. Liu et al.": "and hallucination of unaligned tiny faces.\nIEEE transactions on pattern analysis"
        },
        {
          "12\nC. Liu et al.": "and machine intelligence 42(9), 2148–2164 (2019) 3"
        },
        {
          "12\nC. Liu et al.": "51. Yue, L., Chen, W., Li, X., Zuo, W., Yin, M.: A survey of\nsentiment analysis\nin"
        },
        {
          "12\nC. Liu et al.": "social media. Knowledge and Information Systems 60, 617–663 (2019) 2"
        },
        {
          "12\nC. Liu et al.": "52. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization"
        },
        {
          "12\nC. Liu et al.": "strategy to train strong classifiers with localizable features. In: Proceedings of the"
        },
        {
          "12\nC. Liu et al.": "IEEE/CVF international conference on computer vision. pp. 6023–6032 (2019) 4"
        },
        {
          "12\nC. Liu et al.": "53. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia,\nI.:"
        },
        {
          "12\nC. Liu et al.": "Aff-wild: Valence and arousal\n‘in-the-wild’challenge. In: Computer Vision and Pat-"
        },
        {
          "12\nC. Liu et al.": "tern Recognition Workshops (CVPRW), 2017 IEEE Conference on. pp. 1980–1987."
        },
        {
          "12\nC. Liu et al.": "IEEE (2017) 2"
        },
        {
          "12\nC. Liu et al.": "54. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk"
        },
        {
          "12\nC. Liu et al.": "minimization. arXiv preprint arXiv:1710.09412 (2017) 4"
        },
        {
          "12\nC. Liu et al.": "55. Zhang, T., Liu, C., Liu, X., Liu, Y., Meng, L., Sun, L., Jiang, W., Zhang, F., Zhao,"
        },
        {
          "12\nC. Liu et al.": "J., Jin, Q.: Multi-task learning framework for emotion recognition in-the-wild. In:"
        },
        {
          "12\nC. Liu et al.": "European Conference on Computer Vision. pp. 143–156. Springer (2022) 3"
        },
        {
          "12\nC. Liu et al.": "56. Zhang, W., Ma, B., Qiu, F., Ding, Y.: Multi-modal facial affective analysis based on"
        },
        {
          "12\nC. Liu et al.": "masked autoencoder. In: Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "12\nC. Liu et al.": "Vision and Pattern Recognition. pp. 5792–5801 (2023) 2"
        },
        {
          "12\nC. Liu et al.": "57. Zhang, W., Qiu, F., Liu, C., Li, L., Du, H., Guo, T., Yu, X.: An effective\nen-"
        },
        {
          "12\nC. Liu et al.": "semble learning framework for affective behaviour analysis. In: Proceedings of the"
        },
        {
          "12\nC. Liu et al.": "IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4761–"
        },
        {
          "12\nC. Liu et al.": "4772 (2024) 2"
        },
        {
          "12\nC. Liu et al.": "58. Zhang, W., Qiu, F., Liu, C., Li, L., Du, H., Guo, T., Yu, X.: Affective behaviour"
        },
        {
          "12\nC. Liu et al.": "analysis via integrating multi-modal knowledge (2024), https://arxiv.org/abs/"
        },
        {
          "12\nC. Liu et al.": "2403.10825 2"
        },
        {
          "12\nC. Liu et al.": "59. Zhang, Y., Wang, C., Ling, X., Deng, W.: Learn from all: Erasing attention con-"
        },
        {
          "12\nC. Liu et al.": "sistency for noisy label\nfacial expression recognition. In: European Conference on"
        },
        {
          "12\nC. Liu et al.": "Computer Vision. pp. 418–434. Springer (2022) 2, 3"
        },
        {
          "12\nC. Liu et al.": "60. Zhu, Z., Huang, G., Deng, J., Ye, Y., Huang, J., Chen, X., Zhu, J., Yang, T., Lu,"
        },
        {
          "12\nC. Liu et al.": "J., Du, D., et al.: Webface260m: A benchmark unveiling the power of million-scale"
        },
        {
          "12\nC. Liu et al.": "deep face recognition. In: Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "12\nC. Liu et al.": "Vision and Pattern Recognition. pp. 10492–10502 (2021) 4, 6"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "2",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "IEEE computer society conference on computer vision and pattern recognition (CVPR'05)"
    },
    {
      "citation_id": "3",
      "title": "Bi-center loss for compound facial expression recognition",
      "authors": [
        "R Dong",
        "K Lam"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "4",
      "title": "Dominant and complementary emotion recognition from still images of faces",
      "authors": [
        "J Guo",
        "Z Lei",
        "J Wan",
        "E Avots",
        "N Hajarolasvadi",
        "B Knyazev",
        "A Kuharenko",
        "J Junior",
        "X Baró",
        "H Demirel"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition using enhanced deep 3d convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "6",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Compound facial expression recognition with multi-domain fusion expression based on adversarial learning",
      "authors": [
        "S He",
        "H Zhao",
        "L Yu",
        "J Xiang",
        "C Du",
        "J Jing"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "8",
      "title": "Human emotion recognition from eegbased brain-computer interface using machine learning: a comprehensive review",
      "authors": [
        "E Houssein",
        "A Hammad",
        "A Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "9",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "11",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "12",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "13",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Cowen",
        "S Zafeiriou",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "16",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou",
        "I Kotsia",
        "A Dhall",
        "S Ghosh",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Facial expression recognition via resnet-50",
      "authors": [
        "B Li",
        "D Lima"
      ],
      "year": "2021",
      "venue": "International Journal of Cognitive Computing in Engineering"
    },
    {
      "citation_id": "22",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "23",
      "title": "Temporal label hierachical network for compound emotion recognition",
      "authors": [
        "S Li",
        "H Lian",
        "C Lu",
        "Y Zhao",
        "T Qi",
        "H Yang",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "Temporal label hierachical network for compound emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Bavs: bootstrapping audio-visual segmentation by integrating foundation knowledge",
      "authors": [
        "C Liu",
        "P Li",
        "H Zhang",
        "L Li",
        "Z Huang",
        "D Wang",
        "X Yu"
      ],
      "year": "2023",
      "venue": "Bavs: bootstrapping audio-visual segmentation by integrating foundation knowledge",
      "arxiv": "arXiv:2308.10175"
    },
    {
      "citation_id": "25",
      "title": "Audio-visual segmentation by exploring cross-modal mutual semantics",
      "authors": [
        "C Liu",
        "P Li",
        "X Qi",
        "H Zhang",
        "L Li",
        "D Wang",
        "X Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Affective behaviour analysis via progressive learning",
      "authors": [
        "C Liu",
        "W Zhang",
        "F Qiu",
        "L Li",
        "X Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via progressive learning"
    },
    {
      "citation_id": "27",
      "title": "Compound expression recognition via multi model ensemble for the abaw7 challenge",
      "authors": [
        "X Liu",
        "K Shen",
        "J Yao",
        "B Wang",
        "M Liu",
        "L An",
        "Z Cui",
        "W Feng",
        "X Sun"
      ],
      "year": "2024",
      "venue": "Compound expression recognition via multi model ensemble for the abaw7 challenge"
    },
    {
      "citation_id": "28",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Tutorial on gabor filters",
      "authors": [
        "J Movellan"
      ],
      "year": "2002",
      "venue": "Open source document"
    },
    {
      "citation_id": "31",
      "title": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-the-wild",
      "authors": [
        "D Nguyen",
        "N Ho",
        "S Pant",
        "H Yang"
      ],
      "year": "2023",
      "venue": "A transformer-based approach to video frame-level prediction in affective behaviour analysis in-the-wild",
      "arxiv": "arXiv:2303.09293"
    },
    {
      "citation_id": "32",
      "title": "Local binary patterns",
      "authors": [
        "M Pietikäinen"
      ],
      "year": "2010",
      "venue": "Scholarpedia"
    },
    {
      "citation_id": "33",
      "title": "Deep convolutional neural network for age estimation based on vgg-face model",
      "authors": [
        "Z Qawaqneh",
        "A Mallouh",
        "B Barkana"
      ],
      "year": "2017",
      "venue": "Deep convolutional neural network for age estimation based on vgg-face model",
      "arxiv": "arXiv:1709.01664"
    },
    {
      "citation_id": "34",
      "title": "Emotiongesture: Audio-driven diverse emotional co-speech 3d gesture generation",
      "authors": [
        "X Qi",
        "C Liu",
        "L Li",
        "J Hou",
        "H Xin",
        "X Yu"
      ],
      "year": "2023",
      "venue": "Emotiongesture: Audio-driven diverse emotional co-speech 3d gesture generation",
      "arxiv": "arXiv:2305.18891"
    },
    {
      "citation_id": "35",
      "title": "Diverse 3d hand gesture prediction from body dynamics by bilateral hand disentanglement",
      "authors": [
        "X Qi",
        "C Liu",
        "M Sun",
        "L Li",
        "C Fan",
        "X Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Text-and feature-based models for compound multimodal emotion recognition in the wild",
      "authors": [
        "N Richet",
        "S Belharbi",
        "H Aslam",
        "M Schadt",
        "M González-González",
        "G Cortal",
        "A Koerich",
        "M Pedersoli",
        "A Finkel",
        "S Bacon",
        "E Granger"
      ],
      "year": "2024",
      "venue": "Text-and feature-based models for compound multimodal emotion recognition in the wild"
    },
    {
      "citation_id": "37",
      "title": "A meta-analysis on the influence of gamification in formal educational settings on affective and behavioral outcomes",
      "authors": [
        "A Ritzhaupt",
        "R Huang",
        "M Sommer",
        "J Zhu",
        "A Stephen",
        "N Valle",
        "J Hampton",
        "J Li"
      ],
      "year": "2021",
      "venue": "Educational Technology Research and Development"
    },
    {
      "citation_id": "38",
      "title": "Deep expectation of real and apparent age from a single image without facial landmarks",
      "authors": [
        "R Rothe",
        "R Timofte",
        "L Van Gool"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "39",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "IEEE 19th International Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "40",
      "title": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition",
      "authors": [
        "A Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition"
    },
    {
      "citation_id": "41",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "A Savchenko",
        "L Savchenko",
        "I Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "Suppressing uncertainties for largescale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Learning face representation from scratch",
      "authors": [
        "D Yi",
        "Z Lei",
        "S Liao",
        "S Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "45",
      "title": "Multi-modal facial action unit detection with large pre-trained models for the 5th competition on affective behavior analysis in-the-wild",
      "authors": [
        "Y Yin",
        "M Tran",
        "D Chang",
        "X Wang",
        "M Soleymani"
      ],
      "year": "2023",
      "venue": "Multi-modal facial action unit detection with large pre-trained models for the 5th competition on affective behavior analysis in-the-wild",
      "arxiv": "arXiv:2303.10590"
    },
    {
      "citation_id": "46",
      "title": "Face super-resolution guided by facial component heatmaps",
      "authors": [
        "X Yu",
        "B Fernando",
        "B Ghanem",
        "F Porikli",
        "R Hartley"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "47",
      "title": "Super-resolving very low-resolution face images with supplementary attributes",
      "authors": [
        "X Yu",
        "B Fernando",
        "R Hartley",
        "F Porikli"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "48",
      "title": "Face hallucination with tiny unaligned images by transformative discriminative neural networks",
      "authors": [
        "X Yu",
        "F Porikli"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders",
      "authors": [
        "X Yu",
        "F Porikli"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "50",
      "title": "Can we see more? joint frontalization and hallucination of unaligned tiny faces",
      "authors": [
        "X Yu",
        "F Shiri",
        "B Ghanem",
        "F Porikli"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "51",
      "title": "A survey of sentiment analysis in social media",
      "authors": [
        "L Yue",
        "W Chen",
        "X Li",
        "W Zuo",
        "M Yin"
      ],
      "year": "2019",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "52",
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "authors": [
        "S Yun",
        "D Han",
        "S Oh",
        "S Chun",
        "J Choe",
        "Y Yoo"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "53",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "venue": "Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "54",
      "title": "",
      "authors": [
        "Ieee"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "55",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "56",
      "title": "Multi-task learning framework for emotion recognition in-the-wild",
      "authors": [
        "T Zhang",
        "C Liu",
        "X Liu",
        "Y Liu",
        "L Meng",
        "L Sun",
        "W Jiang",
        "F Zhang",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "Multi-modal facial affective analysis based on masked autoencoder",
      "authors": [
        "W Zhang",
        "B Ma",
        "F Qiu",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "An effective ensemble learning framework for affective behaviour analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "C Liu",
        "L Li",
        "H Du",
        "T Guo",
        "X Yu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Affective behaviour analysis via integrating multi-modal knowledge",
      "authors": [
        "W Zhang",
        "F Qiu",
        "C Liu",
        "L Li",
        "H Du",
        "T Guo",
        "X Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via integrating multi-modal knowledge"
    },
    {
      "citation_id": "60",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "61",
      "title": "Webface260m: A benchmark unveiling the power of million-scale deep face recognition",
      "authors": [
        "Z Zhu",
        "G Huang",
        "J Deng",
        "Y Ye",
        "J Huang",
        "X Chen",
        "J Zhu",
        "T Yang",
        "J Lu",
        "D Du"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}