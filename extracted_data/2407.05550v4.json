{
  "paper_id": "2407.05550v4",
  "title": "Meeg And At-Dgnn: Improving Eeg Emotion Recognition With Music Introducing And Graph-Based Learning",
  "published": "2024-07-08T01:58:48Z",
  "authors": [
    "Minghao Xiao",
    "Zhengxi Zhu",
    "Kang Xie",
    "Bin Jiang"
  ],
  "keywords": [
    "electroencephalogram",
    "emotion recognition",
    "dynamic graph neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present the MEEG dataset, a multi-modal collection of music-induced electroencephalogram (EEG) recordings designed to capture emotional responses to various musical stimuli across different valence and arousal levels. This public dataset facilitates an in-depth examination of brainwave patterns within musical contexts, providing a robust foundation for studying brain network topology during emotional processing. Leveraging the MEEG dataset, we introduce the Attention-based Temporal Learner with Dynamic Graph Neural Network (AT-DGNN), a novel framework for EEG-based emotion recognition. This model combines an attention mechanism with a dynamic graph neural network (DGNN) to capture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA) performance with an accuracy of 83.74% in arousal recognition and 86.01% in valence recognition, outperforming existing SOTA methods. This study advances graph-based learning methodology in brain-computer interfaces (BCI), significantly improving the accuracy of EEGbased emotion recognition. The MEEG dataset and source code are publicly available at https://github.com/xmh1011/AT-DGNN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion plays a key role in interpreting human responses, and Russell's Valence-Arousal (VA) model offers a dimensional framework for quantifying emotional states within a continuous space  [1] . Meanwhile, BCI technology, which decodes EEG signals reflecting cortical voltage fluctuations, translates thoughts into commands for external devices, significantly enhancing human-computer interaction  [2] . With their high temporal resolution, objectivity, affordability, and quick acquisition, EEG signals are crucial for emotion recognition and neuroscience research, particularly in integrating emotional models with advanced decoding methods  [3] .\n\nIn 2011, Koelstra et al. introduced the DEAP dataset, consisting of physiological signals like EEG elicited by emotional responses to musical videos  [4] , followed by Zheng et al.'s SEED dataset in 2015, which provided well-annotated EEG signals from cinematic stimuli across multiple subjects  [5] . These datasets have greatly advanced computational models for emotion analysis.\n\nRecent advances in deep learning have significantly impacted EEG signal processing. In 2018, Lawhern et al. proposed EEGNet, a compact convolutional neural network (CNN) tailored for EEG analysis  [6] . Subsequent developments by Schirrmeister et al. further refined EEG decoding capabilities using deep CNN architectures, introducing sophisticated visualization tools  [7] . The adoption of temporal convolutional networks (TCNs) has introduced robust alternatives for real-time BCI applications. Notably, in 2020, Ingolfsson et al. developed EEG-TCNet, which demonstrated superior accuracy in motor imagery tasks, suggesting that TCNs could outperform traditional methods in specific BCI contexts  [8] . This assertion was corroborated by  Musallam et al. , who demonstrated the versatility and efficiency of TCNs in complex BCI tasks, particularly through the integration of TCNs in motor imagery classification  [9] .\n\nAll aforementioned studies treated EEG signals as twodimensional time-series data, with channels representing EEG electrodes positioned according to the 10-20 system to capture neural activity across brain regions  [2, 10] . Recently, EEG data have been increasingly modeled as graphs, representing the spatial arrangement of electrodes, where features from each electrode add a third dimension to the model  [11, 12] .\n\nIn 2019, Song et al. introduced a novel DGCNN for multichannel EEG emotion recognition, utilizing an adjacency matrix to dynamically model EEG channel relationships and enhance feature discrimination, demonstrating superiority over existing approaches  [13] . Subsequently, advancements in GNNs have been increasingly applied to emotion recognition. Bao et al. integrated a multi-layer GNN with a stylereconfigurable CNN  [14] , while Asadzadeh et al. further improved the DGCNN by incorporating Bayesian signal recovery techniques, both achieving enhanced performance  [15] .\n\nBuilding on prior advancements, greater progress was made in 2023 when Ding et al. introduced the Local-Global-Graph Network (LGGNet) for processing EEG data in an image-like format, achieving improved classification performance  [16] . Alternatively, LGGNet's single-layer graph neural network faces challenges in capturing the dynamic features of EEG signals. Additionally, widely used EEG datasets such as DEAP and SEED primarily involve visual stimuli, which limits exploration of the relationship between music, emotions, and EEG signals. Conversely, EEG datasets and research using musical stimuli are relatively scarce. Moreover, existing SOTA methods achieve lower accuracy on the DEAP dataset, thus limiting comprehensive performance evaluation of models.\n\nTo address these limitations in emotion recognition research, the MEEG (Music EEG) dataset is developed accordingly. This multi-modal dataset, similar to DEAP, utilizes musicinduced emotional states to improve the accuracy of emotion recognition. By integrating a sliding window technique, an attention mechanism, and a multi-layer dynamic graph neural network into the LGGNet architecture, significant improvements in classification accuracy are achieved.\n\nThe contributions of our study are threefold and can be summarized as follows:\n\n1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Multi-Head Attention Mechanism",
      "text": "EEG data is inherently complex, with significant temporal dependencies and discrete features. Multi-head attention (MHA) helps capture diverse temporal patterns and dependencies, enabling the identification of key features. In this module, the attention block consists of a multi-head attention layer with several self-attention heads. As shown by Zhang et al.  [17] , multi-head self-attention effectively integrates multi-source information, improving the model's robustness in handling complex temporal features in EEG data, leading to enhanced classification performance.\n\nEach self-attention head comprises three fundamental components: queries Q, keys K, and values V . These elements facilitate the computation of attention scores that influence the weighting of the values. The process begins with normalizing the input X w via a layer normalization (LayerNorm):\n\nHere, q ht , k ht , and v ht denote the queries, keys, and values at time t for head h, derived from the normalized input. The matrices W Q , W K , and W V belong to R d×d H , where d H represents the dimension of each attention head. This normalization facilitates the efficient computation of attention scores.\n\nThe attention context vector c ht for each head is computed as a weighted sum of the values, with weights α htt ′ determined by the scaled dot-product attention mechanism. The alignment scores e htt ′ are calculated as:\n\nHere, t ′ denotes the different time steps considered. The softmax function converts these scores into a probability distribution, reflecting the importance of each value vector:\n\nwhere α htt ′ represents the normalized alignment scores, indicating the importance of each value vector v ht ′ in forming the context vector c ht for time step t. This context vector c ht captures the most relevant information from the values, highlighting key features at each time step.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Graph Neural Networks",
      "text": "GNNs represent a significant advancement in the domain of neural networks, designed explicitly to process graphstructured data. Unlike CNNs, GNNs excel in capturing the intricate relationships and dependencies among nodes within a graph through processes of aggregation and propagation of information across local neighbors  [18] . A typical graph is denoted as G = (V, E), where V symbolizes the set of nodes and E the set of edges. Each node v i ∈ V and edge e ij = (v i , v j ) ∈ E can be respectively associated with a node and an edge in the graph. The adjacency matrix A is configured as an n × n matrix where A ij = 1 if e ij ∈ E and A ij = 0 otherwise. Node attributes are represented by X, where X ∈ R n×d , with each x i ∈ R d denoting the feature vector of node v i .\n\nDGNNs extend the capabilities of GNNs to address dynamic or time-evolving graph-structured data. In DGNNs, a graph at any given time t is represented as G t = (V t , E t ), with its corresponding adjacency matrix A t . This matrix changes dynamically as nodes and edges are added or removed over time. Node features at time t are likewise dynamic, represented as X t with each row x t,i ∈ R d embodying the evolving feature set of node v i .\n\nThe computational heart of DGNNs lies in the dynamic update rules, where node representations h t,i are recurrently updated based on the temporal graph structure. A prevalent approach involves the temporal graph attention mechanism, where the new node states are computed as: Here, N (v i ) represents the neighbors of node v i , α\n\nt,ij are the attention coefficients indicating the significance of the features of neighbor j to node i, and W (k) and b (k) are trainable parameters of the k-th layer, with σ(•) being a nonlinear activation function.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we introduce the detailed architecture of AT-DGNN from the three aspects of data preprocessing, feature extraction and graph-based learning in turn, and the overall structure of AT-DGNN is shown in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Eeg Data Preprocessing",
      "text": "The initial sampling rate of the MEEG dataset is 1000 Hz. To align with standard datasets typically sampled at 128 Hz or 200 Hz, and to mitigate issues such as high artifact noise and overlapping interference signals, the data are downsampled to 200 Hz. This preprocessing phase involves the use of a band-pass filter with a range of 1 to 50 Hz to enhance the signal quality by minimizing interference. Subsequently, the EEG signals are segmented into five distinct frequency bands for feature extraction: Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-14 Hz), Beta (14-31 Hz), and Gamma (31-50 Hz).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Feature Extraction 1) Temporal Learner:",
      "text": "A temporal learner layer employing multiscale 1D temporal kernels (T kernels) is used to directly extract dynamic temporal representations from EEG data X i ∈ R E×T , where E denotes the number of EEG electrodes, and T is the sample length. These kernels obviate the need for manually extracted features. The i-th kernel's length (S i T ), dictated by the sampling frequency (f s ) of EEG data and scaling coefficients (α i ), can be represented by  [19] :\n\nEEG data processed through these layers yield dynamic time-frequency representations. An average pooling (AvgPool) layer, acting as a window function, calculates the averaged power across shorter segments. The logarithmic activation as described by  [7]  is applied to improve the performance. The output from each layer i, denoted as Z i temp , is formulated as:\n\nHere, Φ log (•) is the logarithmic activation function, Φ sqr (•) represents the square function, and F Conv1-D (•) signifies the 1D convolution operation.\n\nThe outputs across all kernels are concatenated along the feature dimension to yield the final output of the temporal learner layer Z T :\n\nwhere f bn (•) denotes the batch normalization operation.\n\n2) Sliding Window Segmentation: Following the temporal learner, a convolution-based sliding window technique as described in  [20]  is applied to segment the EEG time series X into multiple windows. This method integrates sliding window segmentation with convolutional operations, significantly reducing computational overhead by allowing convolutions to be executed once across all windows. This approach enhances data augmentation while accelerating processing through parallel computation. The time series X is segmented into windows X w ∈ R B×C×W using a window of length W and stride S, where w = 1, . . . , n represents the window index and n is the total number of windows. Each window X w is then processed by subsequent attention and temporal convolution blocks. The number of windows n is calculated as:\n\nHere, length, window size, and stride denote the total length of the time series, the length of each window, and the stride between windows, respectively.\n\n3) Multi-head Attention Module: Each subsequence X w derived from sliding window segmentation is processed as described in  (1) . The attention context vector c ht for each head is computed as a weighted sum of the values, with weights α htt ′ determined by the scaled dot-product attention mechanism as (2), and then used in  (3) .\n\nThe context vectors c 1t , . . . , c Ht from all heads are concatenated and linearly transformed to yield the final output of the MHA layer:\n\nwhere W O ∈ R d H ×d is the output projection matrix. This procedure projects the combined outputs back to the original input dimension. Subsequently, X ′ w is integrated with the original subsequence X w through a residual connection and normalized once more. This advanced MHA framework significantly enhances the model's capacity to discern intricate temporal patterns and dependencies within EEG signals, thereby augmenting the precision of decoding activities.\n\n4) Temporal Convolution: Following the MHA mechanism, the output X ′ w is normalized using LayerNorm. The normalized EEG data are denoted as X i ∈ R c×l , where c represents the number of EEG channels and l represents the temporal dimension's sample length.\n\nTo extract dynamic temporal features, a temporal convolutional block is applied to each window of the EEG data. The normalized EEG data X i serves as the input to this block. The temporal convolutional block processes X i to produce an output tensor Z w tcn ∈ R B×C×Tw , where T w is the temporal length of the window and w is the window index. Batch normalization is applied to ensure training stability, followed by a ReLU(•) activation function to introduce non-linearity. The output Z w tcn is computed as follows:\n\n5) Feature Fusion: The outputs of the temporal learner for each window, denoted as Z w tcn ∈ R B×C×Tw , are assembled into a four-dimensional tensor Z stacked :\n\nThis tensor is rearranged and flattened to meet the convolution layer's input requirements, resulting in dimensions (B, 32, -1), where 32 is the fixed channel count, and -1 represents the flattened dimensions. A fusion convolution layer with a kernel size of 3 and a stride of 1 integrates the outputs across all windows. The final output Z fused ∈ R B×32×L , where L is the combined length, captures global contextual information, allowing for a more discriminative representation of features in EEG signals, which is crucial for constructing nodes in GNNs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Graph-Based Learning",
      "text": "1) Graph Filtering Layer: The method described by  [16]  is adopted for the extracted features, involving the definition of functional areas and local filtering. Electrodes are divided into three functional areas: the general region (G g ), the frontal region (G f ), and the hemispheric region (G h ), as shown in Fig.  2 . Each channel is treated as a node, with the learned dynamic temporal representations considered as node attributes. In the preprocessing step, EEG channels are systematically reordered within predefined groups to ensure adjacency of channels within each local graph. This reordering enhances the effectiveness of localized graph-based operations applied in subsequent stages. Mathematically, the reordering of channel representations, denoted by Z i reorder , is defined by the function F reorder (•), which operates on the fused data from EEG channels at the i-th observation representations Z i fuse . This relationship is expressed as:\n\nAfter reordering, a graph filtering layer is employed to aggregate the transformed representations of EEG channels using a fully connected local adjacency matrix A local , where all elements are set to 1. The filtered output Z i filtered is obtained by applying a trainable filter matrix W local and a bias b local to the reordered data, followed by the application of the ReLU(•) activation function:\n\nThe element-wise product symbol • captures localized interactions within the brain.\n\nThe output of the filtering process, Z i local , is aggregated to form localized feature representations using the aggregation function F aggregate (•). The vector representation for each local graph is:\n\nwhere h 1 local , . . . , h R local are elements of the localized feature representation vector. These features Z i local are used to construct the overall feature matrix Z agg :\n\n2) Stacked DGNN: A stacked Dynamic Graph Neural Network (DGNN) captures complex graph relationships by recalculating the adjacency matrix at each layer based on input features, effectively capturing hierarchical feature variations.\n\nThe architecture incorporates multiple GNN layers. In each layer, features are grouped and aggregated into matrices, resulting in an aggregated feature matrix Z agg . The adjacency matrix for each layer is dynamically computed based on the feature similarity matrix S:\n\nSelf-loops are introduced by adding the identity matrix I to S, and the modified adjacency matrix A is normalized as:\n\nHere, D is the diagonal degree matrix of A, with each diagonal element Dii being the sum of the i-th row of A.\n\nDuring forward propagation, each DGNN layer processes the inputs using a normalized adjacency matrix Ãi , a weight matrix W i , and a bias vector b i for graph convolution. The outputs for each layer are calculated as:\n\nHere, n denotes the total number of layers in the DGNN, which is set to 3 in our study.\n\nThe final Output is computed through a sequence of operations including batch normalization F bn (•), dropout F dropout (•), and softmax activation Φ softmax (•), structured as:\n\nwhere Γ(•) denotes the flattening operation, ensuring that the network output is appropriately structured for subsequent processing or analysis. Finally, the procedure of AT-DGNN can be summarized in Algorithm 1.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Meeg Dataset",
      "text": "In studies on emotion induction, visual stimuli such as images, videos, and text are commonly employed in experiments. However, participants' responses to these stimuli may be influenced by their cultural backgrounds  [21] . Considering that the auditory cortex exhibits emotion-specific functional connections with a wide array of limbic, paralimbic, and neocortical structures, suggesting a more extensive role in emotion processing than previously understood  [22] , we opted to induce emotions in participants using music in the MEEG dataset. Under the guidance of music professors, we selected 20 lesser-known Western classical music clips by composers such as Shostakovich and Tchaikovsky. Each clip lasts one minute and is characterized by distinct levels of valence and arousal. To minimize emotional carryover, a 15-second pause was introduced between clips  [23] . The study involved 32 students from Shandong University, aged between 20 and 25 years, who had no formal music education and were unfamiliar with Western classical music. Each participant signed an informed consent form prior to the study. This experimental design aimed to minimize the influence of social and cultural backgrounds on emotional responses, ensuring that changes in emotions were solely due to musical stimuli, thereby enhancing the accuracy of the experimental results. EEG data were collected using a 32-channel BCI device NeuSen.W32 from Neuracle Tech, employing the same electrode channels as in the DEAP dataset  [4] . The data were sampled at a frequency of 1000 Hz. The EEG data were annotated based on the arousal and valence of the music during various stages of the experiment.\n\nIn contrast to the DEAP dataset, the MEEG dataset reduces experimental bias by rigorously selecting participants and musical stimuli, minimizing subjective factors and improving the effectiveness of emotional elicitation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Experiment Settings",
      "text": "To rigorously evaluate the model's performance, a nested cross-validation approach is employed, featuring a trial-wise 10-fold cross-validation in the outer loop and a 4-fold crossvalidation in the inner loop, as suggested by  Varma [24] . This stratified sampling technique ensures robustness and generalization of the model by assessing its accuracy and reliability across diverse samples.\n\nAdditionally, a two-stage training strategy within the inner loop optimizes the utilization of training data. Initially, the best model identified from the k-fold cross-validation is saved as a preliminary candidate. This model is subsequently refined with the aggregated data from all k folds, fine-tuned at a lower learning rate to avoid overfitting, and further trained for a maximum of 20 epochs or until it reaches 100% training accuracy, ensuring precise calibration. Importantly, test data is excluded from the training to preserve evaluation integrity.\n\nThe integration of two-stage training with 10-fold crossvalidation minimizes variability in assessment results, providing a comprehensive and reliable evaluation framework suitable for diverse research and application domains.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Implement Details",
      "text": "The model was implemented using PyTorch  [25]  library. Cross-entropy loss was chosen as the objective function to guide the training process. The training was divided into two stages, with the first stage capped at 200 epochs and the second at 20 epochs. To reduce training time and prevent overfitting, early stopping was implemented. For the attention module, the window size was set to half the f s . The kernel sizes for the time learner were set to 100, 50, and 25. Training was optimized using the Adam optimizer, starting with an initial learning rate of 1e -3, which was reduced by a factor of 10 during the second stage. For more information on the data processing, model implementation details, and accessing the dataset, please refer to our GitHub repository https://github.com/xmh1011/AT-DGNN.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "In this section, we compare the average ACC and F1 score of AT-DGNN on the MEEG dataset with CNN, TCN, and GNN-based SOTA methods in the BCI domain. The CNNbased methods include: EEGNet  [6] , TSception  [19] , Deep-ConvNet and ShallowConvNet  [7] . The TCN-based methods include: EEG-TCNet  [8] , TCNet-Fusion  [9]  and ATCNet  [20] . The GNN-based methods include: DGCNN  [13]  and LGGNet  [16] . Due to the smaller size of the CNN-based models, the learning rate and the number of training epochs are reduced to avoid overfitting. For the other models, the parameters recommended by the authors are used. Additionally, ablation studies are conducted to reveal the contribution of each component within the AT-DGNN architecture.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Emotion Recognition",
      "text": "As shown in Table  I , the AT-DGNN series, particularly the AT-DGNN-Gen model, significantly outperforms the previous state-of-the-art LGGNet series in both arousal and valence dimensions, with notable increases in F1 scores and accuracy. Specifically, the AT-DGNN-Gen model achieves the highest accuracy of 83.74% in the arousal dimension, surpassing the LGGNet-Gen model by 1.59%, and records the highest accuracy of 86.01% in the valence dimension, marking a substantial improvement of 1.08% over the LGGNet series. This improvement underscores the efficacy of our approach in capturing complex patterns within the MEEG dataset, which is known for its challenging and class-imbalanced nature.\n\nMoreover, the AT-DGNN-Gen model exemplifies the superior performance of the AT-DGNN series, achieving robust results with an average accuracy of 84.88% and an F1 score of 85.12% across emotion dimensions. This highlights the effectiveness of our graph-based approach in handling complex emotion recognition tasks. Furthermore, consistent with the findings of LGGNet  [16] , the graph configuration G g demonstrated superior performance in both AT-DGNN and LGGNet models compared to other graph definitions, indicating its optimal structure for capturing relevant features in emotion analysis.\n\nThe results also show a significant improvement over traditional CNN and TCN-based methods, with average gains in F1 scores and accuracy exceeding previous models by up to 7.16% and 5.78%, respectively. This highlights the enhanced capability of the DGNN approach in analyzing graphstructured data, offering more accurate and reliable emotion recognition. Additionally, the AT-DGNN exhibits a lower standard deviation compared to other models, indicating more balanced performance across various samples and superior generalization ability. The results for are derived from the optimal values among ten trials with distinct random seeds.\n\nOn both the MEEG and DEAP datasets, the AT-DGNN demonstrates strong generalization ability. All models achieve high accuracy on the MEEG dataset, with CNN and TCN models reaching 75% or higher, and GNN models exceeding 80%. Specifically, all models, including AT-DGNN, achieve significantly higher accuracy on the MEEG dataset than on the DEAP dataset, suggesting that music is more effective than video in evoking emotions and indicating the higher quality of the MEEG dataset. On the DEAP dataset, the AT-DGNN achieves an average accuracy of 60.56% across four dimensions, close to other SOTA models and surpassing some of them, which demonstrates the AT-DGNN model's strong generalization capability across different datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Ablation Study",
      "text": "As shown in Table  II , the ablation study systematically evaluates the contributions of key components in the AT-DGNN-Gen model, including sliding window segmentation (S), multi-head attention mechanism (A), graph neural network layers (G), and temporal learner layers (T). The baseline model, incorporating all components with three layers each for G and T, achieved the highest ACC of 84.88% and F1 score of 85.12%.\n\nExcluding the multi-head attention mechanism (A) caused a significant performance drop, with ACC decreasing by 6.57% and F1 score by 5.66%. Similarly, removing the sliding window segmentation (S) resulted in reductions of 3.99% in ACC and 3.96% in F1 score. These results confirm the essential roles of S and A in feature extraction and temporal modeling.\n\nVarying the number of G layers showed that using three layers yielded optimal results, while deviations caused performance declines. Increasing G layers to four resulted in a minor ACC drop of 1.71%, while reducing to one layer led to a larger decline of 2.97%. Similar trends were observed for T layers, where three layers provided the best balance, and exclusion of T caused a substantial drop of 10.24% in ACC and 10.28% in F1 score, highlighting its critical role in capturing temporal dynamics.\n\nOverall, the ablation study demonstrates that each component of the AT-DGNN-Gen model contributes significantly to its performance. The sliding window segmentation and multihead attention mechanism are crucial for effective feature extraction, while the optimal number of layers for the graph neural network and temporal learner is three, balancing model complexity and performance. These findings substantiate the design choices of our model and underscore the importance of the synergistic operation of its components in enhancing emotion recognition from EEG data. The results emphasize how the model's architecture is essential for capturing the complex spatiotemporal patterns inherent in EEG signals, thereby advancing computational neuroscience analysis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study introduces the MEEG dataset and the AT-DGNN framework, significantly advancing EEG-based emotion recognition. The MEEG dataset utilizes music to induce emotional states, providing a unique and critical resource for analyzing brain responses to emotional stimuli. The AT-DGNN framework captures the complex temporal dynamics of brain activity, enhancing emotion recognition accuracy beyond existing SOTA methods. Moreover, the MEEG dataset displays emotional states of subjects with greater precision, enabling SOTA methods to achieve higher accuracy and establishing a more robust benchmark for evaluating EEG-based emotion analysis models. These advancements propel BCI technology forward and facilitate new research into the relationships between music, emotion, and brain activity.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Structure of AT-DGNN. The AT-DGNN model comprises two core modules: a feature extraction module (a) and a dynamic graph neural network",
      "page": 3
    },
    {
      "caption": "Figure 1: A. EEG Data Preprocessing",
      "page": 3
    },
    {
      "caption": "Figure 2: Each channel is treated as a node, with the learned dy-",
      "page": 4
    },
    {
      "caption": "Figure 2: Local-global graph definitions [16]. (a) General definition Gg. (b)",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xiekang@stars.org.cn": "Recent\nadvances\nin\ndeep\nlearning\nhave\nsignificantly\nim-"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "pacted\nEEG signal\nprocessing.\nIn\n2018,\nLawhern\net\nal."
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "proposed EEGNet,\na\ncompact\nconvolutional neural network"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "(CNN)\ntailored\nfor EEG analysis\n[6]. Subsequent\ndevelop-"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "ments\nby\nSchirrmeister\net\nal.\nfurther\nrefined EEG decod-"
        },
        {
          "xiekang@stars.org.cn": "ing\ncapabilities\nusing\ndeep CNN architectures,\nintroducing"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "sophisticated\nvisualization\ntools\n[7]. The\nadoption\nof\ntem-"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "poral\nconvolutional networks\n(TCNs) has\nintroduced robust"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "alternatives for\nreal-time BCI applications. Notably,\nin 2020,"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "Ingolfsson et al. developed EEG-TCNet, which demonstrated"
        },
        {
          "xiekang@stars.org.cn": "superior\naccuracy\nin motor\nimagery\ntasks,\nsuggesting\nthat"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "TCNs could outperform traditional methods\nin specific BCI"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "contexts [8]. This assertion was corroborated by Musallam et"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "al., who demonstrated the versatility and efficiency of TCNs"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "in complex BCI\ntasks, particularly through the integration of"
        },
        {
          "xiekang@stars.org.cn": "TCNs in motor\nimagery classification [9]."
        },
        {
          "xiekang@stars.org.cn": "All\naforementioned\nstudies\ntreated EEG signals\nas\ntwo-"
        },
        {
          "xiekang@stars.org.cn": "dimensional\ntime-series data, with channels representing EEG"
        },
        {
          "xiekang@stars.org.cn": "electrodes positioned according to the 10-20 system to capture"
        },
        {
          "xiekang@stars.org.cn": ""
        },
        {
          "xiekang@stars.org.cn": "neural activity across brain regions [2, 10]. Recently, EEG data"
        },
        {
          "xiekang@stars.org.cn": "have been increasingly modeled as graphs,\nrepresenting the"
        },
        {
          "xiekang@stars.org.cn": "spatial arrangement of electrodes, where features\nfrom each"
        },
        {
          "xiekang@stars.org.cn": "electrode add a third dimension to the model\n[11, 12]."
        },
        {
          "xiekang@stars.org.cn": "In\n2019,\nSong\net\nal.\nintroduced\na\nnovel DGCNN for"
        },
        {
          "xiekang@stars.org.cn": "multichannel\nEEG emotion\nrecognition,\nutilizing\nan\nadja-"
        },
        {
          "xiekang@stars.org.cn": "cency matrix to dynamically model EEG channel relationships"
        },
        {
          "xiekang@stars.org.cn": "and enhance feature discrimination, demonstrating superiority"
        },
        {
          "xiekang@stars.org.cn": "over existing approaches [13]. Subsequently, advancements in"
        },
        {
          "xiekang@stars.org.cn": "GNNs\nhave\nbeen\nincreasingly\napplied\nto\nemotion\nrecogni-"
        },
        {
          "xiekang@stars.org.cn": "tion. Bao et\nal.\nintegrated a multi-layer GNN with a\nstyle-"
        },
        {
          "xiekang@stars.org.cn": "reconfigurable CNN [14], while Asadzadeh et al.\nfurther\nim-"
        },
        {
          "xiekang@stars.org.cn": "proved the DGCNN by incorporating Bayesian signal recovery"
        },
        {
          "xiekang@stars.org.cn": "techniques, both achieving enhanced performance [15]."
        },
        {
          "xiekang@stars.org.cn": "Building on prior advancements, greater progress was made"
        },
        {
          "xiekang@stars.org.cn": "in 2023 when Ding et al.\nintroduced the Local-Global-Graph"
        },
        {
          "xiekang@stars.org.cn": "Network (LGGNet) for processing EEG data in an image-like"
        },
        {
          "xiekang@stars.org.cn": "format,\nachieving improved classification performance\n[16]."
        },
        {
          "xiekang@stars.org.cn": "Alternatively, LGGNet’s\nsingle-layer\ngraph\nneural\nnetwork"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "signals. Additionally, widely used EEG datasets such as DEAP",
          "Here, qht, kht, and vht denote the queries, keys, and values": "t\nat\ntime\nfor\nhead h,\nderived\nfrom the\nnormalized\ninput."
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "and\nSEED primarily\ninvolve\nvisual\nstimuli, which\nlimits",
          "Here, qht, kht, and vht denote the queries, keys, and values": "The matrices W Q, W K, and W V\nbelong to Rd×dH , where"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "exploration of\nthe relationship between music, emotions, and",
          "Here, qht, kht, and vht denote the queries, keys, and values": "the\ndimension\nof\neach\nattention\nhead. This\ndH represents"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "EEG signals. Conversely, EEG datasets\nand research using",
          "Here, qht, kht, and vht denote the queries, keys, and values": "normalization facilitates the efficient computation of attention"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "musical stimuli are relatively scarce. Moreover, existing SOTA",
          "Here, qht, kht, and vht denote the queries, keys, and values": "scores."
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "methods achieve lower accuracy on the DEAP dataset,\nthus",
          "Here, qht, kht, and vht denote the queries, keys, and values": "for each head is computed\nThe attention context vector cht"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "limiting comprehensive performance evaluation of models.",
          "Here, qht, kht, and vht denote the queries, keys, and values": "as a weighted sum of the values, with weights αhtt′ determined"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "To address these limitations in emotion recognition research,",
          "Here, qht, kht, and vht denote the queries, keys, and values": "by the scaled dot-product attention mechanism. The alignment"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "the MEEG (Music EEG)\ndataset\nis\ndeveloped\naccordingly.",
          "Here, qht, kht, and vht denote the queries, keys, and values": "are calculated as:\nscores ehtt′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "This multi-modal\ndataset,\nsimilar\nto DEAP,\nutilizes music-",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "(qht)T kht′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "induced emotional states to improve the accuracy of emotion",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "√\n(2)\nehtt′ ="
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "recognition. By integrating a\nsliding window technique,\nan",
          "Here, qht, kht, and vht denote the queries, keys, and values": "dH"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "attention mechanism, and a multi-layer dynamic graph neural",
          "Here, qht, kht, and vht denote the queries, keys, and values": "t′\nHere,\ndenotes\nthe different\ntime\nsteps\nconsidered. The"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "network into the LGGNet\narchitecture,\nsignificant\nimprove-",
          "Here, qht, kht, and vht denote the queries, keys, and values": "softmax\nfunction\nconverts\nthese\nscores\ninto\na\nprobability"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "ments in classification accuracy are achieved.",
          "Here, qht, kht, and vht denote the queries, keys, and values": "distribution,\nreflecting the importance of each value vector:"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "The\ncontributions of our\nstudy are\nthreefold and can be",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "(3)\ncht =\nαhtt′vht′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "summarized as follows:",
          "Here, qht, kht, and vht denote the queries, keys, and values": "(cid:88) t"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "1) The MEEG dataset, a multi-modal EEG emotion dataset",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "represents\nthe normalized alignment\nscores,\nin-\nwhere αhtt′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "in the DEAP format,\nis enhanced with diverse music to",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "in forming\ndicating the importance of each value vector vht′"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "induce\nemotional\nstates\neffectively.\nIt outperforms\nthe",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "the\ncontext vector\nfor\ntime\nstep t. This\ncontext vector\ncht"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "DEAP dataset\nin emotion induction,\nimproving model",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "captures\nthe most\nrelevant\ninformation from the values,\ncht"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "accuracy (ACC) and F1 scores.",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "highlighting key features at each time step."
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "2) The AT-DGNN framework explores connections within",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "and\nbetween\nbrain\nfunctional\nareas. By\nintegrating\na",
          "Here, qht, kht, and vht denote the queries, keys, and values": "B. Graph Neural Networks"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "sliding window technique, an attention mechanism, and",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "GNNs\nrepresent\na\nsignificant\nadvancement\nin the domain"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "a\nstacked DGNN,\nthis novel\narchitecture\nfor\nemotion",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "of\nneural\nnetworks,\ndesigned\nexplicitly\nto\nprocess\ngraph-"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "recognition enhances\nthe learning of dynamic features",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "structured data. Unlike CNNs, GNNs excel\nin capturing the"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "in EEG recordings.",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "intricate relationships and dependencies among nodes within"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "3)\nPerformance of\nthe AT-DGNN is compared with other",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "a\ngraph\nthrough\nprocesses\nof\naggregation\nand\npropagation"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "CNN, TCN, and GNN-based SOTA methods on MEEG",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "of\ninformation across\nlocal neighbors\n[18]. A typical graph"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "dataset. Ablation experiments provide insights\ninto the",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "is denoted as G = (V, E), where V\nsymbolizes\nthe\nset of"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "AT-DGNN framework.",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "and edge\nnodes and E the set of edges. Each node vi ∈ V"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "be\nrespectively\nassociated with\na\neij = (vi, vj) ∈ E can"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "II. RELATED WORK",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "node\nand an edge\nin the graph. The\nadjacency matrix A is"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "A. Multi-head Attention Mechanism",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "configured as\nan n × n matrix where Aij = 1 if eij ∈ E"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "EEG data\nis\ninherently complex, with significant\ntempo-",
          "Here, qht, kht, and vht denote the queries, keys, and values": "and Aij = 0 otherwise. Node attributes are represented by X,"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "ral dependencies\nand discrete\nfeatures. Multi-head attention",
          "Here, qht, kht, and vht denote the queries, keys, and values": "where X ∈ Rn×d, with each xi ∈ Rd denoting the feature"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "(MHA) helps capture diverse temporal patterns and dependen-",
          "Here, qht, kht, and vht denote the queries, keys, and values": "vector of node vi."
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "DGNNs extend the capabilities of GNNs to address dynamic"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "cies, enabling the identification of key features. In this module,",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "or\ntime-evolving graph-structured data.\nIn DGNNs,\na graph"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "the attention block consists of a multi-head attention layer with",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "t\nat\nany given time\nis\nrepresented as Gt = (Vt, Et), with"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "several\nself-attention heads. As\nshown by Zhang et al.\n[17],",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "multi-head\nself-attention\neffectively\nintegrates multi-source",
          "Here, qht, kht, and vht denote the queries, keys, and values": "its corresponding adjacency matrix At. This matrix changes"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "dynamically as nodes and edges are added or\nremoved over"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "information,\nimproving\nthe model’s\nrobustness\nin\nhandling",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "time. Node features at time t are likewise dynamic, represented"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "complex temporal\nfeatures in EEG data,\nleading to enhanced",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "classification performance.",
          "Here, qht, kht, and vht denote the queries, keys, and values": "as Xt with each row xt,i ∈ Rd embodying the evolving feature"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "set of node vi."
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "Each self-attention head comprises three fundamental com-",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "The\ncomputational heart of DGNNs\nlies\nin the dynamic"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "ponents: queries Q, keys K, and values V . These elements",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "are recurrently\nupdate rules, where node representations ht,i"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "facilitate the computation of attention scores that\ninfluence the",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "updated based on the\ntemporal graph structure. A prevalent"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "weighting of the values. The process begins with normalizing",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "approach involves\nthe\ntemporal graph attention mechanism,"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "the input Xw via a layer normalization (LayerNorm):",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "where the new node states are computed as:"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "qht = W QLayerNorm(Xw,t)",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": " ",
          "Here, qht, kht, and vht denote the queries, keys, and values": " \n \n(cid:88)"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "(1)\nkht = W KLayerNorm(Xw,t)",
          "Here, qht, kht, and vht denote the queries, keys, and values": ""
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "h(k+1)\nα(k)"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "",
          "Here, qht, kht, and vht denote the queries, keys, and values": "= σ\n(4)\nt,ijW (k)h(k)\nt,j + b(k)\nt,i"
        },
        {
          "faces\nchallenges\nin capturing the dynamic\nfeatures of EEG": "vht = W V LayerNorm(Xw,t)",
          "Here, qht, kht, and vht denote the queries, keys, and values": "j∈N (vi)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "learning module (b). The feature extraction module consists of a temporal learner, a multi-head attention mechanism, and a temporal convolution module. These"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "components effectively leverage local\nfeatures of EEG signals through a sliding window technique,\nthereby enhancing the model’s capacity to dynamically"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "extract complex temporal patterns in EEG signals. In the graph-based learning module,\nthe model\ninitially employs local filtering layers to segment and filter"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "features from specific brain regions. Subsequently,\nthe architecture employs three layers of stacked dynamic graph convolutions to capture complex interactions"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "among different brain regions. This structure enhances the AT-DGNN’s capacity for\nintegrating temporal\nfeatures effectively."
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "B. Feature Extraction\nare\nHere, N (vi) represents the neighbors of node vi, α(k)"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "t,ij"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "the\nattention\ncoefficients\nindicating\nthe\nsignificance\nof\nthe"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "1) Temporal Learner: A temporal\nlearner\nlayer\nemploy-"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "j\nb(k)\nfeatures\nof\nneighbor\nto\nnode\ni,\nand W (k)\nand\nare"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "ing multiscale\n1D temporal\nkernels\n(T\nkernels)\nis\nused\nto"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "trainable parameters of\nthe k-th layer, with σ(·) being a non-"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "directly extract dynamic temporal\nrepresentations\nfrom EEG"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "linear activation function."
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "∈ RE×T , where E denotes\nthe\nnumber\nof EEG\ndata Xi"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "electrodes, and T is the sample length. These kernels obviate"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "III. METHODOLOGY"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "the need for manually extracted features. The\ni-th kernel’s"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "length (Si\nT ), dictated by the sampling frequency (fs) of EEG"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "In this section, we introduce the detailed architecture of AT-"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "data and scaling coefficients (αi), can be represented by [19]:"
        },
        {
          "Fig. 1: Structure of AT-DGNN. The AT-DGNN model comprises\ntwo core modules: a feature extraction module (a) and a dynamic graph neural network": "DGNN from the three aspects of data preprocessing,\nfeature"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The outputs across all kernels are concatenated along the": "feature dimension to yield the final output of\nthe\ntemporal",
          "length\nof\nthe window and w is\nthe window index. Batch": "normalization is applied to ensure training stability,\nfollowed"
        },
        {
          "The outputs across all kernels are concatenated along the": "learner\nlayer ZT :",
          "length\nof\nthe window and w is\nthe window index. Batch": "by a ReLU(·)\nactivation function to introduce non-linearity."
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "The output Z w\nis computed as follows:\ntcn"
        },
        {
          "The outputs across all kernels are concatenated along the": "(7)\nZT = fbn(Z 1\ntemp, Z 2\ntemp, Z 3\ntemp)",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "where fbn(·) denotes the batch normalization operation.",
          "length\nof\nthe window and w is\nthe window index. Batch": "Z w\n(10)\ntcn = ReLU(BatchNorm(Conv1d(Xi)))"
        },
        {
          "The outputs across all kernels are concatenated along the": "2)\nSliding Window Segmentation:\nFollowing the temporal",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "5) Feature Fusion: The outputs of the temporal\nlearner for"
        },
        {
          "The outputs across all kernels are concatenated along the": "learner,\na\nconvolution-based\nsliding window technique\nas",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "each window, denoted as Z w\nare\nassembled\ntcn ∈ RB×C×Tw ,"
        },
        {
          "The outputs across all kernels are concatenated along the": "described in [20]\nis applied to segment\nthe EEG time series",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "into a four-dimensional\ntensor Zstacked:"
        },
        {
          "The outputs across all kernels are concatenated along the": "X into multiple windows. This method integrates sliding win-",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "dow segmentation with convolutional operations, significantly",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "reducing computational overhead by allowing convolutions to",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "(11)\nZstacked = [Z 1\ntcn, Z 2\ntcn, . . . , ZW\ntcn ] ∈ RB×C×W ×Tw"
        },
        {
          "The outputs across all kernels are concatenated along the": "be executed once across all windows. This approach enhances",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "data augmentation while accelerating processing through par-",
          "length\nof\nthe window and w is\nthe window index. Batch": "This\ntensor\nis\nrearranged and flattened to meet\nthe\ncon-"
        },
        {
          "The outputs across all kernels are concatenated along the": "allel computation. The time series X is segmented into win-",
          "length\nof\nthe window and w is\nthe window index. Batch": "volution layer’s\ninput\nrequirements,\nresulting in dimensions"
        },
        {
          "The outputs across all kernels are concatenated along the": "dows Xw ∈ RB×C×W using a window of length W and stride",
          "length\nof\nthe window and w is\nthe window index. Batch": "32\n(B, 32, −1), where\nis\nthe fixed\nchannel\ncount,\nand −1"
        },
        {
          "The outputs across all kernels are concatenated along the": "S, where w = 1, . . . , n represents\nthe window index and n",
          "length\nof\nthe window and w is\nthe window index. Batch": "represents the flattened dimensions. A fusion convolution layer"
        },
        {
          "The outputs across all kernels are concatenated along the": "is\nthe\nthen\ntotal number of windows. Each window Xw is",
          "length\nof\nthe window and w is\nthe window index. Batch": "with a kernel size of 3 and a stride of 1 integrates the outputs"
        },
        {
          "The outputs across all kernels are concatenated along the": "processed by subsequent attention and temporal convolution",
          "length\nof\nthe window and w is\nthe window index. Batch": "∈ RB×32×L,"
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "across\nall windows. The\nfinal\noutput Zfused"
        },
        {
          "The outputs across all kernels are concatenated along the": "blocks. The number of windows n is calculated as:",
          "length\nof\nthe window and w is\nthe window index. Batch": "where L is\nthe combined length, captures global contextual"
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "information, allowing for a more discriminative representation"
        },
        {
          "The outputs across all kernels are concatenated along the": "(cid:23)",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "(cid:22) length − window size",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "+ 1\nn =\n(8)",
          "length\nof\nthe window and w is\nthe window index. Batch": "of\nfeatures\nin EEG signals, which is crucial\nfor constructing"
        },
        {
          "The outputs across all kernels are concatenated along the": "stride",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "nodes in GNNs."
        },
        {
          "The outputs across all kernels are concatenated along the": "Here, length, window size, and stride denote the total length",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "C. Graph-based Learning"
        },
        {
          "The outputs across all kernels are concatenated along the": "of\nthe time series,\nthe length of each window, and the stride",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "between windows,\nrespectively.",
          "length\nof\nthe window and w is\nthe window index. Batch": "1) Graph Filtering Layer: The method described by [16]"
        },
        {
          "The outputs across all kernels are concatenated along the": "3) Multi-head Attention Module:\nEach\nsubsequence Xw",
          "length\nof\nthe window and w is\nthe window index. Batch": "is adopted for\nthe extracted features,\ninvolving the definition"
        },
        {
          "The outputs across all kernels are concatenated along the": "derived\nfrom sliding window segmentation\nis\nprocessed\nas",
          "length\nof\nthe window and w is\nthe window index. Batch": "of\nfunctional areas and local filtering. Electrodes are divided"
        },
        {
          "The outputs across all kernels are concatenated along the": "described in (1). The\nattention context vector\nfor\neach\ncht",
          "length\nof\nthe window and w is\nthe window index. Batch": "into three functional areas:\nthe frontal\nthe general region (Gg),"
        },
        {
          "The outputs across all kernels are concatenated along the": "head\nis\ncomputed\nas\na weighted\nsum of\nthe\nvalues, with",
          "length\nof\nthe window and w is\nthe window index. Batch": "shown in\nregion (Gf ), and the hemispheric region (Gh), as"
        },
        {
          "The outputs across all kernels are concatenated along the": "weights αhtt′ determined by the scaled dot-product attention",
          "length\nof\nthe window and w is\nthe window index. Batch": "Fig. 2. Each channel\nis treated as a node, with the learned dy-"
        },
        {
          "The outputs across all kernels are concatenated along the": "mechanism as (2), and then used in (3).",
          "length\nof\nthe window and w is\nthe window index. Batch": "namic temporal\nrepresentations considered as node attributes."
        },
        {
          "The outputs across all kernels are concatenated along the": "The context vectors c1t, . . . , cHt from all heads are concate-",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "nated and linearly transformed to yield the final output of the",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "MHA layer:",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "X′\n(9)\nw = W O[c1t, . . . , cHt] + Xw,",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "where W O ∈ RdH ×d\nis\nthe output projection matrix. This",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "procedure\nprojects\nthe\ncombined\noutputs\nback\nto\nthe\norig-",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "inal\ninput\ndimension. Subsequently, X ′\nis\nintegrated with\nw",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "the original\nresidual\nconnection\nsubsequence Xw through a",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "and normalized once more. This advanced MHA framework",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "Fig. 2: Local-global graph definitions\n[16].\n(b)\n(a) General definition Gg."
        },
        {
          "The outputs across all kernels are concatenated along the": "significantly enhances\nthe model’s\ncapacity to discern intri-",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "Frontal definition Gf .\n(c) Hemispheric definition Gh."
        },
        {
          "The outputs across all kernels are concatenated along the": "cate temporal patterns and dependencies within EEG signals,",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "thereby augmenting the precision of decoding activities.",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "In the preprocessing step, EEG channels are systematically"
        },
        {
          "The outputs across all kernels are concatenated along the": "4) Temporal Convolution: Following the MHA mechanism,",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "reordered within\npredefined\ngroups\nto\nensure\nadjacency\nof"
        },
        {
          "The outputs across all kernels are concatenated along the": "the output X′\nw is normalized using LayerNorm. The normalized",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "channels within each local graph. This reordering enhances the"
        },
        {
          "The outputs across all kernels are concatenated along the": "c\nEEG data\nare\ndenoted\nrepresents\nas Xi ∈ Rc×l, where",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "effectiveness of\nlocalized graph-based operations\napplied in"
        },
        {
          "The outputs across all kernels are concatenated along the": "the number of EEG channels\nand l\nrepresents\nthe\ntemporal",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "subsequent stages. Mathematically,\nthe reordering of channel"
        },
        {
          "The outputs across all kernels are concatenated along the": "dimension’s sample length.",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "is defined by the\nfunc-\nrepresentations, denoted by Z i\nreorder,"
        },
        {
          "The outputs across all kernels are concatenated along the": "To extract dynamic temporal\nfeatures, a temporal convolu-",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "tion Freorder(·), which operates on the fused data from EEG"
        },
        {
          "The outputs across all kernels are concatenated along the": "tional block is applied to each window of\nthe EEG data. The",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "channels\nat\nthe\ni-th\nobservation\nrepresentations Z i\nfuse. This"
        },
        {
          "The outputs across all kernels are concatenated along the": "serves\nas\nthe\ninput\nto this block.\nnormalized EEG data Xi",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "",
          "length\nof\nthe window and w is\nthe window index. Batch": "relationship is expressed as:"
        },
        {
          "The outputs across all kernels are concatenated along the": "to produce an\nThe temporal convolutional block processes Xi",
          "length\nof\nthe window and w is\nthe window index. Batch": ""
        },
        {
          "The outputs across all kernels are concatenated along the": "output\ntensor Z w\nthe\ntemporal\ntcn ∈ RB×C×Tw , where Tw is",
          "length\nof\nthe window and w is\nthe window index. Batch": "Z i\n(12)\nreorder = Freorder(Z i\nfuse)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "aggregate\nthe\ntransformed representations of EEG channels",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "which is set\nto 3 in our study."
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "using a fully connected local adjacency matrix Alocal, where",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "The final Output\nis computed through a sequence of opera-"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "all elements are set to 1. The filtered output Z i\nfiltered is obtained",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "tions including batch normalization Fbn(·), dropout Fdropout(·),"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "to\nby applying a trainable filter matrix Wlocal and a bias blocal",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "and softmax activation Φsoftmax(·), structured as:"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "the reordered data, followed by the application of the ReLU(·)",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": ""
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "(19)\nOutput = Φsoftmax(Fdropout(Γ(Fbn(H (L)))))"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "activation function:",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": ""
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "where Γ(·)\ndenotes\nthe\nflattening\noperation,\nensuring\nthat"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "Z i\n(13)\nfiltered = ReLU(Wlocal ◦ Z i\nreorder + blocal)",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": ""
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "the network output\nis appropriately structured for subsequent"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "The element-wise product\nsymbol ◦ captures\nlocalized in-",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "processing or analysis."
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "teractions within the brain.",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "Finally,\nthe procedure of AT-DGNN can be summarized in"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "The output of\nthe filtering process, Z i\nis aggregated to\nlocal,",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "Algorithm 1."
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "form localized feature\nrepresentations using the\naggregation",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": ""
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "representation for each local\nfunction Faggregate(·). The vector",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "Algorithm 1 AT-DGNN"
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "graph is:",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": ""
        },
        {
          "After\nreordering,\na\ngraph filtering\nlayer\nis\nemployed\nto": "",
          "Here, n denotes\nthe total number of\nlayers\nin the DGNN,": "1:\nInput: EEG data Xi ∈ RE×T , ground truth label y"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "teractions within the brain.": "The output of\nthe filtering process, Z i\nis aggregated to\nlocal,",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "Algorithm 1."
        },
        {
          "teractions within the brain.": "form localized feature\nrepresentations using the\naggregation",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "representation for each local\nfunction Faggregate(·). The vector",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "Algorithm 1 AT-DGNN"
        },
        {
          "teractions within the brain.": "graph is:",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "1:\nInput: EEG data Xi ∈ RE×T , ground truth label y"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "2: Output: pred,\nthe prediction of AT-DGNN"
        },
        {
          "teractions within the brain.": "h1",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": " \n ",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "3:\nInitialization;"
        },
        {
          "teractions within the brain.": ".",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "Z i",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": ".\n(14)\nlocal = Faggregate(Z i\nfiltered) =\n.",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "▷ Feature Extraction"
        },
        {
          "teractions within the brain.": "hR",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "4:\nfor i ← 1 to 3 do"
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "5:\nget\nith temporal kernel size by (5);"
        },
        {
          "teractions within the brain.": "where h1\nare\nelements of\nthe\nlocalized feature\nlocal, . . . , hR",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "get Z i"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "6:\ntemp by (6) using Xi as input;"
        },
        {
          "teractions within the brain.": "representation vector. These\nfeatures Z i\nare used to con-\nlocal",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "7:\nend for"
        },
        {
          "teractions within the brain.": "struct\nthe overall\nfeature matrix Zagg:",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "by (7);\n8: get ZT"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "9: get\nthe window size n and segment\nthe data by (8):"
        },
        {
          "teractions within the brain.": "Z 1",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "10:\nfor w ← 1 to n do"
        },
        {
          "teractions within the brain.": " \n \nZ 2",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "11:\nget window Xw ∈ RB×E×W ;"
        },
        {
          "teractions within the brain.": "(15)\nZagg =\n.",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": ".",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": ".",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "12:\nnormalize Xw using LayerNorm;"
        },
        {
          "teractions within the brain.": "Z 11",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "local",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "13:\nprocess Xw with MHA to get X ′\nw by (9);"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "normalize X ′"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "14:\nw using LayerNorm;"
        },
        {
          "teractions within the brain.": "2)\nStacked DGNN:\nA stacked Dynamic Graph Neural",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "15:\ntcn by (10);\nw to get Z w"
        },
        {
          "teractions within the brain.": "Network\n(DGNN)\ncaptures\ncomplex\ngraph\nrelationships\nby",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "16:\nadd Z w\nto Zstacked;"
        },
        {
          "teractions within the brain.": "recalculating the adjacency matrix at each layer based on input",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "tcn"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "17:\nend for"
        },
        {
          "teractions within the brain.": "features, effectively capturing hierarchical\nfeature variations.",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "18:\napply Fusion Conv to Zstacked\nto get Zfused by (11);"
        },
        {
          "teractions within the brain.": "The architecture incorporates multiple GNN layers. In each",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "▷ Graph-based Learning"
        },
        {
          "teractions within the brain.": "layer,\nfeatures\nare\ngrouped\nand\naggregated\ninto matrices,",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "19: perform graph filtering and aggregation on each node by"
        },
        {
          "teractions within the brain.": "resulting in an aggregated feature matrix Zagg. The adjacency",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "(12)\n-\n(15)\nto get Z i\nlocal;"
        },
        {
          "teractions within the brain.": "matrix for each layer\nis dynamically computed based on the",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "20:\nfor i ← 1 to 3 do"
        },
        {
          "teractions within the brain.": "feature similarity matrix S:",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "21:\nget DGNN output by (18);"
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "22:\nend for"
        },
        {
          "teractions within the brain.": "(16)\nS = Zagg · Z T\nagg",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        },
        {
          "teractions within the brain.": "",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "23: get pred by (19);"
        },
        {
          "teractions within the brain.": "Self-loops are introduced by adding the identity matrix I to",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": "24: Return pred"
        },
        {
          "teractions within the brain.": "S, and the modified adjacency matrix A is normalized as:",
          "Finally,\nthe procedure of AT-DGNN can be summarized in": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "20 lesser-known Western classical music clips by composers": "such as Shostakovich and Tchaikovsky. Each clip lasts one",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "factor of 10 during the second stage. For more information"
        },
        {
          "20 lesser-known Western classical music clips by composers": "minute and is characterized by distinct\nlevels of valence and",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "on\nthe\ndata\nprocessing, model\nimplementation\ndetails,\nand"
        },
        {
          "20 lesser-known Western classical music clips by composers": "arousal. To minimize emotional carryover, a 15-second pause",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "accessing the dataset, please\nrefer\nto our GitHub repository"
        },
        {
          "20 lesser-known Western classical music clips by composers": "was\nintroduced\nbetween\nclips\n[23]. The\nstudy\ninvolved\n32",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "https://github.com/xmh1011/AT-DGNN."
        },
        {
          "20 lesser-known Western classical music clips by composers": "students from Shandong University, aged between 20 and 25",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "years, who had no formal music education and were unfamiliar",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "V. RESULTS AND DISCUSSION"
        },
        {
          "20 lesser-known Western classical music clips by composers": "with Western\nclassical music.\nEach\nparticipant\nsigned\nan",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "In this section, we compare the average ACC and F1 score"
        },
        {
          "20 lesser-known Western classical music clips by composers": "informed consent\nform prior\nto the study. This experimental",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "of AT-DGNN on the MEEG dataset with CNN, TCN,\nand"
        },
        {
          "20 lesser-known Western classical music clips by composers": "design aimed to minimize the influence of social and cultural",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "GNN-based SOTA methods\nin the BCI domain. The CNN-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "backgrounds on emotional\nresponses,\nensuring that\nchanges",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "based methods\ninclude: EEGNet\n[6], TSception [19], Deep-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "in\nemotions were\nsolely\ndue\nto musical\nstimuli,\nthereby",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "ConvNet and ShallowConvNet\n[7]. The TCN-based methods"
        },
        {
          "20 lesser-known Western classical music clips by composers": "enhancing the accuracy of the experimental results. EEG data",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "include: EEG-TCNet [8], TCNet-Fusion [9] and ATCNet [20]."
        },
        {
          "20 lesser-known Western classical music clips by composers": "were collected using a 32-channel BCI device NeuSen.W32",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "The GNN-based methods include: DGCNN [13] and LGGNet"
        },
        {
          "20 lesser-known Western classical music clips by composers": "from Neuracle Tech, employing the same electrode channels",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "[16]. Due to the smaller\nsize of\nthe CNN-based models,\nthe"
        },
        {
          "20 lesser-known Western classical music clips by composers": "as\nin\nthe DEAP dataset\n[4]. The\ndata were\nsampled\nat\na",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "learning rate and the number of training epochs are reduced to"
        },
        {
          "20 lesser-known Western classical music clips by composers": "frequency of 1000 Hz. The EEG data were annotated based",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "avoid overfitting. For the other models,\nthe parameters recom-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "on the arousal and valence of the music during various stages",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "mended by the authors are used. Additionally, ablation studies"
        },
        {
          "20 lesser-known Western classical music clips by composers": "of\nthe experiment.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "are conducted to reveal\nthe contribution of each component"
        },
        {
          "20 lesser-known Western classical music clips by composers": "In contrast\nto the DEAP dataset,\nthe MEEG dataset reduces",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "within the AT-DGNN architecture."
        },
        {
          "20 lesser-known Western classical music clips by composers": "experimental\nbias\nby\nrigorously\nselecting\nparticipants\nand",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "musical stimuli, minimizing subjective factors and improving",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "A. Emotion Recognition"
        },
        {
          "20 lesser-known Western classical music clips by composers": "the effectiveness of emotional elicitation.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "As shown in Table I,\nthe AT-DGNN series, particularly the"
        },
        {
          "20 lesser-known Western classical music clips by composers": "B. Experiment Settings",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "AT-DGNN-Gen model, significantly outperforms the previous"
        },
        {
          "20 lesser-known Western classical music clips by composers": "To rigorously evaluate the model’s performance, a nested",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "state-of-the-art LGGNet\nseries\nin\nboth\narousal\nand\nvalence"
        },
        {
          "20 lesser-known Western classical music clips by composers": "cross-validation approach is employed,\nfeaturing a trial-wise",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "dimensions, with notable increases in F1 scores and accuracy."
        },
        {
          "20 lesser-known Western classical music clips by composers": "10-fold cross-validation in the outer\nloop and a 4-fold cross-",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "Specifically,\nthe AT-DGNN-Gen model\nachieves\nthe highest"
        },
        {
          "20 lesser-known Western classical music clips by composers": "validation\nin\nthe\ninner\nloop,\nas\nsuggested\nby Varma\n[24].",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "accuracy\nof\n83.74% in\nthe\narousal\ndimension,\nsurpassing"
        },
        {
          "20 lesser-known Western classical music clips by composers": "This\nstratified\nsampling\ntechnique\nensures\nrobustness\nand",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "the LGGNet-Gen model by 1.59%,\nand records\nthe highest"
        },
        {
          "20 lesser-known Western classical music clips by composers": "generalization\nof\nthe model\nby\nassessing\nits\naccuracy\nand",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "accuracy\nof\n86.01% in\nthe\nvalence\ndimension, marking\na"
        },
        {
          "20 lesser-known Western classical music clips by composers": "reliability across diverse samples.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "substantial\nimprovement of 1.08% over\nthe LGGNet\nseries."
        },
        {
          "20 lesser-known Western classical music clips by composers": "Additionally, a two-stage training strategy within the inner",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "This improvement underscores the efficacy of our approach in"
        },
        {
          "20 lesser-known Western classical music clips by composers": "loop optimizes the utilization of training data. Initially, the best",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "capturing complex patterns within the MEEG dataset, which"
        },
        {
          "20 lesser-known Western classical music clips by composers": "model\nidentified from the k-fold cross-validation is\nsaved as",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "is known for\nits challenging and class-imbalanced nature."
        },
        {
          "20 lesser-known Western classical music clips by composers": "a preliminary candidate. This model\nis\nsubsequently refined",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "Moreover,\nthe AT-DGNN-Gen model exemplifies the supe-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "with\nthe\naggregated\ndata\nfrom all k\nfolds, fine-tuned\nat\na",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "rior performance of\nthe AT-DGNN series,\nachieving robust"
        },
        {
          "20 lesser-known Western classical music clips by composers": "lower\nlearning rate\nto avoid overfitting,\nand further\ntrained",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "results with an average accuracy of 84.88% and an F1 score"
        },
        {
          "20 lesser-known Western classical music clips by composers": "for a maximum of 20 epochs or until\nit reaches 100% training",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "of\n85.12% across\nemotion\ndimensions. This\nhighlights\nthe"
        },
        {
          "20 lesser-known Western classical music clips by composers": "accuracy, ensuring precise calibration. Importantly,\ntest data is",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "effectiveness of our graph-based approach in handling complex"
        },
        {
          "20 lesser-known Western classical music clips by composers": "excluded from the training to preserve evaluation integrity.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "emotion recognition tasks. Furthermore,\nconsistent with the"
        },
        {
          "20 lesser-known Western classical music clips by composers": "The\nintegration of\ntwo-stage\ntraining with 10-fold cross-",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "findings of LGGNet [16],\nthe graph configuration Gg demon-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "validation minimizes\nvariability\nin\nassessment\nresults,\npro-",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "strated superior performance in both AT-DGNN and LGGNet"
        },
        {
          "20 lesser-known Western classical music clips by composers": "viding\na\ncomprehensive\nand\nreliable\nevaluation\nframework",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "models\ncompared\nto\nother\ngraph\ndefinitions,\nindicating\nits"
        },
        {
          "20 lesser-known Western classical music clips by composers": "suitable for diverse research and application domains.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "optimal\nstructure\nfor\ncapturing relevant\nfeatures\nin emotion"
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "analysis."
        },
        {
          "20 lesser-known Western classical music clips by composers": "C.\nImplement Details",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": ""
        },
        {
          "20 lesser-known Western classical music clips by composers": "",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "The results also show a significant\nimprovement over\ntra-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "The model was implemented using PyTorch [25]\nlibrary.",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "ditional CNN and TCN-based methods, with average gains"
        },
        {
          "20 lesser-known Western classical music clips by composers": "Cross-entropy loss was chosen as the objective function to",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "in F1 scores and accuracy exceeding previous models by up"
        },
        {
          "20 lesser-known Western classical music clips by composers": "guide the training process. The training was divided into two",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "to\n7.16% and\n5.78%,\nrespectively. This\nhighlights\nthe\nen-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "stages, with the first stage capped at 200 epochs and the second",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "hanced capability of the DGNN approach in analyzing graph-"
        },
        {
          "20 lesser-known Western classical music clips by composers": "at 20 epochs. To reduce training time and prevent overfitting,",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "structured data, offering more accurate and reliable emotion"
        },
        {
          "20 lesser-known Western classical music clips by composers": "early stopping was\nimplemented. For\nthe\nattention module,",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "recognition. Additionally,\nthe AT-DGNN exhibits\na\nlower"
        },
        {
          "20 lesser-known Western classical music clips by composers": "the window size was\nset\nto\nhalf\nthe\nkernel\nsizes\nfs. The",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "standard deviation compared to other models,\nindicating more"
        },
        {
          "20 lesser-known Western classical music clips by composers": "for\nthe\ntime\nlearner were\nset\nto 100, 50,\nand 25. Training",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "balanced\nperformance\nacross\nvarious\nsamples\nand\nsuperior"
        },
        {
          "20 lesser-known Western classical music clips by composers": "was\noptimized\nusing\nthe Adam optimizer,\nstarting with\nan",
          "initial\nlearning\nrate\nof\n1e − 3, which was\nreduced\nby\na": "generalization ability."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "MEEG Valence"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "F1 (%)"
        },
        {
          "TABLE I": "73.33∗∗"
        },
        {
          "TABLE I": "73.03∗∗"
        },
        {
          "TABLE I": "80.22∗∗"
        },
        {
          "TABLE I": "81.18∗∗"
        },
        {
          "TABLE I": "54.28∗∗∗"
        },
        {
          "TABLE I": "75.42∗∗"
        },
        {
          "TABLE I": "82.08∗∗"
        },
        {
          "TABLE I": "82.40∗∗"
        },
        {
          "TABLE I": "83.87∗∗"
        },
        {
          "TABLE I": "84.38∗∗"
        },
        {
          "TABLE I": "84.46∗∗"
        },
        {
          "TABLE I": "85.61∗∗"
        },
        {
          "TABLE I": "85.68∗"
        },
        {
          "TABLE I": "85.35∗∗"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "82.60\n3\n2"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "✓\n✓"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "of\nthem, which demonstrates\nthe AT-DGNN model’s\nstrong",
          "✓\n✓": "82.89\n3\n4"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "✓\n✓\n✗"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "74.64\n3"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "generalization capability across different datasets.",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "✓\n✓"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "84.88\n3\n3"
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "B. Ablation Study",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "S: Sliding window segmentation."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "A: Multi-head attention mechanism."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "As\nshown\nin Table\nII,\nthe\nablation\nstudy\nsystematically",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "G: Graph neural network."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "evaluates\nthe\ncontributions\nof\nkey\ncomponents\nin\nthe AT-",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "T: Temporal\nlearner."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "DGNN-Gen model,\nincluding\nsliding window segmentation",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": "✓: Keep the component."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "(S), multi-head attention mechanism (A), graph neural network",
          "✓\n✓": "✗: Exclude the component."
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "layers\n(G),\nand\ntemporal\nlearner\nlayers\n(T). The\nbaseline",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "model,\nincorporating all\ncomponents with three\nlayers\neach",
          "✓\n✓": ""
        },
        {
          "dimensions, close to other SOTA models and surpassing some": "for G and T,\nachieved the highest ACC of 84.88% and F1",
          "✓\n✓": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "TABLE II\nOn\nboth\nthe MEEG and DEAP datasets,\nthe AT-DGNN"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "ABLATION STUDY ON EMOTION OF MEEG USING AT-DGNN-GEN."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "demonstrates strong generalization ability. All models achieve"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "high accuracy on the MEEG dataset, with CNN and TCN"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "S\nA\nG\nT\nACC(%)\nChanges(%)\nF1(%)\nChanges(%)"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "models reaching 75% or higher, and GNN models exceeding"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✗"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "78.31\n-6.57\n79.46\n-5.66\n3\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "80%. Specifically,\nall models,\nincluding AT-DGNN,\nachieve"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✗\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "80.89\n-3.99\n81.16\n-3.96\n3\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "significantly higher\naccuracy on the MEEG dataset\nthan on"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "81.91\n-2.97\n81.81\n-3.31\n1\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "the DEAP dataset,\nsuggesting\nthat music\nis more\neffective"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "82.94\n-1.94\n82.52\n-2.60\n2\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "than\nvideo\nin\nevoking\nemotions\nand\nindicating\nthe\nhigher\n✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "83.17\n-1.71\n83.48\n-1.64\n4\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓\n✗\nquality of\nthe MEEG dataset. On the DEAP dataset,\nthe AT-"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "83.05\n-1.83\n83.89\n-1.23\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "DGNN achieves an average accuracy of 60.56% across\nfour\n✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "82.95\n-1.93\n82.84\n-2.28\n3\n1"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓\ndimensions, close to other SOTA models and surpassing some"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "82.60\n-2.28\n81.91\n-3.21\n3\n2"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "82.89\n-1.99\n83.43\n-1.69\n3\n4\nof\nthem, which demonstrates\nthe AT-DGNN model’s\nstrong"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓\n✗"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "74.64\n-10.24\n74.84\n-10.28\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "generalization capability across different datasets."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓\n✓"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "84.88\n85.12\n-\n-\n3\n3"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "B. Ablation Study"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "S: Sliding window segmentation."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "A: Multi-head attention mechanism."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "As\nshown\nin Table\nII,\nthe\nablation\nstudy\nsystematically"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "G: Graph neural network."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "evaluates\nthe\ncontributions\nof\nkey\ncomponents\nin\nthe AT-"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "T: Temporal\nlearner."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "DGNN-Gen model,\nincluding\nsliding window segmentation"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "✓: Keep the component."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "(S), multi-head attention mechanism (A), graph neural network\n✗: Exclude the component."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "1, 2, 3, 4: The number of DGNN layers or\ntemporal\nlearner.\nlayers\n(G),\nand\ntemporal\nlearner\nlayers\n(T). The\nbaseline"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "Changes: Compared with the baseline AT-DGNN-Gen."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "model,\nincorporating all\ncomponents with three\nlayers\neach"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "for G and T,\nachieved the highest ACC of 84.88% and F1"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "score of 85.12%."
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "where three layers provided the best balance, and exclusion of\nExcluding the multi-head attention mechanism (A) caused"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "T caused a substantial drop of 10.24% in ACC and 10.28%\na\nsignificant\nperformance\ndrop, with ACC decreasing\nby"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "in F1 score, highlighting its critical role in capturing temporal\n6.57% and F1 score by 5.66%. Similarly, removing the sliding"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "dynamics.\nwindow segmentation\n(S)\nresulted\nin\nreductions\nof\n3.99%"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "in ACC and 3.96% in F1 score. These\nresults\nconfirm the\nOverall,\nthe ablation study demonstrates that each compo-"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "essential\nroles of S and A in feature extraction and temporal\nnent of\nthe AT-DGNN-Gen model contributes significantly to"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "modeling.\nits performance. The sliding window segmentation and multi-"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "Varying the number of G layers\nshowed that using three\nhead\nattention mechanism are\ncrucial\nfor\neffective\nfeature"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "layers yielded optimal results, while deviations caused perfor-\nextraction, while the optimal number of\nlayers\nfor\nthe graph"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "mance declines. Increasing G layers to four resulted in a minor\nneural network and temporal\nlearner is three, balancing model"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "complexity and performance. These findings\nsubstantiate the\nACC drop of 1.71%, while reducing to one layer led to a larger"
        },
        {
          "The results for are derived from the optimal values among ten trials with distinct\nrandom seeds.": "decline of 2.97%. Similar\ntrends were observed for T layers,\ndesign choices of our model and underscore the importance"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Man, and Cybernetics (SMC).\nIEEE, 2020, pp. 2958–2965."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "emotion recognition from EEG data. The\nresults\nemphasize",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[9] Y. K. Musallam, N.\nI. AlFassam, G. Muhammad, S. U. Amin,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "how the model’s\narchitecture\nis\nessential\nfor\ncapturing\nthe",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "M. Alsulaiman, W. Abdul, H. Altaheri, M. A. Bencherif,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "complex\nspatiotemporal\npatterns\ninherent\nin EEG signals,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "and M. Algabri, “Electroencephalography-based motor imagery"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "thereby advancing computational neuroscience analysis.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "classification\nusing\ntemporal\nconvolutional\nnetwork\nfusion,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Biomedical Signal Processing and Control, vol. 69, p. 102826,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "VI. CONCLUSION",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "2021."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[10] Y. S. Can, B. Mahesh, and E. Andr´e, “Approaches, applications,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "This\nstudy\nintroduces\nthe MEEG dataset\nand\nthe AT-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "and challenges in physiological emotion recognition—a tutorial"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "DGNN framework,\nsignificantly advancing EEG-based emo-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "overview,” Proceedings of\nthe IEEE, 2023."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "tion recognition. The MEEG dataset utilizes music to induce",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[11] M. Gra˜na\nand\nI. Morais-Quilez,\n“A review of\ngraph\nneural"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "emotional\nstates,\nproviding\na\nunique\nand\ncritical\nresource",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "networks for electroencephalography data analysis,” Neurocom-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "puting, p. 126901, 2023."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "for analyzing brain responses\nto emotional\nstimuli. The AT-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[12] N. Robinson, S.-W. Lee, and C. Guan, “Eeg representation in"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "DGNN framework captures the complex temporal dynamics of",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "deep convolutional neural networks for classification of motor"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "brain activity, enhancing emotion recognition accuracy beyond",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "imagery,” in 2019 IEEE International Conference on Systems,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "existing SOTA methods. Moreover, the MEEG dataset displays",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Man and Cybernetics (SMC).\nIEEE, 2019, pp. 1322–1326."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "emotional\nstates of\nsubjects with greater precision, enabling",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[13] T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recog-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "nition using dynamical graph convolutional neural networks,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "SOTA methods\nto achieve higher\naccuracy and establishing",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "IEEE Transactions on Affective Computing, vol. 11, no. 3, pp."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "a more robust benchmark for evaluating EEG-based emotion",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "532–541, 2018."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "analysis models. These advancements propel BCI\ntechnology",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[14] G. Bao, K. Yang, L. Tong, J. Shu, R. Zhang, L. Wang, B. Yan,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "forward\nand\nfacilitate\nnew research\ninto\nthe\nrelationships",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "and Y. Zeng, “Linking multi-layer dynamical gcn with style-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "between music, emotion, and brain activity.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "based\nrecalibration\ncnn\nfor\neeg-based\nemotion\nrecognition,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Frontiers in Neurorobotics, vol. 16, p. 834952, 2022."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[15]\nS. Asadzadeh, T. Yousefi Rezaii, S. Beheshti, and S. Meshgini,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "VII. ACKNOWLEDGEMENTS",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "“Accurate emotion recognition using bayesian model based eeg"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "This\npaper\nis\nsupported\nby\nthe\nShenzhen\nFundamental",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "sources as dynamic graph convolutional neural network nodes,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Research Program under Grant JCYJ20230807094104009. and",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Scientific Reports, vol. 12, no. 1, p. 10282, 2022."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Key Lab of Information Network Security, Ministry of Public",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[16] Y. Ding, N. Robinson, C. Tong, Q. Zeng, and C. Guan, “Lggnet:"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Security.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Learning\nfrom local-global-graph\nrepresentations\nfor\nbrain–"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "IEEE Transactions\non Neural Networks\ncomputer\ninterface,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "REFERENCES",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "and Learning Systems, 2023."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Journal\nof\n[1]\nJ. A. Russell,\n“A circumplex model\nof\naffect.”",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[17] L. Zhang, F. Xiao,\nand Z. Cao,\n“Multi-channel\neeg\nsignals"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "personality and social psychology, vol. 39, no. 6, p. 1161, 1980.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "classification via cnn and multi-head self-attention on evidence"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[2]\nS. Zhao, G.\nJia,\nJ. Yang, G. Ding,\nand K. Keutzer,\n“Emo-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "theory,” Information Sciences, vol. 642, p. 119107, 2023."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "tion recognition from multiple modalities: Fundamentals\nand",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[18]\nF.\nScarselli, M. Gori, A. C.\nTsoi, M. Hagenbuchner,\nand"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "methodologies,”\nIEEE Signal Processing Magazine,\nvol.\n38,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "G. Monfardini, “The graph neural network model,” IEEE trans-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "no. 6, pp. 59–73, 2021.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "actions on neural networks, vol. 20, no. 1, pp. 61–80, 2008."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[3] R. Foong, K. K. Ang, C. Quek, C. Guan, K. S. Phua, C. W. K.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[19] Y. Ding, N. Robinson, Q. Zeng, D. Chen, A. A. P. Wai, T.-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Kuah, V. A. Deshmukh, L. H. L. Yam, D. K. Rajeswaran,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "S. Lee, and C. Guan, “Tsception: a deep learning framework"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "N. Tang et al., “Assessment of\nthe efficacy of eeg-based mi-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "joint\nfor\nemotion detection using eeg,”\nin 2020 international"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "bci with visual\nfeedback and eeg correlates of mental\nfatigue",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "conference on neural networks (IJCNN).\nIEEE, 2020, pp. 1–7."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "IEEE Transactions\non\nfor\nupper-limb\nstroke\nrehabilitation,”",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[20] H. Altaheri, G. Muhammad,\nand M. Alsulaiman,\n“Physics-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "Biomedical Engineering, vol. 67, no. 3, pp. 786–795, 2019.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "informed\nattention\ntemporal\nconvolutional\nnetwork\nfor\neeg-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[4]\nS. Koelstra, C. Muhl, M. Soleymani,\nJ.-S. Lee, A. Yazdani,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "based motor\nimagery classification,” IEEE transactions on in-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "T. Ebrahimi, T.\nPun, A. Nijholt,\nand\nI.\nPatras,\n“Deap: A",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "dustrial\ninformatics, vol. 19, no. 2, pp. 2249–2258, 2022."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "database\nfor\nemotion\nanalysis;\nusing\nphysiological\nsignals,”",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[21]\nP. N.\nJuslin and D. V¨astfj¨all, “Emotional\nresponses\nto music:"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "IEEE transactions on affective\ncomputing, vol. 3, no. 1, pp.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "The need to consider underlying mechanisms,” Behavioral and"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "18–31, 2011.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "brain sciences, vol. 31, no. 5, pp. 559–575, 2008."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[5] W.-L. Zheng\nand B.-L. Lu,\n“Investigating\ncritical\nfrequency",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[22]\nS. Koelsch, “Brain correlates of music-evoked emotions,” Na-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "bands and channels for eeg-based emotion recognition with deep",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "ture reviews neuroscience, vol. 15, no. 3, pp. 170–180, 2014."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "IEEE Transactions\non\nautonomous mental\nneural\nnetworks,”",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[23]\nS. Sheykhivand, Z. Mousavi, T. Y. Rezaii, and A. Farzamnia,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "development, vol. 7, no. 3, pp. 162–175, 2015.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "“Recognizing emotions\nevoked by music using cnn-lstm net-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[6] V.\nJ. Lawhern, A.\nJ. Solon, N. R. Waytowich, S. M. Gordon,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "works on eeg signals,” IEEE Access, vol. PP, no. 99, pp. 1–1,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "C. P. Hung, and B. J. Lance, “Eegnet: a compact convolutional",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "2020."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "neural network for eeg-based brain–computer interfaces,” Jour-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[24]\nS. Varma\nand R. Simon,\n“Bias\nin error\nestimation when us-"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "nal of neural engineering, vol. 15, no. 5, p. 056013, 2018.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "ing cross-validation for model selection,” BMC bioinformatics,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[7] R. T.\nSchirrmeister,\nJ. T.\nSpringenberg, L. D.\nJ.\nFiederer,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "vol. 7, pp. 1–8, 2006."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "M. Glasstetter, K. Eggensperger, M. Tangermann, F. Hutter,",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "[25] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "W. Burgard,\nand T. Ball,\n“Deep learning with convolutional",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch:"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "neural networks\nfor\neeg decoding and visualization,” Human",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "An imperative style, high-performance deep learning library,”"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "brain mapping, vol. 38, no. 11, pp. 5391–5420, 2017.",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "Advances\nin\nneural\ninformation processing\nsystems,\nvol.\n32,"
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "[8] T. M. Ingolfsson, M. Hersche, X. Wang, N. Kobayashi, L. Cav-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": "2019."
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "igelli, and L. Benini, “Eeg-tcnet: An accurate temporal convo-",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        },
        {
          "of\nthe\nsynergistic operation of\nits\ncomponents\nin enhancing": "lutional network for\nembedded motor-imagery brain–machine",
          "interfaces,” in 2020 IEEE International Conference on Systems,": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from multiple modalities: Fundamentals and methodologies",
      "authors": [
        "S Zhao",
        "G Jia",
        "J Yang",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "3",
      "title": "Assessment of the efficacy of eeg-based mibci with visual feedback and eeg correlates of mental fatigue for upper-limb stroke rehabilitation",
      "authors": [
        "R Foong",
        "K Ang",
        "C Quek",
        "C Guan",
        "K Phua",
        "C Kuah",
        "V Deshmukh",
        "L Yam",
        "D Rajeswaran",
        "N Tang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "4",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "6",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "7",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "8",
      "title": "Eeg-tcnet: An accurate temporal convolutional network for embedded motor-imagery brain-machine interfaces",
      "authors": [
        "T Ingolfsson",
        "M Hersche",
        "X Wang",
        "N Kobayashi",
        "L Cavigelli",
        "L Benini"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "9",
      "title": "Electroencephalography-based motor imagery classification using temporal convolutional network fusion",
      "authors": [
        "Y Musallam",
        "N Alfassam",
        "G Muhammad",
        "S Amin",
        "M Alsulaiman",
        "W Abdul",
        "H Altaheri",
        "M Bencherif",
        "M Algabri"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Approaches, applications, and challenges in physiological emotion recognition-a tutorial overview",
      "authors": [
        "Y Can",
        "B Mahesh",
        "E André"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "11",
      "title": "A review of graph neural networks for electroencephalography data analysis",
      "authors": [
        "M Graña",
        "I Morais-Quilez"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "Eeg representation in deep convolutional neural networks for classification of motor imagery",
      "authors": [
        "N Robinson",
        "S.-W Lee",
        "C Guan"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Linking multi-layer dynamical gcn with stylebased recalibration cnn for eeg-based emotion recognition",
      "authors": [
        "G Bao",
        "K Yang",
        "L Tong",
        "J Shu",
        "R Zhang",
        "L Wang",
        "B Yan",
        "Y Zeng"
      ],
      "year": "2022",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "15",
      "title": "Accurate emotion recognition using bayesian model based eeg sources as dynamic graph convolutional neural network nodes",
      "authors": [
        "S Asadzadeh",
        "T Yousefi Rezaii",
        "S Beheshti",
        "S Meshgini"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "16",
      "title": "Lggnet: Learning from local-global-graph representations for braincomputer interface",
      "authors": [
        "Y Ding",
        "N Robinson",
        "C Tong",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "17",
      "title": "Multi-channel eeg signals classification via cnn and multi-head self-attention on evidence theory",
      "authors": [
        "L Zhang",
        "F Xiao",
        "Z Cao"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "18",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2008",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "19",
      "title": "Tsception: a deep learning framework for emotion detection using eeg",
      "authors": [
        "Y Ding",
        "N Robinson",
        "Q Zeng",
        "D Chen",
        "A Wai",
        "T.-S Lee",
        "C Guan"
      ],
      "year": "2020",
      "venue": "2020 international joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "20",
      "title": "Physicsinformed attention temporal convolutional network for eegbased motor imagery classification",
      "authors": [
        "H Altaheri",
        "G Muhammad",
        "M Alsulaiman"
      ],
      "year": "2022",
      "venue": "IEEE transactions on industrial informatics"
    },
    {
      "citation_id": "21",
      "title": "Emotional responses to music: The need to consider underlying mechanisms",
      "authors": [
        "P Juslin",
        "D Västfjäll"
      ],
      "year": "2008",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "22",
      "title": "Brain correlates of music-evoked emotions",
      "authors": [
        "S Koelsch"
      ],
      "year": "2014",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Recognizing emotions evoked by music using cnn-lstm networks on eeg signals",
      "authors": [
        "S Sheykhivand",
        "Z Mousavi",
        "T Rezaii",
        "A Farzamnia"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Bias in error estimation when using cross-validation for model selection",
      "authors": [
        "S Varma",
        "R Simon"
      ],
      "year": "2006",
      "venue": "BMC bioinformatics"
    },
    {
      "citation_id": "25",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    }
  ]
}