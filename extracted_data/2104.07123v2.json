{
  "paper_id": "2104.07123v2",
  "title": "The Muse 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, And Stress",
  "published": "2021-04-14T20:56:04Z",
  "authors": [
    "Lukas Stappen",
    "Alice Baird",
    "Lukas Christ",
    "Lea Schumann",
    "Benjamin Sertolli",
    "Eva-Maria Messner",
    "Erik Cambria",
    "Guoying Zhao",
    "Bj√∂rn W. Schuller"
  ],
  "keywords": [
    "Multimodal Sentiment Analysis",
    "Affective Computing",
    "Stress Detection",
    "Electrodermal Activity",
    "Multimodal Fusion",
    "Challenge",
    "Benchmark"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the tasks of sentiment and emotion, as well as physiologicalemotion and emotion-based stress recognition through more comprehensively integrating the audio-visual, language, and biological signal modalities. The purpose of MuSe 2021 is to bring together communities from different disciplines; mainly, the audio-visual emotion recognition community (signal-based), the sentiment analysis community (symbol-based), and the health informatics community. We present four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus on continuous emotion (valence and arousal) prediction; MuSe-Sent , in which participants recognise five classes each for valence and arousal; and MuSe-Physio , in which the novel aspect of 'physiological-emotion' is to be predicted. For this year's challenge, we utilise the MuSe-CaR dataset focusing on user-generated reviews and introduce the Ulm-TSST dataset, which displays people in stressful depositions. This paper also provides detail on the state-of-the-art feature sets extracted from these datasets for utilisation by our baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each sub-challenge, a competitive baseline for participants is set; namely, on test, we report a Concordance Correlation Coefficient (CCC) of .4616 CCC for MuSe-Wilder ; .5088 CCC for MuSe-Stress , and .4908 CCC for MuSe-Physio . For MuSe-Sent an F1 score of 32.82 % is obtained. \n CCS CONCEPTS ‚Ä¢ Information systems ‚Üí Multimedia and multimodal retrieval; ‚Ä¢ Computing methodologies ‚Üí Artificial intelligence.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "In the 2nd edition of the Multimodal Sentiment Analysis in Real-life Media (MuSe) Challenge, we address four tasks incorporating novelties in each: emotion, physiological-emotion, and stress recognition as well as sentiment classification. In the Multimodal Continuous Emotions in-the-wild sub-challenge (MuSe-Wilder ) and Multimodal Sentiment in-the-wild Classification sub-challenge (MuSe-Sent ), one has to recognise emotional dimensions (arousal, valence) in a regression and classification manner. These tasks are based on work previously outlined for the MuSe 2020 challenge  [42]  and feature substantially improved methods for target creation. The first improvement is the application of Rater Aligned Annotation Weighting (RAAW), a gold standard fusion method for continuous annotations taking both the varied annotator reaction times (aligning) and inter-rater agreements (subjectivity) into account. Additionally, intelligent extraction of valuable features from continuous emotion gold-standards is used to cluster segment-level signals to representative classes so that contributors are faced with two fiveway classifications of the level of valence and arousal. These two sub-challenges (MuSe-Wilder and MuSe-Sent ) are motivated by the fundamental nature of gold-standard creation on which all tasks and applications of the field are premised. In the Multimodal Emotional Stress sub-challenge (MuSe-Stress ), valence and arousal are predicted, from people in stressed dispositions. This sub-challenge is motivated by the high level of stress many people face in modern societies  [6] . Given the increasing availability of low-resource equipment (e. g., smart-watches) able to record biological signals to Table  1 : Reported are the number (#) of unique videos, and the duration for each sub-challenge hh :mm :ss. Partitioning of the MuSe-CaR dataset is applied for each of the two sub-challenges. Ulm-TSST has a total duration of 5 :47 :27 after preprocessing, using the same split for MuSe-Stress and MuSe-Physio . track wellbeing, we propose the Multimodal Physiological-Arousal sub-challenge (MuSe-Physio ). Adapted from MuSe-Stress , the arousal annotations from humans are fused (using RAAW) with galvanic skin response (also known as Electrodermal Activity (EDA)) signals for predicting physiological-arousal. Both are set up as regression tasks offering additional biological signals (e. g., heart rate, and respiration) for modelling.\n\nFor the introduced sub-challenges, two datasets are utilised. As last year  [42] , we reuse the Multimodal Sentiment Analysis in Car Reviews data (MuSe-CaR)  [43]  for the MuSe-Wilder and MuSe-Sent sub-challenges. Including almost 40 hours of video data, it is the most extensive emotion annotated multimodal dataset, gathered in-the-wild with the intention of further understanding realworld Multimodal Sentiment Analysis (MSA), in particular the emotional engagement that takes place during English-speaking product reviews. Within MuSe-CaR, the subjects are aged between 20 and 60 years, and the spoken word is entirely transcribed. For the first time, a sub-set of the novel audio-visual-text Ulm-Trier Social Stress dataset (Ulm-TSST), featuring German-speaking individuals in a stress-induced situation caused by the Trier Social Stress Test (TSST), is used in this year's MuSe-Stress and MuSe-Physio sub-challenges. The initial state of Ulm-TSST consists of 110 individuals (10 hours), richly annotated by self-reported, and continuous dimensional ratings of emotion (valence and arousal). In addition to audio, video, textual features, the Ulm-TSST includes four biological signals captured at a sampling rate of 1 kHz; EDA, Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM). Both datasets provide a common testing bed with a held-back labelled test set, to explore the modalities and employ state-of-the-art models under well-defined and strictly comparable conditions.\n\nThe goal of the MuSe challenges are to provide a paradigm that is of interest across several communities and to encourage a fusion of disciplines. We ideally aim for participation that strives for the development of unified approaches applicable to what we perceive as synergistic tasks which have arisen from different academic traditions: on the one hand, we have complex, dimensional emotion annotations that reflect a broad variety of emotions, grounded in the psychological and social sciences relating to the expression of behaviour, and on the other hand, we provide sentiment classes as it is common in sentiment analysis from (multimodal) text-focused modelling. These fields are rooted within Affective Computing (AC), of which a core aspect is the intelligent processing of uni-modal signals. Up to now, the focus in AC when predicting emotion such Figure  1 : Sample gold standard computation using RAAW for emotion annotations (dark grey), aligned (warping paths in light grey), and fused from the MuSe-Toolbox  [49] . The instance on the left side is from MuSe-Wilder (id: 100), the resulting signal (orange) is displayed and compared to the average annotation (blue). The right image displays a fusion from MuSe-Physio (id:11), where raters 1 and 2 for arousal (dark grey) are combined with the EDA signal (green).\n\nas by valence and arousal dimensions, was mostly with lower attention to research made on textual information  [21, 39] . However, the communities appear to be converging even more in recent years (such as supported by the MuSe 2020  [42]  challenge), finding great benefit from multimodal approaches  [2, 15, 33] . As an example, both the 2020 and 2021 INTERSPEECH Computational Paralinguistics (ComParE) Challenge have included textual features in an endeavour to more reliably predict valence  [37, 38] . The second motivation of MuSe is to compare the merits of each of the core modalities (audio, visual, biological, social, and textual signal), as well as various multimodal fusion approaches. Participants can extract their own features or use the provided standard feature sets from the baseline models.\n\nThe paper's structure is as follows: First, the four sub-challenges with the corresponding datasets are explained in detail, followed by a description of the challenge conditions. Next, we describe the extracted features from different modalities and the applied pre-processing and alignment for the baseline modelling. Finally, we summarise our baseline results and conclude our findings. A summary of the challenge results can be found in  [46] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Four Sub-Challenges",
      "text": "In the following, we describe and highlight the aforementioned novelties of each sub-challenge, as well as include the guidelines for participation. The evaluation metric for all continuous time-based regression tasks is Concordance Correlation Coefficient (CCC), a well-understood measure  [29]  of reproducibility, often used in challenges  [35, 42, 52] . The classification task (MuSe-Sent ) is evaluated in F1 score (macro), a measure robust to class-imbalance. For all challenges with more than one target, the mean of all measures is taken for the final performance evaluation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Muse-Wilder Sub-Challenge",
      "text": "The MuSe-Wilder is an extension of the MuSe-Wild 2020 subchallenge, where participants had to predict emotional dimensions (valence, arousal) in a time-continuous manner. The amount of data utilised from MuSe-CaR is shown in Table  1 . The valence dimension is often referred to as the emotional component of the generic term sentiment analysis and is often used interchangeably  [26, 32, 51] . Human annotation of continuous emotions leads to disagreements between raters, e. g., due to differences in perception  [19]  and reaction time  [27] , which should be mitigated by fusion to a gold standard. Since this signal is the prediction target, a variety of fusion methods are available in the literature  [16, 28]  and this development has motivated other challenges  [34] . This year's MuSe-Wilder emotion recognition task is based on a completely novel continuous annotator fusion technique RAAW, which targets the difficulties of combining subjective emotion annotations for a gold standard annotation present. For this, we employ our fusion method on a minimum of five different ratings that weights inter-rater agreements as well as considers the varied reaction times as displayed in Figure  1 . The varying rater lag that is inherent to all annotation signals will be targeted by aligning the standardised (per annotator) ratings using a generalised Canonical Time Warping (CTW) method  [57] . The Evaluator Weighted Estimator (EWE)  [16]  is then used to fuse the aligned, individual signals by weighting a signal depending on the inter-rater agreement to the mean of all others. This technique is described in length in  [49] . The resulting distribution is shown in Figure  2 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Muse-Sent Sub-Challenge",
      "text": "Mapping continuous emotion annotations to discrete classes are considered a highly ambiguous and challenging task and have so far hardly been computed successfully in a time-continuous fashion  [53] . In general, classes are often considered a simplified concept for interpretation compared to dimensional representations. In MuSe-Sent , participants will have to predict five advanced sentiment classes for each emotion dimension of valence and arousal on a segment-level, based on audio-visual recordings and the transcribed speech of MuSe-CaR. The sub-challenge uses the topicbased segmentation from MuSe 2020  [42] . The classes are extracted using a novel method of the MuSe-Toolbox  [49] . which aims to find a mapping between continuous dimensional and categorical representations of emotion through the extraction of time-series features and the application of unsupervised clustering.\n\nMore specifically, we first extract a range of time-series features on a segment-level 1  based on the continuous RAAW-fused annotations. The absolute features are normalised depending on the varying length of a segment to limit undesirable properties solely due to the influence of the segment length. To reduce the feature space, we apply Principal Component Analysis (PCA) to project our data to a five dimensional space of principal components which are derived from the eigenvectors of the covariance matrix. The transformed data is further clustered into five class clusters using a) the ùëò-means algorithm  [24]  for valence and b) a Gaussian Mixture clustering model  [10]  for arousal. To ensure that the development and test set have no effect on the generated classes, we only apply this process on the training set segments. The segments belonging to the development and test partitions, are then 'predicted' by assigning the cluster with the closest centre to each data-point. These clusters are evaluated through both qualitative and quantitative measures: (1) the amount of data-points of the smallest class is larger than a quarter of by-chance-level  2  (2) to evaluate cluster cohesion and separation, the widely used Silhouette Coefficient (SILC)  [36]  is calculated, ranging from -1 to 1 (closer to 1 is superior). For the two chosen settings, we achieve a SILC of 0.19 and 0.10, respectively for valence and arousal clusters. The PCA leads to a denser representation along the orthogonal axes, making a higher SILC value hard to achieve, since the metric is prone to error when clusters show different kinds of cluster densities  [22] , which naturally occurs in this setting.\n\nSince the features reflect characteristics of the emotional annotation and not just the mean value as in last years' MuSe-Topic task  [42] , class descriptions, i. e., low, medium, or high would inadequately reflect the meaning. With this in mind, to gain understanding of the classes, we display the most distinctive features in Figure  3  for interpretation showing the named valence classes as ùëâ # and arousal as ùê¥ # while # represents the class number, not implying any specific order. For example, segments from classes ùëâ 1 and ùê¥ 2 have a comparatively large (to the mean of all other classes) 'standard deviation' and 'sum of changes', which indicates a higher annotation fluctuation and intensity than other classes. The distribution of segments across the classes can be found in Table  2 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Muse-Stress Sub-Challenge",
      "text": "In the MuSe-Stress , participants will have to predict valence and arousal in a time-continuous manner. This sub-challenge is motivated by real-world applications for emotion recognition and further motivated by stress in modern life. In this novel sub-challenge, the idea of 'multimodal' sentiment analysis is pushed further by the inclusion of biological signals that have been shown to be applicable for recognising physiological stress  [31] , and for emotion recognition  [40] .\n\nParticipants are provided with the multimodal Ulm-TSST database, in which subjects were recorded under a highly stress-induced free speech task, following the TSST protocol  [20] . For the TSST, after a brief period of preparation the subjects are asked to give an oral presentation, within a job-interview setting, observed by two interviewees who remain silent for the period of five minutes. To allow consistent data partitions, we only keep data recorded under the same experimental conditions. The resulting 69 participants (49 of them female) are aged between 18 and 39 years, providing a total amount of about 6 hours of data for the MuSe-Stress and MuSe-Physio sub-challenges (cf. Table  1 ). Besides audio, video, and text, the participants can optionally utilise the ECG, RESP and BPM signals.\n\nThe dataset has been rated by three annotators continuously for the emotional dimensions of valence and arousal, at a 2Hz sampling rate, and a gold standard is obtained by the fusion of annotator ratings, utilising the RAAW method, as described in Section 2.1 from the MuSe-Toolbox  [49] . When creating the fusion a mean CC inter-rater agreement of 0.204 (¬± 0.200) for valence and 0.186 (¬± 0.230) for arousal is obtained. The distributions of the valence and arousal signals for the dataset are depicted in Figure  2 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Muse-Physio Sub-Challenge",
      "text": "In the cross-modal MuSe-Physio , participants will have to predict a combined signal of arousal and EDA. Again, for this task, the Ulm-TSST dataset is employed, where the TSST was utilised as a standardised and renowned stressor, allowing for a controlled setting with high-quality data while maintaining a naturalistic subject behaviour.\n\nPhysiological signals, including EDA have been used as a feature in at least one other multimodal emotion challenge  [9] . However, we consider this sub-challenge to be the first time the physiological signal is combined with the emotional -human-annotatedsignal. From all the biological signals available in the Ulm-TSST dataset, we choose to use the EDA signal, as not only are the signal characteristics subjectively similar to continuous emotion as exemplarily depicted in Figure  1 , but the signal itself has been shown in the literature to be a psycho-physiological indication of emotional arousal  [7] . Given that in the context of an interview, arousal may also appear to be a more hidden emotion, we consider that the fusion of arousal and EDA may improve recognition and offers a more objective marker for a speaker's arousal  [7] . Further variants are introduced in  [4] . To obtain the combined emotion and EDA signal gold standard, we again utilise the RAAW fusion strategy. However, in this case, the lowest weighted annotator is excluded and replaced with the EDA signal. The EDA signal is downsampled to 2 Hz and smoothed slightly before fusion through a Savitzky-Golay filtering approach (window size of 26 steps), to avoid irrelevant, fine-grained artefacts in the signal. For this gold standard of emotion, we obtain an inter agreement of 0.233 (¬±0.289), which was improved compared to the arousal gold standard obtained in MuSe-Stress .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Challenge Protocol",
      "text": "As part of the mandatory prerequisites required to play a part in the challenge, interested participants are obliged to download and fill in the End User License Agreement (EULA) which is put forward through the homepage  3  . On top of this, participants are further required to hold an academic affiliation. Each participation must be accompanied by a paper (6-8 pages in length including references) reporting the results obtained and methods applied. The organisers also consider general contributions in the field. Peer review is double-blind. To obtain results on the test set, the participants can upload their predictions up to five times per subchallenge, whose labels are unknown to them. We want to point out that the organisers only evaluate the participants' results but do not participate themselves as competitors in the challenge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Features And Model",
      "text": "To save effort and time which would be incurred by the participants while extracting various features from the large datasets provided, we put forth a selection of features drawn from the video data for each sub-challenge. In a more elaborate outline, the available features comprise of seven model-ready video, audio, and linguistic feature sets  4  . The amalgamation of features provided surpasses most other related audio-visual challenges  [9, 11, 55] . In respect to the annotation sampling rate, the features are extracted at a step size of 0.25 s for the MuSe-CaR and 0.5 s for the Ulm-TSST dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pre-Processing",
      "text": "The data of both datasets has been partitioned into a Train, Development, and Test partition. Emotional ratings, speaker independence, and duration are considered when creating the partitions (cf. Table  1  for an overview). Since the amount of recordings made available between sub-challenges can vary, so too does the time required to extract the most applicable features during the pre-processing stages. Aiming to minimise the distortion of the task objectives, we deliberately omit advertisement sections of the videos for the MuSe-CaR-based sub-challenges. In the Ulm-TSST dataset, each video is cut to exclude scenes outside of the TSST setting, e. g., The features shown are standard deviation (ùë†ùë°ùëë), ùëöùëíùëëùëñùëéùëõ, the 90th percentile (ùëû 90 ), percentage of reoccurring datapoints to all datapoints (ùëÉùëüùëíùê∑ùëé), relative energy (ùëüùëíùëôùê∏ùëõùëíùëüùëîùë¶), relative sum of changes (ùëüùëíùëôùëÜùëúùê∂), relative number of peaks (ùëüùëíùëôùëÉùëíùëéùëòùë†), relative count below mean (ùëüùëíùëôùê∂ùêµùëÄùëí), relative longest strike below mean (ùëüùëíùëôùêøùëÜùêµùëÄùëí), and relative longest strike above mean (ùëüùëíùëôùêøùëÜùê¥ùëÄùëí). Features indicated by \"relative\" (ùëüùëíùëô) are normalised by segment length. Additionally, all features are standardised, hence, the mean value of all data is always equal to zero. Illustrations generated by the MuSe-Toolbox  [49]  excluding participants' names. For both datasets, the segments are crafted with the focus on the active voice based on the sentence transcriptions or if a visible face applies. For MuSe-Sent , we adjacent segments in instances where the segments deals with the same topic and the gap is less than two seconds.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Acoustic",
      "text": "openSMILE and DeepSpectrum are well-established tools for the extraction of acoustic emotional feature representations. Most notably, they have proved valuable in the extraction of audio processing tasks in renowned challenges in speech emotion recognition (SER)  [37, 38] . For all acoustic features, a six second window size is applied. In the first step of the pre-processing pipeline, the full audio is extracted from a given video. The second step is the conversion of the audio from stereo to mono to 16 kHz, 16 bit after its normalisation to -3 decibels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Egemaps.",
      "text": "The prevalent open-source openSMILE toolkit  [13]  is used to extract the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [12] . Comprising of 88 acoustic parameters for automatic voice analysis tasks  [45] , it is a minimal set of handcrafted features relying on affective physiological changes in voice production that has previously proven valuable for a variety of emotion research  [3, 42, 44] .  [1]  is to utilise the spectral features acquired from speech instances within a pre-trained image recognition Convolutional Neural Networks (CNNs). The consecutive inputs result in the extraction of feature vectors. A commonly applied architecture in this framework is VGG-19  [41] . Here, we keep the default settings for extraction to obtain a 4 096 dimensional feature set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Deepspectrum. The Prime Function Of Deepspectrum",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vggish.",
      "text": "In addition, we extract VGGish functions  [18]  pretrained on an extensive YouTube audio dataset (AudioSet)  [14] . The underlying data contains 600 classes, and the recordings contain a variety of 'in-the-wild' noises that we expect to be beneficial to obtain robust features from our 'in-the-wild' videos. By aligning the frame and hop size to the annotation sample rate, we extract a 128-dimensional VGGish embedding vector every 0.25 s from the underlying log spectrograms.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vision",
      "text": "Extracting specific image descriptors that match certain attributes, e. g., face, remains the paramount focus of most visual feature extractors. Our offered visual feature sets are inclined to capture the entire surroundings as well as analysing human behaviour synthesised from gesture and facial expressions. Participants are also provided with an array of extracted faces which are directly extracted from the raw frames.  [56]  is used to distinguish facial expressions captured in the videos, pretrained on the data sets WIDER FACE  [54]  and CelebA  [23] . For MuSe-CaR, we examined the extraction as described in detail in  [42] , where an F1 score of 86 % on a labelled subset was achieved. Compared to these highly dynamic camera positionings (zoom, free etc.), Ulm-TSST has a static setting. In an visual inspection aimed to control the performance, an apparently flawless extraction was found. The extractions were ultimately put in use as inputs for VGGface and OpenFace .  [30]  is aimed at the extraction of general facial features for images obtained by MTCNN in cropped versions. The visual geometry group of Oxford introduced the deep CNN referred to as VGG16  [41] . The training data constitutes of 2.6 million faces and over 2 500 identities. The VGGface architecture was originally intended for supervised facial recognition purposes  [41] . However, detaching the top-layer of a pretrained version results in a 512 feature vector output referred to as VG-Gface. Presenting high levels of performance while consuming less data is the main advantage held for VGGface in comparison to other facial recognition models.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Mtcnn . The Mtcnn",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vggface. Vggface (Version 1)",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Openface. Facial Features In 2D (136 Dimensions) And 3D",
      "text": "(204 dimensions), gaze positions (288 dimensions), intensity and activity of 17 Facial Action Units (FAUs) for both center and left side, and 6 head stances were extracted from cropped faces identified using MTCNN . This was achieved through the wide array of facial features offered by the OpenFace  [5]  toolkit. For the Ulm-TSSTdata challenge, we only provide intensity, as activity features appear to be of less use for this task.  [17]  should provide participants with environmental features using stacked residual blocks  5  . Among other challenges, it came in first on the ILSVRC 2015 classification challenge. The network is pre-trained on the ImageNet dataset compromising of 350 million images and 17 000 classes. The then frozen network architecture prepossesses a given frame through the layers until the last fully connected layer from which a 2 048 deep feature dimensional vector is obtained.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Xception . Generally Used To Extrapolate Generic Vision Features, Xception",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Language: Bert",
      "text": "The text feature extraction process employs a Transformer language model, namely Bidirectional Encoder Representations from Transformers (BERT )  [8] , which have already been successfully used for a variety of NLP tasks  [37, 38, 44, 47, 48] . BERT pre-trains its deep representations on context of unlabelled text before finetuning them on a broad selection of down-streaming NLP tasks. During inference, the context-based representations are preserved, excerpting one vector per word. This is in contrast to static word embeddings which give one vector per word independent of the context. Our features are the sum of the last four BERT layers resulting in a 768 dimensional feature vector analogous to  [50] . For MuSe-Wilder and MuSe-Sent , the base variant of BERT , pretrained on English texts, is used. Analogously, as the Ulm-TSST data set is in German, for MuSe-Stress and MuSe-Physio , the BERT (base) pretrained on German texts is utilised.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Alignment",
      "text": "The extensive assortment of features are from three modalities. The corresponding sampling rate of each modality differs, which leads to a different length of the extracted features along the time axis. All visual features are incessant through the video with a frame sampling of 4 Hz for MuSe-CaR and 2 Hz for Ulm-TSST, which is equivalent to the labelling rate. The audio sampling of DeepSpectrum, and that of eGeMAPS apply the same frequency. VGGish and Facial Action Units are the only feature sets relying only on frames where a face is observable. By the nature of text, the corresponding features do not follow a fixed sampling rate, as the duration of a spoken word varies.\n\nFor each sub-challenge, we make label-aligned features available. These have accurately the same stretch and time-stamps as the provided label files. We apply zero-padding to the frames, where the feature type is absent. Such instances include OpenFace , when no face appears or extraction fails, e. g., when only small faces appear in the original frame. The text features are repeated for the interval of a word and non-linguistic parts are also imputed with zero vectors. MuSe-CaR offers automatic, word-aligned transcriptions  [43] . For Ulm-TSST, manual transcripts of the videos are available. We use the Montreal Forced Aligner (MFA)  [25]  tool to obtain time-stamps on the word level. The MFA includes pretrained acoustic models, grapheme-to-phoneme models, and pronunciation dictionaries for various languages. We use the German (Prosodylab) model and the German Prosodylab dictionary to align the Ulm-TSST transcripts. The time-stamps yielded by the MFA are used to align the word embeddings to the 2 Hz frames in the Ulm-TSST dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Model: Lstm-Rnn",
      "text": "In order to address the sequential nature of the input features, we utilise a Long Short-Term Memory (LSTM)-RNN based architecture. The input feature sequences are input into uni-and bi-directional LSTM-RNNs with a hidden state dimensionality of ‚Ñé = {32, 64, 128}, to encode the feature vector sequences. We test different numbers of LSTM-RNN layers ùëõ = {1, 2, 4}. Based on experiences from initial experiments, some hyperparameter searches are task-dependently executed: MuSe-Wilder we search for a suitable learning rate ùëôùëü = {0.0001, 0.001, 0.005}; for MuSe-Sent ùëôùëü = {0.001, 0.005, 0.01}; for MuSe-Stress and MuSe-Physio ùëôùëü = {0.0001, 0.0002, 0.0005, 0.001}. As we observed overfitting in some settings of MuSe-Physio , we also tried L2-Regularisation with a penalty of 0.01 for this task.\n\nThe sequence of hidden vectors from the final LSTM-RNN layer is further encoded by a feed-forward layer that outputs either a one-dimensional prediction sequence of logits for each time step (regression), or a single-value per prediction target (classification).\n\nIn the training processes, the features and labels of every input video are further segmented via a windowing approach  [42, 43, 50] . For MuSe-Wilder and MuSe-Sent , we use a window size of 200 steps (50 s) and a hop size of 100 steps (25 s). For MuSe-Stress and MuSe-Physio , a window size of 300 steps (150 s) and a hop size of 50 steps (25 s) proved to be reasonable choices.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fusion",
      "text": "We apply decision-level (late) fusion to evaluate co-dependencies of the modalities. The experiments are restricted to the best performing features from each modality only. For decision-level fusion, separate models are trained individually for each modality. The predictions of these are fused by training an additional LSTM-RNN model as described above. For all continuous regression tasks, we apply uni-directional version with ùëôùëü = 0.0001, ‚Ñé = 64, and ùëõ = 1, and for MuSe-Sent a bi-directional one with ùëôùëü = 0.005, ‚Ñé = 32, and ùëõ = 2.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Baseline Results",
      "text": "For all sub-challenges, the same network architecture is applied (cf. Section 3.6). For reproducibility, we provide the detailed set of hyperparameters for our best models for each experiment, alongside our code in the corresponding GitHub repository  6  , where also a link to the fully trained model weights can be found. In the following section, we give an overview of all baseline results as summarised in Table  3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Muse-Wilder",
      "text": "We evaluated several feature sets and combinations for the prediction of the continuous valence and arousal (cf. Table  3 ). The input features BERT in combination with our baseline architecture set to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the prediction of valence leading to a CCC of .4613 on the development and .5671 CCC on test set. For the prediction of arousal, using Deep-Spectrumas input features and setting ùëôùëü = 0.001, ‚Ñé = 64, and ùëõ = 2, yields the best result of all applied systems with a CCC of .3386 on the test set. Generally, we found that a unidirectional LSTM-RNN achieves better results for this task than complex bidirectional configurations and is used for the reported MuSe-Wilder results. When fusing the best performing features of all three modalities DeepSpectrum, VGGface, and BERT , the late fusion technique reaches .4863 and .5974 for valence and .4929 and .3257 for arousal on the development and test set, respectively. This technique yields the highest combined metric (mean of valence and arousal) of .4616 (on test) and is our baseline.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Muse-Sent",
      "text": "For the classification tasks in the MuSe-Sent sub-challenge, we give an overview in Table  3  and further provide the confusion matrices for the best uni-modal setups tested on valence and arousal in Figure  4 . For the prediction of valence on uni-modal feature inputs, the best result is achieved using the text-based BERT features as input and a baseline model setting of ùëôùëü = 0.001, ‚Ñé = 64 and ùëõ = 4 (bi-directional), with an F1 score of 32.68 % on the development and 31.90 % on the test set. Using the audio-based DeepSpectrum features with a ùëôùëü = 0.001, ‚Ñé = 128, and ùëõ = 2 (bi-directional), results in our highest F1 score for arousal with 33.52 % on the development and 33.16 % on the test set. Across both targets, we find that LSTM-RNN models with a bidirectional setting and at least two layers tend to achieve better results for this task than smaller architectures. Partially, we see improvements when we apply late fusion. For valence, utilising the predictions of VGGface and BERT yields a performance of 32.91 % F1-score on the test set. For arousal, the audio-visual fusion set-up (VGGish and FAU) also improves on the test set, with an F1 score of 35.12 %. Looking at the combined scores (mean of valence and arousal), using the BERT features alone comes out on top for the development set, reaching a 35.48 % F1 score, while fusing the video-and text-based predictions achieves the highest F1 score of 32.82 % on the test set.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Muse-Stress",
      "text": "The best results from all feature sets and fusion of modalities are reported in Table  3   7 . Having searched the hyperparameter combinations mentioned, we achieve the best results on all settings with a 4-layered unidirectional LSTM equipped with 64-dimensional hidden states and a learning rate of 0.0002 with a maximum of 100 epochs, and early stopping with a patience of 15 epochs. Here, eGeMAPS outperforms all other single feature sets for the prediction of valence, achieving .5845 CCC on development and .5018 CCC on the test set. Regarding arousal, eGeMAPS is the best scoring single feature set, leading to .4304 and .4416 CCC on development and test set, respectively. For both valence and arousal prediction, the fusions of the best audio and vision feature sets result in the best performance overall. They achieve CCC values of .6966 (development) and .5614 (test) for valence, and .5043 (development) 7 Of note, besides eGeMAPS, we also normalise the VGGish features for predicting arousal.  and .4562 (test) for arousal.. It is notable that the text feature set, BERT , performs considerably worse than the best audio and visual features.\n\nWe found that, in general, valence reaches a stronger final result than arousal for this task. While this is not surprising for text features, it counters conventional expectations for the audio modality. A major reason for the poor arousal prediction results may be the TSST scenario, which imitates a job interview. Typically, interviewees try to remain neutral, suppressing nervousness, hence, the arousal shown to their counterpart would be minimal, thus, making arousal more difficult to detect in the Ulm-TSST data set than other comparable multimodal emotion recognition data sets.\n\nAlthough we do not evaluate the provided bio-signal features systematically here, we encourage participants to explore them. To give an example, we achieve .2495 and .1537 CCC for valence on the development and test sets, respectively, by using only the three provided bio-signals (at a sampling rate of 2 Hz) as features in a four-layer LSTM. Similarly, they show also promising results for the prediction of arousal, reaching .1954 CCC on the development and .2189 CCC on test partition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Muse-Physio",
      "text": "For MuSe-Physio , the same LSTM configuration as for MuSe-Stress is applied. The results are reported in Table  3   Using the one-dimensional biological signals as features might also be beneficial here, even though our model fails to generalise for them. We achieve CCCs of .4188 on the development and .3328 on the test set using a 4 LSTM layer setting and a learning rate of 0.01.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we introduced MuSe 2021 -the second Multimodal Sentiment Analysis challenge. MuSe 2021 utilises the MuSe-CaR multimodal corpus of emotional car reviews and the Ulm-TSST corpus, including bio-signals, which are newly featured for the MuSe challenge. The 2021 challenge is comprised of four sub-challenges, aimed for predicting in: i) MuSe-Wilder , the level of the affective dimensions of valence (corresponding to sentiment) and arousal; ii) MuSe-Sent , five classes of each, valence and arousal, from video parts containing certain topics; iii) MuSe-Stress , the level of continuous valence and arousal from stressful situations; and iv) MuSe-Physio a combination of arousal and EDA signals. By intention, we decided to use open-source software to extract a wide range of feature sets to deliver the highest possible transparency and realism for the baselines. Besides the features, we also share the raw data and the developed code for our baselines publicly. The official baseline for each sub-challenge is for MuSe-Wilder .5974 for continuous valence using late fusion and .3386 for continuous arousal using DeepSpectrum features; for the five-class classification MuSe-Sent , an F1 score of 32.91 % for valence utilising late fusion of vision and text and 35.12 % for arousal utilising a late fusion of audio-video; for MuSe-Stress , a CCC of .5614 for valence and .4562 for arousal, both based on fusion of the best audio and visual features; and finally, for MuSe-Physio , a CCC of .4908 for physiological-emotion prediction.\n\nThe baselines are improved through the use of a simple fusion method and show the challenge ahead for multimodal sentiment analysis. In the participants' and future efforts, we hope for novel and exciting combinations of the modalities -such as linking modalities at earlier stages in the pipeline or more closely.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample gold standard computation using RAAW",
      "page": 2
    },
    {
      "caption": "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges",
      "page": 3
    },
    {
      "caption": "Figure 1: The varying rater lag that is",
      "page": 3
    },
    {
      "caption": "Figure 3: for interpretation showing the named valence classes",
      "page": 3
    },
    {
      "caption": "Figure 1: , but the signal itself has been shown in",
      "page": 4
    },
    {
      "caption": "Figure 3: Mean of selected clustering features for each of the created classes which are used in the MuSe-Sent sub-challenge.",
      "page": 5
    },
    {
      "caption": "Figure 4: For the prediction of valence on uni-modal feature inputs,",
      "page": 7
    },
    {
      "caption": "Figure 4: Relative confusion matrices over the 5 va-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Erik Cambria\nGuoying Zhao": "Nanyang Technological University\nUniversity of Oulu",
          "Bj√∂rn W. Schuller": "Imperial College London"
        },
        {
          "Erik Cambria\nGuoying Zhao": "Singapore\nOulu, Finland",
          "Bj√∂rn W. Schuller": "London, United Kingdom"
        },
        {
          "Erik Cambria\nGuoying Zhao": "ABSTRACT",
          "Bj√∂rn W. Schuller": "KEYWORDS"
        },
        {
          "Erik Cambria\nGuoying Zhao": "Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing",
          "Bj√∂rn W. Schuller": "Multimodal Sentiment Analysis; Affective Computing; Stress"
        },
        {
          "Erik Cambria\nGuoying Zhao": "on the tasks of sentiment and emotion, as well as physiological-",
          "Bj√∂rn W. Schuller": "Detection; Electrodermal Activity; Multimodal Fusion; Challenge;"
        },
        {
          "Erik Cambria\nGuoying Zhao": "emotion and emotion-based stress recognition through more com-",
          "Bj√∂rn W. Schuller": "Benchmark"
        },
        {
          "Erik Cambria\nGuoying Zhao": "prehensively integrating the audio-visual, language, and biological",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "ACM Reference Format:"
        },
        {
          "Erik Cambria\nGuoying Zhao": "signal modalities. The purpose of MuSe 2021 is to bring together",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "Lukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin Sertolli,"
        },
        {
          "Erik Cambria\nGuoying Zhao": "communities from different disciplines; mainly, the audio-visual",
          "Bj√∂rn W. Schuller": "Eva-Maria Me√üner, Erik Cambria, Guoying Zhao, and Bj√∂rn W. Schuller."
        },
        {
          "Erik Cambria\nGuoying Zhao": "emotion recognition community (signal-based), the sentiment anal-",
          "Bj√∂rn W. Schuller": "2021. The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment,"
        },
        {
          "Erik Cambria\nGuoying Zhao": "ysis community (symbol-based), and the health informatics commu-",
          "Bj√∂rn W. Schuller": "Emotion, Physiological-Emotion, and Stress. In Proceedings of the 2nd Mul-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "nity. We present four distinct sub-challenges: MuSe-Wilder and",
          "Bj√∂rn W. Schuller": "timodal Sentiment Analysis Challenge (MuSe ‚Äô21), October 24, 2021, Virtual"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/"
        },
        {
          "Erik Cambria\nGuoying Zhao": "MuSe-Stress which focus on continuous emotion (valence and",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "3475957.3484450"
        },
        {
          "Erik Cambria\nGuoying Zhao": "arousal) prediction; MuSe-Sent , in which participants recognise",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "five classes each for valence and arousal; and MuSe-Physio ,\nin",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "1\nINTRODUCTION"
        },
        {
          "Erik Cambria\nGuoying Zhao": "which the novel aspect of ‚Äòphysiological-emotion‚Äô is to be predicted.",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "For this year‚Äôs challenge, we utilise the MuSe-CaR dataset focusing",
          "Bj√∂rn W. Schuller": "In the 2nd edition of the Multimodal Sentiment Analysis in Real-life"
        },
        {
          "Erik Cambria\nGuoying Zhao": "on user-generated reviews and introduce the Ulm-TSST dataset,",
          "Bj√∂rn W. Schuller": "Media (MuSe) Challenge, we address four tasks incorporating novel-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "which displays people in stressful depositions. This paper also pro-",
          "Bj√∂rn W. Schuller": "ties in each: emotion, physiological-emotion, and stress recognition"
        },
        {
          "Erik Cambria\nGuoying Zhao": "vides detail on the state-of-the-art feature sets extracted from these",
          "Bj√∂rn W. Schuller": "as well as sentiment classification. In the Multimodal Continuous"
        },
        {
          "Erik Cambria\nGuoying Zhao": "datasets for utilisation by our baseline model, a Long Short-Term",
          "Bj√∂rn W. Schuller": "Emotions in-the-wild sub-challenge (MuSe-Wilder ) and Multi-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "Memory-Recurrent Neural Network. For each sub-challenge, a com-",
          "Bj√∂rn W. Schuller": "modal Sentiment\nin-the-wild Classification sub-challenge (MuSe-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "petitive baseline for participants is set; namely, on test, we report",
          "Bj√∂rn W. Schuller": "Sent ), one has to recognise emotional dimensions (arousal, va-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "a Concordance Correlation Coefficient\n(CCC) of\n.4616 CCC for",
          "Bj√∂rn W. Schuller": "lence) in a regression and classification manner. These tasks are"
        },
        {
          "Erik Cambria\nGuoying Zhao": "MuSe-Wilder ;\n.5088 CCC for MuSe-Stress , and .4908 CCC for",
          "Bj√∂rn W. Schuller": "based on work previously outlined for the MuSe 2020 challenge [42]"
        },
        {
          "Erik Cambria\nGuoying Zhao": "MuSe-Physio . For MuSe-Sent an F1 score of 32.82 % is obtained.",
          "Bj√∂rn W. Schuller": "and feature substantially improved methods for target creation. The"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "first improvement is the application of Rater Aligned Annotation"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "Weighting (RAAW), a gold standard fusion method for continuous"
        },
        {
          "Erik Cambria\nGuoying Zhao": "CCS CONCEPTS",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "annotations taking both the varied annotator reaction times (align-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "‚Ä¢ Information systems ‚Üí Multimedia and multimodal re-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "ing) and inter-rater agreements (subjectivity) into account. Addi-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "trieval; ‚Ä¢ Computing methodologies ‚Üí Artificial intelligence.",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "tionally, intelligent extraction of valuable features from continuous"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "emotion gold-standards is used to cluster segment-level signals to"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "representative classes so that contributors are faced with two five-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "way classifications of the level of valence and arousal. These two"
        },
        {
          "Erik Cambria\nGuoying Zhao": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "sub-challenges (MuSe-Wilder and MuSe-Sent ) are motivated by"
        },
        {
          "Erik Cambria\nGuoying Zhao": "classroom use is granted without fee provided that copies are not made or distributed",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "Bj√∂rn W. Schuller": "the fundamental nature of gold-standard creation on which all tasks"
        },
        {
          "Erik Cambria\nGuoying Zhao": "on the first page. Copyrights for components of this work owned by others than the",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "and applications of the field are premised. In the Multimodal Emo-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "tional Stress sub-challenge (MuSe-Stress ), valence and arousal are"
        },
        {
          "Erik Cambria\nGuoying Zhao": "republish, to post on servers or to redistribute to lists, requires prior specific permission",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "and/or a fee. Request permissions from permissions@acm.org.",
          "Bj√∂rn W. Schuller": "predicted, from people in stressed dispositions. This sub-challenge"
        },
        {
          "Erik Cambria\nGuoying Zhao": "MuSe ‚Äô21, October 24, 2021, Virtual Event, China",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "is motivated by the high level of stress many people face in mod-"
        },
        {
          "Erik Cambria\nGuoying Zhao": "¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "",
          "Bj√∂rn W. Schuller": "ern societies [6]. Given the increasing availability of low-resource"
        },
        {
          "Erik Cambria\nGuoying Zhao": "ACM ISBN 978-1-4503-8678-4/21/10. . . $15.00",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nGuoying Zhao": "https://doi.org/10.1145/3475957.3484450",
          "Bj√∂rn W. Schuller": "equipment (e. g., smart-watches) able to record biological signals to"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: The valence",
      "data": [
        {
          "Table 1: Reported are the number (#) of unique videos, and": "the duration for each sub-challenge hh :mm :ss. Partition-"
        },
        {
          "Table 1: Reported are the number (#) of unique videos, and": "ing of the MuSe-CaR dataset is applied for each of the two"
        },
        {
          "Table 1: Reported are the number (#) of unique videos, and": "sub-challenges. Ulm-TSST has a total duration of 5 :47 :27 af-"
        },
        {
          "Table 1: Reported are the number (#) of unique videos, and": "ter preprocessing, using the same split for MuSe-Stress and"
        },
        {
          "Table 1: Reported are the number (#) of unique videos, and": "MuSe-Physio ."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The valence",
      "data": [
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "MuSe-Physio ."
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "MuSe-CaR\nUlm-TSST"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Partition\n#\nMuSe-Wilder\nMuSe-Sent\n#\nStress/ Psycho"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Train\n166\n22 :16 :43\n22 :35 :55\n41\n3 :25 :56"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Devel.\n62\n06 :48 :58\n06 :49 :46\n14\n1 :10 :50"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "64\n06 :02 :20\n14\n1 :10 :41\nTest\n06Àô:14 :08"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "(cid:205)\n291\n35 :08 :01\n35 :39 :49\n69\n5 :47 :27"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "track wellbeing, we propose the Multimodal Physiological-Arousal"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "sub-challenge (MuSe-Physio ). Adapted from MuSe-Stress , the"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "arousal annotations from humans are fused (using RAAW) with gal-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "vanic skin response (also known as Electrodermal Activity (EDA))"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "signals for predicting physiological-arousal. Both are set up as re-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "gression tasks offering additional biological signals (e. g., heart rate,"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "and respiration) for modelling."
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "For the introduced sub-challenges, two datasets are utilised. As"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "last year [42], we reuse the Multimodal Sentiment Analysis in Car"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Reviews data (MuSe-CaR) [43] for the MuSe-Wilder and MuSe-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Sent sub-challenges. Including almost 40 hours of video data, it is"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "the most extensive emotion annotated multimodal dataset, gath-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "ered in-the-wild with the intention of further understanding real-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "world Multimodal Sentiment Analysis (MSA),\nin particular the"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "emotional engagement that takes place during English-speaking"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "product reviews. Within MuSe-CaR, the subjects are aged between"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "20 and 60 years, and the spoken word is entirely transcribed. For"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "the first time, a sub-set of the novel audio-visual-text Ulm-Trier"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Social Stress dataset (Ulm-TSST), featuring German-speaking in-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "dividuals in a stress-induced situation caused by the Trier Social"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Stress Test (TSST), is used in this year‚Äôs MuSe-Stress and MuSe-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Physio sub-challenges. The initial state of Ulm-TSST consists of"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "110 individuals (10 hours), richly annotated by self-reported, and"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "continuous dimensional ratings of emotion (valence and arousal)."
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "In addition to audio, video, textual features, the Ulm-TSST includes"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "four biological signals captured at a sampling rate of 1 kHz; EDA,"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM)."
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "Both datasets provide a common testing bed with a held-back la-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "belled test set, to explore the modalities and employ state-of-the-art"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "models under well-defined and strictly comparable conditions."
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "The goal of the MuSe challenges are to provide a paradigm that"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "is of interest across several communities and to encourage a fusion"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "of disciplines. We ideally aim for participation that strives for the"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "development of unified approaches applicable to what we perceive"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "as synergistic tasks which have arisen from different academic tra-"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "ditions: on the one hand, we have complex, dimensional emotion"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "annotations that reflect a broad variety of emotions, grounded in"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": ""
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "the psychological and social sciences relating to the expression of"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "behaviour, and on the other hand, we provide sentiment classes as"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "it is common in sentiment analysis from (multimodal) text-focused"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "modelling. These fields are rooted within Affective Computing (AC),"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "of which a core aspect is the intelligent processing of uni-modal"
        },
        {
          "ter preprocessing, using the same split for MuSe-Stress and": "signals. Up to now, the focus in AC when predicting emotion such"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "MuSe-Wilder , MuSe-Stress , and MuSe-Physio . For all sub-challenges, the value distributions between the partitions are"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "fairly similar."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "generic term sentiment analysis and is often used interchange-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "ably [26, 32, 51]. Human annotation of continuous emotions leads"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "to disagreements between raters, e. g., due to differences in per-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "ception [19] and reaction time [27], which should be mitigated by"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "fusion to a gold standard. Since this signal is the prediction target,"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "a variety of fusion methods are available in the literature [16, 28]"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "and this development has motivated other challenges [34]."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "This year‚Äôs MuSe-Wilder emotion recognition task is based on"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "a completely novel continuous annotator fusion technique RAAW,"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "which targets the difficulties of combining subjective emotion anno-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "tations for a gold standard annotation present. For this, we employ"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "our fusion method on a minimum of five different ratings that"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "weights inter-rater agreements as well as considers the varied re-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "action times as displayed in Figure 1. The varying rater lag that is"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "inherent to all annotation signals will be targeted by aligning the"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "standardised (per annotator) ratings using a generalised Canonical"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "Time Warping (CTW) method [57]. The Evaluator Weighted Esti-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "mator (EWE) [16] is then used to fuse the aligned, individual signals"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "by weighting a signal depending on the inter-rater agreement to"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "the mean of all others. This technique is described in length in [49]."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "The resulting distribution is shown in Figure 2."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "2.2\nThe MuSe-Sent Sub-challenge"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "Mapping continuous emotion annotations to discrete classes are"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "considered a highly ambiguous and challenging task and have so"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "far hardly been computed successfully in a time-continuous fash-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "ion [53]. In general, classes are often considered a simplified con-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "cept for interpretation compared to dimensional representations."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "In MuSe-Sent , participants will have to predict five advanced sen-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "timent classes for each emotion dimension of valence and arousal"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "on a segment-level, based on audio-visual recordings and the tran-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "scribed speech of MuSe-CaR. The sub-challenge uses the topic-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "based segmentation from MuSe 2020 [42]. The classes are extracted"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "using a novel method of the MuSe-Toolbox [49]. which aims to"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "find a mapping between continuous dimensional and categorical"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "representations of emotion through the extraction of time-series"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "features and the application of unsupervised clustering."
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "More specifically, we first extract a range of time-series features"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "on a segment-level1 based on the continuous RAAW-fused anno-"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "tations. The absolute features are normalised depending on the"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "1arousal: median, standard deviation, percentile {10, 90}, relative energy, relative sum"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": ""
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "of changes, relative number of peaks, relative longest strike {below, above} mean, and"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "relative count below mean; Valence: the same features as for arousal, and additionally:"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "mean, percentile {5, 25, 33, 66, 75, 95}, and the percentage of reoccurring data-points"
        },
        {
          "Figure 2: Frequency distribution in the partitions train, (devel)opment, and test for the continuous prediction sub-challenges": "to all data-points"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Distribution of the valence and arousal classes arousal[7].Giventhatinthecontextofaninterview,arousalmay",
      "data": [
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "across partitions used in the MuSe-Sent sub-challenge as",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "a\nresult of our",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "search from the MuSe-",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "Toolbox [49].",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "Arousal",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "Train.",
          "the valence": "",
          "and arousal": "Devel.",
          "classes": "Test"
        },
        {
          "Table": "0",
          "2: Distribution of": "528",
          "the valence": "0",
          "and arousal": "249",
          "classes": "178"
        },
        {
          "Table": "1",
          "2: Distribution of": "552",
          "the valence": "1",
          "and arousal": "135",
          "classes": "194"
        },
        {
          "Table": "2",
          "2: Distribution of": "1178",
          "the valence": "2",
          "and arousal": "96",
          "classes": "53"
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "3",
          "2: Distribution of": "1112",
          "the valence": "3",
          "and arousal": "388",
          "classes": "448"
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "4",
          "2: Distribution of": "837",
          "the valence": "4",
          "and arousal": "467",
          "classes": "387"
        },
        {
          "Table": "(cid:205)",
          "2: Distribution of": "4207",
          "the valence": "(cid:205)",
          "and arousal": "1335",
          "classes": "1260"
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "the idea of ‚Äòmultimodal‚Äô sentiment analysis is pushed further by",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "the inclusion of biological signals that have been shown to be ap-",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "plicable for recognising physiological stress [31], and for emotion",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "recognition [40].",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "Participants are provided with the multimodal Ulm-TSST data-",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "base, in which subjects were recorded under a highly stress-induced",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "free speech task, following the TSST protocol [20]. For the TSST,",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "after a brief period of preparation the subjects are asked to give an",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "oral presentation, within a job-interview setting, observed by two",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "interviewees who remain silent for the period of five minutes. To",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "allow consistent data partitions, we only keep data recorded under",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "the same experimental conditions. The resulting 69 participants",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "(49 of them female) are aged between 18 and 39 years, providing",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "a total amount of about 6 hours of data for the MuSe-Stress and",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "MuSe-Physio sub-challenges (cf. Table 1). Besides audio, video, and",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "text, the participants can optionally utilise the ECG, RESP and BPM",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "signals.",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "The dataset has been rated by three annotators continuously for",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "the emotional dimensions of valence and arousal, at a 2Hz sampling",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "rate, and a gold standard is obtained by the fusion of annotator",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "ratings, utilising the RAAW method, as described in Section 2.1",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "from the MuSe-Toolbox [49]. When creating the fusion a mean CC",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "inter-rater agreement of 0.204 (¬± 0.200) for valence and 0.186 (¬±",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "0.230) for arousal is obtained. The distributions of the valence and",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        },
        {
          "Table": "",
          "2: Distribution of": "arousal signals for the dataset are depicted in Figure 2.",
          "the valence": "",
          "and arousal": "",
          "classes": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe-Sent: clustering features for Valence": "std",
          "MuSe-Sent: clustering features for Arousal": "std"
        },
        {
          "MuSe-Sent: clustering features for Valence": "2",
          "MuSe-Sent: clustering features for Arousal": "2"
        },
        {
          "MuSe-Sent: clustering features for Valence": "",
          "MuSe-Sent: clustering features for Arousal": ""
        },
        {
          "MuSe-Sent: clustering features for Valence": "1",
          "MuSe-Sent: clustering features for Arousal": "1"
        },
        {
          "MuSe-Sent: clustering features for Valence": "",
          "MuSe-Sent: clustering features for Arousal": ""
        },
        {
          "MuSe-Sent: clustering features for Valence": "0",
          "MuSe-Sent: clustering features for Arousal": "0"
        },
        {
          "MuSe-Sent: clustering features for Valence": "",
          "MuSe-Sent: clustering features for Arousal": ""
        },
        {
          "MuSe-Sent: clustering features for Valence": "‚àí1",
          "MuSe-Sent: clustering features for Arousal": "‚àí1"
        },
        {
          "MuSe-Sent: clustering features for Valence": "",
          "MuSe-Sent: clustering features for Arousal": ""
        },
        {
          "MuSe-Sent: clustering features for Valence": "‚àí2",
          "MuSe-Sent: clustering features for Arousal": "‚àí2"
        },
        {
          "MuSe-Sent: clustering features for Valence": "",
          "MuSe-Sent: clustering features for Arousal": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "relPeaks\nrelEnergy": "relSoC"
        },
        {
          "relPeaks\nrelEnergy": "Figure 3: Mean of selected clustering features for each of the created classes which are used in the MuSe-Sent sub-challenge."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "count below mean (ùëüùëíùëôùê∂ùêµùëÄùëí), relative longest strike below mean (ùëüùëíùëôùêøùëÜùêµùëÄùëí), and relative longest strike above mean (ùëüùëíùëôùêøùëÜùê¥ùëÄùëí)."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "128-dimensional VGGish embedding vector every 0.25 s from the"
        },
        {
          "relPeaks\nrelEnergy": "underlying log spectrograms."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "3.3\nVision"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "Extracting specific image descriptors that match certain attributes,"
        },
        {
          "relPeaks\nrelEnergy": "e. g., face, remains the paramount focus of most visual feature ex-"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "tractors. Our offered visual feature sets are inclined to capture the"
        },
        {
          "relPeaks\nrelEnergy": "entire surroundings as well as analysing human behaviour syn-"
        },
        {
          "relPeaks\nrelEnergy": "thesised from gesture and facial expressions. Participants are also"
        },
        {
          "relPeaks\nrelEnergy": "provided with an array of extracted faces which are directly ex-"
        },
        {
          "relPeaks\nrelEnergy": "tracted from the raw frames."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "3.3.1\nMTCNN . The MTCNN [56] is used to distinguish facial ex-"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "pressions captured in the videos, pretrained on the data sets WIDER"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "FACE [54] and CelebA [23]. For MuSe-CaR, we examined the ex-"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "traction as described in detail\nin [42], where an F1 score of 86 %"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "on a labelled subset was achieved. Compared to these highly dy-"
        },
        {
          "relPeaks\nrelEnergy": "namic camera positionings (zoom, free etc.), Ulm-TSST has a static"
        },
        {
          "relPeaks\nrelEnergy": "setting. In an visual inspection aimed to control the performance,"
        },
        {
          "relPeaks\nrelEnergy": "an apparently flawless extraction was found. The extractions were"
        },
        {
          "relPeaks\nrelEnergy": "ultimately put in use as inputs for VGGface and OpenFace ."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "3.3.2\nVGGface. VGGface (version 1) [30] is aimed at the extrac-"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "tion of general facial features for images obtained by MTCNN in"
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "cropped versions. The visual geometry group of Oxford introduced"
        },
        {
          "relPeaks\nrelEnergy": "the deep CNN referred to as VGG16 [41]. The training data consti-"
        },
        {
          "relPeaks\nrelEnergy": "tutes of 2.6 million faces and over 2 500 identities. The VGGface"
        },
        {
          "relPeaks\nrelEnergy": "architecture was originally intended for supervised facial recogni-"
        },
        {
          "relPeaks\nrelEnergy": "tion purposes [41]. However, detaching the top-layer of a pretrained"
        },
        {
          "relPeaks\nrelEnergy": "version results in a 512 feature vector output referred to as VG-"
        },
        {
          "relPeaks\nrelEnergy": "Gface. Presenting high levels of performance while consuming"
        },
        {
          "relPeaks\nrelEnergy": "less data is the main advantage held for VGGface in comparison"
        },
        {
          "relPeaks\nrelEnergy": "to other facial recognition models."
        },
        {
          "relPeaks\nrelEnergy": ""
        },
        {
          "relPeaks\nrelEnergy": "3.3.3\nOpenFace. Facial features in 2D (136 dimensions) and 3D"
        },
        {
          "relPeaks\nrelEnergy": "(204 dimensions), gaze positions (288 dimensions),\nintensity and"
        },
        {
          "relPeaks\nrelEnergy": "activity of 17 Facial Action Units (FAUs) for both center and left side,"
        },
        {
          "relPeaks\nrelEnergy": "and 6 head stances were extracted from cropped faces identified"
        },
        {
          "relPeaks\nrelEnergy": "using MTCNN . This was achieved through the wide array of facial"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "challenge, we only provide intensity, as activity features appear to",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "various languages. We use the German (Prosodylab) model and the"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "be of less use for this task.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "German Prosodylab dictionary to align the Ulm-TSST transcripts."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "The time-stamps yielded by the MFA are used to align the word"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "3.3.4\nXception . Generally used to extrapolate generic vision fea-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "embeddings to the 2 Hz frames in the Ulm-TSST dataset."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "tures, Xception [17] should provide participants with environmen-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "tal features using stacked residual blocks5. Among other challenges,",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "3.6\nBaseline Model: LSTM-RNN"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "it came in first on the ILSVRC 2015 classification challenge. The net-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "In order to address the sequential nature of the input features, we"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "work is pre-trained on the ImageNet dataset compromising of 350",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "utilise a Long Short-Term Memory (LSTM)-RNN based architecture."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "million images and 17 000 classes. The then frozen network archi-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "The input feature sequences are input into uni- and bi-directional"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "tecture prepossesses a given frame through the layers until the last",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "fully connected layer from which a 2 048 deep feature dimensional",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "LSTM-RNNs with a hidden state dimensionality of ‚Ñé = {32, 64, 128},"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "to encode the feature vector sequences. We test different numbers"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "vector is obtained.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "of LSTM-RNN layers ùëõ = {1, 2, 4}. Based on experiences from initial"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "experiments, some hyperparameter searches are task-dependently"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "3.4\nLanguage: Bert",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "executed: MuSe-Wilder we search for a suitable learning rate ùëôùëü ="
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "The text\nfeature extraction process employs a Transformer lan-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "{0.0001, 0.001, 0.005}; for MuSe-Sent ùëôùëü = {0.001, 0.005, 0.01}; for"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "guage model, namely Bidirectional Encoder Representations from",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "MuSe-Stress and MuSe-Physio ùëôùëü = {0.0001, 0.0002, 0.0005, 0.001}."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "Transformers (BERT ) [8], which have already been successfully",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "As we observed overfitting in some settings of MuSe-Physio , we"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "used for a variety of NLP tasks [37, 38, 44, 47, 48]. BERT pre-trains",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "also tried L2-Regularisation with a penalty of 0.01 for this task."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "its deep representations on context of unlabelled text before fine-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "The sequence of hidden vectors from the final LSTM-RNN layer"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "tuning them on a broad selection of down-streaming NLP tasks.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "is further encoded by a feed-forward layer that outputs either a"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "During inference, the context-based representations are preserved,",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "one-dimensional prediction sequence of logits for each time step"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "excerpting one vector per word. This is in contrast to static word",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "(regression), or a single-value per prediction target (classification)."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "embeddings which give one vector per word independent of the con-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "In the training processes, the features and labels of every input"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "text. Our features are the sum of the last four BERT layers resulting",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "video are further segmented via a windowing approach [42, 43, 50]."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "in a 768 dimensional feature vector analogous to [50]. For MuSe-",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "For MuSe-Wilder and MuSe-Sent , we use a window size of 200"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "Wilder and MuSe-Sent , the base variant of BERT , pretrained on",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "steps (50 s) and a hop size of 100 steps (25 s). For MuSe-Stress and"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "English texts,\nis used. Analogously, as the Ulm-TSST data set is",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "MuSe-Physio , a window size of 300 steps (150 s) and a hop size of"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "in German, for MuSe-Stress and MuSe-Physio , the BERT (base)",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "50 steps (25 s) proved to be reasonable choices."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "pretrained on German texts is utilised.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "3.7\nFusion"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "3.5\nAlignment",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "We apply decision-level (late) fusion to evaluate co-dependencies"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "The extensive assortment of\nfeatures are from three modalities.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "of the modalities. The experiments are restricted to the best per-"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "The corresponding sampling rate of each modality differs, which",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "forming features from each modality only. For decision-level fusion,"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "leads to a different length of the extracted features along the time",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "separate models are trained individually for each modality. The"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "axis. All visual\nfeatures are incessant\nthrough the video with a",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "predictions of these are fused by training an additional LSTM-RNN"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "frame sampling of 4 Hz for MuSe-CaR and 2 Hz for Ulm-TSST,",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "model as described above. For all continuous regression tasks, we"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "which is equivalent to the labelling rate. The audio sampling of",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "apply uni-directional version with ùëôùëü = 0.0001, ‚Ñé = 64, and ùëõ = 1,"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "DeepSpectrum, and that of eGeMAPS apply the same frequency.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "and for MuSe-Sent a bi-directional one with ùëôùëü = 0.005, ‚Ñé = 32,"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "VGGish and Facial Action Units are the only feature sets relying",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "and ùëõ = 2."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "only on frames where a face is observable. By the nature of text,",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "the corresponding features do not follow a fixed sampling rate, as",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "4\nEXPERIMENTS AND BASELINE RESULTS"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "the duration of a spoken word varies.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "For each sub-challenge, we make label-aligned features available.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "For all sub-challenges, the same network architecture is applied"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "These have accurately the same stretch and time-stamps as the",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "(cf. Section 3.6). For reproducibility, we provide the detailed set of"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "provided label files. We apply zero-padding to the frames, where the",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "hyperparameters for our best models for each experiment, alongside"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "feature type is absent. Such instances include OpenFace , when no",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "our code in the corresponding GitHub repository6, where also a link"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "face appears or extraction fails, e. g., when only small faces appear in",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "to the fully trained model weights can be found. In the following"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "the original frame. The text features are repeated for the interval of",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "section, we give an overview of all baseline results as summarised"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "a word and non-linguistic parts are also imputed with zero vectors.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "in Table 3."
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "MuSe-CaR offers automatic, word-aligned transcriptions [43]. For",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "Ulm-TSST, manual transcripts of the videos are available. We use",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "4.1\nMuSe-Wilder"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "the Montreal Forced Aligner (MFA) [25] tool to obtain time-stamps",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "We evaluated several feature sets and combinations for the predic-"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "on the word level. The MFA includes pretrained acoustic models,",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "tion of the continuous valence and arousal (cf. Table 3). The input"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "features BERT in combination with our baseline architecture set"
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "5not used for MuSe-Stress or MuSe-Physio as recording environment for Ulm-TSST",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "changes only minimally and participants showed minimal movement due to their",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": ""
        },
        {
          "features offered by the OpenFace [5] toolkit. For the Ulm-TSSTdata": "stressful situation.",
          "grapheme-to-phoneme models, and pronunciation dictionaries for": "6https://github.com/lstappen/MuSe2021"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "prediction of valence leading to a CCC of .4613 on the development",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "and .5671 CCC on test set. For the prediction of arousal, using Deep-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "8.18\n21.38\n13.84\n49.69\n6.92\n5.19\n14.07\n2.22\n15.56\n62.96\nV1\nA1"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "Spectrumas input features and setting ùëôùëü = 0.001, ‚Ñé = 64, and ùëõ = 2,",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "Truth\nTruth\n7.42\n6.55\n43.45\n19.43\n23.14\n16.67\n3.12\n17.71\n40.62\n21.88\nV2\nA2"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "yields the best result of all applied systems with a CCC of .3386",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "on the test set. Generally, we found that a unidirectional LSTM-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "5.93\n16.05\n17.04\n49.88\n11.11\n17.78\n2.32\n3.87\n55.67\n20.36\nV3\nA3"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "RNN achieves better results for this task than complex bidirectional",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "configurations and is used for the reported MuSe-Wilder results.",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "7.02\n6.2\n39.67\n13.64\n33.47\n3.85\n10.92\n1.07\n10.92\n73.23\nV4\nA4"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "When fusing the best performing features of all three modalities",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "0\n1\n2\n3\n4\n0\n1\n2\n3\n4"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "V\nV\nV\nV\nV\nA\nA\nA\nA\nA"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "DeepSpectrum, VGGface, and BERT , the late fusion technique",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "Prediction\nPrediction"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "reaches .4863 and .5974 for valence and .4929 and .3257 for arousal",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "Figure\n4:\nRelative\nconfusion matrices\nover\nthe\n5\nva-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "on the development and test set, respectively. This technique yields",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "lence\n(left)\nand\narousal\n(right)\nclasses\non\nthe\ndevelop-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "the highest combined metric (mean of valence and arousal) of .4616",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "ment partition for\nthe MuSe-Sent sub-challenge. The re-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "(on test) and is our baseline.",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "sults were achieved with the LSTM baseline model using"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "the eGeMAPSfeature set with hyperparameters of ùëõ = 4 (bi-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "4.2\nMuSe-Sent",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "directional), ‚Ñé = 128, and a ùëôùëü = 0.001 for valence, and for"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "For the classification tasks in the MuSe-Sent sub-challenge, we give",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "arousal the BERT features with a uni-directional model set-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "an overview in Table 3 and further provide the confusion matrices",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "ting of ùëõ = 2, ‚Ñé = 64 and a ùëôùëü = 0.01."
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "for the best uni-modal setups tested on valence and arousal\nin",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "Figure 4. For the prediction of valence on uni-modal feature inputs,",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "the best result is achieved using the text-based BERT features as",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "input and a baseline model setting of ùëôùëü = 0.001, ‚Ñé = 64 and ùëõ = 4",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "and .4562 (test) for arousal.. It is notable that the text feature set,"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "(bi-directional), with an F1 score of 32.68 % on the development",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "BERT , performs considerably worse than the best audio and visual"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "and 31.90 % on the test set. Using the audio-based DeepSpectrum",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "features."
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "features with a ùëôùëü = 0.001, ‚Ñé = 128, and ùëõ = 2 (bi-directional), results",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "We found that, in general, valence reaches a stronger final result"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "in our highest F1 score for arousal with 33.52 % on the development",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "than arousal for this task. While this is not surprising for text fea-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "and 33.16 % on the test set. Across both targets, we find that LSTM-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "tures, it counters conventional expectations for the audio modality."
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "RNN models with a bidirectional setting and at least two layers tend",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "A major reason for the poor arousal prediction results may be the"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "to achieve better results for this task than smaller architectures.",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "TSST scenario, which imitates a job interview. Typically, intervie-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "Partially, we see improvements when we apply late fusion. For",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "wees try to remain neutral, suppressing nervousness, hence, the"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "valence, utilising the predictions of VGGface and BERT yields a",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "arousal shown to their counterpart would be minimal, thus, making"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "performance of 32.91 % F1-score on the test set. For arousal, the",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "arousal more difficult to detect in the Ulm-TSST data set than other"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "audio-visual\nfusion set-up (VGGish and FAU) also improves on",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "comparable multimodal emotion recognition data sets."
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "the test set, with an F1 score of 35.12 %. Looking at the combined",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "Although we do not evaluate the provided bio-signal features"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "scores (mean of valence and arousal), using the BERT features alone",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "systematically here, we encourage participants to explore them. To"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "comes out on top for the development set, reaching a 35.48 % F1",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "give an example, we achieve .2495 and .1537 CCC for valence on"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "score, while fusing the video- and text-based predictions achieves",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "the development and test sets, respectively, by using only the three"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "the highest F1 score of 32.82 % on the test set.",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "provided bio-signals (at a sampling rate of 2 Hz) as features in a"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "four-layer LSTM. Similarly, they show also promising results for"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "4.3\nMuSe-Stress",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "the prediction of arousal, reaching .1954 CCC on the development"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "The best results from all feature sets and fusion of modalities are",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "and .2189 CCC on test partition."
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "reported in Table 37. Having searched the hyperparameter combi-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "nations mentioned, we achieve the best results on all settings with",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "4.4\nMuSe-Physio"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "a 4-layered unidirectional LSTM equipped with 64-dimensional",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "For MuSe-Physio ,\nthe same LSTM configuration as for MuSe-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "hidden states and a learning rate of 0.0002 with a maximum of",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "Stress is applied. The results are reported in Table 3. Again, au-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "100 epochs, and early stopping with a patience of 15 epochs. Here,",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "dio and video features considerably outperform the textual BERT"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "eGeMAPS outperforms all other single feature sets for the predic-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "features. While BERT only achieves .2583 and .1604 CCC on de-"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "tion of valence, achieving .5845 CCC on development and .5018",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "velopment and test data, respectively, the best audio feature set"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "CCC on the test set. Regarding arousal, eGeMAPS is the best scoring",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "(DeepSpectrum)\nleads to .4423 and .4162 CCC on development"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "single feature set, leading to .4304 and .4416 CCC on development",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "and test data, respectively. Consistently, visual features outperform"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "and test set, respectively. For both valence and arousal prediction,",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "the textual ones, too. The best visual feature set (VGGface) yields"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "the fusions of the best audio and vision feature sets result in the",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ".3903 and .4582 on development and test data respectively and"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "best performance overall. They achieve CCC values of .6966 (de-",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "hence shows comparable performance to DeepSpectrum. Like in"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "velopment) and .5614 (test) for valence, and .5043 (development)",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "MuSe-Stress , the late fusion of the best audio (VGGish) and video"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "(VGGface) predictions yield the best results, namely .4913 CCC on"
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "7Of note, besides eGeMAPS, we also normalise the VGGish features for predicting",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": ""
        },
        {
          "to ùëôùëü = 0.005, ‚Ñé = 128, and ùëõ = 4 show superior results for the": "arousal.",
          "18.31\n7.04\n40.85\n14.08\n19.72\n29.32\n0.8\n7.63\n34.54\n27.71\nV0\nA0": "development data and .4908 CCC on test data."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: Reporting Valence, Arousal, Combined (0.5¬∑ùê¥ùëüùëúùë¢ùë†ùëéùëô +0.5¬∑ùëâùëéùëôùëíùëõùëêùëí), as well as physical-arousal in CCC for MuSe-",
      "data": [
        {
          "Table 3: Reporting Valence, Arousal, Combined (0.5 ¬∑ ùê¥ùëüùëúùë¢ùë†ùëéùëô + 0.5 ¬∑ ùëâ ùëéùëôùëíùëõùëêùëí), as well as physical-arousal": "Wilder , MuSe-Stress , and MuSe-Physio on the devel(opment) and test partitions. For MuSe-Sent , we report F1 score across",
          "in CCC for MuSe-": ""
        },
        {
          "Table 3: Reporting Valence, Arousal, Combined (0.5 ¬∑ ùê¥ùëüùëúùë¢ùë†ùëéùëô + 0.5 ¬∑ ùëâ ùëéùëôùëíùëõùëêùëí), as well as physical-arousal": "five classes (20 % by chance). As feature sets, we test DeepSpectrum, VGGish, and eGeMAPS for audio; Xception , VGGface",
          "in CCC for MuSe-": ""
        },
        {
          "Table 3: Reporting Valence, Arousal, Combined (0.5 ¬∑ ùê¥ùëüùëúùë¢ùë†ùëéùëô + 0.5 ¬∑ ùëâ ùëéùëôùëíùëõùëêùëí), as well as physical-arousal": "and FAU for video; and BERT for text. All utilised features are aligned to the label timestamps by imputing missing values or",
          "in CCC for MuSe-": ""
        },
        {
          "Table 3: Reporting Valence, Arousal, Combined (0.5 ¬∑ ùê¥ùëüùëúùë¢ùë†ùëéùëô + 0.5 ¬∑ ùëâ ùëéùëôùëíùëõùëêùëí), as well as physical-arousal": "repeating the word embeddings",
          "in CCC for MuSe-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Reporting Valence, Arousal, Combined (0.5¬∑ùê¥ùëüùëúùë¢ùë†ùëéùëô +0.5¬∑ùëâùëéùëôùëíùëõùëêùëí), as well as physical-arousal in CCC for MuSe-",
      "data": [
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": "Features"
        },
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": "DeepSpectrum"
        },
        {
          "repeating the word embeddings": "VGGish"
        },
        {
          "repeating the word embeddings": "eGeMAPS"
        },
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": "Xception"
        },
        {
          "repeating the word embeddings": "VGGface"
        },
        {
          "repeating the word embeddings": "FAU"
        },
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": "Bert"
        },
        {
          "repeating the word embeddings": ""
        },
        {
          "repeating the word embeddings": "best A + V"
        },
        {
          "repeating the word embeddings": "best A + T"
        },
        {
          "repeating the word embeddings": "best V + T"
        },
        {
          "repeating the word embeddings": "best V + A + T"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Reporting Valence, Arousal, Combined (0.5¬∑ùê¥ùëüùëúùë¢ùë†ùëéùëô +0.5¬∑ùëâùëéùëôùëíùëõùëêùëí), as well as physical-arousal in CCC for MuSe-",
      "data": [
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "best V + A + T\n30.37 / 31.01\n36.72 / 33.20\n.4863 / .5974\n.4929 / .3257\n.4896 / .4616",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "33.55 / 32.11\n.6769 / .5349\n.4819 / .3472\n.5794 / .4411\n.4330 / .3205"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "Using the one-dimensional biological signals as features might",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "and exciting combinations of the modalities ‚Äì such as linking modal-"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "also be beneficial here, even though our model fails to generalise",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "ities at earlier stages in the pipeline or more closely."
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "for them. We achieve CCCs of .4188 on the development and .3328",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": ""
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "on the test set using a 4 LSTM layer setting and a learning rate of",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "6\nACKNOWLEDGMENTS"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "0.01.",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": ""
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "This project has received funding from the European Union‚Äôs Hori-"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "zon 2020 research and the DFG‚Äôs Reinhart Koselleck project No."
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "5\nCONCLUSIONS",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": ""
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "lenge, the BMW Group, and audEERING."
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "In this paper, we introduced MuSe 2021 ‚Äì the second Multimodal",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": ""
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "Sentiment Analysis challenge. MuSe 2021 utilises the MuSe-CaR",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "REFERENCES"
        },
        {
          "best V + T\n.4641 / .5874\n.3111 / .1767\n.3876 / .3821\n37.51 / 32.73\n30.17 / 32.91": "multimodal corpus of emotional car reviews and the Ulm-TSST cor-",
          ".5588 / .4250\n.2891 / .1586\n.4240 / .3828\n.2734 / .3000\n33.84 / 32.82": "[1]\nShahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Nicholas Cummins, Michael"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Reporting Valence, Arousal, Combined (0.5¬∑ùê¥ùëüùëúùë¢ùë†ùëéùëô +0.5¬∑ùëâùëéùëôùëíùëõùëêùëí), as well as physical-arousal in CCC for MuSe-",
      "data": [
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "lenge, the BMW Group, and audEERING."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "REFERENCES"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[1]\nShahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Nicholas Cummins, Michael"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Freitag, Sergey Pugachevskiy, Alice Baird, and Bj√∂rn W Schuller. 2017. Snore"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Sound Classification Using Image-Based Deep Spectrum Features.. In Proceedings"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "of INTERSPEECH, Vol. 434. 3512‚Äì3516."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "John Arevalo, Thamar Solorio, Manuel Montes-y G√≥mez, and Fabio A Gonz√°lez.\n[2]"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "2020. Gated multimodal networks. Neural Computing and Applications (2020),"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "1‚Äì20."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[3] Alice Baird, Shahin Amiriparian, and Bj√∂rn Schuller. 2019. Can deep generative"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "audio be emotional? Towards an approach for personalised emotional audio gen-"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "eration. In 2019 IEEE 21st International Workshop on Multimedia Signal Processing"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "(MMSP). IEEE, 1‚Äì5."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[4] Alice Baird, Lukas Stappen, Lukas Christ, Lea Schumann, Eva-Maria Me√üner, and"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Bj√∂rn W Schuller. 2021. A Physiologically-adapted Gold Standard for Arousal"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "During a Stress Induced Scenario. In Proceedings of the 2nd Multimodal Sentiment"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Analysis Challenge, co-located with the 29th ACM International Conference on"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Multimedia (ACMMM). ACM, Changu, China."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[5] Tadas Baltru≈°aitis, Peter Robinson, and Louis-Philippe Morency. 2016. OpenFace:"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "an Open Source Facial Behavior Analysis Toolkit.\nIn Proceedings of\nthe IEEE"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Winter Conference on Applications of Computer Vision. IEEE."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[6] Yekta Said Can, Bert Arnrich, and Cem Ersoy. 2019.\nStress detection in daily"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "life scenarios using smart phones and wearable sensors: A survey.\nJournal of"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Biomedical Informatics 92 (2019)."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[7] Delphine Caruelle, Anders Gustafsson, Poja Shams, and Line Lervik-Olsen. 2019."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "The use of electrodermal activity (EDA) measurement to understand consumer"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "emotions‚Äìa literature review and a call for action. Journal of Business Research"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "104 (2019), 146‚Äì160."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[8]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Pre-training of Deep Bidirectional Transformers for Language Understanding. In"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Proceedings of the 2019 Conference of the North American Chapter of the Association"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "for Computational Linguistics: Human Language Technologies. 4171‚Äì4186."
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "[9] Abhinav Dhall, Garima Sharma, Roland Goecke, and Tom Gedeon. 2020. Emotiw"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "2020: Driver gaze, group emotion, student engagement and physiological signal"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": ""
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "based challenges. In Proceedings of the 2020 International Conference on Multimodal"
        },
        {
          "442218748 (AUDI0NOMOUS). We thank the sponsors of the Chal-": "Interaction (ICMI). 784‚Äì789."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Vol. 3. Wiley New York.",
          "Physics: Conference Series, Vol. 1453.": "[34]\nFabien Ringeval, Bj√∂rn Schuller, Michel Valstar, Roddy Cowie, Heysem Kaya,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[11]\nJ. O. Egede, S. Song, T. A. Olugbade, C. Wang, A. C. D. C. Williams, H. Meng, M.",
          "Physics: Conference Series, Vol. 1453.": "Maximilian Schmitt, Shahin Amiriparian, Nicholas Cummins, Denis Lalanne,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Aung, N. D. Lane, M. Valstar, and N. Bianchi-Berthouze. 2020. EMOPAIN Chal-",
          "Physics: Conference Series, Vol. 1453.": "Adrien Michaud, et al. 2018. AVEC 2018 workshop and challenge: Bipolar disorder"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "lenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions. In",
          "Physics: Conference Series, Vol. 1453.": "and cross-cultural affect recognition. In Proceedings of the 2018 on Audio/Visual"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition",
          "Physics: Conference Series, Vol. 1453.": "Emotion Challenge and Workshop. 3‚Äì13."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "(FG 2020). 849‚Äì856.",
          "Physics: Conference Series, Vol. 1453.": "[35]\nFabien Ringeval, Bj√∂rn Schuller, Michel Valstar, Jonathan Gratch, Roddy Cowie,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[12]\nFlorian Eyben, Klaus R Scherer, Bj√∂rn W Schuller, Johan Sundberg, Elisabeth",
          "Physics: Conference Series, Vol. 1453.": "Stefan Scherer, Sharon Mozgai, Nicholas Cummins, Maximilian Schmitt, and Maja"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Andr√©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S",
          "Physics: Conference Series, Vol. 1453.": "Pantic. 2017. Avec 2017: Real-life depression, and affect recognition workshop"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Narayanan, et al. 2015. The Geneva minimalistic acoustic parameter set (GeMAPS)",
          "Physics: Conference Series, Vol. 1453.": "and challenge. In Proceedings of the 7th Annual Workshop on Audio/Visual Emotion"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "for voice research and affective computing.\nIEEE Transactions on Affective Com-",
          "Physics: Conference Series, Vol. 1453.": "Challenge. 3‚Äì9."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "puting 7, 2 (2015), 190‚Äì202.",
          "Physics: Conference Series, Vol. 1453.": "[36]\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Florian Eyben, Martin W√∂llmer, and Bj√∂rn Schuller. 2010. Opensmile: the munich\n[13]",
          "Physics: Conference Series, Vol. 1453.": "validation of cluster analysis. J. Comput. Appl. Math. 20 (1987), 53‚Äì65."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th",
          "Physics: Conference Series, Vol. 1453.": "[37] Bj√∂rn W Schuller, Anton Batliner, Christian Bergler, Cecilia Mascolo, Jing Han,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "ACM International Conference on Multimedia. 1459‚Äì1462.",
          "Physics: Conference Series, Vol. 1453.": "Iulia Lefter, Heysem Kaya, Shahin Amiriparian, Alice Baird, Lukas Stappen,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[14]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence,",
          "Physics: Conference Series, Vol. 1453.": "et al. 2021. The INTERSPEECH 2021 Computational Paralinguistics Challenge:"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology",
          "Physics: Conference Series, Vol. 1453.": "COVID-19 cough, COVID-19 speech, escalation & primates.\narXiv preprint"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "and human-labeled dataset for audio events. In 2017 IEEE International Conference",
          "Physics: Conference Series, Vol. 1453.": "arXiv:2102.13468 (2021)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 776‚Äì780.",
          "Physics: Conference Series, Vol. 1453.": "[38] Bj√∂rn W Schuller, Anton Batliner, Christian Bergler, Eva-Maria Messner, An-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[15] Raul Gomez, Jaume Gibert, Lluis Gomez, and Dimosthenis Karatzas. 2020. Ex-",
          "Physics: Conference Series, Vol. 1453.": "tonia Hamilton, Shahin Amiriparian, Alice Baird, Georgios Rizos, Maximilian"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "ploring Hate Speech Detection in Multimodal Publications. In The IEEE Winter",
          "Physics: Conference Series, Vol. 1453.": "Schmitt, Lukas Stappen, et al. 2020. The INTERSPEECH 2020 Computational"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Conference on Applications of Computer Vision. 1470‚Äì1478.",
          "Physics: Conference Series, Vol. 1453.": "Paralinguistics Challenge: Elderly Emotion, Breathing & Masks. Proceedings of"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[16] Michael Grimm and Kristian Kroschel. 2005. Evaluation of natural emotions using",
          "Physics: Conference Series, Vol. 1453.": "INTERSPEECH (2020)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "self assessment manikins. In IEEE Workshop on Automatic Speech Recognition and",
          "Physics: Conference Series, Vol. 1453.": "[39] Bj√∂rn W Schuller, Stefan Steidl, Anton Batliner, Peter B Marschik, Harald Baumeis-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Understanding, 2005. IEEE, 381‚Äì385.",
          "Physics: Conference Series, Vol. 1453.": "ter, Fengquan Dong, Simone Hantke, Florian B Pokorny, Eva-Maria Rathner,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual",
          "Physics: Conference Series, Vol. 1453.": "Katrin D Bartl-Pokorny, et al. 2018. The INTERSPEECH 2018 Computational"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "learning for image recognition. In Proceedings of the IEEE Conference on Computer",
          "Physics: Conference Series, Vol. 1453.": "Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats.."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Vision and Pattern Recognition. 770‚Äì778.",
          "Physics: Conference Series, Vol. 1453.": "In Proceedings of INTERSPEECH. 122‚Äì126."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[18]\nShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren",
          "Physics: Conference Series, Vol. 1453.": "[40]\nJainendra Shukla, Miguel Barreda-Angeles, Joan Oliver, GC Nandi, and Domenec"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan",
          "Physics: Conference Series, Vol. 1453.": "Puig. 2019. Feature extraction and selection for emotion recognition from elec-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Seybold, et al. 2017. CNN architectures for large-scale audio classification. In",
          "Physics: Conference Series, Vol. 1453.": "trodermal activity.\nIEEE Transactions on Affective Computing (2019)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing",
          "Physics: Conference Series, Vol. 1453.": "[41] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "(ICASSP). IEEE, 131‚Äì135.",
          "Physics: Conference Series, Vol. 1453.": "for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[19] Holger Hoffmann, Andreas Scheck, Timo Schuster, Steffen Walter, Kerstin Lim-",
          "Physics: Conference Series, Vol. 1453.": "[42]\nLukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis, Xinchen Du,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "brecht, Harald C Traue, and Henrik Kessler. 2012. Mapping discrete emotions",
          "Physics: Conference Series, Vol. 1453.": "Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Bjoern W. Schuller,\nIulia"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "into the dimensional space: An empirical approach. In 2012 IEEE International",
          "Physics: Conference Series, Vol. 1453.": "Lefter, Erik Cambria, and Ioannis Kompatsiaris. 2020. MuSe 2020 Challenge"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Conference on Systems, Man, and Cybernetics (SMC). IEEE, 3316‚Äì3320.",
          "Physics: Conference Series, Vol. 1453.": "and Workshop: Multimodal Sentiment Analysis, Emotion-Target Engagement"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[20] Clemens Kirschbaum, Karl-Martin Pirke, and Dirk H Hellhammer. 1993. The",
          "Physics: Conference Series, Vol. 1453.": "and Trustworthiness Detection in Real-Life Media.\nIn Proceedings of\nthe 1st"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "‚ÄòTrier Social Stress Test‚Äô‚Äìa tool for investigating psychobiological stress responses",
          "Physics: Conference Series, Vol. 1453.": "International on Multimodal Sentiment Analysis in Real-Life Media Challenge and"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "in a laboratory setting. Neuropsychobiology 28, 1-2 (1993), 76‚Äì81.",
          "Physics: Conference Series, Vol. 1453.": "Workshop. ACM, 35‚Äì44."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[21] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos Zafeiriou. 2020.",
          "Physics: Conference Series, Vol. 1453.": "Lukas Stappen, Alice Baird, Lea Schumann, and Bj√∂rn Schuller. 2021. The Mul-\n[43]"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Analysing affective behavior in the first ABAW 2020 competition. arXiv preprint",
          "Physics: Conference Series, Vol. 1453.": "timodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "arXiv:2001.11409 (2020).",
          "Physics: Conference Series, Vol. 1453.": "Insights and Improvements.\nIEEE Transactions on Affective Computing (Early"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[22] Yanchi Liu, Zhongmou Li, Hui Xiong, Xuedong Gao, and Junjie Wu. 2010. Un-",
          "Physics: Conference Series, Vol. 1453.": "https://doi.org/10.1109/TAFFC.2021.3097002\nAccess) (June 2021)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "derstanding of internal clustering validation measures. In Proceedings of the IEEE",
          "Physics: Conference Series, Vol. 1453.": "[44]\nLukas Stappen, Fabian Brunn, and Bj√∂rn Schuller. 2020. Cross-lingual zero-and"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "International Conference on Data Mining. IEEE, 911‚Äì916.",
          "Physics: Conference Series, Vol. 1453.": "few-shot hate speech detection utilising frozen transformer language models and"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face",
          "Physics: Conference Series, Vol. 1453.": "AXEL. arXiv preprint arXiv:2004.13850 (2020)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Attributes in the Wild. In Proceedings of International Conference on Computer",
          "Physics: Conference Series, Vol. 1453.": "[45]\nLukas Stappen, Vincent Karas, Nicholas Cummins, Fabien Ringeval, Klaus Scherer,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Vision (ICCV).",
          "Physics: Conference Series, Vol. 1453.": "and Bj√∂rn Schuller. 2019. From speech to facial activity: towards cross-modal"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[24]\nStuart Lloyd. 1982. Least squares quantization in PCM.\nIEEE Transactions on",
          "Physics: Conference Series, Vol. 1453.": "sequence-to-sequence attention networks. In 2019 IEEE 21st International Work-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Information Theory 28, 2 (1982), 129‚Äì137.",
          "Physics: Conference Series, Vol. 1453.": "shop on Multimedia Signal Processing (MMSP). IEEE, 1‚Äì6."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[25] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan",
          "Physics: Conference Series, Vol. 1453.": "[46]\nLukas\nStappen,\nEva-Maria Me√üner,\nErik\nCambria, Guoying\nZhao,\nand"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Sonderegger. 2017. Montreal Forced Aligner: Trainable Text-Speech Alignment",
          "Physics: Conference Series, Vol. 1453.": "Bj√∂rn W.\nSchuller.\n2021.\nMuSe\n2021 Challenge: Multimodal\nEmotion,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Using Kaldi.. In Proceedings of INTERSPEECH, Vol. 2017. 498‚Äì502.",
          "Physics: Conference Series, Vol. 1453.": "Sentiment,Physiological-Emotion, and Stress Detection. In 29th ACM Interna-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[26]\nSaif M Mohammad. 2016. Sentiment analysis: Detecting valence, emotions, and",
          "Physics: Conference Series, Vol. 1453.": "tional Conference on Multimedia (ACMMM). ACM, Virtual Event, China."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "other affectual states from text.\nIn Emotion Measurement. Elsevier, 201‚Äì237.",
          "Physics: Conference Series, Vol. 1453.": "Lukas Stappen, Georgios Rizos, Madina Hasan, Thomas Hain, and Bj√∂rn W\n[47]"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[27] Mihalis A Nicolaou, Vladimir Pavlovic, and Maja Pantic. 2014. Dynamic proba-",
          "Physics: Conference Series, Vol. 1453.": "Schuller. 2020. Uncertainty-Aware Machine Support for Paper Reviewing on the"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "bilistic cca for analysis of affective behavior and fusion of continuous annotations.",
          "Physics: Conference Series, Vol. 1453.": "INTERSPEECH 2019 Submission Corpus. Proceedings of INTERSPEECH (2020),"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 7 (2014), 1299‚Äì",
          "Physics: Conference Series, Vol. 1453.": "1808‚Äì1812."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "1311.",
          "Physics: Conference Series, Vol. 1453.": "[48]\nLukas Stappen, Bj√∂rn Schuller, Iulia Lefter, Erik Cambria, and Ioannis Kompat-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[28]\nYannis Panagakis, Mihalis A Nicolaou, Stefanos Zafeiriou, and Maja Pantic. 2015.",
          "Physics: Conference Series, Vol. 1453.": "siaris. 2020. Summary of MuSe 2020: Multimodal Sentiment Analysis, Emotion-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Robust correlated and individual component analysis.\nIEEE Transactions on",
          "Physics: Conference Series, Vol. 1453.": "target Engagement and Trustworthiness Detection in Real-life Media. In Proceed-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Pattern Analysis and Machine Intelligence 38, 8 (2015), 1665‚Äì1678.",
          "Physics: Conference Series, Vol. 1453.": "ings of the 28th ACM International Conference on Multimedia. 4769‚Äì4770."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[29] Vedhas Pandit and Bj√∂rn Schuller. 2019. On Many-to-Many Mapping Between",
          "Physics: Conference Series, Vol. 1453.": "[49]\nLukas Stappen, Lea Schumann, Benjamin Sertolli, Alice Baird, Benjamin Weigel,"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Concordance Correlation Coefficient and Mean Square Error.\narXiv preprint",
          "Physics: Conference Series, Vol. 1453.": "Erik Cambria, and Bj√∂rn W Schuller. 2021. MuSe-Toolbox: The Multimodal Senti-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "arXiv:1902.05180 (2019).",
          "Physics: Conference Series, Vol. 1453.": "ment Analysis Continuous Annotation Fusion and Discrete Class Transformation"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[30] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep Face",
          "Physics: Conference Series, Vol. 1453.": "Toolbox. In Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, co-"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Recognition.\nIn Proceedings of\nthe British Machine Vision Conference (BMVC).",
          "Physics: Conference Series, Vol. 1453.": "located with the 29th ACM International Conference on Multimedia (ACMMM)."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "41.1‚Äì41.12.",
          "Physics: Conference Series, Vol. 1453.": "ACM, Changu, China."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Sara Pourmohammadi and Ali Maleki. 2020. Stress detection using ECG and EMG\n[31]",
          "Physics: Conference Series, Vol. 1453.": "Licai Sun, Zheng Lian, Jianhua Tao, Bin Liu, and Mingyue Niu. 2020. Multi-\n[50]"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "signals: A comprehensive study. Computer Methods and Programs in Biomedicine",
          "Physics: Conference Series, Vol. 1453.": "modal Continuous Dimensional Emotion Recognition Using Recurrent Neural"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "193 (2020), 105482.",
          "Physics: Conference Series, Vol. 1453.": "Network and Self-Attention Mechanism. In Proceedings of the 1st International"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[32] Daniel Preo≈£iuc-Pietro, H Andrew Schwartz, Gregory Park, Johannes Eichstaedt,",
          "Physics: Conference Series, Vol. 1453.": "on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Margaret Kern, Lyle Ungar, and Elisabeth Shulman. 2016. Modelling valence and",
          "Physics: Conference Series, Vol. 1453.": "27‚Äì34."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "arousal in facebook posts. In Proceedings of the 7th Workshop on Computational",
          "Physics: Conference Series, Vol. 1453.": "[51] Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Approaches to Subjectivity, Sentiment and Social Media Analysis. 9‚Äì15.",
          "Physics: Conference Series, Vol. 1453.": "2010. Sentiment strength detection in short informal text. Journal of the American"
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "[33] Xiaoyu Qiu, Zhiquan Feng, Xiaohui Yang, and Jinglan Tian. 2020. Multimodal",
          "Physics: Conference Series, Vol. 1453.": "Society for Information Science and Technology 61, 12 (2010), 2544‚Äì2558."
        },
        {
          "[10] Richard O Duda, Peter E Hart, et al. 1973. Pattern classification and scene analysis.": "Fusion of Speech and Gesture Recognition based on Deep Learning. In Journal of",
          "Physics: Conference Series, Vol. 1453.": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "Bilakhia, Sebastian Schnieder, Roddy Cowie, and Maja Pantic. 2013. AVEC 2013:",
          "http://arxiv.org/abs/1511.06523": "[55] Amir Zadeh, Louis-Philippe Morency, Paul Pu Liang, and Soujanya Poria (Eds.)."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "the continuous audio/visual emotion and depression recognition challenge. In",
          "http://arxiv.org/abs/1511.06523": "2020. Second Grand-Challenge and Workshop on Multimodal Language (Challenge-"
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "Proceedings of\nthe 3rd ACM International Workshop on Audio/Visual Emotion",
          "http://arxiv.org/abs/1511.06523": "HML)."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "Challenge. ACM, 3‚Äì10.",
          "http://arxiv.org/abs/1511.06523": "[56] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016.\nJoint Face"
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "[53] Martin W√∂llmer, Florian Eyben, Stephan Reiter, Bj√∂rn Schuller, Cate Cox, Ellen",
          "http://arxiv.org/abs/1511.06523": "Detection and Alignment Using Multitask Cascaded Convolutional Networks."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "Douglas-Cowie, and Roddy Cowie. 2008. Abandoning emotion classes-towards",
          "http://arxiv.org/abs/1511.06523": "IEEE Signal Processing Letters 23 (04 2016)."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "continuous emotion recognition with modelling of long-range dependencies. In",
          "http://arxiv.org/abs/1511.06523": "[57]\nFeng Zhou and Fernando De la Torre. 2015. Generalized canonical time warping."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "Proceedings of INTERSPEECH. 597‚Äì600.",
          "http://arxiv.org/abs/1511.06523": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 2 (2015), 279‚Äì"
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "[54]\nShuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2015. WIDER FACE:",
          "http://arxiv.org/abs/1511.06523": "294."
        },
        {
          "[52] Michel Valstar, Bj√∂rn Schuller, Kirsty Smith, Florian Eyben, Bihan Jiang, Sanjay": "arXiv:1511.06523\nA Face Detection Benchmark. CoRR abs/1511.06523 (2015).",
          "http://arxiv.org/abs/1511.06523": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
      "authors": [
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Nicholas Cummins",
        "Michael Freitag",
        "Sergey Pugachevskiy",
        "Alice Baird",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "2",
      "title": "Gated multimodal networks. Neural Computing and Applications",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y G√≥mez",
        "Fabio Gonz√°lez"
      ],
      "year": "2020",
      "venue": "Gated multimodal networks. Neural Computing and Applications"
    },
    {
      "citation_id": "3",
      "title": "Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation",
      "authors": [
        "Alice Baird",
        "Shahin Amiriparian",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "4",
      "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario",
      "authors": [
        "Alice Baird",
        "Lukas Stappen",
        "Lukas Christ",
        "Lea Schumann",
        "Eva-Maria Me√üner",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, co-located with the 29th ACM International Conference on Multimedia (ACMMM)"
    },
    {
      "citation_id": "5",
      "title": "OpenFace: an Open Source Facial Behavior Analysis Toolkit",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "Stress detection in daily life scenarios using smart phones and wearable sensors: A survey",
      "authors": [
        "Yekta Said Can",
        "Bert Arnrich",
        "Cem Ersoy"
      ],
      "year": "2019",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "7",
      "title": "The use of electrodermal activity (EDA) measurement to understand consumer emotions-a literature review and a call for action",
      "authors": [
        "Delphine Caruelle",
        "Anders Gustafsson",
        "Poja Shams",
        "Line Lervik-Olsen"
      ],
      "year": "2019",
      "venue": "Journal of Business Research"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "9",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "Abhinav Dhall",
        "Garima Sharma",
        "Roland Goecke",
        "Tom Gedeon"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "10",
      "title": "Pattern classification and scene analysis",
      "authors": [
        "Peter Richard O Duda",
        "Hart"
      ],
      "year": "1973",
      "venue": "Pattern classification and scene analysis"
    },
    {
      "citation_id": "11",
      "title": "EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions",
      "authors": [
        "J Egede",
        "S Song",
        "T Olugbade",
        "C Wang",
        "A Williams",
        "H Meng",
        "M Aung",
        "N Lane",
        "M Valstar",
        "N Bianchi-Berthouze"
      ],
      "year": "2020",
      "venue": "EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and Bodily Expressions"
    },
    {
      "citation_id": "12",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Exploring Hate Speech Detection in Multimodal Publications",
      "authors": [
        "Raul Gomez",
        "Jaume Gibert",
        "Lluis Gomez",
        "Dimosthenis Karatzas"
      ],
      "year": "2020",
      "venue": "The IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel"
      ],
      "year": "2005",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "17",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Mapping discrete emotions into the dimensional space: An empirical approach",
      "authors": [
        "Holger Hoffmann",
        "Andreas Scheck",
        "Timo Schuster",
        "Steffen Walter",
        "Kerstin Limbrecht",
        "Harald Traue",
        "Henrik Kessler"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "20",
      "title": "The 'Trier Social Stress Test'-a tool for investigating psychobiological stress responses in a laboratory setting",
      "authors": [
        "Clemens Kirschbaum",
        "Karl-Martin Pirke",
        "Dirk Hellhammer"
      ],
      "year": "1993",
      "venue": "Neuropsychobiology"
    },
    {
      "citation_id": "21",
      "title": "Analysing affective behavior in the first ABAW 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first ABAW 2020 competition",
      "arxiv": "arXiv:2001.11409"
    },
    {
      "citation_id": "22",
      "title": "Understanding of internal clustering validation measures",
      "authors": [
        "Yanchi Liu",
        "Zhongmou Li",
        "Hui Xiong",
        "Xuedong Gao",
        "Junjie Wu"
      ],
      "year": "2010",
      "venue": "Proceedings of the IEEE International Conference on Data Mining"
    },
    {
      "citation_id": "23",
      "title": "Deep Learning Face Attributes in the Wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "24",
      "title": "Least squares quantization in PCM",
      "authors": [
        "Stuart Lloyd"
      ],
      "year": "1982",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "25",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "26",
      "title": "Sentiment analysis: Detecting valence, emotions, and other affectual states from text",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2016",
      "venue": "Emotion Measurement"
    },
    {
      "citation_id": "27",
      "title": "Dynamic probabilistic cca for analysis of affective behavior and fusion of continuous annotations",
      "authors": [
        "A Mihalis",
        "Vladimir Nicolaou",
        "Maja Pavlovic",
        "Pantic"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Robust correlated and individual component analysis",
      "authors": [
        "Yannis Panagakis",
        "A Mihalis",
        "Stefanos Nicolaou",
        "Maja Zafeiriou",
        "Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "On Many-to-Many Mapping Between Concordance Correlation Coefficient and Mean Square Error",
      "authors": [
        "Vedhas Pandit",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "On Many-to-Many Mapping Between Concordance Correlation Coefficient and Mean Square Error",
      "arxiv": "arXiv:1902.05180"
    },
    {
      "citation_id": "30",
      "title": "Deep Face Recognition",
      "authors": [
        "M Omkar",
        "Andrea Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "Proceedings of the British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "31",
      "title": "Stress detection using ECG and EMG signals: A comprehensive study",
      "authors": [
        "Sara Pourmohammadi",
        "Ali Maleki"
      ],
      "year": "2020",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "32",
      "title": "Modelling valence and arousal in facebook posts",
      "authors": [
        "Daniel Preo≈£iuc-Pietro",
        "Andrew Schwartz",
        "Gregory Park",
        "Johannes Eichstaedt",
        "Margaret Kern",
        "Lyle Ungar",
        "Elisabeth Shulman"
      ],
      "year": "2016",
      "venue": "Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "33",
      "title": "Multimodal Fusion of Speech and Gesture Recognition based on Deep Learning",
      "authors": [
        "Xiaoyu Qiu",
        "Zhiquan Feng",
        "Xiaohui Yang",
        "Jinglan Tian"
      ],
      "year": "2020",
      "venue": "In Journal of Physics: Conference Series"
    },
    {
      "citation_id": "34",
      "title": "AVEC 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Denis Lalanne",
        "Adrien Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "35",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "Fabien Ringeval",
        "Bj√∂rn Schuller",
        "Michel Valstar",
        "Jonathan Gratch",
        "Roddy Cowie",
        "Stefan Scherer",
        "Sharon Mozgai",
        "Nicholas Cummins",
        "Maximilian Schmitt",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "36",
      "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
      "authors": [
        "J Peter",
        "Rousseeuw"
      ],
      "year": "1987",
      "venue": "J. Comput. Appl. Math"
    },
    {
      "citation_id": "37",
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "authors": [
        "W Bj√∂rn",
        "Anton Schuller",
        "Christian Batliner",
        "Cecilia Bergler",
        "Jing Mascolo",
        "Iulia Han",
        "Heysem Lefter",
        "Shahin Kaya",
        "Alice Amiriparian",
        "Lukas Baird",
        "Stappen"
      ],
      "year": "2021",
      "venue": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "arxiv": "arXiv:2102.13468"
    },
    {
      "citation_id": "38",
      "title": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion",
      "authors": [
        "W Bj√∂rn",
        "Anton Schuller",
        "Christian Batliner",
        "Eva-Maria Bergler",
        "Antonia Messner",
        "Shahin Hamilton",
        "Alice Amiriparian",
        "Georgios Baird",
        "Maximilian Rizos",
        "Lukas Schmitt",
        "Stappen"
      ],
      "year": "2020",
      "venue": "The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly Emotion"
    },
    {
      "citation_id": "39",
      "title": "The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats",
      "authors": [
        "W Bj√∂rn",
        "Stefan Schuller",
        "Anton Steidl",
        "Batliner",
        "Harald Peter B Marschik",
        "Fengquan Baumeister",
        "Simone Dong",
        "Florian Hantke",
        "Eva-Maria Pokorny",
        "Rathner",
        "D Katrin",
        "Bartl-Pokorny"
      ],
      "year": "2018",
      "venue": "The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical & Self-Assessed Affect, Crying & Heart Beats"
    },
    {
      "citation_id": "40",
      "title": "Feature extraction and selection for emotion recognition from electrodermal activity",
      "authors": [
        "Jainendra Shukla",
        "Miguel Barreda-Angeles",
        "Joan Oliver",
        "G Nandi",
        "Domenec Puig"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "42",
      "title": "Erik Cambria, and Ioannis Kompatsiaris. 2020. MuSe 2020 Challenge and Workshop: Multimodal Sentiment Analysis, Emotion-Target Engagement and Trustworthiness Detection in Real-Life Media",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Georgios Rizos",
        "Panagiotis Tzirakis",
        "Xinchen Du",
        "Felix Hafner",
        "Lea Schumann",
        "Adria Mallol-Ragolta",
        "Bjoern Schuller",
        "Iulia Lefter"
      ],
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-Life Media Challenge and Workshop"
    },
    {
      "citation_id": "43",
      "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lea Schumann",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing (Early Access)",
      "doi": "10.1109/TAFFC.2021.3097002"
    },
    {
      "citation_id": "44",
      "title": "Cross-lingual zero-and few-shot hate speech detection utilising frozen transformer language models and AXEL",
      "authors": [
        "Lukas Stappen",
        "Fabian Brunn",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Cross-lingual zero-and few-shot hate speech detection utilising frozen transformer language models and AXEL",
      "arxiv": "arXiv:2004.13850"
    },
    {
      "citation_id": "45",
      "title": "From speech to facial activity: towards cross-modal sequence-to-sequence attention networks",
      "authors": [
        "Lukas Stappen",
        "Vincent Karas",
        "Nicholas Cummins",
        "Fabien Ringeval",
        "Klaus Scherer",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "MuSe 2021 Challenge: Multimodal Emotion, Sentiment,Physiological-Emotion, and Stress Detection",
      "authors": [
        "Lukas Stappen",
        "Eva-Maria Me√üner",
        "Erik Cambria",
        "Guoying Zhao",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "29th ACM International Conference on Multimedia (ACMMM)"
    },
    {
      "citation_id": "47",
      "title": "Uncertainty-Aware Machine Support for Paper Reviewing on the INTERSPEECH 2019 Submission Corpus",
      "authors": [
        "Lukas Stappen",
        "Georgios Rizos",
        "Madina Hasan",
        "Thomas Hain",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "48",
      "title": "Summary of MuSe 2020: Multimodal Sentiment Analysis, Emotiontarget Engagement and Trustworthiness Detection in Real-life Media",
      "authors": [
        "Lukas Stappen",
        "Bj√∂rn Schuller",
        "Iulia Lefter"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox",
      "authors": [
        "Lukas Stappen",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Alice Baird",
        "Benjamin Weigel",
        "Erik Cambria",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, colocated with the 29th ACM International Conference on Multimedia (ACMMM)"
    },
    {
      "citation_id": "50",
      "title": "Multimodal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "51",
      "title": "Sentiment strength detection in short informal text",
      "authors": [
        "Mike Thelwall",
        "Kevan Buckley",
        "Georgios Paltoglou",
        "Di Cai",
        "Arvid Kappas"
      ],
      "year": "2010",
      "venue": "Journal of the American Society for Information Science and Technology"
    },
    {
      "citation_id": "52",
      "title": "AVEC 2013: the continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "Michel Valstar",
        "Bj√∂rn Schuller",
        "Kirsty Smith",
        "Florian Eyben",
        "Bihan Jiang",
        "Sanjay Bilakhia",
        "Sebastian Schnieder",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "53",
      "title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies",
      "authors": [
        "Martin W√∂llmer",
        "Florian Eyben",
        "Stephan Reiter",
        "Bj√∂rn Schuller",
        "Cate Cox",
        "Ellen Douglas-Cowie",
        "Roddy Cowie"
      ],
      "year": "2008",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "54",
      "title": "WIDER FACE: A Face Detection Benchmark",
      "authors": [
        "Shuo Yang",
        "Ping Luo"
      ],
      "year": "2015",
      "venue": "WIDER FACE: A Face Detection Benchmark",
      "arxiv": "arXiv:1511.06523"
    },
    {
      "citation_id": "55",
      "title": "Second Grand-Challenge and Workshop on Multimodal Language",
      "authors": [
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "56",
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "57",
      "title": "Generalized canonical time warping",
      "authors": [
        "Feng Zhou",
        "Fernando De La"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}