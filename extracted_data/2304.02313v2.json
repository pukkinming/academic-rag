{
  "paper_id": "2304.02313v2",
  "title": "Personality-Aware Human-Centric Multimodal Reasoning: A New Task, Dataset And Baselines",
  "published": "2023-04-05T09:09:10Z",
  "authors": [
    "Yaochen Zhu",
    "Xiangqing Shen",
    "Rui Xia"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personality traits, emotions, and beliefs shape individuals' behavioral choices and decisionmaking processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task emphasized the prediction of future states and behaviors but often neglected the incorporation of individual personality traits. In this work, we introduce a new task called Personalityaware Human-centric Multimodal Reasoning (PHMR) (T 1 ), with the goal of forecasting the future behavior of a particular individual using multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pretrained model, and two multimodal large language models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personalitypredicted Human-centric Multimodal Reasoning task (T 2 ) along with the corresponding dataset and method. We will make our dataset and code available on GitHub. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "An individual's personality traits, emotions, and beliefs are essential components of their individual differences, and they shape their behavioral choices and decision-making processes in different situations. Personality encapsulates an individual's psychological characteristics and behavioral tendencies. The reactions of individuals, character-ized by diverse personality traits, to the identical situations can exhibit significant variations. For example, when faced with an event of \"Person 1 sing a song in public for Person 0 \", a cautious and sensitive Person 0 may feel embarrassed, whereas a confident and lively Person 0 may feel excited.\n\nHowever, on the one hand, in the field of affective computing, there is often a focus on predicting and analyzing personality traits, without applying them to the prediction of future events related to individuals. On the other hand,recently a category of human-centric multimodal reasoning tasks, including Social-IQ  (Zadeh et al., 2019) , VLEP  (Lei et al., 2020) , and PMR  (Dong et al., 2022) , has emerged with the aim to infer individuals' psychological states and behaviors by leveraging the available multimodal information. Existing studies primarily concentrated on objective information related to individuals, while neglecting the incorporation of individual personality traits in reasoning.\n\nIn this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T 1  ). The goal of our task is to forecast the most probable behavior of a particular individual in future scenarios within intricate social interactions that encompass multiple individuals and long-term interactions. This is achieved by integrating multimodal signals from past moments and considering their personality traits.\n\nTo support this task, we construct a dataset based on six television shows from TVQA  (Lei et al., 2018) , which we refer to as the Personalityaware Human-centric Multimodal Reasoning Dataset (PHMRD). We acquire the personality of the main characters through the PDB 2  website, and utilize them as the personality annotations in our dataset. Following the mainstream settings of multimodal reasoning tasks, we also define our task as a multiple choice problem. As shown in Fig.  1 , Person 0 , 1 , and 2 represent different characters within the video clip. Given the following information: 1) the image frame sequence of the video; 2) the utterances of dialogue; 3) the audio waveform of the video; 4) the personalities of the corresponding characters. Our task aims to select an option that best describes the most possible behavior of Person 0 in the future. For example, it should be inferred that \"Person 0 felt gratitude\" is the most possible option.\n\nWe benchmark the PHMR task by designing seven baseline methods: three adapted from related tasks, two pre-trained multimodal model, and two multimodal large language models. We employ a pre-trained multimodal model to extract multimodal features and incorporate personality traits, to predict the most possible behavior. The personality is represented in the form of embeddings and are effectively integrated with other multimodal features.\n\nThe experimental results reveal that the incorporation of personality traits can enhance reasoning performance in both unimodal and multimodal settings. Our proposed approach outperforms the adapted baselines, demonstrating improved performance. Our multimodal ablation experiments reveal that incorporating personality can yield performance improvements in unimodal setting for PHMR.\n\nTo further solve the lack of personality annotation in real-life scenes, we introduce an additional task called Personality-predicted Human-centric Multimodal Reasoning (T 2 ) along with the corresponding dataset. Taking into account the notion that an individual's personality can be inferred from their behavior and speech, we commence by predicting their personality using multimodal information. Subsequently, we leverage these predictions as surrogates for personality annotations to enhance the reasoning process (T 1 ). The experimental results show that our method can effectively predict these personality, and achieves satisfactory multimodal reasoning performance without relying on personality annotations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task",
      "text": "We introduce our primary task as Personality-aware Human-centric Multimodal Reasoning (T 1 ). Our focus lies on scenarios involving multiple individuals in long-term interactions, where each person's behavior is influenced by not only the actions of others, but also their own personality traits.\n\nSpecifically, four types of information are provided for a video clip VC:\n\nThe image frame sequences of the video.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio (A):",
      "text": "The audio waveforms of the video. 3. Dialogue (D): Textual utterances of dialogue, character, and corresponding time span T D . 4. Personality (P): Annotated personality for characters appearing in VC.\n\nFor a fragment VC, it can be divided into n time segments T 1 ∼ T n . We remove all multimodal information at the moment T n . Given the personalities P and multimodal information V, A, D at the T 1 ∼ T n-1 , the purpose of this task is to predict the most probable behavior B of a specific individual at time T n . This can be expressed as follows:\n\nwhere PHMR represents the Personality-aware Human-centric Multimodal Reasoning task.\n\nWe model PHMR as a multiple choice task, following the established settings of other multimodal reasoning tasks  (Zadeh et al., 2019; Lei et al., 2020; Dong et al., 2022) , where the objective is to select the most probable answer from a set of five options, denoted as Choices (C).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Source",
      "text": "We choose TVQA  (Lei et al., 2018)  as the data source. TVQA is a large-scale video question answering dataset based on television shows, encompassing QA pairs related to object relation and human activities. Our task emphasizes human-centric question answering and primarily concerns the behavior and psychology of individuals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Construction Process",
      "text": "Data Filtering To make the data more suitable for our task settings, we use ChatGPT (gpt-3.5turbo) to exclude QA pairs that are not related to the individual's behavior and psychology. In the given instance, we consider the query \"What color is the object?\" to be unrelated to individuals, while the inquiry \"Why does person 0 like this object?\" is regarded as pertinent to human subjects. The prompt and additional examples are provided in Appendix A. The above example can be accurately evaluated by ChatGPT. To evaluate the results of ChatGPT, we manually label two hundred samples. Subsequently, a kappa test is performed on the manually labeled samples and results obtained from ChatGPT. The kappa result is 0.89, indicating a strong consistency between ChatGPT and the manually labeled samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Rewriting Of Behavior Description",
      "text": "We model the multimodal reasoning task as a multiple choice problem, following the mainstream setting of multimodal reasoning tasks (Social-IQ, VLEP, PMR). The correct option in PHMRD is derived from the rewriting of correct option in TVQA, and the process for obtaining the incorrect options followed the same methodology. In our preliminary experiment, it was observed that when the options are presented in the form of QA pairs, the model tends to over emphasize the importance of questions. Therefore, we manually rewrite its QA pairs by converting them into declarative sentences to describe the behaviors. For example, given the question in TVQA: \"What does the person 0 felt when Person 1 sang a song in public?\", and the answer: \"Person 0 felt embarrassed. \", the rewritten result is \"Person 0 felt embarrassed when Person 1 sang a song in public\". An example of PHMRD is presented in Fig.  1 . We utilize ChatGPT to rewrite the behavior description according to the aforementioned settings. We select 500 samples and manually perform the rewriting process to obtain the manually rewriting the behavior description.\n\nTo measure the inter-annotator agreement, we utilize the MASI 3    (Passonneau, 2006)  metric, and the resulting score is 0.72. Personality Annotation We obtain the personality of the characters in six television shows from the Personality Database (PDB) website. Each character's personality is determined through crowdsourced voting on this site. We adopt the personality that constitutes the largest proportion of votes as the character's personality.\n\nTaking into consideration that personality is assessed using multiple evaluation criteria, we have annotated the mainstream methods in psychology, namely Myers-Briggs Type Indicator (MBTI), Five Factor Model (FFM), and Enneagram, as shown in Table  3 . For example, Penny (a character in the television show The Big Bang Theory) possesses ESFP (MBTI), SCUAN (FFM), and 7w8 (Enneagram) personalities, and is an confident and lively individual. More details can be found in the Appendix B.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Statistics",
      "text": "It consists of 225 distinct individuals, each characterized by their own unique personalities, extracted from six television series. Table  1  provides an overview of the statistics pertaining to the PHMRD dataset. On average, each segment in the PHMRD dataset has a duration of 74.95 seconds and featured approximately 3.73 characters. The specific number of samples that conform to the task settings within each television show is presented in Table  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "We propose a preliminary framework, Personalityaware Reasoning Model (PRM), to validate the effectiveness of personality in our PHMR task. The overall structure of PMR is shown in Fig.  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Signal Representation",
      "text": "Since a multimodal model pre-trained on large scale data can better represent features from different modalities, we utilize a pre-trained model, Merlot Reserve  (Zellers et al., 2022) , which is the SOTA of TVQA, for feature extraction. We employ the notation V, A, D, P, C to represent the video, audio, dialogue, personality, and choices mentioned in Section 2.\n\nFor video modality V, we employ RESERVE-L that utilizes a 24-layer ViT-L/16  (Dosovitskiy et al., 2021)  to encode each frame independently. It obtains V ′ ∈ R n V ×d , where n V represents the length of the image frame sequence, and d=1024 denotes the feature dimension.\n\nFor audio modality A, we utilize an Audio Spectrogram Transformer  (Gong et al., 2021)  to encode each subsegment independently. This is achieved by dividing the audio into intervals of 5 seconds. It yields A ′ ∈ R n A ×d , where n A represents the number of subsegments after the segmentation process.\n\nFor dialogue D, the context of D is represented as\n\nHere, n D denotes the number of dialogue utterances, and l D signify the number of words in a single utterance. Specifically, the representation for each word in the utterance is initialized from Merlot Reserve's embedding matrix.\n\nTo represent personality P, we first construct vocabularies corresponding to personality traits based on their discrete distribution. The personality embedding matrix is randomly initialized and optimized in training. We have the option to utilize a single personality trait, such as [ESFP], or to utilize their collective set, such as [ESFP; SLUAN; 7w8]. Ultimately, we obtain the personality feature P ′ ∈ R n P ×l P ×d E of relevant the characters in the VC.\n\nHere, n P denotes the number of relevant characters in VC, and l P represents length of P .\n\nThe representation of the multiple choice C is expressed as C ∈ R n C ×l C ×d E . Here, n C represents the number of multiple choice options, and l C signify the number of words in a single choice, respectively. Specifically, the representation for each word in the utterance is initialized from Merlot Reserve's embedding matrix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Personality-Aware Multimodal Reasoning",
      "text": "First, the candidate behaviors are integrated with multimodal and personality features. In particular, we concatenate P and C and feed them into Answer-Attention. We apply mean-pooling on P C to produce the final P out . Similarly, D and A are processed to obtain D out and A out , respectively.\n\nSubsequently, the multimodal features are fused with the personality features. We first concatenate P out with D out and A out , respectively. Then, multimodal information is fused to obtain the modal answer. Finally, we utilize a softmax layer to derive the final result as follows:\n\n(2)\n\n(3)\n\nIn addition to the above method specifically designed for the task, we also modify and adapt several methods from related tasks (TVQA, PMR) to suit our task. Details can be found in Sec. 6.2.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Extension: Human-Centric Multimodal",
      "text": "Reasoning with Predicted Personality\n\nAs previously mentioned, acquiring personality annotations in real life is challenging. To overcome this challenge, we propose an extension task called Personality-predicted Human-centric Multimodal Reasoning (T 2 ).\n\nAs shown in Fig.  3 , given the availability of multimodal information, we first predict the most probable personality traits P ′ of a specific individual in the VC. We accomplish this through the following process:\n\nSecond, we substitute the annotated personality P with the predicted one P ′ in T 1 and re-evaluate its performance by B = PHMR(D, V, A, P ′ ).\n\n(5)  In order to achieve precise predictions of personality traits and effectively utilize these predictions for future forecasting, it is imperative to ensure the complete separation of the MPPD and PHMRD. This precautionary step is taken to prevent any data leakage, ensuring that the MPPD remains distinct and independent from the PHMRD. To accomplish this, we consider all clips within the TVQA dataset as clips within the MPPD. Subsequently, we exclude any clips that are also present in the PHMRD.\n\nFurthermore, we eliminate characters from the dataset that possess insufficient information, such as Person 0 , who only engages in greetings or constantly eats. This process yields a list of characters associated with each clip. Once we obtain the corresponding personality traits based on these characters, we replace the names of the individuals with name tags, intentionally disrupting the association between the name and the personality traits. This precautionary measure prevents the model from memorizing the link between specific names and corresponding personality traits. The statistical de-tails of the dataset are summarized in Table  4 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "The PHMRD and MPPD dataset is divided into training, validation, and testing sets at a ratio of 14:3:3. We use the Accuracy as the primary evaluation metric.\n\nWe train our PRM model on four RTX 3090 GPUs. For all the models, we train in 20 epochs with Early Stopping. We set the learning rate to 8e -5 for linear layer in feature fusion, 4e -5 for personality attention in personality feature extraction, and 1e -5 for the rest, with linear warmup process. We report the average performance of four runs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Compared Methods",
      "text": "We establish three rule-based methods, three adapted multimodal reasoning models, a modality separation variant, and two large video models, as compared methods to our PRM model.\n\nRule Based Methods. Random, Longest and Shortest selects a random, the longest and the shortest option, respectively. Adapted Methods. FVTA-adapt  (Liang et al., 2018)  concentrates on aligning video and text modalities locally. TVQA-adapt  (Lei et al., 2018)  integrates personality and modality information to acquire features for each modality. Reserveadapt  (Zellers et al., 2022)  fuses video with text and audio during pre-training, and combines them with personality features to generate the final prediction results.\n\nMultimodal Large Language Models. Chat-UniVi (Jin et al., 2023) employs a visual model to convert the images into embeddings and transforms the dialogue and personality into prompts. Gemini  (Anil et al., 2023)  use 16 images and prompt with dialogue, personality, and multiple choices.\n\nModality Separation method. PRM vanilla is a variant of PRM. It generates independent representations for each of the three modalities. This approach allows us to conduct modality ablation experiments more easily and effectively. To address the challenges posed by the integration of video modality features into the audio and text modalities as observed in the Merlot Reserve experiment, we also introduce a vanilla method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Main Results",
      "text": "In Table  5 , we report the results of different compared methods in two settings. In the supervised setting, compared to TVQA-adapt, although the PRM vanilla model's performance is suboptimal without incorporating personality, it achieves the best performance when personality information is integrated. Compared to Reserve-adapt, the PRM model exhibits a significant improvement. It highlights that personality information is effective for the task and our model can more effectively utilize personality information.\n\nIn the zero-shot setting, as shown in Table  6 , the existing multimodal large language models face challenges in PHMR and exhibit performance inferior to fine-tuned pretraining models. In contrast to Chat-UniVi, Gemini exhibits enhanced performance by incorporating personality. This enhancement may be attributed to the inclusion of the correspondence between personality and behavior in Gemini's training corpus, such as the inclusion of task-oriented datasets for personality prediction. comparable results. The p-value obtained from the statistical significance experiment is lower than 5%, signifying that the improvement attributed to the incorporation of personality information is statistically significant.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Of Different Personality Type",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Of Different Modalities",
      "text": "As showed in Table  8 , we conduct the experiments under various modality combinations based on PRM. In the unimodal settings, audio outperforms dialogue both in terms of base performance and the enhancement contributed by personality. The multimodal result is higher than the unimodal one, with a more substantial improvement in personality enhancement.\n\nAs demonstrated in Table  9 , the results for different modalities in PRM vanilla are presented. The audio modality exhibits low performance, thus diminishing the overall effectiveness of the vanilla method across all modalities. Nevertheless, the incorporation of personality information yields the most significant enhancement in the context of multimodality.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Modality",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results On Different Television Shows",
      "text": "In order to account for the diverse types and quantities of episodes in the six television shows, we divide them into partitions to assess the influence of incorporating personality information. Given the limited number of available episodes for each TV show, training separate models individually proves to be a challenge. Therefore, we opt for a trained model on the complete dataset and evaluate its performance on the test sets of these six television shows. The corresponding results can be found in Table  10 . The results show that the introduction of personality in each type of TV shows leads to a improvement, and that the improvement is greater for TV shows with a relatively small number of clips.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments On The Extension Task T 2",
      "text": "We utilize the PRM model to train and predict personality traits on the MPPD dataset. Specifically, we predict the Myers-Briggs Type Indicator (MBTI) and apply the predicted MBTI to T 1 .  can serve as a reasonable approximation of the ground truth.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Personality Prediction In",
      "text": "Effect of Personality-Predicted PHMR To verify the usefulness of the predicted personality, we substitute the annotated personality with the predicted personality and train PRM for T 1 from scratch. Table  12  illustrates the performance, showcasing a marginal decrease of 0.21%. The findings show that multimodal information can be utilized to predict personality to alleviate the shortage of personality annotations in real life.\n\n7 Related Work",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multimodal Reasoning",
      "text": "Multimodal reasoning builds upon unimodal reasoning, with examples such as VCR  (Zellers et al., 2019) , VCG  (Park et al., 2020) , and NExT-QA  (Xiao et al., 2021) , which include both objectcentric and human-centric reasoning.\n\nObject-centric reasoning tasks primarily focus on objects within videos, featuring less content about humans. AGQA  (Grunde-McLaughlin et al., 2021)  predicts object relations using video and relation graphs. Sherlock  (Hessel et al., 2022)  extrapolates missing information based on existing clues in images. TVQA  (Lei et al., 2018)  selects answers based on video, dialogue, question, and time span, with some questions related to humans.\n\nHuman-centric reasoning tasks often feature individuals appearing in a video only once, making it difficult to obtain personality information without numerous samples of the same person. Social-IQ  (Zadeh et al., 2019)  focused on multiple-choice questions using video, dialogue, and behavioral and psychological inquiries. WHYCAT  (Ignat et al., 2021)  assessed motivation based on video and descriptions. PMR  (Dong et al., 2022)  selected the optimal response using images and questions.\n\nThe aforementioned tasks provide the information about the question, whereas others perform reasoning without this information. These tasks usually do not involve individuals's psychological activity. VLEP  (Lei et al., 2020) predicted the next likely action based on video and textual description. VAR  (Liang et al., 2022)  inferred images and descriptions based on videos and surrounding context, following αNLI  (Bhagavatula et al., 2020) , a task concentrating solely on the text modality.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Personality Computing",
      "text": "Personality computing  (Yang et al., 2022a ) is a research field that investigates personality through computational techniques utilizing various sources such as text, multimedia, and social networks. At present, three primary personality evaluation indicators are employed in research: the Five Factor Model (FFM)  (McCrae and Costa, 1987) , Myers-Briggs Type Indicator (MBTI)  (Myers et al., 1985) , and Enneagram  (Palmer, 1988) . The FFM's five dimensions describe predictable surface behavior, while MBTI's four dimensions primarily explain behavior and are closely related to instincts. The Enneagram outlines nine core motivations of individuals, each possessing its own patterns of thinking, feeling, and behaving.\n\nThe work related to personality computing has two main branches: predictive personality and applied personality. The evolution of the personality prediction task is presented as follows. Early studies  (Flekova and Gurevych, 2015)  adopted a simpler binary classification evaluation metric. Subsequent studies  (Ponce-López et al., 2016; Kampman et al., 2018; Aslan and Güdükbay, 2019)  aimed to establish predictions for the Big Five personality traits. In recent years, research efforts  (Gjurkovic and Snajder, 2018; Stajner and Yenikent, 2021; Sang et al., 2022)  have shifted their focus towards the MBTI. There were also some studies  (Zhang et al., 2022; Yang et al., 2022b; Tayarani et al., 2022; Plepi et al., 2022)  that focus on utilizing personality in practical applications.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce a new Personality-aware Human-centric Multimodal Reasoning (PHMR) task (T 1 ) and construct a new dataset, PHMRD, based on six television shows. The experimental results indicate that incorporating personality information enhances the performance of humancentric multimodal reasoning. Moreover, an ablation study reveals that three distinct personality traits contribute to varying degrees of performance improvement. To further solve the lack of personality annotation in real-life scenes, we introduce an extended task called Personality-predicted Humancentric Multimodal Reasoning (T 2 ). The experimental results show that our method can accurately predict personality, and achieves satisfactory multimodal reasoning performance without relying on personality annotations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "Personality-aware human-centric multimodal reasoning is a challenging task. This work is a preliminary study for this task, which was not defined as a generation task.\n\nDue to space limitation, Our focus in this work is on the presentation of the task and the dataset. The proposed baseline is quite simple and has plenty of space for improvement.\n\nPersonality computing is a relatively broad field, our review of literature may not fully cover the research in this area.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethics Statement",
      "text": "We would like to thank  Lei et al. (2018) 's valuable work on TVQA. The TVQA is licensed under a licence of MIT, which allows commercial using, modification, distribution, and private using the material for any purpose. We will also make our PHMRD publicly available later. Personality database (PDB) website is an open source, anyone can get information without registration. All of the datasets and models are in English, which benefits English speakers more. We have employed 2 postgraduates experienced in natural language processing for verify the results of ChatGPT. We pay postgraduates around $7-10 per hour, well above the local average wage, and engage in constructive discussions if they are concerned about the process.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Chatgpt Prompt And Some Samples",
      "text": "A.1 Data Filtering PROMPT:\" {What color is the object?} as unrelated to individuals, while {Why does person 0 like this object?} is regarded as pertinent to human subjects. Is {QUESTION} related to human behavior or psychology?\" Similarly, the question \"What kind of object is on person 0 's leg?\" is considered irrelevant to individuals, whereas the query \"What is person 0 talking about?\" is deemed relevant to human subjects.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Personality Typology B.1 Mbti",
      "text": "Myers-Briggs Type Indicator (MBTI) is an introspective self-report questionnaire used in personality typology to identify various psychological preferences in how people view the environment and make judgments.\n\nFor evaluation, the MBTI splits personality into four dimensions (E/I, S/N, T/F, and J/P). The four dimensions, in particular, have various emphases, as follows:\n\n• Concentrate on attention, Extrovert (E), and Introvert (I) to show the people's primary energy source.\n\n• Concentrate on cognition, Judgment (J), and Perceiving (P) to demonstrate how humans collect information.\n\n• Concentrate on judgment, Intuition (N), and Sensing (S) to demonstrate how individuals make decisions.\n\n• Concentrate on living, Thinking(T), and Feeling(F) to demonstrate how humans behave.\n\nEach dimension includes two opposing labels that may be combined to produce 16 personalities, as seen in Fig.  4(c ).\n\nThe 16 personalities have their own characteristics, as shown in Table  13 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B.2 Ffm",
      "text": "Five Factor Model (FFM), also known as Big Five Model was the model to comprehend the relationship between personality and academic behaviors.this model was defined by several independent sets of researchers who used factor analysis of verbal descriptors of human behavior.these researchers began by studying relationships between a large number of verbal descriptors related to personality traits.\n\nFor evaluation, the FFM splits personality into Five dimensions (I/N, O/U, S/R, A/E, and L/C). The Five dimensions, in particular, have various emphases, as follows:\n\n• Openness to experience (inventive/curious(I) vs. consistent/cautious(N))\n\n• Conscientiousness (efficient/organized(O) vs. extravagant/careless(U))\n\n• Extraversion (outgoing/energetic(S) vs. solitary/reserved(R))\n\n• Agreeableness (friendly/compassionate(A) vs. critical/rational(E))\n\n• Neuroticism (sensitive/nervous(L) vs. resilient/confident(C))\n\nDetailed names of the five personality dimensions in polar opposites as shown in Fig.  4 (a).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B.3 Enneagram",
      "text": "The Enneagram personality unfolds according to the nine horns of the ancient totem, revealing nine different inner dynamics that make each person inherently unique as an individual.the nine personality types described by the Enneagram personality theory are not good or bad; there are recognizable and fundamental differences in the way people with different personality types respond to the world. It is now common knowledge that our personalities are our own, that they filter and interpret what we see and hear, and that the basic principle of Type 9 personality theory is that each of us has one of nine possible \"filters\" that will keep the blueprint for our lives and the general It is used to protect a certain level within our nature and to form our communication strategy with the outside world. Names of the nine personalities as shown in Fig.  4(b) .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Enneagram Description",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Perfect",
      "text": "Emphasizes principles, is not easy to compromise, distinguishes between black and white, has high demands on both oneself and others, and pursues perfection.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Helping",
      "text": "Desire to establish a good relationship with others, people-oriented, willing to accommodate others. Achievement Competitive, and measure their own value by achievements, is a workaholic.\n\nEgo Emotional, afraid of being rejected by others, feeling that others do not understand themselves, doing their own way. Ideal likes to think and analyze, has a strong desire for knowledge, but lacks action, and has low requirements for material life.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Doubtful",
      "text": "Be cautious in doing things, not easy to trust others, have many doubts, like group life, and work hard.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Active",
      "text": "Optimistic, like novelty, like to follow the trend, don't like pressure.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Leader",
      "text": "Pursue power, emphasize strength, do not rely on others, and have a sense of justice.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Peaceful",
      "text": "It takes a long time to make decisions, fears disputes, and prays for harmonious coexistence.\n\nTable  14 : sixteen types of MBTI and their descriptions.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustrations of PHMR and PHMRD. Person 0, 1, and 2 represent the characters appearing in the video",
      "page": 2
    },
    {
      "caption": "Figure 1: We utilize ChatGPT to rewrite",
      "page": 3
    },
    {
      "caption": "Figure 2: PRMpretrain model for PHMR.",
      "page": 4
    },
    {
      "caption": "Figure 3: , given the availability of mul-",
      "page": 5
    },
    {
      "caption": "Figure 3: PRMpretrain for Personality prediction and",
      "page": 5
    },
    {
      "caption": "Figure 4: three personalities.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{yczhu,\nxiangqing.shen,": "Abstract",
          "rxia}@njust.edu.cn": "ized by diverse personality traits,\nto the identical"
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "situations can exhibit significant variations.\nFor"
        },
        {
          "{yczhu,\nxiangqing.shen,": "Personality traits, emotions, and beliefs shape",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "example, when faced with an event of “Person 1"
        },
        {
          "{yczhu,\nxiangqing.shen,": "individuals’ behavioral choices and decision-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "sing a song in public for Person 0 ”, a cautious and"
        },
        {
          "{yczhu,\nxiangqing.shen,": "making processes. However, for one thing, the",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "affective computing community normally fo-",
          "rxia}@njust.edu.cn": "sensitive Person 0 may feel embarrassed, whereas"
        },
        {
          "{yczhu,\nxiangqing.shen,": "cused on predicting personality traits but over-",
          "rxia}@njust.edu.cn": "a confident and lively Person 0 may feel excited."
        },
        {
          "{yczhu,\nxiangqing.shen,": "looks their application in behavior prediction.",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "However, on the one hand, in the field of affec-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "For another, the multimodal reasoning task em-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "tive computing, there is often a focus on predicting"
        },
        {
          "{yczhu,\nxiangqing.shen,": "phasized the prediction of future states and be-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "and analyzing personality traits, without applying"
        },
        {
          "{yczhu,\nxiangqing.shen,": "haviors but often neglected the incorporation",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "them to the prediction of future events related to in-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "of individual personality traits.\nIn this work,",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "dividuals. On the other hand,recently a category of"
        },
        {
          "{yczhu,\nxiangqing.shen,": "we introduce a new task called Personality-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "aware Human-centric Multimodal Reasoning",
          "rxia}@njust.edu.cn": "human-centric multimodal reasoning tasks, includ-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "(PHMR) (T 1), with the goal of forecasting the",
          "rxia}@njust.edu.cn": "ing Social-IQ (Zadeh et al., 2019), VLEP (Lei et al.,"
        },
        {
          "{yczhu,\nxiangqing.shen,": "future behavior of a particular individual using",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "2020), and PMR (Dong et al., 2022), has emerged"
        },
        {
          "{yczhu,\nxiangqing.shen,": "multimodal\ninformation from past\ninstances,",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "with the aim to infer\nindividuals’ psychological"
        },
        {
          "{yczhu,\nxiangqing.shen,": "while integrating personality factors. We ac-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "states and behaviors by leveraging the available"
        },
        {
          "{yczhu,\nxiangqing.shen,": "cordingly construct a new dataset based on six",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "multimodal information. Existing studies primarily"
        },
        {
          "{yczhu,\nxiangqing.shen,": "television shows, encompassing 225 characters",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "concentrated on objective information related to"
        },
        {
          "{yczhu,\nxiangqing.shen,": "and 12k samples. To establish a benchmark",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "for the task, we propose seven baseline meth-",
          "rxia}@njust.edu.cn": "individuals, while neglecting the incorporation of"
        },
        {
          "{yczhu,\nxiangqing.shen,": "ods:\nthree adapted from related tasks, two pre-",
          "rxia}@njust.edu.cn": "individual personality traits in reasoning."
        },
        {
          "{yczhu,\nxiangqing.shen,": "trained model, and two multimodal\nlarge lan-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "In this work, we introduce a new task called"
        },
        {
          "{yczhu,\nxiangqing.shen,": "guage models. The experimental results demon-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "Personality-aware Human-centric Multimodal Rea-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "strate that\nincorporating personality traits en-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "soning (PHMR) (T 1). The goal of our task is to"
        },
        {
          "{yczhu,\nxiangqing.shen,": "hances human-centric multimodal\nreasoning",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "forecast\nthe most probable behavior of a particu-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "performance. To further solve the lack of per-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "lar individual\nin future scenarios within intricate"
        },
        {
          "{yczhu,\nxiangqing.shen,": "sonality annotation in real-life scenes, we in-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "troduce an extension task called Personality-",
          "rxia}@njust.edu.cn": "social interactions that encompass multiple individ-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "predicted Human-centric Multimodal Reason-",
          "rxia}@njust.edu.cn": "uals and long-term interactions. This is achieved by"
        },
        {
          "{yczhu,\nxiangqing.shen,": "ing task (T 2) along with the corresponding",
          "rxia}@njust.edu.cn": "integrating multimodal signals from past moments"
        },
        {
          "{yczhu,\nxiangqing.shen,": "dataset and method. We will make our dataset",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "and considering their personality traits."
        },
        {
          "{yczhu,\nxiangqing.shen,": "and code available on GitHub. 1",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "To support\nthis\ntask, we\nconstruct\na dataset"
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "based on six television shows from TVQA (Lei"
        },
        {
          "{yczhu,\nxiangqing.shen,": "1\nIntroduction",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "et al., 2018), which we refer to as the Personality-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "An individual’s personality traits, emotions, and",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "aware\nHuman-centric Multimodal\nReasoning"
        },
        {
          "{yczhu,\nxiangqing.shen,": "beliefs are essential components of their individ-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "Dataset (PHMRD). We acquire the personality of"
        },
        {
          "{yczhu,\nxiangqing.shen,": "ual differences, and they shape their behavioral",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "the main characters through the PDB2 website, and"
        },
        {
          "{yczhu,\nxiangqing.shen,": "choices and decision-making processes in different",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "utilize them as the personality annotations in our"
        },
        {
          "{yczhu,\nxiangqing.shen,": "situations. Personality encapsulates an individual’s",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "dataset. Following the mainstream settings of mul-"
        },
        {
          "{yczhu,\nxiangqing.shen,": "psychological characteristics and behavioral\nten-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "timodal reasoning tasks, we also define our task"
        },
        {
          "{yczhu,\nxiangqing.shen,": "dencies. The reactions of individuals, character-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "",
          "rxia}@njust.edu.cn": "as a multiple choice problem. As shown in Fig. 1,"
        },
        {
          "{yczhu,\nxiangqing.shen,": "1We have presented 10 examples of PHMRD in anony-",
          "rxia}@njust.edu.cn": ""
        },
        {
          "{yczhu,\nxiangqing.shen,": "mous GitHub.",
          "rxia}@njust.edu.cn": "2https://www.personality-database.com"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "clip. Our task is to predict the most plausible behavior description at a Tn based on video, dialogue, audio and"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "personalities information."
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "Person 0 , 1 , and 2 represent different characters"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "within the video clip. Given the following infor-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "mation: 1) the image frame sequence of the video;"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "2) the utterances of dialogue; 3) the audio wave-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "form of the video; 4) the personalities of the cor-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "responding characters. Our task aims to select an"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "option that best describes the most possible behav-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "ior of Person 0 in the future. For example, it should"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "be inferred that “Person 0 felt gratitude” is the most"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "possible option."
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "We benchmark the PHMR task by designing"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "seven baseline methods:\nthree adapted from related"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "tasks, two pre-trained multimodal model, and two"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "multimodal\nlarge language models. We employ"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "a pre-trained multimodal model\nto extract multi-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "modal features and incorporate personality traits,"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "to predict the most possible behavior. The person-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "ality is represented in the form of embeddings and"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "are effectively integrated with other multimodal"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "features."
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "The experimental results reveal\nthat\nthe incor-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "poration of personality traits can enhance reason-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "ing performance in both unimodal and multimodal"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "settings. Our proposed approach outperforms the"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "adapted baselines, demonstrating improved perfor-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "mance. Our multimodal ablation experiments re-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "veal\nthat\nincorporating personality can yield per-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "formance improvements in unimodal setting for"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": ""
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "PHMR."
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "To further solve the lack of personality annota-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "tion in real-life scenes, we introduce an additional"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "task called Personality-predicted Human-centric"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "Multimodal Reasoning (T 2) along with the corre-"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "sponding dataset. Taking into account the notion"
        },
        {
          "Figure 1: Illustrations of PHMR and PHMRD. Person 0 , 1 , and 2 represent the characters appearing in the video": "that an individual’s personality can be inferred from"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train": "2602",
          "Dev": "559",
          "Test": "558",
          "Total": "3719",
          "TV Show": "# Friends"
        },
        {
          "Train": "2548",
          "Dev": "549",
          "Test": "553",
          "Total": "3650",
          "TV Show": "# BBT"
        },
        {
          "Train": "1525",
          "Dev": "329",
          "Test": "333",
          "Total": "2187",
          "TV Show": "# Castle"
        },
        {
          "Train": "",
          "Dev": "",
          "Test": "",
          "Total": "",
          "TV Show": ""
        },
        {
          "Train": "843",
          "Dev": "214",
          "Test": "190",
          "Total": "1247",
          "TV Show": "# Met"
        },
        {
          "Train": "",
          "Dev": "",
          "Test": "",
          "Total": "",
          "TV Show": ""
        },
        {
          "Train": "745",
          "Dev": "167",
          "Test": "165",
          "Total": "1077",
          "TV Show": "# House"
        },
        {
          "Train": "505",
          "Dev": "120",
          "Test": "111",
          "Total": "736",
          "TV Show": "# Grey"
        },
        {
          "Train": "",
          "Dev": "",
          "Test": "",
          "Total": "",
          "TV Show": ""
        },
        {
          "Train": "8768",
          "Dev": "1938",
          "Test": "1910",
          "Total": "12616",
          "TV Show": "Sum"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "# House\n745\n167\n165\n1077"
        },
        {
          "# Met\n843\n214\n190\n1247": "# Grey\n505\n120\n111\n736"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Sum\n8768\n1938\n1910\n12616"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Table 2: Statistics regarding the sample numbers of six"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "television shows. BBT, Met, House, and Grey represent"
        },
        {
          "# Met\n843\n214\n190\n1247": "The Big Bang Theory, How I Met You Mother, House"
        },
        {
          "# Met\n843\n214\n190\n1247": "M.D. and Grey, respectively."
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Personality\nVocabulary"
        },
        {
          "# Met\n843\n214\n190\n1247": "FFM\n[RLOAI, SCUEN, · · ·\n, RCUEI, SLOAN]"
        },
        {
          "# Met\n843\n214\n190\n1247": "MBTI\n[INTJ, ESFP, · · ·\n, ENFJ, ISTP]"
        },
        {
          "# Met\n843\n214\n190\n1247": "Enneagram\n[1w2, 2w1, · · ·\n, 9w1, 1w9]"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Table 3: Statistical of the vocabularies of different per-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "sonalities."
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Rewriting of Behavior Description\nWe model"
        },
        {
          "# Met\n843\n214\n190\n1247": "the multimodal reasoning task as a multiple choice"
        },
        {
          "# Met\n843\n214\n190\n1247": "problem, following the mainstream setting of mul-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "timodal reasoning tasks (Social-IQ, VLEP, PMR)."
        },
        {
          "# Met\n843\n214\n190\n1247": "The correct option in PHMRD is derived from the"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "rewriting of correct option in TVQA, and the pro-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "cess for obtaining the incorrect options followed"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "the same methodology.\nIn our preliminary exper-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "iment,\nit was observed that when the options are"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "presented in the form of QA pairs, the model tends"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "to over emphasize the importance of questions."
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Therefore, we manually rewrite its QA pairs by"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "converting them into declarative sentences to de-"
        },
        {
          "# Met\n843\n214\n190\n1247": "scribe the behaviors. For example, given the ques-"
        },
        {
          "# Met\n843\n214\n190\n1247": "tion in TVQA: “What does the person 0 felt when"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "Person 1 sang a song in public?”, and the answer:"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "“Person 0 felt embarrassed. ”, the rewritten result"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "is “Person 0 felt embarrassed when Person 1 sang"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "a song in public”. An example of PHMRD is pre-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "sented in Fig. 1. We utilize ChatGPT to rewrite"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "the behavior description according to the afore-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "mentioned settings. We select 500 samples and"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "manually perform the rewriting process to obtain"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "the manually rewriting the behavior description."
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "To measure the inter-annotator agreement, we uti-"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "lize the MASI3 (Passonneau, 2006) metric, and the"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "resulting score is 0.72."
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "3A measure of the distance between collections used to"
        },
        {
          "# Met\n843\n214\n190\n1247": ""
        },
        {
          "# Met\n843\n214\n190\n1247": "quantify the degree of overlap between annotations of collec-"
        },
        {
          "# Met\n843\n214\n190\n1247": "tion data."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: provides an FordialogueD,thecontextofD isrepresented",
      "data": [
        {
          "Figure 2: PRMpretrain model for PHMR.": "Personality Annotation\nWe obtain the personal-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "ity of the characters in six television shows from the"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "Personality Database (PDB) website. Each char-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "acter’s personality is determined through crowd-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "sourced voting on this site. We adopt the personal-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "ity that constitutes the largest proportion of votes"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "as the character’s personality."
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "Taking into consideration that personality is as-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "sessed using multiple evaluation criteria, we have"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "annotated the mainstream methods in psychology,"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "namely Myers-Briggs Type Indicator (MBTI), Five"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "Factor Model (FFM), and Enneagram, as shown in"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "Table 3. For example, Penny (a character in the tele-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "vision show The Big Bang Theory) possesses ESFP"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "(MBTI), SCUAN (FFM), and 7w8 (Enneagram) per-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "sonalities, and is an confident and lively individual."
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "More details can be found in the Appendix B."
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "3.3\nDataset Statistics"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "It consists of 225 distinct individuals, each charac-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "terized by their own unique personalities, extracted"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "from six television series.\nTable 1 provides an"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "overview of the statistics pertaining to the PHMRD"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "dataset. On average, each segment in the PHMRD"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "dataset has a duration of 74.95 seconds and fea-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "tured approximately 3.73 characters. The specific"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "number of samples that conform to the task settings"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "within each television show is presented in Table 2."
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "4\nMethod"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": ""
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "We propose a preliminary framework, Personality-"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "aware Reasoning Model\n(PRM),\nto validate the"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "effectiveness of personality in our PHMR task. The"
        },
        {
          "Figure 2: PRMpretrain model for PHMR.": "overall structure of PMR is shown in Fig. 2."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "Here, nP denotes the number of relevant charac-"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "ters in VC, and lP represents length of P ."
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "The representation of the multiple choice C is"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "expressed as C ∈ RnC ×lC ×dE . Here, nC repre-"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "sents the number of multiple choice options, and"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "lC signify the number of words in a single choice,"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "respectively.\nSpecifically,\nthe representation for"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "each word in the utterance is initialized from Mer-"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "lot Reserve’s embedding matrix."
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "4.2\nPersonality-aware Multimodal Reasoning"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "First, the candidate behaviors are integrated with"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "multimodal and personality features.\nIn particu-"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "lar, we concatenate P and C and feed them into"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "Answer-Attention. We apply mean-pooling on PC"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "to produce the final P out. Similarly, D and A are"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "processed to obtain Dout and Aout, respectively."
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "Subsequently, the multimodal features are fused"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "with the personality features. We first concatenate"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "P out with Dout and Aout, respectively. Then, mul-"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": ""
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "timodal\ninformation is fused to obtain the modal"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "answer. Finally, we utilize a softmax layer to derive"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": "the final result as follows:"
        },
        {
          "RnP ×lP ×dE of relevant\nthe characters in the VC.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Shortest\n16.70\n-\n-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "6\nExperiments",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Random\n20.00\n-\n-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "34.08\nLongest\n-\n-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "6.1\nExperimental Settings",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "FVTA-adapt\n33.64\n34.35\n0.71"
        },
        {
          "tails of the dataset are summarized in Table 4.": "The PHMRD and MPPD dataset\nis divided into",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "43.37\nTVQA-adapt\n43.92\n0.55"
        },
        {
          "tails of the dataset are summarized in Table 4.": "training, validation, and testing sets at a ratio of",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "44.14\n1.11\n43.03\nPRMvanilla"
        },
        {
          "tails of the dataset are summarized in Table 4.": "14:3:3. We use the Accuracy as the primary evalua-",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Reserve-adapt\n52.43\n53.07\n0.64"
        },
        {
          "tails of the dataset are summarized in Table 4.": "tion metric.",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "PRM\n53.13\n54.06\n0.93"
        },
        {
          "tails of the dataset are summarized in Table 4.": "We train our PRM model on four RTX 3090",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "GPUs. For all\nthe models, we train in 20 epochs",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Table 5: Accuracy for rule-based, deep learning meth-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "with Early Stopping. We set\nthe learning rate to",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "ods and video large model. Per. represents personality,"
        },
        {
          "tails of the dataset are summarized in Table 4.": "8e − 5 for linear layer in feature fusion, 4e − 5 for",
          "Model\nw/o Per.\nw/ Per.\nImp.": "while Imp. denotes the enhancement achieved by incor-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "personality attention in personality feature extrac-",
          "Model\nw/o Per.\nw/ Per.\nImp.": "porating personality information."
        },
        {
          "tails of the dataset are summarized in Table 4.": "tion, and 1e − 5 for the rest, with linear warmup",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "process. We report the average performance of four",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Model\nw/o Per.\nw/ Per.\nImp."
        },
        {
          "tails of the dataset are summarized in Table 4.": "runs.",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Chat-UniVi\n23.50\n20.61\n-2.89"
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Gemini\n33.17\n34.08\n0.91"
        },
        {
          "tails of the dataset are summarized in Table 4.": "6.2\nCompared Methods",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "We\nestablish\nthree\nrule-based methods,\nthree",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Table 6: Accuracy for zero-shot multimodal large lan-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "adapted multimodal\nreasoning models, a modal-",
          "Model\nw/o Per.\nw/ Per.\nImp.": "guage models."
        },
        {
          "tails of the dataset are summarized in Table 4.": "ity separation variant, and two large video models,",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "as compared methods to our PRM model.",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "6.3\nMain Results"
        },
        {
          "tails of the dataset are summarized in Table 4.": "Rule Based Methods.\nRandom, Longest and",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "In Table 5, we report the results of different com-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "Shortest\nselects a random,\nthe longest and the",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "pared methods in two settings.\nIn the supervised"
        },
        {
          "tails of the dataset are summarized in Table 4.": "shortest option, respectively.",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "setting, compared to TVQA-adapt, although the"
        },
        {
          "tails of the dataset are summarized in Table 4.": "Adapted Methods.\nFVTA-adapt (Liang et al.,",
          "Model\nw/o Per.\nw/ Per.\nImp.": "suboptimal\nPRMvanilla model’s performance is"
        },
        {
          "tails of the dataset are summarized in Table 4.": "2018)\nconcentrates on aligning video and text",
          "Model\nw/o Per.\nw/ Per.\nImp.": "without incorporating personality,\nit achieves the"
        },
        {
          "tails of the dataset are summarized in Table 4.": "modalities locally. TVQA-adapt (Lei et al., 2018)",
          "Model\nw/o Per.\nw/ Per.\nImp.": "best performance when personality information is"
        },
        {
          "tails of the dataset are summarized in Table 4.": "integrates personality and modality information",
          "Model\nw/o Per.\nw/ Per.\nImp.": "integrated. Compared to Reserve-adapt, the PRM"
        },
        {
          "tails of the dataset are summarized in Table 4.": "to acquire features for each modality. Reserve-",
          "Model\nw/o Per.\nw/ Per.\nImp.": "model exhibits a significant improvement. It high-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "adapt (Zellers et al., 2022) fuses video with text",
          "Model\nw/o Per.\nw/ Per.\nImp.": "lights that personality information is effective for"
        },
        {
          "tails of the dataset are summarized in Table 4.": "and audio during pre-training, and combines them",
          "Model\nw/o Per.\nw/ Per.\nImp.": "the task and our model can more effectively utilize"
        },
        {
          "tails of the dataset are summarized in Table 4.": "with personality features to generate the final pre-",
          "Model\nw/o Per.\nw/ Per.\nImp.": "personality information."
        },
        {
          "tails of the dataset are summarized in Table 4.": "diction results.",
          "Model\nw/o Per.\nw/ Per.\nImp.": "In the zero-shot setting, as shown in Table 6, the"
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "existing multimodal\nlarge language models face"
        },
        {
          "tails of the dataset are summarized in Table 4.": "Multimodal Large Language Models.\nChat-",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "challenges in PHMR and exhibit performance in-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "UniVi (Jin et al., 2023) employs a visual model to",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "ferior to fine-tuned pretraining models. In contrast"
        },
        {
          "tails of the dataset are summarized in Table 4.": "convert the images into embeddings and transforms",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "to Chat-UniVi, Gemini exhibits enhanced perfor-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "the dialogue and personality into prompts. Gem-",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "mance by incorporating personality. This enhance-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "ini (Anil et al., 2023) use 16 images and prompt",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "ment may be attributed to the inclusion of the cor-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "with dialogue, personality, and multiple choices.",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "respondence between personality and behavior in"
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Gemini’s training corpus, such as the inclusion of"
        },
        {
          "tails of the dataset are summarized in Table 4.": "Modality Separation method.\nPRMvanilla is a",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "task-oriented datasets for personality prediction."
        },
        {
          "tails of the dataset are summarized in Table 4.": "variant of PRM.\nIt generates independent\nrepre-",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "sentations for each of the three modalities. This",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "",
          "Model\nw/o Per.\nw/ Per.\nImp.": "6.4\nResults of Different Personality Type"
        },
        {
          "tails of the dataset are summarized in Table 4.": "approach allows us to conduct modality ablation ex-",
          "Model\nw/o Per.\nw/ Per.\nImp.": ""
        },
        {
          "tails of the dataset are summarized in Table 4.": "periments more easily and effectively. To address",
          "Model\nw/o Per.\nw/ Per.\nImp.": "Table 7 displays the outcomes derived from exper-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "the challenges posed by the integration of video",
          "Model\nw/o Per.\nw/ Per.\nImp.": "iments involving different personality types. The"
        },
        {
          "tails of the dataset are summarized in Table 4.": "modality features into the audio and text modalities",
          "Model\nw/o Per.\nw/ Per.\nImp.": "experimental results demonstrate that\nthe combi-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "as observed in the Merlot Reserve experiment, we",
          "Model\nw/o Per.\nw/ Per.\nImp.": "nation of three personalities yields the best perfor-"
        },
        {
          "tails of the dataset are summarized in Table 4.": "also introduce a vanilla method.",
          "Model\nw/o Per.\nw/ Per.\nImp.": "mance, followed by MBTI and FFM, which achieve"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 10: The results show that the introduction",
      "data": [
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "0.48",
          "Test Domain": "Friends",
          "w/o Per.": "52.92",
          "w/ Per.": "53.40"
        },
        {
          "Personality": "Baseline",
          "Accuracy": "53.13",
          "Imp.": "",
          "Test Domain": "",
          "w/o Per.": "",
          "w/ Per.": ""
        },
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "0.66",
          "Test Domain": "BBT",
          "w/o Per.": "55.57",
          "w/ Per.": "56.23"
        },
        {
          "Personality": "w/ MBTI",
          "Accuracy": "54.03",
          "Imp.": "",
          "Test Domain": "",
          "w/o Per.": "",
          "w/ Per.": ""
        },
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "0.90",
          "Test Domain": "Castle",
          "w/o Per.": "48.44",
          "w/ Per.": "49.34"
        },
        {
          "Personality": "w/ FFM",
          "Accuracy": "54.04",
          "Imp.": "",
          "Test Domain": "",
          "w/o Per.": "",
          "w/ Per.": ""
        },
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "2.98",
          "Test Domain": "Met",
          "w/o Per.": "56.14",
          "w/ Per.": "59.12"
        },
        {
          "Personality": "w/ Enneagram",
          "Accuracy": "53.89",
          "Imp.": "",
          "Test Domain": "",
          "w/o Per.": "",
          "w/ Per.": ""
        },
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "0.81",
          "Test Domain": "House",
          "w/o Per.": "48.68",
          "w/ Per.": "49.49"
        },
        {
          "Personality": "w/ All",
          "Accuracy": "54.06",
          "Imp.": "",
          "Test Domain": "",
          "w/o Per.": "",
          "w/ Per.": ""
        },
        {
          "Personality": "",
          "Accuracy": "",
          "Imp.": "1.20",
          "Test Domain": "Grey",
          "w/o Per.": "57.65",
          "w/ Per.": "58.85"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 10: The results show that the introduction",
      "data": [
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "HL↓\nRL↓\nAP↑"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "Dialogue\n51.02\n51.27\n0.25",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "PRM\n25.52\n25.71\n77.68"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "Audio\n52.82\n53.55\n0.73",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "53.13\n54.06\n0.93\nAll",
          "Model": "Table 11: Results of personality prediction. Here, HL"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "means Hamming Loss, and RL means Ranking Loss,"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "Table 8: Accuracy for different modality for PRM. All",
          "Model": "AP means Average Precision."
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "represents input of all modality information.",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "6.6\nResults on Different Television Shows"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "comparable results.",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "In order to account for the diverse types and quan-"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "The p-value obtained from the statistical signifi-",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "tities of episodes in the six television shows, we"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "cance experiment is lower than 5%, signifying that",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "divide them into partitions to assess the influence"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "the improvement attributed to the incorporation of",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "of incorporating personality information. Given the"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "personality information is statistically significant.",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "limited number of available episodes for each TV"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "show, training separate models individually proves"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "6.5\nResults of Different Modalities",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "to be a challenge. Therefore, we opt for a trained"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "As showed in Table 8, we conduct the experiments",
          "Model": "model on the complete dataset and evaluate its per-"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "under various modality combinations based on",
          "Model": "formance on the test sets of\nthese six television"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "PRM. In the unimodal settings, audio outperforms",
          "Model": "shows. The corresponding results can be found in"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "dialogue both in terms of base performance and",
          "Model": "Table 10. The results show that\nthe introduction"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "the enhancement contributed by personality. The",
          "Model": "of personality in each type of TV shows leads to a"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "multimodal result is higher than the unimodal one,",
          "Model": "improvement, and that the improvement is greater"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "with a more substantial improvement in personality",
          "Model": "for TV shows with a relatively small number of"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "enhancement.",
          "Model": "clips."
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "As demonstrated in Table 9, the results for dif-",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "6.7\nExperiments on the Extension Task T 2"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "ferent modalities in PRMvanilla are presented. The",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "We utilize the PRM model\nto train and predict"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "audio modality exhibits low performance, thus di-",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "personality traits on the MPPD dataset.\nSpecifi-"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "minishing the overall effectiveness of the vanilla",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "cally, we predict the Myers-Briggs Type Indicator"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "method across all modalities. Nevertheless, the in-",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "(MBTI) and apply the predicted MBTI to T 1."
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "corporation of personality information yields the",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "most significant enhancement in the context of mul-",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "Effect of Personality Prediction\nIn Table 11, the"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "timodality.",
          "Model": ""
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "lower is better for hamming loss and ranking loss,"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "while the greater\nis better\nfor average precision."
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "",
          "Model": "The result indicates that the predicted personality"
        },
        {
          "Modality\nw/o Per.\nw/ Per.\nImp.": "Modality\nw/o Per.\nw/ Per.\nImp.",
          "Model": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "can serve as a reasonable approximation of\nthe": "ground truth.",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Personality computing (Yang et al., 2022a) is a re-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "search field that\ninvestigates personality through"
        },
        {
          "can serve as a reasonable approximation of\nthe": "Effect of Personality-Predicted PHMR\nTo ver-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "computational techniques utilizing various sources"
        },
        {
          "can serve as a reasonable approximation of\nthe": "ify the usefulness of the predicted personality, we",
          "7.2\nPersonality Computing": "such as text, multimedia, and social networks. At"
        },
        {
          "can serve as a reasonable approximation of\nthe": "substitute the annotated personality with the pre-",
          "7.2\nPersonality Computing": "present, three primary personality evaluation indi-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "dicted personality and train PRM for T 1\nfrom",
          "7.2\nPersonality Computing": "cators are employed in research:\nthe Five Factor"
        },
        {
          "can serve as a reasonable approximation of\nthe": "scratch. Table 12 illustrates the performance, show-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Model (FFM) (McCrae and Costa, 1987), Myers-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "casing a marginal decrease of 0.21%. The findings",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Briggs Type Indicator (MBTI) (Myers et al., 1985),"
        },
        {
          "can serve as a reasonable approximation of\nthe": "show that multimodal information can be utilized",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "and Enneagram (Palmer, 1988). The FFM’s five"
        },
        {
          "can serve as a reasonable approximation of\nthe": "to predict personality to alleviate the shortage of",
          "7.2\nPersonality Computing": "dimensions describe predictable surface behavior,"
        },
        {
          "can serve as a reasonable approximation of\nthe": "personality annotations in real life.",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "while MBTI’s four dimensions primarily explain"
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "behavior and are closely related to instincts. The"
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Enneagram outlines nine core motivations of indi-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "7\nRelated Work",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "viduals, each possessing its own patterns of think-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "ing, feeling, and behaving."
        },
        {
          "can serve as a reasonable approximation of\nthe": "7.1\nMultimodal Reasoning",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "The work related to personality computing has"
        },
        {
          "can serve as a reasonable approximation of\nthe": "Multimodal reasoning builds upon unimodal rea-",
          "7.2\nPersonality Computing": "two main branches: predictive personality and ap-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "soning, with\nexamples\nsuch\nas VCR (Zellers",
          "7.2\nPersonality Computing": "plied personality. The evolution of the personality"
        },
        {
          "can serve as a reasonable approximation of\nthe": "et al., 2019), VCG (Park et al., 2020), and NExT-",
          "7.2\nPersonality Computing": "prediction task is presented as follows. Early stud-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "QA (Xiao et al., 2021), which include both object-",
          "7.2\nPersonality Computing": "ies (Flekova and Gurevych, 2015) adopted a sim-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "centric and human-centric reasoning.",
          "7.2\nPersonality Computing": "pler binary classification evaluation metric. Subse-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "quent studies (Ponce-López et al., 2016; Kampman"
        },
        {
          "can serve as a reasonable approximation of\nthe": "Object-centric reasoning tasks primarily focus",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "et al., 2018; Aslan and Güdükbay, 2019) aimed to"
        },
        {
          "can serve as a reasonable approximation of\nthe": "on objects within videos,\nfeaturing less content",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "establish predictions for the Big Five personality"
        },
        {
          "can serve as a reasonable approximation of\nthe": "about humans. AGQA (Grunde-McLaughlin et al.,",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "traits. In recent years, research efforts (Gjurkovic"
        },
        {
          "can serve as a reasonable approximation of\nthe": "2021) predicts object relations using video and re-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "and Snajder, 2018; Stajner and Yenikent, 2021;"
        },
        {
          "can serve as a reasonable approximation of\nthe": "lation graphs. Sherlock (Hessel et al., 2022) extrap-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Sang et al., 2022) have shifted their focus towards"
        },
        {
          "can serve as a reasonable approximation of\nthe": "olates missing information based on existing clues",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "the MBTI. There were also some studies (Zhang"
        },
        {
          "can serve as a reasonable approximation of\nthe": "in images. TVQA (Lei et al., 2018) selects answers",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "et al., 2022; Yang et al., 2022b; Tayarani et al.,"
        },
        {
          "can serve as a reasonable approximation of\nthe": "based on video, dialogue, question, and time span,",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "2022; Plepi et al., 2022)\nthat\nfocus on utilizing"
        },
        {
          "can serve as a reasonable approximation of\nthe": "with some questions related to humans.",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "personality in practical applications."
        },
        {
          "can serve as a reasonable approximation of\nthe": "Human-centric reasoning tasks often feature in-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "dividuals appearing in a video only once, making",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "8\nConclusion"
        },
        {
          "can serve as a reasonable approximation of\nthe": "it difficult to obtain personality information with-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "out numerous samples of the same person. Social-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "In this work, we introduce a new Personality-aware"
        },
        {
          "can serve as a reasonable approximation of\nthe": "IQ (Zadeh et al., 2019) focused on multiple-choice",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "Human-centric Multimodal Reasoning (PHMR)"
        },
        {
          "can serve as a reasonable approximation of\nthe": "questions using video, dialogue, and behavioral and",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "task (T 1) and construct a new dataset, PHMRD,"
        },
        {
          "can serve as a reasonable approximation of\nthe": "psychological\ninquiries. WHYCAT (Ignat et al.,",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "based on six television shows.\nThe experimen-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "2021) assessed motivation based on video and de-",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "tal results indicate that\nincorporating personality"
        },
        {
          "can serve as a reasonable approximation of\nthe": "scriptions. PMR (Dong et al., 2022) selected the",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "information enhances the performance of human-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "optimal response using images and questions.",
          "7.2\nPersonality Computing": ""
        },
        {
          "can serve as a reasonable approximation of\nthe": "",
          "7.2\nPersonality Computing": "centric multimodal\nreasoning. Moreover, an ab-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "The aforementioned tasks provide the informa-",
          "7.2\nPersonality Computing": "lation study reveals that three distinct personality"
        },
        {
          "can serve as a reasonable approximation of\nthe": "tion about\nthe question, whereas others perform",
          "7.2\nPersonality Computing": "traits contribute to varying degrees of performance"
        },
        {
          "can serve as a reasonable approximation of\nthe": "reasoning without\nthis information. These tasks",
          "7.2\nPersonality Computing": "improvement. To further solve the lack of person-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "usually do not involve individuals’s psychological",
          "7.2\nPersonality Computing": "ality annotation in real-life scenes, we introduce an"
        },
        {
          "can serve as a reasonable approximation of\nthe": "activity. VLEP (Lei et al., 2020)predicted the next",
          "7.2\nPersonality Computing": "extended task called Personality-predicted Human-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "likely action based on video and textual description.",
          "7.2\nPersonality Computing": "centric Multimodal Reasoning (T 2). The experi-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "VAR (Liang et al., 2022) inferred images and de-",
          "7.2\nPersonality Computing": "mental results show that our method can accurately"
        },
        {
          "can serve as a reasonable approximation of\nthe": "scriptions based on videos and surrounding context,",
          "7.2\nPersonality Computing": "predict personality, and achieves satisfactory multi-"
        },
        {
          "can serve as a reasonable approximation of\nthe": "following αNLI (Bhagavatula et al., 2020), a task",
          "7.2\nPersonality Computing": "modal reasoning performance without relying on"
        },
        {
          "can serve as a reasonable approximation of\nthe": "concentrating solely on the text modality.",
          "7.2\nPersonality Computing": "personality annotations."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-"
        },
        {
          "Limitations": "Personality-aware human-centric multimodal rea-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "nah Rashkin, Doug Downey, Wen-tau Yih, and Yejin"
        },
        {
          "Limitations": "soning is a challenging task. This work is a prelim-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Choi. 2020. Abductive commonsense reasoning.\nIn"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "8th International Conference on Learning Represen-"
        },
        {
          "Limitations": "inary study for this task, which was not defined as",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "tations,\nICLR 2020, Addis Ababa, Ethiopia, April"
        },
        {
          "Limitations": "a generation task.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "26-30, 2020. OpenReview.net."
        },
        {
          "Limitations": "Due to space limitation, Our focus in this work is",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Laura Ana Maria Bostan, Evgeny Kim, and Roman"
        },
        {
          "Limitations": "on the presentation of the task and the dataset. The",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Klinger. 2020.\nGoodnewseveryone: A corpus of"
        },
        {
          "Limitations": "proposed baseline is quite simple and has plenty of",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "news headlines annotated with emotions, semantic"
        },
        {
          "Limitations": "space for improvement.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "roles, and reader perception.\nIn Proceedings of The"
        },
        {
          "Limitations": "Personality computing is a relatively broad field,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "12th Language Resources and Evaluation Confer-"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "ence, LREC 2020, Marseille, France, May 11-16,"
        },
        {
          "Limitations": "our\nreview of\nliterature may not\nfully cover\nthe",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "2020, pages 1554–1566. European Language Re-"
        },
        {
          "Limitations": "research in this area.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "sources Association."
        },
        {
          "Limitations": "Ethics Statement",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Kristina Toutanova. 2019.\nBERT: pre-training of"
        },
        {
          "Limitations": "We would like to thank Lei et al.\n(2018)’s valu-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "deep bidirectional transformers for language under-"
        },
        {
          "Limitations": "able work on TVQA. The TVQA is licensed under",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "standing.\nIn Proceedings of the 2019 Conference of"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "the North American Chapter of the Association for"
        },
        {
          "Limitations": "a licence of MIT, which allows commercial us-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Computational Linguistics: Human Language Tech-"
        },
        {
          "Limitations": "ing, modification, distribution, and private using",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "nologies, NAACL-HLT 2019, Minneapolis, MN, USA,"
        },
        {
          "Limitations": "the material for any purpose. We will also make",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "June 2-7, 2019, Volume 1 (Long and Short Papers),"
        },
        {
          "Limitations": "our PHMRD publicly available later. Personality",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "pages 4171–4186. Association for Computational"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Linguistics."
        },
        {
          "Limitations": "database (PDB) website is an open source, anyone",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "can get information without registration. All of the",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng,"
        },
        {
          "Limitations": "datasets and models are in English, which bene-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Shoujie Tong, Haoran Meng, Lin Xu, Zhongyu Wei,"
        },
        {
          "Limitations": "fits English speakers more. We have employed 2",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Weidong Zhan, Baobao Chang, Sujian Li, Tianyu Liu,"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "and Zhifang Sui. 2022. Premise-based multimodal"
        },
        {
          "Limitations": "postgraduates experienced in natural language pro-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "reasoning: Conditional inference on joint textual and"
        },
        {
          "Limitations": "cessing for verify the results of ChatGPT. We pay",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "visual clues.\nIn Proceedings of the 60th Annual Meet-"
        },
        {
          "Limitations": "postgraduates around $7-10 per hour, well above",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "ing of the Association for Computational Linguistics"
        },
        {
          "Limitations": "the local average wage, and engage in constructive",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "May 22-27, 2022, pages 932–946. Association for"
        },
        {
          "Limitations": "discussions if they are concerned about the process.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Computational Linguistics."
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander"
        },
        {
          "Limitations": "References",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Kolesnikov,\nDirk Weissenborn,\nXiaohua\nZhai,"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Thomas Unterthiner, Mostafa Dehghani, Matthias"
        },
        {
          "Limitations": "Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Minderer, Georg Heigold,\nSylvain Gelly,\nJakob"
        },
        {
          "Limitations": "Baptiste Alayrac,\nJiahui Yu, Radu Soricut,\nJohan",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Uszkoreit,\nand Neil Houlsby. 2021.\nAn image"
        },
        {
          "Limitations": "Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Mil-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "is worth 16x16 words:\nTransformers\nfor\nimage"
        },
        {
          "Limitations": "lican, David Silver, Slav Petrov, Melvin Johnson,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "recognition at scale.\nIn 9th International Conference"
        },
        {
          "Limitations": "Ioannis Antonoglou,\nJulian Schrittwieser, Amelia",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "on Learning Representations,\nICLR 2021, Virtual"
        },
        {
          "Limitations": "Glaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Event, Austria, May 3-7, 2021. OpenReview.net."
        },
        {
          "Limitations": "crap, Angeliki Lazaridou, Orhan Firat, James Molloy,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "Michael\nIsard, Paul Ronald Barham, Tom Henni-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Christoph Feichtenhofer, Haoqi Fan,\nJitendra Malik,"
        },
        {
          "Limitations": "gan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "and Kaiming He. 2019. Slowfast networks for video"
        },
        {
          "Limitations": "Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "recognition.\nIn 2019 IEEE/CVF International Con-"
        },
        {
          "Limitations": "Meyer, Eliza Rutherford, Erica Moreira, Kareem",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "ference on Computer Vision, ICCV 2019, Seoul, Ko-"
        },
        {
          "Limitations": "Ayoub, Megha Goel, George Tucker, Enrique Pi-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "rea (South), October 27 - November 2, 2019, pages"
        },
        {
          "Limitations": "queras, Maxim Krikun, Iain Barr, Nikolay Savinov,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "6201–6210. IEEE."
        },
        {
          "Limitations": "Ivo Danihelka, Becca Roelofs, Anaïs White, Anders",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "Andreassen, Tamara von Glehn, Lakshman Yagati,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Lucie Flekova and Iryna Gurevych. 2015.\nPersonal-"
        },
        {
          "Limitations": "Mehran Kazemi, Lucas Gonzalez, Misha Khalman,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "ity profiling of fictional characters using sense-level"
        },
        {
          "Limitations": "Jakub Sygnowski, and et al. 2023. Gemini: A fam-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "links between lexical resources.\nIn Proceedings of"
        },
        {
          "Limitations": "ily of highly capable multimodal models.\nCoRR,",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "the 2015 Conference on Empirical Methods in Nat-"
        },
        {
          "Limitations": "abs/2312.11805.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "ural Language Processing, EMNLP 2015, Lisbon,"
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Portugal, September 17-21, 2015, pages 1805–1816."
        },
        {
          "Limitations": "Süleyman Aslan and Ugur Güdükbay. 2019. Multi-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "The Association for Computational Linguistics."
        },
        {
          "Limitations": "modal video-based apparent personality recognition",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": ""
        },
        {
          "Limitations": "using long short-term memory and convolutional neu-",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "Matej Gjurkovic and Jan Snajder. 2018. Reddit: A gold"
        },
        {
          "Limitations": "ral networks. CoRR, abs/1911.00381.",
          "Chandra Bhagavatula,\nRonan Le Bras,\nChaitanya": "mine for personality prediction.\nIn Proceedings of"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Second Workshop on Computational Modeling": "of People’s Opinions, Personality, and Emotions in",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "son, Kenji Hata,\nJoshua Kravitz, Stephanie Chen,"
        },
        {
          "the Second Workshop on Computational Modeling": "Social Media, PEOPLES@NAACL-HTL 2018, New",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Yannis Kalantidis, Li-Jia Li, David A. Shamma,"
        },
        {
          "the Second Workshop on Computational Modeling": "Orleans, Louisiana, USA, June 6, 2018, pages 87–97.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Michael S. Bernstein, and Li Fei-Fei. 2017b. Vi-"
        },
        {
          "the Second Workshop on Computational Modeling": "Association for Computational Linguistics.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "sual genome: Connecting language and vision using"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Int. J. Com-\ncrowdsourced dense image annotations."
        },
        {
          "the Second Workshop on Computational Modeling": "Yuan Gong, Yu-An Chung, and James R. Glass. 2021.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "put. Vis., 123(1):32–73."
        },
        {
          "the Second Workshop on Computational Modeling": "AST: audio spectrogram transformer.\nIn Interspeech",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "2021, 22nd Annual Conference of the International",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-"
        },
        {
          "the Second Workshop on Computational Modeling": "Speech Communication Association, Brno, Czechia,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "son, Kenji Hata,\nJoshua Kravitz, Stephanie Chen,"
        },
        {
          "the Second Workshop on Computational Modeling": "30 August - 3 September 2021, pages 571–575. ISCA.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Yannis Kalantidis, Li-Jia Li, David A. Shamma,"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Michael S. Bernstein, and Li Fei-Fei. 2017c.\nVi-"
        },
        {
          "the Second Workshop on Computational Modeling": "Madeleine Grunde-McLaughlin, Ranjay Krishna, and",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "sual genome: Connecting language and vision using"
        },
        {
          "the Second Workshop on Computational Modeling": "Maneesh Agrawala. 2021. AGQA: A benchmark for",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Int. J. Com-\ncrowdsourced dense image annotations."
        },
        {
          "the Second Workshop on Computational Modeling": "compositional spatio-temporal reasoning.\nIn IEEE",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "put. Vis., 123(1):32–73."
        },
        {
          "the Second Workshop on Computational Modeling": "Conference on Computer Vision and Pattern Recog-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "nition, CVPR 2021, virtual, June 19-25, 2021, pages",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg."
        },
        {
          "the Second Workshop on Computational Modeling": "11287–11297. Computer Vision Foundation / IEEE.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "2018. TVQA: localized, compositional video ques-"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "tion answering.\nIn Proceedings of the 2018 Confer-"
        },
        {
          "the Second Workshop on Computational Modeling": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "ence on Empirical Methods in Natural Language Pro-"
        },
        {
          "the Second Workshop on Computational Modeling": "Sun. 2016. Deep residual learning for image recogni-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "cessing, Brussels, Belgium, October 31 - November"
        },
        {
          "the Second Workshop on Computational Modeling": "tion.\nIn 2016 IEEE Conference on Computer Vision",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "4, 2018, pages 1369–1379. Association for Computa-"
        },
        {
          "the Second Workshop on Computational Modeling": "and Pattern Recognition, CVPR 2016, Las Vegas,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "tional Linguistics."
        },
        {
          "the Second Workshop on Computational Modeling": "NV, USA, June 27-30, 2016, pages 770–778. IEEE",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "Computer Society.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Jie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal."
        },
        {
          "the Second Workshop on Computational Modeling": "Jack Hessel, Jena D. Hwang, Jae Sung Park, Rowan",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "2020. What\nis more likely to happen next? video-"
        },
        {
          "the Second Workshop on Computational Modeling": "Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "and-language future event prediction.\nIn Proceedings"
        },
        {
          "the Second Workshop on Computational Modeling": "Saenko, and Yejin Choi. 2022.\nThe abduction of",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "of the 2020 Conference on Empirical Methods in Nat-"
        },
        {
          "the Second Workshop on Computational Modeling": "sherlock holmes: A dataset for visual abductive rea-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "ural Language Processing, EMNLP 2020, Online,"
        },
        {
          "the Second Workshop on Computational Modeling": "soning.\nIn Computer Vision - ECCV 2022 - 17th",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "November 16-20, 2020, pages 8769–8784. Associa-"
        },
        {
          "the Second Workshop on Computational Modeling": "European Conference, Tel Aviv, Israel, October 23-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "tion for Computational Linguistics."
        },
        {
          "the Second Workshop on Computational Modeling": "27, 2022, Proceedings, Part XXXVI, volume 13696 of",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "Lecture Notes in Computer Science, pages 558–575.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng"
        },
        {
          "the Second Workshop on Computational Modeling": "Springer.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Yu, and Jingjing Liu. 2020. HERO: hierarchical en-"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "coder for video+language omni-representation pre-"
        },
        {
          "the Second Workshop on Computational Modeling": "Oana Ignat, Santiago Castro, Hanwen Miao, Weiji Li,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "training.\nIn Proceedings of the 2020 Conference on"
        },
        {
          "the Second Workshop on Computational Modeling": "and Rada Mihalcea. 2021. Whyact:\nIdentifying ac-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Empirical Methods in Natural Language Processing,"
        },
        {
          "the Second Workshop on Computational Modeling": "tion reasons in lifestyle vlogs.\nIn Proceedings of the",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "EMNLP 2020, Online, November 16-20, 2020, pages"
        },
        {
          "the Second Workshop on Computational Modeling": "2021 Conference on Empirical Methods in Natural",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "2046–2065. Association for Computational Linguis-"
        },
        {
          "the Second Workshop on Computational Modeling": "Language Processing, EMNLP 2021, Virtual Event",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "tics."
        },
        {
          "the Second Workshop on Computational Modeling": "/ Punta Cana, Dominican Republic, 7-11 November,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "2021, pages 4770–4785. Association for Computa-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Chen Liang, Wenguan Wang, Tianfei Zhou, and Yi Yang."
        },
        {
          "the Second Workshop on Computational Modeling": "tional Linguistics.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "2022. Visual abductive reasoning.\nIn IEEE/CVF"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "the Second Workshop on Computational Modeling": "Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "nition, CVPR 2022, New Orleans, LA, USA, June"
        },
        {
          "the Second Workshop on Computational Modeling": "Cao, and Li Yuan. 2023.\nChat-univi: Unified vi-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "18-24, 2022, pages 15544–15554. IEEE."
        },
        {
          "the Second Workshop on Computational Modeling": "sual representation empowers large language mod-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "els with image and video understanding.\nCoRR,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, and"
        },
        {
          "the Second Workshop on Computational Modeling": "abs/2311.08046.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Alexander G. Hauptmann. 2018. Focal visual-text at-"
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "tention for visual question answering.\nIn 2018 IEEE"
        },
        {
          "the Second Workshop on Computational Modeling": "Onno Kampman, Elham J. Barezi, Dario Bertero, and",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "the Second Workshop on Computational Modeling": "Pascale Fung. 2018.\nInvestigating audio, video, and",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "nition, CVPR 2018, Salt Lake City, UT, USA, June"
        },
        {
          "the Second Workshop on Computational Modeling": "text fusion methods for end-to-end automatic person-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "18-22, 2018, pages 6135–6143. Computer Vision"
        },
        {
          "the Second Workshop on Computational Modeling": "ality prediction.\nIn Proceedings of the 56th Annual",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Foundation / IEEE Computer Society."
        },
        {
          "the Second Workshop on Computational Modeling": "Meeting of\nthe Association for Computational Lin-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "guistics, ACL 2018, Melbourne, Australia, July 15-20,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Robert R McCrae and Paul T Costa. 1987. Validation"
        },
        {
          "the Second Workshop on Computational Modeling": "2018, Volume 2: Short Papers, pages 606–611. Asso-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "of the five-factor model of personality across instru-"
        },
        {
          "the Second Workshop on Computational Modeling": "ciation for Computational Linguistics.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Journal of personality and\nments and observers."
        },
        {
          "the Second Workshop on Computational Modeling": "Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "social psychology, 52(1):81."
        },
        {
          "the Second Workshop on Computational Modeling": "and Juan Carlos Niebles. 2017a. Dense-captioning",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": ""
        },
        {
          "the Second Workshop on Computational Modeling": "events in videos.\nIn IEEE International Conference",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Isabel Briggs Myers, Mary H McCaulley, and Robert"
        },
        {
          "the Second Workshop on Computational Modeling": "on Computer Vision, ICCV 2017, Venice, Italy, Oc-",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "Most. 1985. Manual, a guide to the development and"
        },
        {
          "the Second Workshop on Computational Modeling": "tober 22-29, 2017, pages 706–715. IEEE Computer",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "use of\nthe Myers-Briggs type indicator.\nconsulting"
        },
        {
          "the Second Workshop on Computational Modeling": "Society.",
          "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-": "psychologists press."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "yourself and the others in your life. Harper & Row.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Vinciarelli. 2022. What an \"ehm\" leaks about you:"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Mapping fillers\ninto personality traits with quan-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Jae Sung Park, Chandra Bhagavatula, Roozbeh Mot-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "IEEE\ntum evolutionary feature selection algorithms."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "taghi, Ali Farhadi, and Yejin Choi. 2020. Visual-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Trans. Affect. Comput., 13(1):108–121."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "comet: Reasoning about\nthe dynamic context of a",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "still image.\nIn Computer Vision - ECCV 2020 - 16th",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "European Conference, Glasgow, UK, August 23-28,",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Mohammad Norouzi, Wolfgang Macherey, Maxim"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "2020, Proceedings, Part V, volume 12350 of Lecture",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Notes in Computer Science, pages 508–524. Springer.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Klingner, Apurva Shah, Melvin Johnson, Xiaobing"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Rebecca J. Passonneau. 2006. Measuring agreement on",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Taku Kudo, Hideto Kazawa, Keith Stevens, George"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "set-valued items (MASI) for semantic and pragmatic",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "annotation.\nIn Proceedings of the Fifth International",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Conference on Language Resources and Evaluation,",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Greg Corrado, Macduff Hughes, and Jeffrey Dean."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "LREC 2006, Genoa, Italy, May 22-28, 2006, pages",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "2016. Google’s neural machine translation system:"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "831–836. European Language Resources Association",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Bridging the gap between human and machine trans-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "(ELRA).",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "lation. CoRR, abs/1609.08144."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Joan Plepi, Béla Neuendorf, Lucie Flek, and Charles",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Welch. 2022. Unifying data perspectivism and per-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Chua. 2021.\nNext-qa:\nNext phase of question-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "sonalization: An application to social norms.\nIn",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "answering to explaining temporal actions.\nIn IEEE"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Proceedings of\nthe 2022 Conference on Empirical",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Conference on Computer Vision and Pattern Recog-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Methods in Natural Language Processing, EMNLP",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "nition, CVPR 2021, virtual, June 19-25, 2021, pages"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "2022, Abu Dhabi, United Arab Emirates, December",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "9777–9786. Computer Vision Foundation / IEEE."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "7-11, 2022, pages 7391–7402. Association for Com-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Liang Yang, Shuqun Li, Xi Luo, Bo Xu, Yuanling Geng,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "putational Linguistics.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Zeyuan Zeng, Fan Zhang, and Hongfei Lin. 2022a."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Computational personality: a survey. Soft Comput.,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Víctor Ponce-López, Baiyu Chen, Marc Oliu, Ciprian A.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "26(18):9587–9605."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Corneanu, Albert Clapés,\nIsabelle Guyon, Xavier",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Baró, Hugo Jair Escalante, and Sergio Escalera. 2016.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Qi Yang, Sergey Nikolenko, Alfred Huang, and Alek-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Chalearn LAP 2016: First round challenge on first",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "sandr Farseev. 2022b. Personality-driven social mul-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "impressions - dataset and results.\nIn Computer Vision",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "timedia content recommendation.\nIn MM ’22: The"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "- ECCV 2016 Workshops - Amsterdam, The Nether-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "30th ACM International Conference on Multimedia,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "lands, October 8-10 and 15-16, 2016, Proceedings,",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Lisboa, Portugal, October 10 - 14, 2022, pages 7290–"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Part III, volume 9915 of Lecture Notes in Computer",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "7299. ACM."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Science, pages 400–418.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Yisi Sang, Xiangyang Mou, Mo Yu, Dakuo Wang, Jing",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Tong, and Louis-Philippe Morency. 2019.\nSocial-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Li, and Jeffrey Stanton. 2022. MBTI personality",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "iq: A question answering benchmark for artificial"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "prediction for fictional characters using movie scripts.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "social\nintelligence.\nIn IEEE Conference on Com-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "CoRR, abs/2210.10994.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "puter Vision and Pattern Recognition, CVPR 2019,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Long Beach, CA, USA, June 16-20, 2019, pages 8807–"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Rico Sennrich, Barry Haddow, and Alexandra Birch.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "8817. Computer Vision Foundation / IEEE."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "2016. Neural machine translation of rare words with",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "the 54th Annual\nsubword units.\nIn Proceedings of",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Meeting of\nthe Association for Computational Lin-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Choi. 2019. From recognition to cognition: Visual"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "commonsense reasoning.\nIn IEEE Conference on"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "many, Volume 1: Long Papers. The Association for",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Computer Vision and Pattern Recognition, CVPR"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Computer Linguistics.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "2019, Long Beach, CA, USA, June 16-20, 2019, pages"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "6720–6731. Computer Vision Foundation / IEEE."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Yang, and Tat-Seng Chua. 2019. Annotating objects",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "and relations in user-generated videos.\nIn Proceed-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusu-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "ings of the 2019 on International Conference on Mul-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "pati,\nJack Hessel, Ali Farhadi,\nand Yejin Choi."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "timedia Retrieval, ICMR 2019, Ottawa, ON, Canada,",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "2022. MERLOT RESERVE: neural script knowl-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "June 10-13, 2019, pages 279–287. ACM.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "edge through vision and language and sound.\nIn"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Sanja Stajner and Seren Yenikent. 2021. Why is MBTI",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "tern Recognition, CVPR 2022, New Orleans, LA,"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "personality detection from texts a difficult task?\nIn",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "USA, June 18-24, 2022, pages 16354–16366. IEEE."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Proceedings of the 16th Conference of the European",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": ""
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "Chapter of\nthe Association for Computational Lin-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Le Zhang, Songyou Peng, and Stefan Winkler. 2022."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "guistics: Main Volume, EACL 2021, Online, April 19",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "Persemon: A deep network for joint analysis of ap-"
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "- 23, 2021, pages 3580–3589. Association for Com-",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "parent personality, emotion and their\nrelationship."
        },
        {
          "Helen Palmer. 1988. The Enneagram: Understanding": "putational Linguistics.",
          "Mohammad Tayarani, Anna Esposito, and Alessandro": "IEEE Trans. Affect. Comput., 13(1):298–305."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "IEEE Trans. Knowl.\nmulti-label learning algorithms.",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "Data Eng., 26(8):1819–1837.",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": "Model was\nthe model"
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "A\nChatGPT Prompt and Some Samples",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "A.1\nData Filtering",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "PROMPT:“ {What color is the object?} as unre-",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "lated to individuals, while {Why does person 0",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "like this object?} is regarded as pertinent\nto hu-",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": "traits."
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "man subjects. Is {QUESTION} related to human",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": "For evaluation,"
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "behavior or psychology?”",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "",
          "B.2\nFFM": ""
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "Similarly, the question “What kind of object is",
          "B.2\nFFM": "The Five dimensions,"
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "on person 0 ’s leg?”\nis considered irrelevant\nto",
          "B.2\nFFM": "emphases, as follows:"
        },
        {
          "Min-Ling Zhang and Zhi-Hua Zhou. 2014. A review on": "individuals, whereas the query “What\nis person",
          "B.2\nFFM": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "Similarly, the question “What kind of object is"
        },
        {
          "behavior or psychology?”": "on person 0 ’s leg?”\nis considered irrelevant\nto"
        },
        {
          "behavior or psychology?”": "individuals, whereas the query “What\nis person"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "0 talking about?”\nis deemed relevant\nto human"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "subjects."
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "B\nPersonality typology"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "B.1\nMBTI"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "Myers-Briggs Type Indicator\nis an intro-\n(MBTI)"
        },
        {
          "behavior or psychology?”": "spective self-report questionnaire used in person-"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "ality typology to identify various psychological"
        },
        {
          "behavior or psychology?”": ""
        },
        {
          "behavior or psychology?”": "preferences in how people view the environment"
        },
        {
          "behavior or psychology?”": "and make judgments."
        },
        {
          "behavior or psychology?”": "For evaluation, the MBTI splits personality into"
        },
        {
          "behavior or psychology?”": "four dimensions (E/I, S/N, T/F, and J/P). The four"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "in polar opposites as shown in Fig. 4(a)."
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "B.3\nEnneagram"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "The Enneagram personality unfolds according to"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "the nine horns of the ancient totem, revealing nine"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "different\ninner dynamics that make each person"
        },
        {
          "Detailed names of the five personality dimensions": "inherently unique as an individual.the nine person-"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "ality types described by the Enneagram personality"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "theory are not good or bad; there are recognizable"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "and fundamental differences in the way people with"
        },
        {
          "Detailed names of the five personality dimensions": "different personality types respond to the world. It"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "is now common knowledge that our personalities"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "are our own, that they filter and interpret what we"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "see and hear, and that the basic principle of Type"
        },
        {
          "Detailed names of the five personality dimensions": "9 personality theory is that each of us has one of"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "nine possible \"filters\" that will keep the blueprint"
        },
        {
          "Detailed names of the five personality dimensions": ""
        },
        {
          "Detailed names of the five personality dimensions": "for our lives and the general It\nis used to protect"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4:\nthree personalities.": "Description"
        },
        {
          "Figure 4:\nthree personalities.": "These people tend to be reserved yet willful, with a rational outlook on life. They compose"
        },
        {
          "Figure 4:\nthree personalities.": "their actions carefully and carry them out with methodical purpose."
        },
        {
          "Figure 4:\nthree personalities.": "These people tend to be warm and unassuming in their own steady way. They’re efficient"
        },
        {
          "Figure 4:\nthree personalities.": "and responsible, giving careful attention to practical details in their daily lives."
        },
        {
          "Figure 4:\nthree personalities.": "They tend to approach life with deep thoughtfulness and imagination. Their inner vision,"
        },
        {
          "Figure 4:\nthree personalities.": "personal values, and a quiet, principled version of humanism guide them in all things."
        },
        {
          "Figure 4:\nthree personalities.": "These thoughtful tacticians love perfecting the details of life, applying creativity and ratio-"
        },
        {
          "Figure 4:\nthree personalities.": "nality to everything they do. Their inner world is often a private, complex one."
        },
        {
          "Figure 4:\nthree personalities.": "They tend to have an individualistic mindset, pursuing goals without needing much external"
        },
        {
          "Figure 4:\nthree personalities.": "connection. They engage in life with inquisitiveness and personal skill, varying their approach"
        },
        {
          "Figure 4:\nthree personalities.": "as needed."
        },
        {
          "Figure 4:\nthree personalities.": "They tend to have open minds, approaching life, new experiences, and people with grounded"
        },
        {
          "Figure 4:\nthree personalities.": "warmth. Their ability to stay in the moment helps them uncover exciting potentials."
        },
        {
          "Figure 4:\nthree personalities.": "These rare personality types tend to be quiet, open-minded, and imaginative, and they apply"
        },
        {
          "Figure 4:\nthree personalities.": "a caring and creative approach to everything they do."
        },
        {
          "Figure 4:\nthree personalities.": "These flexible thinkers enjoy taking an unconventional approach to many aspects of life."
        },
        {
          "Figure 4:\nthree personalities.": "They often seek out unlikely paths, mixing willingness to experiment with personal creativity."
        },
        {
          "Figure 4:\nthree personalities.": "They tend to be energetic and action-oriented, deftly navigating whatever is in front of them."
        },
        {
          "Figure 4:\nthree personalities.": "They love uncovering life’s opportunities, whether socializing with others or in more personal"
        },
        {
          "Figure 4:\nthree personalities.": "pursuits."
        },
        {
          "Figure 4:\nthree personalities.": "These people love vibrant experiences, engaging in life eagerly and taking pleasure in"
        },
        {
          "Figure 4:\nthree personalities.": "discovering the unknown. They can be very social, often encouraging others into shared"
        },
        {
          "Figure 4:\nthree personalities.": "activities."
        },
        {
          "Figure 4:\nthree personalities.": "These people tend to embrace big ideas and actions that reflect\ntheir sense of hope and"
        },
        {
          "Figure 4:\nthree personalities.": "goodwill toward others. Their vibrant energy can flow in many directions."
        },
        {
          "Figure 4:\nthree personalities.": "They tend to be bold and creative, deconstructing and rebuilding ideas with great mental"
        },
        {
          "Figure 4:\nthree personalities.": "agility. They pursue their goals vigorously despite any resistance they might encounter."
        },
        {
          "Figure 4:\nthree personalities.": "They possess great fortitude, emphatically following their own sensible judgment. They"
        },
        {
          "Figure 4:\nthree personalities.": "often serve as a stabilizing force among others, able to offer solid direction amid adversity."
        },
        {
          "Figure 4:\nthree personalities.": "They are attentive and people-focused, and they enjoy taking part in their social community."
        },
        {
          "Figure 4:\nthree personalities.": "Their achievements are guided by decisive values, and they willingly offer guidance to others."
        },
        {
          "Figure 4:\nthree personalities.": "These warm, forthright\ntypes love helping others, and they tend to have strong ideas and"
        },
        {
          "Figure 4:\nthree personalities.": "values. They back their perspective with the creative energy to achieve their goals."
        },
        {
          "Figure 4:\nthree personalities.": "They are decisive people who love momentum and accomplishment. They gather information"
        },
        {
          "Figure 4:\nthree personalities.": "to construct their creative visions but rarely hesitate for long before acting on them."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Enneagram": "Perfect",
          "Description": "Emphasizes principles, is not easy to compromise, distinguishes between black and white,"
        },
        {
          "Enneagram": "",
          "Description": "has high demands on both oneself and others, and pursues perfection."
        },
        {
          "Enneagram": "Helping",
          "Description": "Desire to establish a good relationship with others, people-oriented, willing to accommodate"
        },
        {
          "Enneagram": "",
          "Description": "others."
        },
        {
          "Enneagram": "Achievement",
          "Description": "Competitive, and measure their own value by achievements, is a workaholic."
        },
        {
          "Enneagram": "Ego",
          "Description": "Emotional, afraid of being rejected by others, feeling that others do not understand them-"
        },
        {
          "Enneagram": "",
          "Description": "selves, doing their own way."
        },
        {
          "Enneagram": "Ideal",
          "Description": "likes to think and analyze, has a strong desire for knowledge, but lacks action, and has low"
        },
        {
          "Enneagram": "",
          "Description": "requirements for material life."
        },
        {
          "Enneagram": "Doubtful",
          "Description": "Be cautious in doing things, not easy to trust others, have many doubts, like group life, and"
        },
        {
          "Enneagram": "",
          "Description": "work hard."
        },
        {
          "Enneagram": "Active",
          "Description": "Optimistic, like novelty, like to follow the trend, don’t like pressure."
        },
        {
          "Enneagram": "Leader",
          "Description": "Pursue power, emphasize strength, do not rely on others, and have a sense of justice."
        },
        {
          "Enneagram": "Peaceful",
          "Description": "It takes a long time to make decisions, fears disputes, and prays for harmonious coexistence."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Yonghui Wu",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth",
        "Katie Millican",
        "David Silver",
        "Slav Petrov",
        "Melvin Johnson",
        "Ioannis Antonoglou",
        "Julian Schrittwieser",
        "Amelia Glaese",
        "Jilin Chen",
        "Emily Pitler",
        "Timothy Lillicrap",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "James Molloy",
        "Michael Isard",
        "Paul Barham",
        "Tom Hennigan",
        "Benjamin Lee",
        "Fabio Viola",
        "Malcolm Reynolds",
        "Yuanzhong Xu",
        "Ryan Doherty",
        "Eli Collins",
        "Clemens Meyer",
        "Eliza Rutherford",
        "Erica Moreira",
        "Kareem Ayoub",
        "Megha Goel",
        "George Tucker",
        "Enrique Piqueras",
        "Maxim Krikun",
        "Iain Barr",
        "Nikolay Savinov",
        "Ivo Danihelka",
        "Becca Roelofs",
        "Anaïs White",
        "Anders Andreassen",
        "Lakshman Tamara Von Glehn",
        "Mehran Yagati",
        "Lucas Kazemi",
        "Misha Gonzalez",
        "Jakub Khalman",
        "Sygnowski"
      ],
      "year": "2023",
      "venue": "",
      "doi": "10.48550/ARXIV.2312.11805"
    },
    {
      "citation_id": "2",
      "title": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks",
      "authors": [
        "Süleyman Aslan",
        "Ugur Güdükbay"
      ],
      "year": "2019",
      "venue": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks"
    },
    {
      "citation_id": "3",
      "title": "Abductive commonsense reasoning",
      "authors": [
        "Chandra Bhagavatula",
        "Le Ronan",
        "Chaitanya Bras",
        "Keisuke Malaviya",
        "Ari Sakaguchi",
        "Hannah Holtzman",
        "Doug Rashkin",
        "Wen-Tau Downey",
        "Yejin Yih",
        "Choi"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "4",
      "title": "Goodnewseveryone: A corpus of news headlines annotated with emotions, semantic roles, and reader perception",
      "authors": [
        "Laura Ana",
        "Maria Bostan",
        "Evgeny Kim",
        "Roman Klinger"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020"
    },
    {
      "citation_id": "5",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
      "doi": "10.18653/v1/n19-1423"
    },
    {
      "citation_id": "6",
      "title": "Premise-based multimodal reasoning: Conditional inference on joint textual and visual clues",
      "authors": [
        "Qingxiu Dong",
        "Ziwei Qin",
        "Heming Xia",
        "Tian Feng",
        "Shoujie Tong",
        "Haoran Meng",
        "Lin Xu",
        "Zhongyu Wei",
        "Weidong Zhan",
        "Baobao Chang",
        "Sujian Li",
        "Tianyu Liu",
        "Zhifang Sui"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.66"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event"
    },
    {
      "citation_id": "8",
      "title": "Slowfast networks for video recognition",
      "authors": [
        "Christoph Feichtenhofer",
        "Haoqi Fan",
        "Jitendra Malik",
        "Kaiming He"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)",
      "doi": "10.1109/ICCV.2019.00630"
    },
    {
      "citation_id": "9",
      "title": "Personality profiling of fictional characters using sense-level links between lexical resources",
      "authors": [
        "Lucie Flekova",
        "Iryna Gurevych"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/d15-1208"
    },
    {
      "citation_id": "10",
      "title": "Reddit: A gold mine for personality prediction",
      "authors": [
        "Matej Gjurkovic",
        "Jan Snajder"
      ],
      "year": "2018",
      "venue": "Proceedings of the Second Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media, PEOPLES@NAACL-HTL 2018",
      "doi": "10.18653/v1/w18-1112"
    },
    {
      "citation_id": "11",
      "title": "AST: audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association",
      "doi": "10.21437/Interspeech.2021-698"
    },
    {
      "citation_id": "12",
      "title": "AGQA: A benchmark for compositional spatio-temporal reasoning",
      "authors": [
        "Madeleine Grunde-Mclaughlin",
        "Ranjay Krishna",
        "Maneesh Agrawala"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual",
      "doi": "10.1109/CVPR46437.2021.01113"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "14",
      "title": "The abduction of sherlock holmes: A dataset for visual abductive reasoning",
      "authors": [
        "Jack Hessel",
        "Jena Hwang",
        "Jae Park",
        "Rowan Zellers",
        "Chandra Bhagavatula",
        "Anna Rohrbach",
        "Kate Saenko",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "The abduction of sherlock holmes: A dataset for visual abductive reasoning",
      "doi": "10.1007/978-3-031-20059-5_32"
    },
    {
      "citation_id": "15",
      "title": "Proceedings, Part XXXVI",
      "year": "2022",
      "venue": "Proceedings, Part XXXVI"
    },
    {
      "citation_id": "16",
      "title": "Whyact: Identifying action reasons in lifestyle vlogs",
      "authors": [
        "Oana Ignat",
        "Santiago Castro",
        "Hanwen Miao",
        "Weiji Li",
        "Rada Mihalcea"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.392"
    },
    {
      "citation_id": "17",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Caiwan Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "doi": "10.48550/ARXIV.2311.08046"
    },
    {
      "citation_id": "18",
      "title": "Investigating audio, video, and text fusion methods for end-to-end automatic personality prediction",
      "authors": [
        "Onno Kampman",
        "J Elham",
        "Dario Barezi",
        "Pascale Bertero",
        "Fung"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018",
      "doi": "10.18653/v1/P18-2096"
    },
    {
      "citation_id": "19",
      "title": "Dense-captioning events in videos",
      "authors": [
        "Ranjay Krishna",
        "Kenji Hata",
        "Frederic Ren",
        "Li Fei-Fei",
        "Juan Niebles"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2017",
      "doi": "10.1109/ICCV.2017.83"
    },
    {
      "citation_id": "20",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz",
        "Stephanie Chen",
        "Yannis Kalantidis",
        "Li-Jia Li",
        "David Shamma",
        "Michael Bernstein",
        "Li Fei-Fei"
      ],
      "year": "2017",
      "venue": "Int. J. Comput. Vis",
      "doi": "10.1007/s11263-016-0981-7"
    },
    {
      "citation_id": "21",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz",
        "Stephanie Chen",
        "Yannis Kalantidis",
        "Li-Jia Li",
        "David Shamma",
        "Michael Bernstein",
        "Li Fei-Fei"
      ],
      "year": "2017",
      "venue": "Int. J. Comput. Vis",
      "doi": "10.1007/s11263-016-0981-7"
    },
    {
      "citation_id": "22",
      "title": "TVQA: localized, compositional video question answering",
      "authors": [
        "Jie Lei",
        "Licheng Yu",
        "Mohit Bansal",
        "Tamara Berg"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/d18-1167"
    },
    {
      "citation_id": "23",
      "title": "What is more likely to happen next? videoand-language future event prediction",
      "authors": [
        "Jie Lei",
        "Licheng Yu",
        "Tamara Berg",
        "Mohit Bansal"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.706"
    },
    {
      "citation_id": "24",
      "title": "HERO: hierarchical encoder for video+language omni-representation pretraining",
      "authors": [
        "Linjie Li",
        "Yen-Chun Chen",
        "Yu Cheng",
        "Zhe Gan",
        "Licheng Yu",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.161"
    },
    {
      "citation_id": "25",
      "title": "Visual abductive reasoning",
      "authors": [
        "Chen Liang",
        "Wenguan Wang",
        "Tianfei Zhou",
        "Yi Yang"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022",
      "doi": "10.1109/CVPR52688.2022.01512"
    },
    {
      "citation_id": "26",
      "title": "Focal visual-text attention for visual question answering",
      "authors": [
        "Junwei Liang",
        "Lu Jiang",
        "Liangliang Cao",
        "Li-Jia Li",
        "Alexander Hauptmann"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018",
      "doi": "10.1109/CVPR.2018.00642"
    },
    {
      "citation_id": "27",
      "title": "Validation of the five-factor model of personality across instruments and observers",
      "authors": [
        "Paul Robert R Mccrae",
        "Costa"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "28",
      "title": "Manual, a guide to the development and use of the Myers-Briggs type indicator",
      "authors": [
        "Isabel Briggs Myers",
        "Mary Mccaulley",
        "Robert Most"
      ],
      "year": "1985",
      "venue": "Manual, a guide to the development and use of the Myers-Briggs type indicator"
    },
    {
      "citation_id": "29",
      "title": "The Enneagram: Understanding yourself and the others in your life",
      "authors": [
        "Helen Palmer"
      ],
      "year": "1988",
      "venue": "The Enneagram: Understanding yourself and the others in your life"
    },
    {
      "citation_id": "30",
      "title": "Visualcomet: Reasoning about the dynamic context of a still image",
      "authors": [
        "Jae Sung Park",
        "Chandra Bhagavatula",
        "Roozbeh Mottaghi",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020 -16th European Conference",
      "doi": "10.1007/978-3-030-58558-7_30"
    },
    {
      "citation_id": "31",
      "title": "Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation",
      "authors": [
        "Rebecca Passonneau"
      ],
      "year": "2006",
      "venue": "Proceedings of the Fifth International Conference on Language Resources and Evaluation, LREC 2006"
    },
    {
      "citation_id": "32",
      "title": "Unifying data perspectivism and personalization: An application to social norms",
      "authors": [
        "Joan Plepi",
        "Béla Neuendorf",
        "Lucie Flek",
        "Charles Welch"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Chalearn LAP 2016: First round challenge on first impressions -dataset and results",
      "authors": [
        "Víctor Ponce-López",
        "Baiyu Chen",
        "Marc Oliu",
        "Ciprian Corneanu",
        "Albert Clapés",
        "Isabelle Guyon",
        "Xavier Baró",
        "Hugo Escalante",
        "Sergio Escalera"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops -Amsterdam",
      "doi": "10.1007/978-3-319-49409-8_32"
    },
    {
      "citation_id": "34",
      "title": "MBTI personality prediction for fictional characters using movie scripts",
      "authors": [
        "Yisi Sang",
        "Xiangyang Mou",
        "Mo Yu",
        "Dakuo Wang",
        "Jing Li",
        "Jeffrey Stanton"
      ],
      "year": "2022",
      "venue": "MBTI personality prediction for fictional characters using movie scripts",
      "doi": "10.48550/arXiv.2210.10994"
    },
    {
      "citation_id": "35",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
      "doi": "10.18653/v1/p16-1162"
    },
    {
      "citation_id": "36",
      "title": "Annotating objects and relations in user-generated videos",
      "authors": [
        "Xindi Shang",
        "Donglin Di",
        "Junbin Xiao",
        "Yu Cao",
        "Xun Yang",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 on International Conference on Multimedia Retrieval",
      "doi": "10.1145/3323873.3325056"
    },
    {
      "citation_id": "37",
      "title": "Why is MBTI personality detection from texts a difficult task?",
      "authors": [
        "Sanja Stajner",
        "Seren Yenikent"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021",
      "doi": "10.18653/v1/2021.eacl-main.312"
    },
    {
      "citation_id": "38",
      "title": "What an \"ehm\" leaks about you: Mapping fillers into personality traits with quantum evolutionary feature selection algorithms",
      "authors": [
        "Mohammad Tayarani",
        "Anna Esposito",
        "Alessandro Vinciarelli"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2019.2930695"
    },
    {
      "citation_id": "39",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Yonghui Wu",
        "Mike Schuster",
        "Zhifeng Chen",
        "Quoc Le",
        "Mohammad Norouzi",
        "Wolfgang Macherey",
        "Maxim Krikun",
        "Yuan Cao",
        "Qin Gao",
        "Klaus Macherey",
        "Jeff Klingner",
        "Apurva Shah",
        "Melvin Johnson",
        "Xiaobing Liu",
        "Lukasz Kaiser",
        "Stephan Gouws",
        "Yoshikiyo Kato",
        "Taku Kudo",
        "Hideto Kazawa",
        "Keith Stevens",
        "George Kurian",
        "Nishant Patil",
        "Wei Wang",
        "Cliff Young",
        "Jason Smith",
        "Jason Riesa",
        "Alex Rudnick",
        "Oriol Vinyals",
        "Greg Corrado",
        "Macduff Hughes",
        "Jeffrey Dean"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation"
    },
    {
      "citation_id": "40",
      "title": "Next-qa: Next phase of questionanswering to explaining temporal actions",
      "authors": [
        "Junbin Xiao",
        "Xindi Shang",
        "Angela Yao",
        "Tat-Seng Chua"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual",
      "doi": "10.1109/CVPR46437.2021.00965"
    },
    {
      "citation_id": "41",
      "title": "2022a. Computational personality: a survey",
      "authors": [
        "Liang Yang",
        "Shuqun Li",
        "Xi Luo",
        "Bo Xu",
        "Yuanling Geng",
        "Zeyuan Zeng",
        "Fan Zhang",
        "Hongfei Lin"
      ],
      "venue": "Soft Comput",
      "doi": "10.1007/s00500-022-06786-6"
    },
    {
      "citation_id": "42",
      "title": "Alfred Huang, and Aleksandr Farseev. 2022b. Personality-driven social multimedia content recommendation",
      "authors": [
        "Qi Yang",
        "Sergey Nikolenko"
      ],
      "year": "2022",
      "venue": "MM '22: The 30th ACM International Conference on Multimedia",
      "doi": "10.1145/3503161.3548769"
    },
    {
      "citation_id": "43",
      "title": "Socialiq: A question answering benchmark for artificial social intelligence",
      "authors": [
        "Amir Zadeh",
        "Michael Chan",
        "Paul Liang",
        "Edmund Tong",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2019.00901"
    },
    {
      "citation_id": "44",
      "title": "From recognition to cognition: Visual commonsense reasoning",
      "authors": [
        "Rowan Zellers",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2019.00688"
    },
    {
      "citation_id": "45",
      "title": "MERLOT RESERVE: neural script knowledge through vision and language and sound",
      "authors": [
        "Rowan Zellers",
        "Jiasen Lu",
        "Ximing Lu",
        "Youngjae Yu",
        "Yanpeng Zhao",
        "Mohammadreza Salehi",
        "Aditya Kusupati",
        "Jack Hessel",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022",
      "doi": "10.1109/CVPR52688.2022.01589"
    },
    {
      "citation_id": "46",
      "title": "Persemon: A deep network for joint analysis of apparent personality, emotion and their relationship",
      "authors": [
        "Le Zhang",
        "Songyou Peng",
        "Stefan Winkler"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2019.2951656"
    }
  ]
}