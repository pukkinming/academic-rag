{
  "paper_id": "2211.08233v3",
  "title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach For Speech Emotion Recognition",
  "published": "2022-11-14T13:35:01Z",
  "authors": [
    "Jiaxin Ye",
    "Xin-cheng Wen",
    "Yujie Wei",
    "Yong Xu",
    "Kunhong Liu",
    "Hongming Shan"
  ],
  "keywords": [
    "Speech emotion recognition",
    "bi-direction",
    "multi-scale",
    "dynamic fusion",
    "temporal modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate the superior performance of TIM-Net, gaining 2.34% and 2.61% improvements of the average UAR and WAR over the second-best on each corpus. The source code is available at https://github.com/Jiaxin-Ye/TIM-Net_SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is to automatically recognize human emotion and affective states from speech signals, enabling machines to communicate with humans emotionally  [1] . It becomes increasingly important with the development of the human-computer interaction technique.\n\nThe key challenge in SER is how to model emotional representations from speech signals. Traditional methods  [2, 3]  focus on the efficient extraction of hand-crafted features, ‚Ä†: Co-corresponding author.\n\nwhich are fed into conventional machine learning methods, such as Support Vector Machine (SVM). More recent methods based on deep learning techniques aim to learn the class-discriminative features in an end-to-end manner, which employ various architectures such as Convolutional Neural Network (CNN)  [4, 5] , Recurrent Neural Network (RNN)  [6, 7] , or the combination of CNN and RNN  [8] .\n\nIn particular, various temporal modeling approaches, such as Long Short-Term Memory (LSTM), Gate Recurrent Unit (GRU), and Temporal Convolution Network (TCN), are widely adopted in SER, aiming to capture dynamic temporal variations of speech signals. For example, Wang et al.  [7]  proposed a dual-level LSTM to harness temporal information from different time-frequency resolutions. Zhong et al.  [9]  used CNN with Bi-GRU and focal loss for learning integrated spatiotemporal features. Rajamani et al.  [6]  presented an attention-based ReLU within GRU to capture long-range interactions among the features. Zhao et al.  [8]  leveraged fully CNN and Bi-LSTM to learn the spatiotemporal features. However, these methods suffer from the following drawbacks: 1) they lack sufficient capacity to capture longrange dependencies for context modeling, where the capture of the context in speech is crucial for SER since human emotions are usually highly context-dependent; and 2) they do not explore the dynamic receptive field of the model, while learning dynamic instead of maximal ones can improve model generalization ability to unknown data or corpus.\n\nTo overcome these limitations in SER, we propose a Temporal-aware bI-direction Multi-scale Network, termed TIM-Net, which is a novel temporal emotional modeling approach to learn multi-scale contextual affective representations from various time scales. The contributions are threefold. First, we propose a temporal-aware block based on the Dilated Causal Convolution (DC Conv) as a core unit in TIM-Net. The dilated convolution can enlarge and refine the receptive field of temporal patterns. The causal convolution combined with dilated convolution can help model relax the assumption of first-order Markov property compared with",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Temporal-Aware Bi-Direction Multi-Scale Network",
      "text": "Speech MFCC       RNNs  [10] . In this way, we can incorporate an ùëÅ-order (ùëÅ denotes the number of all previous frames) connection into the network to aggregate information from different temporal locations. Second, we devise a novel bi-direction architecture integrating complementary information from the past and the future for modeling long-range temporal dependencies. To the best of our knowledge, TIM-Net is the first bi-direction temporal network by focusing on multi-scale fusion in the SER, rather than simply concatenating forward and backward hidden states. Third, we design a dynamic fusion module by combining dynamic receptive fields for learning the interdependencies at different temporal scales, so as to improve the model generalizability. Due to the articulation speed and pause time varying significantly across speakers, the speech requires different efficient receptive fields (i.e., the time scale that reflects the affective characteristics) for each low-level feature (e.g., MFCC).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Input Pipeline",
      "text": "To illustrate the temporal modeling capacity of our TIM-Net, we use the most commonly-used Mel-Frequency Cepstral Coefficients (MFCCs) features  [11]  as the inputs to TIM-Net.\n\nWe first set the sampling rate to the 22.050 kHz of each corpus and apply framing operation and Hamming window to each speech signal with 50-ms frame length and 12.5-ms shift.\n\nThen, the speech signal undergoes a mel-scale triangular filter bank analysis after performing a 2,048-point fast Fourier transform to each frame. Finally, each frame of the MFCCs is processed by the discrete cosine transformation, where the first 39 coefficients are extracted to obtain the low-frequency envelope and high-frequency details.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal-Aware Bi-Direction Multi-Scale Network",
      "text": "We propose a novel temporal emotional modeling approach called TIM-Net, which learns long-range emotional dependencies from the forward and backward directions and captures multi-scale features at frame-level. Fig.  1  presents the detailed network architecture of TIM-Net. For learning multiscale representations with long-range dependencies, the TIM-Net consists of ùëõ Temporal-Aware Blocks (TABs) in both forward and backward directions with different temporal receptive fields. Next, we detail each component.\n\nTemporal-aware block. We design the TAB to capture dependencies between different frames and automatically select the affective frames, severing as a core unit of TIM-Net. As shown in Fig.  1 , T denotes a TAB, each of which consists of two sub-blocks and a sigmoid function ùúé(‚Ä¢) to learn temporal attention maps A, so as to produce the temporal-aware feature ùë≠ by element-wise production of the input and A.\n\nFor the two identical sub-blocks of the ùëó-th TAB T ùëó , each sub-block starts by adding a DC Conv with the exponentially increasing dilated rate 2 ùëó -1 and causal constraint. The dilated convolution enlarges and refines the receptive field and the causal constraint ensures that the future information is not leaked to the past. The DC Conv is then followed by a batch normalization, a ReLU function, and a spatial dropout.\n\nBi-direction temporal modeling. To integrate complementary information from the past and the future for the judgement of emotion polarity and modeling long-range temporal dependencies, we devise a novel bi-direction architecture based on the multi-scale features as shown in Fig.  1 . For- mally, for the √¨ T ùëó+1 in the forward direction with the input √¨ ùë≠ ùëó from previous TAB, the output √¨ ùë≠ ùëó+1 is given by Eq. (  1 ):\n\nwhere √¨ ùë≠ 0 comes from the output of the first 1 √ó 1 Conv layer and the backward direction can be defined similarly in Eq.  (2) .\n\nWe then combine bidirectional semantic dependencies and compact global contextual representation at utterance level to perceive context as follows:\n\nwhere the global temporal pooling operation G takes an average over temporal dimension, yielding a representation vector for one specific receptive field from the ùëó-th TAB.\n\nMulti-scale dynamic fusion. Furthermore, since the pronunciation habits (e.g., speed or pause time) vary from speaker to speaker, the utterances have the characteristics of temporal scale variation. SER benefits from taking dynamic temporal receptive fields into consideration. We design the dynamic fusion module to adaptively process speech input at different scales, aiming to determine suitable temporal scale for the current input during the training phase. We adopt a weighted summation operation to fuse the features with Dynamic Receptive Fields (DRF) fusion weights ùíò drf from different TABs. The DRF fusion is defined as follows:\n\nwhere ùíò drf = [ùë§ 1 , ùë§ 2 , . . . , ùë§ ùëõ ] T are trainable parameters.\n\nOnce the emotional representation ùíò drf is generated with great discriminability, we can simply use one fully-connected layer with the softmax function for emotion classification.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets. To demonstrate the effectiveness of the proposed TIM-Net, we compare TIM-Net with State-Of-The-Art (SOTA) methods on 6 benchmark SER corpora. CASIA  [20]  is a Chinese corpus collected from 4 Chinese speakers exhibiting 6 emotional states. EMODB  [21]  is a German corpus that covers 7 emotions by 10 German speakers. EMOVO  [22]  is an Italian corpus recorded by 6 Italian speakers simulating 7 emotional states. IEMOCAP  [23]  is an English corpus that covers 4 emotions from 10 American speakers. RAVDESS  [24]  is an English corpus of 8 emotions by 24 British speakers. SAVEE  [25]  is an English corpus recorded by 4 British speakers in 7 emotions. Implementation details. In the experiments, 39-dimensional MFCCs are extracted from the Librosa toolbox  [26] . The cross-entropy criterion is used as the objective function and the overall epoch is set to 500. Adam algorithm is adopted to optimize the model with an initial learning rate ùõº = 0.001, and a batch size of 64. To avoid over-fitting during the training phase, we implement label smoothing with factor 0.1 as a form of regularization. For the ùëó-th TAB T ùëó , there are 39 kernels of size 2 in Conv layers, the dropout rate is 0.1, and the dilated rate is 2 ùëó -1 . To guarantee that the maximal receptive field covers the input sequences, we set the number of TAB ùëõ in both directions to 10 for IEMOCAP and 8 for others. For fair comparisons with the SOTA approaches in experiments, following previous works  [2, 16, 18] , we mainly perform 10fold cross-validation (CV) with 90% training data and 10% testing data in one fold to evaluate fitting ability of the model. To evaluate the generalization ability of the model, we further conduct experiments on six corpora under another evaluation setting. As shown in Table  1 , the superscript ' * ' implies a 10-fold CV with 90% and 10% samples in train and test sets, whose model is only evaluated at the last epoch using the testing set. Evaluation metrics. Due to the class imbalance, we use two widely-used metrics, Weighted Average Recall (WAR) (i.e., accuracy) and Unweighted Average Recall (UAR), to evaluate the performance of each method. WAR uses the class probabilities to balance the recall metric of different classes while UAR treats each class equally.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "Comparison with SOTA methods. To demonstrate the effectiveness of our approach on each corpus, we select representative approaches on each corpus following the 10-fold CV strategy. Table  1  presents the overall results on 6 corpora, showing that our method significantly and consistently outperforms all these compared methods by a large margin. Remarkably, our approach gains 2.34% and 2.61% improvements of the average UAR and WAR scores than the second-best on each corpus under the second evaluating setting. However, most previous methods focus on evaluating the fitting ability of the model, leading to overfitting issues. We further evaluate the generalization ability of the model under another evaluation setting. As shown in Table  1 , although performance has declined, the TIM-Net still has competitive performance and good generalization ability on several corpora. Fig.  2  shows that TIM-Net does not exhibit significant overfitting issues, and its convergence curves remain relatively stable. Moreover, it can be observed that the affective discrimination ability of TIM-Net in short-term speech (e.g., CASIA, EMODB, EMOVO, and RAVDESS) is generally stronger than that in long-term speech (e.g., IEMO-CAP and SAVEE), which means that long-term dependence is still a challenging issue. Please refer to our GitHub repo 1  for extra experimental details and results.  Visualization of learned affective representation. To investigate the impact of TIM-Net on representation learning, we visualize the representations learned by TIM-Net and GM-TCN  [14]  through the t-SNE technique  [27]  in Fig.  3 . For a fair comparison, we first use the same 8:2 hold-out validation on CASIA corpus for the two methods, and visualize the representations of the same test data after an identical training phase. Although GM-TCN also focuses on multi-scale and temporal modeling. The generalization of the model to unseen domain/corpus is critically important for SER. Inspired by the domain-adaptation study in CAAM  [17] , we likewise validate the generalizability of TIM-Net on the cross-corpus SER task, following the same experimental setting as CAAM except that TIM-Net does not have access to the target domain. Specifically, we likewise choose 5 emotional classes for a fair comparison, i.e., angry, fear, happy, neutral, and sad, shared among these 5 corpora (except for IEMOCAP, which has only 4 emotions). These 5 corpora form 20 cross-corpus combinations. And we report the average UAR and WAR, and their standard deviation from 10 random runs for each task in Table  2 .\n\nThe performance of TCN over different corpora is close to random guessing with odds equal to 25%, and TIM-Net has a significant improvement over TCN. Surprisingly, TIM- Net outperforms CAAM, one latest task-specific domainadaptation method. The results suggest that our TIM-Net is effective in modeling emotion with strong generalizability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies on all the corpus datasets, including the following variations of TIM-Net: TCN: the TIM-Net is replaced with TCN; w/o BD: the backward TABs are removed while keeping the forward TABs; w/o MS: the multiscale fusion is removed and ùíà ùëõ is used as ùíà drf corresponding to max-scale receptive field; w/o DF: the average fusion is used to confirm the advantages of dynamic fusion. The results of ablation studies are shown in Table  3 . We have the following observations. First, all components contribute positively to the overall performance. Second, our method achieves 8.31% and 8.41% performance gains in UAR and WAR over TCN that also utilizes DC Conv. Since the inability of TCN to capture contextual multi-scale features, capturing intra-and interdependencies at different temporal scales is critical to SER. Third, when removing the backward TABs or multi-scale strategy, the results substantially drop due to the weaker capacity to model temporal dependencies and perceive the sentimental features with different scales. Finally, TIM-Net without dynamic fusion performs worse than TIM-Net, which verifies the benefits of deploying dynamic fusion to adjust the model adaptively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel temporal emotional modeling approach, termed TIM-Net, to learn multi-scale contextual affective representations from various time scales. TIM-Net can capture long-range temporal dependency through bidirection temporal modeling and fuse multi-scale information dynamically for better adaptation to temporal scale variation. Our experimental results indicate that learning representation from the context information with dynamic temporal scales is crucial for the SER task. The ablation studies, visualizations, and domain generalization analysis further confirm the advantages of TIM-Net. In the future, we will investigate the disentanglement of emotion and speech content through the proposed temporal modeling approach for better generalization in cross-corpus SER tasks.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of the TIM-Net for learning affective features, whose feature extraction part is composed of a bi-direction",
      "page": 2
    },
    {
      "caption": "Figure 1: presents the",
      "page": 2
    },
    {
      "caption": "Figure 1: , T denotes a TAB, each of which consists",
      "page": 2
    },
    {
      "caption": "Figure 2: The accuracy and loss curves for 10-fold cross validation‚àóon the RAVDESS corpus.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows that TIM-Net does not ex-",
      "page": 4
    },
    {
      "caption": "Figure 3: t-SNE visualizations of features learned from SOTA",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) shows heavy overlapping",
      "page": 5
    },
    {
      "caption": "Figure 3: (b) shows that the different representations are clustered",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4School of Film, Xiamen University, Xiamen, China": "5Shanghai Center for Brain Science and Brain-Inspired Technology, Shanghai, China"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "which\nare\nfed\ninto\nconventional machine\nlearning meth-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "ods,\nsuch as Support Vector Machine (SVM). More recent"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "methods\nbased\non\ndeep\nlearning\ntechniques\naim to\nlearn"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "the\nclass-discriminative\nfeatures\nin an end-to-end manner,"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "which employ various\narchitectures\nsuch as Convolutional"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "Neural Network (CNN)\n[4, 5], Recurrent Neural Network"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "(RNN) [6, 7], or the combination of CNN and RNN [8]."
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "In\nparticular,\nvarious\ntemporal modeling\napproaches,"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "such as Long Short-Term Memory (LSTM), Gate Recurrent"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "Unit (GRU), and Temporal Convolution Network (TCN), are"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "widely adopted in SER, aiming to capture dynamic temporal"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "variations of speech signals.\nFor example, Wang et al.\n[7]"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "proposed a dual-level LSTM to harness temporal information"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "from different\ntime-frequency resolutions.\nZhong et al.\n[9]"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "used CNN with Bi-GRU and focal\nloss\nfor\nlearning inte-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "grated spatiotemporal features. Rajamani et al. [6] presented"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "an attention-based ReLU within GRU to capture long-range"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "interactions among the features.\nZhao et al.\n[8]\nleveraged"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "fully CNN and Bi-LSTM to learn the\nspatiotemporal\nfea-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "tures.\nHowever,\nthese methods\nsuffer\nfrom the\nfollowing"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "drawbacks: 1)\nthey lack sufficient capacity to capture long-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "range dependencies for context modeling, where the capture"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "of the context in speech is crucial for SER since human emo-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "tions are usually highly context-dependent; and 2) they do not"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "explore the dynamic receptive field of the model, while learn-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "ing dynamic\ninstead of maximal ones\ncan improve model"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "generalization ability to unknown data or corpus."
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "To\novercome\nthese\nlimitations\nin SER, we\npropose\na"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "Temporal-aware bI-direction Multi-scale Network,\ntermed"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "TIM-Net, which is\na novel\ntemporal\nemotional modeling"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "approach to learn multi-scale contextual affective representa-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "tions from various time scales. The contributions are three-"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "fold.\nFirst, we propose a temporal-aware block based on"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "the Dilated Causal Convolution (DC Conv) as a core unit\nin"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "TIM-Net. The dilated convolution can enlarge and refine the"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "receptive field of temporal patterns. The causal convolution"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": ""
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "combined with dilated convolution can help model relax the"
        },
        {
          "4School of Film, Xiamen University, Xiamen, China": "assumption of first-order Markov property compared with"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DC": "Neutral\nœÉ\nBN\nReLU\nSD"
        },
        {
          "DC": "Happy"
        },
        {
          "DC": "Conv"
        },
        {
          "DC": "Fig. 1. The framework of the TIM-Net for learning affective features, whose feature extraction part is composed of a bi-direction"
        },
        {
          "DC": "(cid:174)"
        },
        {
          "DC": "T ùëó are the same structure with different inputs.\nmodule and a dynamic fusion module. Note that the forward (cid:174)Tùëó and backward"
        },
        {
          "DC": "envelope and high-frequency details.\nRNNs [10].\nIn this way, we can incorporate an ùëÅ-order (ùëÅ"
        },
        {
          "DC": "denotes the number of all previous frames) connection into"
        },
        {
          "DC": "the network to aggregate information from different temporal"
        },
        {
          "DC": "2.2. Temporal-aware Bi-direction Multi-scale Network"
        },
        {
          "DC": "locations. Second, we devise a novel bi-direction architecture"
        },
        {
          "DC": "integrating complementary information from the past and the"
        },
        {
          "DC": "We propose a novel\ntemporal emotional modeling approach"
        },
        {
          "DC": "future for modeling long-range temporal dependencies.\nTo"
        },
        {
          "DC": "called TIM-Net, which learns long-range emotional depen-"
        },
        {
          "DC": "the best of our knowledge, TIM-Net\nis the first bi-direction"
        },
        {
          "DC": "dencies from the forward and backward directions and cap-"
        },
        {
          "DC": "temporal network by focusing on multi-scale fusion in the"
        },
        {
          "DC": "tures multi-scale features at frame-level. Fig. 1 presents the"
        },
        {
          "DC": "SER, rather than simply concatenating forward and backward"
        },
        {
          "DC": "detailed network architecture of TIM-Net. For learning multi-"
        },
        {
          "DC": "hidden states.\nThird, we design a dynamic fusion module"
        },
        {
          "DC": "scale representations with long-range dependencies, the TIM-"
        },
        {
          "DC": "by combining dynamic receptive fields for learning the inter-"
        },
        {
          "DC": "Net consists of ùëõ Temporal-Aware Blocks (TABs) in both for-"
        },
        {
          "DC": "dependencies at different\ntemporal scales,\nso as to improve"
        },
        {
          "DC": "ward and backward directions with different\ntemporal recep-"
        },
        {
          "DC": "the model generalizability. Due to the articulation speed and"
        },
        {
          "DC": "tive fields. Next, we detail each component."
        },
        {
          "DC": "pause time varying significantly across speakers,\nthe speech"
        },
        {
          "DC": "Temporal-aware block.\nWe design the TAB to capture de-"
        },
        {
          "DC": "requires different efficient receptive fields (i.e., the time scale"
        },
        {
          "DC": "pendencies between different frames and automatically select"
        },
        {
          "DC": "that\nreflects the affective characteristics)\nfor each low-level"
        },
        {
          "DC": "the affective frames, severing as a core unit of TIM-Net. As"
        },
        {
          "DC": "feature (e.g., MFCC)."
        },
        {
          "DC": "shown in Fig. 1, T denotes a TAB, each of which consists"
        },
        {
          "DC": "of two sub-blocks and a sigmoid function ùúé(¬∑)\nto learn tem-"
        },
        {
          "DC": "2. PROPOSED METHOD"
        },
        {
          "DC": "poral attention maps A, so as to produce the temporal-aware"
        },
        {
          "DC": "feature ùë≠ by element-wise production of\nthe input and A."
        },
        {
          "DC": "2.1.\nInput Pipeline"
        },
        {
          "DC": "For\nthe two identical\nsub-blocks of\nthe\nùëó-th TAB Tùëó , each"
        },
        {
          "DC": "sub-block starts by adding a DC Conv with the exponentially\nTo illustrate the temporal modeling capacity of our TIM-Net,"
        },
        {
          "DC": "and causal constraint.\nThe di-\nincreasing dilated rate 2 ùëó ‚àí1\nwe use\nthe most\ncommonly-used Mel-Frequency Cepstral"
        },
        {
          "DC": "lated convolution enlarges and refines the receptive field and\nCoefficients (MFCCs) features [11] as the inputs to TIM-Net."
        },
        {
          "DC": "the causal constraint ensures that the future information is not\nWe first set the sampling rate to the 22.050 kHz of each corpus"
        },
        {
          "DC": "leaked to the past. The DC Conv is then followed by a batch\nand apply framing operation and Hamming window to each"
        },
        {
          "DC": "normalization, a ReLU function, and a spatial dropout.\nspeech signal with 50-ms\nframe length and 12.5-ms\nshift."
        },
        {
          "DC": "Bi-direction temporal modeling.\nTo integrate complemen-\nThen,\nthe speech signal undergoes a mel-scale triangular fil-"
        },
        {
          "DC": "ter bank analysis after performing a 2,048-point fast Fourier\ntary information from the past and the future for\nthe judge-"
        },
        {
          "DC": "ment of emotion polarity and modeling long-range tempo-\ntransform to each frame. Finally, each frame of the MFCCs"
        },
        {
          "DC": "ral dependencies, we devise a novel bi-direction architecture\nis processed by the discrete cosine transformation, where the"
        },
        {
          "DC": "based on the multi-scale features as shown in Fig. 1.\nFor-\nfirst 39 coefficients are extracted to obtain the low-frequency"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The overall results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%) / WAR(%).",
      "data": [
        {
          "Table 1. The overall": "",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        },
        {
          "Table 1. The overall": "10-fold cross-validation with 90% and 10% samples in train and test sets respectively, whose model is only evaluated at the last",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        },
        {
          "Table 1. The overall": "epoch. The ‚Äò‚àó‚àó‚Äô implies a 10-fold cross-validation that 90% of samples are used for training and 10% for both validating and",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        },
        {
          "Table 1. The overall": "testing. Note that methods without superscript means that there is no source code to verify their specific experimental details.",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        },
        {
          "Table 1. The overall": "Model",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "CASIA",
          "/ WAR(%).": "EMOVO"
        },
        {
          "Table 1. The overall": "DT-SVM [12]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "85.08 / 85.08",
          "/ WAR(%).": "68.93 / 68.93"
        },
        {
          "Table 1. The overall": "TLFMRF [13]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "85.83 / 85.83",
          "/ WAR(%).": "73.30 / 73.30"
        },
        {
          "Table 1. The overall": "GM-TCN‚àó‚àó [14]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "90.17 / 90.17",
          "/ WAR(%).": "79.08 / 79.08"
        },
        {
          "Table 1. The overall": "CPAC‚àó‚àó [17]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "92.75 / 92.75",
          "/ WAR(%).": "85.40 / 85.40"
        },
        {
          "Table 1. The overall": "TIM-Net‚àó",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "91.08 / 91.08",
          "/ WAR(%).": "86.56 / 86.56"
        },
        {
          "Table 1. The overall": "TIM-Net‚àó‚àó",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "94.67 / 94.67",
          "/ WAR(%).": "92.00 / 92.00"
        },
        {
          "Table 1. The overall": "Model",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "IEMOCAP",
          "/ WAR(%).": "SAVEE"
        },
        {
          "Table 1. The overall": "MHA+DRN [18]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "67.40 /\n-",
          "/ WAR(%).": "-\n/ 82.10"
        },
        {
          "Table 1. The overall": "CNN+Bi-GRU [9]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "71.72 / 70.39",
          "/ WAR(%).": "83.38 / 84.79"
        },
        {
          "Table 1. The overall": "SPU+MSCNN [11]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "68.40 / 66.60",
          "/ WAR(%).": "83.69 / 85.63"
        },
        {
          "Table 1. The overall": "LightSER‚àó‚àó [16]",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "70.76 / 70.23",
          "/ WAR(%).": "83.88 / 86.02"
        },
        {
          "Table 1. The overall": "TIM-Net‚àó",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "69.00 / 68.29",
          "/ WAR(%).": "77.26 / 79.36"
        },
        {
          "Table 1. The overall": "TIM-Net‚àó‚àó",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "72.50 / 71.65",
          "/ WAR(%).": "86.07 / 87.71"
        },
        {
          "Table 1. The overall": "mally, for the",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": "is generated with"
        },
        {
          "Table 1. The overall": "",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        },
        {
          "Table 1. The overall": "",
          "results of different SOTA methods on 6 SER corpora. Evaluation measures are UAR(%)": "",
          "/ WAR(%).": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: presents the overall results on 6",
      "data": [
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "300",
          "1-fold Accuracy": "200",
          "2-fold Loss": "300",
          "2-fold Accuracy": "200"
        },
        {
          "1-fold Loss": "Epoch",
          "1-fold Accuracy": "Epoch",
          "2-fold Loss": "Epoch",
          "2-fold Accuracy": "Epoch"
        },
        {
          "1-fold Loss": "3-fold Loss",
          "1-fold Accuracy": "3-fold Accuracy",
          "2-fold Loss": "4-fold Loss",
          "2-fold Accuracy": "4-fold Accuracy"
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "300",
          "1-fold Accuracy": "200",
          "2-fold Loss": "300",
          "2-fold Accuracy": "200"
        },
        {
          "1-fold Loss": "Epoch",
          "1-fold Accuracy": "Epoch",
          "2-fold Loss": "Epoch",
          "2-fold Accuracy": "Epoch"
        },
        {
          "1-fold Loss": "5-fold Loss",
          "1-fold Accuracy": "5-fold Accuracy",
          "2-fold Loss": "6-fold Loss",
          "2-fold Accuracy": "6-fold Accuracy"
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "300",
          "1-fold Accuracy": "200",
          "2-fold Loss": "300",
          "2-fold Accuracy": "200"
        },
        {
          "1-fold Loss": "Epoch",
          "1-fold Accuracy": "Epoch",
          "2-fold Loss": "Epoch",
          "2-fold Accuracy": "Epoch"
        },
        {
          "1-fold Loss": "7-fold Loss",
          "1-fold Accuracy": "7-fold Accuracy",
          "2-fold Loss": "8-fold Loss",
          "2-fold Accuracy": "8-fold Accuracy"
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "300",
          "1-fold Accuracy": "200",
          "2-fold Loss": "300",
          "2-fold Accuracy": "200"
        },
        {
          "1-fold Loss": "Epoch",
          "1-fold Accuracy": "Epoch",
          "2-fold Loss": "Epoch",
          "2-fold Accuracy": "Epoch"
        },
        {
          "1-fold Loss": "9-fold Loss",
          "1-fold Accuracy": "9-fold Accuracy",
          "2-fold Loss": "10-fold Loss",
          "2-fold Accuracy": "10-fold Accuracy"
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "",
          "1-fold Accuracy": "",
          "2-fold Loss": "",
          "2-fold Accuracy": ""
        },
        {
          "1-fold Loss": "300",
          "1-fold Accuracy": "200",
          "2-fold Loss": "300",
          "2-fold Accuracy": "200"
        },
        {
          "1-fold Loss": "Epoch",
          "1-fold Accuracy": "Epoch",
          "2-fold Loss": "Epoch",
          "2-fold Accuracy": "Epoch"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: presents the overall results on 6",
      "data": [
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "fair comparisons with the SOTA approaches in experiments,",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "corpora,\nshowing that our method significantly and consis-"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "following previous works [2, 16, 18], we mainly perform 10-",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "tently outperforms all\nthese compared methods by a large"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "fold cross-validation (CV) with 90% training data and 10%",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "margin. Remarkably, our approach gains 2.34% and 2.61%"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "testing data in one fold to evaluate fitting ability of the model.",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "improvements of\nthe\naverage UAR and WAR scores\nthan"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "To evaluate the generalization ability of the model, we further",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "the second-best on each corpus under the second evaluating"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "conduct experiments on six corpora under another evaluation",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "setting.\nHowever, most previous methods\nfocus on evalu-"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "setting. As shown in Table 1,\nthe superscript\n‚Äò‚àó‚Äô\nimplies a",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "ating the fitting ability of\nthe model,\nleading to overfitting"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "10-fold CV with 90% and 10% samples in train and test sets,",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "issues. We further evaluate the generalization ability of\nthe"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "whose model\nis only evaluated at\nthe last epoch using the",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "model under another evaluation setting. As shown in Table 1,"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "testing set.",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "although performance has declined,\nthe TIM-Net\nstill has"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "Evaluation metrics.\nDue to the class\nimbalance, we use",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "competitive performance and good generalization ability on"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "two widely-used metrics, Weighted Average Recall\n(WAR)",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "several corpora.\nFig. 2 shows\nthat TIM-Net does not ex-"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "(i.e., accuracy) and Unweighted Average Recall\n(UAR),\nto",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "hibit significant overfitting issues, and its convergence curves"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "evaluate the performance of each method. WAR uses the class",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "remain relatively stable. Moreover,\nit can be observed that"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "probabilities to balance the recall metric of different classes",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "the affective discrimination ability of TIM-Net\nin short-term"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "while UAR treats each class equally.",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "speech (e.g., CASIA, EMODB, EMOVO, and RAVDESS) is"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "generally stronger than that in long-term speech (e.g., IEMO-"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "CAP and SAVEE), which means that\nlong-term dependence"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "3.2. Results and Analysis",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": ""
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "is still a challenging issue. Please refer to our GitHub repo1"
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "Comparison with SOTA methods.",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "for extra experimental details and results."
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "To\ndemonstrate\nthe",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": ""
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "effectiveness\nof\nour\napproach\non\neach\ncorpus, we\nselect",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": ""
        },
        {
          "in both directions to 10 for IEMOCAP and 8 for others. For": "representative approaches on each corpus following the 10-",
          "fold CV strategy.\nTable 1 presents the overall\nresults on 6": "1https://github.com/Jiaxin-Ye/TIM-Net_SER"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: The average performance of ablation studies and",
      "data": [
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "3.3. Ablation Study"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "We conduct ablation studies on all the corpus datasets, includ-"
        },
        {
          "effective in modeling emotion with strong generalizability.": "ing the following variations of TIM-Net: TCN: the TIM-Net"
        },
        {
          "effective in modeling emotion with strong generalizability.": "is replaced with TCN; w/o BD:\nthe backward TABs are re-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "moved while keeping the forward TABs; w/o MS: the multi-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "scale fusion is removed and ùíàùëõ is used as ùíàdrf corresponding"
        },
        {
          "effective in modeling emotion with strong generalizability.": "to max-scale receptive field; w/o DF:\nthe average fusion is"
        },
        {
          "effective in modeling emotion with strong generalizability.": "used to confirm the advantages of dynamic fusion.\nThe re-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "sults of ablation studies are shown in Table 3. We have the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "following observations."
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "First,\nall components contribute positively to the over-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "all performance.\nSecond, our method achieves 8.31% and"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "8.41% performance gains in UAR and WAR over TCN that"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "also utilizes DC Conv. Since the inability of TCN to capture"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "contextual multi-scale\nfeatures,\ncapturing intra-\nand inter-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "dependencies at different\ntemporal scales is critical\nto SER."
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "Third, when removing the backward TABs or multi-scale"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "strategy,\nthe\nresults\nsubstantially\ndrop\ndue\nto\nthe weaker"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "capacity to model\ntemporal dependencies and perceive the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "sentimental features with different scales. Finally, TIM-Net"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "without dynamic fusion performs worse than TIM-Net, which"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "verifies the benefits of deploying dynamic fusion to adjust the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "model adaptively."
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "4. CONCLUSIONS"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "In this paper, we propose a novel temporal emotional model-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "ing approach,\ntermed TIM-Net,\nto learn multi-scale contex-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "tual affective representations from various time scales. TIM-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "Net can capture long-range temporal dependency through bi-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "direction temporal modeling and fuse multi-scale information"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "dynamically for better adaptation to temporal scale variation."
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "Our experimental results indicate that learning representation"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "from the context\ninformation with dynamic temporal scales"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "is crucial\nfor\nthe SER task. The ablation studies, visualiza-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "tions, and domain generalization analysis further confirm the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "advantages of TIM-Net.\nIn the future, we will investigate the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "disentanglement of emotion and speech content\nthrough the"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "proposed temporal modeling approach for better generaliza-"
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        },
        {
          "effective in modeling emotion with strong generalizability.": "tion in cross-corpus SER tasks."
        },
        {
          "effective in modeling emotion with strong generalizability.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Luo, Chang-Li Wu, Li-Yan Chen, and Kunhong Liu,\n‚ÄúGM-"
        },
        {
          "5. REFERENCES": "[1] Bj¬®orn W Schuller, ‚ÄúSpeech emotion recognition: Two decades",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "TCNet: Gated multi-scale temporal convolutional network us-"
        },
        {
          "5. REFERENCES": "Commun.\nin a nutshell, benchmarks,\nand ongoing trends,‚Äù",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "ing emotion causality for speech emotion recognition,‚Äù Speech"
        },
        {
          "5. REFERENCES": "ACM, vol. 61, no. 5, pp. 90‚Äì99, 2018.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Commun., vol. 145, pp. 21‚Äì35, 2022."
        },
        {
          "5. REFERENCES": "[2] T¬®urker Tuncer, Seng¬®ul Dogan, and U. Rajendra Acharya, ‚ÄúAu-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[15]\nJ Ancilin and A Milton, ‚ÄúImproved speech emotion recognition"
        },
        {
          "5. REFERENCES": "tomated accurate\nspeech emotion recognition system using",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "with Mel frequency magnitude coefficient,‚Äù Applied Acoustics,"
        },
        {
          "5. REFERENCES": "twine shuffle pattern and iterative neighborhood component",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "vol. 179, pp. 108046, 2021."
        },
        {
          "5. REFERENCES": "analysis techniques,‚Äù Knowl. Based Syst., vol. 211, pp. 106547,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[16] Arya Aftab, Alireza Morsali, Shahrokh Ghaemmaghami, et al.,"
        },
        {
          "5. REFERENCES": "2021.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "‚ÄúLIGHT-SERNET: A lightweight\nfully convolutional neural"
        },
        {
          "5. REFERENCES": "[3] Mustaqeem and Soonil Kwon,\n‚ÄúOptimal\nfeature\nselection",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "network for speech emotion recognition,‚Äù\nin ICASSP 2022,"
        },
        {
          "5. REFERENCES": "based speech emotion recognition using two-stream deep con-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Virtual and Singapore, 23-27 May 2022. 2022, pp. 6912‚Äì6916,"
        },
        {
          "5. REFERENCES": "volutional neural network,‚Äù\nInt. J. Intell. Syst., vol. 36, no. 9,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "IEEE."
        },
        {
          "5. REFERENCES": "pp. 5116‚Äì5135, 2021.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[17] Xin-Cheng Wen,\nJiaxin Ye, Yan Luo, Yong Xu, Xuan-Ze"
        },
        {
          "5. REFERENCES": "[4]\nIlyas Ozer, ‚ÄúPseudo-colored rate map representation for speech",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Wang, Chang-Li Wu, and Kun-Hong Liu,\n‚ÄúCTL-MTNet: A"
        },
        {
          "5. REFERENCES": "emotion recognition,‚Äù Biomed. Signal Process. Control., vol.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "novel CapsNet and transfer learning-based mixed task net for"
        },
        {
          "5. REFERENCES": "66, pp. 102502, 2021.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "single-corpus and cross-corpus speech emotion recognition,‚Äù"
        },
        {
          "5. REFERENCES": "[5] Xin-Cheng Wen, Kun-Hong Liu, Wei-Ming Zhang, and Kai",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "in IJCAI 2022, Vienna, Austria, 23-29 July 2022, 2022, pp."
        },
        {
          "5. REFERENCES": "Jiang, ‚ÄúThe application of Capsule neural network based CNN",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "2305‚Äì2311."
        },
        {
          "5. REFERENCES": "for\nspeech emotion recognition,‚Äù\nin in ICPR 2020, Virtual",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[18] Runnan Li, Zhiyong Wu, Jia Jia, et al.,\n‚ÄúDilated residual net-"
        },
        {
          "5. REFERENCES": "Event\n/ Milan,\nItaly, January 10-15, 2021. 2020, pp. 9356‚Äì",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "work with multi-head self-attention for speech emotion recog-"
        },
        {
          "5. REFERENCES": "9362, IEEE.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "nition,‚Äù in ICASSP 2019, Brighton, United Kingdom, May 12-"
        },
        {
          "5. REFERENCES": "[6]\nSrividya Tirunellai Rajamani, Kumar T. Rajamani, Adria",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "17, 2019. 2019, pp. 6675‚Äì6679, IEEE."
        },
        {
          "5. REFERENCES": "Mallol-Ragolta, Shuo Liu, and Bj¬®orn W. Schuller,\n‚ÄúA novel",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[19] Misbah\nFarooq,\nFawad Hussain,\nNaveed Khan\nBaloch,"
        },
        {
          "5. REFERENCES": "attention-based gated recurrent unit and its efficacy in speech",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Fawad Riasat Raja, Heejung Yu, and Yousaf Bin Zikria,\n‚ÄúIm-"
        },
        {
          "5. REFERENCES": "emotion recognition,‚Äù in ICASSP 2021, Toronto, ON, Canada,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "pact of feature selection algorithm on speech emotion recogni-"
        },
        {
          "5. REFERENCES": "June 6-11, 2021. 2021, pp. 6294‚Äì6298, IEEE.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "tion using deep convolutional neural network,‚Äù\nSensors, vol."
        },
        {
          "5. REFERENCES": "[7]\nJianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "20, no. 21, pp. 6008, 2020."
        },
        {
          "5. REFERENCES": "Jie Ding,\nand Vahid Tarokh,\n‚ÄúSpeech emotion recognition",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[20]\nJianhua Tao, Fangzhou Liu, Meng Zhang, and Huibin Jia, ‚ÄúDe-"
        },
        {
          "5. REFERENCES": "with dual-sequence LSTM architecture,‚Äù\nin ICASSP 2020,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "sign of speech corpus for Mandarin text\nto speech,‚Äù\nin The"
        },
        {
          "5. REFERENCES": "Barcelona, Spain, May 4-8, 2020. 2020, pp. 6474‚Äì6478, IEEE.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Blizzard Challenge 2008 workshop, 2008."
        },
        {
          "5. REFERENCES": "[8] Ziping Zhao, Yu Zheng, Zixing Zhang, and othersi,\n‚ÄúExplor-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[21]\nFelix Burkhardt, Astrid Paeschke, M. Rolfes,\net\nal.,\n‚ÄúA"
        },
        {
          "5. REFERENCES": "ing spatio-temporal\nrepresentations by integrating attention-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "database of German emotional\nspeech,‚Äù\nin INTERSPEECH"
        },
        {
          "5. REFERENCES": "based Bi-directional-LSTM-RNNs and FCNs for speech emo-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "2005, Lisbon, Portugal, September 4-8, 2005, 2005, vol. 5, pp."
        },
        {
          "5. REFERENCES": "tion recognition,‚Äù\nin Interspeech 2018, Hyderabad, India, 2-6",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "1517‚Äì1520."
        },
        {
          "5. REFERENCES": "September 2018. 2018, pp. 272‚Äì276, ISCA.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[22] Giovanni Costantini,\nIacopo Iaderola, Andrea Paoloni,\nand"
        },
        {
          "5. REFERENCES": "[9] Ying Zhong, Ying Hu, Hao Huang, and Wushour Silamu,\n‚ÄúA",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Massimiliano Todisco, ‚ÄúEMOVO corpus: an Italian emotional"
        },
        {
          "5. REFERENCES": "lightweight model based on separable convolution for speech",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "speech database,‚Äù in LREC 2014, 2014, pp. 3501‚Äì3504."
        },
        {
          "5. REFERENCES": "emotion recognition,‚Äù\nin Interspeech 2020, Virtual Event,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[23] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, et al.,\n‚ÄúIEMO-"
        },
        {
          "5. REFERENCES": "Shanghai, China, 25-29 October 2020. 2020, pp. 3331‚Äì3335,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "CAP:\ninteractive emotional dyadic motion capture database,‚Äù"
        },
        {
          "5. REFERENCES": "ISCA.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Lang. Resour. Evaluation, vol. 42, no. 4, pp. 335‚Äì359, 2008."
        },
        {
          "5. REFERENCES": "[10]\nSteffen Jung,\nIsabel Schlangen, and Alexander Charlish,\n‚ÄúA",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "mnemonic Kalman filter for non-linear systems with extensive",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[24] Steven R Livingstone and Frank A Russo, ‚ÄúThe ryerson audio-"
        },
        {
          "5. REFERENCES": "temporal dependencies,‚Äù IEEE Signal Processing Letters, vol.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "visual database of emotional\nspeech and song (RAVDESS):"
        },
        {
          "5. REFERENCES": "27, pp. 1005‚Äì1009, 2020.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "A dynamic, multimodal\nset of\nfacial and vocal expressions"
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "in north american english,‚Äù\nPLOS ONE, vol. 13, no. 5, pp."
        },
        {
          "5. REFERENCES": "[11] Zixuan Peng, Yu Lu, Shengfeng Pan, and Yunfeng Liu,\n‚ÄúEf-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "e0196391, 2018."
        },
        {
          "5. REFERENCES": "ficient speech emotion recognition using multi-scale CNN and",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "attention,‚Äù in ICASSP 2021, Toronto, ON, Canada, June 6-11,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[25] Philip Jackson and SJUoSG Haq,\n‚ÄúSurrey audio-visual ex-"
        },
        {
          "5. REFERENCES": "2021. 2021, pp. 3020‚Äì3024, IEEE.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "University of Surrey:\npressed emotion (SAVEE) database,‚Äù"
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Guildford, UK, 2014."
        },
        {
          "5. REFERENCES": "[12] Linhui Sun, Sheng Fu, and Fu Wang,\n‚ÄúDecision tree SVM",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "model with Fisher feature selection for speech emotion recog-",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[26] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis,"
        },
        {
          "5. REFERENCES": "nition,‚Äù EURASIP J. Audio Speech Music. Process., vol. 2019,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "Matt McVicar, Eric Battenberg, and Oriol Nieto, ‚Äúlibrosa: Au-"
        },
        {
          "5. REFERENCES": "pp. 2, 2019.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "dio and music signal analysis in python,‚Äù in Proceedings of the"
        },
        {
          "5. REFERENCES": "",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "14th Python in Science Conference, 2015, vol. 8, pp. 18‚Äì25."
        },
        {
          "5. REFERENCES": "[13] Luefeng Chen, Wanjuan Su, Yu Feng, Min Wu,\nJinhua She,",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        },
        {
          "5. REFERENCES": "and Kaoru Hirota, ‚ÄúTwo-layer fuzzy multiple random forest for",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "[27] Laurens Van der Maaten and Geoffrey Hinton,\n‚ÄúVisualizing"
        },
        {
          "5. REFERENCES": "Inf.\nspeech emotion recognition in human-robot\ninteraction,‚Äù",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": "data using t-SNE.,‚Äù J. Mach. Learn. Res., vol. 9, no. 11, 2008."
        },
        {
          "5. REFERENCES": "Sci., vol. 509, pp. 150‚Äì163, 2020.",
          "[14]\nJiaxin Ye, Xin-Cheng Wen, Xuan-Ze Wang, Yong Xu, Yan": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Bj√∂rn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "3",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T√ºrker Tuncer",
        "Seng√ºl Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "4",
      "title": "Optimal feature selection based speech emotion recognition using two-stream deep convolutional neural network",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "Int. J. Intell. Syst"
    },
    {
      "citation_id": "5",
      "title": "Pseudo-colored rate map representation for speech emotion recognition",
      "authors": [
        "Ilyas Ozer"
      ],
      "year": "2021",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "6",
      "title": "The application of Capsule neural network based CNN for speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Kun-Hong Liu",
        "Wei-Ming Zhang",
        "Kai Jiang"
      ],
      "year": "2020",
      "venue": "The application of Capsule neural network based CNN for speech emotion recognition"
    },
    {
      "citation_id": "7",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "Tirunellai Srividya",
        "Rajamani",
        "T Kumar",
        "Adria Rajamani",
        "Shuo Mallol-Ragolta",
        "Bj√∂rn Liu",
        "Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with dual-sequence LSTM architecture",
      "authors": [
        "Jianyou Wang",
        "Michael Xue",
        "Ryan Culhane",
        "Enmao Diao",
        "Jie Ding",
        "Vahid Tarokh"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition with dual-sequence LSTM architecture"
    },
    {
      "citation_id": "9",
      "title": "Exploring spatio-temporal representations by integrating attentionbased Bi-directional-LSTM-RNNs and FCNs for speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Yu Zheng",
        "Zixing Zhang",
        "Othersi"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attentionbased Bi-directional-LSTM-RNNs and FCNs for speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "A lightweight model based on separable convolution for speech emotion recognition",
      "authors": [
        "Ying Zhong",
        "Ying Hu",
        "Hao Huang",
        "Wushour Silamu"
      ],
      "year": "2020",
      "venue": "Interspeech 2020, Virtual Event"
    },
    {
      "citation_id": "11",
      "title": "A mnemonic Kalman filter for non-linear systems with extensive temporal dependencies",
      "authors": [
        "Steffen Jung",
        "Isabel Schlangen",
        "Alexander Charlish"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "12",
      "title": "Efficient speech emotion recognition using multi-scale CNN and attention",
      "authors": [
        "Zixuan Peng",
        "Yu Lu",
        "Shengfeng Pan",
        "Yunfeng Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021"
    },
    {
      "citation_id": "13",
      "title": "Decision tree SVM model with Fisher feature selection for speech emotion recognition",
      "authors": [
        "Linhui Sun",
        "Sheng Fu",
        "Fu Wang"
      ],
      "year": "2019",
      "venue": "EURASIP J. Audio Speech Music. Process"
    },
    {
      "citation_id": "14",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "Luefeng Chen",
        "Wanjuan Su",
        "Yu Feng",
        "Min Wu",
        "Jinhua She",
        "Kaoru Hirota"
      ],
      "year": "2020",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "15",
      "title": "GM-TCNet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Xuan-Ze Wang",
        "Yong Xu",
        "Yan Luo",
        "Chang-Li Wu",
        "Li-Yan Chen",
        "Kunhong Liu"
      ],
      "year": "2022",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "16",
      "title": "Improved speech emotion recognition with Mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "17",
      "title": "LIGHT-SERNET: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "18",
      "title": "CTL-MTNet: A novel CapsNet and transfer learning-based mixed task net for single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Jiaxin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "IJCAI 2022"
    },
    {
      "citation_id": "19",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "Runnan Li",
        "Zhiyong Wu",
        "Jia Jia"
      ],
      "year": "2019",
      "venue": "ICASSP 2019"
    },
    {
      "citation_id": "20",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "Misbah Farooq",
        "Fawad Hussain",
        "Naveed Khan Baloch",
        "Fawad Riasat Raja",
        "Heejung Yu",
        "Yousaf Bin"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Design of speech corpus for Mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge 2008 workshop"
    },
    {
      "citation_id": "22",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes"
      ],
      "year": "2005",
      "venue": "INTERSPEECH 2005"
    },
    {
      "citation_id": "23",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "LREC 2014"
    },
    {
      "citation_id": "24",
      "title": "IEMO-CAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "25",
      "title": "The ryerson audiovisual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "26",
      "title": "Surrey audio-visual expressed emotion (SAVEE) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "27",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Oriol Battenberg",
        "Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "28",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "J. Mach. Learn. Res"
    }
  ]
}