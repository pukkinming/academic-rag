{
  "paper_id": "2501.06215v1",
  "title": "Fitting Different Interactive Information: Joint Classification Of Emotion And Intention",
  "published": "2025-01-05T05:23:27Z",
  "authors": [
    "Xinger Li",
    "Zhiqiang Zhong",
    "Bo Huang",
    "Yang Yang"
  ],
  "keywords": [
    "low-resource",
    "interactive",
    "joint classification",
    "pseudo-label"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper is the first-place solution for ICASSP MEIJU@2025 Track I, which focuses on low-resource multimodal emotion and intention recognition. How to effectively utilize a large amount of unlabeled data, while ensuring the mutual promotion of different difficulty levels tasks in the interaction stage, these two points become the key to the competition. In this paper, pseudo-label labeling is carried out on the model trained with labeled data, and samples with high confidence and their labels are selected to alleviate the problem of low resources. At the same time, the characteristic of easy represented ability of intention recognition found in the experiment is used to make mutually promote with emotion recognition under different attention heads, and higher performance of intention recognition is achieved through fusion. Finally, under the refined processing data, we achieve the score of 0.5532 in the Test set, and win the championship of the track.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "With the continuous development of the multi-modal field, we pay increasingly attention to the complementary information between different modals to achieve more superior task performance  [1]    [8] . In MEIJU25 Challenge@ICASSP2025 competition, Video-based image, text and audio multi-modal emotion and intention recognition tasks also try to use information interaction among different tasks to achieve better classification performance  [2]    [10] . The purpose of Chinese language track I MEIJU25 is to explore the interaction between emotion recognition intention recognition tasks with a few annotated data, so as to improve their respective performance.\n\nOur main contributions of this paper are as follows:\n\n• Experiment find intention recognition is more easily represented by emotional features, and the performance of intention recognition can be improved through the interaction of task information under different self-attention multi-heads. • The use of high confidence pseudo-labels and refined text punctuation improves the performance of each task.\n\n• The global average embedding of each modal is a better representation of the overall emotion and intent features of the video. Finally, we integrate the reasoning results of intent recognition under different multi-head to achieve complementary intent performance. We believe that the interaction between emotion and intent recognition with different multi-head have greatly improved classification accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Method",
      "text": "Figure  1  shows the overall architecture of our solution. Inspired by Liu, Fan et al.  [3] , the final model mainly consists of a multi-modal feature fusion module based on multi-head self-attention and gated mechanism interaction module for the mutual promotion of different difficulty tasks. The details will be explained in subsections D and E.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Data Preprocessing",
      "text": "Visual: Considering the large number of duplicate pictures in the near moment of the video, we sample 1 picture from the video every 30 frames (about 1s) in order to avoid excessive redundancy and noise.\n\nAudio: By using ffmpeg tool, the original video is extracted into audio with sample rate of 16K, and save into WAV format files.\n\nText: Although the organizer providing corresponding audio text, manual inspection revealed some errors. Attempts with models like Qwen2.5-7B were unfruitful. We manually corrected a few words and symbols, such as \"?\", to better classify emotions like Surprise and intents like Questioning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Pseudo-Label Generation",
      "text": "In order to make full use of unlabeled data and generate pseudo-labels to expand the dataset, we use the existing labeled data to train the initial model, and then use the model to predict the unlabeled data and generate pseudolabels. Specifically, we set a confidence threshold of 0.99 to select samples and false labels with high confidence, and arXiv:2501.06215v1 [cs.CV] 5 Jan 2025 adopt reasonable sampling strategies to ensure the balance of emotion categories and intention categories  [6] . Finally, these pseudo-label samples are added to the training set as additional labeled data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Modal Processing",
      "text": "Based on Lian's work and local experiment  [4] , we select CLIP-large, HUBERT-large, and Chinese-Roberta for extracting embeddings from Visual, Audio, and Text modalities, respectively.\n\nFor the Visual modality, TalkNet  [5]  is used to extract the speaker's face from videos, followed by CLIP-large for embedding extraction. For Audio, we sum the last four hidden layers of the HUBERT-large model to obtain embeddings. For Text, embeddings are extracted for each word and punctuation.\n\nTo align modal information and accommodate varying video lengths, we compute the average embedding for each modality as the final representation  [1]  [7]  [9] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "D. Multimodal Fusion Module",
      "text": "We employ the same encoding module for both emotion and intention recognition tasks. For audio and visual embeddings, we use LSTM encoders to extract higher-dimensional features and apply max pooling to generate low-dimensional, compact and context-specific representations. For text embeddings, we use a single-layer TextCNN to achieve similar compactness. Through experiments, we find that these low-dimensional embeddings facilitate task interaction and improve performance. Finally, we fuse embeddings from all three modalities using a single-layer multi-head Transformer, resulting in stronger multi-modal representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Multi-Head Self-Attention, Gated Mechanism Interaction",
      "text": "We utilize a multi-head self-attention mechanism to capture modal information of different dimensions, with fewer heads and more heads for intent tasks to get different embedding representations. During the interaction process, fusional Emo and Int embeddings serve as the Query respectively, while other embeddings act as the Value and Key to compute the initial interaction result A. This result is further refined through a second interaction step, using A as the Value and Key. A gating mechanism then learns the weight of the second interaction feature, which is combined with the Query to produce the final feature. To utilize the more easier representation ability in intention tasks, we use self-attention with one and two heads. Through fusion, we achieve improved performance while keeping the interaction module unchanged.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiment",
      "text": "The experimental results are shown in Table  1 . Through experiments, we observe two key findings: (1) Emotion and intention recognition are interrelated, but achieving optimal accuracy for both tasks is challenging due to differences in task complexity. (2) Intention recognition is simpler represented. Considering these, we employ multi-head self-attention and modal fusion while adding Gaussian noise to embeddings for enhanced robustness. We adopt a two-stage training strategy: first, we train the model on clean data, and then we fine-tune it using pseudo-labeled data. This approach improves alignment, enriches modal representations, and enhances overall task performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "There is a corresponding relationship between emotion recognition and intention recognition, but the learning rate of different tasks is different, which makes it difficult to achieve the best accuracy of each task at the same time. Considering that intention recognition is a task easier to represent, by allowing self-attention of emotion recognition and intention recognition under different heads to interact, intention recognition pays attention to different information, so as to achieve better results than that under a single head. On the refined data set, the final model fuses different head",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the overall architecture of our solution.",
      "page": 1
    },
    {
      "caption": "Figure 1: The overall architecture of our solution.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "huangbo@njust.edu.cn": "Abstract—This paper\nis\nthe first-place\nsolution for\nICASSP",
          "yyang@njust.edu.cn": "• The global average embedding of each modal\nis a better"
        },
        {
          "huangbo@njust.edu.cn": "MEIJU@2025 Track I, which focuses on low-resource multimodal",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "representation of\nthe overall emotion and intent\nfeatures"
        },
        {
          "huangbo@njust.edu.cn": "emotion\nand\nintention\nrecognition. How to\neffectively\nutilize",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "of\nthe video."
        },
        {
          "huangbo@njust.edu.cn": "a\nlarge\namount\nof unlabeled data, while\nensuring\nthe mutual",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "promotion of different difficulty levels\ntasks\nin the\ninteraction",
          "yyang@njust.edu.cn": "Finally, we integrate the reasoning results of\nintent\nrecog-"
        },
        {
          "huangbo@njust.edu.cn": "stage,\nthese\ntwo points become\nthe key to the\ncompetition.\nIn",
          "yyang@njust.edu.cn": "nition under different multi-head to achieve\ncomplementary"
        },
        {
          "huangbo@njust.edu.cn": "this paper, pseudo-label\nlabeling\nis\ncarried out\non the model",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "intent performance. We believe\nthat\nthe\ninteraction between"
        },
        {
          "huangbo@njust.edu.cn": "trained with\nlabeled\ndata,\nand\nsamples with\nhigh\nconfidence",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "emotion and intent recognition with different multi-head have"
        },
        {
          "huangbo@njust.edu.cn": "and their\nlabels\nare\nselected\nto\nalleviate\nthe\nproblem of\nlow",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "greatly improved classification accuracy."
        },
        {
          "huangbo@njust.edu.cn": "resources. At the same time, the characteristic of easy represented",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "ability of intention recognition found in the experiment is used to",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "II. METHOD"
        },
        {
          "huangbo@njust.edu.cn": "make mutually promote with emotion recognition under different",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "attention heads, and higher performance of intention recognition",
          "yyang@njust.edu.cn": "Figure\n1\nshows\nthe\noverall\narchitecture\nof\nour\nsolution."
        },
        {
          "huangbo@njust.edu.cn": "is achieved through fusion. Finally, under the refined processing",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "Inspired by Liu, Fan et al. [3],\nthe final model mainly consists"
        },
        {
          "huangbo@njust.edu.cn": "data, we achieve the score of 0.5532 in the Test set, and win the",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "of a multi-modal\nfeature fusion module based on multi-head"
        },
        {
          "huangbo@njust.edu.cn": "championship of\nthe track.",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "self-attention and gated mechanism interaction module for the"
        },
        {
          "huangbo@njust.edu.cn": "Index\nTerms—low-resource,\ninteractive,\njoint\nclassification,",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "pseudo-label.",
          "yyang@njust.edu.cn": "mutual promotion of different difficulty tasks. The details will"
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "be explained in subsections D and E."
        },
        {
          "huangbo@njust.edu.cn": "I.\nINTRODUCTION",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "A. Data preprocessing"
        },
        {
          "huangbo@njust.edu.cn": "With the continuous development of\nthe multi-modal field,",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "Visual: Considering the large number of duplicate pictures"
        },
        {
          "huangbo@njust.edu.cn": "we pay increasingly attention to the complementary informa-",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "in the near moment of the video, we sample 1 picture from the"
        },
        {
          "huangbo@njust.edu.cn": "tion between different modals\nto achieve more superior\ntask",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "video every 30 frames (about 1s)\nin order\nto avoid excessive"
        },
        {
          "huangbo@njust.edu.cn": "performance\n[1]\n[8].\nIn MEIJU25 Challenge@ICASSP2025",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "redundancy and noise."
        },
        {
          "huangbo@njust.edu.cn": "competition, Video-based image,\ntext and audio multi-modal",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "Audio: By using ffmpeg tool,\nthe original video is extracted"
        },
        {
          "huangbo@njust.edu.cn": "emotion and intention recognition tasks\nalso try to use\nin-",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "into audio with sample rate of 16K, and save into WAV format"
        },
        {
          "huangbo@njust.edu.cn": "formation interaction among different\ntasks\nto achieve better",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "files."
        },
        {
          "huangbo@njust.edu.cn": "classification performance [2]\n[10]. The purpose of Chinese",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "Text: Although the organizer providing corresponding audio"
        },
        {
          "huangbo@njust.edu.cn": "language\ntrack I MEIJU25 is\nto explore\nthe\ninteraction be-",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "text, manual\ninspection revealed some errors. Attempts with"
        },
        {
          "huangbo@njust.edu.cn": "tween emotion recognition intention recognition tasks with a",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "models\nlike Qwen2.5-7B were unfruitful. We manually cor-"
        },
        {
          "huangbo@njust.edu.cn": "few annotated data,\nso as\nto improve their\nrespective perfor-",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "rected a few words and symbols, such as ”?”, to better classify"
        },
        {
          "huangbo@njust.edu.cn": "mance.",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "emotions like Surprise and intents like Questioning."
        },
        {
          "huangbo@njust.edu.cn": "Our main contributions of\nthis paper are as follows:",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "",
          "yyang@njust.edu.cn": "B. Pseudo-label generation"
        },
        {
          "huangbo@njust.edu.cn": "• Experiment\nfind\nintention\nrecognition\nis more\neasily",
          "yyang@njust.edu.cn": ""
        },
        {
          "huangbo@njust.edu.cn": "represented by emotional\nfeatures, and the performance",
          "yyang@njust.edu.cn": "In order\nto make full use of unlabeled data and generate"
        },
        {
          "huangbo@njust.edu.cn": "of intention recognition can be improved through the in-",
          "yyang@njust.edu.cn": "pseudo-labels\nto\nexpand\nthe\ndataset, we\nuse\nthe\nexisting"
        },
        {
          "huangbo@njust.edu.cn": "teraction of task information under different self-attention",
          "yyang@njust.edu.cn": "labeled\ndata\nto\ntrain\nthe\ninitial model,\nand\nthen\nuse\nthe"
        },
        {
          "huangbo@njust.edu.cn": "multi-heads.",
          "yyang@njust.edu.cn": "model\nto\npredict\nthe\nunlabeled\ndata\nand\ngenerate\npseudo-"
        },
        {
          "huangbo@njust.edu.cn": "• The use of high confidence pseudo-labels and refined text",
          "yyang@njust.edu.cn": "labels. Specifically, we\nset\na\nconfidence\nthreshold\nof\n0.99"
        },
        {
          "huangbo@njust.edu.cn": "punctuation improves the performance of each task.",
          "yyang@njust.edu.cn": "to select\nsamples and false labels with high confidence, and"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "THE EXPERIMENTAL RESULTS.NOTE THAT THE SCORES CORRESPONDING"
        },
        {
          "TABLE I": "TO THE FIFTH ROW ARE ALL INTENT SCORES."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "emotion score/intent score"
        },
        {
          "TABLE I": "0.5265"
        },
        {
          "TABLE I": "0.5305"
        },
        {
          "TABLE I": "0.5725"
        },
        {
          "TABLE I": "0.5134"
        },
        {
          "TABLE I": "0.5114"
        },
        {
          "TABLE I": "0.5725"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nThe overall architecture of our solution.": "adopt\nreasonable sampling strategies to ensure the balance of\ngating mechanism then learns the weight of\nthe second inter-"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "emotion categories and intention categories [6]. Finally,\nthese\naction feature, which is combined with the Query to produce"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "pseudo-label samples are added to the training set as additional\nthe final\nfeature. To\nutilize\nthe more\neasier\nrepresentation"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "labeled data.\nability in intention tasks, we use self-attention with one and"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "two heads. Through fusion, we achieve improved performance"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "C. Modal processing"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "while keeping the interaction module unchanged."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "Based on Lian’s work and local experiment\n[4], we select"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "III. EXPERIMENT"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "CLIP-large, HUBERT-large, and Chinese-Roberta for extract-"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "The\nexperimental\nresults\nare\nshown in Table 1. Through"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "ing\nembeddings\nfrom Visual, Audio,\nand Text modalities,"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "experiments, we observe two key findings:\n(1) Emotion and"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "respectively."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "intention recognition are\ninterrelated, but\nachieving optimal"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "For\nthe Visual modality, TalkNet\n[5]\nis\nused\nto\nextract"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "accuracy for both tasks is challenging due to differences in task"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "the\nspeaker’s\nface\nfrom videos,\nfollowed by CLIP-large\nfor"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "complexity.\n(2)\nIntention recognition is\nsimpler\nrepresented."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "embedding extraction. For Audio, we sum the last four hidden"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "Considering these, we\nemploy multi-head self-attention and"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "layers of the HUBERT-large model\nto obtain embeddings. For"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "modal\nfusion while adding Gaussian noise to embeddings for"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "Text, embeddings are extracted for each word and punctuation."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "enhanced robustness. We adopt a two-stage training strategy:"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "To align modal information and accommodate varying video"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "first, we train the model on clean data, and then we fine-tune it"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "lengths, we compute the average embedding for each modality"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "using pseudo-labeled data. This approach improves alignment,"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "as the final\nrepresentation [1]\n[7]\n[9]."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "enriches modal\nrepresentations,\nand\nenhances\noverall\ntask"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "performance.\nD. Multimodal\nfusion module"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "We employ the same encoding module for both emotion and"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "TABLE I"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "intention recognition tasks. For audio and visual embeddings,"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "THE EXPERIMENTAL RESULTS.NOTE THAT THE SCORES CORRESPONDING"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "we use LSTM encoders to extract higher-dimensional features\nTO THE FIFTH ROW ARE ALL INTENT SCORES."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "and apply max pooling to generate low-dimensional, compact"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "and context-specific representations. For\ntext embeddings, we\nmethod\nemotion score/intent score\nintent score\noverall score"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "baseline\n0.5265\n0.5051\n0.5156"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "use a single-layer TextCNN to achieve similar compactness.\n+ pesudo label\n0.5305\n0.5114\n0.5208"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "+ pesudo label + 1textcnn\n0.5725\n0.4831\n0.5240\nThrough experiments, we find that\nthese low-dimensional em-"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "+ one attention head\n0.5134\n0.5132\n0.5133"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "+ one attention head + two attention head\n0.5114\n0.5132\n0.5353\nbeddings facilitate task interaction and improve performance."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "ensemble\n0.5725\n0.5353\n0.5532"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "Finally, we fuse embeddings\nfrom all\nthree modalities using"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "a\nsingle-layer multi-head Transformer,\nresulting in stronger"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "IV. CONCLUSION"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "multi-modal\nrepresentations."
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "There\nis\na\ncorresponding\nrelationship\nbetween\nemotion"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "E. Multi-head self-attention, gated mechanism interaction"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "recognition\nand\nintention\nrecognition,\nbut\nthe\nlearning\nrate"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "of\ndifferent\ntasks\nis\ndifferent, which makes\nit\ndifficult\nto\nWe utilize a multi-head self-attention mechanism to capture"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "achieve\nthe\nbest\naccuracy\nof\neach\ntask\nat\nthe\nsame\ntime.\nmodal\ninformation of different dimensions, with fewer heads"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "Considering\nthat\nintention\nrecognition\nis\na\ntask\neasier\nto\nand more heads\nfor\nintent\ntasks\nto get different\nembedding"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "represent, by allowing self-attention of\nemotion recognition\nrepresentations. During the interaction process,\nfusional Emo"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "and\nintention\nrecognition\nunder\ndifferent\nheads\nto\ninteract,\nand Int\nembeddings\nserve\nas\nthe Query respectively, while"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "intention recognition pays attention to different\ninformation,\nother embeddings act as\nthe Value and Key to compute the"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "so as\nto achieve better\nresults\nthan that under a single head.\ninitial interaction result A. This result is further refined through"
        },
        {
          "Fig. 1.\nThe overall architecture of our solution.": "On the refined data set,\nthe final model\nfuses different head\na second interaction step, using A as\nthe Value and Key. A"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the online test set, achieving the optimal performance of\nthe": "overall\ntask."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "REFERENCES"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[1] Yang Y, Pan H,\nJiang Q Y, et al. Learning to Rebalance Multi-Modal"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "Optimization\nby Adaptively Masking Subnetworks[J].\narXiv\npreprint"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "arXiv:2404.08347, 2024."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[2]\nLiu R, Zuo H, Lian Z, et al. Emotion and Intent Joint Understanding in"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "Multimodal Conversation: A Benchmarking Dataset[J]. arXiv preprint"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "arXiv:2407.02751, 2024."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[3]\nFan Q, Li Y, Xin Y,\net\nal. Leveraging contrastive\nlearning and self-"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "training for multimodal emotion recognition with limited labeled sam-"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "ples[C]//Proceedings of\nthe 2nd International Workshop on Multimodal"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "and Responsible Affective Computing. 2024: 72-77."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[4]\nLian\nZ,\nSun\nL,\nRen\nY\n,\net\nal. Merbench:\nA\nunified\nevaluation"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "benchmark\nfor multimodal\nemotion\nrecognition[J].\narXiv\npreprint"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "arXiv:2401.03429, 2024."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[5] Beliaev\nS,\nRebryk\nY\n,\nGinsburg\nB.\nTalkNet:\nFully-convolutional"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "non-autoregressive\nspeech\nsynthesis\nmodel[J].\narXiv\npreprint"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "arXiv:2005.05514, 2020."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[6]\nJiang Q Y, Chi Z, Yang Y. Multimodal Classification via Modal-Aware"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "Interactive Enhancement[J]. arXiv preprint arXiv:2407.04587, 2024."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[7] Yang Y, Guo J, Li G, et al. Alignment efficient\nimage-sentence retrieval"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "considering transferable\ncross-modal\nrepresentation learning[J]. Fron-"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "tiers of Computer Science, 2024, 18(1): 181335."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[8] Yang Y, Zhan D C, Wu Y F, et al. Semi-supervised multi-modal cluster-"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "ing and classification with incomplete modalities[J].\nIEEE Transactions"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "on Knowledge and Data Engineering, 2019, 33(2): 682-695."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[9] Yang Y, Wan F, Jiang Q Y, et al. Facilitating Multimodal Classification"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "via Dynamically Learning Modality Gap[C]//The Thirty-eighth Annual"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "Conference on Neural\nInformation Processing Systems. 2024."
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "[10] Yang Y, Zhang J, Gao F, et al. DOMFN: A divergence-orientated multi-"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "modal\nfusion\nnetwork\nfor\nresume\nassessment[C]//Proceedings\nof\nthe"
        },
        {
          "the online test set, achieving the optimal performance of\nthe": "30th ACM International Conference on Multimedia. 2022: 1612-1620."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks",
      "authors": [
        "Y Yang",
        "H Pan",
        "Q Y Jiang"
      ],
      "year": "2024",
      "venue": "Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks",
      "arxiv": "arXiv:2404.08347"
    },
    {
      "citation_id": "2",
      "title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian"
      ],
      "year": "2024",
      "venue": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "3",
      "title": "Leveraging contrastive learning and selftraining for multimodal emotion recognition with limited labeled samples",
      "authors": [
        "Q Fan",
        "Y Li",
        "Y Xin"
      ],
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "5",
      "title": "TalkNet: Fully-convolutional non-autoregressive speech synthesis model",
      "authors": [
        "S Beliaev",
        "Y Rebryk",
        "B Ginsburg"
      ],
      "year": "2020",
      "venue": "TalkNet: Fully-convolutional non-autoregressive speech synthesis model",
      "arxiv": "arXiv:2005.05514"
    },
    {
      "citation_id": "6",
      "title": "Multimodal Classification via Modal-Aware Interactive Enhancement",
      "authors": [
        "Q Jiang",
        "Chi Yang"
      ],
      "year": "2024",
      "venue": "Multimodal Classification via Modal-Aware Interactive Enhancement",
      "arxiv": "arXiv:2407.04587"
    },
    {
      "citation_id": "7",
      "title": "Alignment efficient image-sentence retrieval considering transferable cross-modal representation learning[J]. Frontiers of Computer Science",
      "authors": [
        "Y Yang",
        "J Guo",
        "G Li"
      ],
      "year": "2024",
      "venue": "Alignment efficient image-sentence retrieval considering transferable cross-modal representation learning[J]. Frontiers of Computer Science"
    },
    {
      "citation_id": "8",
      "title": "Semi-supervised multi-modal clustering and classification with incomplete modalities[J]",
      "authors": [
        "Y Yang",
        "D Zhan",
        "Y F Wu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "9",
      "title": "Facilitating Multimodal Classification via Dynamically Learning Modality Gap",
      "authors": [
        "Y Yang",
        "F Wan",
        "Q Y Jiang"
      ],
      "year": "2024",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "DOMFN: A divergence-orientated multimodal fusion network for resume assessment",
      "authors": [
        "Y Yang",
        "J Zhang",
        "F Gao"
      ],
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    }
  ]
}