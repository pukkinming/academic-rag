{
  "paper_id": "2112.09596v2",
  "title": "Linguistic And Gender Variation In Speech Emotion Recognition Using Spectral Features",
  "published": "2021-12-17T16:14:51Z",
  "authors": [
    "Zachary Dair",
    "Ryan Donovan",
    "Ruairi O'Reilly"
  ],
  "keywords": [
    "Affective Computing",
    "Speech Emotion Recognition",
    "Machine Learning",
    "Prosody Analysis",
    "Convolutional Neural Networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work explores the effect of gender and linguistic-based vocal variations on the accuracy of emotive expression classification. Emotive expressions are considered from the perspective of spectral features in speech (Mel-frequency Cepstral Coefficient, Melspectrogram, Spectral Contrast). Emotions are considered from the perspective of Basic Emotion Theory. A convolutional neural network is utilised to classify emotive expressions in emotive audio datasets in English, German, and Italian. Vocal variations for spectral features assessed by (i) a comparative analysis identifying suitable spectral features, (ii) the classification performance for mono, multi and cross-lingual emotive data and (iii) an empirical evaluation of a machine learning model to assess the effects of gender and linguistic variation on classification accuracy. The results showed that spectral features provide a potential avenue for increasing emotive expression classification. Additionally, the accuracy of emotive expression classification was high within mono and cross-lingual emotive data, but poor in multi-lingual data. Similarly, there were differences in classification accuracy between gender populations. These results demonstrate the importance of accounting for population differences to enable accurate speech emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is the classification of the emotional states of a speaker from speech. These emotional states can either be discrete experiences, such as the basic emotions (Anger, Disgust, Fear, Joy, Sadness, and Surprise), or attributes of emotional states (Arousal, Valence, Dominance). The ability to accurately classify these emotional states through SER enables the provision of tailored services that can adapt to the psychological needs of user groups. SER attempts to classify these emotional states via verbal and non-verbal components of speech. Verbal components of speech are the specific words a speaker says. Verbal SER involves classifying emotional expressions from the word choice or associations, syntax, use of colloquialisms or sarcasm concerning the intent of the speaker  [13] . Non-verbal components of speech refer to how the speaker expresses their speech. Non-verbal SER involves classifying emotional expressions from the speaker's acoustic features (Pitch, Tone, Timbre, Number of pauses, Loudness, Speech Rate) while speaking  [14] .\n\nResearch has shown that non-verbal components provide valuable information, distinct from verbal content, in accurately classifying emotional expressions. For example, non-verbal features are capable of indicating emotional signals when the verbal content of the speech is neutral  [29]    [25] . Many non-verbal features have been shown to exist across multiple languages, enabling generalisable analysis  [25]    [27] . Additionally, non-verbal features have been shown to have a direct impact on emotion recognition processes in the brain, indicating they are vital to achieving accurate emotion classification  [28] . Overall, the larger amount of features available for non-verbal SER, in comparison to verbal SER, enables more accurate emotion classification.\n\nTraditional non-verbal SER 1 approaches recruit human observers, who are asked to classify emotions from non-verbal content collected from one or several speakers. While traditional approaches are useful for relatively small-scale datasets, as they demonstrate high levels of accuracy  [26]  [27]  [4] , they are impractical for dealing with large and continuously growing datasets that characterise modern human-computer interactions. In an attempt to scale the benefits of SER, research and industrial applications have been developed to conduct SER automatically. These applications range from open-source research tools (for example, Audeering) to commercial tools (for example, Good Vibrations, Vokaturi, and deep affect API )  [11]  [9], all of which employ machine learning-based approaches. If SER can be accurately automated, it would facilitate increased efficacy in human-computer interactions, user modelling, and personalisation services.\n\nDespite the potential benefits of automated SER, it has not been widely implemented in everyday life settings  [1] . This is despite reviews of the SER literature demonstrating that automated SER approaches perform similarly or even better than traditional approaches  [1] . One reason for the limited uptake of automated SER approaches is that they are not as generalisable and adaptable as human classifiers, who can use contextual information (for example, the gender of the speaker and the language of the speech) when classifying speech. These two factors, gender and language, have been shown to significantly affect the accuracy of automated SER approaches  [17]    [23] .\n\nSER models that account for gender differences are more accurate than those that do not  [31] . It is difficult, however, to accurately model for these differences. While there exists many features of speech that are significantly influenced by gender, there exists considerable overlap between males and females on these same features. This overlap makes it difficult to distinguish between the gender and linguistic features of emotions that reliably indicate emotions and those that do not. Concerning language, SER models trained on one language significantly decrease in performance when tested on another language. This contrasts to human observers in traditional SER, who are capable of detecting emotional expressions in speech in languages they do not speak  [4]  [26]  [27] .\n\nOne potential mechanism for improving the accuracy of SER across different gender and linguistic populations are spectral features. Spectral features of audio are computed by converting a time-based signal into the frequency domain. These features represent the distribution of energy across a frequency and the harmonic components of sound. Components such as pitch changes in the audio signal, can assist in gender differentiation, and voice quality, can be identified through voice level as either tense, harsh, breathy, or lax  [12] . Aggregating spectral features' continuous, qualitative, and spectral data aids differentiation between genders and linguistic populations thereby enabling more generalisable and accurate emotive expression classifications.\n\nThis study assesses the effect of spectral features on the performance of automatic SER across different gender and linguistic populations by (i) conducting a comparative analysis of spectral features to identify a suitable feature set, (ii) comparing classification performance of SER approaches when using monolingual, multi-lingual, and cross-lingual emotive data, and (iii) empirically evaluating the performance of a machine learning (ML) model to assess the effect of gender and linguistic variations on SER classification accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Previous research has examined the differences between gender populations in acoustic features and their effect on SER performance. Several acoustic features have been described as \"gender-dependent\". Gender dependent in this case means that gender significantly influences the expression of that feature  [30] . Pitch is an example of a gender-dependent feature. Females score higher, on average, than males on pitch  [16] . This information, however, is not sufficient to enable generalisable SER classification, because there is considerable overlap between males and females on this feature  [16] . This is also the case for several other acoustic features described as \"gender-dependent\".\n\nSimilarly, previous research has examined the impact of language in developing accurate SER models. Human observers can accurately classify emotions in their native language, and non-native languages  [4] . This result has been replicated across various cultures, including indigenous tribes, demonstrating the generalisability of traditional SER  [26]    [27] . Similarly, automated SER models achieve a high level of accuracy in mono-lingual settings, where the models are trained and evaluated in the same language. The performance of these models, however, significantly drop when they are tested across languages  [10]    [21] .\n\nIf ML models can differentiate between acoustic features that indicate emotional expressions for each gender and linguistic population reliably, then one should expect generalisability of SER performance between both populations  [31] . To achieve this, spectral features that reliably indicate emotional experiences are first required.\n\nA survey  [8]  analysed features across 17 distinctive datasets. These datasets comprise 9 languages, both male and female speakers, professional and nonprofessional acting sessions, recorded call centre conversations and speech recorded under simulated and naturally occurring stress situations.\n\nA pool of acoustic features and SER were analysed in  [8]  and  [1] . Which can be divided into several categories (Continuous, Qualitative, Spectral and Nonlinear Teager Energy Operated (TEO). The results of both reviews indicated that Continuous and Spectral features were related to emotional expressions in speech. A weaker relationship was found between emotional expressions and Qualitative and TEO features.\n\nTo automatically classify relevant spectral features, a ML methodology is proposed in  [15] . The methodology uses a convolutional neural network (CNN) and SER analysis. The approach consists of using spectral features such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-scaled spectrogram, Chromagram, Spectral contrast and Tonnetz representation extracted from emotive audio belonging to three distinct datasets. This work, however, focused on a combined gender classification of emotive expression, inhibits the analysis of vocal variations between gender. This study extends this work to by analysing the effect of gender and linguistic populations on the emotive expression classification of spectral features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "To assess the effect of spectral features for emotive expression classification across different gender and linguistic populations a series of experiments were conducted. Which required the identification of a suitable set of gender and/or language-dependent spectral features. The classification was enabled through the usage of the CNN described in 3.5, the performance of this CNN will be compared across experiments that account for linguistic and gender differences and experiments that do not account for these differences. The gender populations are male and female and the linguistic populations are English, German, and Italian.\n\nDiscrete emotional models identify several distinct emotions to be classified. This study uses Basic Emotion Theory (BET), as the discrete emotional model for SER. In BET the distinct emotions are Anger, Disgust, Fear, Joy, Sadness, and Surprise. These six emotions are considered basic as (i) they consistently correlate with psychological, behavioural, and neurophysiological activity  [7] , which makes their objective measurement possible, and (ii) they interact to form more cognitively and culturally mediated emotions, such as shame or guilt  [7] . The BET contrasts from dimensional models of emotions that focus on attributes of emotions (for example, arousal, valence). BET models have been extensively used in SER research as they capture a wider range of emotions and are intuitive to label in comparison to dimensional models  [1] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotive Speech Data",
      "text": "Three distinct emotive audio datasets were used (see Table  1 ) to enable a comparative analysis of variations between the data. These variations can originate from many factors such as the gathering method, emotions exhibited, language, speaker gender, or sampling rate.\n\nRAVDESS contains emotional speech data constituting statements and songs in English. For the proposed work, only speech samples are considered. Participants uttered two neutral statements across multiple trials. For each trial, participants were asked to utter the statement in a manner that conveyed one of the six basic emotions. The statements were controlled to ensure equal levels of syllables, word frequency, and familiarity to the speaker. EMO-DB contains emotional speech data constituting statements in German. Participants uttered ten statements, comprised of everyday language and syntactic structure, in various lengths to simulate natural speech. Each utterance was evaluated with regards to recognisability and naturalness of emotions exhibited. The emotive reaction of surprise is not considered.\n\nEMOVO contains emotional speech data constituting statements in Italian. The participants uttered fourteen distinct semantically neutral statements. Each conveys a basic emotion and is spoken naturally. An important consideration in the creation of this dataset was the presence of all phonemes of the Italian language and a balanced presence of voiced and unvoiced consonants.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Feature Selection",
      "text": "In order to identify spectral features indicative of emotive expression, feature selection is performed. Which identifies information such as pitch, energy, voice levels and energy distribution for classification. Spectral features were extracted, using the audio analysis library Librosa  [19] .\n\nThese features are as follows: Mel-Frequency Cepstral Coefficients (MFCC) which represents the shape of a signal spectrum, and is achieved by collectively representing a Mel-frequency cepstrum per frame. Chroma Energy Normalized (CENS) which represents statistics indicative of normalised values quantifying tempo, articulation, and pitch deviations. Zero-Crossing Rate (ZCR) which represents the rate of change in the audio signal from positive to negative or negative to positive, through zero. Chromagram which represents a transformed signal spectrum built using the chromatic scale capturing the harmonic and melodic characteristics. Melspectrogram which represents a spectrogram where the frequency is converted from a linear scale to a Mel-scale, resulting in a spectro-gram reflective of how humans perceive frequencies, capturing the amplitude of the signal. Spectral Contrast which represents the energy contrast computed by comparing the peak energy and valley energy in each band converted from spectrogram frames. Tonnetz which represents the tonal features in a 6-dimensional format, capturing traditional harmonic relationships. (perfect fifth, minor third and major third).\n\nIn order to identify a set of spectral features that captures sufficient emotive data, a classifier is trained on these features individually before being trained on permutations of the features across the datasets to identify the best performing feature set.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Linguistic Variation",
      "text": "To evaluate the accuracy of a SER model in relation to linguistic variation three experiments are conducted across three languages (English, German, Italian) as follows:\n\n1. Mono-Lingual -The CNN is used to classify spectral features indicative of emotive expression from each language independently. The results will be used as a baseline performance of the model's classification performance in a single language. 2. Multi-Lingual -This experiment consists of three permutations. For each, the CNN is first trained on one of the chosen languages and then evaluated for SER accuracy against the remaining languages. The intent is to identify spectral features capacity to classify emotive expression independent of language. 3. Cross-Lingual -The CNN is trained on an aggregation of all training data. This will provide insights into the performance of a CNN model when its training corpus contains multiple languages. The intent is to identify the generalisability of the chosen spectral features for emotive expression classification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Gender Emotion",
      "text": "To assess the effect of gender populations on SER, an experiment was conducted using the spectral features identified in Section. 3.2, comprising of two steps. In step one, the data is classified using the six basic emotions. This initial classification provided a baseline accuracy where gender is not specified. In step two, the same dataset is classified with gender-emotion labels (for example, Male-Anger/Female-Anger; Male-Joy/Female-Joy). The intent is to evaluate if the CNN can identify gender-dependent acoustic elements from the spectral features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification Of Emotive Speech",
      "text": "The architecture of the CNN used for classification is depicted in Figure  1 . A CNN was re-implemented, due to the high emotive expression classification performance using spectral features as exhibited in  [6] . Optimal hyper-parameters were identified from a comparative analysis against related approaches. The model is trained on the extracted spectral features over 150 epochs using a batch size of 16 and 5-Fold validation is performed. During each epoch a portion of data is used to evaluate the model. The model performance is evaluated across the metrics precision, recall, F1-score for each of the six basic emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "Feature extraction -The comparative analysis of the spectral features in isolation provides insights into the performance on a per feature basis. MFCC, Melspectrogram and Spectral Contrast were the highest performing individual spectral features across the datasets. When combined these features formed a vector of 155 data-points, and the highest performing permutation of spectral features as denoted in Table . 2 Linguistic variation -The results for emotive analysis in mono, multi and cross-lingual data are denoted in Table . 3, these highlight the considerations for SER across multiple languages. Mono-lingual and Cross-lingual approaches achieve high accuracies. A degradation in performance is experienced from multilingual approaches.\n\nCross-Gender Emotion -Table . 4 identifies discrepancies in the emotive classification performance, across both gender and language. This indicates a degree of vocal variance stemming from the population differences, highlighting considerations for gender-specific SER approaches.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Feature extraction -There are several notable results from the feature extraction stage that are worth discussion. Firstly, the results of the comparative analysis showed that the MFCC feature enabled the highest emotion recognition accuracy within each dataset. Demonstrating the importance of modelling for phonetic properties, found within the speech signal shape, in enabling accurate emotion classification across languages. Secondly, the performance of the Melspectrogram feature varied across the datasets. Melspectrogram performed highly on EMO-DB and EMOVO and poorly on RAVDESS. This inconsistency was caused by the significantly lower amplitude within the audio data of RAVDESS compared to EMO-DB and EMOVO denoted in Table . 2. Participants in the RAVDESS dataset were given instructions to exhibit emotions with varying intensity normal and strong, additionally, post-processing procedures are likely the cause for the lower level of amplitude. This has significant consequence for classification when amplitude is utilised as a measure of emotive expression. Thirdly, Spectral Contrast in isolation provides greater average accuracy than the remaining individual features. Therefore, the contrast between peak and valley energy is partially indicative of emotion in emotive speech. The high performance of these features in isolation indicates their potential suitability for emotive expression classification. Finally, the results showed that combining these three features increased accuracies for each data set. This demonstrates the importance of including such spectral features in SER classifications.\n\nLinguistic variation -SER accuracy decreased significantly between the different types of analysis. The high performance on mono-lingual analysis indicates the importance of the selected spectral features for each language. The poor performance in multi-lingual analysis, however, indicates a lack of universality between spectral representations of emotions across the datasets. For example, there was significant variation in the amplitude and pitch range across the languages, as identified in  [20] . Additionally, these differences likely stemmed from differences in data collection (equipment, volume, actor-microphone distance) across the datasets, thereby decreasing the performance of the CNN in multi and cross-lingual analysis. This highlights the need for standardised recording Cross-Gender Emotion -There were substantial discrepancies in spectral representations between the gender populations. As a result, SER performance between the genders differed significantly across the six basic emotions. These differences were varied across languages, indicating that there is a generalisable effect of gender on SER performance when using spectral features, however this is impacted by linguistic variation. For example, high energy emotions (such as Anger and Joy) were more clearly detected in German-speaking males than in other linguistic or gender groups. The German language is characterised by higher amplitude, energy and harsher speech when articulated within males  [20] . This likely contributed to higher accuracy of detecting those emotions from German-speaking males. Disgust and Fear, in contrast, were more accurately classified from the speech of females across each language. This may have resulted from differences in vocals, particularly pitch, between the gender groups  [22] . Additionally, female voices tend to articulate in a softer manner  [24]  conducive to representing softer, lower amplitude emotions.\n\nThese results likely contributed to the weaker performance of combined gender classification in comparison to gender-specific classifications of emotion. Combined approaches can account for variations between genders and languages, however, in certain cases, a specific gender may act as a limiting factor reducing the overall accuracy.\n\nLimitations -The major limitations of this work concern sampling issues. Firstly, the sample size across the three datasets is small (N = 40). Since sample sizes can exaggerate the variances between populations, then the small sample size in this study may have exaggerated real differences in spectral features between gender and linguistic populations. Secondly, the datasets were comparing linguistic populations across an unequal number of speakers. Different sample sizes will have higher or lower ranges of variability. Comparing different sample sizes makes it difficult to determine whether the results stem from real betweengroup differences or are the result of a higher level of \"noisy\" data found in small sample sizes. These limitations damage the generalisability of the work. In a follow-up study, the sample size of the overall dataset will be increased and the number of speakers per linguistic group will be controlled.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This work was an exploratory analysis of linguistic and gender variation on the emotive classification of spectral features. The results showed that features using the Mel-Scale and representations of amplitude and energy are important in accurate SER across different gender and linguistic populations. Additionally, the results showed that higher energy emotions such as Anger, Joy, Surprise were easier to identify originating from male voices in a high amplitude, harsh language such as German. Similarly lower energy emotions such as Disgust and Fear were easier to identify from female voices in each language. These observations highlight the importance of signal amplitude and energy when analysing emotion across gender and language.\n\nThe performance of emotive expression classification across mono, multi and cross-lingual data provides insights into linguistic variation of emotive audio. Mono-lingual approaches are suitable as baselines in comparison to multi and cross-lingual approaches. A linguistic variance between emotive expression represented by spectral features can be identified. To overcome these variances, it is recommended that cross-linguistic approaches, that combine languages for training, are implemented.\n\nFuture work should explore whether classification accuracy is affected by (i) model optimization in terms of structure and/or (ii) comprehensive hyperparameter tuning (epochs, batch size, optimizers) using a grid search technique. Additionally, the relationships between linguistics, gender and emotion should be explored from a vocal, psychological and technical classification perspective, with a larger sample size to garner further insights into improving SER generalisability.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of the proposed approach, detailing the CNN layer structure.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ) to enable a com-",
      "page": 4
    },
    {
      "caption": "Table 1: Details of the emotive expression datasets utilised.",
      "page": 5
    },
    {
      "caption": "Table 2: The classiﬁers F1 scores for individual spectral features and the top 4 combi-",
      "page": 8
    },
    {
      "caption": "Table 4: The classiﬁers Precision (Prec.), Recall (Rec.) and F1 scores (F1) per BET",
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2019.12.001"
    },
    {
      "citation_id": "2",
      "title": "Improved speech emotion recognition with Mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "A Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics",
      "doi": "/10.1016/j.apacoust.2021.108046"
    },
    {
      "citation_id": "3",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "4",
      "title": "The voice conveys emotion in ten globalized cultures and one remote village in Bhutan",
      "authors": [
        "D Cordaro",
        "D Keltner",
        "S Tshering",
        "D Wangchuk",
        "L Flynn"
      ],
      "year": "2016",
      "venue": "Emotion"
    },
    {
      "citation_id": "5",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iadarola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "EMOVO corpus: an Italian emotional speech database"
    },
    {
      "citation_id": "6",
      "title": "Classification of emotive expression using verbal and non verbal components of speech",
      "authors": [
        "Z Dair",
        "R Donovan",
        "R O'reilly"
      ],
      "year": "2021",
      "venue": "Classification of emotive expression using verbal and non verbal components of speech",
      "doi": "10.1109/ISSC52156.2021.9467869"
    },
    {
      "citation_id": "7",
      "title": "Emotions revealed",
      "authors": [
        "P Ekman"
      ],
      "year": "2004",
      "venue": "Bmj"
    },
    {
      "citation_id": "8",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recongnition",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "9",
      "title": "Real-time robust recognition of speakers' emotions and characteristics on mobile platforms",
      "authors": [
        "F Eyben",
        "B Huber",
        "E Marchi",
        "D Schuller",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "ACII"
    },
    {
      "citation_id": "10",
      "title": "Cross-language acoustic emotion recognition: An overview and some tendencies",
      "authors": [
        "S Feraru",
        "D Schuller"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "11",
      "title": "Emotion detection: a technology review",
      "authors": [
        "J Garcia-Garcia",
        "V Penichet",
        "M Lozano"
      ],
      "year": "2017",
      "venue": "Proceedings of the XVIII international conference on human computer interaction"
    },
    {
      "citation_id": "12",
      "title": "The role of voice quality in communicating emotion, mood and attitude",
      "authors": [
        "C Gobl",
        "A Chasaide"
      ],
      "year": "2003",
      "venue": "Speech Communication",
      "doi": "10.1016/S0167-6393(02)00082-1"
    },
    {
      "citation_id": "13",
      "title": "Semantic Analysis: A Practical Introduction",
      "authors": [
        "C Goddard"
      ],
      "year": "2011",
      "venue": "Semantic Analysis: A Practical Introduction"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition considering nonverbal vocalization in affective conversations",
      "authors": [
        "J Hsu",
        "M Su",
        "C Wu",
        "Y Chen"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Fatih Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "BSPC",
      "doi": "10.1016/j.bspc.2020.101894"
    },
    {
      "citation_id": "16",
      "title": "Discriminating male and female voices: differentiating pitch and gender",
      "authors": [
        "M Latinus",
        "M Taylor"
      ],
      "year": "2012",
      "venue": "Brain topography"
    },
    {
      "citation_id": "17",
      "title": "Gender differences in the recognition of vocal emotions",
      "authors": [
        "A Lausen",
        "A Schacht"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "18",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS)",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "19",
      "title": "Oriol Nieto: Librosa: Audio and Music Signal Analysis in Python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Battenberg"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference",
      "doi": "10.25080/Majora-7b98e3ed-003"
    },
    {
      "citation_id": "20",
      "title": "Pitching it differently : a comparison of the pitch ranges of German and English speakers",
      "authors": [
        "I Mennen",
        "F Schaeffler",
        "G Docherty"
      ],
      "year": "2007",
      "venue": "Pitching it differently : a comparison of the pitch ranges of German and English speakers"
    },
    {
      "citation_id": "21",
      "title": "Cross-lingual and multilingual speech emotion recognition on English and French",
      "authors": [
        "M Neumann"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Voice, speech and gender:: Male-female acoustic differences and cross-language variation in English and French speakers",
      "authors": [
        "E Pépiot"
      ],
      "year": "2015",
      "venue": "corela",
      "doi": "10.4000/corela.3783"
    },
    {
      "citation_id": "23",
      "title": "Influences of languages in speech emotion recognition: A comparative study using malay, english and mandarin languages",
      "authors": [
        "R Rajoo",
        "C Aun"
      ],
      "year": "2016",
      "venue": "ISCAIE"
    },
    {
      "citation_id": "24",
      "title": "Women faculty at work in the classroom, or, why it still hurts to be a woman in labor",
      "authors": [
        "B Sandier"
      ],
      "year": "1991",
      "venue": "Communication Education",
      "doi": "10.1080/03634529109378821"
    },
    {
      "citation_id": "25",
      "title": "Perceptual cues in nonverbal vocal expressions of emotion",
      "authors": [
        "D Sauter",
        "F Eisner",
        "A Calder",
        "S Scott"
      ],
      "year": "2010",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "26",
      "title": "Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations",
      "authors": [
        "D Sauter",
        "F Eisner",
        "P Ekman",
        "S Scott"
      ],
      "year": "2010",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "27",
      "title": "Emotional vocalizations are recognized across cultures regardless of the valence of distractors",
      "authors": [
        "D Sauter",
        "F Eisner",
        "P Ekman",
        "S Scott"
      ],
      "year": "2015",
      "venue": "Psychological science"
    },
    {
      "citation_id": "28",
      "title": "Temporal signatures of processing voiceness and emotion in sound",
      "authors": [
        "A Schirmer",
        "T Gunter"
      ],
      "year": "2017",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "29",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "30",
      "title": "Improving automatic emotion recognition from speech via gender differentiaion",
      "authors": [
        "T Vogt",
        "E André"
      ],
      "year": "2006",
      "venue": "Improving automatic emotion recognition from speech via gender differentiaion"
    },
    {
      "citation_id": "31",
      "title": "On the effects of speaker gender in emotion recognition training data",
      "authors": [
        "Z Xu",
        "P Meyer",
        "T Fingscheidt"
      ],
      "year": "2018",
      "venue": "Speech Communication; 13th ITG-Symposium"
    }
  ]
}