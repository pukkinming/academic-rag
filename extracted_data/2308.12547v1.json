{
  "paper_id": "2308.12547v1",
  "title": "Hybrid Models For Facial Emotion Recognition In Children",
  "published": "2023-08-24T04:20:20Z",
  "authors": [
    "Rafael Zimmer",
    "Marcos Sobral",
    "Helio Azevedo"
  ],
  "keywords": [
    "Neural Networks",
    "Computer Vision",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper focuses on the use of emotion recognition techniques to assist psychologists in performing children¬¥s therapy through remotely robot operated sessions. In the field of psychology, the use of agent-mediated therapy is growing increasingly given recent advances in robotics and computer science. Specifically, the use of Embodied Conversational Agents (ECA) as an intermediary tool can help professionals connect with children who face social challenges such as Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD) or even who are physically unavailable due to being in regions of armed conflict, natural disasters, or other circumstances. In this context, emotion recognition represents an important feedback for the psychotherapist. In this article, we initially present the result of a bibliographical research associated with emotion recognition in children. This research revealed an initial overview on algorithms and datasets widely used by the community. Then, based on the analysis carried out on the results of the bibliographical research, we used the technique of dense optical flow features to improve the ability of identifying emotions in children in uncontrolled environments. From the output of a hybrid model of Convolutional Neural Network, two intermediary features are fused before being processed by a final classifier. The proposed architecture was called HybridCNNFusion. Finally, we present the initial results achieved in the recognition of children's emotions using a dataset of Brazilian children.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Human cognitive development goes through several stages from birth to maturity. Childhood represents the phase where one acquires the basis of learning to relate with others and with the world  [27] . Unfortunately, the mental development process of a child can be hampered by mental disorders such as anxiety, stress, obsessivecompulsive behavior or emotional, sexual or physical abuse  [38] . The solution or reduction of consequences for these afflictions is achieved with therapeutic processes carried out by professionals in the field of psychology. Due to limited child maturity, the process involves not only assessment sessions with the child, but also interviews with parents and educators, observation of the child in the residential and school environments and data collection through drawings, compositions, games and other activities  [4] ,  [39] .\n\nIn this process, leisure resources such as: games, theater activities, puppets, toys and others gain special prominence and are used as support in therapy  [7] . As a way to contribute to this approach, Embodied Conversational Agents (ECA) are used as a tool in psycho-therapeutic applications. Provoost et al.  [28]  performed a scoping review on the use of ECAs in psychology. After selection, the search revealed 49 references associated with the following mental disorders: autism, depression, anxiety disorder, post-traumatic stress disorder, psychotic disorder and substance use. According to the authors, \"ECA applications are very interesting and show promising results, but their complex nature makes it difficult to prove that they are effective and safe for use in clinical practice\". Actually, the strategy suggested by Provoost et al. involves increasing the evidence base through interventions using low-technology agents that are rapidly developed, tested, and applied in responsible clinical practice.\n\nThe recognition of emotions during psycho-therapeutic sessions can act as an aid to the psychology professional involved in the process, with a still big room for improvement considering the depth of the task at hand  [2] .\n\nThe objective of this work is to discuss and comment on the use of images generated by cameras in uncontrolled children's psychotherapy sessions to classify their emotional state at any given moment in one of the following basic emotion categories: anger, disgust, fear, happiness, sadness, surprise, contempt  [9] . Given the diversity of Machine Learning algorithms for emotion recognition tasks overall, correctly addressing our objective is much more complex than simply choosing the most powerful or recent algorithm  [25] . For applications in psychology, compared to other humancentered tasks, the solution has to be almost fail-proof and be able to function in real uncontrolled scenarios, which is in itself extremely challenging and therefore raises multiple ethical and morally debatable questions about the viability of such models  [16] . In this context, it is important to study and consider the environments for which a specific algorithm will be used for even before beginning to develop or train it  [24] .\n\nIn Section 2, we briefly discuss the performed bibliographical research on the state of the art for emotion recognition in children.\n\nThe training datasets, as well as the implemented model architecture and produced code are presented in Sections 3.1 and 3.2 respectively. Results obtained using the suggested model and conclusions are discussed in Sections 4 and 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Bibliographical Research",
      "text": "A bibliographical research was performed to determine the State of The Art (SOTA) for emotion recognition tasks (FER) in children using computer algorithms. The search was made using the \"Web of Science\" repository  [37] , covering the last 5 years, with the following search key: child* AND emotion AND (recognition OR detection) AND (algorithm OR \"machine learning\" OR \"computer vision\") An initial number of 152 references were selected, with a total of 42 accepted for in-depth reading (39 from the original search, and 3 additional references). A further reading analysis was done, by tagging each paper according to a select number of categories, including, but not limited to: datasets used; age of the patients; psychological procedure adopted; data format (such as video, photos or scans); algorithm category (deep learning techniques, pure machine learning, etc). The detailed result of this categorization can be seen in the spreadsheet available in the Google Drive  [40] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Types Of Algorithms And Datasets",
      "text": "In Fig.  1  we present the main datasets identified during the bibliographical research. The FER-2013 dataset  [30]  is one of the most used by researchers with 9 references. We can mention the works by Sreedharan et al.  [32]  which makes use of this dataset for training FER model using a novel optimisation technique (Grey Wolf optimisation), for instance. Overall, we found out that Facial Emotion Recognition (FER) algorithms have had significant improvement in recent years  [21] , driven by the success of deep learning-based approaches. In Fig.  2  we present the most frequently used algorithms for emotion recognition. The convolutional neural network architecture (DL-CNN) was the most used, with 22 references. As DL-CNN examples, we can cite the works of Haque and Valles  [12]  and Cuadrado et al.  [4] . Both of these propose an architecture for a Deep Convolutional Neural Network for a specific FER task, namely for robot tutors in primary schools and identifying emotions in children with autism spectrum disorder.\n\nWith the demand for high-performance algorithms, numerous novel models, such as the DeepFace system  [34]  or the Transformer architecture for sequential features  [35]  have also made great steps in improving the overall accuracy and time efficiency for emotion classification models.\n\nAmong the most popular paradigms currently used for FER, Convolutional Neural Networks (CNNs) have demonstrated high performance in detecting and recognizing emotion features from facial expressions in images  [16]  by applying moving filters over an image, also called convolution kernels. These models use hierarchical feature extraction techniques to construct region-based information from facial images, which are then used for classification. One of the first wide-spread CNN-based models that have been used for FER is the VGG-16 network, which uses 16 convolution layers and 3 fully connected layers to classify emotions  [31] . In addition to CNNs, other models such as Recurrent Neural Networks (RNNs) or a combination of both have also been proposed for FER.\n\nOverall, FER is an active area of research, and there is ongoing work to improve the accuracy and robustness of existing solutions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Classic Emotion Capture Strategies",
      "text": "In Fig.  3  we present the origin of the still emotion pictures present in the datasets. We can observe that 48.8% of the studies used \"Posed\" emotions, such that the emotions expressed are artificial, and their enactment requested by an evaluator. As an example of works that use \"Posed\" emotions we mention Sreedharan et al.  [32]  which uses the CK+ dataset of posed emotions, and Kalantarian et. al  [19] , in which children with Autism Spectrum Disorder (ASD) are requested to imitate the emotions shown by prompts in a mobile game.\n\nThe \"Induced\" group of emotions contributes to 23.3% of the found papers, for which we can mention the work of Goulart et al.  [11] , where children emotions are induced by interaction with a robot tutor and recorded. Differently from posed emotions, which are obtained by explicitly requesting participants to imitate the facial expression, induced emotions are implicitly obtained by showing the participants emotion inducing scenes, such as videos, photographs and texts.\n\nThe \"Spontaneous\" group appears in only 16.3% of the studies, possibly due to the difficulty in capturing emotions in-the-wild (ITW), such as the dataset discussed in Kahou et. al  [18] , that is, when the individual is not aware of the purpose or the existence of ongoing video recording or photography. It is important to note that facial expressions do not completely correlate to what the individual is feeling, as is the case with posed facial expressions, but is generally used as an acceptable indicator for emotion, even when used in combination with other indicators  [1] ,  [23] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hybrid Architectures",
      "text": "Models that combine multiple networks into one architecture, called hybrid models, are becoming increasingly accurate, particularly those that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs)  [22]  for facial emotion recognition (FER) tasks.\n\nIn addition, recent research has shown that the integration of recurrent layers, such as the long-short term memory layer (LSTM layer)  [14] , which processes inputs recursively, making them particularly useful for capturing the temporal dynamics of facial expressions and inserting these layers into hybrid models can further improve their performance  [15] .\n\nAnother promising research line for improving FER accuracy is the use of multiple features such as audio and processed images in addition to facial color (RGB) images  [3] . These additional features can provide complementary information that can improve the robustness and accuracy of the FER system.\n\nHowever, there are still challenges that need to be addressed, such as how to effectively fuse multiple features and how to effectively train such time-consuming models. Anyhow, hybrid models with transformers or LSTMs classifiers, as well as multiple features are a promising direction for improving the state of the art in FER  [15] ,  [29] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets Used For Training And Prediction",
      "text": "Considering the need for an architecture that can provide adequate accuracy and real-time response for predicting emotions in children in uncontrolled environments, we create the architecture HybridC-NNFusion to process the real-time sequence of frames.\n\nTo accomplish the task in hand, we planned on training our model on the two datasets publicly available with the highest accuracy for FER task in children  [2] . The datasets used are the FER-2013  [30]  and the Karolinska Directed Emotional Faces  [5] . Most datasets for FER tasks are aimed towards adults and with posed expressions, therefore we decided to use the ChildEFES  [26] , a private video dataset of Brazilian children posing emotions for fine-tunning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hybridcnnfusion Architecture Model",
      "text": "In Fig.  4  we present the elements that make up the HybridCNNFusion architecture. The first step in building our architecture was to allow the model to be used in in-the-wild scenarios, by implementing a Haarcascade  [36]  region detection algorithm to center and crop the children faces. These cropped images are then passed to a Convolutional Neural Network (CNN), specifically the InceptionNet  [33] , to process the cropped RGB pixels generated by the Haarcascade algorithm. In parallel, we use the Gunner Farneback's algorithm  [10]  to retrieve the dense optical flow values from the current and previous cropped frames. This is made to allow the network to process the variation in facial muscles and skin movement over time. The optical flow matrices are then passed to a second CNN, specifically a variation of the ResNet  [13] .\n\nAfter calculating these two separate features, they are concatenated and used as input for a final recurrent block, specifically made with layers of LSTM cells to generate the concatenated intermediary output. This takes advantage of the sequential nature of the video frames to output a final vector of predicted probabilities for each emotion. The aforementioned model uses a technique called Late Fusion  [15] , in which two separate features are concatenated inside the architecture and used as input for the final output layers.\n\nThe Late fusion technique allows for a better usage of the motion generated by separate facial Action Units  [8]  by having to distinctly trained CNNs, one for raw RGB values (outputed by the Inception-Net) and another for dense HSV motion matrices (outputed by the ResNet + OpticalFlow combination). The use of the optical flow features as input for the ResNet allows for processing sequential information, specifically, that of motion, through clever manipulation of the raw RGB values.\n\nThe step by step used for a single video classification iteration is presented in the Algorithm 1 section below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ethical Aspects And Considerations Of The Solution",
      "text": "The task of facial emotion recognition (FER) in children is particularly challenging due to the ethical issues and the need for a high level of precision and interpretability.\n\nMost existing FER approaches focus on non-ethically critical situations, such as customer satisfaction or in controlled lab conditions  [25] . On the other hand, the task of FER in emotionally vulnerable children requires a much greater level of trustworthiness in accordance to the ethical constraints of the psychologist-patient relationship  [6] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Algorithm 1 Hybridcnnfusion Pseudo Algorithm",
      "text": "Input: ùëÅ √ó 1920 √ó 1080 RGB frames (√¨ ùë• ùëñ ) and a one-hot-vector for the emotion label throughout the video (ùëí). Output: ùê∏ ùëó for each 10 second window of the frames.\n\nStep-by-step: for √¨ ùëí ùëñ , ùëñ = 0 : ùëÄ do Cut each vector √¨ ùëí ùëñ using the Haarcascade cropping algorithm to center the faces, to ùëõ √ó ùëõ sized images.\n\nApply the Gunner Farneback's algorithm to the cropped √¨ ùëê ùëñ frames.\n\nGroup the cropped and optical flow features into groups of 30 frames.\n\nBatch input them into two separate CNNs, respectively: CNNFlow = InceptionNet  (3, 8)  and CNNRaw = ResNet34  (3, 8) . end for for ùëîùëüùëúùë¢ùëù ùëó , 0 : ùëÅ /30 do Concatenate the cropped and optical flow features.\n\nInput the concatenated vectors into a 3-layer LSTM and generate a sequence of predictions based on the previous emotion probabilities. Append group emotion label to a sequence of labels for the entire video. end for This specific research branch of FER tasks demands the ability to accurately detect and interpret facial expressions in real-time videos of children in in-the-wild (ITW) situations, all the while ensuring the confidence of the information being generated  [20] ,  [24] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The final model implementation had memory limitations that compromised the deployment of the HybridCNNFusion architecture. Despite this limitation, the final model was trained on both the FER2013 and KDEF datasets and fine tuned on ChildEFES dataset to maximize accuracy. The entire model could not be entirely fitted through our private dataset, so we measured partial accuracy for the intermediary models. The InceptionNet had an accuracy of about 70%, while the ResNet had an accuracy of about 72%. Overall, the model had a speed averaging 2.5s for a single iteration, for videos averaging 10s of duration.\n\nThe input images are cropped to the required size of both networks. The output consists of a stochastic vector of probabilities predicting one of 7 possible base emotions, as well as a neutral emotion, totaling 8 possible labels  [9] . Both intermediary CNNs have an output vector of size 32, and the concatenated feature is a vector with 64 entries. The final output layer has a size of (ùëÅ /30) √ó 8, with ùëÅ /30 equal to the total duration of the video divided in groups of 30 frames, each group with a separate predicted emotion label.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "Considering the technological aspects and the initial results obtained, the architecture proposed is a continuous push towards identifying children emotions in in-the-wild conditions, altough not yet fit for real-world usage.\n\nThe fusion of dense optical flow features in conjunction with a hybrid CNN and a recurrent model represents a promising approach in the challenging task of facial emotion recognition (FER) in children, specifically in uncontrolled environments. Being a critical need in the field of psychology, this approach offers a potential solution.\n\nFor ethically sensible situations, there are still important metrics that have to be calculated, such as the Area under the ROC Curve (AOC), which can indicate whether the model is prone to miss important emotion predictions within small and specific frames, also called micro-expressions  [17] .\n\nIn fact, there is a large gap on current ethical questions for the task, but we believe that improving the interpretability of the architecture, explainability and security of transmission of the processed information should be the focus of future models and frameworks instead of just the overall accuracy. This will ensure that the technology can be used safely and effectively to support the emotional well-being of children.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: we present the main datasets identified during the biblio-",
      "page": 2
    },
    {
      "caption": "Figure 1: Datasets used for training.",
      "page": 2
    },
    {
      "caption": "Figure 2: we present the most frequently used algorithms for emotion recog-",
      "page": 2
    },
    {
      "caption": "Figure 2: Algorithms for emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 3: we present the origin of the still emotion pictures present in",
      "page": 2
    },
    {
      "caption": "Figure 3: Emotion capture strategies.",
      "page": 3
    },
    {
      "caption": "Figure 4: we present the elements that make up the HybridCNNFusion",
      "page": 3
    },
    {
      "caption": "Figure 4: HybridCNNFusion architecture. The full implemen-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Brazil\nBrazil": "rafael.zimmer@usp.br\nmarcos.lima2@estudante.ifto.edu.br",
          "Technology Center": "Brazil"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "hazevedo.cti@gmail.com"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "in psycho-therapeutic applications. Provoost et al. [28] performed"
        },
        {
          "Brazil\nBrazil": "ABSTRACT",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "a scoping review on the use of ECAs in psychology. After selection,"
        },
        {
          "Brazil\nBrazil": "This paper focuses on the use of emotion recognition techniques",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "the search revealed 49 references associated with the following men-"
        },
        {
          "Brazil\nBrazil": "to assist psychologists in performing children¬¥s therapy through",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "tal disorders: autism, depression, anxiety disorder, post-traumatic"
        },
        {
          "Brazil\nBrazil": "remotely robot operated sessions. In the field of psychology, the",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "stress disorder, psychotic disorder and substance use. According"
        },
        {
          "Brazil\nBrazil": "use of agent-mediated therapy is growing increasingly given recent",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "to the authors, \"ECA applications are very interesting and show"
        },
        {
          "Brazil\nBrazil": "advances in robotics and computer science. Specifically, the use of",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "promising results, but their complex nature makes it difficult to"
        },
        {
          "Brazil\nBrazil": "Embodied Conversational Agents (ECA) as an intermediary tool can",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "prove that they are effective and safe for use in clinical practice\"."
        },
        {
          "Brazil\nBrazil": "help professionals connect with children who face social challenges",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "Actually, the strategy suggested by Provoost et al. involves increas-"
        },
        {
          "Brazil\nBrazil": "such as Attention Deficit Hyperactivity Disorder (ADHD), Autism",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "ing the evidence base through interventions using low-technology"
        },
        {
          "Brazil\nBrazil": "Spectrum Disorder (ASD) or even who are physically unavailable",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "agents that are rapidly developed, tested, and applied in responsible"
        },
        {
          "Brazil\nBrazil": "due to being in regions of armed conflict, natural disasters, or",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "clinical practice."
        },
        {
          "Brazil\nBrazil": "other circumstances. In this context, emotion recognition represents",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "The recognition of emotions during psycho-therapeutic sessions"
        },
        {
          "Brazil\nBrazil": "an important feedback for the psychotherapist. In this article, we",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "can act as an aid to the psychology professional\ninvolved in the"
        },
        {
          "Brazil\nBrazil": "initially present the result of a bibliographical research associated",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "process, with a still big room for improvement considering the"
        },
        {
          "Brazil\nBrazil": "with emotion recognition in children. This research revealed an",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "depth of the task at hand [2]."
        },
        {
          "Brazil\nBrazil": "initial overview on algorithms and datasets widely used by the",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "The objective of this work is to discuss and comment on the"
        },
        {
          "Brazil\nBrazil": "community. Then, based on the analysis carried out on the results",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "use of images generated by cameras in uncontrolled children‚Äôs psy-"
        },
        {
          "Brazil\nBrazil": "of\nthe bibliographical research, we used the technique of dense",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "chotherapy sessions to classify their emotional state at any given"
        },
        {
          "Brazil\nBrazil": "optical flow features to improve the ability of identifying emotions",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "moment in one of the following basic emotion categories: anger,"
        },
        {
          "Brazil\nBrazil": "in children in uncontrolled environments. From the output of a",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "disgust, fear, happiness, sadness, surprise, contempt [9]. Given the"
        },
        {
          "Brazil\nBrazil": "hybrid model of Convolutional Neural Network, two intermediary",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "diversity of Machine Learning algorithms for emotion recognition"
        },
        {
          "Brazil\nBrazil": "features are fused before being processed by a final classifier. The",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "tasks overall, correctly addressing our objective is much more com-"
        },
        {
          "Brazil\nBrazil": "proposed architecture was called HybridCNNFusion. Finally, we",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "plex than simply choosing the most powerful or recent algorithm"
        },
        {
          "Brazil\nBrazil": "present the initial results achieved in the recognition of children‚Äôs",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "[25]. For applications in psychology, compared to other human-"
        },
        {
          "Brazil\nBrazil": "emotions using a dataset of Brazilian children.",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "centered tasks, the solution has to be almost fail-proof and be able to"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "function in real uncontrolled scenarios, which is in itself extremely"
        },
        {
          "Brazil\nBrazil": "KEYWORDS",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "challenging and therefore raises multiple ethical and morally de-"
        },
        {
          "Brazil\nBrazil": "Neural Networks, Computer Vision, Emotion Recognition.",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "batable questions about the viability of such models [16]. In this"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "context, it is important to study and consider the environments for"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "which a specific algorithm will be used for even before beginning"
        },
        {
          "Brazil\nBrazil": "1\nINTRODUCTION",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "to develop or train it [24]."
        },
        {
          "Brazil\nBrazil": "Human cognitive development goes through several stages from",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "In Section 2, we briefly discuss the performed bibliographical"
        },
        {
          "Brazil\nBrazil": "birth to maturity. Childhood represents the phase where one ac-",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "research on the state of the art for emotion recognition in children."
        },
        {
          "Brazil\nBrazil": "quires the basis of learning to relate with others and with the world",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "The training datasets, as well as the implemented model architecture"
        },
        {
          "Brazil\nBrazil": "[27]. Unfortunately, the mental development process of a child can",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "and produced code are presented in Sections 3.1 and 3.2 respectively."
        },
        {
          "Brazil\nBrazil": "be hampered by mental disorders such as anxiety, stress, obsessive-",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "Results obtained using the suggested model and conclusions are"
        },
        {
          "Brazil\nBrazil": "compulsive behavior or emotional, sexual or physical abuse [38].",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "discussed in Sections 4 and 5."
        },
        {
          "Brazil\nBrazil": "The solution or reduction of consequences for these afflictions is",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "achieved with therapeutic processes carried out by professionals in",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "the field of psychology. Due to limited child maturity, the process",
          "Technology Center": "2\nBIBLIOGRAPHICAL RESEARCH"
        },
        {
          "Brazil\nBrazil": "involves not only assessment sessions with the child, but also inter-",
          "Technology Center": "A bibliographical research was performed to determine the State"
        },
        {
          "Brazil\nBrazil": "views with parents and educators, observation of the child in the",
          "Technology Center": "of The Art (SOTA) for emotion recognition tasks (FER) in children"
        },
        {
          "Brazil\nBrazil": "residential and school environments and data collection through",
          "Technology Center": "using computer algorithms. The search was made using the ‚ÄùWeb"
        },
        {
          "Brazil\nBrazil": "drawings, compositions, games and other activities [4], [39].",
          "Technology Center": "of Science‚Äù repository [37], covering the last 5 years, with the"
        },
        {
          "Brazil\nBrazil": "In this process, leisure resources such as: games, theater activ-",
          "Technology Center": "following search key:"
        },
        {
          "Brazil\nBrazil": "ities, puppets,\ntoys and others gain special prominence and are",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "used as support in therapy [7]. As a way to contribute to this ap-",
          "Technology Center": "child* AND emotion AND (recognition OR detection) AND"
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "(1)"
        },
        {
          "Brazil\nBrazil": "proach, Embodied Conversational Agents (ECA) are used as a tool",
          "Technology Center": ""
        },
        {
          "Brazil\nBrazil": "",
          "Technology Center": "(algorithm OR \"machine learning\" OR \"computer vision\")"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Figure 2: Algorithms for emotion recognition."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "information from facial images, which are then used for classifica-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "used by researchers with 9 references. We can mention the works": "by Sreedharan et al. [32] which makes use of this dataset for train-"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "ing FER model using a novel optimisation technique (Grey Wolf"
        },
        {
          "used by researchers with 9 references. We can mention the works": "optimisation), for instance."
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Figure 1: Datasets used for training."
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Overall, we found out that Facial Emotion Recognition (FER)"
        },
        {
          "used by researchers with 9 references. We can mention the works": "algorithms have had significant improvement in recent years [21],"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "driven by the success of deep learning-based approaches. In Fig. 2"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "we present the most frequently used algorithms for emotion recog-"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "nition. The convolutional neural network architecture (DL-CNN)"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "was the most used, with 22 references. As DL-CNN examples, we"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "can cite the works of Haque and Valles [12] and Cuadrado et al.[4]."
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Both of these propose an architecture for a Deep Convolutional"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Neural Network for a specific FER task, namely for robot tutors in"
        },
        {
          "used by researchers with 9 references. We can mention the works": "primary schools and identifying emotions in children with autism"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "spectrum disorder."
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "With the demand for high-performance algorithms, numerous"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "novel models, such as the DeepFace system [34] or the Transformer"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "architecture for sequential features [35] have also made great steps"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "in improving the overall accuracy and time efficiency for emotion"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "classification models."
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Among the most popular paradigms currently used for FER,"
        },
        {
          "used by researchers with 9 references. We can mention the works": ""
        },
        {
          "used by researchers with 9 references. We can mention the works": "Convolutional Neural Networks (CNNs) have demonstrated high"
        },
        {
          "used by researchers with 9 references. We can mention the works": "performance in detecting and recognizing emotion features from"
        },
        {
          "used by researchers with 9 references. We can mention the works": "facial expressions in images [16] by applying moving filters over"
        },
        {
          "used by researchers with 9 references. We can mention the works": "an image, also called convolution kernels. These models use hier-"
        },
        {
          "used by researchers with 9 references. We can mention the works": "archical feature extraction techniques to construct region-based"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "tation is available here."
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "These cropped images are then passed to a Convolutional Neural"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "Network (CNN), specifically the InceptionNet [33], to process the"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "cropped RGB pixels generated by the Haarcascade algorithm. In"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "parallel, we use the Gunner Farneback‚Äôs algorithm [10] to retrieve"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "the dense optical flow values from the current and previous cropped"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "frames. This is made to allow the network to process the variation"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "in facial muscles and skin movement over time. The optical flow"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "matrices are then passed to a second CNN, specifically a variation"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "of the ResNet [13]."
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "After calculating these two separate features, they are concate-"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "nated and used as input for a final recurrent block, specifically made"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "with layers of LSTM cells to generate the concatenated intermedi-"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "ary output. This takes advantage of the sequential nature of the"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "video frames to output a final vector of predicted probabilities for"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "each emotion. The aforementioned model uses a technique called"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "Late Fusion [15], in which two separate features are concatenated"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "inside the architecture and used as input for the final output layers."
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "The Late fusion technique allows for a better usage of the motion"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "generated by separate facial Action Units [8] by having to distinctly"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": "trained CNNs, one for raw RGB values (outputed by the Inception-"
        },
        {
          "Figure 4: HybridCNNFusion architecture. The full implemen-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "identifying children emotions in in-the-wild conditions, altough"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "not yet fit for real-world usage."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "The fusion of dense optical flow features in conjunction with"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "a hybrid CNN and a recurrent model represents a promising ap-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "proach in the challenging task of facial emotion recognition (FER)"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "in children, specifically in uncontrolled environments. Being a crit-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "ical need in the field of psychology, this approach offers a potential"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "solution."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "For ethically sensible situations, there are still important metrics"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "that have to be calculated, such as the Area under the ROC Curve"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "(AOC), which can indicate whether the model\nis prone to miss"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "important emotion predictions within small and specific frames,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "also called micro-expressions [17]."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "In fact, there is a large gap on current ethical questions for the"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "task, but we believe that improving the interpretability of the archi-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "tecture, explainability and security of transmission of the processed"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "information should be the focus of future models and frameworks"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "instead of just the overall accuracy. This will ensure that the tech-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "nology can be used safely and effectively to support the emotional"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "well-being of children."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "REFERENCES"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[1]\nLisa Feldman Barrett, Ralph Adolphs, Stacy Marsella, Aleix M. Martinez, and"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Seth D. Pollak. 2019. Emotional expressions reconsidered: challenges to in-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "ferring emotion from human facial movements. English. PSYCHOLOGICAL"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "SCIENCE IN THE PUBLIC INTEREST, 20, 1, (July 2019), 1‚Äì68. doi: 10.1177/1529"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "100619832930."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "De‚ÄôAira Bryant and Ayanna Howard. 2019. A comparative analysis of emotion-\n[2]"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "detecting al systems with respect to algorithm performance and dataset diver-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "sity. English. In AIES ‚Äò19: Proceedings of the 2019 AAAI/ACM Conference on AI,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Ethics, and Society, 377‚Äì382. isbn: 978-1-4503-6324-2. doi: 10.1145/3306618.331"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "4284."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[3]\nM. Catalina Camacho, Helmet T. Karim, and Susan B. Perlman. 2019. Neural"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "architecture supporting active emotion processing in children: a multivariate"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "approach. English. NEUROIMAGE, 188, (Mar. 2019), 171‚Äì180. doi: 10.1016/j.ne"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "uroimage.2018.12.013."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "L. I. Cuadrado, M. R. Angeles, and F. P. Lopez. 2019. Fer in primary school chil-\n[4]"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "dren for affective robot tutors. In FROM BIOINSPIRED SYSTEMS AND BIOMED-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "(Lecture Notes in Com-\nICAL APPLICATIONS TO MACHINE LEARNING, PT II"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "puter Science). Vol. 11487. Spanish CYTED; Red Nacl Computac Nat & Artificial,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Programa Grupos Excelencia Fundac Seneca & Apliquem Microones 21 s l,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "461‚Äì471. doi: 10.1007/978-3-030-19651-6_45."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[5]\nA. √ñhman D. E.Lundqvist A. Flykt. 1998. The karolinska directed emotional"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "face. (1998). https://www.kdef.se/."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[6]\nArnaud Dapogny, Charline Grossard, Stephanie Hun, Sylvie Serret, Ouriel"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Grynszpan, Severine Dubuisson, David Cohen, and Kevin Bailly. 2019. On"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "automatically assessing children‚Äôs facial expressions quality: a study, database,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "and protocol. English. 1, (Oct. 2019). doi: 10.3389/fcomp.2019.00005."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[7]\nCynthia Borges de Moura and M.R.Z.S. Azevedo. 2000. Estrat√©gias l√∫dicas para"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "uso em terapia comportamental infantil. In Sobre comportamento e cogni√ß√£o:"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "questionando e ampliando a teoria e as interven√ß√µes cl√≠nicas e em outros contextos."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Vol. 6. R. C. Wielenska, (Ed.) Santo Andr√©, 163‚Äì170."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[8]\nP. Ekman and W.V. Friesen. 1978. Facial Action Coding System. Number v. 1."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Consulting Psychologists Press. https://books.google.com.br/books?id=08l6wg"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "EACAAJ."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[9]\nP. Ekman and K. Scherer. 1984. Expression and the Nature of Emotion. lawrence"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Erlbaum Associates. https://www.paulekman.com/wp-content/uploads/2013/0"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "7/Expression-And-The-Nature-Of-Emotion.pdf."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[10]\nGunnar Farneb√§ck. 2003. Two-frame motion estimation based on polynomial"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "expansion. Josef Bigun and Tomas Gustavsson, (Eds.) Berlin, Heidelberg, (2003)."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[11]\nChristiane Goulart, Carlos Valadao, Denis Delisle-Rodriguez, Douglas Fu-"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "nayama, Alvaro Favarato, Guilherme Baldo, Vinicius Binotte, Eliete Caldeira,"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "and Teodiano Bastos-Filho. 2019. Visual and thermal\nimage processing for"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "facial specific landmark detection to infer emotions in a child-robot interaction."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "English. SENSORS, 19, 13, (July 2019). doi: 10.3390/s19132844."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "[12]\nM. I. U. Haque and D. Valles. 2018. A facial expression recognition approach"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "using dcnn for autistic children to identify emotions.\nIn S Chakrabarti and"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "HN Saha, (Eds.) Inst Engn & Management; IEEE Vancouver Sect; UBC; Univ"
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": "Engn & Management. IEEE, 546‚Äì551."
        },
        {
          "Rafael Zimmer, Marcos Sobral, and Helio Azevedo": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[13]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[14]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[15]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[16]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[17]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[18]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[19]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[20]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[21]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[22]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[23]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[24]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[25]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[26]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[27]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[28]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[29]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[30]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[31]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[32]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[33]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": "[34]"
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        },
        {
          "Hybrid Models for Facial Emotion Recognition in Children": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional expressions reconsidered: challenges to inferring emotion from human facial movements. English. PSYCHOLOGICAL SCIENCE IN THE PUBLIC INTEREST",
      "authors": [
        "Lisa Feldman",
        "Ralph Adolphs",
        "Stacy Marsella",
        "Aleix Martinez",
        "Seth Pollak"
      ],
      "year": "2019",
      "venue": "Emotional expressions reconsidered: challenges to inferring emotion from human facial movements. English. PSYCHOLOGICAL SCIENCE IN THE PUBLIC INTEREST",
      "doi": "10.1177/1529100619832930"
    },
    {
      "citation_id": "2",
      "title": "A comparative analysis of emotiondetecting al systems with respect to algorithm performance and dataset diversity. English",
      "authors": [
        "Aira De",
        "Ayanna Bryant",
        "Howard"
      ],
      "year": "2019",
      "venue": "AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "doi": "10.1145/3306618.3314284"
    },
    {
      "citation_id": "3",
      "title": "Neural architecture supporting active emotion processing in children: a multivariate approach",
      "authors": [
        "Catalina Camacho",
        "Helmet Karim",
        "Susan Perlman"
      ],
      "year": "2019",
      "venue": "English. NEUROIMAGE",
      "doi": "10.1016/j.neuroimage.2018.12.013"
    },
    {
      "citation_id": "4",
      "title": "Fer in primary school children for affective robot tutors",
      "authors": [
        "L Cuadrado",
        "M Angeles",
        "F Lopez"
      ],
      "year": "2019",
      "venue": "FROM BIOINSPIRED SYSTEMS AND BIOMED-ICAL APPLICATIONS TO MACHINE LEARNING, PT II",
      "doi": "10.1007/978-3-030-19651-6_45"
    },
    {
      "citation_id": "5",
      "title": "The karolinska directed emotional face",
      "authors": [
        "A √ñhman",
        "D Lundqvist",
        "A Flykt"
      ],
      "year": "1998",
      "venue": "The karolinska directed emotional face"
    },
    {
      "citation_id": "6",
      "title": "On automatically assessing children's facial expressions quality: a study, database, and protocol",
      "authors": [
        "Arnaud Dapogny",
        "Charline Grossard",
        "Stephanie Hun",
        "Sylvie Serret",
        "Severine Ouriel Grynszpan",
        "David Dubuisson",
        "Kevin Cohen",
        "Bailly"
      ],
      "year": "2019",
      "venue": "On automatically assessing children's facial expressions quality: a study, database, and protocol",
      "doi": "10.3389/fcomp.2019.00005"
    },
    {
      "citation_id": "7",
      "title": "Estrat√©gias l√∫dicas para uso em terapia comportamental infantil",
      "authors": [
        "Cynthia Borges De Moura",
        "M Azevedo"
      ],
      "year": "2000",
      "venue": "Estrat√©gias l√∫dicas para uso em terapia comportamental infantil"
    },
    {
      "citation_id": "8",
      "title": "Facial Action Coding System",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Consulting Psychologists Press"
    },
    {
      "citation_id": "9",
      "title": "Expression and the Nature of Emotion",
      "authors": [
        "P Ekman",
        "K Scherer"
      ],
      "year": "1984",
      "venue": "Expression and the Nature of Emotion"
    },
    {
      "citation_id": "10",
      "title": "Two-frame motion estimation based on polynomial expansion. Josef Bigun and Tomas Gustavsson",
      "authors": [
        "Gunnar Farneb√§ck"
      ],
      "year": "2003",
      "venue": "Two-frame motion estimation based on polynomial expansion. Josef Bigun and Tomas Gustavsson"
    },
    {
      "citation_id": "11",
      "title": "Visual and thermal image processing for facial specific landmark detection to infer emotions in a child-robot interaction",
      "authors": [
        "Christiane Goulart",
        "Carlos Valadao",
        "Denis Delisle-Rodriguez",
        "Douglas Funayama",
        "Alvaro Favarato",
        "Guilherme Baldo",
        "Vinicius Binotte",
        "Eliete Caldeira",
        "Teodiano Bastos-Filho"
      ],
      "year": "2019",
      "venue": "English. SENSORS",
      "doi": "10.3390/s19132844"
    },
    {
      "citation_id": "12",
      "title": "A facial expression recognition approach using dcnn for autistic children to identify emotions",
      "authors": [
        "M Haque",
        "D Valles"
      ],
      "year": "2018",
      "venue": "Inst Engn & Management; IEEE Vancouver Sect; UBC; Univ Engn & Management"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition",
      "arxiv": "arXiv:1512.03385[cs.CV"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Late fusion-based video transformer for facial micro-expression recognition",
      "authors": [
        "Jiuk Hong",
        "Chaehyeon Lee",
        "Heechul Jung"
      ],
      "year": "2022",
      "venue": "English. APPLIED SCIENCES-BASEL",
      "doi": "10.3390/app12031169"
    },
    {
      "citation_id": "16",
      "title": "A review on facial emotion recognition and classification analysis with deep learning. English",
      "authors": [
        "Asha Jaison",
        "C Deepa"
      ],
      "year": "2021",
      "venue": "BIOSCIENCE BIOTECHNOLOGY RESEARCH COMMUNICATIONS",
      "doi": "10.21786/bbrc/14.5/29"
    },
    {
      "citation_id": "17",
      "title": "Compound emotion recognition of autistic children during meltdown crisis based on deep spatio-temporal analysis of facial geometric features",
      "authors": [
        "Marwa Salma Kammoun Jarraya",
        "Mohamed Masmoudi",
        "Hammami"
      ],
      "year": "2020",
      "venue": "English. IEEE ACCESS",
      "doi": "10.1109/ACCESS.2020.2986654"
    },
    {
      "citation_id": "18",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Vincent Michalski",
        "Kishore Konda",
        "Roland Memisevic",
        "Christopher Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction",
      "doi": "10.1145/2818346.2830596"
    },
    {
      "citation_id": "19",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "Haik Kalantarian"
      ],
      "year": "2020",
      "venue": "English. JMIR MENTAL HEALTH",
      "doi": "10.2196/13174"
    },
    {
      "citation_id": "20",
      "title": "The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study",
      "authors": [
        "Haik Kalantarian"
      ],
      "year": "2020",
      "venue": "English. JMIR MENTAL HEALTH",
      "doi": "10.2196/13174"
    },
    {
      "citation_id": "21",
      "title": "Analysis of machine learning algorithms for facial expression recognition. English",
      "authors": [
        "Akhilesh Kumar",
        "Awadhesh Kumar"
      ],
      "year": "2022",
      "venue": "ADVANCED NETWORK TECHNOLOGIES AND INTELLIGENT COMPUTING, ANTIC 2021",
      "doi": "10.1007/978-3-030-96040-7\\_55"
    },
    {
      "citation_id": "22",
      "title": "Bi-modality fusion for emotion recognition in the wild. English",
      "authors": [
        "S Li",
        "W Zheng",
        "Y Zong",
        "C Lu",
        "C Tang",
        "X Jiang",
        "J Liu",
        "W Xia"
      ],
      "year": "2019",
      "venue": "ASSOC COMPUTING MACHINERY, 1601 Broadway, 10th Floor",
      "doi": "10.1145/3340555.3355719"
    },
    {
      "citation_id": "23",
      "title": "Expression recognition of classroom children's game video based on improved convolutional neural network. English. SCIENTIFIC PRO-GRAMMING",
      "authors": [
        "Xiaohong Li"
      ],
      "year": "2022",
      "venue": "Expression recognition of classroom children's game video based on improved convolutional neural network. English. SCIENTIFIC PRO-GRAMMING",
      "doi": "10.1155/2022/5203022"
    },
    {
      "citation_id": "24",
      "title": "Smart doll: emotion recognition using embedded deep learning",
      "authors": [
        "Jose Luis Espinosa-Aranda",
        "Noelia Vallez",
        "Maria Rico-Saavedra",
        "Javier Parra-Patino",
        "Gloria Bueno",
        "Matteo Sorci",
        "David Moloney",
        "Dexmont Pena",
        "Oscar Deniz"
      ],
      "year": "2018",
      "venue": "English. SYMMETRY-BASEL",
      "doi": "10.3390/sym10090387"
    },
    {
      "citation_id": "25",
      "title": "The promises and perils of automated facial action coding in studying children's emotions. English",
      "authors": [
        "M Aleix",
        "Martinez"
      ],
      "year": "2019",
      "venue": "DEVELOPMENTAL PSYCHOLOGY",
      "doi": "10.1037/dev0000728"
    },
    {
      "citation_id": "26",
      "title": "The child emotion facial expression set: a database for emotion recognition in children",
      "authors": [
        "Juliana Gioia"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2021.666245"
    },
    {
      "citation_id": "27",
      "title": "The Origins of Intelligence in Children",
      "authors": [
        "Jean Piaget"
      ],
      "year": "1952",
      "venue": "The Origins of Intelligence in Children"
    },
    {
      "citation_id": "28",
      "title": "Embodied Conversational Agents in Clinical Psychology: A Scoping Review",
      "authors": [
        "Simon Provoost",
        "Ming Ho",
        "Jeroen Lau",
        "Heleen Ruwaard",
        "Riper"
      ],
      "year": "2017",
      "venue": "Journal of Medical Internet Research",
      "doi": "10.2196/jmir.6553"
    },
    {
      "citation_id": "29",
      "title": "Ensemble of machine learning models for an improved facial emotion recognition. English",
      "authors": [
        "Sergio Pulido-Castro",
        "Nubia Palacios-Quecan",
        "Michelle Ballen-Cardenas",
        "Sandra Cancino-Suarez",
        "Alejandra Rizo-Arevalo",
        "Juan Lopez"
      ],
      "year": "2021",
      "venue": "2021 IEEE URUCON. IEEE URUCON Conference (IEEE URUCON)",
      "doi": "10.1109/URUCON53396.2021.9647375"
    },
    {
      "citation_id": "30",
      "title": "FER-2013 Learn facial expressions from an image",
      "authors": [
        "Manas Sambare"
      ],
      "year": "2022",
      "venue": "FER-2013 Learn facial expressions from an image"
    },
    {
      "citation_id": "31",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556[cs.CV"
    },
    {
      "citation_id": "32",
      "title": "Grey wolf optimisation-based feature selection and classification for facial emotion recognition",
      "authors": [
        "Ninu Preetha",
        "Nirmala Sreedharan",
        "Brammya Ganesan",
        "Ramya Raveendran",
        "Praveena Sarala",
        "Binu Dennis",
        "Rajakumar Boothalingam"
      ],
      "year": "2018",
      "venue": "IET BIOMETRICS",
      "doi": "10.1049/iet-bmt.2017.0160"
    },
    {
      "citation_id": "33",
      "title": "Going deeper with convolutions",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "year": "2014",
      "venue": "Going deeper with convolutions",
      "arxiv": "arXiv:1409.4842[cs.CV"
    },
    {
      "citation_id": "34",
      "title": "Deep-Face: closing the gap to human-level performance in face verification",
      "authors": [
        "Yaniv Taigman",
        "Ming Yang",
        "Marc'aurelio Ranzato",
        "Lior Wolf"
      ],
      "year": "2014",
      "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/cvpr.2014.220"
    },
    {
      "citation_id": "35",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762[cs.CL"
    },
    {
      "citation_id": "36",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "Paul Viola",
        "Michael Jones"
      ],
      "year": "2001",
      "venue": "CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"
    },
    {
      "citation_id": "37",
      "title": "Web of Science platform",
      "year": "2022",
      "venue": "Web of Science platform"
    },
    {
      "citation_id": "38",
      "title": "Evidence-Based Psychotherapies for Children and Adolescents",
      "authors": [
        "John Weisz",
        "Alan Kazdin"
      ],
      "year": "2010",
      "venue": "Evidence-Based Psychotherapies for Children and Adolescents"
    },
    {
      "citation_id": "39",
      "title": "Emotion monitoring for preschool children based on face recognition and emotion recognition algorithms",
      "authors": [
        "Guiping Yu"
      ],
      "year": "2021",
      "venue": "English. COMPLEXITY",
      "doi": "10.1155/2021/6654455"
    },
    {
      "citation_id": "40",
      "title": "Spreadsheet with Reference Classification Groups",
      "authors": [
        "R Zimmer",
        "M Sobral",
        "H Azevedo"
      ],
      "year": "2023",
      "venue": "Spreadsheet with Reference Classification Groups"
    }
  ]
}