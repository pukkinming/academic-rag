{
  "paper_id": "2112.05666v2",
  "title": "An Ensemble 1D-Cnn-Lstm-Gru Model With Data Augmentation For Speech Emotion Recognition",
  "published": "2021-12-10T16:57:53Z",
  "authors": [
    "Md. Rayhan Ahmed",
    "Salekul Islam",
    "Ph. D",
    "A. K. M. Muzahidul Islam",
    "Ph. D",
    "Swakkhar Shatabda",
    "Ph. D"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Human-Computer Interaction",
    "1D CNN GRU LSTM Network",
    "Ensemble Learning",
    "Data Augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Precise recognition of emotion from speech signals aids in enhancing human-computer interaction (HCI). The performance of a speech emotion recognition (SER) system depends on the derived features from speech signals. However, selecting the optimal set of feature representations remains the most challenging task in SER because the effectiveness of features varies with emotions. Most studies extract hidden local speech features ignoring the global long-term contextual representations of speech signals. The existing SER system suffers from low recognition performance mainly due to the scarcity of available data and sub-optimal feature representations. Motivated by the efficient feature extraction of convolutional neural network (CNN), long short-term memory (LSTM), and gated recurrent unit (GRU), this article proposes an ensemble utilizing the combined predictive performance of three different architectures. The first architecture uses 1D CNN followed by Fully Connected Networks (FCN). In the other two architectures, LSTM-FCN and GRU-FCN layers follow the CNN layer respectively. All three individual models focus on extracting both local and long-term global contextual representations of speech signals. The ensemble uses a weighted average of the individual models. We evaluated the model's performance on five benchmark datasets: TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D. We have augmented the data by injecting additive white gaussian noise, pitch shifting, and stretching the signal level to obtain better model generalization. Five categories of features were extracted from the speech samples: mel-frequency cepstral coefficients, log mel-scaled spectrogram, zero-crossing rate, chromagram, and root mean square value from each audio file in those datasets. All four models perform exceptionally well in the SER task; notably, the ensemble model accomplishes the state-of-the-art (SOTA) weighted average accuracy of 99.46% for TESS, 95.42% for EMO-DB, 95.62% for RAVDESS, 93.22% for SAVEE, and 90.47% for CREMA-D datasets and thus significantly outperformed the SOTA models using the same datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The interaction between humans and computers is progressing swiftly. The human-computer interaction (HCI) studies how humans interconnect with computers and to which extent computers are developed to make those human interactions more productive. Interactions between these two entities should be as spontaneous as human-to-human conversations. Therefore, the effective design, proper implementation, and evaluation of interfaces through which the interactions occur are some of the essential focuses of HCI. It aspires to comprehend, assess, and create a range of human experiences, including enjoyment, excitement, concentration, focus, productivity, knowledge, and behavior modification  (O'Brien, Roll, Kampen, & Davoudi, 2022) . Speech is the principal mode of communication among human beings. Through speech, we humans express one of our most fundamental components, emotions, and the emotion recognition of that speech is one of the active research zones of HCI as well as digital signal processing. The process of distinguishing emotions from speech signals is known as speech emotion recognition (SER). SER is imperative for enhancing the domain of HCI and influential in setting up the direction in which modern-day electronic devices are rapidly moving (J.  Chatterjee, Mukesh, Hsu, Vyas, & Liu, 2018) . Various significant applications such as intelligent robots, audio surveillance, criminal investigations, automated smart home appliances, movie or music recommendation systems, dialogue systems, etc., which rely on the user's emotional state could do with a system that automatically detects the user's emotion from the speech. Researchers have developed various techniques in the last decade to provide a robust and lightweight SER system  (Abbaschian, Sierra-Sosa, & Elmaghraby, 2021; Anvarjon, Mustaqeem, & Kwon, 2020; Khalil et al., 2019) . However, due to a lack of technologies and tools, ambiguous nature of emotions, diversity in language and accent across different cultures, frequency and amplitude variation in human utterance regarding gender and age, recognizing the human's emotional states from speech has proven to be complicated and challenging.\n\nA generic high-level overview of the workflow of the proposed SER system is represented in Fig.  1 . In the first stage (preprocessing), all the sample audio files are resized to the fixed length, and data augmentation is performed to increase the number of samples and address the data imbalance issues in the datasets by adding AWGN, shifting the pitch, and stretching the time. Next, in the 2 nd stage, features from time and frequency domains, as well as commonly used spectral features are extracted ML-based methods such as SVM  (Bhavan et al., 2019) , and Random Forest (RF) (Z.  Zhang, 2021; Zvarevashe & Olugbara, 2020) , or suffers from lower SER rates.\n\nMotivated by the vast success and effectiveness of deep neural networks (DNN) in various classification tasks and higher predictive performance of ensemble learning (EL), in this paper, we propose four DL-based frameworks: first, a baseline dilated 1D CNNs-FCNs based framework; second, a 1D CNNs-LSTM-FCNs based framework; third, 1D CNNs-GRU-FCNs based framework; and fourth an ensemble of those three frameworks through a weighted average mechanism. We have used five publicly available benchmark datasets that are extensively used in the literature: Toronto Emotional Speech Set (TESS)  (Pichora-Fuller, Kathleen; , & Dupuis, 2020) , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  (Livingstone & Russo, 2018) , Surrey Audio-Visual Expressed Emotion (SAVEE)  (Haq & Jackson, 2014) , Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D)  (Cao et al., 2014) , and Berlin Database of Emotional Speech (EMO-DB)  (Burkhardt, Paeschke, Rolfes, Sendlmeier, & Weiss, 2005) . The number of samples in each of these datasets is relatively low for a DL-based model to train properly without any overfitting issues. Besides some of these datasets, have class imbalance issues. To address these challenges, we performed audio data augmentation by injecting additive white gaussian noise (AWGN), changing the pitch of the signal, and stretching the signal level. Additionally, to deal with overfitting we apply kernel and bias regularization (L2=0.01) that reduces the weights squared magnitude, and add dropout layers that arbitrarily eliminate the neurons during the training of the models. The model was trained with augmented data along with the original dataset, yielding a higher accuracy rate with improved generalization ability. Initially, MFCC, LMS, Chromagram, ZCR, and RMS value features are extracted from the audio samples, as previous studies have suggested their efficacy in the SER task  (Hajarolasvadi & Demirel, 2019; Lee, Roh, Kim, Kim, & Hong, 2008; Nantasri et al., 2020) . The mean value of these features is calculated and is used to train the model to detect human emotions such as \"happiness,\" \"sadness,\" \"fearful\", \"surprise,\" \"anger,\" \"surprise,\" \"boredom,\" \"neutral\" etc., from audio signals with improved recognition performance. Combined with data augmentation, each proposed model produces exceptional SOTA results for the SER task. The noteworthy contribution of this work is as follows:\n\n• We propose four DNN-based models built using CNN-based local feature-acquiring blocks (LFABs) and LSTM-GRU-based global feature-acquiring block (GFAB). This work first extracts the LLD features from the speech audio signals.\n\n• The baseline model-A uses seven sequential LFABs to better understand the high-level hidden local features from those extracted LLD features during model training followed by Fully Connected Network (FCN) layers and a softmax layer for classification. The other two models, model-B and model-C, are proposed by adding a GFAB after the final LFAB. Model-B employs LSTM-FCNs, and model-C utilizes GRU-FCNs architecture to acquire long-term global contextual representations from the speech signals. A weighted ensemble framework (model-D) is also proposed which combines the three individual models by adjusting their weights and achieves better performance than the individual models (i.e., model-A, B, and C).\n\n• We have extensively experimented with the models on five widely used publicly available benchmark datasets for SER: TESS, RAVDESS, SAVEE, CREMA-D, and EMO-DB, covering two languages: English and German.\n\n• Data augmentation is performed to increase the training samples, reduce the overfitting problem and make the models more generalized. SER accuracy increased by 3% to 32% from the model trained with the original dataset only.\n\n• The performance of all the proposed models is compared with the previous SOTA models. Amongst all four models, the ensemble model-D achieves the SOTA weighted average accuracy of 99.46% for TESS, 95.42% for EMO-DB, 95.62% for RAVDESS, 93.22% for SAVEE, and 90.47% for CREMA-D datasets. These are significantly improved results compared to the single models and the previous SOTA methods on each dataset.\n\nThe remainder of this paper is assembled as follows. Section 2 presents the existing literature review in the SER task to grasp the current trend, intuition getting, and find the scope for improving the task. Section 3 provides an overview of the architecture of the proposed models. Section 4 is covered by an in-depth discussion about utilized datasets, data augmentation techniques, the feature extraction process, and model training. We comprehensively analyze and compare the experimental results of the proposed individual model-A, B, C, and weighted ensemble model-D with SOTA SER benchmarks in section 5. Finally, in section 6, we conclude with a discussion about the existing challenges and possible future research directions in SER.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "Digital signal processing (DSP) is a matter of great interest among the research community, and researchers have come up with several approaches for a robust improvement by eliminating the existing issues in the SER task. For any SER task, feature selection is the most critical part because irrelevant features directly affect the next part, which is speech emotion classification. Currently, researchers worldwide are utilizing DL for SER-related tasks due to their vast triumphs in representing features and the ability to find hidden patterns from the extracted speech-based feature set. In time-domain representation, some of the commonly utilized SER features are amplitude envelope, ZCR, and RMS  (Das et al., 2022) . Notable frequency domain-represented speech features include band energy ratio, Mel-scaled spectrogram, Chromagram, and various spectral features such as centroid, flux, contrast, and roll-off  (Alnuaim, Zakariah, & Alhadlaq, 2022) . The most significant and widely used cepstral-based feature for the SER task is MFCC. Statistical features include entropy, skewness, kurtosis, etc. Recently gammatone cepstral coefficients (GTCC) are being heavily explored in the field of SER  (Bandela & Kumar, 2021; S. Zhao, Yang, Cohen, & Zhang, 2021) . Speech spectrogram is one of the significant features utilized by most researchers nowadays regarding SER tasks  (Alnuaim et al., 2022; Mustaqeem & Kwon, 2021b; Sultana et al., 2022) . It is a two-dimensional (2D) depiction of speech signals. It visually represents a signal's power, where different frequencies over time are shown in the waveform. MFCC is the most widely used feature in terms of SER tasks. By converting the traditional frequency to mel-scale, MFCC accounts for human insight for sensitivity at acceptable frequencies, making it appropriate for SER tasks. When training models, 12 to 20 MFCCs are usually taken into account, containing information about the changes in rate in the different spectrum bands  (Abdel-Hamid, 2020; Christy, Vaithyasubramanian, Jesudoss, & Praveena, 2020; Issa, Fatih Demirci, & Yazici, 2020; Nantasri et al., 2020) . The ZCR feature represents the positive and negative sign changes in a signal (J.  Chatterjee et al., 2018) . Chroma-based audio features have proven effective for investigating, evaluating, and extracting information from music audio and SER-related tasks  (Issa et al., 2020) . It is observed that methods such as traditional ML, DL, and their fusion are extensively used in recent literature for the SER task. The researchers are also exploring several attention mechanisms along with DL methods to focus more on the region of interest in the speech signal. Aside from these mentioned methods, ensemble learning and transfer learning-based methods are gaining momentum due to their increased performances in various classification tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Traditional Machine Learning-Based Models",
      "text": "Numerous methods based on SVM, Multi-Layer perceptron (MLP), Gaussian Mixture Modelling (GMM), Naïve Bayes (NB), and Hidden Markov models (HMM) were utilized in earlier efforts for SER from the speech signal.  (Ancilin & Milton, 2021)  use the SVM classifier for the SER task on SAVEE, EMO-DB, RAVDESS, eNTERFACE, EMO-VO, and Urdu datasets. The magnitude spectrum was employed instead of energy spectrum to extract the Mel frequency magnitude coefficient features from the speech signals. In addition, traditional MFCC, log frequency power coefficient, and linear prediction cepstral coefficient were also extracted and utilized for model training. SER performance is enhanced by using the magnitude spectrum instead of the power spectrum and using the log magnitude coefficients directly rather than the cosine transformed coefficients. (Z. T.  Liu, Wu, et al., 2018)  propose an SER framework that selects features based on correlation analysis and Fisher criterion. This process reduces the number of irrelevant and redundant features. Then they used the extreme-ML decision tree scheme to classify the emotions into different categories by utilizing the Chinese emotion corpus, Chinese Academy of Science Institute of Automation (CASIA).  (Wang, An, Li, Zhang, & Li, 2015)  suggest a new kind of speech feature, Fourier parameter functions along with the first and second-order differences of harmony-based features, estimated by Fourier analysis. They have extracted and combined two types of features: Fourier parameter and MFCC from the utilized EMO-DB, CASIA, and Chinese elderly emotion database (EESDB) datasets and employed the combined features as input to the SVM and a Bayesian classifier for the SER task.  (Palo, Chandra, & Mohanty, 2017)  recommend an SER technique using MLP and GMM for the Oriya Language. They used various feature extraction techniques, including MFCC, Perceptual linear prediction, and linear predictive coding. MLP achieves the highest SER accuracy of 87%.  (Demircan & Kahramanli, 2018)  utilize type-1 fuzzy C-means method to the extracted MFCC and linear prediction coefficient features and after that identified the cluster centers and fed them to different classifiers such as SVM, KNN, and NB for the classification task. The SVM classifier achieved the highest classification rate of 92.86%. However, there exist some issues with methods like HMM and GMM, for example, finding the most likely sequence of hidden states, given a sequence of observations. Another significant disadvantage of these methods is that they underperform while modeling nonlinear data  (Venkataramanan & Rajamohan, 2019) .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Deep Learning-Based Models",
      "text": "Deep learning (DL) methods such as ANN, DNN, CNN, RNN, GRU, LSTM, Bi-LSTM, and Bi-GRU have been leveraged as feature extractors to facilitate the learning of discriminative representations, with varying degrees of success  (Anvarjon et al., 2020; Mustaqeem et al., 2020; Yadav & Vishwakarma, 2020; S. Zhang et al., 2020) .  (Nantasri et al., 2020)  propose an SER model by collecting 20-MFCC, 20-delta, and 20-delta-delta features and computing their mean values. These mean values are used as the input for the artificial neural network (ANN) classifier. They have evaluated their model with RAVDESS and EMO-DB datasets and achieved 82.3% and 87.8% accuracy, respectively. To reduce the error rate of ANN and proper selection of optimal weights and biases for the model to train,  (Moghanian, Saravi, Javidi, & Sheybani, 2020)  propose a new technique named GOAMLP.  (Lalitha, Tripathi, & Gupta, 2019)  propose a deep neural network (DNN) model for the SER task to investigate the effective predictive performance of perceptual-based speech features.  (Anvarjon et al., 2020)  propose an SER model based on extracting high-level features from the spectrograms of speech utterances. They have used plain rectangular kernels with a revised pooling strategy. The model's performance was evaluated with two datasets. It achieved 77.01% and 92.02% accuracy for the Interactive Emotional Dyadic Motion Capture (IEMOCAP) and EMO-DB datasets, respectively.  (Yoon, Byun, & Jung, 2018)  implement a dual recurrent encoder model approach for SER tasks by utilizing text and audio data from the IEMOCAP dataset. The authors employ MFCC derivatives and prosodic features along with text tokens as the input features of the proposed framework. Their multimodal method led to an accuracy of 71.8% on the IEMOCAP dataset.  (Tiwari, Soni, Chakraborty, Panda, & Kopparapu, 2020)  propose an utterance-level parametric generative noise model to test the robustness of the SER model when exposed to the presence of additive noise. Their proposed architecture is advantageous for suppressing unseen noise because the manufactured noise can encompass the total noise space in the energy domain of the Mel-filter bank. However, even with the performed data augmentation (DA), the achieved DNN-based SER model's performance is not very significant, with an accuracy of 76.77% on the EMO-DB dataset and 53.35% on the IEMOCAP dataset.  (Neumann & Vu, 2019)  integrates unsupervised auto-encoder strategy along with the CNN method to classy emotions from speech, however, this unsupervised approach achieves less satisfactory performances in terms of SER. Recently unsupervised DL-based algorithms are being explored for the SER task due to the shortage of sufficient data samples in each of the publicly available datasets. DA through unsupervised DL-based algorithms such as generative adversarial networks (GAN)  (Chatziagapi et al., 2019) , conditional GAN  (Ma, Li, Ni, Huang, & Zhang, 2022) , cycle consistent GAN  (Bao, Neumann, & Vu, 2019)  is performed in many studies, however, one noticeable thing is, the use of completely synthetic data in those studies achieve unsatisfactory performance regarding SER task.  (Praseetha & Joby, 2021 ) employed a GRU-based DL model for the SER task and extracts the filter-bank energies of the speech signals to train the model. The model achieves an accuracy of 93% in the augmented TESS dataset. In another work of SER,  (Jothimani, S and Premalatha, 2022)  utilize the CNN and LSTM-based models where the experimental analysis was carried out using the SAVEE, CREMA, RAVDESS, and TESS datasets. The authors used the MFCC, ZCR, and RMS value as the features for model training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hybrid Models",
      "text": "Due to the success of these DL-based architectures, interest in fusing these network types into a single architecture to capture both local and long-term contextual dependencies of data has increased recently (S.  Li et al., 2021; U. Kumaran, Rammohan, Nagarajan, & Prathik, 2021; Xu, Zhang, & Zhang, 2021) , ensemble learning  (Chalapathi et al., 2022; Zheng et al., 2020 ) make up most SER architectures that use neural networks.  (Sultana et al., 2022)  performs a cross-lingual SER study by utilizing CNN and the Bi-LSTM network that tries to capture both temporal and sequential representations of emotions.  (Mustaqeem & Kwon, 2020b)  extract spatiotemporal features for the SER task using a ConvLSTM model. Using four blocks of 1D CNN and LSTM, the authors have gathered the most significant distinctive emotional features. The extracted features are then fed into the GRU-based network, which is used to re-adjust the global weights. By utilizing the 1D CNN with LSTM network in one model and 2D CNN with LSTM network in another model, (J.  Zhao et al., 2019)  propose two SER models. The 2D CNN LSTM model achieves better emotion recognition results by focusing on capturing local correlations as well as global contextual information from LMS features. Researchers have experimented with traditional ML-based methods and DL-based methods in the same work and did a comparative analysis of these models' SER performance.  (Singh, Puri, Aggarwal, & Gupta, 2020)  leverages the CREMA-D dataset to train two classifiers (SVM and RNN) with prosodic and spectral features, that account for variance in speech intensity. The classifiers were trained at three stages of intensity: low, medium, and high. The \"Happy\" and \"Neutral\" labeled emotions have the highest classification accuracy, while the \"Disgust\" labeled emotion has the lowest.  (Kerkeni et al., 2019)  suggest an automatic SER system based on machine learning methods. The authors extract modulation spectral and MFCC features from speech signals in two corpora of EMO-DB and Spanish speech utterances and classifies them using SVM, Multivariate linear regression (MLR), and RNN classifiers. Feature selection was used to identify the most relevant feature subset. SER reported the highest recognition rate of 94% using the RNN classifier without speaker normalization and feature selection on the Spanish dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attention-Based Models",
      "text": "Recently different attention mechanisms are being extensively integrated into the SER domain due to their ability to explore distinctive regions of data.  (Guo et al., 2022)  integrate phase information of a signal with the magnitude information for the SER task. To capitalize on the complementary nature of magnitude and phase information, this study employs a single-channel model along with a multi-channel model with attention based on magnitude spectrograms, modified group delay cepstral coefficients, and dynamic relative phase. Incorporating phase information makes it possible to capture more comprehensive acoustic features. (Z.  Zhao et al., 2021)  propose a hybrid deep CNN architecture that leverages parallel convolutional layers combined with a squeezeand-excitation network incorporated with a self-attention-based dilated residual network. The architecture is trained with connectionist temporal classification loss for discrete SER tasks and effectively captures long-term contextual dependencies. To investigate the autocorrelation of phonemes in speech, (D.  Li et al., 2021)  combine the self-attention mechanism with the Bi-LSTM network. The self-attention mechanism can provide different weights to frames of varying emotional intensity, but it can also determine the autocorrelation between frames. (S.  Li et al., 2021)  propose a composite model that combines a spatiotemporal attention network, with a frequency-based attention network. The proposed network narrows down the emotional frequency regions from a spectrogram image to focus on the desired emotional regions. They have also developed a large-margin learning technique to deal with the problem of feature aliasing. It improves intra-class compactness while increasing inter-class distances among features. (G. K.  Liu, 2018)  demonstrates that a feature set consisting of gammatone frequency cepstral coefficients improves the SER accuracy by 3.6% over MFCCs by investigating three frameworks: Fully Connected Networks (FCN), LSTM, and Attention-LSTM networks.  (Yoon et al., 2019)  present a multi-hop attention framework for the SER task by extracting hidden contextual information from speech data using two streams Bi-LSTMs and then applying the multi-hop attention strategy to generate the final weights for emotion recognition.  (Meng, Yan, Yuan, & Wei, 2019)  propose a 3D LMS-based residual dilated CNN and memory attention mechanisms. They utilize a composite of static LMS feature, delta, and deltas-deltas feature to build the feature vector from the raw speech signal as input for the model. The dilated CNN assists the model to obtain more receptive fields than using the conventional pooling layer. In the IEMOCAP (speaker-dependent) dataset the model achieved 74.96% accuracy, and in the IEMOCAP (speaker-independent) dataset it achieved 69.32% accuracy. The model achieved the best accuracy of 90.37% on the EMO-DB (speaker-dependent) dataset.  (Mustaqeem & Kwon, 2021b ) designed a self-attention module based on DL for the SER system. It receives the transitional feature maps and uses it to build the channel and spatial attention map with minimal overhead. The authors employ a dilated CNN architecture in spatial attention to extract spatial information and a multi-layer perceptron in channel attention to extract global cues from the input tensor. The proposed model archives 78.01%, 80.00%, and 93.00% accuracy on IEMOCAP, RAVDESS, and EMO-DB datasets, respectively.  (Xie et al., 2019)  propose an SER system based on modified attention-LSTM architecture. They have extracted frame-level speech features from the waveform to replace traditional statistical features, preserving the timing relations in the original speech through the sequence of frames. The forget gate of the LSTM was replaced with an attention gate in order to reduce complexity. Additionally, they increased the system's efficiency by applying the attention mechanism on both time and feature dimensions rather than simply forwarding the previous iteration's output in LSTM. Although attention modules have become an integral component of modern SER systems, they are not indispensable for achieving high SER performances or even SOTA results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Transfer Learning-Based Models",
      "text": "Methods based on the use of pre-trained neural networks frequently produce superior performances to more traditional procedures. Transfer learning (TL) has the potential to overcome SER's cross-domain barrier. (S.  Zhang, Zhang, Huang, & Gao, 2018)  employed pre-trained AlexNet architecture  (Krizhevsky, Sutskever, & Hinton, 2012)  for learning high-level feature representations from the extracted three channels of the LMS feature. Additionally, the authors suggest an approach for pooling named discriminant temporal pyramid matching (DTPM) features to discriminative utterance-level representations. AlexNet finetuned for emotional speech outperformed the simpler Depp CNN model in four distinct datasets, while DTPM-based pooling outperformed the traditional average pooling method. A 2D CNN-based model that uses spectrograms generated from the EMO-DB dataset,  (Badshah, Ahmad, Rahim, & Baik, 2017)  propose an SER architecture. They have also explored the field of transfer learning and utilized pre-trained AlexNet architecture but got unsatisfactory results. The initial proposed model achieved 84.3% accuracy on the test set.  (Xi, Li, Song, Jiang, & Dai, 2019 ) used a residual adapter to minimize domain-specific parameters while increasing domain-agnostic parameters sharing.  (Aggarwal, Srivastava, Agarwal, & Chahal, 2022)  propose a two-way feature extraction method for the SER task. In the first approach, they extract the MFCC, spectrogram, spectral centroid, and roll-off features. Then in the second approach, they extract 2-dimensional LMS images from the speech signals. They utilize the pre-trained VGG-16 network for the SER task.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ensemble Learning-Based Models",
      "text": "Ensemble Learning (EL)-based methods have higher predictive accuracy compared to individual estimators. It combines the predictions from two or more ML or DL models to produce a more stable, accurate, and robust prediction.  (Chalapathi et al., 2022)  utilize the adaptive boosting ensemble method along with the fuzzy c-means approach to deal with the high-dimensional acoustic features.  (Zvarevashe & Olugbara, 2020)  employ bagging classifiers such as random decision forest, bagging with SVM, MLP, and boosting classifiers such as gradient boosting machine, and AdaBoost with CART for the SER task. In another study, (Z. Zhang, 2021) used the RF classifier along with the weighted binary cuckoo search method to select the optimal feature subset. Though time required to train multiple ML and DL-based architectures to perform the ensemble mechanism for the classification task is still a matter of concern. However, emotion is a sensitive topic, and recognition of emotion from the speech is a challenging task. A combination of multiple individual estimators and utilizing each of their feature learning strengths in the SER domain using an ensemble mechanism even at the cost of larger training time should be considered because of the need for more accurate and stable recognition performance of speech emotions. Inspired by the efficient and stable predictive performance of EL-based architecture, we adopt the EL mechanism for the final model-D, which combines the predictive results of three proposed individual models-A, B, and C in a weighted average method.\n\nIn Tables  4 to 9 , we provide an extensive comparative evaluation between our proposed work and the notable works discussed above in the literature review section. In the comparison, we highlight the methodology, extracted features, utilized datasets, feature dimension, data augmentation methods, achieved results, and year of publications.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Proposed Methods",
      "text": "The main objective of this work is to investigate the efficiency of an ensemble architecture combining multiple novel DL-based models for a multi-lingual SER system. Factors such as utilized datasets, number of samples in those datasets, class imbalance, data augmentation (DA), feature extraction from speech signals, and selection of proper ML or DL-based classifiers play a significant role in the SER performance of an ensemble architecture. Fig.  2 . summarizes our approach to an ensemble architecture for the SER task. This study utilizes five benchmark SER datasets (TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D) covering English and German languages. Since there is a shortage of sufficient sample audio files in those datasets, and to deal with the adverse impact of this data shortage issue on the performance of DL-based architecture, we performed three types of DA techniques (AWGN addition, pitch shifting, and time stretching) to increase the samples of those datasets to obtain proper convergence and generalizability of the proposed DL-based models. The proposed approach involves extracting a combination of time-domain, frequency-domain, and cepstral-based features from raw audio recordings to provide the DL-based models as input. The predictions from the individual DL-based models are then weighted, and a weighted average ensemble prediction is performed with SOTA SER performance. We present further details about the utilized datasets, adopted DA techniques, and extracted speech features in section 4. The details of the proposed individual DL model-A, B, C, and ensemble model-D are presented below.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Proposed Baseline Model-A (1D Cnns-Fcns)",
      "text": "This study uses 1D CNN followed by FCNs to build the first baseline model for SER. 1D CNN performs well with structured data. In terms of audio data, 1D CNN extracts the temporal information within the speech signal. The extracted ZCR, Chromagram, MFCC, RMS, and LMS features from the speech signals are stored in an array creating a vector of features. This vector of features is fed to the proposed baseline model as input. Using seven sequential LFABs containing convolutional, max-pooling, batch normalization (BN), and dropout layers, the model extracts hidden local patterns from the speech audio signals, as shown in Fig.  3 . The 1Dconvolution layer, max-pooling layer, and BN layers are the essential layers of the LFABs. Two FCNs collect the ultimate global features from the speech signals.\n\nThe proposed 1D CNNs-FCNs Model-A takes 155x1feature vector arrays as the input. The first LFAB block has 256 filters, with a kernel size of eight, padding = 'same', dilation rate = (1x1), and a stride of one. The dilation rate reduces the input vector feature map. The Rectified Linear Unit (ReLU) then triggers its output after BN is added. By solving the vanishing gradient problem, the BN layer aids all layers of the neural network in learning at a normalized rate. It speeds up the training process by normalizing the hidden layer activation. In addition, to cope with the model overfitting issue, we used the dropout layer and kernel regularization (L1 and L2) methods with a rate of 0.01. The output of a preceding input layer is received by the second layer in this stack, which consists of identical 256 filters with the corresponding kernel size, dilation rate, and stride. ReLU also enables the output of this layer, and then dropout at a rate of 0.25 is added. Following that, BN is performed, with the output being fed to a 1D max-pooling layer with a window size of two. The following six LFABs with filters of 256, 128, 128, 128, 256, and 64 filters use the kernel size, dilation rate, and stride configuration as previous blocks. The flattening layer and 50% dropout follow the ultimate LFAB. This flattening layer output is received by two FCNs of 128, and 64 units with a dropout of 50%, and finally, the output layer with softmax activation function which is utilized to distinguish the emotion according to the hidden features learned through LFABs. Depending on the task, the LFABs can be customized differently. The changes in LFAB configuration are primarily reflected in the convolution, dilation, pooling, and batch normalization settings.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Proposed Model-B (1D Cnns-Lstm-Fcns) And Model-C (1D Cnns-Gru-Fcns)",
      "text": "Proposed model-B and model-C, as shown in Fig.  4  and 5 respectively, are built on top of model-A. Here, we see that after the final LFAB in baseline model-A, one additional global feature acquiring block (GFAB) comprising of LSTM layer (model-B), and GRU layer (model-C) of 512 units is added to learn the global contextual correlations from the features engineered through the LFABs, as well as adjusting the global weights. GFAB is followed by a dropout layer of 50%. The FCNs configuration remains the same as model-A. We adopt GRU and LSTM architecture to obtain global long-term contextual representations in speech utterances. In an LSTM cell as shown in Fig.  4 , there are three gates: forget, input, and output gate. The three gates control the transfer of information into and out of the cell, and the cell retains values over different periods. Gates are a way to allow information to pass through selectively.  The forget gate ( t f ) determines what essential information is to retain from the preceding cell state ( t C ) and can be calculated using Eq. (  1 ). The input gate ( t I ) defines what pertinent information can be incorporated from the current time step, while the output gate ( t O ) defines the current hidden state that will be sent to the subsequent LSTM unit. The   3 ) and (  4 ) respectively. First, a sigmoid layer determines which value to update. After that, to regulate the network, a hyper tangent (tanh) layer generates a feature vector t C , with possible values between -1 and 1. The following step updates the information from the previous cell state to the new cell state through Eq. (  5 ). Usually, the length of the feature of frame-level speech changes with the number of speech frames. The LSTM learns deep global contextual features with fixed length by choosing the output of the last timestep from the variable-length frame-level speech features  (Xie et al., 2019) . Finally, the output is calculated using Eq. (  6 ) and (7) (J.  Zhao et al., 2019) .\n\n)\n\nWhere 1 t h -is the hidden later output at the previous timestep, t .  is the logistic sigmoid function which is calculated using Eq.\n\n(2).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "() X",
      "text": "x e  = +\n\n(2)\n\n)\n\n* tanh( ) The GRU is similar to the LSTM. It only has one hidden state compared to LSTM's two states: cell and hidden  (Chung, Gulcehre, Cho, & Bengio, 2014) . Due to the gating mechanisms, this hidden state can hold both long-term and short-term dependencies simultaneously. As shown in Fig.  5 , the GRU cell is a combination of two gates: update, and reset gates, but the internal structure is different from LSTM. While training, the gates learn what information is essential to retain or overlook. The update gate in the GRU replaces the forget and input gates of the LSTM. Reset gate aid in the capture of the sequence's momentary representations. The reset gate, update gate, candidate hidden state, and the final hidden state of the GRU can be calculated through the following equations (  8 ) to (11)  (Ravanelli, Brakel, Omologo, & Bengio, 2018) .\n\nThe output of the reset gate is obtained by multiplying the preceding hidden state\n\nSince GRU has fewer gates than LSTM, it is less complicated and faster to train. GRU should be used if the dataset is relatively small; otherwise, LSTM should be applied for large-volume datasets.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Proposed Weighted Ensemble Model-D",
      "text": "Ensemble learning (EL) combines the learning procedures of several models to achieve a more stable and comprehensive prediction with a maximum accuracy that is superior to the individual DL models' accuracy. Specific models are good at modeling one part of the data, and others are good at modeling another. EL succeeds because several models will not make identical errors in the same test dataset. It assures that the most accurate and reliable prediction is generated. Many features in the field of SER can reflect the emotion of speech. When the distinct advantages and accuracy of various SER-related models are merged and the features are combined, the recognition efficiency can be significantly enhanced. A weighted average ensemble was performed in this study by combining model-A, B, and C (see Fig.  6 ). At first, we select the optimal weights for each of the individual models through the Grid-Search technique. Then using the tensordot function of NumPy, we multiply the selected optimal weights with the prediction results of each model, calculate the sum of this product of elements over the specified axis to calculate the weighted prediction result, and then find the class with the largest predicted probability. Then from this weighted prediction result, the class with the largest predicted probability is chosen for the final prediction. The weighted average ensemble model-D, tested with the original dataset and augmented data, achieved higher weighted average accuracy (WAA) than the individual models-A, B, and C.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Experimental Analysis",
      "text": "Primarily, an SER framework is comprised of two components. The first one is the preprocessing component that obtains appropriate features from the speech utterances of the utilized datasets, and the second one is a classifier that uses those obtained features to execute the SER task. This section provides details about the datasets utilized in this study, data augmentation techniques, extracted features, and model training.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Datasets",
      "text": "To perform a meticulous evaluation of the proposed models, five datasets were explored covering two languages: English and German. We performed data augmentation in all the datasets since the number of samples in each of them is not significant for a DL-based model to train appropriately. A summary of each of them is provided below. Toronto Emotional Speech Set (TESS): TESS  (Pichora-Fuller et al., 2020)  is the first dataset that this study explored. It contains 200 target words. Those words were spoken by two English actresses, ages 26 & 64 years, respectively. The dataset is well balanced and contains 2800 audio files and depicts seven emotions: \"angry,\" \"neutral,\" \"happy,\" \"disgust,\" \"surprise,\" \"fear,\" and \"sad\". Note that this dataset has not been extensively used in SER studies previously. After performing data augmentation, the samples increased to 8400 samples. The average sample duration for all datasets is 2.8 seconds, with TESS being the outcast with an avg. period of 2.1 seconds. Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): RAVDESS  (Livingstone & Russo, 2018)  is one of the most explored datasets in the SER tasks. It includes both audio and video recordings of twelve male and twelve female actors reciting English sentences while exhibiting eight distinct emotional expressions. For this study, only the speech audio samples were utilized. The total number of audio files is 1440 with a sampling rate of 48 kHz, with 60 trials per actor. Only the speech audio sample from the dataset of the following eight categories are covered in this study: \"sad,\" \"happy,\" \"angry,\" \"calm,\" \"fearful,\" \"surprised,\" \"disgust,\" and \"neutral\". It is a balanced dataset though the \"neutral\" class has a smaller number of records compared to other classes. After performing data augmentation, the samples increased to 7200. Surrey Audio-Visual Expressed Emotion (SAVEE): SAVEE  (Haq & Jackson, 2014 ) consists of 480 speech utterances spoken by four English actors aged 27 to 31 years in seven diverse emotions: \"angry,\" \"happy,\" \"neutral,\" \"disgust,\" \"sad,\" \"fear,\" and \"surprise\" in a phonetically stable manner. The utterances are sampled at a rate of 44.1 kHz with a resolution of 16 bits However, this dataset has a class imbalance issue, with the \"neutral\" class being almost double compared to all the other classes. For this study, only the speech audio samples were utilized. Data augmentation increased the samples to 1920. Berlin Database of Emotional Speech (EMO-DB): EMO-DB  (Burkhardt et al., 2005)  is the most well-known and extensively used dataset in the SER research field. The utterances are sampled at a rate of 16 kHz with a resolution of 16 bits. It comprises 535 audio recordings in the German language categorized into seven emotional kinds: \"anger,\" \"fear,\" \"sadness,\" \"happiness,\" \"disgust,\" \"boredom,\" and \"neutral\". However, this dataset has a class imbalance issue, with the \"anger,\" class utterance number being large compared to other classes. With data augmentation, the samples increased to 2140. Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D): CREMA-D  (Cao et al., 2014)  is the least explored dataset in the SER research field. It uses 7442 recordings from ninety-one actors/actresses (48 male actors and 43 female actresses) from diverse races and customs, making it the most complicated to use. Actors spoke from a group of twelve sentences of six different emotional categories: \"angry,\" \"happy,\" \"neutral,\" \"disgust,\" \"fear,\" and \"sad\". Though the original number of samples is quite large compared to the other four datasets, it is still considered one of the most challenging datasets to work with because of its diverse number of male and female speakers. Data augmentation was also performed in this dataset with increased samples of 44652. Fig.  7  shows the number of the class-wise utterance of each of the datasets.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Data Augmentation",
      "text": "Frequently observed issues in the SER task include the insufficient size and class imbalance of datasets. As the complexity and scale of DNNs expand, a substantial dataset is required for their optimal performance. One solution is to increase the dataset using  diverse data augmentation (DA) techniques. DA is the method of applying minor modifications to our original training dataset to produce new artificial training samples. Since the number of speech utterance records in each class is relatively low, this study performs three types of audio DA, additive white gaussian noise (AWGN) injection, time-stretching, and pitch shifting in the audio files. The impact is more data for proper training of the models. The impact of these techniques is visually presented in Fig.  8 . The obtained signal with AWGN is equal to the transmitted signal with some added noise, which is statistically independent of the signal. AWGNs are random samples dispersed at consistent intervals with a mean value of zero and a standard deviation of one. We added AWGN to the samples by using NumPy's normal and uniform method with a rate of 0.020, and 0.025. We can adjust the speed or duration of a sound sample without changing the pitch by stretching time. We performed this task by using the time_stretch method of python's librosa library, with a factor of 0.7 and 0.8. We also changed the sound's pitch without affecting the speed. Pitch shifting was done by using the pitch_shift method of librosa, with a factor of 0.6 and 0.7. Several other studies have performed DA for the SER task using GAN-based methods  (Bao et al., 2019; Shilandari, Marvi, Khosravi, & Wang, 2022; Tiwari et al., 2020) . However, those augmentation methods did not yield higher SER performances. We present a comparative analysis of the SER performance utilizing the augmentation methods of this study with the existing augmentation methods in Table  9 . We augmented the datasets without degrading the SER system performance. After DA, the updated data samples are 8400, 7200, 1920, 2140, and 44652 for TESS, RAVDESS, SAVEE, EMO-DB, and CREMA-D datasets, respectively.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Feature Extraction",
      "text": "Extracting salient features from speech audio signals is one of the most important measures in SER-related activities. Precise extraction of crucial features improves the performance in terms of the SER accuracy of the model. Traditionally it is observed that low-level handcrafted features contain significant emotional cues about speech utterances and with proper feature engineering, work well with 1D CNN architecture (S.  Zhang, Tao, Chuang, & Zhao, 2021) . Properly configured 1D CNN architecture with a combination of LSTM, Bi-LSTM, GRU, and Bi-GRU architectures can perform effective feature engineering to acquire both local and global contextual cues from handcrafted speech features, and achieve excellent SER performance (J.  Zhao et al., 2019) . Specifically, this study uses five different spectral features: MFCC, LMS, ZCR, Chromagram, and RMS values of the speech audio files as the input for the proposed dilated 1D CNNs-FCNs, 1D CNNs-LSTM-FCNs, 1D CNNs-GRU-FCNs, and an ensemble of those three models. The brief details of the extracted features are given below.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Mel-Frequency Cepstral Coefficients (Mfcc):",
      "text": "Human-generated sounds are filtered through the vocal tract shape that includes tongue and teeth elements, which also is unique for each individual. The structure of these elements determines the voice of an individual. A precise measurement of the shape represents the phoneme being created. This shape is exhibited in the short-time power spectrum envelope, which is represented by MFCCs, and this feature is commonly used in SER research (Abdel-Hamid, 2020; Hajarolasvadi & Demirel, 2019; Z. T.  Liu, Xie, et al., 2018; Nantasri et al., 2020) . The MFCC feature extraction process is depicted in Fig.  9 . It starts with the speech signal being converted into a short frame of 20-30ms window, and every 10ms, it is advanced, allowing the temporal features of individual speech signals to be traced. Then Discrete Fourier Transform (DFT) is performed on every windowed frame, and they are converted into magnitude spectrum using Eq. (  12 ).\n\nHere, () hn is the hamming window, k which defines the DFT length, () xnrepresents the time-domain signal, i defines the frame number, and N defines the number of points used to calculate the DFT. After that, applying 26 filters in the previous signal the Mel-Scaled Filter-bank (MSFB) is calculated. MSFB is a measurement unit that is dependent on the frequency perception of the human ear. As a result, we have 26 numbers that describe the energy of each frame. The log energies are then calculated to obtain log filter-bank energies. The estimation of Mel from the physical frequency can be quantified through Eq. (  13 ).\n\nHere, f denotes the physical frequency (Hz) and Mel f denotes the frequency perception of the human ear. Finally, Discrete Cosine Transform (DCT) is performed to get the MFCCs from the log filter-bank energies. For this study, 13-lower dimensions MFCCs were extracted from each audio file. Envelopes are sufficient to reflect the differences between phonemes, allowing us to recognize phonemes using MFCC. The sampling rate was set at 44.1 kHz, with DCT-2.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Chromagram And Pitch:",
      "text": "The Chromagram is a time-frequency transformation of an acoustic signal into a briefly changing predecessor of the pitch and is used extensively in the SER task  (Birajdar & Patil, 2020; Issa et al., 2020) . It is related to the twelve diverse classes of the pitch. Applying Short-Time Fourier Transforms (STFT) to the waveform created from dataset audio files Chromagram features are collected. For this study, 12 Chromagram-bins were extracted from each audio file. The sound wave's frequencies determine the pitch feature in the SER task  (Noroozi, Sapiński, Kamińska, & Anbarjafari, 2017) . While the frequency is high, the pitch is considered high, and when the frequency is low, the pitch is considered as low too. In this study, the pitch factor was set at 0.6 and 0.7 during DA to create more samples for the training.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Log-Mel Spectrogram (Lms):",
      "text": "The spectrogram portrays a signal's intensity in terms of the time-frequency domain. A spectrogram is generated by dividing a time-domain signal into equal-length segments. After that, each segment is subjected to the fast Fourier transform (FFT). The spectrogram is a plot of each segment's spectrum. It is a significant feature for any speech-related classification task and performs exceptionally with CNN  (Hajarolasvadi & Demirel, 2019; Meng et al., 2019) . For this study, 128 LMS features were extracted from each audio file. The use of multiple audio features rather than just one integrates several sound characteristics such as pitch, tone, harmony, etc., into a single training speech. This gives the SER models a more detailed interpretation of a speech sound sample, which improves their performance. A few of the randomly selected waveforms of the dataset's recordings and their corresponding spectrogram, MFCC, and Chromagram features are graphically represented in Fig.  10 .\n\n-2 1 0 ( ) ( ) ( ) Zero Crossing Rate (ZCR): ZCR is widely used for SER as well as music information collection-related tasks  (Widiyanti & Endah, 2018) . ZCR measures the number of times the amplitude of speech signals passes through zero value in a given period. ZCR is the best way to tell the difference between voiced and unvoiced expressions. There is no authoritative low-frequency fluctuation where there are frequent zero crosses. Mathematically, ZCR can be defined through Eq. (  14 ), where s denotes the signal of length T and 0 1\n\n is an indicator function.\n\nRoot Mean Square (RMS) value: It computes the RMS value for each frame from the speech audio It performs an analysis of the overall amplitude of the signal, describing the average signal amplitude. RMS uses the magnitude of a signal as a measurement of signal strength, irrespective of the amplitude's positive or negative level. RMS and root mean square energy (RMSE) techniques are used by researchers  (Mustaqeem & Kwon, 2020b; Yi, Mak, & Member, 2020)  for speech audio that uses a signal's magnitude as a metric for signal power. For a given signal,\n\nThis study's total number of extracted features is 13 MFCC, 12 Chromagram, 128 LMS, and two ZCR and RMS features, creating a feature vector of dimension 155 (128+13+12+1+1=155).",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Model Training",
      "text": "After getting the feature vector, the study performs data normalization by calculating the mean and standard deviation of the features. Data is divided into training data and testing parts with an 80:20 proportion. Those data are then turned into arrays and fed to the DL model as input. Since we deal with categorical data, each label is given a specific number dependent on alphabetical order. 20% of the data is used for model validation, and the remaining 80% of the data is used to train the models. Every individual model-A, B, and C is trained on both original datasets and augmented datasets. The whole process was carried out using the Keras framework for DL  (Chollet, 2018) . Grid-Search was applied to tune the hyper-parameter such as optimizer, batch size, learning rate, and weights to define the optimal values for all four models. According to Grid-Search's output, the batch size is set at 32, and weights were calculated to produce the highest WAA in the ensemble model-D. 'Adam' was selected as the optimizer, and we have chosen 'categorical cross entropy' as the loss function. Each individual model-A, B, and C was trained for 1000 epochs on Tesla P100-PCIE GPU. The prediction results of model-A, B, and C are then weighted and the ensemble model-D performs the final prediction based on those weighted models' predictive results. Assigning optimal weights to each individual model-A, B, and C provides the best results for the ensemble model-D.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Results Analysis",
      "text": "The number of correctly classified speech emotion labels (True Positives-TP), correctly classified instances that do not belong to the speech emotion label (True Negatives-TN), and instances that were either incorrectly classified to the speech emotion label (False Positives-FP) or were not classified as the speech emotion label (False Negatives-FN) will all be used to evaluate the correctness of a speech emotion classification task. A confusion matrix is made up of these four measurements  (Sokolova & Lapalme, 2009) . In this section, we present a detailed analysis of our experimental results. We show the training vs. validation accuracy curve and confusion matrix of every experiment performed in all five datasets for all three individual model-A, B, C, and ensemble model-D. Since the ensemble model-D performed best, we present the confusion matrix for this model both before and after performing DA. The confusion matrix for model-A, B, and C represents the model's performance after DA only.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We analyze the performance of individual model-A, B, and C in terms of a weighted average (WA) score, since the datasets have an imbalanced distribution of classes. We also provide the precision, recall, and F1 score of the individual models in each dataset because of the class imbalance issues in those datasets. For the ensemble Model-D, we have utilized the WAA metric, which calculates accuracy by adjusting the weights of each model. For a distinct emotion label Li, we define the evaluation by TPi; TNi; FNi; FPi; and Accuracy, Precision, and Recall are computed from the counts for Li.\n\nThe accuracy metric presents the overall efficacy of the SER classifier  (Sokolova & Lapalme, 2009) .\n\nThe WAA metric computes the average accuracy by assigning weights to each individual model of the ensemble Model-D classifier (Z.  Zhao et al., 2021) .\n\nThe precision metric calculates the number of TPi recognition that fall into the positive speech emotion labels (TPi + FPi) in a multiclass SER task.\n\nThe recall represents the proportion of correctly recognized positive speech emotion labels across all labels.\n\nF1-score combines precision and recalls into a single metric that captures properties of both in a multiclass SER task.\n\nMacro-F1 calculates the F1-score for each class in the dataset and returns the average value without considering the percentage for each label without using any weights. All class is treated equally  (Prasanth, Roshni Thanka, Bijolin Edwin, & Nagaraj, 2021) .\n\nSimilarly, we can obtain the macro-recall and macro-precision scores by calculating the within-category values  (Tan, 2005) .",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Performance Analysis",
      "text": "All four models performed exceptionally well in each of the evaluated metrics. Table  2  presents the proposed individual model-A, B, and C's mean accuracy and ensemble model-D's weighted average accuracy in terms of SER performance with and without performing DA in each of the utilized datasets. At first, each model's performance is evaluated in the original dataset. All four models performed remarkably well in the original TESS dataset with over 96% mean weighted average accuracy. After performing DA, the performance improved further and achieved a WAA of 99.46% in the ensemble model-D. For the EMO-DB dataset, the SER performance of all four models was not up to the mark and drew the issue of overfitting, which is a common issue when deep models are trained on a relatively smaller size dataset. Another reason for this is the class distribution imbalance of the EMO-DB dataset and the low number of samples for the deep models to train efficiently. The same reason applies to the SAVEE dataset, with the WAA being poor for the original dataset with a very low number of samples for training the models. However, the performance of all the models significantly improved after performing DA and trained with an increased number of samples. The performance increased by around 32% from the non-augmented EMO-DB dataset and 22% from the non-augmented SAVEE dataset. In the EMO-DB dataset, the emotion \"neutral\" is often classified as \"boredom,\" after rechecking the audio files, we saw that the spectral entropy properties of these two categories are pretty similar; that is why all the models are misclassifying these two types. The same reasoning goes for the SAVEE dataset for the emotions \"happy\" and \"surprise.\" After rechecking, we found that some recordings with the \"happy\" labels are high pitched, almost the same as the emotion \"surprise.\" However, the highest WAA achieved by the ensemble model-D in the EMO-DB and SAVEE datasets is 95.42% and 93.22%, respectively. When tested against the RAVDESS dataset, the models performed well, with the highest WAA achieved in the original dataset, and the augmented dataset is 89.19% and 95.62%, respectively. CREMA-D is the least explored dataset in SER-related studies. It is challenging to work with because many actors and actresses from different races uttered different sentences in the dataset. The human accuracy of this dataset's audio-only part is around 40.9%. All four models exceeded that number, with the ensemble achieving around 68.14% WAA in the original dataset and 90.47% WAA in the augmented dataset. Tested against the original CREMA-D dataset, the \"neutral\" and \"disgust\" emotions are often misclassified as \"sad\" because both have a lower pitch and amplitude in the signal waveform; with similar spectral properties. When we trained all three individual models with the increased augmented data, all the discussed issues were resolved significantly. The accuracy curve for training versus validation after DA is also presented for all four models in Fig.  11 -15. The confusion matrix for the ensemble model-D is achieved by adjusting the proper weights of model-A, B, and C after performing a grid-search technique. In Fig.  16 , we provide the confusion matrix of the ensemble model-D to evaluate the performance before and after data augmentation in each of the utilized datasets.   Table  3  presents the class-wise SER performance of each individual model-A, B, and C after performing DA in terms of precision, recall, F1-score, mean accuracy, average macro-precision, macro-recall, and macro-F1 for the TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D datasets. Fig.  17  illustrates the overall performance of model-A, B, and C in terms of their average macro precision, macro recall, mean accuracy, and macro F1-score values. Macro precision and macro recall values are significantly high for the TESS and RAVDES datasets. One reason is that all the models converge well in those datasets because of their superior balanced class distribution compared to EMO-DB and SAVEE datasets. Baseline model-A achieves the highest macro precision score in all the utilized datasets. In terms of macro recall, model-B achieves the highest results apart from the CREMA-D dataset only. Model-C's overall performance is lower than the other models, especially in the EMO-DB and SAVEE datasets. Model-B achieves the highest mean accuracy and macro F1-score value in all the utilized datasets, except for the CREMA-D dataset. However, for the CREMA-D dataset, model-A performs significantly better than other individual models. It is observed that the performance of model-C is poor compared to the other two individual models in both mean accuracy and macro F1-score metrics.",
      "page_start": 16,
      "page_end": 20
    },
    {
      "section_name": "Comparative Analysis With Other Methods",
      "text": "There has already been extensive research in the field of SER. However, comparing performance was tough because only a few performed DA in those datasets for SER  (Padi, Manocha, & Sriram, 2020; Tiwari et al., 2020; Yi et al., 2020)  using GAN or multiwindow method, and by adding generative noise. We have identified that augmenting data is necessary for the utilized datasets because the sizes of these datasets are significantly lesser for the proper training of a DL-based model. Besides that, only a few bits of literature utilized the TESS and CREMA-D datasets for this task. Some of the existing articles  (Demircan & Kahramanli, 2018; Hajarolasvadi & Demirel, 2019; S. Li et al., 2021)  use only a subset of those datasets; some perform feature extraction from the audio, text, and video samples of those datasets  (Ristea, Dutu, & Radoi, 2019; Yoon et al., 2018) . Our scope in this research is only audio samples. Some  (Chen, He, Yang, & Zhang, 2018; Kim, Englebienne, Truong, & Evers, 2017; Mustaqeem & Kwon, 2021c)  evaluate their framework's performance using a different metric from ours, like unweighted average accuracy, recall (K.  Feng & Chaspari, 2020; Meng et al., 2019) ; Some choose a questionable training and testing split ratio of 90:10  (Bhavan et al., 2019) ; therefore, we only compare with those articles that match our criterion.\n\nIn Table  4 -8, we present the performance comparison of our work with previous work for TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D datasets, respectively. Table  9  compares our work with those articles adopting different data augmentation methods to increase the SER accuracy using the utilized datasets of this study. The comparison shows that this study's data augmentation approach uses a lesser feature dimension than other methods, providing improved results.\n\nIn addition to the better SER performance achieved by the proposed DL-based model-A, B, C and weighted ensemble model-D in all the utilized datasets (see Table  2 3 4 5 6 7 8 9 ), the training time complexity and size of our proposed models are relatively lightweight and occupy less memory compared to other reported SOTA SER architectures. The training time of model-A, B, and C are 3685 s, 3929 s, and 3798 s on the TESS dataset, 23429 s, 25170 s, and 25475 s on the CREMA-D dataset, 972 s, 989 s, and 981 s on the EMO-DB dataset, 3038 s, 3241 s, and 3108 s on the RAVDESS dataset, and 919 s, 931 s, and 922 s on the SAVEE dataset, respectively. Though it should be noted that, training time varies based on the utilized GPU's and allocated memory. In terms of model size, all three proposed models-A, B, and C are less memory consuming compared to reported SER benchmarks such as CB-SER  (Mustaqeem et al., 2020) , ADRNN  (Meng et al., 2019) , ATT-Net  (Mustaqeem & Kwon, 2021b) , ACRNN  (Chen et al., 2018) , QCNN  (Muppidi & Radfar, 2021) , and DSCNN  (Mustaqeem & Kwon, 2020a)  Due to the increased model complexity used in the individual models, we trade off the ensemble model-D's evaluation time for robust SER performance, which can only be completed after training all three separate model-A, B, and C. For all datasets, each of the proposed individual models exhibits better generalization during the experimental assessment, ensuring higher recognition accuracy with minimal computation cost. The lightweight property makes all the models suitable for real-time applications for human-computer interaction. Among all four models, ensemble model-D performed best in terms of SER accuracy in all the datasets. The individual model's excellent performance in detecting emotion from speech across all five datasets and adjusting the proper weights for each model for the ensemble prediction contributes mainly to the improved recognition rate of weighted ensemble model-D.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Conclusion And Future Works",
      "text": "Inadequate data could prohibit any DNN-based model from achieving its maximum ability, which is a significant challenge in the DL-based SER task. The lack of data samples often leads a deep and complex model to suffer from overfitting issue. This paper presents a comprehensive study of different DL-based SER systems utilizing five different datasets, covering two languages: English and German. We have handcrafted five types of LLD features from each audio file. We have designed multiple LFABs inside the baseline model-A to learn local hidden features of the speech signals. An additional GFAB is added to both model-B and model-C that extracts long-term global contextual dependencies and correlations from the learned features of LFABs. The effectiveness of a weighted ensemble setting of three new DL-based models is assessed on five standard benchmark SER datasets. With data augmentation, the result of the proposed weighted ensemble model-D is significant, achieving a SOTA WAA of 99.46%, 95.42%, 95.62%, 93.22%, and 90.47% for the TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D datasets respectively.\n\nAlthough there have been steady advancements in methods, features, and obtained accuracy in SER, many limitations are yet to be addressed for an effective and industry-grade SER scheme. The majority of the datasets are acted, scripted, and only cover a few discrete statements and expressions throughout the corpus. There can be major differences between working with real and acted data. Moreover, in most cases, the sample size in those datasets is insufficient to adequately train a DL-based model. Experiments conducted to create those datasets are simulated and semi-natural. They are not noisy and are far away from the natural environment in a real-world scenario. This brings questions about the ability of a developed system which are built using those datasets to detect the correct emotion in a real-world noisy scenario. The detection of emotions in a dialogue between multiple actors from continuous speech audio is an area that needs further research. A possible extension of this research is to develop a multi-label problem where an utterance in any conversation often contains multiple emotion types. The fusion of information from multi-modal CNN architectures that capture different optimal acoustic features from speech signals needs more addressing and further investigation. Most of the utilized acoustic features contain information about magnitude and phase. However, the traditional SER system mostly focuses on the magnitude information only. The exploration of effective phase-based features is another point of research direction. Even though this study performs exceptionally well in SER across five datasets, we believe that further analysis on this subject is necessary. In the future, we hope to reduce the training time needed for the individual models to make an ensemble prediction by focusing more on the optimal feature selection method and integrating different attention mechanisms to get more optimal cues for the SER task.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Credit Authorship Contribution Statement",
      "text": "",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In the first stage",
      "page": 1
    },
    {
      "caption": "Figure 1: A graphical illustration of the generic workflow of the SER systems.",
      "page": 2
    },
    {
      "caption": "Figure 2: summarizes our approach to an ensemble architecture for the SER task.",
      "page": 7
    },
    {
      "caption": "Figure 2: A graphical representation of the framework of the proposed ensemble architecture.",
      "page": 7
    },
    {
      "caption": "Figure 4: and 5 respectively, are built on top of model-A. Here, we see that after the",
      "page": 8
    },
    {
      "caption": "Figure 4: , there are three gates: forget, input, and output gate. The three gates control the transfer of information",
      "page": 8
    },
    {
      "caption": "Figure 3: The architecture of the proposed baseline model-A (1D CNNs-FCNs).",
      "page": 8
    },
    {
      "caption": "Figure 4: The architecture of the proposed model-B (1D CNNs-LSTM-FCNs).",
      "page": 8
    },
    {
      "caption": "Figure 5: , the GRU cell is a combination of two gates: update, and reset gates, but the internal structure",
      "page": 9
    },
    {
      "caption": "Figure 6: ). At first, we select the optimal weights for each of the individual models",
      "page": 10
    },
    {
      "caption": "Figure 6: Visual representation of the proposed weighted ensemble model-D.",
      "page": 10
    },
    {
      "caption": "Figure 5: The architecture of the proposed model-C (1D CNNs-GRU-FCNs).",
      "page": 10
    },
    {
      "caption": "Figure 7: Class-wise utterance distribution in all five datasets, (a) CREMA-D, (b) RAVDESS, (c) SAVEE, (d) EMO-DB, (e) TESS, and (f) Combined.",
      "page": 11
    },
    {
      "caption": "Figure 8: A pictorial illustration of how various data augmentation techniques impacts speech utterances. Here, (a) is the original sound waveform, (b) is the AWGN",
      "page": 12
    },
    {
      "caption": "Figure 9: It starts with the speech signal being converted into a short frame of 20-30ms window, and every 10ms, it is advanced,",
      "page": 12
    },
    {
      "caption": "Figure 9: A graphical illustration of the MFCC feature extraction process.",
      "page": 13
    },
    {
      "caption": "Figure 10: Speech audio waveforms, and graphical representations of the spectrogram, MFCC, and Chromagram features of a few of the randomly selected",
      "page": 14
    },
    {
      "caption": "Figure 11: -15. The confusion matrix for the ensemble model-D is achieved by adjusting the proper",
      "page": 16
    },
    {
      "caption": "Figure 16: , we provide the confusion matrix of the ensemble",
      "page": 16
    },
    {
      "caption": "Figure 11: Performance evaluation (after performing DA) of the proposed models in the TESS dataset. (a), (b), and (c) show the training vs. validation accuracy curve",
      "page": 17
    },
    {
      "caption": "Figure 12: Performance evaluation (after performing DA) of the proposed models in the EMO-DB dataset. (a), (b), and (c) show the training vs. validation accuracy",
      "page": 17
    },
    {
      "caption": "Figure 13: Performance evaluation (after performing DA) of the proposed models in the RAVDESS dataset. (a), (b), and (c) show the training vs. validation accuracy",
      "page": 17
    },
    {
      "caption": "Figure 14: Performance evaluation (after performing DA) of the proposed models in the SAVEE dataset. (a), (b), and (c) show the training vs. validation accuracy",
      "page": 18
    },
    {
      "caption": "Figure 15: Performance evaluation (after performing DA) of the proposed models in the CREMA-D dataset. (a), (b), and (c) shows the training vs. validation",
      "page": 18
    },
    {
      "caption": "Figure 16: Performance evaluation of the proposed ensemble model-D both before and after performing data augmentation (DA) in the utilized datasets. Here,",
      "page": 20
    },
    {
      "caption": "Figure 17: illustrates the overall performance of model-A, B, and C in terms of their average macro precision, macro",
      "page": 20
    },
    {
      "caption": "Figure 17: Graphical bar plot depictions of the individual models - A, B, and C in terms of (a) macro precision (b) macro recall (c) mean accuracy, and (d) macro F1-",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Comparison of all four proposed models based on the SER performance on TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D datasets.",
      "data": [
        {
          "Datasets": "",
          "Model Name": "",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "Without data augmentation (%)"
        },
        {
          "Datasets": "TESS",
          "Model Name": "Model-A",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "96.78"
        },
        {
          "Datasets": "",
          "Model Name": "Model-B",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "96.00"
        },
        {
          "Datasets": "",
          "Model Name": "Model-C",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "95.68"
        },
        {
          "Datasets": "",
          "Model Name": "Weighted Ensemble model-D",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "98.00"
        },
        {
          "Datasets": "EMO-DB",
          "Model Name": "Model-A",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "65.88"
        },
        {
          "Datasets": "",
          "Model Name": "Model-B",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "64.32"
        },
        {
          "Datasets": "",
          "Model Name": "Model-C",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "65.18"
        },
        {
          "Datasets": "",
          "Model Name": "Weighted Ensemble model-D",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "67.74"
        },
        {
          "Datasets": "RAVDESS",
          "Model Name": "Model-A",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "86.11"
        },
        {
          "Datasets": "",
          "Model Name": "Model-B",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "88.54"
        },
        {
          "Datasets": "",
          "Model Name": "Model-C",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "86.77"
        },
        {
          "Datasets": "",
          "Model Name": "Weighted Ensemble model-D",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "89.19"
        },
        {
          "Datasets": "SAVEE",
          "Model Name": "Model-A",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "68.00"
        },
        {
          "Datasets": "",
          "Model Name": "Model-B",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "65.87"
        },
        {
          "Datasets": "",
          "Model Name": "Model-C",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "68.14"
        },
        {
          "Datasets": "",
          "Model Name": "Weighted Ensemble model-D",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "71.00"
        },
        {
          "Datasets": "CREMA-D",
          "Model Name": "Model-A",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "66.60"
        },
        {
          "Datasets": "",
          "Model Name": "Model-B",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "66.00"
        },
        {
          "Datasets": "",
          "Model Name": "Model-C",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "65.88"
        },
        {
          "Datasets": "",
          "Model Name": "Weighted Ensemble model-D",
          "Mean Accuracy (Model-A, B, C) / Weighted Average Accuracy (WAA) \n(Ensemble Model-D)": "68.14"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 2: Comparison of all four proposed models based on the SER performance on TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D datasets.",
      "data": [
        {
          "TESS dataset": "Category"
        },
        {
          "TESS dataset": ""
        },
        {
          "TESS dataset": "Angry"
        },
        {
          "TESS dataset": "Disgust"
        },
        {
          "TESS dataset": "Fear"
        },
        {
          "TESS dataset": "Happy"
        },
        {
          "TESS dataset": "Neutral"
        },
        {
          "TESS dataset": "Sad"
        },
        {
          "TESS dataset": "Surprise"
        },
        {
          "TESS dataset": "Macro \nAverage"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EMO-DB dataset": "Angry"
        },
        {
          "EMO-DB dataset": "Boredom"
        },
        {
          "EMO-DB dataset": "Disgust"
        },
        {
          "EMO-DB dataset": "Fear"
        },
        {
          "EMO-DB dataset": "Happy"
        },
        {
          "EMO-DB dataset": "Neutral"
        },
        {
          "EMO-DB dataset": "Sadness"
        },
        {
          "EMO-DB dataset": "Macro \nAverage"
        },
        {
          "EMO-DB dataset": "RAVDESS dataset"
        },
        {
          "EMO-DB dataset": "Angry"
        },
        {
          "EMO-DB dataset": "Calm"
        },
        {
          "EMO-DB dataset": "Disgust"
        },
        {
          "EMO-DB dataset": "Fear"
        },
        {
          "EMO-DB dataset": "Happy"
        },
        {
          "EMO-DB dataset": "Neutral"
        },
        {
          "EMO-DB dataset": "Sad"
        },
        {
          "EMO-DB dataset": "Surprise"
        },
        {
          "EMO-DB dataset": "Macro \nAverage"
        },
        {
          "EMO-DB dataset": "SAVEE dataset"
        },
        {
          "EMO-DB dataset": "Angry"
        },
        {
          "EMO-DB dataset": "Disgust"
        },
        {
          "EMO-DB dataset": "Fear"
        },
        {
          "EMO-DB dataset": "Happy"
        },
        {
          "EMO-DB dataset": "Neutral"
        },
        {
          "EMO-DB dataset": "Sad"
        },
        {
          "EMO-DB dataset": "Surprise"
        },
        {
          "EMO-DB dataset": "Macro \nAverage"
        },
        {
          "EMO-DB dataset": "CREMA-D dataset"
        },
        {
          "EMO-DB dataset": "Angry"
        },
        {
          "EMO-DB dataset": "Disgust"
        },
        {
          "EMO-DB dataset": "Fear"
        },
        {
          "EMO-DB dataset": "Happy"
        },
        {
          "EMO-DB dataset": "Neutral"
        },
        {
          "EMO-DB dataset": "Sad"
        },
        {
          "EMO-DB dataset": "Macro \nAverage"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 5: Performance comparison of this work with recent literature in the EMO-DB dataset.",
      "data": [
        {
          "Reference": "(Mekruksavanich, Jitpattanakul, & Hnoohom, \n2020)",
          "Methodology": "DCNN",
          "Features": "MFCC",
          "Accuracy": "55.71%"
        },
        {
          "Reference": "(R. Chatterjee et al., 2021)",
          "Methodology": "1D-CNN",
          "Features": "MSFB-Cepstral Coefficients",
          "Accuracy": "95.79%"
        },
        {
          "Reference": "(Praseetha & Vadivel, 2018)",
          "Methodology": "DNN, RNN, GRU",
          "Features": "MFCC, LMS",
          "Accuracy": "95.82%"
        },
        {
          "Reference": "(Aggarwal et al., 2022)",
          "Methodology": "DNN, VGG-16",
          "Features": "2D LMS",
          "Accuracy": "97.15%"
        },
        {
          "Reference": "(Venkataramanan & Rajamohan, 2019)",
          "Methodology": "2D CNN",
          "Features": "LMS",
          "Accuracy": "62.00%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNNs-FCNs",
          "Features": "MFCC, LMS, ZCR, Chromagram, \nand RMS value",
          "Accuracy": "99.05%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-LSTM-FCNs",
          "Features": "",
          "Accuracy": "98.40%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-GRU-FCNs",
          "Features": "",
          "Accuracy": "98.10%"
        },
        {
          "Reference": "",
          "Methodology": "Ensemble Model-D",
          "Features": "",
          "Accuracy": "99.46%"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 5: Performance comparison of this work with recent literature in the EMO-DB dataset.",
      "data": [
        {
          "Reference": "(Issa et al., 2020)",
          "Methodology": "1D CNN",
          "Features": "MFCC, LMS, Chromagram, Spectral \ncontrast, Tonnetz",
          "Accuracy": "86.10%"
        },
        {
          "Reference": "(Tiwari et al., 2020)",
          "Methodology": "DNN",
          "Features": "ZCR, RMS energy, MFCC, and \nstatistical features",
          "Accuracy": "82.73%"
        },
        {
          "Reference": "(Yadav & Vishwakarma, 2020)",
          "Methodology": "1D CNN, Bi-LSTM",
          "Features": "Acoustic features",
          "Accuracy": "94.00%"
        },
        {
          "Reference": "(J. Zhao et al., 2019)",
          "Methodology": "1D-2D DCNN, LSTM",
          "Features": "Spectral features",
          "Accuracy": "95.33%"
        },
        {
          "Reference": "(Anvarjon et al., 2020)",
          "Methodology": "2D CNN",
          "Features": "LMS",
          "Accuracy": "92.02%"
        },
        {
          "Reference": "(Mustaqeem et al., 2020)",
          "Methodology": "Bi-LSTM",
          "Features": "LMS",
          "Accuracy": "85.57%"
        },
        {
          "Reference": "(Mustaqeem & Kwon, 2021b)",
          "Methodology": "CNN, Channel Attention",
          "Features": "LMS",
          "Accuracy": "93.00%"
        },
        {
          "Reference": "(D. Li et al., 2021)",
          "Methodology": "Bi-LSTM",
          "Features": "MFCC, Spectral centroid, roll-off, \nflux, and spread, ZCR, RMS, \nChromagram, Pitch, entropy",
          "Accuracy": "85.95%"
        },
        {
          "Reference": "(Ancilin & Milton, 2021)",
          "Methodology": "SVM",
          "Features": "Mel Frequency Magnitude \nCoefficient",
          "Accuracy": "81.50%"
        },
        {
          "Reference": "(Farooq et al., 2020)",
          "Methodology": "DCNN, SVM, MLP",
          "Features": "LMS",
          "Accuracy": "95.10%"
        },
        {
          "Reference": "(Nantasri et al., 2020)",
          "Methodology": "ANN",
          "Features": "MFCCs, Delta, Delta-Deltas",
          "Accuracy": "87.80%"
        },
        {
          "Reference": "(Yi et al., 2020)",
          "Methodology": "DNN, SVM, GAN, Autoencoder",
          "Features": "MFCC, ZCR, RMS",
          "Accuracy": "84.49%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNNs-FCNs",
          "Features": "MFCC, LMS, ZCR, Chromagram, and \nRMS value",
          "Accuracy": "92.26%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-LSTM-FCNs",
          "Features": "",
          "Accuracy": "92.38%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-GRU-FCNs",
          "Features": "",
          "Accuracy": "91.66%"
        },
        {
          "Reference": "",
          "Methodology": "Ensemble Model-D",
          "Features": "",
          "Accuracy": "95.42%"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 5: Performance comparison of this work with recent literature in the EMO-DB dataset.",
      "data": [
        {
          "Reference": "(Issa et al., 2020)",
          "Methodology": "1D CNN",
          "Features": "MFCC, LMS, Chromagram, Spectral \ncontrast, Tonnetz",
          "Accuracy": "71.61%"
        },
        {
          "Reference": "(Yadav & Vishwakarma, 2020)",
          "Methodology": "1D CNN, Bi-LSTM",
          "Features": "Acoustic features",
          "Accuracy": "73.00%"
        },
        {
          "Reference": "(Mekruksavanich et al., 2020)",
          "Methodology": "DCNN",
          "Features": "MFCC",
          "Accuracy": "75.83%"
        },
        {
          "Reference": "(Farooq et al., 2020)",
          "Methodology": "DCNN, SVM, MLP",
          "Features": "LMS",
          "Accuracy": "81.30%"
        },
        {
          "Reference": "(Padi et al., 2020)",
          "Methodology": "CNN",
          "Features": "MFCC, Chromagram, and Time-\ndomain features",
          "Accuracy": "88.00%"
        },
        {
          "Reference": "(Nantasri et al., 2020)",
          "Methodology": "ANN",
          "Features": "MFCCs, Delta, Delta-Deltas",
          "Accuracy": "82.30%"
        },
        {
          "Reference": "(Mustaqeem et al., 2020)",
          "Methodology": "Bi-LSTM",
          "Features": "LMS",
          "Accuracy": "77.02%"
        },
        {
          "Reference": "(Mustaqeem & Kwon, 2020b)",
          "Methodology": "1D CNN",
          "Features": "-",
          "Accuracy": "80.00%"
        },
        {
          "Reference": "(Mustaqeem & Kwon, 2021b)",
          "Methodology": "CNN, Channel Attention",
          "Features": "LMS",
          "Accuracy": "80.00%"
        },
        {
          "Reference": "(Ancilin & Milton, 2021)",
          "Methodology": "SVM",
          "Features": "Mel frequency magnitude \ncoefficient",
          "Accuracy": "64.31%"
        },
        {
          "Reference": "(Aggarwal et al., 2022)",
          "Methodology": "DNN",
          "Features": "MFCC, Chromagram, LMS, Spectral \ncentroid and roll-off",
          "Accuracy": "73.95%"
        },
        {
          "Reference": "This Work",
          "Methodology": "1D CNNs-FCNs",
          "Features": "MFCC, LMS, ZCR, Chromagram, and \nRMS value",
          "Accuracy": "94.38%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-LSTM-FCNs",
          "Features": "",
          "Accuracy": "94.00%"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 7: Performance comparison of this work with recent literature in the SAVEE dataset.",
      "data": [
        {
          "Reference": "(Hajarolasvadi & Demirel, 2019)",
          "Methodology": "3D CNN",
          "Features": "LMS",
          "Accuracy": "81.05%"
        },
        {
          "Reference": "(Farooq et al., 2020)",
          "Methodology": "DCNN, SVM, MLP",
          "Features": "LMS",
          "Accuracy": "82.10%"
        },
        {
          "Reference": "(Padi et al., 2020)",
          "Methodology": "CNN",
          "Features": "MFCC, Chromagram, and Time-\ndomain features",
          "Accuracy": "70.00%"
        },
        {
          "Reference": "(Ancilin & Milton, 2021)",
          "Methodology": "SVM",
          "Features": "Mel frequency magnitude \ncoefficient",
          "Accuracy": "75.63%"
        },
        {
          "Reference": "(Mekruksavanich et al., 2020)",
          "Methodology": "DCNN",
          "Features": "MFCC",
          "Accuracy": "65.83%"
        },
        {
          "Reference": "(Z. T. Liu, Xie, et al., 2018)",
          "Methodology": "GA, PCA, LLD",
          "Features": "MFCC",
          "Accuracy": "76.40%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNNs-FCNs",
          "Features": "MFCC, LMS, ZCR, Chromagram, and \nRMS value",
          "Accuracy": "92.00%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-LSTM-FCNs",
          "Features": "",
          "Accuracy": "92.70%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-GRU-FCNs",
          "Features": "",
          "Accuracy": "88.28%"
        },
        {
          "Reference": "",
          "Methodology": "Ensemble Model-D",
          "Features": "",
          "Accuracy": "93.22%"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 7: Performance comparison of this work with recent literature in the SAVEE dataset.",
      "data": [
        {
          "Reference": "(Mekruksavanich et al., 2020)",
          "Methodology": "DCNN",
          "Features": "MFCC",
          "Accuracy": "65.77%"
        },
        {
          "Reference": "(Singh et al., 2020)",
          "Methodology": "SVM",
          "Features": "MFCC, ZCR, RMSE",
          "Accuracy": "58.22%"
        },
        {
          "Reference": "(Scheidwasser-clow, Kegler, Beckmann, \nCernak, & Epfl, 2022)",
          "Methodology": "Convolutional Transformer",
          "Features": "-",
          "Accuracy": "76.90%"
        },
        {
          "Reference": "(Mocanu & Tapu, 2021)",
          "Methodology": "CNN",
          "Features": "Acoustic features",
          "Accuracy": "64.85%"
        },
        {
          "Reference": "(Huang, Tao, Liu, & Lian, 2020)",
          "Methodology": "LSTM, Vector of Locally Aggregated \nDescriptors",
          "Features": "MFCC, Low level \ndescriptors",
          "Accuracy": "63.50%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNNs-FCNs",
          "Features": "MFCC, LMS, ZCR, \nChromagram, and RMS \nvalue",
          "Accuracy": "90.22%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-LSTM-FCNs",
          "Features": "",
          "Accuracy": "84.27%"
        },
        {
          "Reference": "",
          "Methodology": "1D CNNs-GRU-FCNs",
          "Features": "",
          "Accuracy": "84.39%"
        },
        {
          "Reference": "",
          "Methodology": "Ensemble Model-D",
          "Features": "",
          "Accuracy": "90.47%"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 7: Performance comparison of this work with recent literature in the SAVEE dataset.",
      "data": [
        {
          "Reference": "(Tiwari et al., 2020)",
          "Methodology": "DNN",
          "Datasets": "EMO-DB",
          "Mean \nAccuracy/WAA": "76.77%",
          "Features": "ZCR, RMS energy, MFCC, \nand statistical features",
          "Feature \ndimension": "6552",
          "Data augmentation \nmethod": "Generative noise \nmodel"
        },
        {
          "Reference": "(Yi et al., 2020)",
          "Methodology": "DNN, SVM, GAN, Auto \nencoder",
          "Datasets": "EMO-DB",
          "Mean \nAccuracy/WAA": "84.49%",
          "Features": "MFCC, ZCR, RMS",
          "Feature \ndimension": "4368",
          "Data augmentation \nmethod": "Adversarial data \naugmentation \nnetwork"
        },
        {
          "Reference": "(S. Zhang et al., \n2018)",
          "Methodology": "Deep CNN",
          "Datasets": "EMO-DB",
          "Mean \nAccuracy/WAA": "87.31%",
          "Features": "2D LMS",
          "Feature \ndimension": "-",
          "Data augmentation \nmethod": "Increased overlap \nlength of speech \nsignals"
        },
        {
          "Reference": "(T. Feng, Hashemi, \nAnnavaram, & \nNarayanan, 2022)",
          "Methodology": "CNN, Adversarial \nlearning",
          "Datasets": "CREMA-D",
          "Mean \nAccuracy/WAA": "69.80%",
          "Features": "2D LMS",
          "Feature \ndimension": "-",
          "Data augmentation \nmethod": "Addition of AWGN"
        },
        {
          "Reference": "(Praseetha & Joby, \n2021)",
          "Methodology": "GRU",
          "Datasets": "TESS",
          "Mean \nAccuracy/WAA": "93.00%",
          "Features": "Filter-bank Energies",
          "Feature \ndimension": "-",
          "Data augmentation \nmethod": "Tempo and speed \nperturbation"
        },
        {
          "Reference": "(Padi et al., 2020)",
          "Methodology": "CNN",
          "Datasets": "SAVEE",
          "Mean \nAccuracy/WAA": "70.00%",
          "Features": "MFCC, Chromagram, and \nTime-domain features",
          "Feature \ndimension": "34",
          "Data augmentation \nmethod": "Multi-Window based \nmethod"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Datasets": "RAVDESS",
          "Mean \nAccuracy/WAA": "88.00%",
          "Features": "",
          "Feature \ndimension": "",
          "Data augmentation \nmethod": ""
        },
        {
          "Reference": "",
          "Methodology": "CNN, LSTM",
          "Datasets": "RAVDESS",
          "Mean \nAccuracy/WAA": "92.60%",
          "Features": "MFCC, ZCR, RMS",
          "Feature \ndimension": "",
          "Data augmentation \nmethod": ""
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "SAVEE",
          "89.90%": "84.90%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "TESS",
          "89.90%": "99.60%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "(Lalitha, Gupta, \nZakariah, & Alotaibi, \n2020)",
          "CREMA-D": "EMO-DB",
          "89.90%": "87.30%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": "Synthetic Minority \nOver-sampling \nTechnique (SMOTE)"
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "SAVEE",
          "89.90%": "75.20%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "This work",
          "CREMA-D": "EMO-DB",
          "89.90%": "92.26%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": "Injecting AWGN, \nstretching the speech \naudio files, and \nmodification of the \npitch of the sound"
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "92.38%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "91.66%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "95.42%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "SAVEE",
          "89.90%": "92.00%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "92.70%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "88.28%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "93.22%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "RAVDESS",
          "89.90%": "94.38%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "94.00%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "93.86%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "95.62%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "CREMA-D",
          "89.90%": "90.22%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "84.27%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "84.39%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        },
        {
          "(Jothimani, S and \nPremalatha, 2022)": "",
          "CREMA-D": "",
          "89.90%": "90.47%",
          "Noise Removal, White \nNoise Injection, and \nPitch Tuning": ""
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors (Switzerland)",
      "doi": "10.3390/s21041249"
    },
    {
      "citation_id": "2",
      "title": "Egyptian Arabic speech emotion recognition using prosodic, spectral and wavelet features",
      "authors": [
        "L Abdel-Hamid"
      ],
      "year": "2020",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2020.04.005"
    },
    {
      "citation_id": "3",
      "title": "Two-Way Feature Extraction for Speech Emotion Recognition Using Deep Learning",
      "authors": [
        "A Aggarwal",
        "A Srivastava",
        "A Agarwal",
        "N Chahal"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2019",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2019.12.001"
    },
    {
      "citation_id": "5",
      "title": "Human-Computer Interaction with Detection of Speaker Emotions Using Convolution Neural Networks",
      "authors": [
        "A Alnuaim",
        "M Zakariah",
        "A Alhadlaq"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience",
      "doi": "10.1155/2022/7463091"
    },
    {
      "citation_id": "6",
      "title": "Improved speech emotion recognition with Mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "A Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics",
      "doi": "10.1016/j.apacoust.2021.108046"
    },
    {
      "citation_id": "7",
      "title": "Deep-net: A lightweight cnn-based speech emotion recognition system using deep frequency features",
      "authors": [
        "T Anvarjon",
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors (Switzerland)",
      "doi": "10.3390/s20185212"
    },
    {
      "citation_id": "8",
      "title": "Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network. 2017 International Conference on Platform Technology and Service",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network. 2017 International Conference on Platform Technology and Service",
      "doi": "10.1109/PlatCon.2017.7883728"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised feature selection and NMF de-noising for robust Speech Emotion Recognition",
      "authors": [
        "S Bandela",
        "T Kumar"
      ],
      "year": "2021",
      "venue": "Applied Acoustics",
      "doi": "10.1016/j.apacoust.2020.107645"
    },
    {
      "citation_id": "10",
      "title": "CycleGAN-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",
      "doi": "10.21437/Interspeech.2019-2293"
    },
    {
      "citation_id": "11",
      "title": "Bagged support vector machines for emotion recognition from speech. Knowledge-Based Systems",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "Hitkul",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Bagged support vector machines for emotion recognition from speech. Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2019.104886"
    },
    {
      "citation_id": "12",
      "title": "Speech/music classification using visual and spectral chromagram features",
      "authors": [
        "G Birajdar",
        "M Patil"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing",
      "doi": "10.1007/s12652-019-01303-4"
    },
    {
      "citation_id": "13",
      "title": "A database of German emotional speech. 9th European Conference on Speech Communication and Technology",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech. 9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "14",
      "title": "CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244.CREMA-D"
    },
    {
      "citation_id": "15",
      "title": "Ensemble Learning by High-Dimensional Acoustic Features for Emotion Recognition from Speech Audio Signal. Security and Communication Networks",
      "authors": [
        "M Chalapathi",
        "M Kumar",
        "N Sharma",
        "S Shitharth"
      ],
      "year": "2022",
      "venue": "Ensemble Learning by High-Dimensional Acoustic Features for Emotion Recognition from Speech Audio Signal. Security and Communication Networks",
      "doi": "10.1155/2022/8777026"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using cross-correlation and acoustic features",
      "authors": [
        "J Chatterjee",
        "V Mukesh",
        "H Hsu",
        "G Vyas",
        "Z Liu"
      ],
      "year": "2018",
      "venue": "Proceedings -IEEE 16th International Conference on Dependable, Autonomic and Secure Computing, IEEE 16th International Conference on Pervasive Intelligence and Computing, IEEE 4th International Conference on Big Data Intelligence and Computing and IEEE",
      "doi": "10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00050"
    },
    {
      "citation_id": "17",
      "title": "Real-time speech emotion analysis for smart home assistants",
      "authors": [
        "R Chatterjee",
        "S Mazumdar",
        "R Sherratt",
        "R Halder",
        "T Maitra",
        "D Giri"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Consumer Electronics",
      "doi": "10.1109/TCE.2021.3056421"
    },
    {
      "citation_id": "18",
      "title": "Data augmentation using GANs for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2019-Septe",
      "doi": "10.21437/Interspeech.2019-2561"
    },
    {
      "citation_id": "19",
      "title": "3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2018.2860246"
    },
    {
      "citation_id": "20",
      "title": "Keras: The python deep learning library",
      "authors": [
        "F Chollet"
      ],
      "year": "2018",
      "venue": "Astrophysics Source Code Library"
    },
    {
      "citation_id": "21",
      "title": "Multimodal speech emotion recognition and classification using convolutional neural network techniques",
      "authors": [
        "A Christy",
        "S Vaithyasubramanian",
        "A Jesudoss",
        "M Praveena"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-020-09713-y"
    },
    {
      "citation_id": "22",
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"
    },
    {
      "citation_id": "23",
      "title": "BanglaSER: A speech emotion recognition dataset for the Bangla language",
      "authors": [
        "R Das",
        "N Islam",
        "M Ahmed",
        "S Islam",
        "S Shatabda",
        "A Islam"
      ],
      "year": "2022",
      "venue": "Data in Brief",
      "doi": "10.1016/j.dib.2022.108091"
    },
    {
      "citation_id": "24",
      "title": "Application of fuzzy C-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "S Demircan",
        "H Kahramanli"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-016-2712-y"
    },
    {
      "citation_id": "25",
      "title": "Efficient Feature-Aware Hybrid Model of Deep Learning Architectures for Speech Emotion Recognition",
      "authors": [
        "M Ezz-Eldin",
        "A Khalaf",
        "H Hamed",
        "A Hussein"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3054345"
    },
    {
      "citation_id": "26",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria",
        "Bin"
      ],
      "year": "2020",
      "venue": "Sensors (Switzerland)",
      "doi": "10.3390/s20216008"
    },
    {
      "citation_id": "27",
      "title": "Advanced Fusion-Based Speech Emotion Recognition System Using a Dual-Attention Mechanism with Conv-Caps and",
      "authors": [
        "B Features",
        "B Maji",
        "M Swain"
      ],
      "year": "2022",
      "venue": "Electronics (Switzerland)",
      "doi": "10.3390/electronics11091328"
    },
    {
      "citation_id": "28",
      "title": "A siamese neural network with modified distance loss for transfer learning in speech emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "A siamese neural network with modified distance loss for transfer learning in speech emotion recognition"
    },
    {
      "citation_id": "29",
      "title": "Enhancing Privacy Through Domain Adaptive Noise Injection For Speech Emotion Recognition",
      "authors": [
        "T Feng",
        "H Hashemi",
        "M Annavaram",
        "S Narayanan"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition on speech signals using machine learning",
      "authors": [
        "M Ghai",
        "S Lal",
        "S Duggal",
        "S Manik"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 International Conference On Big Data Analytics and Computational Intelligence",
      "doi": "10.1109/ICBDACI.2017.8070805"
    },
    {
      "citation_id": "31",
      "title": "Learning affective representations based on magnitude and dynamic relative phase information for speech emotion recognition",
      "authors": [
        "Guo"
      ],
      "year": "2021",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2021.11.005"
    },
    {
      "citation_id": "32",
      "title": "3D CNN-based speech emotion recognition using k-means clustering and spectrograms",
      "authors": [
        "N Hajarolasvadi",
        "H Demirel"
      ],
      "year": "2019",
      "venue": "Entropy",
      "doi": "10.3390/e21050479"
    },
    {
      "citation_id": "33",
      "title": "Surrey Audio-Visual Expressed Emotion (SAVEE) Database",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2014",
      "venue": "Surrey Audio-Visual Expressed Emotion (SAVEE) Database"
    },
    {
      "citation_id": "34",
      "title": "Learning utterance-level representations with label smoothing for speech emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, 2020-Octob",
      "doi": "10.21437/Interspeech.2020-1391"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Fatih Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2020.101894"
    },
    {
      "citation_id": "36",
      "title": "MFF-SAug: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network",
      "authors": [
        "S Jothimani",
        "K Premalatha"
      ],
      "year": "2022",
      "venue": "Chaos, Solitons & Fractals"
    },
    {
      "citation_id": "37",
      "title": "Automatic speech emotion recognition using an optimal combination of features based on EMD-TKEO",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "K Raoof",
        "M Mbarki",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2019.09.002"
    },
    {
      "citation_id": "38",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2936124"
    },
    {
      "citation_id": "39",
      "title": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "ArXiv Preprint",
      "doi": "10.21437/Interspeech.2017-736"
    },
    {
      "citation_id": "40",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "ACM International Conference Proceeding Series",
      "doi": "10.1145/3383972.3383975"
    },
    {
      "citation_id": "41",
      "title": "Investigation of multilingual and mixed-lingual emotion recognition using enhanced cues with data augmentation",
      "authors": [
        "S Lalitha",
        "D Gupta",
        "M Zakariah",
        "Y Alotaibi"
      ],
      "year": "2020",
      "venue": "Applied Acoustics",
      "doi": "10.1016/j.apacoust.2020.107519"
    },
    {
      "citation_id": "42",
      "title": "Enhanced speech emotion detection using deep neural networks",
      "authors": [
        "S Lalitha",
        "S Tripathi",
        "D Gupta"
      ],
      "year": "2019",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-018-09572-8"
    },
    {
      "citation_id": "43",
      "title": "Speech emotion recognition using spectral entropy",
      "authors": [
        "W Lee",
        "Y Roh",
        "D Kim",
        "J Kim",
        "K Hong"
      ],
      "year": "2008",
      "venue": "International Conference on Intelligent Robotics and Applications",
      "doi": "10.1007/978-3-540-88518-4_6"
    },
    {
      "citation_id": "44",
      "title": "Speech emotion recognition using recurrent neural networks with directional self-attention",
      "authors": [
        "D Li",
        "J Liu",
        "Z Yang",
        "L Sun",
        "Z Wang"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2021.114683"
    },
    {
      "citation_id": "45",
      "title": "Spatiotemporal and frequential cascaded attention networks for speech emotion recognition",
      "authors": [
        "S Li",
        "X Xing",
        "W Fan",
        "B Cai",
        "P Fordson",
        "X Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2021.02.094"
    },
    {
      "citation_id": "46",
      "title": "Evaluating gammatone frequency cepstral coefficients with neural networks for emotion recognition from speech",
      "authors": [
        "G Liu"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition based on feature selection and extreme learning machine decision tree",
      "authors": [
        "Z Liu",
        "M Wu",
        "W Cao",
        "J Mao",
        "J Xu",
        "G Tan"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2017.07.050"
    },
    {
      "citation_id": "48",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z Liu",
        "Q Xie",
        "M Wu",
        "W Cao",
        "Y Mei",
        "J Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2018.05.005"
    },
    {
      "citation_id": "49",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE",
      "doi": "10.5281/zenodo.1188976"
    },
    {
      "citation_id": "50",
      "title": "Data Augmentation for Audio-Visual Emotion Recognition with an Efficient Multimodal Conditional GAN",
      "authors": [
        "F Ma",
        "Y Li",
        "S Ni",
        "S Huang",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Applied Sciences (Switzerland)",
      "doi": "10.3390/app12010527"
    },
    {
      "citation_id": "51",
      "title": "Negative Emotion Recognition using Deep Learning for Thai Language",
      "authors": [
        "S Mekruksavanich",
        "A Jitpattanakul",
        "N Hnoohom"
      ],
      "year": "2020",
      "venue": "Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering, ECTI DAMT and NCON 2020",
      "doi": "10.1109/ECTIDAMTNCON48261.2020.9090768"
    },
    {
      "citation_id": "52",
      "title": "Speech Emotion Recognition from 3D Log-Mel Spectrograms with Deep Learning Network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2938007"
    },
    {
      "citation_id": "53",
      "title": "Speech emotion recognition using GhostVLAD and sentiment metric learning",
      "authors": [
        "B Mocanu",
        "R Tapu"
      ],
      "year": "2021",
      "venue": "International Symposium on Image and Signal Processing and Analysis, ISPA, 2021-Septe",
      "doi": "10.1109/ISPA52656.2021.9552068"
    },
    {
      "citation_id": "54",
      "title": "GOAMLP: Network Intrusion Detection with Multilayer Perceptron and Grasshopper Optimization Algorithm",
      "authors": [
        "S Moghanian",
        "F Saravi",
        "G Javidi",
        "E Sheybani"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3040740"
    },
    {
      "citation_id": "55",
      "title": "Speech emotion recognition using quaternion convolutional neural networks",
      "authors": [
        "A Muppidi",
        "M Radfar"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings, 2021-June",
      "doi": "10.1109/ICASSP39728.2021.9414248"
    },
    {
      "citation_id": "56",
      "title": "A CNN-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors (Switzerland)",
      "doi": "10.3390/s20010183"
    },
    {
      "citation_id": "57",
      "title": "CLSTM: Deep feature-based speech emotion recognition using the hierarchical convlstm network",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics",
      "doi": "10.3390/math8122133"
    },
    {
      "citation_id": "58",
      "title": "1D-CNN: Speech Emotion Recognition System Using a Stacked Network with",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Dilated CNN Features. Computers, Materials and Continua",
      "doi": "10.32604/cmc.2021.015070"
    },
    {
      "citation_id": "59",
      "title": "Att-Net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing",
      "doi": "10.1016/j.asoc.2021.107101"
    },
    {
      "citation_id": "60",
      "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2020.114177"
    },
    {
      "citation_id": "61",
      "title": "Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM",
      "authors": [
        "Sajjad Mustaqeem",
        "M Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2990405"
    },
    {
      "citation_id": "62",
      "title": "A Light-Weight Artificial Neural Network for Speech Emotion Recognition using Average Values of MFCCs and Their Derivatives",
      "authors": [
        "P Nantasri",
        "E Phaisangittisagul",
        "J Karnjana",
        "S Boonkla"
      ],
      "year": "2020",
      "venue": "17th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2020",
      "doi": "10.1109/ECTI-CON49241.2020.9158221"
    },
    {
      "citation_id": "63",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech, Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "Vocal-based emotion recognition using random forests and decision tree",
      "authors": [
        "F Noroozi",
        "T Sapiński",
        "D Kamińska",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-017-9396-2"
    },
    {
      "citation_id": "65",
      "title": "Rethinking (Dis)engagement in human-computer interaction",
      "authors": [
        "H O'brien",
        "I Roll",
        "A Kampen",
        "N Davoudi"
      ],
      "year": "2022",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2021.107109"
    },
    {
      "citation_id": "66",
      "title": "Multi-Window Data Augmentation Approach for Speech Emotion Recognition",
      "authors": [
        "S Padi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2020",
      "venue": "Multi-Window Data Augmentation Approach for Speech Emotion Recognition"
    },
    {
      "citation_id": "67",
      "title": "Emotion recognition using MLP and GMM for Oriya language",
      "authors": [
        "H Palo",
        "M Chandra",
        "M Mohanty"
      ],
      "year": "2017",
      "venue": "International Journal of Computational Vision and Robotics",
      "doi": "10.1504/IJCVR.2017.084987"
    },
    {
      "citation_id": "68",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "Pichora-Fuller",
        "; Kathleen",
        "M Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "69",
      "title": "Speech emotion recognition based on machine learning tactics and algorithms",
      "authors": [
        "S Prasanth",
        "M Roshni Thanka",
        "E Bijolin Edwin",
        "V Nagaraj"
      ],
      "year": "2021",
      "venue": "Materials Today: Proceedings",
      "doi": "10.1016/j.matpr.2020.12.207"
    },
    {
      "citation_id": "70",
      "title": "Speech emotion recognition using data augmentation",
      "authors": [
        "V Praseetha",
        "P Joby"
      ],
      "year": "2021",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-021-09883-3"
    },
    {
      "citation_id": "71",
      "title": "Deep learning models for speech emotion recognition",
      "authors": [
        "V Praseetha",
        "S Vadivel"
      ],
      "year": "2018",
      "venue": "Journal of Computer Science",
      "doi": "10.3844/jcssp.2018.1577.1587"
    },
    {
      "citation_id": "72",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "S Rajamani",
        "K Rajamani",
        "A Mallol-Ragolta",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings, 2021-June",
      "doi": "10.1109/ICASSP39728.2021.9414489"
    },
    {
      "citation_id": "73",
      "title": "Light Gated Recurrent Units for Speech Recognition",
      "authors": [
        "M Ravanelli",
        "P Brakel",
        "M Omologo",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
      "doi": "10.1109/TETCI.2017.2762739"
    },
    {
      "citation_id": "74",
      "title": "Emotion recognition system from speech and visual information based on convolutional neural networks",
      "authors": [
        "N Ristea",
        "L Dutu",
        "A Radoi"
      ],
      "year": "2019",
      "venue": "10th International Conference on Speech Technology and Human-Computer Dialogue, SpeD",
      "doi": "10.1109/SPED.2019.8906538"
    },
    {
      "citation_id": "75",
      "title": "SERAB-A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak",
        "D Epfl"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "Speech emotion recognition using data augmentation method by cycle-generative adversarial networks",
      "authors": [
        "A Shilandari",
        "H Marvi",
        "H Khosravi",
        "W Wang"
      ],
      "year": "2022",
      "venue": "Signal, Image and Video Processing",
      "doi": "10.1007/s11760-022-02156-9"
    },
    {
      "citation_id": "77",
      "title": "An Efficient Language-Independent Acoustic Emotion Classification System",
      "authors": [
        "R Singh",
        "H Puri",
        "N Aggarwal",
        "V Gupta"
      ],
      "year": "2020",
      "venue": "Arabian Journal for Science and Engineering",
      "doi": "10.1007/s13369-019-04293-9"
    },
    {
      "citation_id": "78",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "M Sokolova",
        "G Lapalme"
      ],
      "year": "2009",
      "venue": "Information Processing and Management",
      "doi": "10.1016/j.ipm.2009.03.002"
    },
    {
      "citation_id": "79",
      "title": "Bangla Speech Emotion Recognition and Cross-Lingual Study Using Deep CNN and BLSTM Networks",
      "authors": [
        "Sultana"
      ],
      "year": "2022",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3136251"
    },
    {
      "citation_id": "80",
      "title": "Neighbor-weighted K-nearest neighbor for unbalanced text corpus",
      "authors": [
        "S Tan"
      ],
      "year": "2005",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2004.12.023"
    },
    {
      "citation_id": "81",
      "title": "Multi-Conditioning and Data Augmentation Using Generative Noise Model for Speech Emotion Recognition in Noisy Conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings, 2020-May",
      "doi": "10.1109/ICASSP40776.2020.9053581"
    },
    {
      "citation_id": "82",
      "title": "Fusion of mel and gammatone frequency cepstral coefficients for speech emotion recognition using deep C-RNN",
      "authors": [
        "U Kumaran",
        "S Rammohan",
        "S Nagarajan",
        "A Prathik"
      ],
      "year": "2021",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-020-09792-x"
    },
    {
      "citation_id": "83",
      "title": "Emotion Recognition from Speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion Recognition from Speech",
      "doi": "10.1007/978-3-319-02732-6_7"
    },
    {
      "citation_id": "84",
      "title": "Fast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2008",
      "venue": "Signal Processing",
      "doi": "10.1016/j.sigpro.2008.07.001"
    },
    {
      "citation_id": "85",
      "title": "Speech emotion recognition using Fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2392101"
    },
    {
      "citation_id": "86",
      "title": "Feature Selection for Music Emotion Recognition",
      "authors": [
        "E Widiyanti",
        "S Endah"
      ],
      "year": "2018",
      "venue": "nd International Conference on Informatics and Computational Sciences",
      "doi": "10.1109/ICICOS.2018.8621783"
    },
    {
      "citation_id": "87",
      "title": "Speaker to emotion: Domain adaptation for speech emotion recognition with residual adapters",
      "authors": [
        "Y Xi",
        "P Li",
        "Y Song",
        "Y Jiang",
        "L Dai"
      ],
      "year": "2019",
      "venue": "Speaker to emotion: Domain adaptation for speech emotion recognition with residual adapters"
    },
    {
      "citation_id": "88",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "doi": "10.1109/APSIPAASC47483.2019.9023339"
    },
    {
      "citation_id": "89",
      "title": "Speech Emotion Classification Using Attention-Based LSTM",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
      "doi": "10.1109/TASLP.2019.2925934"
    },
    {
      "citation_id": "90",
      "title": "Head Fusion: Improving the Accuracy and Robustness of Speech Emotion Recognition on the IEMOCAP and RAVDESS Dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3067460"
    },
    {
      "citation_id": "91",
      "title": "A Multilingual Framework of CNN and Bi-LSTM for Emotion Classification",
      "authors": [
        "A Yadav",
        "Di Vishwakarma"
      ],
      "year": "2020",
      "venue": "11th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2020",
      "doi": "10.1109/ICCCNT49239.2020.9225614"
    },
    {
      "citation_id": "92",
      "title": "Improving Speech Emotion Recognition With Adversarial Data Augmentation Network",
      "authors": [
        "L Yi",
        "M Mak",
        "S Member"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "93",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "Yoon"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "94",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop",
      "doi": "10.1109/SLT.2018.8639583"
    },
    {
      "citation_id": "95",
      "title": "Automatic gender recognition using linear prediction coefficients and artificial neural network on speech signal",
      "authors": [
        "M Yusnita",
        "A Hafiz",
        "M Fadzilah",
        "A Zulhanip",
        "M Idris"
      ],
      "year": "2017",
      "venue": "Proceedings -7th IEEE International Conference on Control System, Computing and Engineering",
      "doi": "10.1109/ICCSCE.2017.8284437"
    },
    {
      "citation_id": "96",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems",
      "doi": "10.1007/s40747-020-00250-4"
    },
    {
      "citation_id": "97",
      "title": "Learning deep binaural representations with deep convolutional neural networks for spontaneous speech emotion recognition",
      "authors": [
        "S Zhang",
        "A Chen",
        "W Guo",
        "Y Cui",
        "X Zhao",
        "L Liu"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2969032"
    },
    {
      "citation_id": "98",
      "title": "Learning deep multimodal affective features for spontaneous speech emotion recognition",
      "authors": [
        "S Zhang",
        "X Tao",
        "Y Chuang",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2020.12.009"
    },
    {
      "citation_id": "99",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "100",
      "title": "Attention Based Fully Convolutional Network for Speech Emotion Recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "Attention Based Fully Convolutional Network for Speech Emotion Recognition"
    },
    {
      "citation_id": "101",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 -Proceedings",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 -Proceedings",
      "doi": "10.23919/APSIPA.2018.8659587"
    },
    {
      "citation_id": "102",
      "title": "Speech feature selection and emotion recognition based on weighted binary cuckoo search",
      "authors": [
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Alexandria Engineering Journal",
      "doi": "10.1016/j.aej.2020.11.004"
    },
    {
      "citation_id": "103",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2018.08.035"
    },
    {
      "citation_id": "104",
      "title": "Speech Emotion Recognition Using Auditory Spectrogram and Cepstral Features. European Signal Processing Conference",
      "authors": [
        "S Zhao",
        "Y Yang",
        "I Cohen",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition Using Auditory Spectrogram and Cepstral Features. European Signal Processing Conference",
      "doi": "10.23919/EUSIPCO54536.2021.9616144"
    },
    {
      "citation_id": "105",
      "title": "Combining a parallel 2D CNN with a self-attention Dilated Residual Network for CTC-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "W Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2021.03.013"
    },
    {
      "citation_id": "106",
      "title": "An ensemble model for multi-level speech emotion recognition",
      "authors": [
        "C Zheng",
        "C Wang",
        "N Jia"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10010205"
    },
    {
      "citation_id": "107",
      "title": "Ensemble Learning of Hybrid Acoustic Features for Speech Emotion Recognition",
      "authors": [
        "K Zvarevashe",
        "O Olugbara"
      ],
      "year": "2020",
      "venue": "Agorithms",
      "doi": "10.26782/jmcms.2020.09.00016"
    }
  ]
}