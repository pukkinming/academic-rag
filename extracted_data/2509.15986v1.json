{
  "paper_id": "2509.15986v1",
  "title": "Emoheal: An End-To-End System For Personalized Therapeutic Music Retrieval From Fine-Grained Emotions",
  "published": "2025-09-19T13:52:22Z",
  "authors": [
    "Xinchen Wan",
    "Jinhua Liang",
    "Huan Zhang"
  ],
  "keywords": [
    "Digital Wellness",
    "Emotion Recognition",
    "Music Healing",
    "Multimodal Content Retrieval",
    "Personalized Intervention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, presleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and \"one-size-fits-all\", failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, maping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLaMP3 model to guide users from their current state toward a calmer one (\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, p < 0.001) and high perceived emotion recognition accuracy (M=4.05, p < 0.001). A strong correlation between perceived accuracy and therapeutic outcome (r = 0.72, p < 0.001) validates our fine-grained approach. These findings establishes the viability of theory-driven, emotionaware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The need for accessible mental wellness tools is critical, with anxiety disorders affecting 359 million people globally  [1]  and the digital mental health market projected to exceed $27 billion by 2025  [2] . However, dominant applications like Calm  1  and Headspace 2  rely on a static, library-based paradigm, requiring users to self-diagnose and manually select pre-recorded content. This model suffers from two fundamental flaws: a static approach that fails to adapt to users' free-form expression, and a lack of therapeutic dynamism required to implement principles like the \"iso-principle,\" which often creates an \"emotional mismatch.\"\n\nTo address these limitations, we propose EmoHeal, a novel system that creates personalized supportive journeys by guiding users through a three-stage \"match-guide-target\" narrative that operationalizes the iso-principle. It integrates three core components: a fine-tuned XLM-RoBERTa model for fine-grained emotion recognition, a knowledge graph to map emotions to musical parameters, and a CLaMP3 model for real-time content retrieval. Our contributions are threefold: (1) a novel end-to-end system integrating these state-of-the-art AI technologies; (2) a computational blueprint for operationalizing music therapy principles; and (3) an empirical validation through a 40-participant user study demonstrating the efficacy of our theory-driven, personalized approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Our work integrates advancements from four key domains. First, in fine-grained emotion recognition, some advanced approaches such as XLM-RoBERTa  [3, 4]  have provided powerful language representations capable of capturing subtle affective cues. Together with large-scale benchmarks like GoEmotions  [5] , these developments have enabled a shift beyond coarse sentiment analysis toward more nuanced emotion understanding. These models have proven effective in complex domains, including health-related online contexts  [6] . However, a key challenge remains in applying these models, often trained on public social media data, to the more private and nuanced characteristic of wellness setting  [7] .\n\nThe second pillar is computational music therapy. Our work is grounded in psychological frameworks like GEMS, which defines music-specific emotional dimensions  [8] , and long-standing therapeutic theories such as the iso-principle  [9] . Neuroscience research, showing that music engages unique emotional processing pathways in the brain  [10] , further motivates music's use as a therapeutic modality. The primary challenge here is computationally operationalizing these complex, dynamic principles into an automated system.\n\nFor content delivery, we build on advances in multimodal retrieval. Early audio-language models such as MuLan and CLAP established strong text-audio alignment  [11, 12] , while some recent systems like CLaMP3  [13, 14]  further improved scalability and retrieval accuracy. More recently, large language models (LLMs) have been extended to audio  [15, 16] , showing promise as generalpurpose learners, but their heavy computational demands restrict deployment in resource-constrained settings. In parallel, generative models such as MusicLM can synthesize novel music  [17, 18, 19, 20, 21] , though their potential in wellness applications remains largely untapped. LLM-driven audio generators  [22, 23]  adapt to task-specific needs by integrating specialist models, yet they face the same efficiency challenges as other LLM-based systems. In contrast, our retrieval-based strategy ensures high-fidelity, professionally produced audiovisual content while remaining computationally efficient-an essential requirement for wellness applications.\n\nFinally, the user experience is informed by Human-Computer Interaction (HCI). We apply concepts of \"Calm Technology\"  [24]  and established guidelines for digital wellness tools  [25] . Furthermore, creating a sense of \"presence\" and immersion is known to enhance the efficacy of virtual environments  [26]   been fragmented. Few systems cohesively merge these technical and psychological pillars into a single, end-to-end pipeline evaluated on user-centric outcomes. EmoHeal directly addresses this integration gap.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Input: [Cls] + Tokenized Text + [Sep]",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Xlm-Roberta-Base",
      "text": "Transformer Encoder Layer",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Emoheal System",
      "text": "As illustrated in Fig.  1 , our system, EmoHeal, is an end-to-end pipeline that generates personalized supportive music experiences by processing user text input into a final audio-visual presentation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fine-Grained Emotion Computation",
      "text": "To capture nuanced emotional states, we selected XLM-RoBERTabase as our backbone network. As shown in Fig.  2 , a linear classification head (768 × 27) is added on top of the model's [CLS] token representation to predict a 27-dimensional probability vector, e ∈ R 27 , corresponding to the 27 fine-grained emotion classes defined by the GoEmotions dataset  [5] . For this task, the pre-trained XLM-ROBERTa-base model was fine-tuned on a custom multicorpus dataset. This dataset combines two sources: the foundational English GoEmotions dataset  [5] , which defines our 27 fine-grained emotion labels, and the NLPCC-2014 Chinese Emotion Analysis Dataset. To align the label spaces, as the NLPCC-2014 dataset uses a different set of coarser emotion labels, we developed a programmatic mapping strategy. For instance, the coarse label 'joy' from the Chinese dataset was heuristically mapped to a multi-label vector where the corresponding fine-grained GoEmotions labels for 'joy', 'amusement', and 'excitement' were all activated (set to 1), while all other labels were set to 0. The training utilized the Focal Loss function, formulated as:\n\nwhere pt is the class probability and we set the focusing parameter γ = 2. The model was fine-tuned using an AdamW optimizer (LR=2e-5) with 5-fold cross-validation, achieving a validation Macro-F1 of 0.64 and Weighted-F1 of 0.71.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion-Music Knowledge Graph",
      "text": "This module translates the emotion vector e into a set of six musical parameters p, including tempo (continuous, e.g., 60-120 BPM), mode (e.g., major/minor), timbre (e.g., bright/dark), harmony (e.g., consonant/dissonant), register (e.g., high/low), and density (e.g., sparse/dense). The logic, grounded in music psychology  [8] , is operationalized via a two-tier inference system shown in Fig.  4 . The first tier handles unambiguous emotions, defined as cases where a single primary emotion score exceeds a high-intensity threshold, which was set empirically to τ = 0.7 based on pilot testing. For these cases, an expert-curated rule is triggered to provide a rapid, consistent response, following the form:\n\nwhere ei is the score for an emotion, pj a musical parameter, and v a target value, directly applying the aforementioned threshold τ . If no rule is triggered, the vector is treated as a complex emotion (e.g., a mix of 'nostalgia' and 'optimism') and passed to the second tier: a dynamic adjustment model. This model projects the vector e through a handcrafted, theory-driven weight matrix W ∈ R 27×6 . This matrix, derived from music emotion recognition literature  [27] , contains normalized weights in [-1.0, 1.0] encoding the influence of each emotion on each parameter. The final blended parameters are yielded by: p = σ(eW ),\n\nwhere p ∈ R 6 represents the continuous parameter set and σ(•) is a normalization function. This weighted mapping enables a finegrained adaptation beyond categorical rules.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal",
      "text": "key of methodology was the of custom over 600 4K video clips. These were derived from commercially films ation', curated based on principles from environmental psychology, particularly Attention Restoration Theory (ART)  [28] . A programmatic pipeline segmented these films by first detecting scene boundaries via color histograms, then identifying 'calm segments' with low motion magnitude using optical flow, and finally partitioning these segments into non-overlapping 3-minute clips. For offline feature extraction, each clip's audio track was converted into a fixeddimension 128-d embedding using a pre-trained VGGish model  [29]  and temporal average pooling. These embeddings were indexed into a Faiss library using an Inverted File (IVF) index to enable near realtime Approximate Nearest Neighbor (ANN) search.\n\nThe real-time retrieval process is enabled by the CLaMP3 model  [13] , whose jointly trained text and audio encoders map semantically similar content to nearby vectors in a shared multimodal embedding space, thus allowing for direct text-to-audio similarity comparison. During a user session, as detailed in Fig.  3 , the musical parameters p from the knowledge graph are first converted into a descriptive natural language prompt using a template-based function. This prompt is then fed into the CLaMP3 text encoder to generate a query text embedding. This query is used to perform a cosine similarity search against the pre-indexed audio embeddings, and the top-3 most similar videos are retrieved for the user.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "User Interface And Interaction Design",
      "text": "The system was implemented as a web application  3  . The frontend, designed for a responsive and dynamic user experience, was built with HTML5, CSS3, and JavaScript (ES6+), utilizing the Bootstrap framework for its UI components. The back-end consists of a Python-based REST API, developed with the Flask framework, which serves the core machine learning models. The interface design follows \"Calm Technology\" principles  [24] , employing a dark theme and a \"progressive disclosure\" model to minimize cognitive load and protect user privacy by using all data ephemerally.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "A total of 40 participants were recruited for the within-subjects study. The cohort consisted primarily of students from Queen Mary University of London, recruited using a snowball sampling method through university-affiliated WhatsApp and WeChat student groups. The demographics of the participants are as follows: the mean age was 26.2 years (SD=4.8), with a gender distribution of 22 female and 18 male. Regarding language proficiency, 60.0% reported English as their primary language, while 40.0% reported Chinese. As this was a non-clinical study, inclusion criteria required participants to have no severe mental health conditions requiring clinical intervention.\n\nThe experimental procedure was straightforward. Each participant provided a text description of their current mood, watched the 3-minute therapeutic video generated in real-time by the system, and subsequently completed a post-session questionnaire. Primary outcomes were assessed using a 5-point Likert scale (1=Strongly Disagree, 5=Strongly Agree), where participants rated their agreement with statements such as, \"The generated video accurately reflected the emotions I described in my text\" (System Responsiveness) and \"Watching the video had a positive impact on my mood\" (Supportive Effect). We also collected qualitative feedback through open-ended questions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "To provide a qualitative illustration of the system's personalization capabilities, Fig.  5  showcases how two distinct emotional inputs result in markedly different audiovisual outputs. The primary quantitative results, summarized in Table  1  and Table  2 , confirm the system's effectiveness. It demonstrated a significant positive impact on user mood (M=4.12, p < 0.001) and was perceived as highly responsive to their described emotions (M=4.05, p < 0.001). The high ratings for overall atmosphere and multimodal coherence (both M=4.18) further suggest a high-quality user experience. These findings are further supported by the rating distributions, where 85.0% of participants rated the emotion accuracy and 87.5% rated their mood improvement as \"Agree\" (Score 4) or \"Strongly Agree\" (Score 5). A notable 78% also explicitly mentioned feeling \"understood\" by the system's personalized response.\n\nOur key finding, detailed in Table  2 , is the strong, positive correlation between the perceived accuracy of emotion recognition and the subsequent mood improvement (r = 0.72, p < 0.001). This statistically validates our entire fine-grained approach, providing the empirical backbone for the following discussion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "This work introduced EmoHeal, an AI system that translates finegrained emotional states from text into personalized, theory-driven musical experiences. Results show that moving beyond coarse emotion models is essential: recognition accuracy strongly correlated with therapeutic efficacy (r = 0.72), underscoring the role of nuanced recognition in fostering a sense of being \"understood.\" A key contribution is the hybrid design that combines neural networks with a knowledge graph grounded in constructs such as GEMS and the iso-principle. This neuro-symbolic approach produced coherent and interpretable mappings, enhancing both theoretical grounding and trust. Participants also valued the system's simplicity and personalization, which reduced cognitive load and supported emotional processing. Limitations include the single-session design, reliance on self-report, and a homogenous sample. Future work should extend to diverse populations, multimodal signals, and clinical integration.\n\nOverall, EmoHeal demonstrates that fine-grained emotion recognition coupled with hybrid AI can deliver measurable therapeutic benefits and provide a scalable blueprint for digital wellness tools.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the Emotion-Driven Three-Stage Music Therapy System. The system integrates four core modules: a Fine-Grained",
      "page": 2
    },
    {
      "caption": "Figure 2: Architecture of the Emotion Computation Module.",
      "page": 2
    },
    {
      "caption": "Figure 1: , our system, EmoHeal, is an end-to-end",
      "page": 2
    },
    {
      "caption": "Figure 2: , a linear clas-",
      "page": 2
    },
    {
      "caption": "Figure 3: Workflow of the CLaMP3 Multimodal Retrieval Module.",
      "page": 3
    },
    {
      "caption": "Figure 4: Workflow of the Emotion-Music KG Module.",
      "page": 3
    },
    {
      "caption": "Figure 3: , the musical parameters p",
      "page": 3
    },
    {
      "caption": "Figure 5: Demonstration of the EmoHeal system. Two distinct user inputs describing anxiety (top) and sadness (bottom) are processed by",
      "page": 4
    },
    {
      "caption": "Figure 5: showcases how two distinct emotional inputs re-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Queen Mary University of London, London, UK": "validation through a 40-participant user\nstudy demonstrating the"
        },
        {
          "Queen Mary University of London, London, UK": "efficacy of our theory-driven, personalized approach."
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "2. RELATED WORK"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "Our work integrates advancements from four key domains. First, in"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "fine-grained emotion recognition, some advanced approaches such"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "as XLM-RoBERTa [3, 4] have provided powerful\nlanguage repre-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "sentations capable of capturing subtle affective cues. Together with"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "large-scale benchmarks\nlike GoEmotions\n[5],\nthese developments"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "have enabled a shift beyond coarse sentiment analysis toward more"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "nuanced emotion understanding. These models have proven effec-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "tive in complex domains,\nincluding health-related online contexts"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "[6]. However, a key challenge remains in applying these models,"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "often trained on public social media data,\nto the more private and"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "nuanced characteristic of wellness setting [7]."
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "The second pillar is computational music therapy. Our work"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "is grounded in psychological\nframeworks\nlike GEMS, which de-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "fines music-specific emotional dimensions\n[8],\nand long-standing"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "therapeutic theories such as the iso-principle [9]. Neuroscience re-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "search,\nshowing that music engages unique emotional processing"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "pathways in the brain [10], further motivates music’s use as a thera-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "peutic modality. The primary challenge here is computationally op-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "erationalizing these complex, dynamic principles into an automated"
        },
        {
          "Queen Mary University of London, London, UK": "system."
        },
        {
          "Queen Mary University of London, London, UK": "For content delivery, we build on advances in multimodal\nre-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "trieval.\nEarly audio–language models such as MuLan and CLAP"
        },
        {
          "Queen Mary University of London, London, UK": "established strong text–audio alignment\n[11, 12], while some re-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "cent systems like CLaMP3 [13, 14] further improved scalability and"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "retrieval accuracy. More recently,\nlarge language models (LLMs)"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "have been extended to audio [15, 16], showing promise as general-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "purpose learners, but\ntheir heavy computational demands\nrestrict"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "deployment\nin resource-constrained settings.\nIn parallel, genera-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "tive models such as MusicLM can synthesize novel music [17, 18,"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "19, 20, 21],\nthough their potential\nin wellness applications remains"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "largely untapped.\nLLM-driven audio generators [22, 23] adapt\nto"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "task-specific needs by integrating specialist models, yet\nthey face"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "the same efficiency challenges as other LLM-based systems. In con-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "trast, our\nretrieval-based strategy ensures high-fidelity, profession-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "ally produced audiovisual content while remaining computationally"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "efficient—an essential requirement for wellness applications."
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "Finally,\nthe user experience is informed by Human-Computer"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "Interaction (HCI). We apply concepts of “Calm Technology” [24]"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "and established guidelines for digital wellness tools [25].\nFurther-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "more, creating a sense of “presence” and immersion is known to en-"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "hance the efficacy of virtual environments [26], a principle we aimed"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "to incorporate. A key research gap remains in designing for users"
        },
        {
          "Queen Mary University of London, London, UK": ""
        },
        {
          "Queen Mary University of London, London, UK": "in vulnerable emotional states, where traditional usability heuristics"
        },
        {
          "Queen Mary University of London, London, UK": "may not apply."
        },
        {
          "Queen Mary University of London, London, UK": "While progress in each area is significant, prior work has often"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Encoder\nGenerator\nCosine\nMatched": "Video\nXLM-"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "VGGish +\nMusic\nsimilarity\nVideos"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Library\nRoBERTa"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "librosa\nParams \nRetrieval"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Fig. 1. Architecture of the Emotion-Driven Three-Stage Music Therapy System. The system integrates four core modules: a Fine-Grained"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Emotion Computation module to analyze user text, an Emotion-Music Knowledge Graph to translate emotions into musical parameters, a"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "CLaMP3 Multimodal Retrieval Module to find corresponding audiovisual content, and an Immersive User Interface to present the therapeutic"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "journey."
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "been fragmented. Few systems cohesively merge these technical and"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "psychological pillars into a single, end-to-end pipeline evaluated on"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "user-centric outcomes. EmoHeal directly addresses this integration"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "gap."
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Input: [CLS] + Tokenized Text + [SEP]"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "XLM-RoBERTa-base"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Transformer Encoder Layer"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "x 12"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "[CLS] Token Representation"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "(768-dim Vector)"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Linear (768 → 27)"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Sigmoid"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "Fig. 2. Architecture of the Emotion Computation Module."
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "3. THE PROPOSED EMOHEAL SYSTEM"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "As\nillustrated in Fig. 1, our\nsystem, EmoHeal,\nis\nan end-to-end"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "pipeline that generates personalized supportive music experiences"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "by processing user text input into a final audio-visual presentation."
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": ""
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "3.1. Fine-Grained Emotion Computation"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "To capture nuanced emotional states, we selected XLM-RoBERTa-"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "base as our backbone network. As shown in Fig. 2, a linear clas-"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "sification head (768 × 27)\nis added on top of\nthe model’s [CLS]"
        },
        {
          "Encoder\nGenerator\nCosine\nMatched": "token representation to predict a 27-dimensional probability vector,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "a Faiss library using an Inverted File (IVF) index to enable near real-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "time Approximate Nearest Neighbor (ANN) search."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "The real-time retrieval process is enabled by the CLaMP3 model"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "[13], whose jointly trained text and audio encoders map semantically"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "similar content to nearby vectors in a shared multimodal embedding"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "space,\nthus allowing for direct\ntext-to-audio similarity comparison."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "During a user session, as detailed in Fig. 3, the musical parameters p"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "from the knowledge graph are first converted into a descriptive nat-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "ural language prompt using a template-based function. This prompt"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "is then fed into the CLaMP3 text encoder\nto generate a query text"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "embedding. This query is used to perform a cosine similarity search"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "against the pre-indexed audio embeddings, and the top-3 most simi-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "lar videos are retrieved for the user."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "3.4. User Interface and Interaction Design"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "The system was\nimplemented as a web application3.\nThe front-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "end, designed for a responsive and dynamic user experience, was"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "built with HTML5, CSS3, and JavaScript (ES6+), utilizing the Boot-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "strap framework for\nits UI components.\nThe back-end consists of"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "a Python-based REST API, developed with the Flask framework,"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "which serves the core machine learning models. The interface de-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "sign follows ”Calm Technology” principles [24], employing a dark"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "theme and a ”progressive disclosure” model\nto minimize cognitive"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "load and protect user privacy by using all data ephemerally."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "4. EVALUATION"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "4.1. Experimental Setup"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "A total of 40 participants were\nrecruited for\nthe within-subjects"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "study. The cohort consisted primarily of students from Queen Mary"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "University of London, recruited using a snowball sampling method"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "through university-affiliated WhatsApp and WeChat student groups."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "The demographics of the participants are as follows:\nthe mean age"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "was 26.2 years (SD=4.8), with a gender distribution of 22 female and"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "18 male. Regarding language proficiency, 60.0% reported English as"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "their primary language, while 40.0% reported Chinese. As this was"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "a non-clinical study,\ninclusion criteria required participants to have"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "no severe mental health conditions requiring clinical intervention."
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "The experimental procedure was straightforward. Each partici-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "pant provided a text description of their current mood, watched the"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": ""
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "3A\nproject\nshowcase,\nincluding\nvideo\ndemonstrations,\nis\navail-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "able at:\n[https://jeweled-scarer-812.notion.site/EmoHeal-Project-Showcase-"
        },
        {
          "Fig. 3. Workflow of the CLaMP3 Multimodal Retrieval Module.": "27278a073579807b8e82e5fac88b821f]"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\"It's been a tough week. Feeling really": "lonely and just empty inside. I don't"
        },
        {
          "\"It's been a tough week. Feeling really": "have the energy to do anything, just"
        },
        {
          "\"It's been a tough week. Feeling really": "want to lie here and listen to the rain.\""
        },
        {
          "\"It's been a tough week. Feeling really": "Fig. 5. Demonstration of\nthe EmoHeal system. Two distinct user"
        },
        {
          "\"It's been a tough week. Feeling really": "the system, resulting in distinct, personalized audiovisual outputs. The audio component"
        },
        {
          "\"It's been a tough week. Feeling really": "difference in acoustic features."
        },
        {
          "\"It's been a tough week. Feeling really": "3-minute therapeutic video generated in real-time by the system, and"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "subsequently completed a post-session questionnaire. Primary out-"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "comes were assessed using a 5-point Likert scale (1=Strongly Dis-"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "agree, 5=Strongly Agree), where participants rated their agreement"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "with statements such as, “The generated video accurately reflected"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "the emotions I described in my text” (System Responsiveness) and"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "“Watching the video had a positive impact on my mood” (Supportive"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "Effect). We also collected qualitative feedback through open-ended"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "questions."
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "4.2. Results"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "To provide a qualitative illustration of the system’s personalization"
        },
        {
          "\"It's been a tough week. Feeling really": "capabilities, Fig. 5 showcases how two distinct emotional inputs re-"
        },
        {
          "\"It's been a tough week. Feeling really": "sult\nin markedly different audiovisual outputs. The primary quanti-"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "tative results, summarized in Table 1 and Table 2, confirm the sys-"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "tem’s effectiveness.\nIt demonstrated a significant positive impact on"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "user mood (M=4.12, p < 0.001) and was perceived as highly re-"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "sponsive to their described emotions (M=4.05, p < 0.001).\nThe"
        },
        {
          "\"It's been a tough week. Feeling really": "high ratings for overall atmosphere and multimodal coherence (both"
        },
        {
          "\"It's been a tough week. Feeling really": "M=4.18) further suggest a high-quality user experience. These find-"
        },
        {
          "\"It's been a tough week. Feeling really": "ings are further supported by the rating distributions, where 85.0% of"
        },
        {
          "\"It's been a tough week. Feeling really": "participants rated the emotion accuracy and 87.5% rated their mood"
        },
        {
          "\"It's been a tough week. Feeling really": "improvement as “Agree” (Score 4) or “Strongly Agree” (Score 5). A"
        },
        {
          "\"It's been a tough week. Feeling really": "notable 78% also explicitly mentioned feeling “understood” by the"
        },
        {
          "\"It's been a tough week. Feeling really": "system’s personalized response."
        },
        {
          "\"It's been a tough week. Feeling really": "Our key finding, detailed in Table 2,\nis the strong, positive cor-"
        },
        {
          "\"It's been a tough week. Feeling really": "relation between the perceived accuracy of emotion recognition and"
        },
        {
          "\"It's been a tough week. Feeling really": "the subsequent mood improvement\n(r = 0.72, p < 0.001).\nThis"
        },
        {
          "\"It's been a tough week. Feeling really": "statistically validates our entire fine-grained approach, providing the"
        },
        {
          "\"It's been a tough week. Feeling really": "empirical backbone for the following discussion."
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "5. DISCUSSION AND CONCLUSION"
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": ""
        },
        {
          "\"It's been a tough week. Feeling really": "This work introduced EmoHeal, an AI system that\ntranslates fine-"
        },
        {
          "\"It's been a tough week. Feeling really": "grained emotional states from text\ninto personalized,\ntheory-driven"
        },
        {
          "\"It's been a tough week. Feeling really": "musical experiences. Results show that moving beyond coarse emo-"
        },
        {
          "\"It's been a tough week. Feeling really": "tion models\nis essential:\nrecognition accuracy strongly correlated"
        },
        {
          "\"It's been a tough week. Feeling really": "with therapeutic efficacy (r = 0.72), underscoring the role of nu-"
        },
        {
          "\"It's been a tough week. Feeling really": "anced recognition in fostering a sense of being “understood.”"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "mon Dixon, and Shinichi Furuya,\n“Llaqo: Towards a query-"
        },
        {
          "6. REFERENCES": "[1] World Health Organization,\n“Anxiety disorders,” WHO Fact",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "based coach in expressive music performance assessment,”\nin"
        },
        {
          "6. REFERENCES": "Sheet, 2021, Accessed: September 15, 2025.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "ICASSP 2025 - 2025 IEEE International Conference on Acous-"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "tics, Speech and Signal Processing (ICASSP), 2025, pp. 1–5."
        },
        {
          "6. REFERENCES": "[2] Research and Markets,\n“Digital mental health market\nreport",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "2025,” Research and Markets, 2025.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[17] Andrea Agostinelli, Timo I Denk, Zal´an Borsos, Jesse Engel,"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Mauro Verzetti, Antoine Caillon, et al., “Musiclm: Generating"
        },
        {
          "6. REFERENCES": "[3] Alexis Conneau, Kartik Khandelwal, Naman Goyal, Vishrav",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "music from text,” 2023."
        },
        {
          "6. REFERENCES": "Chaudhary, Guillaume Wenzek,\nFrancisco Guzm´an,\net\nal.,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "“Unsupervised cross-lingual representation learning at scale,”",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[18] Tanisha Hisariya, Huan Zhang, and Jinhua Liang,\n“Bridging"
        },
        {
          "6. REFERENCES": "the 58th Annual Meeting of\nthe Association\nin Proceedings of",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "paintings and music–exploring emotion based music genera-"
        },
        {
          "6. REFERENCES": "for Computational Linguistics (ACL), 2020, pp. 8440–8451.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "tion through paintings,”\nin 17th International Symposium on"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Computer Music Multidisciplinary Research (CMMR 2025),"
        },
        {
          "6. REFERENCES": "[4]\nJiehui\nJia, Huan Zhang,\nand Jinhua Liang,\n“Bridging dis-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "2025."
        },
        {
          "6. REFERENCES": "crete and continuous: A multimodal strategy for complex emo-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "tion detection,”\nin IEEE International Workshop on Machine",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[19] Huan Zhang, Shreyan Chowdhury, Carlos Eduardo Cancino-"
        },
        {
          "6. REFERENCES": "Learning for Signal Processing (MLSP) 2025, 2024.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Chac´on,\nJinhua Liang, Simon Dixon, and Gerhard Widmer,"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "“Dexter:\nLearning and controlling performance\nexpression"
        },
        {
          "6. REFERENCES": "[5] Dorottya Demszky, Dana Movshovitz-Attias,\nJeongwoo Ko,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "with diffusion models,” Applied Sciences, vol. 14, no. 15, pp."
        },
        {
          "6. REFERENCES": "Alan Cowen, Gaurav Nemade, and Sujith Ravi, “Goemotions:",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "6543, 2024."
        },
        {
          "6. REFERENCES": "A dataset of fine-grained emotions,” in Proceedings of the 58th",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "Annual Meeting of the Association for Computational Linguis-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[20] Huan Zhang, Akira Maezawa, and Simon Dixon, “Renderbox:"
        },
        {
          "6. REFERENCES": "tics (ACL), 2020, pp. 4040–4054.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Expressive performance rendering with text control,” 2025."
        },
        {
          "6. REFERENCES": "[6] Hina Khanpour and Cornelia Caragea, “Fine-grained emotion",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[21] Yi Yuan, Haohe Liu, Jinhua Liang, Xubo Liu, Mark D Plumb-"
        },
        {
          "6. REFERENCES": "detection in health-related online posts,” in Proceedings of the",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "ley, and Wenwu Wang,\n“Leveraging pre-trained audioldm for"
        },
        {
          "6. REFERENCES": "2018 Conference on Empirical Methods in Natural Language",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "sound generation: A benchmark study,”\nin 2023 31st Euro-"
        },
        {
          "6. REFERENCES": "Processing (EMNLP), 2018, pp. 1678–1687.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "pean Signal Processing Conference (EUSIPCO). IEEE, 2023,"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "pp. 765–769."
        },
        {
          "6. REFERENCES": "[7]\nFrancis Awindaogo Acheampong, Henry Nunoo-Mensah, and",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "Wenyu Chen, “Text-based emotion detection: Advances, chal-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[22] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui,"
        },
        {
          "6. REFERENCES": "lenges, and opportunities,” Engineering Reports, vol. 2, no. 7,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D"
        },
        {
          "6. REFERENCES": "pp. e12189, 2020.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Plumbley, et al.,\n“Wavjourney: Compositional audio creation"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "IEEE Transactions on Audio,\nwith large language models,”"
        },
        {
          "6. REFERENCES": "[8] Marcel Zentner, Didier Grandjean,\nand Klaus R Scherer,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Speech and Language Processing, 2025."
        },
        {
          "6. REFERENCES": "“Emotions evoked by the sound of music: Characterization,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "classification, and measurement,”\nEmotion, vol. 8, no. 4, pp.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[23]\nJinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang"
        },
        {
          "6. REFERENCES": "494–521, 2008.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Kong, Xubo Liu, Wenwu Wang, Mark Plumbley, Huy Phan,"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "and Emmanouil Benetos, “Wavcraft: Audio editing and gener-"
        },
        {
          "6. REFERENCES": "[9]\nSuzanne B Hanser,\nThe New Music Therapist’s Handbook,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "ation with large language models,” in ICLR 2024 Workshop on"
        },
        {
          "6. REFERENCES": "Berklee Press, 1999.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "LLM Agents, 2024."
        },
        {
          "6. REFERENCES": "[10]\nStefan Koelsch, Thomas Fritz, D Yves Von Cramon, Karsten",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[24] Mark Weiser, “Calm technology,” in The Disappearing Com-"
        },
        {
          "6. REFERENCES": "M¨uller, and Angela D Friederici,\n“Investigating emotion with",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "puter, Gilbert Cockton, Ed., pp. 75–85. Springer, 1996."
        },
        {
          "6. REFERENCES": "music: an fmri study,” Human brain mapping, vol. 27, no. 3,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "pp. 239–250, 2006.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[25]\nJoseph Kvedar, Molly Joel Coye, and Wendy Everett, “Digital"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "therapeutics: Clinical robustness and regulatory frameworks,”"
        },
        {
          "6. REFERENCES": "[11]\nS Wu, A Jansen, J Lee, R Ganti, and J Y Li, “MuLan: A joint",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "NPJ Digital Medicine, vol. 5, no. 1, pp. 179, 2022."
        },
        {
          "6. REFERENCES": "embedding of music audio and natural language,” in Proceed-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "ings of\nthe 23rd International Society for Music Information",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Presence:\n[26] Mel Slater,\n“A note on presence and immersion,”"
        },
        {
          "6. REFERENCES": "Retrieval Conference (ISMIR), 2022, pp. 1–8.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Teleoperators and Virtual Environments, vol. 12, no. 3, pp."
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "355–374, 2003."
        },
        {
          "6. REFERENCES": "[12] Q Huang, A Jansen, J Lee, R Ganti, J Y Li, and D P W El-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "lis, “CLAP: Contrastive language-audio pre-training for cross-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[27] Reep Panda, R. Malheiro, and R. P. Paiva, “Audio features for"
        },
        {
          "6. REFERENCES": "modal symbolic music information retrieval,” 2022.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "IEEE Transactions on\nmusic emotion recognition: A survey,”"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Affective Computing, vol. 11, no. 1, pp. 56–76, 2020."
        },
        {
          "6. REFERENCES": "[13]\nS Wu, A Jansen, J Lee, R Ganti, and J Y Li, “CLaMP 3: Uni-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "versal music information retrieval across unaligned modalities",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[28]\nStephen Kaplan, “The restorative benefits of nature: Toward a"
        },
        {
          "6. REFERENCES": "and unseen languages,” 2025.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "conceptual framework,” Journal of environmental psychology,"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "vol. 15, no. 3, pp. 169–182, 1995."
        },
        {
          "6. REFERENCES": "[14]\nJinhua Liang, Xubo Liu, Haohe Liu, Huy Phan, Emmanouil",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "Benetos, Mark Plumbley,\nand Wenwu Wang,\n“Adapting",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "[29]\nShawn Hershey, Sourish Chaudhuri, Daniel P W Ellis, Jort F"
        },
        {
          "6. REFERENCES": "language-audio models as few-shot audio learners,” in INTER-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal,"
        },
        {
          "6. REFERENCES": "SPEECH 2023, 2023, pp. 276–280.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Devin Platt, Malcolm Slaney, and Ron J Weiss,\n“CNN archi-"
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "tectures for large-scale audio classification,” in Proceedings of"
        },
        {
          "6. REFERENCES": "[15]\nJinhua Liang, Xubo Liu, Wenwu Wang, Mark D Plumbley,",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "the IEEE International Conference on Acoustics, Speech and"
        },
        {
          "6. REFERENCES": "Huy Phan, and Emmanouil Benetos,\n“Acoustic prompt\ntun-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": "Signal Processing (ICASSP). IEEE, 2017, pp. 131–135."
        },
        {
          "6. REFERENCES": "ing: Empowering large language models with audition capa-",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "IEEE Transactions on Audio, Speech and Language\nbilities,”",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        },
        {
          "6. REFERENCES": "Processing, 2023.",
          "[16] Huan Zhang, Vincent K.M. Cheung, Hayato Nishioka, Si-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "World Health Organization",
      "year": "2021",
      "venue": "WHO Fact Sheet"
    },
    {
      "citation_id": "3",
      "title": "Digital mental health market report 2025",
      "authors": [
        "Markets Research"
      ],
      "year": "2025",
      "venue": "Research and Markets"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartik Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "5",
      "title": "Bridging discrete and continuous: A multimodal strategy for complex emotion detection",
      "authors": [
        "Jiehui Jia",
        "Huan Zhang",
        "Jinhua Liang"
      ],
      "year": "2024",
      "venue": "IEEE International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "7",
      "title": "Fine-grained emotion detection in health-related online posts",
      "authors": [
        "Hina Khanpour",
        "Cornelia Caragea"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Text-based emotion detection: Advances, challenges, and opportunities",
      "authors": [
        "Francis Awindaogo",
        "Henry Nunoo-Mensah",
        "Wenyu Chen"
      ],
      "year": "2020",
      "venue": "Engineering Reports"
    },
    {
      "citation_id": "9",
      "title": "Emotions evoked by the sound of music: Characterization, classification, and measurement",
      "authors": [
        "Marcel Zentner",
        "Didier Grandjean",
        "Klaus Scherer"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "10",
      "title": "The New Music Therapist's Handbook",
      "authors": [
        "Suzanne Hanser"
      ],
      "year": "1999",
      "venue": "The New Music Therapist's Handbook"
    },
    {
      "citation_id": "11",
      "title": "Investigating emotion with music: an fmri study",
      "authors": [
        "Stefan Koelsch",
        "Thomas Fritz",
        "D Yves",
        "Von Cramon",
        "Karsten Müller",
        "Angela Friederici"
      ],
      "year": "2006",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "12",
      "title": "MuLan: A joint embedding of music audio and natural language",
      "authors": [
        "Wu",
        "J Jansen",
        "R Lee",
        "J Y Ganti",
        "Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)"
    },
    {
      "citation_id": "13",
      "title": "CLAP: Contrastive language-audio pre-training for crossmodal symbolic music information retrieval",
      "authors": [
        "Huang",
        "J Jansen",
        "R Lee",
        "J Y Ganti",
        "D P W Li",
        "Ellis"
      ],
      "year": "2022",
      "venue": "CLAP: Contrastive language-audio pre-training for crossmodal symbolic music information retrieval"
    },
    {
      "citation_id": "14",
      "title": "CLaMP 3: Universal music information retrieval across unaligned modalities and unseen languages",
      "authors": [
        "Wu",
        "J Jansen",
        "R Lee",
        "J Y Ganti",
        "Li"
      ],
      "year": "2025",
      "venue": "CLaMP 3: Universal music information retrieval across unaligned modalities and unseen languages"
    },
    {
      "citation_id": "15",
      "title": "Adapting language-audio models as few-shot audio learners",
      "authors": [
        "Jinhua Liang",
        "Xubo Liu",
        "Haohe Liu",
        "Huy Phan",
        "Emmanouil Benetos",
        "Mark Plumbley",
        "Wenwu Wang"
      ],
      "year": "2023",
      "venue": "INTER-SPEECH 2023"
    },
    {
      "citation_id": "16",
      "title": "Acoustic prompt tuning: Empowering large language models with audition capabilities",
      "authors": [
        "Jinhua Liang",
        "Xubo Liu",
        "Wenwu Wang",
        "Mark Plumbley",
        "Huy Phan",
        "Emmanouil Benetos"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Llaqo: Towards a querybased coach in expressive music performance assessment",
      "authors": [
        "Huan Zhang",
        "K Vincent",
        "Hayato Cheung",
        "Simon Nishioka",
        "Shinichi Dixon",
        "Furuya"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Caillon"
      ],
      "year": "2023",
      "venue": "Musiclm: Generating music from text"
    },
    {
      "citation_id": "19",
      "title": "Bridging paintings and music-exploring emotion based music generation through paintings",
      "authors": [
        "Tanisha Hisariya",
        "Huan Zhang",
        "Jinhua Liang"
      ],
      "venue": "17th International Symposium on Computer Music Multidisciplinary Research (CMMR 2025)"
    },
    {
      "citation_id": "20",
      "title": "Dexter: Learning and controlling performance expression with diffusion models",
      "authors": [
        "Huan Zhang",
        "Shreyan Chowdhury",
        "Carlos Eduardo Cancino-Chacón",
        "Jinhua Liang",
        "Simon Dixon",
        "Gerhard Widmer"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "21",
      "title": "Renderbox: Expressive performance rendering with text control",
      "authors": [
        "Huan Zhang",
        "Akira Maezawa",
        "Simon Dixon"
      ],
      "year": "2025",
      "venue": "Renderbox: Expressive performance rendering with text control"
    },
    {
      "citation_id": "22",
      "title": "Leveraging pre-trained audioldm for sound generation: A benchmark study",
      "authors": [
        "Yi Yuan",
        "Haohe Liu",
        "Jinhua Liang",
        "Xubo Liu",
        "Mark Plumbley",
        "Wenwu Wang"
      ],
      "year": "2023",
      "venue": "2023 31st European Signal Processing Conference"
    },
    {
      "citation_id": "23",
      "title": "Wavjourney: Compositional audio creation with large language models",
      "authors": [
        "Xubo Liu",
        "Zhongkai Zhu",
        "Haohe Liu",
        "Yi Yuan",
        "Meng Cui",
        "Qiushi Huang",
        "Jinhua Liang",
        "Yin Cao",
        "Qiuqiang Kong",
        "Mark Plumbley"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Wavcraft: Audio editing and generation with large language models",
      "authors": [
        "Jinhua Liang",
        "Huan Zhang",
        "Haohe Liu",
        "Yin Cao",
        "Qiuqiang Kong",
        "Xubo Liu",
        "Wenwu Wang",
        "Mark Plumbley",
        "Huy Phan",
        "Emmanouil Benetos"
      ],
      "year": "2024",
      "venue": "ICLR 2024 Workshop on LLM Agents"
    },
    {
      "citation_id": "25",
      "title": "Calm technology",
      "authors": [
        "Mark Weiser"
      ],
      "year": "1996",
      "venue": "The Disappearing Computer, Gilbert Cockton"
    },
    {
      "citation_id": "26",
      "title": "Digital therapeutics: Clinical robustness and regulatory frameworks",
      "authors": [
        "Joseph Kvedar",
        "Molly Joel Coye",
        "Wendy Everett"
      ],
      "year": "2022",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "27",
      "title": "Presence: Teleoperators and Virtual Environments",
      "authors": [
        "Mel Slater"
      ],
      "year": "2003",
      "venue": "Presence: Teleoperators and Virtual Environments"
    },
    {
      "citation_id": "28",
      "title": "Audio features for music emotion recognition: A survey",
      "authors": [
        "R Reep Panda",
        "R Malheiro",
        "Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "The restorative benefits of nature: Toward a conceptual framework",
      "authors": [
        "Stephen Kaplan"
      ],
      "year": "1995",
      "venue": "Journal of environmental psychology"
    },
    {
      "citation_id": "30",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P W Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Malcolm Platt",
        "Ron Slaney",
        "Weiss"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}