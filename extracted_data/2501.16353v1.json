{
  "paper_id": "2501.16353v1",
  "title": "Synthetic Data Generation By Supervised Neural Gas Network For Physiological Emotion Recognition Data",
  "published": "2025-01-19T15:34:05Z",
  "authors": [
    "S. Muhammad Hossein Mousavi"
  ],
  "keywords": [
    "Data Scarcity",
    "Synthetic Data Generation",
    "Neural Gas Network",
    "Physiological Signal",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Data scarcity remains a significant challenge in the field of emotion recognition using physiological signals, as acquiring comprehensive and diverse datasets is often prevented by privacy concerns and logistical constraints. This limitation restricts the development and generalization of robust emotion recognition models, making the need for effective synthetic data generation methods more critical. Emotion recognition from physiological signals such as EEG, ECG, and GSR plays a pivotal role in enhancing human-computer interaction and understanding human affective states. Utilizing these signals, this study introduces an innovative approach to synthetic data generation using a Supervised Neural Gas (SNG) network, which has demonstrated noteworthy speed advantages over established models like Conditional VAE, Conditional GAN, diffusion model, and Variational LSTM. The Neural Gas network, known for its adaptability in organizing data based on topological and feature-space proximity, provides a robust framework for generating real-world-like synthetic datasets that preserve the intrinsic patterns of physiological emotion data. Our implementation of the SNG efficiently processes the input data, creating synthetic instances that closely mimic the original data distributions, as demonstrated through comparative accuracy assessments. In experiments, while our approach did not universally outperform all models, it achieved superior performance against most of the evaluated models and offered significant improvements in processing time. These outcomes underscore the potential of using SNG networks for fast, efficient, and effective synthetic data generation in emotion recognition applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition  [1, 54, 55]  is the process of identifying human emotions using various data inputs and algorithms, playing a critical role in enhancing Human-Computer Interaction  [2, 20] . This capability is crucial for advancing fields such as personalized marketing  [18] , mental health monitoring  [19] , and adaptive learning systems  [3] , where understanding human feelings can significantly optimize interactions and outcomes. Physiological signals, such as electroencephalograms (EEG), electrocardiograms (ECG), and galvanic skin response (GSR), are key in this domain due to their direct measurement of bodily states that reflect emotional conditions  [3] . Figure  2  depicts EEG, ECG, and GSR sample signals belonging to the joy emotion state. These signals are particularly valuable in applications like lie detection, patient monitoring in healthcare, and enhancing user engagement in gaming and virtual reality, where accurate emotion detection can greatly enhance user experience and outcomes  [3, 4] . The use of EEG, ECG, and GSR in emotion recognition taps into unique aspects of physiological responses, enabling the detection of nuanced emotional states with a level of precision not achievable through behavioral analysis alone. EEG measures electrical activity in the brain to reveal patterns associated with different emotional states, while ECG assesses heart rate variability as an indicator of emotional arousal. GSR monitors changes in skin conductance, which varies with emotional intensity  [3, 4] . Together, these signals provide a comprehensive physiological footprint of emotional states. However, a significant challenge in utilizing these signals for emotion recognition is data scarcity  [5] [6] [7] [8] . The difficulties in collecting large, diverse, and representative datasets stem from privacy concerns, high collection costs, and the technical complexity of accurately capturing and processing these signals. This scarcity prevents the development of robust models that perform well across different populations and environments. Addressing this issue is crucial for the advancement of reliable and generalizable emotion recognition technologies, underscoring the need for innovative solutions like Synthetic Data Generation (SDG)  [9, 6, 10]  to bridge the data gap.\n\nSDG involves creating artificial datasets that statistically mirror real-world data, offering a promising solution to the issue of data scarcity in emotion recognition from physiological signals. By employing algorithms capable of learning and replicating the complex patterns found in actual physiological data, synthetic data can be generated to enhance existing datasets without compromising individual privacy. This method is particularly beneficial in fields where data collection is limited by ethical concerns, such as in health-related research  [11] . In the context of emotion recognition, synthetic datasets allow researchers and developers to train and test algorithms with a broader range of data inputs, increasing the robustness and accuracy of predictive models. Moreover, these synthetic datasets help overcome the barriers of limited sample sizes and lack of diversity in training data, thus supporting the development of emotion recognition systems that are both effective and adaptable across various real-world scenarios. Figure  1  illustrates the number of publications per year for emotion recognition, synthetic data generation, and physiological signal augmentation topics extracted from the PubMed dataset 1  . All of them show a growing number of publications in years.\n\nThe Neural Gas Network (NGN)  [17]  is a type of artificial neural network that adapts to input data without a predetermined network structure, efficiently organizing itself to reflect the topology of the data it processes. This flexibility makes NGN particularly useful in applications such as vector quantization  [15] , clustering, dimensionality reduction 2  , image segmentation  [12] , and feature extraction  [13] . The inherent adaptability of NGN to different data distributions allows it to capture complex patterns in high-dimensional spaces effectively. Extending NGN into a supervised learning framework enhances its applicability to tasks involving classification and prediction. In a supervised setting, NGN can utilize labeled data, guiding the network's adaptation process more precisely toward task-specific objectives. This makes Supervised Neural Gas (SNG)  [15]  well-suited for applications where precise categorization of complex patterns is crucial, such as in text categorization, image recognition, and bioinformatics  [16] .\n\nIn the synthetic data generation for emotion recognition using physiological signals like EEG, ECG, and GSR, SNG offers distinct advantages. By integrating the classification labels directly into the learning process, SNG can generate synthetic data that not only resembles the original data in terms of distribution but also aligns accurately with specific emotional states. This capability is critical for developing robust emotion recognition systems that require extensive, varied, and accurately labeled datasets for training. Compared to traditional methods, SNG provides a more direct mechanism for controlling the generation process based on the topology and distribution of input data, resulting in faster processing times and potentially higher accuracy in reflecting complex physiological and emotional correlations. This makes SNG an effective tool in overcoming the challenges of data scarcity and enhancing the performance of emotion recognition systems. According to our research, this is the first time the SNG has been used for SDG in this research. Here, we are looking forward to answering the following research questions. RQ1: Is SNG capable of generating real-world-like emotion recognition physiological EEG, ECG, and GSR signals by capturing complex relations in between signals? RQ2: Can SNG-generated data effectively outperform other SDG methods in terms of diversity, accuracy, and training speed? RQ3: Does SNG SDG address the challenge of data scarcity in emotion recognition using physiological signals such as EEG, ECG, GSR, and probably other physiological signals?\n\nWe have successfully applied Supervised Neural Gas (SNG) for synthetic data generation in emotion recognition, pioneering its use in creating realistic physiological datasets. Our approach has effectively bridged the gap in data scarcity, enhancing the robustness and training speed of emotion recognition systems. This achievement demonstrates the practical benefits and versatility of SNG, affirming its value as a powerful tool in effective computing research. The result section explains the achievement in detail.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "This section covers research conducted by other researchers in the field of SDG of physiological signals, especially in the field of emotion recognition. To save space, the chronicle is reported in Table  1  in an organized manner.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Theoretical Background",
      "text": "The Neural Gas Network (NGN)  [17]  is a type of artificial neural network known for its adaptability and self-organizing capabilities. Unlike traditional neural networks, NGN does not require a pre-defined network structure. Instead, it organically arranges itself to mirror the topology of the input data it processes. This flexibility allows NGN to efficiently handle various applications such as vector quantization, clustering, dimensionality reduction, image segmentation, and feature extraction. NGN's ability to adapt to different data distributions enables it to effectively capture complex patterns in high-dimensional spaces, making it highly effective for tasks that involve intricate data structures.\n\nTo convert a Neural Gas Network (NGN) into a supervised learning framework, creating a Supervised Neural Gas (SNG)  [15]  involves integrating target labels directly into the learning process. The primary step is to modify the typical unsupervised training method of NGN, which focuses on finding the optimal representation of data without considering any external labels. In SNG, during each iteration of the training process, not only are the nearest neurons to the input data points activated but they are also associated with specific target labels from the training dataset. This association allows the network to adjust its weights not just based on the proximity of the data points but also based on the correctness of the label prediction. Both NGN and SNG use competitive learning, where all neurons in the network compete to be closer to the current input data point. The winning neuron (the one closest to the input) and its neighbors (defined by a neighborhood function) are adjusted to be even closer to that data point. Over time, this competitive process results in a network that reflects the topology and, in the case of SNG, the class structure of the input space. NGN and SNG generally consist of two layers. The input layer receives the input features and connects each feature to every neuron in the next layer. Also, the competition layer is where neurons compete to be closest to the input vector; each neuron adjusts based on its distance from the input and is associated with a class label. There are no hidden layers as found in more traditional neural networks. The second layer comprises neurons that compete to be closer to the input data through a process that adjusts their weights. The learning rate and the neighborhood function, crucial parameters in NGN, are customized to decrease over time in a way that reflects both the error in label prediction and the topological accuracy. Additionally, the cost function in SNG is designed to incorporate a penalty for misclassification, thus aligning the neuron adjustments more closely with the supervised learning objectives. This method effectively transforms the NGN into an SNG, enabling it to perform classification tasks by utilizing the structured adaptation of neurons in response to labeled data, enhancing both the accuracy and applicability of the network in complex pattern recognition scenarios. The typical learning update equation for a Neural Gas Network (NGN) is as follows: ùëÅùê∫ùëÅ = ùë§ (ùë° + 1) = ùë§ (ùë°) + ùúñ(ùë°) ‚ãÖ ‚Ñé (ùë°, ùëò(ùëñ, ùë•)) ‚ãÖ (ùë• -ùë§ (ùë°))\n\n(1) Where, ùë§ (ùë°) is the weight vector of the ùëñ-th neuron at the time ùë°. ùë• is the current input vector. ùúñ(ùë°) is the learning rate at the time ùë°, which decreases over time. ‚Ñé (ùë°, ùëò(ùëñ, ùë•)) is the neighborhood function around the winning neuron. This function decreases with increasing rank ùëò(ùëñ, ùë•) of the neuron ùëñ when ordered by distance from the input vector ùë•. The function is also dependent on a parameter ùúÜ(ùë°), which is a measure of the neighborhood size that decreases over time. ùëò(ùëñ, ùë•) is the rank of neuron ùëñ in terms of its distance from the input vector ùë•, with the closest neuron having the rank 0. Also, ùë° + 1 represents the next time step.\n\nThe adaptation of NGN to a Supervised Neural Gas (SNG) involves modifying the update rule to include the influence of the target label. The modified update equation is as follows: ùëÜùëÅùê∫ = ùë§ (ùë° + 1) = ùë§ (ùë°) + ùúñ(ùë°) ‚ãÖ ‚Ñé (ùë°, ùëò(ùëñ, ùë•)) ‚ãÖ (ùë• -ùë§ (ùë°)) + ùõº(ùë°) ‚ãÖ ùëî(ùë°, ùë¶, ùë¶ ) ‚ãÖ (ùë¶ -ùë¶ )\n\n(2) Where, ùë¶ is the actual target label for the input ùë•. ùë¶ is the predicted label from the neuron ùëñ. ùõº(ùë°) is an additional learning rate parameter governing the adaptation based on the label mismatch. Furthermore, ùëî(ùë°, ùë¶, ùë¶ ) is a function that measures the error in label prediction, which could be a simple delta function ùõø(ùë¶, ùë¶ ) indicating 1 when ùë¶ ‚â† ùë¶ and 0 otherwise. Figure  3  represents how NGN fills the topology of the destination. There are 200 blue dot samples as the destination topology and 150 red dot samples or neurons for NGN, in which over 300 iterations of NGN tris fill the shape with those neurons. That 150 neurons is exactly the desired number of synthetic samples that we are looking for. This figure shows the potential of NGN to generate similar-like samples by fitting the topology.\n\nWhere, ùë§ , (ùë°) is the weight vector of the ùëñ-th neuron for class ùëê at iteration ùë°. These weights represent the prototype vectors that are being adapted to the data. ùë• is the input vector from the training dataset associated with the class ùëê. Also, ùúÇ(ùë°) is the learning rate at iteration ùë°, which decreases over time. It governs how much the neuron weights are updated in each step and is calculated as:\n\n‚Ñé (ùë°, ùëò(ùëê, ùëñ, ùë•)) is the neighborhood function, which decreases with the rank ùëò(ùëê, ùëñ, ùë•). This function weakens the influence of the input vector based on the rank of the neuron within the class: ‚Ñé (ùë°, ùëò) = ùëí ( )  (5) ùúÜ(ùë°) is the neighborhood range, which also decreases over time, controls the extent of the local neighborhood around the best-matching unit that gets updated:\n\nFinally, for the generating synthetic sample, we have the following: The training phase (3) involves adapting neuron weights to minimize the distance from the input vectors while maintaining the structure of the input space defined by class labels. The generation phase  (7)  then uses these adapted neuron weights as centers to generate new data points by adding controlled Gaussian noise, effectively creating synthetic examples that are statistically similar to the original samples but include slight variations to enhance robustness and data privacy. This two-phase approach allows SNG to not only classify and categorize data effectively but also to generate new samples that can be used to augment the original dataset, addressing issues such as data scarcity and enhancing model training without compromising data privacy. Figure  4  depicts the flowchart of the proposed method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluations And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "ÔÇ∑ Dataset",
      "text": "We employed two physiological datasets in our experiments for validation as follows. First, we used a standard IEEE emotion recognition dataset called \"BRAINWAVE EEG DATASET\"  [40, 41] , which is available online at 4 [42]. This dataset consists of brainwave EEG signals from eight subjects collected in a lab-controlled environment under a specific visualization experiment. The data include simple timestamps followed by the five bands of brainwave signals reading from the five electrodes of the emotive insight sensor: Theta, Alpha, Low Beta, High Beta, and Gamma. More than 10,000 brainwaves were collected. However, after applying several data filtering techniques, including the removal of noise signals and margins from the start and end of each picture showing time, only 1550 brainwaves remained. insider threats by analyzing brain activity through EEG signals involves understanding how certain brain activity patterns will correlate with deceptive or malicious intent, possibly identifying individuals who will pose an insider threat before any malicious actions occur. The concept involves using EEG to detect subconscious or conscious signs of malicious intent or deception in individuals. By analyzing brainwaves, researchers aim to find biomarkers or patterns that indicate a risk of insider threats. Each picture is attached with two main values: Valence, which shows the degree of positive or negative effect the image evokes, and Arousal, which shows the intensity of the effect the image evokes. Images with a valence value equal to one are labeled as zero: ''High Risk''. Images with valence values equal to two and three are labeled as one: ''Medium Risk''. Images with valence values equal to four and five are labeled as two: ''Normal''. Images with valence values equal to six and seven are labeled as three: ''Low Risk''. All images selected as part of this experiment had arousal values of more than five to ensure their intense impact on the participants.\n\nThe second dataset is called \"Emotional Status Determination using Physiological Parameters Data Set\" or, in short, ESD  [44] , available online by 5 [43]. This dataset is created using the Galvanic Skin Response Sensor and Electrocardiogram sensor of MySignals Healthcare Toolkit. MySignals toolkit consists of the Arduino Uno board and different sensor ports. The sensors were connected to the different ports of the hardware kit, which Arduino SDK controlled. MySignals is a development platform for medical devices and e-Health applications. It is a multichannel physiological signal recorder that measures more than 15 different biometric parameters such as pulse, breath rate, oxygen in blood, electrocardiogram signals, blood pressure, muscle electromyography signals, glucose levels, galvanic skin response, lung capacity, snore waves, patient position, airflow and body scale parameters (weight, bone mass, body fat, muscle mass, body water, visceral fat, Basal Metabolic Rate and Body Mass Index). This novel dataset can be applied for training and evaluating deep learning, machine learning, and data analytics models to deal with binary and multi-class stress and emotion classification problems. The dataset consisted of 253 samples of 14 features of GSR and ECG with different statistical properties. Final columns indicates one of four emotional classes of Fear, Angry, Happy, and Sad. Furthermore, the elicitation was done using 17 videos on all participants, and they reported their emotions on forms after watching them.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "ÔÇ∑ Classifier And Metrics",
      "text": "For the classification, we selected XGBoost  [45, 46]  because we found it the most effective of others during the experiment. XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework, providing a scalable, fast, and accurate method for regression, classification, and ranking problems. For evaluation, seven metrics of accuracy, standard deviation (std), precision, recall, F-1 score, train runtime, and Mean Square Error (MSE) between the original and the synthetic samples have been used. Accuracy is a measure of how often a model correctly predicts the outcome, representing the ratio of correct predictions to the total number of predictions. The std quantifies the amount of variation or dispersion within a set of data values, indicating how spread out the data points are from the mean. Precision, often used in classification problems, measures the accuracy of positive predictions, defined as the ratio of true positive results to the total predicted positives. Recall, also known as sensitivity, assesses the model's ability to identify all relevant instances, calculated as the ratio of true positive results to the actual total positives in the data. The F1-score combines precision and recall into a single metric by calculating their harmonic mean, providing a balanced measure of a model's accuracy, particularly useful when dealing with imbalanced datasets  [45, 46] . Train runtime is to evaluate the complexity of each algorithm. Finally, the MSE has been used to evaluate the similarity between the original and the synthetic sample. The evaluation is based on three categories: the original or the baseline, the synthetic data, and a mix of them for all metrics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ÔÇ∑ Comparison Algorithms",
      "text": "For comparison, we used four main algorithms in the field of SDG: Conditional Variational Auto Encoders (C-VAE)  [47] , Conditional Generative Adversarial Networks (C-GAN)  [48] , Single-Step Diffusion Model  [51] , and Variational Long Short-Term Memory (V-LSTM)  [49, 50] . Conditional comes from the model's ability to generate data conditioned on specific input information, such as labels or other related features.\n\nA Conditional Variational Autoencoder (C-VAE) is a type of generative model that extends the basic Variational Autoencoder (VAE) framework by incorporating conditional parameters, enabling the generation of data with specific attributes. In SDG, C-VAEs are particularly useful for creating complex and diverse datasets that adhere to specified conditions or labels. The model achieves this by conditioning both the encoder and decoder on additional input features, allowing it to learn a conditioned distribution of the data. This makes C-VAEs ideal for tasks where control over certain characteristics of the generated data is crucial, such as generating patient data with specific medical attributes or images with designated object types. Conditional Generative Adversarial Networks (C-GANs) adapt the Generative Adversarial Network (GAN) architecture by incorporating label information into both the generator and discriminator, guiding the data generation process to produce data with specific characteristics. In SDG, C-GANs are valued for their ability to generate highly realistic and detailed samples under controlled conditions. The discriminator in a C-GAN learns to verify not only the authenticity of the generated data but also its alignment with the conditional labels, while the generator strives to produce data that passes the discriminator's tests. This dual-drive mechanism enables C-GANs to create precise and diverse synthetic datasets, which are useful in scenarios where fidelity to real-world data distributions is critical, such as in training machine learning models where real data may be scarce or sensitive. The Single-Step Diffusion Model offers a streamlined approach to SDG for tabular data. This model simplifies the traditional, multi-step denoising process seen in conventional diffusion models by condensing it into a single denoising step. It works by adding Gaussian noise to the original data and then using a neural network to recover the clean, noise-free data. This adaptation is particularly suitable for tabular data, where the relationships between variables can be effectively captured and modeled through a single recovery phase. By focusing on a single-step recovery, the model efficiently learns the underlying data distribution, which is crucial for generating high-quality synthetic datasets that maintain the statistical properties of the original data without the computational complexity of traditional diffusion processes. The V-LSTM for SDG is an advanced technique that integrates variational dropout into the LSTM architecture to enhance its effectiveness in generating synthetic sequential data. By applying the same dropout mask at each time step across the hidden units, this method maintains temporal consistency in dropout application, which is crucial for learning dependencies in sequence data. This consistency allows the LSTM to better model the intricate temporal dynamics and reduce overfitting, resulting in more robust generalization. The key advantage of SDG is that Variational Dropout LSTM can generate high-quality, diverse sequences that closely mirror real-world distributions while managing the risk of overfitting to the training data. This makes it particularly useful for applications where the authenticity and variability of synthetic sequences are critical, such as in financial forecasting, healthcare data simulation, and other areas where sequence data is central.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "ÔÇ∑ Experiment Setup",
      "text": "2000 synthetic samples were generated for each dataset and algorithm. All experiments were conducted using 70% training and 30% testing over five runs. Also, all algorithms passed through 100 iterations to synthesize samples. Noise level and batch size were also considered 0.1 and 32 for all algorithms, respectively. Specifically for the SNG, the number of neurons is considered 10, as less than this number brings unreliable classification accuracy with high MSE, and higher values bring no significant performance improvement but increase complexity. Also, the number of samples for the Brain Wave EEG dataset is 1550, which in mixed with synthetic will be 3550, and the EDS dataset covers 253 samples, which in mixed with synthetic data will be 2253 samples. The number of samples to be generated for each class is considered to be equal. Figure  5  illustrates the comparison between a sample signal from Brain Wave EEG data in the original and the synthetic form by the SNG algorithm as a line plot. By looking at the data points, a high level of similarity is visible. However, they are not the same as we achieved an MSE of 0.059 by SNG for this dataset. Also, Figure  6  depicts a scatter plot of features three and four from the ESD dataset regarding original and synthetic samples generated by the SNG algorithm. The similarity between the two plots shows that SNG could successfully capture the data points samples relation of the data and generate corresponding synthetic samples with NGN algorithmic structural distribution. Figure  7  represents the t-distributed Stochastic Neighbor Embedding (t-SNE) plot for both synthetic datasets. The t-SNE is a powerful machine learning algorithm used to visualize high-dimensional data by reducing it to two or three dimensions, making it easier to plot and interpret visually  [52] . This technique is particularly well-suited for the visualization of datasets with complex structures at multiple scales. t-SNE works by converting similarities between data points to joint probabilities and then minimizing the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. This results in a plot where similar data points are placed close together and dissimilar points are placed far apart, thus revealing intrinsic patterns in the data, such as clusters or groups. The plots reveal several tightly grouped clusters as well as some outliers, suggesting natural groupings within the data. However, the ESD t-SNE plot is more distinctive than the Brain Wave EEG t-SNE plot. Figure  8  depicts the SNG algorithm's loss plot for generating synthetic samples of both datasets. This plot shows the training loss decreasing from around six to below two over 100 iterations. The sharp decline early on suggests that the model quickly learned a significant amount of information from the dataset, and then the rate of decrease slowed, indicating diminishing returns on learning as the training progressed. The training loss for the ESD dataset starts at around three and decreases to around one, also over 100 iterations. Similar to the Brain Wave EEG plot, there is a notable rapid decrease in loss at the beginning, followed by a gradual flattening of the curve, which typically reflects the model reaching a point of stabilization where additional learning provides smaller improvements.   [53]  is a method of plotting numeric data. It is similar to a box plot but with a rotated kernel density plot on each side. This type of plot provides a deeper understanding of the distribution of the data, showing peaks, valleys, and tails more clearly than box plots. The violin shape of the plot displays the density of the data at different values, with the width of the plot representing the frequency of data occurrences at each level. This makes it excellent for comparing multiple distributions, particularly to highlight differences in distribution shape, central tendency, and variability. Violin plots are often used in exploratory data analysis to visualize and compare the distribution of data across different categories or groups. The first row of this figure belongs to the Brain Wave EEG dataset. Plots in this row illustrate that models trained on original data tend to have slightly higher accuracy, maintaining a narrow distribution around 0.95 to 0.98. Synthetic data shows a broader distribution, indicating greater variability in model performance, with accuracies ranging broadly around 0.89 to 0.94. The combined data retains high accuracy similar to the original but with a slightly increased variability compared to the original alone. As for the second row of the ESD dataset, the original data also shows high accuracy but with a wider distribution than seen in the Brain Wave EEG, suggesting more variability in model performance on this dataset. Synthetic data for ESD also shows high variability but maintains a relatively high accuracy. The combined data seems to perform the best in terms of both median accuracy and consistency, suggesting that combining original and synthetic data may provide a stability benefit in model training for this dataset. Furthermore, Table  2  covers all experiment results for different algorithms using different metrics on both datasets using the XGBoost classifier for the test phase.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "ÔÇ∑ Discussion",
      "text": "According to the results reported in Table  2 , across all SDG methods and both datasets, combining synthetic data with baseline data consistently enhances performance metrics such as accuracy, precision, recall, and F1 score compared to using synthetic data alone. This suggests that synthetic data, while beneficial, is most effective when combined with the original/baseline data due to the fact that it has more samples and is diverse for training. C-GAN and SS-Diffusion Models show particularly strong performance enhancements when synthetic data is combined with baseline data, especially for the ESD dataset where accuracy improves significantly (C-GAN: 90.45% to 91.57%, SS-Diffusion: 98.90% to 98.33%). V-LSTM demonstrates lower accuracy overall, particularly with the Brain Wave EEG dataset. However, there's a notable improvement when synthetic data is used in combination, underscoring the potential for synthetic data to boost weaker models. SNG exhibits high stability and effectiveness, with minimal standard deviation and high accuracy, making it a robust choice across datasets. The standard deviation in performance metrics across methods varies, with some methods, like SNG, showing remarkable consistency. Training runtime also varies significantly across methods, with some, like SNG, being exceptionally quick, suggesting efficiency in training. The confusion matrix for SNG synthetic data indicates high class-specific accuracy, particularly in classes 1 and 4. Misclassifications are mostly contained within adjacent classes, which could point to areas where the model's discrimination between similar classes could be improved. The C-VAE method exhibits moderate MSE values, indicating a fair approximation to the original data but with potential for further refinement. C-GAN, with higher MSE values, shows greater deviation, suggesting that while it can generate diverse data, it may not always closely mirror the original dataset. The SS-Diffusion Model stands out with lower MSE values, indicating that it closely replicates the original data and effectively captures the underlying patterns. V-LSTM shows the highest MSE, indicating significant discrepancies and the least fidelity to the original data among the methods tested. Lastly, the NGN demonstrates low MSE values, suggesting high accuracy and a close match to the original dataset, making it one of the more reliable methods in terms of data generation fidelity. Additionally, it has to be mentioned that 100 iterations were sufficient for all algorithms as both datasets were small-sized, and after 80 iterations, all of them converged.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This study has significantly advanced the field of emotion recognition using physiological signals by employing an SNG network for synthetic data generation. Our research clearly demonstrates that SNG, while a novel application in this domain, effectively addresses the critical challenge of data scarcity that hampers the development of robust emotion recognition systems. By generating synthetic data that closely mirrors the complex relationships and distributions of real physiological signals, SNG has shown its potential to enhance the accuracy, diversity, and speed of training emotion recognition models. Our experiments indicate that the integration of synthetic data with baseline data consistently improves model performance across various metrics, including accuracy, precision, recall, and F1 score. Notably, SNG exhibited superior stability and minimal variability in performance, making it a robust choice for synthetic data generation across different datasets. This is particularly evident in its high class-specific accuracy and the efficiency of its training process, as reflected in the markedly low mean squared errors and short training times compared to other tested methods such as C-VAE, C-GAN, SS-Diffusion Model, and V-LSTM. The findings affirm that SNG not only successfully generates data that enhances model training but also contributes to more accurate and reliable emotion recognition systems. As we move forward, the application of SNG in other domains of affective computing and beyond holds promising potential to overcome similar challenges of data limitation. This pioneering use of SNG in emotion recognition sets a precedent for further research and development in the field, potentially revolutionizing how synthetic data generation is approached in enhancing human-computer interaction. As for future works, using SNG SDG for other modalities, such as image and body motion, is in progress. Furthermore, integrating SNG with other machine learning models and ensemble methods such as C-GAN, C-VAE, and transformers could enhance the robustness and accuracy of systems designed for emotion recognition data synthesis.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: depicts EEG, ECG, and GSR sample signals belonging to the",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the",
      "page": 2
    },
    {
      "caption": "Figure 1: The number of publications per year for emotion recognition, synthetic data generation, and physiological signal augmentation topics",
      "page": 3
    },
    {
      "caption": "Figure 2: EEG, ECG, and GSR sample signals belonging to the joy emotional state",
      "page": 3
    },
    {
      "caption": "Figure 3: represents how NGN fills the topology of the",
      "page": 5
    },
    {
      "caption": "Figure 3: NGN topology fitting process over 300 iterations using 150 neurons",
      "page": 5
    },
    {
      "caption": "Figure 4: depicts the flowchart of the proposed method.",
      "page": 6
    },
    {
      "caption": "Figure 4: Proposed method‚Äôs flowchart",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates the comparison between a sample signal from Brain Wave",
      "page": 9
    },
    {
      "caption": "Figure 6: depicts a scatter plot of features three and four from the ESD dataset",
      "page": 9
    },
    {
      "caption": "Figure 5: Line plot of a sample from the Brain Wave EEG dataset (left the original and right the synthetic generated by SNG)",
      "page": 9
    },
    {
      "caption": "Figure 6: The scatter plot of features three and four from the ESD dataset regarding original and synthetic samples generated by the SNG",
      "page": 9
    },
    {
      "caption": "Figure 7: represents the t-distributed Stochastic Neighbor Embedding (t-SNE) plot for both synthetic",
      "page": 9
    },
    {
      "caption": "Figure 7: The t-SNE plot of both synthetic datasets (Brain Wave EEG dataset: Class 0: Low Risk, Class 1:Low Medium Risk, Class 2:Medium",
      "page": 10
    },
    {
      "caption": "Figure 8: depicts the SNG algorithm‚Äôs loss plot for generating synthetic samples of both datasets. This",
      "page": 10
    },
    {
      "caption": "Figure 8: Training Loss Over 100 Iterations for Brain Wave EEG and ESD Datasets",
      "page": 10
    },
    {
      "caption": "Figure 9: illustrates violin plots for both datasets with different data combinations. A violin plot [53] is",
      "page": 10
    },
    {
      "caption": "Figure 9: Violin plot of both datasets in different combinations (first row: Brain Wave EEG dataset and the second row ESD dataset)",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: in an organized manner.",
      "data": [
        {
          "#": "1",
          "Author(s)": "Nita, Sihem, et al",
          "Subject": "ECG SDG",
          "Challenge": "Data Scarcity",
          "Contribution/Solution": "Using the CNN algorithm for emotional ECG signals \nSDG by the DREAMER database",
          "Year": "2022",
          "Cite": "[21]"
        },
        {
          "#": "2",
          "Author(s)": "Guo,  Gengyuan,  et \nal",
          "Subject": "ECG, \nGSR, \nand \nrespiration \n(RSP) \nSDG",
          "Challenge": "Data Scarcity",
          "Contribution/Solution": "Using CNN-SVM technique for SDG of ECG, GSR, \nand RSP signals for emotion recognition data",
          "Year": "2022",
          "Cite": "[22]"
        },
        {
          "#": "3",
          "Author(s)": "Chen, \nYu, \nRui \nChang",
          "Subject": "EEG SDG",
          "Challenge": "Data Scarcity",
          "Contribution/Solution": "Using SMOTE CNN algorithm for EEG signals SDG \nfor emotion recognition data on DEAP dataset",
          "Year": "2021",
          "Cite": "[23]"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "5",
          "Ari, Berna, et al": "Nasrallah,  Chawki, \net al",
          "EEG SDG": "Electromyography \n(EMG) SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Conditional  GAN  has  been  used  for  EMG  signals \nSDG or augmentation for emotion recognition data",
          "2022": "2023",
          "[24]": "[25]"
        },
        {
          "4": "6",
          "Ari, Berna, et al": "Wang, Fang, et al",
          "EEG SDG": "EEG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using  CNN  for  EEG  signals  SDG  and  for  emotion \nrecognition data",
          "2022": "2018",
          "[24]": "[26]"
        },
        {
          "4": "7",
          "Ari, Berna, et al": "Hasnul, M.A., et al",
          "EEG SDG": "ECG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "They used multiple filter techniques to augment ECG \nemotional \nsignals \nand \nevaluate \nusing \ndifferent \nclassifiers  on  three  datasets  of  DREAMER,  A2ES, \nand AMIGOS",
          "2022": "2023",
          "[24]": "[27]"
        },
        {
          "4": "8",
          "Ari, Berna, et al": "Kalashami, M.P., et \nal",
          "EEG SDG": "EEG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using Conditional Wasserstein GAN (CWGAN) for \nEEG signal SDG for emotion recognition data on the \nDEAP dataset",
          "2022": "2022",
          "[24]": "[28]"
        },
        {
          "4": "9",
          "Ari, Berna, et al": "Furdui, Andrei, et al",
          "EEG SDG": "GSR and ECG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using Auxiliary Conditioned Wasserstein Generative \nAdversarial  Network  with  Gradient  Penalty  (AC-\nWGAN-GP)  to  synthesize/augment  GSR  and  ECG \nsignals for emotion recognition data",
          "2022": "2021",
          "[24]": "[29]"
        },
        {
          "4": "10",
          "Ari, Berna, et al": "Grossi, A., et al",
          "EEG SDG": "Photoplethysmogram \n(PPG) SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using  Complete  Ensemble  Empirical  Mode  De-\ncomposition with Adaptive  Noise  (CEEMDAN)  for \nPPG signals SDG",
          "2022": "2023",
          "[24]": "[30]"
        },
        {
          "4": "11",
          "Ari, Berna, et al": "Adib,  Edmond.,  et \nal",
          "EEG SDG": "ECG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using GAN for augmenting ECG signals",
          "2022": "2021",
          "[24]": "[31]"
        },
        {
          "4": "12",
          "Ari, Berna, et al": "Hazra,  Debapriya., \net al",
          "EEG SDG": "ECG, \nEEG, \nEMG, \nPPG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "They \npropose \na \nnovel \nGAN  model, \nnamed \nSynSigGAN,  for  automating  the  generation  of  any \nsynthetic physiological signals",
          "2022": "2020",
          "[24]": "[32]"
        },
        {
          "4": "13",
          "Ari, Berna, et al": "Silva, Diogo., et al",
          "EEG SDG": "Heart Rate (HR) SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "They  used  a  stochastic  system  of  Gaussian  copulas \nintegrated in a Markov chain to augment HR signals",
          "2022": "2020",
          "[24]": "[33]"
        },
        {
          "4": "14",
          "Ari, Berna, et al": "Pereira, \nDiogo \nFilipe., et al",
          "EEG SDG": "ECG \nand \nballistocardiography \n(BCG) SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using  Gaussian  Copula  for  ECG  and  BCG  signals \nSDG",
          "2022": "2019",
          "[24]": "[34]"
        },
        {
          "4": "15",
          "Ari, Berna, et al": "Saldanha, Jane, et al",
          "EEG SDG": "RSP SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using  Variational  Autoencoders \nlike  Multilayer \nPerceptron  VAE  (MLP-VAE),  Convolutional  VAE \n(CVAE), and Conditional VAE for RSP SDG",
          "2022": "2022",
          "[24]": "[35]"
        },
        {
          "4": "16",
          "Ari, Berna, et al": "Soingern,  Nutapol, \net al",
          "EEG SDG": "EEG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using the diffusion model method to augment EEG \nsignals",
          "2022": "2023",
          "[24]": "[36]"
        },
        {
          "4": "17",
          "Ari, Berna, et al": "Siddhad, Gourav, et \nal",
          "EEG SDG": "EEG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using the diffusion model method to augment EEG \nsignals on DEAP dataset for emotion recognition data",
          "2022": "2024",
          "[24]": "[37]"
        },
        {
          "4": "18",
          "Ari, Berna, et al": "Takahashi, Kahoko, \net al",
          "EEG SDG": "EEG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "Using the LSTM algorithm to augment EEG signals",
          "2022": "2022",
          "[24]": "[38]"
        },
        {
          "4": "19",
          "Ari, Berna, et al": "Li, Xiaomin., et al",
          "EEG SDG": "ECG SDG",
          "Data Scarcity": "Data Scarcity",
          "Proposed Extreme Learning Machine Wavelet Auto \nEncoder  (ELM-W-AE)  for  EEG  signals  SDG  for \nemotion recognition data": "They  proposed  TTS-CGAN,  a  Transformer-based \nTime-Series  Conditional  GAN \nto  augment  ECG \nsignals from the PTB Diagnostic ECG dataset",
          "2022": "2022",
          "[24]": "[39]"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: covers all experiment",
      "data": [
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "Synthetic",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "Synthetic"
        },
        {
          "SDG Method": "C-VAE \nConverge: itr 73",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "62.30 %",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "83.85 %"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.020",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.023"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.61",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.83"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.62",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.84"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.62",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.84"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "2 min, 3 sec",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "35 sec"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.093",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.156"
        },
        {
          "SDG Method": "C-GAN \nConverge: itr 88",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "78.10 %",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "90.45 %"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.095",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.025"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.78",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.89"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.77",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.90"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.78",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.90"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "19 sec",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "17 sec"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.130",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.183"
        },
        {
          "SDG Method": "SS-Diffusion \nModel \nConverge: itr 65",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "86.20 %",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "98.90 %"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.028",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.029"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.86",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.99"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.85",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.98"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.86",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.98"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "18 sec",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "14 sec"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.186",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.127"
        },
        {
          "SDG Method": "V-LSTM \nConverge: itr 90",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "50.67 %",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "80.33 %"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.028",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.403"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.50",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.80"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.50",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.81"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.50",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.80"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "50 sec",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "23 sec"
        },
        {
          "SDG Method": "",
          "Brain Wave EEG Dataset ‚Äì \nBaseline: 96.20%, std:0.016": "0.287",
          "ESD Dataset \n Baseline:95.31 %, std: 0.029": "0.301"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: , across all SDG methods and both datasets, combining",
      "data": [
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "std",
          "91.90 %": "0.006",
          "93.35 %": "0.011",
          "96.01 %": "0.009",
          "97.37 %": "0.006"
        },
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "Precision",
          "91.90 %": "0.91",
          "93.35 %": "0.93",
          "96.01 %": "0.95",
          "97.37 %": "0.96"
        },
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "Recall",
          "91.90 %": "0.92",
          "93.35 %": "0.92",
          "96.01 %": "0.95",
          "97.37 %": "0.96"
        },
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "F1 Score",
          "91.90 %": "0.91",
          "93.35 %": "0.93",
          "96.01 %": "0.95",
          "97.37 %": "0.96"
        },
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "Train Runtime",
          "91.90 %": "8 Sec",
          "93.35 %": "-",
          "96.01 %": "2 sec",
          "97.37 %": "-"
        },
        {
          "SNG \nConverge: itr 81": "",
          "Avg Acc": "MSE",
          "91.90 %": "0.059",
          "93.35 %": "-",
          "96.01 %": "0.101",
          "97.37 %": "-"
        },
        {
          "SNG \nConverge: itr 81": "NGN Synthetic Data  \nConfusion Matrix ‚Äì Class \nnames are based on Figure \nseven‚Äôs caption",
          "Avg Acc": "Class 1 \nClass 2 \nClass 3 \nClass 4",
          "91.90 %": "[94.6  0.6    0.      0.    ] \n [ 0.     92.8  4.4    4.4  ] \n [ 0.     6.      86.6  6.6  ] \n [ 0.     4.8    5.6    89.6]",
          "93.35 %": "",
          "96.01 %": "[98.8  0.      0.2    0.   ] \n [ 0.     91.6  1.8    5.2 ] \n [ 0.2   2.6    97.6  0.2 ] \n [ 0.     6.4    1.4    91. ]",
          "97.37 %": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Handbook of human-computer interaction",
      "authors": [
        "Martin Helander"
      ],
      "year": "2014",
      "venue": "Handbook of human-computer interaction"
    },
    {
      "citation_id": "3",
      "title": "A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals",
      "authors": [
        "Patricia Bota"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from physiological signal analysis: A review",
      "authors": [
        "Maria Egger",
        "Matthias Ley",
        "Sten Hanke"
      ],
      "year": "2019",
      "venue": "Computer Science"
    },
    {
      "citation_id": "5",
      "title": "Machine learning for synthetic data generation: a review",
      "authors": [
        "Yingzhou Lu"
      ],
      "year": "2023",
      "venue": "Machine learning for synthetic data generation: a review",
      "arxiv": "arXiv:2302.04062"
    },
    {
      "citation_id": "6",
      "title": "Survey on synthetic data generation, evaluation methods and GANs",
      "authors": [
        "Alvaro Figueira",
        "Bruno Vaz"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "7",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "Lin Shu"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "SynSigGAN: Generative adversarial networks for synthetic biomedical signal generation",
      "authors": [
        "Debapriya Hazra",
        "Yung-Cheol Byun"
      ],
      "year": "2020",
      "venue": "Biology"
    },
    {
      "citation_id": "9",
      "title": "Comprehensive exploration of synthetic data generation: A survey",
      "authors": [
        "Andr√© Bauer"
      ],
      "year": "2024",
      "venue": "Comprehensive exploration of synthetic data generation: A survey",
      "arxiv": "arXiv:2401.02524"
    },
    {
      "citation_id": "10",
      "title": "A review: Data pre-processing and data augmentation techniques",
      "authors": [
        "Kiran Maharana",
        "Surajit Mondal",
        "Bhushankumar Nemade"
      ],
      "year": "2022",
      "venue": "Global Transitions Proceedings"
    },
    {
      "citation_id": "11",
      "title": "Synthetic data generation: State of the art in health care domain",
      "authors": [
        "Murtaza",
        "Hajra"
      ],
      "year": "2023",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "12",
      "title": "Neural Gas Network Image Features and Segmentation for Brain Tumor Detection Using Magnetic Resonance Imaging Data",
      "authors": [
        "S Mousavi"
      ],
      "year": "2023",
      "venue": "Neural Gas Network Image Features and Segmentation for Brain Tumor Detection Using Magnetic Resonance Imaging Data",
      "arxiv": "arXiv:2301.12176"
    },
    {
      "citation_id": "13",
      "title": "PSO Fuzzy XGBoost Classifier Boosted with Neural Gas Features on EEG Signals in Emotion Recognition",
      "authors": [
        "Seyed Mousavi",
        "Muhammad Hossein"
      ],
      "year": "2024",
      "venue": "PSO Fuzzy XGBoost Classifier Boosted with Neural Gas Features on EEG Signals in Emotion Recognition",
      "arxiv": "arXiv:2407.09950"
    },
    {
      "citation_id": "14",
      "title": "Neural-gas' network for vector quantization and its application to time-series prediction",
      "authors": [
        "Thomas Martinetz",
        "G Stanislav",
        "Klaus Berkovich",
        "Schulten"
      ],
      "year": "1993",
      "venue": "Neural-gas' network for vector quantization and its application to time-series prediction"
    },
    {
      "citation_id": "15",
      "title": "Supervised neural gas with general similarity measure",
      "authors": [
        "Barbara Hammer",
        "Marc Strickert",
        "Thomas Villmann"
      ],
      "year": "2005",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "16",
      "title": "Supervised neural gas for classification of functional data and its application to the analysis of clinical proteom spectra",
      "authors": [
        "Frank Schleif",
        "Thomas Michael",
        "Barbara Villmann",
        "Hammer"
      ],
      "year": "2007",
      "venue": "Computational and Ambient Intelligence: 9th International Work-Conference on Artificial Neural Networks, IWANN 2007"
    },
    {
      "citation_id": "17",
      "title": "A\" neural-gas\" network learns topologies",
      "authors": [
        "Thomas Martinetz",
        "Klaus Schulten"
      ],
      "year": "1991",
      "venue": "A\" neural-gas\" network learns topologies"
    },
    {
      "citation_id": "18",
      "title": "Brand Marketing Strategy Based on User Emotion Recognition Model of Consumer",
      "authors": [
        "Wei Wang",
        "Liwen Sun"
      ],
      "year": "2024",
      "venue": "Brand Marketing Strategy Based on User Emotion Recognition Model of Consumer"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using supervised deep recurrent system for mental health monitoring",
      "authors": [
        "Nelly Elsayed"
      ],
      "year": "2022",
      "venue": "2022 IEEE 8th World Forum on Internet of Things"
    },
    {
      "citation_id": "20",
      "title": "Using Genetic Programming for Making a New Evolutionary Artwork, Based on Human-Computer Interactions for Autism Rehabilitation",
      "authors": [
        "M Mousavi",
        "Narges Aghsaghloo"
      ],
      "year": "2018",
      "venue": "The third International Conference on Intelligent Decision Science (IDS 2018) At: Tehran-Iran"
    },
    {
      "citation_id": "21",
      "title": "A new data augmentation convolutional neural network for human emotion recognition based on ECG signals",
      "authors": [
        "Nita",
        "Sihem"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition using cnn-svm with data augmentation",
      "authors": [
        "Gengyuan Guo"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "23",
      "title": "Effects of data augmentation method borderline-SMOTE on emotion recognition of EEG signals based on convolutional neural network",
      "authors": [
        "Yu Chen",
        "Rui Chang",
        "Jifeng Guo"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Wavelet ELM-AE based data augmentation and deep learning for efficient emotion recognition using EEG recordings",
      "authors": [
        "Berna Ari"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "25",
      "title": "SEMG signal generation for data augmentation using time series transformer based conditional GAN",
      "authors": [
        "Chawki Nasrallah"
      ],
      "year": "2023",
      "venue": "2023 Seventh International Conference on Advances in Biomedical Engineering (ICABME)"
    },
    {
      "citation_id": "26",
      "title": "Data augmentation for EEG-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "Fang Wang"
      ],
      "year": "2018",
      "venue": "MultiMedia Modeling: 24th International Conference, MMM 2018"
    },
    {
      "citation_id": "27",
      "title": "Augmenting ECG data with multiple filters for a better emotion recognition system",
      "authors": [
        "Muhammad Hasnul",
        "Nor Anas",
        "Azlina Ab",
        "Azlan Aziz",
        "Abd",
        "Aziz"
      ],
      "year": "2023",
      "venue": "Arabian Journal for Science and Engineering"
    },
    {
      "citation_id": "28",
      "title": "EEG feature extraction and data augmentation in emotion recognition",
      "authors": [
        "Mahsa Kalashami",
        "Pourhosein",
        "Hossein Mir Mohsen Pedram",
        "Sadr"
      ],
      "year": "2022",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "29",
      "title": "AC-WGAN-GP: Augmenting ECG and GSR signals using conditional generative models for arousal classification",
      "authors": [
        "Andrei Furdui"
      ],
      "year": "2021",
      "venue": "Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "30",
      "title": "On the Exploitation of CEEMDAN for PPG Synthetic Data Generation",
      "authors": [
        "Alessandra Grossi",
        "Francesca Gasparini",
        "Aurora Saibene"
      ],
      "year": "2023",
      "venue": "Italian Forum of Ambient Assisted Living"
    },
    {
      "citation_id": "31",
      "title": "Synthetic ecg signal generation using generative neural networks",
      "authors": [
        "Edmond Adib",
        "Fatemeh Afghah",
        "John Prevost"
      ],
      "year": "2021",
      "venue": "Synthetic ecg signal generation using generative neural networks",
      "arxiv": "arXiv:2112.03268"
    },
    {
      "citation_id": "32",
      "title": "SynSigGAN: Generative adversarial networks for synthetic biomedical signal generation",
      "authors": [
        "Debapriya Hazra",
        "Yung-Cheol Byun"
      ],
      "year": "2020",
      "venue": "Biology"
    },
    {
      "citation_id": "33",
      "title": "Copula-based data augmentation on a deep learning architecture for cardiac sensor fusion",
      "authors": [
        "Diogo Silva",
        "Steffen Leonhardt",
        "Christoph Hoog"
      ],
      "year": "2020",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "34",
      "title": "Synthesis of cardiac signals using a copula-approach",
      "authors": [
        "Diogo Pereira",
        "Filipe"
      ],
      "year": "2019",
      "venue": "AIP Conference Proceedings"
    },
    {
      "citation_id": "35",
      "title": "Data augmentation using Variational Autoencoders for improvement of respiratory disease classification",
      "authors": [
        "Jane Saldanha"
      ],
      "year": "2022",
      "venue": "Plos one"
    },
    {
      "citation_id": "36",
      "title": "Data Augmentation for EEG Motor Imagery Classification Using Diffusion Model",
      "authors": [
        "Nutapol Soingern"
      ],
      "year": "2023",
      "venue": "International Conference on Data Science and Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Enhancing EEG Signal-Based Emotion Recognition with Synthetic Data: Diffusion Modeel Approach",
      "authors": [
        "Gourav Siddhad",
        "Masakazu Iwamura",
        "Partha Roy"
      ],
      "year": "2024",
      "venue": "Enhancing EEG Signal-Based Emotion Recognition with Synthetic Data: Diffusion Modeel Approach",
      "arxiv": "arXiv:2401.16878"
    },
    {
      "citation_id": "38",
      "title": "Data augmentation for Convolutional LSTM based brain computer interface system",
      "authors": [
        "Kahoko Takahashi"
      ],
      "year": "2022",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "39",
      "title": "Tts-cgan: A transformer time-series conditional gan for biosignal data augmentation",
      "authors": [
        "Xiaomin Li",
        "Anne Hee",
        "Hiong Ngu",
        "Vangelis Metsis"
      ],
      "year": "2022",
      "venue": "Tts-cgan: A transformer time-series conditional gan for biosignal data augmentation",
      "arxiv": "arXiv:2206.13676"
    },
    {
      "citation_id": "40",
      "title": "Brainwave EEG Dataset",
      "authors": [
        "Ahmed Alhammadi"
      ],
      "year": "2020",
      "venue": "IEEE Dataport",
      "doi": "10.21227/n2fv-ae84"
    },
    {
      "citation_id": "41",
      "title": "Novel EEG sensor-based risk framework for the detection of insider threats in safety critical industrial infrastructure",
      "authors": [
        "Al Hammadi"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "42",
      "title": "Emotional Status Determination using Physiological Parameters Data Set",
      "authors": [
        "Sadhana Tiwari",
        "Sonali Agarwal"
      ],
      "year": "2020",
      "venue": "IEEE Dataport",
      "doi": "10.21227/e1n2-jx32"
    },
    {
      "citation_id": "43",
      "title": "Understanding machine learning: From theory to algorithms",
      "authors": [
        "Shalev-Shwartz",
        "Shai Shai",
        "Ben-David"
      ],
      "year": "2014",
      "venue": "Understanding machine learning: From theory to algorithms"
    },
    {
      "citation_id": "44",
      "title": "Machine learning",
      "authors": [
        "Ethem Alpaydin"
      ],
      "venue": "Machine learning"
    },
    {
      "citation_id": "45",
      "title": "Learning structured output representation using deep conditional generative models",
      "authors": [
        "Kihyuk Sohn",
        "Honglak Lee",
        "Xinchen Yan"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "46",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "Mehdi Mirza",
        "Simon Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "47",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "48",
      "title": "A theoretically grounded application of dropout in recurrent neural networks",
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "49",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "51",
      "title": "Violin plots: a box plot-density trace synergism",
      "authors": [
        "Jerry Hintze",
        "Ray Nelson"
      ],
      "year": "1998",
      "venue": "The American Statistician"
    },
    {
      "citation_id": "52",
      "title": "Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach",
      "authors": [
        "Seyed Mousavi",
        "Muhammad Hossein"
      ],
      "year": "2023",
      "venue": "Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach",
      "arxiv": "arXiv:2307.06396"
    },
    {
      "citation_id": "53",
      "title": "Bees Local Phase Quantisation Feature Selection for RGB-D Facial Expression Recognition",
      "authors": [
        "Seyed Mousavi",
        "Atiye Muhammad Hossein",
        "Ilanloo"
      ],
      "year": "2024",
      "venue": "Bees Local Phase Quantisation Feature Selection for RGB-D Facial Expression Recognition"
    }
  ]
}