{
  "paper_id": "2310.04456v1",
  "title": "Multimodal Prompt Transformer With Hybrid Contrastive Learning For Emotion Recognition In Conversation",
  "published": "2023-10-04T13:54:46Z",
  "authors": [
    "Shihao Zou",
    "Xianying Huang",
    "Xudong Shen"
  ],
  "keywords": [
    "emotion recognition in conversation",
    "multimodal prompt information",
    "transformer",
    "hybrid contrastive learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems: (1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues extraction was performed on modalities with strong representation ability, and feature filters were designed as multimodal prompt information for modalities with weak representation ability. Then, we designed a Multimodal Prompt Transformer (MPT) to perform cross-modal information fusion. MPT embeds multimodal fusion information into each attention layer of the Transformer, allowing prompt information to participate in encoding textual features and being fused with multi-level textual information to obtain better multimodal fusion features. Finally, we used the Hybrid Contrastive Learning (HCL) strategy to optimize the model's ability to handle labels with few samples. This strategy uses unsupervised contrastive learning to improve the representation ability of multimodal fusion and supervised contrastive learning to mine the information of labels with few samples. Experimental results show that our proposed model outperforms state-of-the-art models in ERC on two benchmark datasets. \n CCS CONCEPTS â€¢ Information systems â†’ Sentiment analysis; â€¢ Computing methodologies â†’ Natural language processing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of social networks, there has been a lot of attention given to building dialogue systems that can understand user emotions and intentions and engage in effective dialogue interaction. Emotion Recognition in Conversation (ERC) is a task that assigns emotional labels with contextual relationships to each utterance made by speakers during a conversation. As a relevant task for dialogue systems, ERC has made important contributions to the development of engaging, interactive, and empathetic dialogue systems by analyzing user emotions in the context of a conversation. It has greatly propelled the advancement of human-machine interaction  [28] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Single Modal",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visual Text",
      "text": "A good friend of mine passed away the other day.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "[Neutral]",
      "text": "[Sad]",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal",
      "text": "A good friend of mine passed away the other day.\n\nA good friend of mine passed away the other day.\n\ninformation fusion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "[Sad]",
      "text": "A good friend of mine passed away the other day.\n\n[Neutral] In the development of ERC, early ERC studies  [8, 17, 29]  focused mainly on research methods that only used text features. However, researchers found that text contains many types of utterances that are difficult for models to fully understand, such as irony, and that emotions in conversation also change with the dynamic interactive process. Since it is difficult to fully understand the conversational context, using only text is insufficient to capture emotional cues in the conversation for accurate emotion prediction. Therefore, people have attempted to expand the model's input from a single modality to multiple modalities (such as visual, audio, videos, etc.). As shown in Figure  1 , using a single modality always leads to prediction errors, but using more types of modality information, the problem of semantic limitations of using only text can be mitigated. However, after introducing multiple modalities, since each modality is in a different semantic space, cross-modal information interactions will introduce a lot of noise if the semantic gaps in them are not considered. For example, Hu et al.  [15]  constructed a modality interaction graph by treating different modality features as nodes. Although this method uses multiple modality information, it does not consider the semantic gap between modalities and directly performs information interaction across modalities, leading to suboptimal fusion effects and affecting the final emotion prediction.\n\nIn addition, because the ERC datasets contained many semantically similar but few sample labels (e.g., fear and disgust, happy and excited), some previous studies  [13]  concluded that the number of these label samples was less and their results were not statistically significant, so they were merged into other similar emotion category samples. Although this approach can improve the accuracy of emotion prediction, from a psychological perspective  [1, 7, 20]  , each emotion category reflects its independent emotional intensity, and simply combining emotion categories leads to oversimplification and may not accurately capture the emotional complexity and diversity of experience. Furthermore, since each emotion label represents the user's emotional state, accurate predictions should be obtained for each category label in real-world scenarios.\n\nTo address the above problems, we propose a new model for ERC, namely Multimodal Prompt Transformer with Hybrid Contrastive Learning (MPT-HCL). Firstly, in order to extract emotional cues from the text, we construct exclusive relationship graphs from both the speaker and context levels to extract emotional cues at different levels. For the audio and visual modalities, we filter the features using a designed modal feature filter, which filters out low-level features with more noise and retains high-level features with valid information. Due to previous research demonstrating the importance of text modality  [39, 42] , we consider text as the main modality feature and refer to the filtered features of audio and visual as textual prompt information. Then, we use MPT for information interaction between modalities. MPT embeds the interaction of multiple modalities into each attention layer of the Transformer, allowing the multimodal prompt information to participate in text feature encoding and fusion with multi-level text information, thus reducing noise generation and using the multimodal fusion result for emotion prediction of each utterance. Finally, to better optimize the model's performance, on one hand, we use unsupervised contrastive learning (UCL) to repeatedly extract mutual information  [2]  between the fusion features and each unimodal modality, in order to mine the relationship between modalities and optimize the fusion feature representation. On the other hand, we use supervised contrastive learning (SCL) to mine the relationship between fused features and labels in the sample. As SCL aggregates less sample labels by treating all samples with the same label in a batch as positive examples, it enhances the presence of less sample labels in the batch. Through this hybrid contrastive learning method, the feature representation of multimodal fusion can be optimized, thus effectively improving the accuracy of prediction for less sample labels. In summary, the main contributions of this paper are summarized as follows:\n\nâ€¢ We propose a novel approach of using filtered modality information as multimodal prompt information, and designing a multimodal prompt transformer for cross-modal information interaction to enhance the fusion effect of multiple modalities. â€¢ For the first time in multimodal ERC, we introduced hybrid contrastive learning to separately explore the information between the fused modal features and each modality, as well as the information in the labels of the samples.\n\nâ€¢ We propose a new ERC model, MPT-HCL, which adopts a multimodal fusion method with hybrid contrastive learning to improve context understanding and the accuracy of multimodal ERC. â€¢ We conducted extensive experiments on two public benchmark multimodal datasets, including IEMOCAP  [3]  and MELD  [32] .\n\nThe results showed that our proposed MPT-HCL model is more effective and superior to all SOTA baseline models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works 2.1 Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversation, as an important research area in natural language processing (NLP), has received extensive attention in recent years. Existing research on ERC mainly has two types of data input, text-based and multimodal-based: (1) Text-based: DialogueGCN  [8]  uses graph networks for modeling dependencies between self-and inter-of speakers, which effectively solves the DialogueRNN  [29]  suffers from the context propagation problem; Ishiwatari et al.  [17]  proposes that R-GAT with relational location encoding not only captures the dependency relations between speakers, but also provides sequential information about the relational graph structure; Shen et al.  [34]  designs a directed acyclic graph (DAG) neural network to encode the utterance to better model the intrinsic structure in the conversation and thus explicitly model the information of each speaker in the conversation; TODKAT  [41]  utilizes the encoder-decoder architecture, which combines the representation of topic information with common-sense information in ERC; (2) Multimodal-based: ICON  [10]  and CMN  [11]  both model information in conversation sequences by GRU; MulT  [35]  uses Transformer's  [37]  fusion approach of the basic module-multihead attention mechanism to achieve cross-modal information fusion by using different modalities as query, key, and value in attention respectively; Li et al.  [23]  proposes a new structure called Emoformer to extract multimodal emotion vectors from different modalities and fuse them with sentence vectors into an emotion capsule; MM-DFN  [13]  uses a new multimodal dynamic fusion network to capture dynamic changes of contextual information in different semantic spaces.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Learning",
      "text": "In the field of computer vision, SimCLR  [4]  optimizes contrast loss by using images obtained from the same image by randomly different data enhancement as positive samples and other images as negative samples. In natural language pre-training, ConSERT  [38]  introduces self-supervised contrast loss in the fine-tuning phase of BERT  [5]  in order to address the poor performance of sentence representation in semantic similarity tasks; Li et al.  [22]  uses supervised contrast learning on top of BART  [21]  as the backbone network to make different emotions to be mutually exclusive to better identify similar emotions. In terms of multimodal learning, TupleInfoNCE  [25]  is a method for learning representations of multimodal data using contrast loss that learns complementary synergies between modalities; MMIM  [9]  maintains task-relevant information by maximizing mutual information in single-peaked input pairs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Definition",
      "text": "In ERC, the data consists of multiple conversations {ğ‘ 1 , ğ‘",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Prompt Transformer With Hybrid Contrastive Learning",
      "text": "In this section, we first introduce the learning methods for different modalities. Then, we present the designed cross-modal fusion method, the Multimodal Prompt Transformer (MPT). Finally, we show our hybrid contrastive learning (HCL) optimization strategy.\n\nThe architecture of our model MPT-HCL is shown in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contextual Capture",
      "text": "For each modality, we use Bi-LSTM for contextual capture, which is calculated as follows:\n\nwhere â„ ğ‘š ğ‘– is the i-th hidden layer state of Bi-LSTM. ğ¿ denotes the conversation sequence length in the batch, and we use\n\nas the initial representation of the node.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modal Feature Filter",
      "text": "To reduce noise in the modal features, we have designed a modal feature filter (MFF) that selects the features with more useful information while filtering out the low-level features with excessive noise. This allows us to obtain the multimodal prompt information. The feature filter is mainly composed of a dynamic gating module. The dynamic gate predicts a normalized vector for each modality, which represents the degree to which information needs to be obtained from that modality. We take the visual modality as an example, in the dynamic gate, ğ‘§ (ğ‘™ )\n\nğ‘£ represents the vector that indicates the degree to which the visual modality provides information to the text modality in the l-th layer of the Transformer. We first calculate the logit of the gating signal ğœƒ ğ‘£ :\n\nwhere ğ‘“ (â€¢) denotes the Leaky_ReLU activation function, ğ‘ƒ (â€¢) represents the average pooling, which is used to generate the average vector by weighting the average of the utterances in the current batch, and ğ‘Š ğ‘™ is the parameter matrix of the linear layer. After that, a probability vector ğ‘§ ğ‘£ is generated for the visual feature representation as follows:\n\nBased on the probability results obtained from the dynamic gating, we obtain the final aggregated visual gating feature ğ‘‰ ğ‘”ğ‘ğ‘¡ğ‘’ , and then feed the aggregation result into the lower and upper projection layers for better information fusion:\n\nwhere ğ‘† ğ‘£ âˆˆ R ğ¿Ã—ğ‘‘ is the visual prompt feature representation obtained by the modal feature filter, the audio prompt information ğ‘† ğ‘ âˆˆ R ğ¿Ã—ğ‘‘ is obtained in the same way as the visual, ğ‘‘ is the dimension of the modal features after alignment. ğœ (â€¢) is the sigmoid activation function, and {ğ‘Š ğ‘‘ ,ğ‘Š ğ‘¢ , ğ‘ ğ‘‘ , ğ‘ ğ‘¢ } are the trainable parameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Speaker & Context-Aware Rgcn",
      "text": "Inspired by  [24] , but we don't use it for the same purpose. We design speaker-aware RGCN (Sa-RGCN) and context-aware RGCN (Ca-RGCN) as the core modules for emotion cue extraction in text modality. In these modules, edges are used to measure the importance of connections between nodes. The type of edge defines the method of propagation of different information between nodes. Sa-RGCN and Ca-RGCN have the same edges, but each edge represents a different dependency, and we will describe the composition of these two modules in detail next.\n\nEdge: For each node, its interaction with the context nodes should be considered. If each node ğ‘£ ğ‘– interacts with all the context nodes ğ‘£ ğ‘— , the constructed graph contains a large number of edges, which usually leads to the problem of computational difficulties due to the huge number of parameters. To solve this problem, we fix the context window size to ğ‘¤ , so that each node ğ‘£ ğ‘– interacts with only {ğ‘£ ğ‘— } ğ‘šğ‘–ğ‘› (ğ‘–+ğ‘¤,ğ¿) ğ‘—=ğ‘šğ‘ğ‘¥ (ğ‘– -ğ‘¤,1) context nodes. In our model implementation, we perform the selection of ğ‘¤ within ğ‘¤ âˆˆ {1, 2, 3, 4} and the edges ğ‘’ ğ‘– ğ‘— denote nodes ğ‘£ ğ‘– to ğ‘£ ğ‘— .\n\nSpeaker-aware RGCN: Sa-RGCN uses different speakers and their spoken utterances to capture the dependencies of the speakers in a conversation. Specifically, we assign a speaker identifier ğ›¼ ğ‘– ğ‘— âˆˆ ğ›¼ to each edge ğ‘’ ğ‘– ğ‘— . Here ğ›¼ denotes the set of speaker types in the conversation, and |ğ›¼ | denotes the number of ğ›¼. For each edge ğ‘’ ğ‘– ğ‘— , ğ›¼ ğ‘– ğ‘— serves as the set of ğ‘ ğ‘  (ğ‘¢ ğ‘– ) â†’ ğ‘ ğ‘  (ğ‘¢ ğ‘— ) , where ğ‘ ğ‘  (ğ‘¢ ğ‘– ) and ğ‘ ğ‘  (ğ‘¢ ğ‘— ) denote the speaker identifiers of ğ‘¢ ğ‘– and ğ‘¢ ğ‘— , respectively.\n\nContext-aware RGCN: Ca-RGCN uses contextual information to capture the contextual dependencies in a conversation. Specifically, we assign a context type identifier ğ›½ ğ‘– ğ‘— âˆˆ |ğ›½ | to each edge ğ‘’ ğ‘– ğ‘— , where ğ›½ denotes the set of context types in the conversation.\n\nBased on the relative positions of ğ‘¢ ğ‘– and ğ‘¢ ğ‘— in the conversation, we will perform ğ›½ ğ‘– ğ‘— value determination from {ğ‘ğ‘ğ‘ ğ‘¡, ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘’ğ‘›ğ‘¡, ğ‘“ ğ‘¢ğ‘¡ğ‘¢ğ‘Ÿğ‘’}, so |ğ›½ |=3.  information through the edges, and the parameters depend on the type of the edges. The calculation is shown as follows:\n\nwhere After obtaining the speaker and context dependencies in the text, we fuse them to obtain the enhanced textual feature representation ğ‘  ğ‘¡ ğ‘– , which is computed as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Interaction",
      "text": "We consider the audio and visual features obtained by the modal feature filters module as textual prompt information, and update all text states with the prompt information when executing crossmodal attention sequentially. In this way, the final fusion feature will have the ability to encode both the context and cross-modal semantic information, which effectively alleviates the problem of noise generated by unrelated elements in the process of multimodal fusion.\n\nPrompt Attention. First, we project the textual feature sequence ğ‘† ğ‘¡ = {ğ‘  ğ‘¡ ğ‘– } ğ¿ ğ‘–=1 âˆˆ R ğ¿Ã—ğ‘‘ enhanced with emotional cues into the query/key/value vector, i.e., ğ‘„ = ğ‘† ğ‘¡ ğ‘Š ğ‘„ , ğ¾ = ğ‘† ğ‘¡ ğ‘Š ğ¾ , ğ‘‰ = ğ‘† ğ‘¡ ğ‘Š ğ‘‰ , where ğ‘Š (â€¢) is the trainable parameter matrix. Then we prepend the visual prompt feature sequences ğ‘† ğ‘£ = {ğ‘  ğ‘£ ğ‘– } ğ¿ ğ‘–=1 âˆˆ R ğ¿Ã—ğ‘‘ and audio prompt feature sequences ğ‘† ğ‘ = {ğ‘  ğ‘ ğ‘– } ğ¿ ğ‘–=1 âˆˆ R ğ¿Ã—ğ‘‘ obtained from the modal feature filters to each attention layer of the Transformer, respectively, and we call this proposed prepending method Prompt Attention (PA), by which the prompt features are used for effective cross-modal interaction, as implemented in the following equation shown:\n\nwhere [; ]denote feature concatenate, ğ‘Œ ğ‘¡ ğ‘£ âˆˆ R ğ¿Ã—ğ‘‘ is the result of the weight of a layer of PA. In this approach, multiple attentions are combined to obtain the output results of the multihead attention layer as follows:\n\nwhere ğ‘Œ 1 ğ‘¡ ğ‘£ , . . . , ğ‘Œ ğ‘› ğ‘¡ ğ‘£ is the output of each attention layer, ğ‘› is the number of attention layers, and ğ‘Š is the trainable parameter matrix. PA is used to perform cross-modal interaction processes between text modalities and multimodal prompt information.\n\nMultimodal Prompt Transformer. Based on the above PA, we design the Multimodal Prompt Transformer (MPT) with the structure shown in Figure  2(c ). Since we embed multimodal interactions into each attention layer of the Transformer, visual and audio features can participate in the encoding of text features and fuse with multi-layered text information. Low-level syntactic features encoded by the shallow Transformer layer and high-level semantic features encoded by the deep Transformer layer  [30, 37]  interact with the visual and audio prompt features, enabling the fusion of information across modalities. In addition depending on the multimodal prompt features used, as more effective feature representation can fully exploit the fusion effect between modalities.\n\nWe use a residual connectivity layer with regularization to normalize the output of the multihead attention layer of Eq.(  8 ) and use a position feedforward sublayer to obtain the output of the attention: ğ‘ = ğ‘ğ‘œğ‘Ÿğ‘š(ğ‘Œ ğ‘¡ ğ‘£ + ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–â„ğ‘’ğ‘ğ‘‘ (ğ‘Œ ğ‘¡ ğ‘£ ))\n\nwhere ğ‘Š 1 ,ğ‘Š 2 is the weight parameter and ğ‘ 1 , ğ‘ 2 is the bias parameter. In this process each modal prompt feature is continuously updated with the feature of the text by the above method to obtain better fusion results, and finally we use self-attention to collect the sequence information of the modal fusion features to obtain the multimodal fusion result ğ‘‹ âˆˆ R ğ¿Ã—ğ‘‘ for the current conversation:\n\nConnecting the above Eq.(  8 ) to  (10) , we can input different modal prompt features and text features to MPT to obtain the modal fusion feature representation:\n\nwhere ğ‘† ğ‘¡ , ğ‘† ğ‘£ , ğ‘† ğ‘ denote the text, visual and audio features obtained by different learning methods as the input to the MPT, and ğ‘‹ ğ‘¡ ğ‘£ , ğ‘‹ ğ‘¡ğ‘ denote the multimodal fusion results obtained after prompting the text with visual prompt features and audio prompt features, respectively. We concatenate the outputs from different MPTs to obtain the fused features ğ‘‹ ğ‘šğ‘ğ‘¡ âˆˆ R ğ¿Ã—2ğ‘‘ :\n\nFinally, in order to fully utilize the text feature representation, we combine ğ‘‹ ğ‘šğ‘ğ‘¡ with the text modality to obtain the final representation of the current utterance used for emotion prediction ğ‘‹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› :\n\nğ‘‹ ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› = ğ‘† ğ‘¡ âŠ• ğ‘‹ ğ‘šğ‘ğ‘¡ (13)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hybrid Contrastive Learning",
      "text": "Unsupervised contrastive learning. Although a better representation of the fusion features is obtained by the MPT, the relationship between each unimodal modal and the fusion features is not entirely explored, so we use unsupervised contrastive learning(UCL) to exploit the connection between them and thus optimize the obtained fusion features. We repeat the mutual information maximization between the fusion results and the input modalities, and the optimization goal is to fuse the network from each unimodal modal to the fusion features. Since we now obtain the multimodal fusion result ğ‘‹ ğ‘šğ‘ğ‘¡ by the constructed MPT network , but the mining of the connection from the fusion feature ğ‘‹ ğ‘šğ‘ğ‘¡ to each unimodal input ğ‘† ğ‘¥ , ğ‘¥ âˆˆ {ğ‘, ğ‘£, ğ‘¡ } is missing. So we follow the operation of  [36]  and use the score function ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ (â€¢) with normalized prediction and true vector to measure the connection between them as follows:\n\nwhere ğº ğœ‘ is a neural network with parameter ğœ‘ that generates a prediction for ğ‘  ğ‘¥ from ğ‘‹ ğ‘šğ‘ğ‘¡ , and || â€¢ || 2 is the Euclidean norm. We treat all other representations of the modality in the same batch as negative samples, and thus calculate the loss between the individual modality and the fused features:\n\nFinally, the loss function of UCL consists of the losses between the fused features ğ‘‹ ğ‘šğ‘ğ‘¡ (noted here as ğ‘¥) and the text, visual and audio, respectively:\n\nSupervised contrastive learning. Supervised contrastive learning(SCL) assumes that attention will be paid to certain key labels, and then treats all samples in the batch with the same label as positive samples and those with different labels as negative samples by making full use of the label information. Specifically for ERC, since the number of samples in each category in the dataset is very imbalanced, such that the information of these samples is masked in the process of calculating the loss. In addition, if only one sample exists in a category batch, it cannot be directly applied to the loss calculation. Therefore, in order to ensure that a sufficient number of feature representations are available for the loss calculation each time, we combine the obtained multimodal fusion features and text features at the position of sequence length. For a batch with ğ¿ training samples, we can obtain 2ğ¿ samples in this way, after which the SCL loss L ğ‘†ğ¶ğ¿ is calculated as follows:\n\nwhere ğ¶ âˆˆ R 2ğ¿Ã—ğ‘‘ , ğ‘– âˆˆ ğ¼ = {1, 2, . . . , 2ğ¿} denotes the index of samples in a batch, ğœ âˆˆ ğ‘… + denotes the temperature coefficient used to control the distance between samples, ğ‘ƒ (ğ‘–) = ğ¼ ğ‘—=ğ‘– -{ğ‘–} denotes the samples with the same emotion category as ğ‘– but excluding ğ‘– itself, |ğ‘ƒ (ğ‘–)| denotes the number of samples, and ğ´(ğ‘–) = ğ¼ -{ğ‘–} denotes the samples in a batch other than itself.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Training",
      "text": "The loss of the model training process consists of three components: the logarithmic loss of standard cross-entropy L ğ¶ğ¸ , the supervised contrastive loss L ğ‘†ğ¶ğ¿ and the unsupervised contrastive loss L ğ‘ˆ ğ¶ğ¿ .\n\nwhere ğ‘ is the number of conversations, ğ½ denotes the number of emotion categories, ğ‘¦ ğ‘–,ğ‘— is the true emotion label of utterance ğ‘–, Å·ğ‘–,ğ‘— denotes the probability distribution that the prediction of utterance ğ‘– is category ğ‘—, and ğœ† 1 , ğœ† 2 denote the weights of supervised and unsupervised contrastive loss, respectively. In the training process we use the Adam  [19]  optimizer with stochastic gradient descent to train our network model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment 5.1 Datasets And Evaluations",
      "text": "We evaluated the effectiveness of our model on two benchmark datasets, IEMOCAP  [3]  and MELD  [32] . Both datasets are multimodal ERC datasets containing text, audio, and visual. We divided the datasets according to  [13] . The data distribution of two datasets are shown in Table  1  and emotion distribution information of two datasets are shown in Table  2 . IEMOCAP: The multimodal ERC dataset. Each conversation in IEMOCAP is from two actors' performances based on the script. There are 7433 utterances and 151 conversations in IEMOCAP. Each utterance in the conversation is labeled with six categories of emotions: happy, sad, neutral, angry, excited, and frustrated.\n\nMELD: The data were obtained from the TV show Friends and included a total of 13708 utterances and 1433 conversations. Unlike the IEMOCAP dyadic dataset, MELD has three or more speakers in a conversation, and each utterance in the conversation is labeled with seven categories of emotions: neutral, surprise, fear, sadness, joy, disgust, and anger.\n\nEvaluation Metrics: We use the F1-score to evaluate the performance for each emotion class and use the weighted average of accuracy and F1-score(W-F1) to evaluate the overall performance on the two datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "â€¢ BC-LSTM  [31]  encodes contextual semantic information through a Bi-LSTM network, thus making emotion predictions.\n\nâ€¢ ICON  [10]  uses two GRUs to model the speaker's information, additional global GRUs are used to track changes in emotional states throughout the conversation, and a multilayer memory network is used to model global emotional states.\n\nâ€¢ DialogueRNN  [29]  models the speaker and sequential information in a conversation through three different GRUs (global GRU, speaker GRU, and emotion GRU). â€¢ DialogueGCN  [8]  applies GCN to ERC, and the generated features can integrate rich information. RGCN and GCN are both nonspectral domain GCN models for encoding graphs. â€¢ DialogueXL  [33]  uses the XLNet model for ERC to obtain global contextual information.\n\nâ€¢ DialogueCRN  [14]  introduces a cognitive phase that extracts and integrates emotional cues from the context retrieved during the perception phase. â€¢ BiDDIN  [40]  specializes inter-modal and intra-modal interactive modules for corresponding modalities, as well as models contextual influence with an extra positional attention. It is set under the multimodal scenario and employs separate modality-shared and modality-specific modules.\n\nâ€¢ MMGCN  [15]  uses GCN networks to obtain contextual information, which can not only make use of multimodal dependencies effectively, but also leverage speaker information.\n\nâ€¢ MVN  [27]  explores the emotion representation of the query utterance from the word-and utterance-level views, which can effectively capture the word-level dependencies among utterances and utterance-level dependencies in the context. â€¢ CoG-BART  [22]  uses supervised contrastive learning to better distinguish between similar emotional labels and augments the model's ability to handle context with an auxiliary generation task. â€¢ MM-DFN  [13]  fuses multimodal contextual information by designing a new graph-based dynamic fusion module to fully understand multimodal conversational contexts to recognize emotions in utterances.\n\nâ€¢ COGMEN  [18]  is a multimodal context-based graph neural network which using local information ( speaker information) and global information (contextual information) in the conversation.\n\nâ€¢ EmoCaps  [23]  uses the new structure Emoformer to extract emotion vectors from different modalities and fuse them with sentence vectors into a emotion capsule for emotion prediction of utterance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "Our proposed model is implemented on the Pytorch framework. The hyperparameters are set as follows: the number of layers of the Transformer in MPT is 5 (ğ‘™ = 5), where the number of prompt attention heads is 5 (ğ‘š = 5). The coefficient ğœ† 1 for supervised contrast loss in hybrid contrastive learning is 0.1, and the coefficient ğœ† 2 for unsupervised contrast loss is 0.05. The dropout in both IEMOCAP and MELD is 0.2. The learning rate in IEMOCAP is 0.0001 and in MELD is 0.0003. Each training and testing procedure was run on a single RTX 3090 GPU, and the reports of our implemented models are based on the average scores of five random runs on the test set.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Feature Extraction",
      "text": "In this paper, we use pre-extracted unimodal features following identical extraction procedures as previous methods  [11, 14, 34] . Text Feature. To extract better utterance representation with strong representational ability, we use the large general pretrained language model RoBERTa-Large  [26]  for text vector encoding extraction. However, unlike other downstream tasks, we use the transformer structure to encode the utterances without classifying or decoding them. More specifically, for each utterance in the text modal, we precede its token with a special token [ğ¶ğ¿ğ‘†] to make it of the form of {[ğ¶ğ¿ğ‘†], ğ‘¤ ğ‘–1 , ğ‘¤ ğ‘–2 , . . . , ğ‘¤ ğ‘–ğ‘› ğ‘– }. Then we use the pooled Audio and Visual Feature. In terms of audio features, OpenSmile  [6]  is used with the IS13 comparison profile, which extracted a total of 6373 features for each utterance video, we reduced the dimensionality to 1582 for the IEMOCAP and 300 for the MELD dataset by using a fully connected layer. The visual facial features were extracted by pretraining on the Facial Expression Recognition Plus (FER+) corpus using DenseNet  [16] . This captures changes in the expression of the speakers, which is very important information for ERC. Finally, a 342-dimensional visual feature representation was obtained.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Analysis 6.1 Main Result",
      "text": "Tables 3 and Tables 4 show the experimental results of our proposed MPT-HCL model and the baselines on the IEMOCAP and MELD datasets. Models marked with \"*\" only use the text modality, and \"-\" indicates that the results were not reported. The best results are highlighted in bold. All other results are reported in their respective papers. On the one hand, compared to existing methods, our model achieved better utterance representations through pretrained language models in sentence encoding. On the other hand, as shown in Tables 3 and Tables 4, we found that: (1) Our proposed MPT-HCL outperforms all baseline models in both evaluation metrics, demonstrating the effectiveness of our model in multimodal ERC. (2) MPT-HCL achieves the best results in almost all emotion categories when compared to EmoCaps. Taking the MELD dataset as an example, our approach outperforms EmoCaps by a large margin in predicting labels with few samples, such as Fear and Disgust. This demonstrates the effectiveness of our multimodal fusion method and the specifically designed hybrid contrastive learning strategy for handling labels with few samples. (3) The overall effect of MPT-HCL is better than MM-DFN, which is a recent model using speaker information.This highlights the effectiveness of utilizing dialogue information in our approach. We consider the reasons for emotional changes in human communication in real life, which are often influenced either by the words spoken by others or by one's own mood. Therefore, we extract contextual cues from both the speaker's own utterances(Speaker-aware) and the interaction between speakers(Context-aware), which leads to superior results compared to MM-DFN.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "6.2.1 Various Modalities. Table  5  shows the performance of our model on the MELD and IEMOCAP datasets under different modal combinations. We can observe that:  (1)  The performance of multimodal inputs is better than that of unimodal inputs. Furthermore, among the three modalities of text, audio, and visual, just as we intended to use text as the main modality and other auxiliary modality features as prompt, the text modality performs better than the other two modalities. (2) After adding the text modality, the combination of visual and audio shows a significant improvement in performance. This suggests that the text modality plays an important role in ERC, while audio and visual serve as auxiliary features to improve the accuracy of the model's recognition. This is consistent with our goal of filtering the audio and visual modalities to extract more effective features. (3) When comparing the normal modal combination (i.e., T+A+V) with our feature-filtered modal combination, we found that the latter is more effective for modality interaction. This is because the visual and audio modals were originally used as auxiliary modalities. By further filtering their features and retaining their useful high-level features as prompt information for the text modality, we obtained a more effective multimodal fusion feature representation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Module Analysis.",
      "text": "To study the contribution of each module in MPT-HCL, we performed an ablation study on both datasets. We consider the following settings:\n\nâ€¢ Full A: We do not use MFF to filter audio feature.\n\nâ€¢ Full V: We do not use MFF to filter visual feature. Table  6  shows the results of our ablation experiments, from which we can conclude that: (1) With Full A/V, we can see that unfiltered modality features introduce a lot of noise when exchanging information across modalities, and the visual modality has more noise than the audio modality due to the complex scenes in the data.  (2)  We can observe that not using MPT yields poor performance. This can be attributed to the lack of information interaction between modalities and the inability of simple concatenation to leverage complementary effects. It introduces significant noise and leads to a confused feature representation, resulting in subpar performance. (3) Our proposed HCL is very effective as removing any contrastive loss leads to a decrease in model performance. This module performs better on the MELD dataset, as this dataset has more minority class labels. (4) After removing the Sa/Ca-RGCN emotion cue extraction module, a significant performance drop was observed on both datasets. This is because both datasets involve conversation between two or more speakers, and capturing emotional cues from speakers and contextual information in conversation is crucial. However, although we constructed a speaker-aware relational graph and utilized the relationships between speakers, we did not fully explore the independent information of each speaker, especially in the case of MELD, which consists entirely of multi-party dialogues. Therefore, the performance of this module on the MELD dataset was not as significant as that on the IEMOCAP dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Potency Of Hybrid Contrastive Learning",
      "text": "In order to conduct a qualitative analysis of the hybrid contrastive learning(HCL), we used t-SNE  [12]  to visualize the initial distribution of some data and the hidden layer status of the model after using HCL. As shown in Figure  3 , when HCL loss is not used, the samples between different labels are randomly scattered, and some samples with similar emotions also overlap, which increases the difficulty of the model learning decision boundaries and leads to large errors in predicting similar labels. With the use of HCL, it can be clearly seen that the coupling degree between different categories gradually increases, resulting in a significant category aggregation effect. This shows that the hybrid contrastive learning strategy we designed plays an important role in sample classification. It is worth noting that although our hybrid contrastive learning has already achieved good category division of samples, we can still see obvious errors in some samples. We analyzed the reason to be that the number of this sample compared to the other labeled samples in the entire batch was the highest, which led to the problem of predicting other labels as this category, and this is a problem that we need to further address.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "We compare the predictions of different methods for the simultaneous presence of emotion transfer as well as category less labels in utterances. Figure  4  provides a representative example of the MELD test set. In this example, ğ‘ƒ ğ´ and ğ‘ƒ ğµ try around whether the chick can swim or not, resulting in a final finding that the chick is about to get into danger resulting in an emotion shift from neutral to fear. We observe that our model is more accurate in making emotion predictions compared to the unimodal DialogueRNN and CoG_BART models for the following reasons: for turn 4, the utterance ends up being incorrectly predicted due to its forward position without much contextual information and without other modal information as an aid. In MM-DFN, for turn 7, the utterance had too little content and the corresponding other modal features could not provide effective help, which led to the model not getting a good fusion representation in the multimodal fusion process of the sentence, resulting in the wrong prediction of the final emotion.\n\nOur model first enhances the extracted conversational context with two levels of emotional cue extraction to avoid the problem of capturing insufficient feature content, and also adds less sample labels to the model when predicting the emotion after obtaining valid features by using a hybrid contrastive learning method. Therefore, our model achieves good results in the whole prediction process in both emotion transfer and few-sample prediction.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "We present a new multimodal prompt transformer with hybrid contrastive learning (MPT-HCL) model for ERC. The RGCN is used to extract emotional clues in the conversation, and different levels of emotional clues are extracted to enhance the text modality. Modal feature filters are designed for the visual and audio modalities to filter features and obtain prompt information for the text modality, through MPT to result in better multimodal fusion feature representation. On this basis, hybrid contrastive learning is used to optimize the fusion feature representation and to explore the information of few labels in the samples, thereby improving the prediction accuracy of few sample labels. In the future, we will explore the joint methods for identifying emotional causes and emotional recognition to alleviate the problem of error propagation in information.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of unimodal vs. multimodal in IEMOCAP.",
      "page": 1
    },
    {
      "caption": "Figure 1: , using a single modality always leads to prediction er-",
      "page": 1
    },
    {
      "caption": "Figure 2: (a) shows the specific structure of MPT-HCL. (b) illustrates the architecture of the Modality Feature Filter (MFF)",
      "page": 4
    },
    {
      "caption": "Figure 2: (c). Since we embed multimodal inter-",
      "page": 4
    },
    {
      "caption": "Figure 3: , when HCL loss is not used, the",
      "page": 9
    },
    {
      "caption": "Figure 3: The t-SNE visualization results of the model output",
      "page": 9
    },
    {
      "caption": "Figure 4: provides a representative example of the",
      "page": 9
    },
    {
      "caption": "Figure 4: Case study in MELD.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Turn": "",
          "Conversation\npA\npB": "",
          "Emotion": "",
          "Prediction": "DialogueRNN"
        },
        {
          "Turn": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
          "Conversation\npA\npB": "What cha doing?\nHaving a swim.\nWhat about the chick?\nChick donâ€™t swim.\nAre you sure?\nI donâ€™t know. Should we try it?\nSure.\nSee, I told you they donâ€™t swim.\nWait. Give him a minute.\nNoo!  Oh, itâ€™s okay, itâ€™s okay, baby.",
          "Emotion": "",
          "Prediction": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A neuropsychological theory of positive affect and its influence on cognition",
      "authors": [
        "Gregory Ashby",
        "Alice Isen"
      ],
      "year": "1999",
      "venue": "Psychological review"
    },
    {
      "citation_id": "2",
      "title": "Mutual Information Neural Estimation",
      "authors": [
        "Mohamed Ishmael Belghazi",
        "Aristide Baratin",
        "Sai Rajeswar",
        "Sherjil Ozair",
        "Yoshua Bengio",
        "R Hjelm",
        "Aaron Courville"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "4",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "5",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
      "doi": "10.18653/v1/n19-1423"
    },
    {
      "citation_id": "6",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th International Conference on Multimedia 2010",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "7",
      "title": "Emotion words shape emotion percepts",
      "authors": [
        "Maria Gendron",
        "Kristen Lindquist",
        "Lawrence Barsalou",
        "Lisa Barrett"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "8",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "9",
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.723"
    },
    {
      "citation_id": "10",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/d18-1280"
    },
    {
      "citation_id": "11",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
      "doi": "10.18653/v1/n18-1193"
    },
    {
      "citation_id": "12",
      "title": "Stochastic neighbor embedding",
      "authors": [
        "Geoffrey Hinton",
        "Sam Roweis"
      ],
      "year": "2002",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9747397"
    },
    {
      "citation_id": "14",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "15",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "16",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Relationaware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "18",
      "title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Vikram Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "19",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "20",
      "title": "Basic emotion questions",
      "authors": [
        "Robert W Levenson"
      ],
      "year": "2011",
      "venue": "Emotion review"
    },
    {
      "citation_id": "21",
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
      "doi": "10.18653/v1/2020.acl-main.703"
    },
    {
      "citation_id": "22",
      "title": "Contrast and Generation Make BART a Good Dialogue Emotion Recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022"
    },
    {
      "citation_id": "23",
      "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL. 1610-1618",
      "doi": "10.18653/v1/2022.findings-acl.126"
    },
    {
      "citation_id": "24",
      "title": "GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2022",
      "venue": "GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation",
      "doi": "10.48550/arXiv.2203.02177"
    },
    {
      "citation_id": "25",
      "title": "Contrastive Multimodal Fusion with TupleInfoNCE",
      "authors": [
        "Yunze Liu",
        "Qingnan Fan",
        "Shanghang Zhang",
        "Hao Dong",
        "Thomas Funkhouser",
        "Li Yi"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV",
      "doi": "10.1109/ICCV48922.2021.00079"
    },
    {
      "citation_id": "26",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "27",
      "title": "A multi-view network for real-time emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Xuejun Pan",
        "Yijia Zhang",
        "Zhihao Yang"
      ],
      "year": "2022",
      "venue": "Knowl. Based Syst",
      "doi": "10.1016/j.knosys.2021.107751"
    },
    {
      "citation_id": "28",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Yukun Ma",
        "Linh Khanh",
        "Frank Nguyen",
        "Erik Xing",
        "Cambria"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.06.011"
    },
    {
      "citation_id": "29",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "30",
      "title": "Deep Contextualized Word Representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
      "doi": "10.18653/v1/n18-1202"
    },
    {
      "citation_id": "31",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "32",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "33",
      "title": "DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "35",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
      "doi": "10.18653/v1/p19-1656"
    },
    {
      "citation_id": "36",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "AÃ¤ron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "37",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems,NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
      "authors": [
        "Yuanmeng Yan",
        "Rumei Li",
        "Sirui Wang",
        "Fuzheng Zhang",
        "Wei Wu",
        "Weiran Xu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.393"
    },
    {
      "citation_id": "39",
      "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
      "doi": "10.18653/v1/2021.naacl-main.79"
    },
    {
      "citation_id": "40",
      "title": "Modeling both Intra-and Inter-modal Influence for Real-Time Emotion Detection in Conversations",
      "authors": [
        "Dong Zhang",
        "Weisheng Zhang",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413949"
    },
    {
      "citation_id": "41",
      "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP",
      "doi": "10.18653/v1/2021.acl-long.125"
    },
    {
      "citation_id": "42",
      "title": "Improving multimodal fusion with Main Modal Transformer for emotion recognition in conversation",
      "authors": [
        "Shihao Zou",
        "Xianying Huang",
        "Xudong Shen",
        "Hankai Liu"
      ],
      "year": "2022",
      "venue": "Knowl. Based Syst",
      "doi": "10.1016/j.knosys.2022.109978"
    }
  ]
}