{
  "paper_id": "2508.04481v1",
  "title": "Emotion Detection Using Conditional Generative Adversarial Networks (Cgan): A Deep Learning Approach",
  "published": "2025-08-06T14:32:22Z",
  "authors": [
    "Anushka Srivastava"
  ],
  "keywords": [
    "Emotion Recognition",
    "Conditional GAN",
    "Facial Expression Synthesis",
    "Data Augmentation",
    "FER-2013",
    "Deep Learning",
    "Adversarial Training",
    "Class Imbalance",
    "Generative Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a key task in affective computing with applications in healthcare, human-computer interaction, and surveillance systems. Traditional supervised learning models often struggle with limited and imbalanced datasets, particularly in facial emotion recognition. This study proposes a Conditional Generative Adversarial Network (cGAN)-based approach to generate synthetic emotion-specific facial images to augment training data and mitigate class imbalance. The generator learns to synthesize grayscale 64×64 facial images conditioned on emotion labels, while the discriminator distinguishes between real and generated images using label conditioning. The model was trained on the FER-2013 dataset and evaluated over 300 epochs. Training results demonstrate stable adversarial loss convergence, indicating effective learning and generation capability. Visual inspection confirms that the generated samples exhibit distinct features corresponding to various emotions. This work highlights the potential of cGANs in improving emotion recognition pipelines by enhancing data diversity and representation for underrepresented emotion classes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION detection is a fundamental component of af- fective computing, with wide-ranging applications in human-computer interaction, mental health monitoring, and intelligent virtual assistants.  [1]  Emotion detection plays a critical role in enabling machines to understand and respond to human emotions, which is essential for applications in healthcare, human-computer interaction, customer service, and surveillance systems. Traditional approaches to emotion recognition rely on rule-based systems or supervised learning using handcrafted features.  [2]  However, these methods often struggle with generalization due to limited training data and the complex, subjective nature of emotions. In the last few years, there have been huge advances in the field of Artificial Intelligence and Deep Learning. These advancements improved the performance of Emotion Detection. Among these, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have outperformed the most. However, these models remain data hungry and sensitive to imbalanced datasets, often failing to generate robust performance across diverse demographics and real-world conditions. GANs, introduced by Ian Goodfellow in his 2014 paper Generative Adversarial Nets, offer a groundbreaking solution to this challenge. This innovative framework has transformed generative modeling, making it easier to develop models and algorithms capable of creating high-quality, realistic data.  [3] A Conditional GAN is a type of generative adversarial network that includes additional information, called \"labels\" or \"conditions\". This research explores the use of Conditional GANs for augmenting emotion datasets and improving emotion classification performance. We hypothesize that the integration of cGANgenerated synthetic samples can enhance emotion recognition accuracy, particularly in imbalanced or small datasets. The study focuses on evaluating cGAN effectiveness in both data generation quality and downstream classification improvements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Proposed Methodology",
      "text": "This section explains the complete workflow for Emotion Detection using Conditional Generative Adversarial Network(cGANs).\n\n1) Dataset Collection and Prepossessing: The dataset used for the proposed system is cleaned and preprocessed version of the FER-2013 (Facial Expression Recognition 2013). The data set contains grayscaled images of 64 × 64 pixels resolution.The total data set contains 28,709 images belonging to seven different classes.The corresponding label array shaped as (28709, ) and image array shaped as (28709, 64, 64, 1), indicating single-channel (grayscale) images. of synthetic emotion-specific images to balance the class distribution and improve generalization performance.\n\n2) Conditional GAN Architecture: To handle the class imbalanced data and enrich with diverse samples, Conditional GAN was implemented(cGAN).Unlike the original GAN framework proposed by Goodfellow et al.  [4] , where data generation is uncontrolled, cGANs introduce supervision in the form of class labels, enabling targeted generation of samples for specific emotion categories  [5] .\n\nThe cGAN comprises two adversarial networks: a generator G and a discriminator D. The generator receives a random noise vector z concatenated with a one-hot encoded emotion label y, producing a synthetic image G(z|y). The discriminator takes an image-label pair as input and learns to distinguish between real and generated (fake) images conditioned on the given label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Training Procedure",
      "text": "The cGAN model is trained using the Binary Cross-Entropy (BCE) loss function:\n\nThe networks are optimized using the Adam optimizer with a learning rate of 0.0002 and β 1 = 0.5  [7] . Training is conducted for 200 epochs with a batch size of 128. Generator and discriminator are updated alternately during each batch iteration.\n\nConditional Label Encoding Emotion labels are one-hot encoded. For the generator, these are concatenated with the noise vector. In the discriminator, labels are reshaped to match the spatial dimensions of the input image and concatenated along the channel dimension. This conditioning approach enforces class control over both image generation and discrimination, improving the relevance and diversity of generated outputs  [5] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation For Minority Classes",
      "text": "After training convergence, the generator is used to create new synthetic samples for minority emotion classes like Disgust, Fear, and Surprise. Generated images are filtered based on discriminator confidence or visual quality. These synthetic samples are combined with real data to balance the dataset and improve classification model performance, a strategy supported by prior research  [8] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "This section outlines the technical environment, data partitioning strategy, and evaluation metrics used to assess the performance of the proposed emotion detection framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "1) System Configuration:",
      "text": "• All experiments were conducted on a system with an Intel(R)Iris(R) Xe Graphics, Intel Core i5 processor, and 16GB RAM and on Kaggle's Cloud Based GPU platform for training and evaluation process. • The model was implemented using Python 3.9 with the PyTorch framework. • Supporting libraries include NumPy, Pandas, Matplotlib, Seaborn,Tensorflow, Keras and OpenCV. 2) Training Parameters: The Conditional GAN model was trained for 300 epochs with a batch size of 64. The Adam optimizer was used with a learning rate of 2e-4 and β 1 = 0.5 for both discriminator and generator.\n\n3) Evaluation Metrics: Model performance was evaluated using discriminator loss and generator loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Results And Analysis",
      "text": "This section presents the training behavior and qualitative evaluation of the Conditional Generative Adversarial Network (cGAN) used to synthesize emotion-specific facial images. Since the focus of this experiment was generative modeling, the evaluation is based on generator/discriminator loss trends and visual quality of the generated outputs.\n\nDuring training, generator loss and discriminator loss were recorded over 300 epochs. The generator initially exhibits high loss as it learns to produce images that can deceive the discriminator. Over epochs, the loss stabilizes, indicating that the generator is learning useful representations of facial emotions. The discriminator's loss also fluctuates early in training but gradually converges, suggesting a healthy adversarial dynamic where both networks improve iteratively.\n\nA relatively balanced adversarial loss dynamic, without either network overpowering the other, indicates that the cGAN training was stable and did not suffer from issues like mode collapse or vanishing gradients.\n\nTo evaluate the visual quality of generated samples, images were saved at the end of each epoch using a callback function.\n\nThe visual inspection reveals that the generator successfully captures distinct features associated with different emotions. For instance, 'happy' class samples typically display curved mouths, while 'angry' samples exhibit furrowed brows. The grayscale outputs are consistent with the FER dataset and show progressive improvement over epochs.\n\nEven without a classifier, the loss convergence and visual fidelity of the generated images indicate that the cGAN learned a meaningful mapping between latent vectors and labeled emotional expressions. This confirms the model's potential as a data augmentation technique to address class imbalance in emotion recognition datasets.\n\nFuture work will involve evaluating these generated images quantitatively by:\n\n• Training a classifier with and without cGAN-generated data to compare performance. • Applying metrics like Inception Score (IS) or Fréchet Inception Distance (FID) to measure image diversity and quality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Conclusion",
      "text": "This research demonstrates the effectiveness of Conditional Generative Adversarial Networks (cGANs) in generating emotion-specific facial images to address dataset imbalance. By incorporating class labels into the GAN framework, we successfully synthesized high-quality images corresponding to various emotional categories.\n\nThe training loss dynamics and visual evaluation of outputs confirm the model's ability to learn meaningful representations of emotions. While no quantitative classification evaluation was performed in this study, the generated outputs qualitatively align with expected facial expressions for different emotions.\n\nIn future work, we aim to:\n\n• Integrate cGAN-augmented data into emotion classifiers to evaluate performance gains. • Perform quantitative evaluations using FID and IS scores.\n\n• Extend this approach to multi-modal datasets combining text, audio, and images. This work highlights the potential of generative models in boosting the performance of affective computing applications, especially when data scarcity or class imbalance hinders traditional supervised learning approaches.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "0\n1\n2\n3\n4\n5\n6",
          "Emotion": "Angry\nDisgust\nFear\nHappy\nNeutral\nSad\nSurprise",
          "Number of Images": "2,983\n436\n2,945\n6,411\n3,657\n4,649\n3,171"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Epoch": "50\n100\n150\n200\n250\n300",
          "Generator Loss (G)": "1.61\n2.71\n3.70\n4.54\n4.79\n5.07",
          "Discriminator Loss (D)": "0.47\n0.29\n0.19\n0.30\n0.29\n0.18"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal deep learning for emotion detection from text, audio, and facial expressions",
      "authors": [
        "A Madaan",
        "P Verma",
        "M Sharma",
        "A Kumar"
      ],
      "venue": "Multimodal deep learning for emotion detection from text, audio, and facial expressions"
    },
    {
      "citation_id": "2",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Generative adversarial networks (gans)",
      "authors": [
        "Ibm"
      ],
      "venue": "iBM Think Blog"
    },
    {
      "citation_id": "4",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "5",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "arxiv": "arXiv:1511.06434"
    },
    {
      "citation_id": "7",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "8",
      "title": "Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification",
      "authors": [
        "A Frid-Adar",
        "E Klang",
        "M Amitai"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    }
  ]
}