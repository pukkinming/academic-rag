{
  "paper_id": "2107.14529v1",
  "title": "Recognizing Emotions Evoked By Movies Using Multitask Learning",
  "published": "2021-07-30T10:21:40Z",
  "authors": [
    "Hassan Hayat",
    "Carles Ventura",
    "Agata Lapedriza"
  ],
  "keywords": [
    "Affective computing",
    "Evoked emotion recognition",
    "Multi-task Learning",
    "Multi-modality"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding the emotional impact of movies has become important for affective movie analysis, ranking, and indexing. Methods for recognizing evoked emotions are usually trained on human annotated data. Concretely, viewers watch video clips and have to manually annotate the emotions they experienced while watching the videos. Then, the common practice is to aggregate the different annotations, by computing average scores or majority voting, and train and test models on these aggregated annotations. With this procedure a single aggregated evoked emotion annotation is obtained per each video. However, emotions experienced while watching a video are subjective: different individuals might experience different emotions. In this paper, we model the emotions evoked by videos in a different manner: instead of modeling the aggregated value we jointly model the emotions experienced by each viewer and the aggregated value using a multi-task learning approach. Concretely, we propose two deep learning architectures: a Single-Task (ST) architecture and a Multi-Task (MT) architecture. Our results show that the MT approach can more accurately model each viewer and the aggregated annotation when compared to methods that are directly trained on the aggregated annotations. Furthermore, our approach outperforms the current state-of-theart results on the COGNIMUSE benchmark.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "U NDERSTANDING the emotional experience of users when watching a movie has been broadly studied in psychology  [1] -  [3]  and has recently attracted the attention among computer scientists  [4] -  [8] . Models that can automatically infer the emotions evoked by movies are useful in multiple applications and scenarios, such as recommendation systems, affective multimedia retrieval, or for producers to understand the impact of the content they create.\n\nOne of the challenges to create automatic systems to infer emotions evoked by movies is the subjective nature of the task: the same content can evoke different emotions to different people  [9] . This challenge has not been explicitly studied before in the context of emotions evoked by movies, and it is actually the focus of our work. In general, works on evoked emotion recognition focus on modelling aggregated annotations of multiple viewers, such as the average  [10] -  [13] . However, our results show that, given a crowd of viewers, jointly modelling the perception of each viewer and the average across viewers in a multi-task manner can actually 978-1-6654-0019-0/21/$31.00 ©2021 European Union produce more accurate results than just modelling the average viewer in a single-task manner. The intuition behind this idea is the following. Each viewer has different sensitivities and preferences, which affect the emotion experienced while watching a movie. Thus, to model the evoked emotion for a specific viewer, a machine learning system needs to find the patterns in the input data (the movie) that are informative for inferring the evoked emotion for that viewer. However, when we aggregate the experienced emotion of different viewers, the viewer specific patterns get merged, making the patterns of the input data to infer the aggregated annotation harder to find. On the contrary, with the multi-task approach that we suggest, the model needs to find the patterns corresponding to each viewer to model each viewer. Thus, the representation of these viewer specific patterns will be encoded in the latent representation of the CNN, and will be available for the classification branch of the aggregated annotations, making the modelling of the aggregated annotations easier.\n\nIn this paper we propose a Multi-Task Deep Learning architecture (MT) to jointly model the perception of each viewer and the aggregated annotation of the average viewer, as illustrated in Fig.  1 . Concretely, our MT architecture is an extension of our Single-Task formulation (ST), since it uses the same backbone modules for the feature extraction of the text and visual modalities as in the ST architecture. However, after the fusion layer we add two fully connected layers, and then we create separate classification branches, one per each of the viewers and one more for the aggregated annotation. The details of our architectures are described in Sect.III.\n\nTo validate our approach we conduct experiments on the COGNIMUSE dataset  [14] . In particular we use the standard benchmark for evoked emotion understanding, which includes clips of 7 Hollywood movies. The COGNIMUSE benchmark is annotated on evoked valence, arousal and dominance by 7 different viewers. In this work we focus on the valence dimension, which is the most popular dimension for emotion recognition related tasks  [15] -  [19] . For our study we follow the common practice of using two discrete values  [20] -  [22] , i.e. positive vs. negative. As discussed in Sect.II, COGNIMUSE is the only publicly available dataset for evoked emotion recognition in movies that provides the annotations of separate viewers, instead of just an aggregated value, which is the key requirement for our study. More details about the dataset and the data partitions used in our experiments are  provided in Sect.V.\n\nOur experiments are presented in Sect.V-A. First, we compare our proposed ST and MT models. For that we create a collection of data splits, as described in Sect. IV-C, and we report the average accuracy obtained by ST and MT models on each viewer as well as on the average viewer. In this case we observe that the MT obtains higher accuracies than the ST. Then, we perform ablation experiments on the separate modalities, and observe again that the MT outperforms the ST for both the text and the visual modalities. Furthermore, our results of the ablation study are consistent with previous findings showing higher accuracies for the visual modality when compared to the text modality. Finally, we compare our approach with the results reported in  [20] , which are the current state-of-the-art results for valence classification in the COGNIMUSE benchmark. The modelling approach proposed in  [20]  is referred as Baseline in our paper and it is briefly described in Sect.II-A. As shown in Table  V , we observe that our proposed MT approach outperforms the Baseline model.\n\nIn summary, the results of our work suggest that, for the problem of evoked emotion recognition, jointly modelling each viewer and the average viewer can be a better solution than just modelling the average viewer in a single task manner.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Recognizing emotions evoked by movies has been getting more attention due to its potential applications in affective based recommender or affective multimedia retrieval systems  [8] ,  [23] ,  [24] , among others. For example, Bao et al  [25]  used smartphone sensors to detect the viewer's emotional cues, such as laughter, during the movie, while Lee et al.  [26]  used 3d fuzzy visual and EEG features to understand the emotional state of viewers while watching a movie. More recently, Thao et al.  [7]  used audiovisual data for predicting affective responses of viewers watching a movie, while Nguyen et al.  [20]  proposed a multimodal approach that uses audiovisual and text data to understand the emotions evoked by movies. The authors in  [20]  proposed different Convolutional Neural Netwokrs (CNNs) per each of the modalities considered in their work (visual, text, and audio). In this work the architectures of  [20]  are used as a baseline to compare our results. Sect.II-A gives a deeper explanation on this baseline approach.\n\nOne of the challenges of evoked emotion recognition is that evoked emotion is subjective, meaning that the same content can evoke different emotions to different viewers. Our approach in this paper is to jointly learn the individual experienced emotion and the aggregated experienced emotion in a Multi-task manner to better approximate both the individual and the aggregated experiences. However, the problem of dealing with subjective annotations has been recently addressed by  [34]  in a different way. The authors proposed a probabilistic approach to estimate reliability and regularity of viewers in order to aggregate annotations. Reliability computes how likely the viewer's response is serious (instead of random), while regularity measures how the viewer agrees with the other viewers. Then, these reliability and regularity scores are used to compute a final aggregate value per each instance, which is considered the ground truth. More generally, subjectivenessaware methods are getting more and more attention due to the fact that the established model outperforms human raters on average in terms of the perception uncertainty  [12] ,  [13] . On the other hand,  [27]  proposed user-level emotion modeling of different viewers and then combined individual annotations using geometric mean and unweighted majority voting for final predictions. The performance gain using user-level modeling is better than using the ground truth labels obtained by averaging or majority voting directly. Similarly,  [28]  used the majority of annotations (hard label) and the distribution of annotations (soft label) simultaneously for joint emotion modeling. Later they concatenated all the outputs and used Softmax for the final predictions. Also,  [29]  proposed a personalized emotion recognition model that integrated different user's attributes for emotion classification. To address subjectivity  [30]  introduced user-dependent weight to predict the final emotional state.\n\nA key component to train automatic systems for evoked emotion recognition is data. In this work we use the COG-NIMUSE dataset  [14] . COGNIMUSE is the only dataset for evoked emotion recognition in movies that provides the annotations of each individual viewer (the data used in our study is explained in more depth in Sect. IV). However, there are other labeled datasets for evoked emotion in videos. For example, the HUMAINE dataset  [31]  consists of 50 video clips, having 0.5 -3 minutes in length. Each clip was annotated at two levels: Global level (emotions related state, context labels, key events, etc) and frame-by-frame (intensity, arousal, valence, dominance). FilmStim  [32]  has 70 movies with the length ranging from 1 to 7 minutes. It uses unique global labels for emotion ranking (subjective arousal, positive and negative affect, six emotions discreteness score, etc). LIRIS-ACCEDE database  [33]  contains 9, 800 videos excerpts, having a range from 8 to 12 seconds, and annotated in 2D (arousal, valence) domain. Unfortunately these datasets just provide an aggregate annotation per instance, such as the average or the majority voting, which makes them not suitable for our study.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Baseline Model",
      "text": "The CNN-based architectures proposed in  [20]  are considered as the Baseline models in our experiments. In particular, we compare our results with the multi-modal approach from  [20] , as well as with the separate text and visual modalities, referred as Baseline-Text and Baseline-Visual, respectively.\n\nThe Baseline-Text consists of three convolution layers and each layer has 100 kernels, which extract the high-level text features for classification. Each convolution layer is followed by a ReLu activation function, a max-pooling layer, and a Fuzzified kernel. The words from the sentence were preprocessed before passing through the layers of the convolutional neural network. A maximum length of 18 words was considered and a particular word was padded at the end of shorter sentences.\n\nThe Baseline-Visual has three convolutional layers having 64, 96, and 128 kernels, respectively. The input of the CNN is the fusion of two different features: the orientation features, which were calculated based on 0°, 45°, 90°, 135°; and the color features, which were obtained by converting RGB to HSI color space. The final features are constructed by computing a 32-bin histogram of each 7 channels (4 from orientation and 3 from color).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Emotion Modeling",
      "text": "In our experiments we use two types of CNN architectures: a Single-Task (ST) architecture and a Multi-Task (MT) architecture. Both architectures use the same backbone modules for the text and the visual modalities, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Single-Task Architecture (St)",
      "text": "Our ST architecture is illustrated in Fig.  2 . The architecture has two input branches, one per each modality (visual -top branch-and text -bottom branch-). Per each branch we use the corresponding backbone to extract the features (the backbone modules are described below in this section). Then, the features are fused by concatenation. After that, our architecture has three fully connected layers with 1024, 512, and 256 units, respectively. The sigmoid function is applied to the output layer to get the final prediction. Due to the high difference between the numbers of positive and negative labels in each viewer's dataset (see Fig.  4 ), we used the weighted log-loss to compensate for the imbalanced data. This way, the contribution to the loss of both positive and negative examples is the same regardless of the distribution of the labels in the training dataset. Adam optimizer  [35]  with learning rate 10 -3 is used for training. Finally, to overcome overfitting we use a Lasso regularization term  [36] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Multi-Task Architecture (Mt)",
      "text": "Our MT architecture is illustrated in Fig.  1 . In this case we use the same backbone feature extraction branches for the visual and the text modality as in the ST architecture. Then we concatenate the features and add two fully connected layers of 2048 and 1024 units, respectively. After that we have separate branches per each viewer, including all 7 viewers",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Clip(I)",
      "text": "Zeta function correspond to singularities in space-time.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Subtitle(I)",
      "text": "RGB-I3D (Pre-Trained ) 3x3 conv, 300\n\nMax-pooling 3x3 conv, 300 Max-pooling    and the average viewer. Each of the separate branches have 3 fully connected layers of 512, 256, and 128 units, respectively. Finally we use a sigmoid and log loss, again with a Lasso regularizer. Notice that, as illustrated in Fig.  1 , we have one separate output branch per each one of the 7 viewers (V 1 -V 7 ), as well as a last branch for the average viewer (V avg ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Feature Extractor",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Text Backbone Branch",
      "text": "Our backbones for the text and visual modalities are illustrated in both Fig.  2  and Fig.  1 . For the text modality we learn a word-embedding matrix to map every word in a sentence to a d-dimensional vector. This way, sentences can be represented as vectors of numerical values. Since the length of the sentences is variable, a maximum length of 18 words is considered and a particular word is padded at the end of shorter sentences. As a result, all the sentences have the same length. After that we use a sequence of two pairs of convolutional layer plus max pooling. We initialize this text feature extraction branch randomly and we train it with the labeled data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Visual Backbone Branch",
      "text": "For the visual modality, we used a fixed pre-trained RGB-I3D model  [37]  on the Kinectic-400 Dataset. The architecture uses 3D convolutions and max-pooling operations to learn seamless spatio-temporal features from video. The I3D model is known as the Inflated 3D that is based on the state-of-theart Inception-V1  [38]  model. All the convolutional and pooling filters of Inception-V1 were converted from 2D into 3D. This additional dimension is known as the temporal dimension and helps the model in learning temporal patterns of the video. Each convolutional layer is followed by batch normalization  [38]  and a ReLU activation function. The I3D model has multiple end-points to collect the features of the given input video. In our experiments, we used the features of the last endpoint of the model that is \"Mixed-5c\". We processed a batch of 16 consecutive frames with a stride of 8 frames of a single clip and the features are then global average-pooled. To get the most important segments of the clip we then maxpooled across the temporal domain.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Data",
      "text": "In our experiments we use the COGNIMUSE dataset  [14] . COGNIMUSE is a collection of movie clips and travel documentary clips with human annotations on different tasks: audio-visual and semantic saliency, audio-visual events and action detection, cross-media relations, and emotion recognition. In our study we use data from the emotion recognition benchmark. This section describes the data used in our study and the data split generated for our experimental protocol. We also provide at the end of this section a brief statistical analysis of the data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Cognimuse: Evoked Emotions",
      "text": "The standard benchmark for emotional understanding  [14]  includes 7 Hollywood movies: \"A beautiful Mind\" (BMI), \"Chicago\" (CHI), \"Crash\" (CRA), \"Finding Nemo\" (FNE), \"Gladiator\" (GLA), \"The Departed\" (DEP), and \"Lord of the Rings -the Return of the King\" (LOR) and each movie has 30 minutes in length. The emotions evoked by the movies are represented in the valence and arousal space. Valence represents how positive or negative the emotion evoked by the clip is, while Arousal encodes viewer's excitement, agitation, or readiness to act. The frame rate of each movie is 25 fps and 7 different viewers provided annotations for each frame in continuous values from -1 to +1 of arousal and valence domains. In this work we focus on the Valence dimension.\n\nThe dataset also includes the movie subtitles of each video segment (notice that the total number of video frames is different for each video segment). We consider each video segment as a sample for our model, where the input is the text associated to that video segment and the output is the averaged valence value along the time dimension. This output value results from averaging all valence values annotated (we have a value every 40 ms). Once the averaged valence value is obtained, it is binarized using the value 0 as a threshold. The values greater than the threshold are considered as positive, whereas the values below the threshold are considered as negative. Using this discretization for valence recognition is a common practice, as we can also see in  [20] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Data Distribution Analysis",
      "text": "In this section we analyze the distribution of the data and the agreement among the different viewers. First, Fig.  3  shows the correlations between the different viewer pairs. We can observe that viewer 1 has very low correlations with the other viewers. Furthermore, we notice that several correlation values below 0.4. We also observe that some viewers are more correlated to the average viewer than others. For example, viewer 5 is highly correlated with the average viewer, viewer 7 is poorly correlated with the average viewer, and viewer 1 has almost 0 correlation with the average viewer.\n\nFig.  4  shows the histogram (total counts) of positive and negative labels per viewer and per movie. Again, we observe notable differences among the viewers. In particular, we can observe how the histogram of viewer 1 is very different from the other histograms, and how the histogram of viewer 5 and average viewer are the most similar ones.\n\nOverall, these statistics illustrate the subjectiveness of the viewers: they often experience different emotions when viewing the same movie segment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Data Folds",
      "text": "We define seven different folds for cross-validation to perform an accurate evaluation of the proposed models. For each fold, one movie is left for test and the remaining six movies are randomly split into training (5 movies) and validation (1 movie). For reproducibility purposes, Table  I  summarizes the 7 data folds used in our study, which are denoted by F 1 , ..., F 7 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "We perform extended experiments to compare the different approaches for modelling the emotion evoked by movies. First, Sect.V-A shows the results obtained when using the ST and MT multi-modal approaches to model each viewer, including the average viewer. Second, Sect.V-B presents our ablation study, where we show the results obtained by each separate modality (i.e. text and visual). Then, we compare our models with the Baseline model  [20]  and, finally we discuss the obtained accuracies and show qualitative results.      [20]  45    [20]  53",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. St And Mt Multi-Modal Architectures",
      "text": "Table  II  shows the results obtained when modelling each viewer with the ST model vs. the MT model. Concretely, each column corresponds to one viewer, V i , and shows the average accuracies obtained when approximating V i for the different data folds, F j . We denote by V avg the average viewer, computed by averaging the annotations of all the 7 individual viewers. As a reference, the first three rows of the table show the results obtained by a random classifier, a Positive classifier (i.e. assigning a positive output to any input instance), and a Negative classifier (i.e. assigning a negative output to any input instance). To facilitate the comparisons we add in the last column (denoted by Mean) the average result per row.\n\nAs expected, we observe that the MT models obtain higher accuracies in average, both in the case of the individual viewers (65.64 vs. 73.61) as well as for the average viewer (65.72 vs. 77.80). These results are coherent with other studies in the context of Affective Computing, that also demonstrated how a personalized multi-task approach to model mood, stress, and health of different participants is better than a single-task approach to model each participant separately  [39] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Ablation Study",
      "text": "In this section we present our ablations studies. The goal of these experiments is to compare the performance of the visual and the text modalities, separately. For each modality we use the corresponding ST and MT architectures described in Sect.III, but we just use one input branch (text or visual).\n\nTables  III  and IV  show the results obtained for the separate text and visual modalities, respectively. For each of the two modalities we also provide, as a reference, the results obtained by the Baseline model for the corresponding modality.\n\nFor the text modality we observe that the Baseline approach is highly dependent on the data distribution. Concretely, we observed that most of the samples are classified as positive, which is the most represented class. Thus, when we experimented the Baseline-Text approach for every single viewer using cross-validation folds we got a significant drop in performance. For most of the viewers (V 1 , V 4 , V 5 , V 7 and V avg ), the accuracy is less than the accuracy obtained by a random classifier (see Table  III ). We observe that our ST-Text and MT-Text architectures obtain better accuracies for the average viewer and for all the individual viewers except one (V 3 ). Notice that only the ST-Text model for V 7 gives an accuracy worse than the random. Furthermore, Table III also shows that MT-Text architecture outperformed ST-Text in modeling all the viewers (52.96 vs. 65.04 mean accuracy) and also the average viewer (51.70 vs. 70.  16 ). This supports our hypothesis that the MT approach suits better in modeling evoked emotions for multiple viewers rather than taking an average from multiple viewers first and then modeling the average evoked emotion.\n\nIn contrast with the text modality, the visual modality is richer and thus provides better results in terms of accuracy (see Table  IV ). We again tested Baseline-Visual for every single viewer and the viewer using cross-validation folds and found that our ST-Visual and MT-Visual approaches obtain better results than the Baseline-Visual. The results are shown in Table  IV . We observe that the MT-Visual approach, when compared to the Baseline-Visual approach, is able to generalize better for each individual viewer plus the average viewer (66.60 vs. 70.10 mean accuracy) and average of all viewers (64.70 vs. 74.80).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Comparing Mt With The Baseline Model",
      "text": "In this section we compare the performance of our best model (MT) with the performance of the Baseline multi-modal model  [20] . For this purpose we report accuracies on the data split used in  [20] , since the code for the Baseline multi-modal approach is not available and we could not train it in our fold of data partitions. Concretely, the data split of the Baseline model is the following: 5 movies for training (BMI, CHI, FNE, GLA, LOR), and 2 movies for testing (CRA, DEP). We compare Baseline accuracies with the accuracies we got from V avg branch of our MT-text, MT-visual, and MT-multimodal models. The results are shown in Tab.V. We observe that our MT approach outperforms the Baseline in the text (65.1 vs 75.48) and the visual (82.8 vs 84.88) modalities. Our multimodal approach, which uses just text and visual modalities, also outperforms the Baseline multimodal approach, which also uses the audio modality (83.2 vs 89.50).\n\nFinally, Tab.VI shows for our MT models the accuracies obtained per viewer on the Baseline movie split. As before, we observe that the MT multi-modal model is the one obtaining the highest accuracies for all the viewers (75.69), when compared to MT models that use just the text modality (68.01) or just the visual modality (71.36).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Discussion",
      "text": "Our results show that the proposed MT approach to model the aggregated (average) viewer (V avg ) obtains the highest accuracies in the valence classification. This is observed in our cross-fold validation as well as in the data split reported in the Baseline model paper  [20] . Another important observation is that the MT approach obtains better accuracies in approximating each viewer with respect to the ST approach. Again, this is observed both in our cross-fold validation as well as in the data split of the Baseline model. In our ablation studies, we also observe the MT approach showing the highest accuracies for both the text and the visual modalities. Furthermore, we observe the visual modality achieves better results than the text modality, when the modalities are tested separately. A higher performance on the visual modality, when compared to the text modality, was also observed in  [20] . Interestingly, we also observe that in our results the worst performance,",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Movie Lor",
      "text": "It's the tree Gandalf. Gandalf. Yes, the White Tree of Gondor.\n\nThe tree of the king.\n\nLord Denethor, however, is not king.\n\nHe is a steward only, a caretaker of the throne.  across all the viewers, is usually obtained by V 1 . Notice that, as shown in Sect.4, V 1 was actually the viewer that was less correlated with the others, as shown in Fig.  3 . Finally, Fig.  5  provides some qualitative results. Concretely, for four of the movies (LOR, CRA, FNE, CHI), we show the ground truth annotation per viewer (GT column) and the predictions made by our MT-multimodal model (Pred. column) for some segments. First, we observe how, for the same segment, different viewers provide different ground truth annoations. This qualitatively illustrates the observations we made in Sect.IV-B, where we showed the histograms of the annotation distribution per viewer (Fig.  4 ). In these examples we also observe how V 1 often disagrees with the rest of the viewers, as shown also before in Sect.IV-B, when we analyzed the viewers correlations (Fig.  3 ). Finally, we observe how the MT model can often approximate the different opinions of the viewers, while approximating the average viewer V avg .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gt",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper we propose a Multi-Task (MT) Learning approach for modeling emotion evoked by movies. Our MT approach jointly models the evoked emotion per each viewer as well as the average evoked emotion. Concretely, our architecture has two separate blocks: a shared convolutional block for all the viewers, with specific backbones for the visual and text modality, respectively, and a dedicated fully-convolutional block for each viewer, including the average viewer. We show that the accuracy obtained by this MT architecture is significantly higher than other techniques directly trained on the average viewer.\n\nFor our experiments we use the COGNIMUSE dataset. Concretely, we perform an extended set of experiments by considering cross-validation instead of evaluating the models in a single data partition. Our results show that the proposed MT approach obtains the highest accuracies in the valence classification of the aggregated annotations. Furthermore, the MT approach also obtains better accuracies in approximating each single viewer. This is observed in our multimodal formulation as well as in our ablations studies that evaluate, separately, the visual and the text modalities.\n\nWe believe that the proposed MT approach can be useful for other tasks that need to deal with subjective judgements or perceptions, such as recommender systems or emotion recognition in other contexts. Also, we hope our results will encourage the release of individual annotations instead of just the aggregated annotations in future data collection efforts.\n\nThe code is available in our project repository 1  .",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the Multi-Task model. This architecture is applied",
      "page": 2
    },
    {
      "caption": "Figure 2: Architecture of the Single-Task (ST) model. It consists of feature",
      "page": 3
    },
    {
      "caption": "Figure 3: Average correlation matrix of all movies. Correlation values are",
      "page": 3
    },
    {
      "caption": "Figure 4: shows the histogram (total counts) of positive and",
      "page": 4
    },
    {
      "caption": "Figure 4: Viewer annotation distribution for Valence (Positive vs. Negative) on each movie, including the average viewer (last plot). Notice that, for all the",
      "page": 5
    },
    {
      "caption": "Figure 5: Qualitative results. For some randomly selected segments of the movies LOR, CRA, FNE, and CHI, the ﬁgure shows the ground truth annotation per",
      "page": 7
    },
    {
      "caption": "Figure 3: Finally, Fig. 5 provides some qualitative results. Concretely,",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F1": "F2",
          "CRA, DEP, FNE, GLA, LOR": "BMI, DEP, FNE, GLA, LOR",
          "CHI": "CRA",
          "BMI": "CHI"
        },
        {
          "F1": "F3",
          "CRA, DEP, FNE, GLA, LOR": "BMI, CHI, FNE, GLA, LOR",
          "CHI": "DEP",
          "BMI": "CRA"
        },
        {
          "F1": "F4",
          "CRA, DEP, FNE, GLA, LOR": "BMI, CHI, CRA, GLA, LOR",
          "CHI": "FNE",
          "BMI": "DEP"
        },
        {
          "F1": "F5",
          "CRA, DEP, FNE, GLA, LOR": "BMI, CHI, CRA, DEP, LOR",
          "CHI": "GLA",
          "BMI": "FNE"
        },
        {
          "F1": "F6",
          "CRA, DEP, FNE, GLA, LOR": "BMI, CHI, CRA, DEP, FNE",
          "CHI": "LOR",
          "BMI": "GLA"
        },
        {
          "F1": "F7",
          "CRA, DEP, FNE, GLA, LOR": "BMI, CRA, DEP, FNE, GLA",
          "CHI": "CHI",
          "BMI": "LOR"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "V1": "V2",
          "74.30": "77.46",
          "63.45": "67.50",
          "72.86": "69.63"
        },
        {
          "V1": "V3",
          "74.30": "76.37",
          "63.45": "68.70",
          "72.86": "70.27"
        },
        {
          "V1": "V4",
          "74.30": "73.71",
          "63.45": "71.28",
          "72.86": "68.70"
        },
        {
          "V1": "V5",
          "74.30": "85.89",
          "63.45": "65.44",
          "72.86": "70.42"
        },
        {
          "V1": "V6",
          "74.30": "71.52",
          "63.45": "70.82",
          "72.86": "76.90"
        },
        {
          "V1": "V7",
          "74.30": "70.63",
          "63.45": "68.90",
          "72.86": "70.77"
        },
        {
          "V1": "Mean",
          "74.30": "75.69",
          "63.45": "68.01",
          "72.86": "71.36"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "It's the tree": "",
          "Yes, the White Tree \nof Gondor.": "",
          "The tree of the \nking.": "",
          "Lord Denethor, \nhowever, is not king.": "",
          "He is a steward only, \na caretaker of the \nthrone.": ""
        },
        {
          "It's the tree": "GT",
          "Yes, the White Tree \nof Gondor.": "GT",
          "The tree of the \nking.": "GT",
          "Lord Denethor, \nhowever, is not king.": "GT",
          "He is a steward only, \na caretaker of the \nthrone.": "GT"
        },
        {
          "It's the tree": "Neg",
          "Yes, the White Tree \nof Gondor.": "Neg",
          "The tree of the \nking.": "Neg",
          "Lord Denethor, \nhowever, is not king.": "Pos",
          "He is a steward only, \na caretaker of the \nthrone.": "Neg"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Pos",
          "Lord Denethor, \nhowever, is not king.": "Pos",
          "He is a steward only, \na caretaker of the \nthrone.": "Pos"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Pos",
          "Lord Denethor, \nhowever, is not king.": "Pos",
          "He is a steward only, \na caretaker of the \nthrone.": "Pos"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Pos",
          "Lord Denethor, \nhowever, is not king.": "Neg",
          "He is a steward only, \na caretaker of the \nthrone.": "Pos"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Pos",
          "Lord Denethor, \nhowever, is not king.": "Pos",
          "He is a steward only, \na caretaker of the \nthrone.": "Pos"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Pos",
          "Lord Denethor, \nhowever, is not king.": "Pos",
          "He is a steward only, \na caretaker of the \nthrone.": "Pos"
        },
        {
          "It's the tree": "Pos",
          "Yes, the White Tree \nof Gondor.": "Pos",
          "The tree of the \nking.": "Neg",
          "Lord Denethor, \nhowever, is not king.": "Neg",
          "He is a steward only, \na caretaker of the \nthrone.": "Neg"
        },
        {
          "It's the tree": "Neg",
          "Yes, the White Tree \nof Gondor.": "Neg",
          "The tree of the \nking.": "Neg",
          "Lord Denethor, \nhowever, is not king.": "Neg",
          "He is a steward only, \na caretaker of the \nthrone.": "Neg"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hello, little Fella": "",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": ""
        },
        {
          "Hello, little Fella": "GT",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "GT"
        },
        {
          "Hello, little Fella": "Neg",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Neg"
        },
        {
          "Hello, little Fella": "Neg",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Pos"
        },
        {
          "Hello, little Fella": "Pos",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Pos"
        },
        {
          "Hello, little Fella": "Neg",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Neg"
        },
        {
          "Hello, little Fella": "Pos",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Pos"
        },
        {
          "Hello, little Fella": "Pos",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Pos"
        },
        {
          "Hello, little Fella": "Neg",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Neg"
        },
        {
          "Hello, little Fella": "Neg",
          "Beaity, isn’t he? I \nfound that guy \nstruggling for life and \nI saved him": "Neg"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion, core affect, and psychological construction",
      "authors": [
        "J Russell"
      ],
      "year": "2009",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "2",
      "title": "The Affective Power of Movies",
      "authors": [
        "C Plantinga"
      ],
      "year": "2013",
      "venue": "Psychocinematics"
    },
    {
      "citation_id": "3",
      "title": "The Routledge Companion to Philosophy and Film",
      "authors": [
        "C Plantinga"
      ],
      "year": "2008",
      "venue": "The Routledge Companion to Philosophy and Film"
    },
    {
      "citation_id": "4",
      "title": "The mediaeval 2016 emotional impact of movies task",
      "authors": [
        "E Dellandréa",
        "L Chen",
        "Y Baveye",
        "M Sjöberg",
        "C Chamaret"
      ],
      "year": "2016",
      "venue": "InCEUR Workshop Proceedings"
    },
    {
      "citation_id": "5",
      "title": "The MediaEval 2015 Affective Impact of Movies Task. InMediaEval",
      "authors": [
        "M Sjöberg",
        "Y Baveye",
        "H Wang",
        "V Quang",
        "B Ionescu",
        "E Dellandréa",
        "M Schedl",
        "C Demarty",
        "L Chen"
      ],
      "year": "2015",
      "venue": "The MediaEval 2015 Affective Impact of Movies Task. InMediaEval"
    },
    {
      "citation_id": "6",
      "title": "Corpus development for affective video indexing",
      "authors": [
        "M Soleymani",
        "M Larson",
        "T Pun",
        "A Hanjalic"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Multimodal Deep Models for Predicting Affective Responses Evoked by Movies",
      "authors": [
        "H Thao",
        "D Herremans",
        "G Roig"
      ],
      "year": "2019",
      "venue": "InICCV Workshops"
    },
    {
      "citation_id": "8",
      "title": "Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis",
      "authors": [
        "S Poria",
        "H Peng",
        "A Hussain",
        "N Howard",
        "E Cambria"
      ],
      "year": "2017",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Recognizing Induced Emotions of Movie Audiences from Multimodal Information",
      "authors": [
        "M Muszynski",
        "L Tian",
        "C Lai",
        "J Moore",
        "T Kostoulas",
        "P Lombardo",
        "T Pun",
        "G Chanel"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Multimodal Deep Models for Predicting Affective Responses Evoked by Movies",
      "authors": [
        "H Thao",
        "D Herremans",
        "G Roig"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)"
    },
    {
      "citation_id": "11",
      "title": "Feature Selection and Multimodal Fusion for Estimating Emotions Evoked by Movie Clips",
      "authors": [
        "Y Timar",
        "N Karslioglu",
        "H Kaya",
        "A Salah"
      ],
      "year": "2018",
      "venue": "InProceedings of the 2018 ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "12",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "W Liu",
        "J Qiu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349"
    },
    {
      "citation_id": "13",
      "title": "Modeling multimodal cues in a deep learning-based framework for emotion recognition in the wild",
      "authors": [
        "S Pini",
        "O Ahmed",
        "M Cornia",
        "L Baraldi",
        "R Cucchiara",
        "B Huet"
      ],
      "year": "2017",
      "venue": "InProceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "14",
      "title": "COGNIMUSE: A multimodal video database annotated with saliency, events, semantics and emotion with application to summarization",
      "authors": [
        "Zlatintsi Koutras",
        "P Evangelopoulos",
        "G Malandrakis",
        "N Efthymiou",
        "N Pastra",
        "K Potamianos",
        "A Maragos"
      ],
      "year": "2017",
      "venue": "EURASIP Journal on Image and Video Processing"
    },
    {
      "citation_id": "15",
      "title": "Identifying Stable Patterns over Time for Emotion Recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition Based on High-Resolution EEG Recordings and Reconstructed Brain Sources",
      "authors": [
        "H Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Albera"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "SentiDiff: Combining Textual Information and Sentiment Diffusion Patterns for Twitter Sentiment Analysis",
      "authors": [
        "L Wang",
        "J Niu",
        "S Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "18",
      "title": "Multimodal Sentiment Analysis of Spanish Online Videos",
      "authors": [
        "V Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "19",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "A multimodal convolutional neuro-fuzzy network for emotion understanding of movie clips",
      "authors": [
        "T Nguyen",
        "S Kavuri",
        "M Lee"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "21",
      "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "22",
      "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "MediaEval 2016 Emotional Impact of Movies Task",
      "authors": [
        "Jan Gaus",
        "Y Meng",
        "H Zhang",
        "F Bul"
      ],
      "year": "2016",
      "venue": "InMediaEval"
    },
    {
      "citation_id": "24",
      "title": "THU-HCSI at MediaEval 2016: Emotional Impact of Movies Task",
      "authors": [
        "Y Ma",
        "Z Ye",
        "M Xu"
      ],
      "year": "2016",
      "venue": "InMediaEval"
    },
    {
      "citation_id": "25",
      "title": "Your reactions suggest you liked the movie: Automatic content rating via reaction sensing",
      "authors": [
        "X Bao",
        "S Fan",
        "A Varshavsky",
        "K Li",
        "Roy Choudhury"
      ],
      "year": "2013",
      "venue": "InProceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition based on 3D fuzzy visual and EEG features in movie clips",
      "authors": [
        "G Lee",
        "M Kwon",
        "S Sri",
        "M Lee"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "2016 international joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "28",
      "title": "Every rating matters: Joint learning of subjective labels and individual annotators for speech emotion classification",
      "authors": [
        "H Chou",
        "C Lee"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
      "authors": [
        "J Li",
        "C Lee"
      ],
      "year": "2019",
      "venue": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile"
    },
    {
      "citation_id": "30",
      "title": "From Hard to Soft",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "The HUMAINE Database: Addressing the Collection and Annotation of Naturalistic and Induced Emotional Data",
      "authors": [
        "E Douglas-Cowie",
        "R Cowie",
        "I Sneddon",
        "C Cox",
        "O Lowry",
        "M Mcrorie",
        "J.-C Martin",
        "L Devillers",
        "S Abrilian",
        "A Batliner",
        "N Amir",
        "K Karpouzis"
      ],
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "32",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "A Schaefer",
        "F Nils",
        "X Sanchez",
        "P Philippot"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "33",
      "title": "LIRIS-ACCEDE: A Video Database for Affective Content Analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "Liming Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Probabilistic Multigraph Modeling for Improving the Quality of Crowdsourced Affective Data",
      "authors": [
        "J Ye",
        "J Li",
        "M Newman",
        "R Adams",
        "J Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "36",
      "title": "Regression shrinkage and selection via the lasso: a retrospective",
      "authors": [
        "R Tibshirani"
      ],
      "year": "2011",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)"
    },
    {
      "citation_id": "37",
      "title": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "39",
      "title": "Personalized Multitask Learning for Predicting Tomorrow's Mood. Stress, and Health",
      "authors": [
        "S Taylor",
        "N Jaques",
        "E Nosakhare",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}