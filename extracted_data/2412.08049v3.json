{
  "paper_id": "2412.08049v3",
  "title": "Emoverse: Exploring Multimodal Large Language Models For Sentiment And Emotion Understanding",
  "published": "2024-12-11T02:55:00Z",
  "authors": [
    "Ao Li",
    "Longwei Xu",
    "Chen Ling",
    "Jinghui Zhang",
    "Pengwei Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sentiment and emotion understanding are essential to applications such as human-computer interaction and depression detection. While Multimodal Large Language Models (MLLMs) demonstrate robust general capabilities, they face considerable challenges in the field of affective computing, particularly in detecting subtle facial expressions and handling complex emotion-related tasks, such as emotion reason inference and understanding emotions in long-context scenarios. Furthermore, there is a lack of a unified MLLM that can effectively handle both sentiment and emotionrelated tasks. To address these challenges, we explore multi-task training strategies for MLLMs in affective computing and introduce Emotion Universe (EmoVerse), an MLLM designed to handle a broad spectrum of sentiment and emotionrelated tasks. In addition, EmoVerse is capable of deeply analyzing the underlying causes of emotional states. We also introduce the Affective Multitask (AMT) Dataset, which supports multimodal sentiment analysis, multimodal emotion recognition, facial expression recognition, emotion reason inference, and emotion cause-pair extraction tasks. Extensive experiments demonstrate that EmoVerse outperforms existing methods, achieving state-ofthe-art results in sentiment and emotion-related tasks. The code is available at https://github.com/ liaolea/EmoVerse.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sentiment and emotion understanding are crucial for applications such as human-computer interaction and mental health monitoring. Traditional unimodal approaches, which rely solely on facial expressions  [Jiang et al., 2020] , text  [Lei et al., 2023] , or audio  [Hsu et al., 2021] , have made progress but remain inherently limited, as each modality captures only a partial view of human emotions, making it challenging to achieve a comprehensive understanding. To address these limitations, researchers  [Yang et al., 2023; Li et al., 2023c ; † Equal contribution. * Corresponding authors.  Lv et al., 2021; Ghosal et al., 2019; Hu et al., 2021a; Hu et al., 2022a; Sun et al., 2021; Hu et al., 2021c; Zhang et al., 2024a; Cheng et al., 2023; Wang et al., 2024b]  have increasingly turned to multimodal methods that integrate multiple modalities for sentiment analysis or emotion recognition. While these approaches have enhanced classification accuracy, they predominantly focus on identifying sentiment or emotions, rather than delving into their underlying causes or reasoning about emotional contexts. Recent multimodal large language models (MLLMs) have made significant strides in general visual-language understanding  [Liu et al., 2024] . However, in the field of affective computing, their zero-shot performance remains limited, particularly in tasks such as facial expression recognition, emotion reason inference, and contextual emotion understanding. Moreover, there is currently a lack of an MLLM capable of performing multiple emotion-related tasks.\n\nMLLMs can effectively perform multiple visual tasks in a zero-shot setting after visual instruction fine-tuning. However, in the field of affective computing, existing MLLMs continue to struggle with sentiment understanding after finetuning with emotion-related instruction data, as shown in Figure  1  and exemplified by models such as Emotion-LLaMA  [Cheng et al., 2024a] . This discrepancy highlights a broader issue: existing MLLMs fail to capture the intricate relationship between sentiment and emotion. A promising solution to this challenge is multitask learning, where the model is trained simultaneously on multimodal sentiment analysis (MSA) and multimodal emotion recognition (MER) tasks. An example of this approach is UniMSE  [Hu et al., 2022b] , which demonstrates the effectiveness of multitask learning. However, a significant open challenge remains: how to optimally leverage multiple tasks to enhance MLLM training?\n\nIn response, we propose to train a general-purpose MLLM for affective computing using multitask learning. In addition to MSA and MER, we incorporate facial expression recognition (FER), emotion reasoning inference (ERI), and emotion cause-pair extraction (ECPE) tasks, aiming to improve the model's capabilities in facial expression analysis, causal reasoning, and long-context comprehension. The ECPE task requires the model to identify the emotion in a specific sentence within a dialogue and determine the cause of that emotion, which is linked to a previous sentence with multimodal setting. This task demands a high level of reasoning and context understanding from the model. By examining the relationships among these tasks, we propose an optimal training strategy called Multistage Multitask Sentiment and Emotion Instruction Tuning Strategy (M2SE), which is designed to enable MLLMs to achieve strong performance across a variety of affective tasks. This training strategy is adaptable to different MLLMs. Based on the M2SE strategy, we develop a MLLM named Emotion Universe (EmoVerse).\n\nAdditionally, existing datasets are often insufficient for training MLLMs in multitask settings due to their lack of diversity and task coverage. To address this issue, we construct the Affective Multitask (AMT) dataset, which includes the five tasks mentioned above. By incorporating these tasks, the AMT dataset facilitates the development of models capable of recognizing, reasoning, and inferring the causes of sentiment and emotion.\n\nIn summary, our main contributions are as follows:\n\n• We construct the AMT dataset. Each piece of data in this dataset contains queries for five tasks and the corresponding labels. The AMT dataset encourages the model to capture the relationship between sentiment and emotion from the perspective of different tasks, enabling the model to extract richer contextual information.\n\n• We explore the relationships between different sentiment and emotion-related tasks within a multitask learning framework. Through this exploration, we identify the optimal strategy, called M2SE. This exploration is the first multitask investigation in the field of affective computing.\n\n• We develop the EmoVerse model. EmoVerse unifies tasks in the sentiment and emotion domains by leveraging the M2SE strategy.\n\n• EmoVerse demonstrates outstanding performance across a wide range of tasks. On the CMU-MOSEI dataset for MSA  [Zadeh et al., 2018] , it achieves an Acc2 score of 88.51%. Similarly, on the MELD dataset for MER  [Poria et al., 2018] , it attains a weighted F1 score of 66.74.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Large Language Models",
      "text": "Multimodal Large Language Models (MLLMs) have gained significant attention due to their powerful reasoning capabilities. These models are typically trained for general-purpose tasks using a standard framework that includes modal encoders, a large language model (LLM), and a mapping layer (e.g.,  Linear [Chen et al., 2023] , MLP  [Liu et al., 2024; Chen et al., 2024a] , or Q-Former  [Li et al., 2023b] ). While they excel in general tasks, MLLMs struggle with MSA and MER due to insufficient training on sentiment-specific datasets and emotion-related knowledge. Recent efforts have focused on improving MLLMs by incorporating multimodal emotion datasets and emotion reasoning tasks  [Cheng et al., 2024a; Lian et al., 2024] . However, these models still face challenges in leveraging contextual information effectively, particularly in sentiment analysis, where understanding the nuances is crucial.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multitask Learning",
      "text": "Multitask Learning (MTL) is a machine learning paradigm that enables models to simultaneously learn multiple related tasks, thereby allowing knowledge from one task to be transferred to others. In affective computing, where multimodal inputs (e.g., text, audio, and visual frames) carry diverse and distinct information, MTL frameworks aim to leverage the synergies between tasks for improved performance. For instance, UniMSE  [Hu et al., 2022b]  unifies MSA and MER within a single framework. However, it relies solely on labeled data to establish the relationship between sentiment and emotion, without fully utilizing multimodal connections.\n\nM2Seq2Seq  [Zhang et al., 2023]  proposes a multimodal multitask learning model based on the encoder-decoder architecture. Recently, Emotion-LLaMA  [Cheng et al., 2024a]  has shown promising results in MER by incorporating multitask learning, including emotion cause inference. However, despite its success in MER, Emotion-LLaMA still struggles with MSA tasks and fails to effectively synthesize contextual information for emotion inference.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Affective Multitask Dataset",
      "text": "The AMT dataset is constructed by integrating data from the CMU-MOSEI, MELD, and ECF2.0  [Wang et al., 2024a]  datasets to support five distinct tasks: MSA, MER, FER, ERI, and ECPC. Figure  2  illustrates a sample from the AMT dataset.\n\nFor the MSA and MER tasks, we use the original sentiment labels and emotion annotations from the source datasets as task labels. Due to the complexity of annotating the ECPE task, we refer to the annotation method of the ECF2.0 dataset and manually annotate a portion of the data. This is then combined with the existing ECF2.0 data to form the ECPE portion of the AMT dataset for further emotional causality analysis.\n\nIn the FER task, we adapt methods from the MERR dataset  [Cheng et al., 2024a]  by extracting peak frames based on the Action Units (AUs) calculated for each frame using the OpenFace tool. The composite score for each frame is determined by summing the AU values, as shown in the following equation:\n\nwhere S f is the composite score of frame f , and AU i (f ) represents the Action Unit value for the i-th facial action unit at frame f . The peak frame with the highest score is selected to represent the most expressive moment. For videos featuring multiple characters, such as MELD dataset, we improve the method by extracting peak frames separately for each character and then selecting the final peak frame by comparing the highest composite scores from all characters:\n\nwhere\n\nf is the composite score for character i and k represents the total number of characters in the frame. The final peak frame is the one with the highest composite score among all characters.\n\nAfter selecting the peak frames, we extract the facial expression captions by combining the AUs from the peak frame with the predefined AU list corresponding to the emotion. Let AU frame (f ) represent the set of AUs from the peak frame for a given emotion, and AU emotion (e) represent the predefined AU list for a specific emotion e. The common AUs are obtained by taking the intersection of the two sets:\n\nwhere AU common (f, e) is the set of AUs common to both the peak frame and the emotion-specific AU list. These common AUs are then mapped to textual information to generate the FER annotations for the task. This approach enables the model to link the facial expressions captured in the peak frames with the corresponding emotional state.\n\nIn the ERI task, we input the video into the Internvl2-8B 1 model to generate detailed descriptions of both the characters and the scene. These descriptions allow the model to analyze emotional causes by considering facial expressions, speech, and environmental context. Finally, we use the LLaMA 3.1-8B 2 model to infer the underlying causes of the characters' emotions by synthesizing the character and scene descriptions.\n\nWe conduct a comprehensive preprocessing and quality control procedure on the raw data from the three datasets, systematically removing problematic or noisy samples. In parallel, we manually curate high-quality samples according to the methodology described above. The final refined dataset, detailed in Table  1 , includes the number of tasks per category. It 1 https://huggingface.co/OpenGVLab/InternVL2-8B 2 https://github.com/meta-llama/llama3 is important to note that each data sample may be associated with a varying number of tasks. Table  1  provides a detailed breakdown of the tasks available for each category.\n\nAdditionally, we transform the dataset into a structured format, which includes three key elements: query, response, and images/videos. This format facilitates the instruction tuning of MLLMs. In this section, we explore how different tasks and varying sampling ratios for each task impact the final performance of the MLLM. After identifying the optimal training strategy, we introduce EmoVerse. Generally, the training process for MLLMs is divided into two stages: pretraining and instruction tuning. Inspired by the human learning process, which progresses from simple to complex, we also divide the affective domain fine-tuning process into these two stages, adopting a training approach that goes from simple to difficult. We conduct experiments using EmoVerse-4B as well as the AMT dataset (refer to Section 5.2).\n\nIn the AMT dataset, the simplest task is MSA, which requires the model to perform binary classification. The more challenging tasks are MER and FER, while ERI and ECPE, which involve reasoning and inference, are the most difficult. Based on this, our approach is to first train the model using MSA, MER, and FER in a multitask learning setup, with MSA receiving the highest sampling ratio due to its simplicity. In the second stage, we focus on training the model with MER, ERI, and ECPE in a multitask learning setup, building on the model's performance from the previous stage.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multitask Pretraining",
      "text": "In this phase, we randomly select 15,000 samples from the AMT dataset. The primary objective is to enable the model to recognize facial expressions and, through these expressions, capture the relationship between sentiment and emotion. As discussed earlier, MSA is the simplest task, and therefore, it is assigned the highest sampling ratio, while MER, being a more complex task, receives a lower sampling ratio. In this stage, we also investigate the impact of varying the sampling ratio for the FER task on the model's performance. To assess the model's performance during this phase, we use the CMU-MOSEI test set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Acc2",
      "text": "EmoVerse (6:1:3) 77.09 / 87.47 EmoVerse (6:3:1) 82.61 / 87.50 As shown in Table  2 , increasing the sampling rate for the FER task leads to improved performance on the MSA task. Consequently, for the first stage, we implement a multitask training setup with a sampling ratio of MSA:FER:MER = 6:3:1. This configuration enables the model to not only understand the relationship between sentiment and emotion but also accurately recognize facial action units (AUs), thereby enhancing MSA performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multitask Reason Fine-Tuning",
      "text": "For this phase, we randomly select the remaining samples from the AMT dataset and distribute them across the tasks. As previously mentioned, in the second phase, we introduce more challenging tasks, including MER, ERI, and ECPE. The final performance of the MSA task is evaluated on the CMU-MOSEI dataset, the MER task performance on the MELD dataset, and the ECPE task performance on the ECF2.0 dataset (refer to Section 5.1). We begin with the MER task and gradually introduce additional tasks to examine the impact of training with different tasks. The results are summarized in Tabel 3.  When we train the model in the second phase using only these three tasks, the final model performs poorly on the MSA task. This indicates the presence of catastrophic forgetting in MLLM multitask training. However, when we allocate a small portion of the MSA task in the second phase, overall performance improves. This suggests that revisiting MSA-related knowledge helps the model deepen its understanding of the relationship between sentiment and emotion. In the second phase, the final sampling ratio we chose is MER:ERI:ECPE:MSA = 3:3:3:1.\n\nTherefore, the optimal training strategy we have identified involves using the MSA, FER, and MER tasks in the first phase, and the MSA, MER, ERI, and ECPE tasks in the second phase for multi-stage multitask training. We refer to this strategy as M2SE.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emoverse",
      "text": "To validate the effectiveness of the M2SE strategy, we select the most widely used MLLM framework without introducing any multitask modifications to its architecture. This ensures that any observed performance improvements can be directly attributed to the proposed training strategy, rather than architectural changes. The proposed MLLM architecture, Emo-Verse, as shown in Figure  3 , consists of a visual encoder, a linear projector, and the LLM.\n\nWe train EmoVerse following M2SE strategy using the AMT datasets. For a data sample D, the input consists of the pair {v, t}, where v represents the video and t represents the text. First, the video v is processed to extract visual tokens using a Vision Transformer (ViT)  [Dosovitskiy et al., 2021] , producing a set of visual tokens T v . These visual tokens T v are then passed through a linear transformation W v to align the visual features with the feature space of the LLM. Mathematically, this can be expressed as:\n\nwhere T ′ v represents the aligned visual features. Next, the text t, task identifiers, and instructions are tokenized and passed through the embedding layer of the LLM, producing the tokenized representation T t . The tokenized visual features T ′ v and the tokenized text T t are then concatenated to form the final input to the model:\n\nwhere X is the combined input. The model processes this concatenated input and outputs the response ŷ, which corresponds to the task at hand:\n\nwhere M is the model and ŷ is the model's prediction or answer.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments Setup",
      "text": "EmoVerse is trained solely on the AMT dataset without the addition of any external data, and it is tested on the following task-specific test sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "We use the CMU-MOSEI datasets for MSA. The CMU-MOSEI dataset is an extended version of the CMU-MOSI dataset. It encompasses 3228 videos from 1000 speakers, comprising a total of 23453 sentences. Similar to the CMU-MOSI dataset, it covers multiple emotional dimensions, employing the same annotation labeling approach. The results for Acc-2 calculated based on negative/non-negative (N/N) scheme, while the results on the right are calculated according to negative/positive (N/P) definition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Recognition",
      "text": "We use the MELD datasets for ER. The MELD dataset is an emotional dialogue dataset comprising 1433 dialogue segments and 13708 utterances from movies and TV shows. Each statement in the dialogue is labeled with one of seven emotions: anger, disgust, sadness, joy, neutral, surprise, and fear. Furthermore, the MELD data set provides annotations for emotional polarity (positive, negative, and neutral) for each utterance. We use Acc and weighted f1-sorce for evaluation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Cause-Pair Extraction",
      "text": "We use the ECF2.0 datasets for ECPE. The ECF2.0 dataset is a comprehensive resource for emotion cause-pair extraction, comprising 1,715 conversations and 16,720 utterances. Each utterance in the dataset is annotated with emotion causepair information, which helps in understanding the emotional context and its causes. The dataset is divided into training and evaluation sets, with 1,374 conversations and 13,619 utterances in the training set, and 341 conversations and 3,101 utterances in the test set. We refer to previous evaluations and assess the ECF2.0 test set using the F1 Score and Weighted F1 Score.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "For the visual encoder, we use a pre-trained ViT with input images resized to 448 × 448 pixels. During fine-tuning, the visual backbone is frozen, and training focuses on the linear projection layer. For the LLM, EmoVerse-4B uses Phi-3-Mini  [Abdin et al., 2024] , and EmoVerse-8B uses InternLM-2.5-7B-Chat 3  , both augmented with LoRA  [Hu et al., 2021b]  for parameter-efficient fine-tuning.\n\nTraining is done for 2 epochs with LoRA hyperparameters r = 8 and a = 32, a learning rate of 1 × 10 -5 , and a warmup cosine decay schedule. EmoVerse-4B training takes approximately 48 hours on 2 NVIDIA RTX 3090 GPUs, while EmoVerse-8B requires about 55 hours on a single NVIDIA A100 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Comparison",
      "text": "For MSA task, we compare our EmoVerse model with several SOTA baselines, including MulT  [Tsai et al., 2019] , PMR  [Lv et al., 2021] ,  MISA [Hazarika et al., 2020] , MMIM [Han   et al., 2022a] , EmoCaps  [Li et al., 2022] , DisGCN  [Sun et al., 2021] , MMGCN  [Hu et al., 2021c] , GA2MIF  [Li et al., 2023a] , UniMSE  [Hu et al., 2022b] , and FacialMMT  [Zheng et al., 2023] .\n\nAdditionally, for both MSA and MER, we select models capable of processing both images and videos, including LLaVA-Video  [Zhang et al., 2024b]  and InternVL2  [Chen et al., 2024b] . For the ECPE task, we conduct tests solely on MLLMs, as they are better equipped to handle the complexity and multimodal nature of the task. The performance comparison results, shown in Table  4  clearly demonstrate that Emo-Verse achieves SOTA performance across both tasks, outperforming all the aforementioned baselines.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis Of Emotion Reasoning",
      "text": "To demonstrate EmoVerse's qualitative performance, we compare emotion reasoning results across five models: GPT-4o, Video-LLaMA2  [Cheng et al., 2024b] , LLaMA-Video, In-ternVL2, and EmoVerse, as shown in Figure  4 . The video depicts a woman displaying a strong surprise reaction, highlighting the models' ability to infer emotional states from multimodal inputs.\n\nGPT-4o and Video-LLaMA2 misclassify the emotion as \"confident\" or \"assertive,\" while LLaMA-Video and In-ternVL2 correctly identify it as \"surprise.\" However, these models fail to fully integrate multimodal information for emotion reasoning. LLaMA-Video relies solely on facial expressions, while InternVL2 combines facial cues with textual analysis but lacks detailed explanation of the emotional cause. In contrast, EmoVerse effectively combines video and text to accurately infer the emotional cause, providing a comprehensive interpretation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Of The M2Se Strategy With Single-Task Fine-Tuning",
      "text": "To assess the impact of the M2SE strategy within the same framework and compare its performance across tasks, we conduct an ablation study.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we explore the impact of different affective tasks on MLLM under a multistage and multitask training framework, identifying and validating the optimal strategy called M2SE. Using the M2SE strategy, we train EmoVerse, addressing the lack of a general-purpose MLLM in affective computing. Additionally, we construct the AMT dataset, designed to support sentiment and emotion perception training for MLLMs. Experimental results demonstrate that Emo-Verse achieves state-of-the-art performance across multiple tasks. This study highlights the effectiveness of the M2SE strategy in unifying sentiment and emotion tasks, paving the way for more emotionally intelligent AI systems.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: and exemplified by models such as Emotion-",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy. (a) The MSA-trained model performs",
      "page": 2
    },
    {
      "caption": "Figure 2: The construction process of the AMT dataset. The AMT dataset includes five tasks: multimodal sentiment analysis, multimodal",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates a sample from the AMT",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of the M2SE training strategy and EmoVerse architecture. For different training stages, we assign tasks to the data",
      "page": 4
    },
    {
      "caption": "Figure 3: , consists of a visual encoder, a",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-",
      "page": 7
    },
    {
      "caption": "Figure 4: The video",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "liaolea/EmoVerse."
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "1\nIntroduction"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "Sentiment and emotion understanding are crucial for applica-"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "tions such as human-computer interaction and mental health"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "monitoring.\nTraditional unimodal\napproaches, which rely"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "solely on facial expressions [Jiang et al., 2020],\ntext [Lei et"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "al., 2023], or audio [Hsu et al., 2021], have made progress"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "but remain inherently limited, as each modality captures only"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "a partial view of human emotions, making it challenging to"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "achieve a comprehensive understanding.\nTo address\nthese"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        },
        {
          "tasks. The code is available at https://github.com/": "limitations,\nresearchers [Yang et al., 2023; Li et al., 2023c;"
        },
        {
          "tasks. The code is available at https://github.com/": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Shandong University 2Xi’an Jiaotong University": "Abstract"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "Sentiment\nand emotion understanding are\nessen-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tial\nto applications\nsuch as human-computer\nin-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "teraction and depression detection. While Multi-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "modal Large Language Models (MLLMs) demon-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "strate\nrobust general\ncapabilities,\nthey face\ncon-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "siderable challenges in the field of affective com-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "puting, particularly in detecting subtle facial ex-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "pressions\nand handling complex emotion-related"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tasks, such as emotion reason inference and under-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "standing emotions in long-context scenarios. Fur-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "thermore,\nthere is a lack of a unified MLLM that"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "can effectively handle both sentiment and emotion-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "related tasks. To address these challenges, we ex-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "plore multi-task training strategies for MLLMs in"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "affective computing and introduce Emotion Uni-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "verse\n(EmoVerse),\nan MLLM designed to han-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "dle a broad spectrum of\nsentiment and emotion-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "related tasks.\nIn addition, EmoVerse\nis\ncapable"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "of deeply analyzing the underlying causes of emo-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tional states. We also introduce the Affective Mul-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "titask (AMT) Dataset, which supports multimodal"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "sentiment analysis, multimodal emotion recogni-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tion, facial expression recognition, emotion reason"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "inference, and emotion cause-pair extraction tasks."
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "Extensive experiments demonstrate that EmoVerse"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "outperforms existing methods, achieving state-of-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "the-art\nresults\nin\nsentiment\nand\nemotion-related"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tasks. The code is available at https://github.com/"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "liaolea/EmoVerse."
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "1\nIntroduction"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "Sentiment and emotion understanding are crucial for applica-"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "tions such as human-computer interaction and mental health"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "monitoring.\nTraditional unimodal\napproaches, which rely"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "solely on facial expressions [Jiang et al., 2020],\ntext [Lei et"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "al., 2023], or audio [Hsu et al., 2021], have made progress"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "but remain inherently limited, as each modality captures only"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "a partial view of human emotions, making it challenging to"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "achieve a comprehensive understanding.\nTo address\nthese"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "limitations,\nresearchers [Yang et al., 2023; Li et al., 2023c;"
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": ""
        },
        {
          "1Shandong University 2Xi’an Jiaotong University": "† Equal contribution. * Corresponding authors."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "and emotion."
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "In summary, our main contributions are as follows:"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "• We construct\nthe AMT dataset.\nEach piece of data in"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "this dataset contains queries for five tasks and the cor-"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "responding labels.\nThe AMT dataset encourages\nthe"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "model to capture the relationship between sentiment and"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "emotion from the perspective of different tasks, enabling"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "the model to extract richer contextual information."
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "• We\nexplore\nthe\nrelationships between different\nsenti-"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "ment and emotion-related tasks within a multitask learn-"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "ing framework.\nThrough this exploration, we identify"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "the optimal strategy, called M2SE. This exploration is"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "the first multitask investigation in the field of affective"
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "computing."
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": ""
        },
        {
          "recognizing, reasoning, and inferring the causes of sentiment": "• We develop the EmoVerse model.\nEmoVerse unifies"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "well on MSA but poorly on MER. (b) The ER-trained model performs well on MER but poorly on MSA. (c) EmoVerse,\ntrained with the"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "M2SE strategy, excels on both MSA and MER tasks and successfully performs emotion reasoning inference."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "setting. This task demands a high level of reasoning and con-\ntasks\nin the sentiment and emotion domains by lever-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "text understanding from the model. By examining the rela-\naging the M2SE strategy."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "tionships among these tasks, we propose an optimal\ntraining"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "• EmoVerse demonstrates outstanding performance across"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "strategy called Multistage Multitask Sentiment and Emotion"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "a wide range of tasks. On the CMU-MOSEI dataset for"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "Instruction Tuning Strategy (M2SE), which is designed to en-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "MSA[Zadeh et al., 2018],\nit achieves an Acc2 score of"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "able MLLMs to achieve strong performance across a variety"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "88.51%. Similarly, on the MELD dataset for MER[Poria"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "of affective tasks. This training strategy is adaptable to dif-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "et al., 2018], it attains a weighted F1 score of 66.74."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "ferent MLLMs. Based on the M2SE strategy, we develop a"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "MLLM named Emotion Universe (EmoVerse)."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "Additionally,\nexisting datasets\nare often insufficient\nfor"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "2\nRelated Work"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "training MLLMs in multitask settings due to their lack of di-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "2.1\nMultimodal Large Language Models\nversity and task coverage. To address this issue, we construct"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "the Affective Multitask (AMT) dataset, which includes the"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "Multimodal Large Language Models (MLLMs) have gained"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "five tasks mentioned above. By incorporating these tasks, the"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "significant attention due to their powerful reasoning capabil-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "AMT dataset facilitates the development of models capable of"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "ities. These models are typically trained for general-purpose"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "recognizing, reasoning, and inferring the causes of sentiment"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "tasks using a standard framework that\nincludes modal en-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "and emotion."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "coders, a large language model (LLM), and a mapping layer"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "In summary, our main contributions are as follows:\n(e.g., Linear\n[Chen et al., 2023], MLP [Liu et al., 2024;"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "Chen et al., 2024a], or Q-Former [Li et al., 2023b]). While"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "• We construct\nthe AMT dataset.\nEach piece of data in"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "they\nexcel\nin\ngeneral\ntasks, MLLMs\nstruggle with MSA"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "this dataset contains queries for five tasks and the cor-"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "and MER due to insufficient\ntraining on sentiment-specific"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "responding labels.\nThe AMT dataset encourages\nthe"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "datasets and emotion-related knowledge. Recent efforts have"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "model to capture the relationship between sentiment and"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "focused on improving MLLMs by incorporating multimodal"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "emotion from the perspective of different tasks, enabling"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "emotion datasets and emotion reasoning tasks [Cheng et al.,"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "the model to extract richer contextual information."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "2024a; Lian et al., 2024]. However,\nthese models still\nface"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "• We\nexplore\nthe\nrelationships between different\nsenti-\nchallenges in leveraging contextual\ninformation effectively,"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "ment and emotion-related tasks within a multitask learn-\nparticularly in sentiment analysis, where understanding the"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "ing framework.\nThrough this exploration, we identify\nnuances is crucial."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "the optimal strategy, called M2SE. This exploration is"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "the first multitask investigation in the field of affective\n2.2\nMultitask Learning"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "computing."
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "Multitask Learning (MTL)\nis a machine learning paradigm"
        },
        {
          "Figure 1: Comparison of MLLMs trained on the MSA task, MER task, and using the M2SE strategy.\n(a) The MSA-trained model performs": "• We develop the EmoVerse model.\nEmoVerse unifies\nthat enables models to simultaneously learn multiple related"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "emotion recognition, emotion reason inference, facial expression recognition, and emotion cause-pair extraction."
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "tasks, thereby allowing knowledge from one task to be trans-\nFor\nthe MSA and MER tasks, we use the original senti-"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "ferred to others.\nIn affective computing, where multimodal\nment labels and emotion annotations from the source datasets"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "inputs (e.g.,\ntext, audio, and visual frames) carry diverse and\nas task labels. Due to the complexity of annotating the ECPE"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "distinct\ninformation, MTL frameworks aim to leverage the\ntask, we refer to the annotation method of the ECF2.0 dataset"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "synergies between tasks for improved performance. For in-\nand manually annotate a portion of the data. This is then com-"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "stance, UniMSE [Hu et al., 2022b] unifies MSA and MER\nbined with the existing ECF2.0 data to form the ECPE portion"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "within a single framework. However,\nit\nrelies solely on la-\nof the AMT dataset for further emotional causality analysis."
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "beled data\nto establish the\nrelationship between sentiment\nIn\nthe FER task, we\nadapt methods\nfrom the MERR"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "and emotion, without fully utilizing multimodal connections.\ndataset [Cheng et al., 2024a] by extracting peak frames based"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "M2Seq2Seq [Zhang et al., 2023] proposes a multimodal mul-\non the Action Units (AUs) calculated for each frame using the"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "titask learning model based on the encoder-decoder archi-\nOpenFace tool. The composite score for each frame is deter-"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "tecture.\nRecently, Emotion-LLaMA [Cheng et al., 2024a]\nmined by summing the AU values, as shown in the following"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "has shown promising results in MER by incorporating mul-\nequation:"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "titask learning, including emotion cause inference. However,"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "despite its success in MER, Emotion-LLaMA still struggles"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "n(cid:88) i\n(1)\nSf =\nAUi(f )"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "with MSA tasks and fails to effectively synthesize contextual"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "=1"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "information for emotion inference."
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "where Sf\nis the composite score of frame f , and AUi(f )"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "represents the Action Unit value for the i-th facial action unit"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "3\nAffective Multitask Dataset"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "at frame f . The peak frame with the highest score is selected"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "The AMT dataset\nis\nconstructed by integrating data\nfrom\nto represent\nthe most expressive moment. For videos featur-"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "the CMU-MOSEI, MELD, and ECF2.0 [Wang et al., 2024a]\ning multiple characters, such as MELD dataset, we improve"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "datasets\nto support five distinct\ntasks: MSA, MER, FER,\nthe method by extracting peak frames\nseparately for each"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "ERI, and ECPC. Figure 2 illustrates a sample from the AMT\ncharacter and then selecting the final peak frame by compar-"
        },
        {
          "Figure 2: The construction process of the AMT dataset. The AMT dataset\nincludes five tasks: multimodal sentiment analysis, multimodal": "dataset.\ning the highest composite scores from all characters:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: provides a detailed",
      "data": [
        {
          "where S(i)\nis the composite score for character i and k rep-": "f",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "mat, which includes three key elements: query, response, and"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "resents the total number of characters in the frame. The final",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "images/videos. This format facilitates the instruction tuning"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "peak frame is the one with the highest composite score among",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "all characters.",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "After selecting the peak frames, we extract\nthe facial ex-",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "Number of Entries"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "pression captions by combining the AUs from the peak frame",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "with the predefined AU list corresponding to the emotion. Let",
          "Additionally, we transform the dataset into a structured for-": "25859"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "AUframe(f ) represent the set of AUs from the peak frame for a",
          "Additionally, we transform the dataset into a structured for-": "25859"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "15870"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "given emotion, and AUemotion(e) represent the predefined AU",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "4839"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "list for a specific emotion e. The common AUs are obtained",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "7081"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "by taking the intersection of the two sets:",
          "Additionally, we transform the dataset into a structured for-": ""
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "",
          "Additionally, we transform the dataset into a structured for-": "32940"
        },
        {
          "where S(i)\nis the composite score for character i and k rep-": "(3)\nAUcommon(f, e) = AUframe(f ) ∩ AUemotion(e)",
          "Additionally, we transform the dataset into a structured for-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: provides a detailed",
      "data": [
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "based on their respective sampling rates. The EmoVerse model is then trained using the assigned data. The model architecture include visual",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "encoder, linear projector, and LLM.",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "is important to note that each data sample may be associated"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "(cid:16)\n(cid:17)",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "{S(1)\n, S(2)\n, . . . , S(k)\n}",
          "For different\ntraining stages, we assign tasks to the data": "with a varying number of tasks. Table 1 provides a detailed"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "(2)\nSffinal = max\nf\nf\nf",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "breakdown of the tasks available for each category."
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "where S(i)\nis the composite score for character i and k rep-",
          "For different\ntraining stages, we assign tasks to the data": "Additionally, we transform the dataset into a structured for-"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "f",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "mat, which includes three key elements: query, response, and"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "resents the total number of characters in the frame. The final",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "images/videos. This format facilitates the instruction tuning"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "peak frame is the one with the highest composite score among",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "of MLLMs."
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "all characters.",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "After selecting the peak frames, we extract\nthe facial ex-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Tasks\nNumber of Entries"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "pression captions by combining the AUs from the peak frame",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "with the predefined AU list corresponding to the emotion. Let",
          "For different\ntraining stages, we assign tasks to the data": "Multimodal Sentiment Analysis\n25859"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "AUframe(f ) represent the set of AUs from the peak frame for a",
          "For different\ntraining stages, we assign tasks to the data": "Multimodal Emotion Recognition\n25859"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Facial Expression Recognition\n15870"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "given emotion, and AUemotion(e) represent the predefined AU",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Emotion Reason Inference\n4839"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "list for a specific emotion e. The common AUs are obtained",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Emotion Cause-Pair Extraction\n7081"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "by taking the intersection of the two sets:",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Total\n32940"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "(3)\nAUcommon(f, e) = AUframe(f ) ∩ AUemotion(e)",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Table 1: The Number of Entrie in the AMT Dataset"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "where AUcommon(f, e) is the set of AUs common to both",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "the peak frame and the emotion-specific AU list. These com-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "mon AUs are then mapped to textual\ninformation to gener-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "4\nExploring the MLLM design space: How do"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "ate the FER annotations for the task. This approach enables",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "different tasks influence the performance"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "the model\nto link the facial expressions captured in the peak",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "In this section, we explore how different\ntasks and varying"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "frames with the corresponding emotional state.",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "sampling ratios for each task impact the final performance of"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "In the ERI task, we input the video into the Internvl2-8B 1",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "the MLLM. After\nidentifying the optimal\ntraining strategy,"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "model to generate detailed descriptions of both the characters",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "we introduce EmoVerse."
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "and the scene. These descriptions allow the model to analyze",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "Generally, the training process for MLLMs is divided into"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "emotional causes by considering facial expressions, speech,",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "two stages:\npretraining and instruction tuning.\nInspired by"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "and environmental context. Finally, we use the LLaMA 3.1-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "the human learning process, which progresses from simple to"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "8B 2 model\nto infer the underlying causes of the characters’",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "complex, we also divide the affective domain fine-tuning pro-"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "emotions by synthesizing the character and scene descrip-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "cess into these two stages, adopting a training approach that"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "tions.",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "goes from simple to difficult. We conduct experiments using"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "We\nconduct\na\ncomprehensive preprocessing and quality",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "EmoVerse-4B as well as the AMT dataset\n(refer\nto Section"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "control procedure on the raw data from the three datasets, sys-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "5.2)."
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "tematically removing problematic or noisy samples. In paral-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "In the AMT dataset,\nthe simplest\ntask is MSA, which re-"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "lel, we manually curate high-quality samples according to the",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "quires the model\nto perform binary classification. The more"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "methodology described above. The final refined dataset, de-",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "challenging tasks are MER and FER, while ERI and ECPE,"
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "tailed in Table 1, includes the number of tasks per category. It",
          "For different\ntraining stages, we assign tasks to the data": ""
        },
        {
          "Figure 3: Overview of\nthe M2SE training strategy and EmoVerse architecture.": "",
          "For different\ntraining stages, we assign tasks to the data": "which involve reasoning and inference, are the most difficult."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: , increasing the sampling rate for the",
      "data": [
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "ity.\nIn the second stage, we focus on training the model with",
          "MSA task.\nThis indicates the presence of catastrophic for-": "getting in MLLM multitask training. However, when we al-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "MER, ERI, and ECPE in a multitask learning setup, building",
          "MSA task.\nThis indicates the presence of catastrophic for-": "locate a small portion of the MSA task in the second phase,"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "on the model’s performance from the previous stage.",
          "MSA task.\nThis indicates the presence of catastrophic for-": "overall performance improves. This suggests that\nrevisiting"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "MSA-related knowledge helps the model deepen its under-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "4.1\nMultitask Pretraining",
          "MSA task.\nThis indicates the presence of catastrophic for-": "standing of the relationship between sentiment and emotion."
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "In the second phase,\nthe final\nsampling ratio we chose is"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "In this phase, we randomly select 15,000 samples from the",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "MER:ERI:ECPE:MSA = 3:3:3:1."
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "AMT dataset. The primary objective is to enable the model to",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "Therefore, the optimal training strategy we have identified"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "recognize facial expressions and,\nthrough these expressions,",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "involves using the MSA, FER, and MER tasks\nin the first"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "capture the relationship between sentiment and emotion. As",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "phase, and the MSA, MER, ERI, and ECPE tasks in the sec-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "discussed earlier, MSA is the simplest\ntask, and therefore,\nit",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "ond phase for multi-stage multitask training. We refer to this"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "is assigned the highest sampling ratio, while MER, being a",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "strategy as M2SE."
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "more complex task,\nreceives a lower sampling ratio.\nIn this",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "stage, we also investigate the impact of varying the sampling",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "4.3\nEmoVerse"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "ratio for the FER task on the model’s performance. To assess",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "the model’s performance during this phase, we use the CMU-",
          "MSA task.\nThis indicates the presence of catastrophic for-": "To validate the effectiveness of the M2SE strategy, we select"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "MOSEI test set.",
          "MSA task.\nThis indicates the presence of catastrophic for-": "the most widely used MLLM framework without introducing"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "any multitask modifications to its architecture. This ensures"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "that any observed performance improvements can be directly"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "Model\nAcc2",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "attributed to the proposed training strategy, rather than archi-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "EmoVerse (6:1:3)\n77.09 / 87.47",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "tectural changes.\nThe proposed MLLM architecture, Emo-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "EmoVerse (6:3:1)\n82.61 / 87.50",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "Verse, as shown in Figure 3, consists of a visual encoder, a"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "linear projector, and the LLM."
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "Table 2: Effect of the FER Task. MSA:FER:MER",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "We\ntrain EmoVerse\nfollowing M2SE strategy using the"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "AMT datasets.\nFor a data sample D,\nthe input consists of"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "the pair {v, t}, where v represents the video and t represents"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "As shown in Table 2,\nincreasing the sampling rate for the",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "the text. First, the video v is processed to extract visual tokens"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "FER task leads to improved performance on the MSA task.",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "using a Vision Transformer (ViT)[Dosovitskiy et al., 2021],"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "Consequently,\nfor\nthe first stage, we implement a multitask",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "producing a set of visual\ntokens Tv. These visual\ntokens Tv"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "training setup with a sampling ratio of MSA:FER:MER =",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "are then passed through a linear transformation Wv to align"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "6:3:1. This configuration enables the model\nto not only un-",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "the visual features with the feature space of the LLM. Math-"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "derstand the relationship between sentiment and emotion but",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "",
          "MSA task.\nThis indicates the presence of catastrophic for-": "ematically, this can be expressed as:"
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "also accurately recognize facial action units (AUs),\nthereby",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        },
        {
          "MSA receiving the highest sampling ratio due to its simplic-": "enhancing MSA performance.",
          "MSA task.\nThis indicates the presence of catastrophic for-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: , increasing the sampling rate for the",
      "data": [
        {
          "and gradually introduce additional\ntasks to examine the im-": ""
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "pact of training with different\ntasks. The results are summa-"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": ""
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "rized in Tabel 3."
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": ""
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "CMU-MOSEI\nMELD\nECF2.0"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "Model"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "Acc2\nAcc\nW.F1\nF1\nW.F1"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "EmoVerse (First stage)\n82.61 / 87.50\n-\n-\n-\n-"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "EmoVerse (MER)\n81.68 / 87.20\n66.74\n66.24\n25.40\n26.69"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "EmoVerse (MER,ERI,EPCE)\n79.00 / 86.78\n66.97\n65.67\n69.85\n69.95"
        },
        {
          "and gradually introduce additional\ntasks to examine the im-": "67.78\n66.74\n72.30\n72.21\nEmoVerse (MER,ERI,EPCE,MSA)\n85.51 / 87.80"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ConKI": "ConFEDE",
          "82.73": "81.65",
          "86.25": "85.82",
          "-": "-"
        },
        {
          "ConKI": "PMR",
          "82.73": "-",
          "86.25": "83.30",
          "-": "-"
        },
        {
          "ConKI": "MRC-D3AE",
          "82.73": "83.10",
          "86.25": "85.50",
          "-": "-"
        },
        {
          "ConKI": "DialogueGCN",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "DialogueCRN",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "DAG-ERC",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "MM-DFN",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "EmoCaps",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "DisGCN",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "MMGCN",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "GA2MIF",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "UniMSE",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "FacialMMT",
          "82.73": "-",
          "86.25": "-",
          "-": "-"
        },
        {
          "ConKI": "LLaVA-Video",
          "82.73": "-",
          "86.25": "82.82",
          "-": "22.78"
        },
        {
          "ConKI": "InternVL2-4B",
          "82.73": "-",
          "86.25": "82.09",
          "-": "33.35"
        },
        {
          "ConKI": "InternVL2-8B",
          "82.73": "-",
          "86.25": "85.30",
          "-": "40.51"
        },
        {
          "ConKI": "EmoVerse-4B",
          "82.73": "85.51",
          "86.25": "87.80",
          "-": "72.21"
        },
        {
          "ConKI": "EmoVerse-8B",
          "82.73": "85.93",
          "86.25": "88.51",
          "-": "73.62"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        },
        {
          "CMU-MOSEI": "",
          "MELD": "",
          "ECF2.0": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "Table 4: Comparison of EmoVerse with other SOTA methods across different tasks."
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "Sentiment Analysis",
          "67.78\n66.74\n73.54\n73.62": "and evaluation sets, with 1,374 conversations and 13,619 ut-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "We use\nthe CMU-MOSEI datasets\nfor MSA. The CMU-",
          "67.78\n66.74\n73.54\n73.62": "terances in the training set, and 341 conversations and 3,101"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "MOSEI dataset\nis an extended version of\nthe CMU-MOSI",
          "67.78\n66.74\n73.54\n73.62": "utterances in the test set. We refer to previous evaluations and"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "dataset.\nIt encompasses 3228 videos\nfrom 1000 speakers,",
          "67.78\n66.74\n73.54\n73.62": "assess the ECF2.0 test set using the F1 Score and Weighted"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "comprising a total of 23453 sentences. Similar to the CMU-",
          "67.78\n66.74\n73.54\n73.62": "F1 Score."
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "MOSI dataset,\nit covers multiple emotional dimensions, em-",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "5.2\nImplementation Details"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "ploying the same annotation labeling approach. The results",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "for Acc-2 calculated based on negative/non-negative (N/N)",
          "67.78\n66.74\n73.54\n73.62": "For the visual encoder, we use a pre-trained ViT with input"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "scheme, while the results on the right are calculated accord-",
          "67.78\n66.74\n73.54\n73.62": "images resized to 448 × 448 pixels. During fine-tuning,\nthe"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "ing to negative/positive (N/P) definition.",
          "67.78\n66.74\n73.54\n73.62": "visual backbone is\nfrozen, and training focuses on the lin-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "ear projection layer. For the LLM, EmoVerse-4B uses Phi-3-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "Emotion Recognition",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "Mini [Abdin et al., 2024], and EmoVerse-8B uses InternLM-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "We use the MELD datasets for ER. The MELD dataset\nis an",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "2.5-7B-Chat3, both augmented with LoRA [Hu et al., 2021b]"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "emotional dialogue dataset comprising 1433 dialogue seg-",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "for parameter-efficient fine-tuning."
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "ments\nand 13708 utterances\nfrom movies\nand TV shows.",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "Training is done for 2 epochs with LoRA hyperparameters"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "Each statement\nin the dialogue is labeled with one of seven",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "r = 8 and a = 32, a learning rate of 1 × 10−5, and a warm-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "emotions: anger, disgust, sadness,\njoy, neutral, surprise, and",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "up cosine decay schedule.\nEmoVerse-4B training takes ap-"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "fear. Furthermore,\nthe MELD data set provides annotations",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "proximately 48 hours on 2 NVIDIA RTX 3090 GPUs, while"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "for emotional polarity (positive, negative,\nand neutral)\nfor",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "EmoVerse-8B requires about 55 hours on a single NVIDIA"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "each utterance. We use Acc and weighted f1-sorce for evalu-",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "A100 GPU."
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "ation.",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "Emotion Cause-pair Extraction",
          "67.78\n66.74\n73.54\n73.62": "5.3\nResults"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "We use the ECF2.0 datasets for ECPE. The ECF2.0 dataset",
          "67.78\n66.74\n73.54\n73.62": "Performance comparison"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "is a comprehensive resource for emotion cause-pair extrac-",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "For MSA task, we compare our EmoVerse model with several"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "tion, comprising 1,715 conversations and 16,720 utterances.",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "SOTA baselines,\nincluding MulT [Tsai et al., 2019], PMR"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "Each utterance in the dataset is annotated with emotion cause-",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "",
          "67.78\n66.74\n73.54\n73.62": "[Lv et al., 2021], MISA [Hazarika et al., 2020], MMIM [Han"
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "pair information, which helps in understanding the emotional",
          "67.78\n66.74\n73.54\n73.62": ""
        },
        {
          "85.93\n88.51\nEmoVerse-8B": "context and its causes.\nThe dataset\nis divided into training",
          "67.78\n66.74\n73.54\n73.62": "3https://github.com/InternLM/InternLM"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 5: lists the fine-tuning",
      "data": [
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "2023a], UniMSE [Hu et al., 2022b], and FacialMMT [Zheng",
          "cause. In contrast, EmoVerse effectively combines video and": "text to accurately infer the emotional cause, providing a com-"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "et al., 2023].",
          "cause. In contrast, EmoVerse effectively combines video and": "prehensive interpretation."
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "Additionally,\nfor both MSA and MER, we\nselect mod-",
          "cause. In contrast, EmoVerse effectively combines video and": ""
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "els capable of processing both images and videos,\nincluding",
          "cause. In contrast, EmoVerse effectively combines video and": "CMU-MOSEI\nMELD\nECF2.0"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "",
          "cause. In contrast, EmoVerse effectively combines video and": "Model"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "LLaVA-Video[Zhang et al., 2024b] and InternVL2[Chen et",
          "cause. In contrast, EmoVerse effectively combines video and": ""
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "",
          "cause. In contrast, EmoVerse effectively combines video and": "Acc2\nAcc\nW.F1\nF1\nW.F1"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "al., 2024b].\nFor\nthe ECPE task, we conduct\ntests solely on",
          "cause. In contrast, EmoVerse effectively combines video and": ""
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "",
          "cause. In contrast, EmoVerse effectively combines video and": "EmoVerse(Zero-shot)\n82.09 / 83.87\n39.46\n40.59\n33.24\n33.35"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "MLLMs, as they are better equipped to handle the complexity",
          "cause. In contrast, EmoVerse effectively combines video and": "EmoVerse(MSA)\n83.96 / 88.16\n48.12\n31.27\n41.67\n41.14"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "and multimodal nature of the task. The performance compar-",
          "cause. In contrast, EmoVerse effectively combines video and": "EmoVerse(MER)\n78.99 / 80.17\n66.21\n65.71\n40.61\n38.23"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "",
          "cause. In contrast, EmoVerse effectively combines video and": "72.69\n72.59\nEmoVerse(EPCE)\n83.27 / 85.85\n32.38\n33.43"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "ison results, shown in Table 4 clearly demonstrate that Emo-",
          "cause. In contrast, EmoVerse effectively combines video and": ""
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "Verse achieves SOTA performance across both tasks, outper-",
          "cause. In contrast, EmoVerse effectively combines video and": "67.78\n66.74\nEmoVerse(M2SE)\n85.51 / 87.80\n72.30\n72.21"
        },
        {
          "al., 2021], MMGCN [Hu et al., 2021c], GA2MIF [Li et al.,": "forming all the aforementioned baselines.",
          "cause. In contrast, EmoVerse effectively combines video and": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: lists the fine-tuning",
      "data": [
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "the reasons behind the emotions are incomplete.\nIn contrast,"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "GPT-4o and Video-LLaMA2 misclassify the emotion as"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "”confident”\nor\n”assertive,” while LLaMA-Video\nand\nIn-"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "ternVL2 correctly identify it as ”surprise.” However,\nthese"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "models\nfail\nto\nfully\nintegrate multimodal\ninformation\nfor"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "emotion reasoning. LLaMA-Video relies solely on facial ex-"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "pressions, while InternVL2 combines\nfacial cues with tex-"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "tual analysis but\nlacks detailed explanation of the emotional"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "cause. In contrast, EmoVerse effectively combines video and"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "text to accurately infer the emotional cause, providing a com-"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "prehensive interpretation."
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "CMU-MOSEI\nMELD\nECF2.0"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "Model"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "Acc2\nAcc\nW.F1\nF1\nW.F1"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "EmoVerse(Zero-shot)\n82.09 / 83.87\n39.46\n40.59\n33.24\n33.35"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "EmoVerse(MSA)\n83.96 / 88.16\n48.12\n31.27\n41.67\n41.14"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "EmoVerse(MER)\n78.99 / 80.17\n66.21\n65.71\n40.61\n38.23"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "72.69\n72.59\nEmoVerse(EPCE)\n83.27 / 85.85\n32.38\n33.43"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "67.78\n66.74\nEmoVerse(M2SE)\n85.51 / 87.80\n72.30\n72.21"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "Table 5: Comparison of the M2SE Strategy with Single-Task Fine-"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "Tuning"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "5.4\nComparison of the M2SE Strategy with"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "Single-Task Fine-Tuning"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": ""
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "To assess the impact of\nthe M2SE strategy within the same"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "framework and compare\nits performance\nacross\ntasks, we"
        },
        {
          "Figure 4: Comparison of different models on ERI. It can be seen that both GPT-4o and Video-LLaMA2 make errors in MER. While Llava-": "conduct\nan\nablation\nstudy.\nTable\n5\nlists\nthe fine-tuning"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tasks (in parentheses) with their corresponding datasets. The": "M2SE strategy uses the AMT dataset\nfor\ntraining.\nResults",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Yan\nHe,\nJingdong\nSun,\nKai Wang,\nYuxiang\nLin,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "show that while some metrics may fall short of single-task",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Zheng\nLian,\nXiaojiang\nPeng,\nand Alexander Haupt-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "learning, EmoVerse with M2SE outperforms other models",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "mann.\nEmotion-llama: Multimodal emotion recognition"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "overall.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv preprint\nand reasoning with instruction tuning."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv:2406.11161, 2024."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "6\nConclusion",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Cheng et al., 2024b] Zesen Cheng,\nSicong\nLeng,\nHang"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "In this work, we explore the impact of different affective tasks",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "on MLLM under a multistage and multitask training frame-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "work,\nidentifying and validating the optimal strategy called",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Bing. Videollama 2: Advancing spatial-temporal model-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "M2SE. Using the M2SE strategy, we train EmoVerse,\nad-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "ing and audio understanding in video-llms. arXiv preprint"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "dressing the lack of a general-purpose MLLM in affective",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv:2406.07476, 2024."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "computing. Additionally, we construct the AMT dataset, de-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "signed to support sentiment and emotion perception training",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "for MLLMs.\nExperimental\nresults demonstrate that Emo-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Thomas Unterthiner, Mostafa Dehghani, Matthias Min-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Verse achieves state-of-the-art performance across multiple",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "derer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "tasks.\nThis study highlights the effectiveness of\nthe M2SE",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Neil Houlsby.\nAn image is worth 16x16 words: Trans-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "strategy in unifying sentiment and emotion tasks, paving the",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "formers for image recognition at scale, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "way for more emotionally intelligent AI systems.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Ghosal et al., 2019] Deepanway\nGhosal,\nNavonil\nMa-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "jumder, Soujanya Poria, Niyati Chhaya,\nand Alexander"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Acknowledgments",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Gelbukh.\nDialoguegcn:\nA graph convolutional neural"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "This work was supported by the Key R&D Program of Shan-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv\nnetwork for emotion recognition in conversation."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "dong Province, China (Major Scientific and Technological In-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "preprint arXiv:1908.11540, 2019."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "novation Project) (NO.2022CXGC010504) and National Nat-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Han et al., 2021a] Wei Han, Hui Chen, Alexander Gelbukh,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "ural Science Foundation of China under Grant 61301253.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Amir Zadeh, Louis-philippe Morency, and Soujanya Po-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "ria. Bi-bimodal modality fusion for correlation-controlled"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "References",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "multimodal sentiment analysis. In Proceedings of the 2021"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "[Abdin et al., 2024] Marah\nAbdin,\nJyoti\nAneja,\nHany",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "international conference on multimodal interaction, pages"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Awadalla, Ahmed Awadallah, Ammar Ahmad Awan,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "6–15, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Nguyen Bach, Amit Bahree, Arash Bakhtiari,\nJianmin",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Han et al., 2021b] Wei Han, Hui Chen, and Soujanya Poria."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Bao, Harkirat Behl, et al. Phi-3 technical report: A highly",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Improving multimodal fusion with hierarchical mutual in-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "arXiv\ncapable language model\nlocally on your phone.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "formation maximization for multimodal sentiment analy-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "preprint arXiv:2404.14219, 2024.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "sis. arXiv preprint arXiv:2109.00412, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "[Chen et al., 2023]\nJun Chen, Deyao Zhu, Xiaoqian Shen,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Hazarika et al., 2020] Devamanyu Hazarika, Roger Zim-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "mermann, and Soujanya Poria. Misa: Modality-invariant"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mo-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "and-specific\nrepresentations\nfor multimodal\nsentiment"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "hamed Elhoseiny. Minigpt-v2:\nlarge language model as a",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "the 28th ACM international\nanalysis.\nIn Proceedings of"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "unified interface for vision-language multi-task learning.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "conference on multimedia, pages 1122–1131, 2020."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "arXiv preprint arXiv:2310.09478, 2023.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Hsu et al., 2021] Wei-Ning Hsu,\nBenjamin\nBolte,\nYao-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "[Chen et al., 2024a] Zhe Chen, Weiyun Wang, Hao Tian,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhut-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "dinov,\nand Abdelrahman Mohamed.\nHubert:\nSelf-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Kongzhi Hu,\nJiapeng Luo, Zheng Ma,\net al.\nHow far",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "supervised speech representation learning by masked pre-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "are we to gpt-4v?\nclosing the gap to commercial mul-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "diction of hidden units. IEEE/ACM transactions on audio,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "arXiv preprint\ntimodal models with open-source suites.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "speech, and language processing, 29:3451–3460, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "arXiv:2404.16821, 2024.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Hu et al., 2021a] Dou Hu,\nLingwei Wei,\nand Xiaoyong"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "[Chen et al., 2024b] Zhe Chen, Jiannan Wu, Wenhai Wang,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Huai.\nDialoguecrn: Contextual\nreasoning networks\nfor"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qing-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv\npreprint\nemotion\nrecognition\nin\nconversations."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "long Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "arXiv:2106.01978, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Tong Lu, Yu Qiao, and Jifeng Dai.\nInternvl: Scaling up",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "vision foundation models and aligning for generic visual-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Hu et al., 2021b] Edward J Hu, Yelong Shen, Phillip Wallis,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "linguistic tasks, 2024.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "and Weizhu Chen.\nLora: Low-rank adaptation of\nlarge"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "[Cheng et al., 2023] Zebang Cheng, Yuxiang Lin, Zhaoru",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "language models. arXiv preprint arXiv:2106.09685, 2021."
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Chen, Xiang Li, Shuyi Mao, Fan Zhang, Daijun Ding,",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": ""
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Bowen Zhang, and Xiaojiang Peng. Semi-supervised mul-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "[Hu et al., 2021c]\nJingwen Hu, Yuchen Liu, Jinming Zhao,"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "timodal emotion recognition with expression mae.\nIn Pro-",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "and Qin Jin. Mmgcn: Multimodal fusion via deep graph"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "ceedings of\nthe 31st ACM International Conference on",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "convolution network for emotion recognition in conversa-"
        },
        {
          "tasks (in parentheses) with their corresponding datasets. The": "Multimedia, pages 9436–9440, 2023.",
          "[Cheng et al., 2024a] Zebang Cheng, Zhi-Qi Cheng,\nJun-": "tion. arXiv preprint arXiv:2107.06779, 2021."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Lianxin Jiang, and Yang Mo. Mm-dfn: Multimodal dy-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "arXiv preprint\nfor emotion recognition in conversations."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "namic\nfusion network for\nemotion recognition in con-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "arXiv:1810.02508, 2018."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "versations.\nIn ICASSP 2022-2022 IEEE International",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Shen et al., 2021] Weizhou Shen, Siyue Wu, Yunyi Yang,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Conference on Acoustics, Speech and Signal Processing",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "and Xiaojun Quan.\nDirected\nacyclic\ngraph\nnetwork"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "(ICASSP), pages 7037–7041. IEEE, 2022.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "arXiv preprint\nfor conversational emotion recognition."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Hu et al., 2022b] Guimin\nHu,\nTing-En\nLin,\nYi\nZhao,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "arXiv:2105.12907, 2021."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Guangming Lu, Yuchuan Wu, and Yongbin Li. Unimse:",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Sun et al., 2021] Yang Sun, Nan Yu, and Guohong Fu. A"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Towards unified multimodal sentiment analysis and emo-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "discourse-aware graph neural network for emotion recog-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "tion recognition. arXiv preprint arXiv:2211.11256, 2022.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "the As-\nnition in multi-party conversation.\nIn Findings of"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Jiang et al., 2020] Xingxun\nJiang, Yuan Zong, Wenming",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "sociation for Computational Linguistics: EMNLP 2021,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, and",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "pages 2949–2958, 2021."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Jiateng Liu. Dfew: A large-scale database for recognizing",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Tsai et al., 2019] Yao-Hung\nHubert\nTsai,\nShaojie\nBai,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "dynamic facial expressions in the wild.\nIn Proceedings",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "of\nthe 28th ACM international conference on multimedia,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Ruslan Salakhutdinov.\nMultimodal\ntransformer\nfor un-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "pages 2881–2889, 2020.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "aligned multimodal\nlanguage sequences.\nIn Proceedings"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Lei et al., 2023] Shanglin Lei, Guanting Dong, Xiaoping",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "of\nthe conference. Association for computational\nlinguis-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Wang, Keheng Wang,\nand\nSirui Wang.\nInstructerc:",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "tics. Meeting, volume 2019, page 6558. NIH Public Ac-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Reforming\nemotion\nrecognition\nin\nconversation with\na",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "cess, 2019."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "arXiv\npreprint\nretrieval multi-task\nllms\nframework.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Wang et al., 2024a] Fanfan Wang, Heqing Ma, Rui Xia,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "arXiv:2309.11911, 2023.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Jianfei Yu,\nand Erik Cambria.\nSemEval-2024 task 3:"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Li et al., 2022] Zaijing Li, Fengxiao Tang, Ming Zhao, and",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Multimodal emotion cause analysis in conversations.\nIn"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Yusen Zhu.\nEmocaps:\nEmotion capsule based model",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Atul Kr. Ojha, A. Seza Do˘gru¨oz, Harish Tayyar Mad-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "arXiv preprint\nfor conversational emotion recognition.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "abushi, Giovanni Da San Martino, Sara Rosenthal,\nand"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "arXiv:2203.13504, 2022.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Aiala Ros´a, editors, Proceedings of the 18th International"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Li et al., 2023a]\nJiang Li, Xiaoping Wang, Guoqing Lv, and",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Workshop on Semantic Evaluation (SemEval-2024), pages"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Zhigang Zeng. Ga2mif:\ngraph and attention based two-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "2039–2050, Mexico City, Mexico, June 2024. Association"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "stage multi-source information fusion for conversational",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "for Computational Linguistics."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "IEEE Transactions on affective com-\nemotion detection.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Wang et al., 2024b] Yuanzhi Wang, Yong Li, and Zhen Cui."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "puting, 15(1):130–143, 2023.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Incomplete multimodality-diffused emotion recognition."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Li et al., 2023b]\nJunnan Li, Dongxu Li,\nSilvio Savarese,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Advances in Neural Information Processing Systems, 36,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "and Steven Hoi.\nBlip-2: Bootstrapping language-image",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "2024."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "pre-training with frozen image\nencoders\nand large\nlan-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Yang et al., 2023]\nJiuding Yang, Yakun Yu, Di Niu, Wei-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "guage models.\nIn International conference on machine",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "dong Guo, and Yu Xu. Confede: Contrastive feature de-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "learning, pages 19730–19742. PMLR, 2023.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "composition for multimodal sentiment analysis.\nIn Pro-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Li et al., 2023c] Yong Li, Yuanzhi Wang,\nand Zhen Cui.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "ceedings of\nthe 61st Annual Meeting of\nthe Association"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Decoupled multimodal distilling for emotion recognition.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "In Proceedings of the IEEE/CVF Conference on Computer",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "pages 7617–7630, 2023."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Vision and Pattern Recognition, pages 6631–6640, 2023.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Yu et al., 2021] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Lian et al., 2024] Zheng Lian, Haiyang Sun, Licai Sun, Hao",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Wu. Learning modality-specific representations with self-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "supervised multi-task learning for multimodal\nsentiment"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Xu, Ke Xu, Kang Chen, Lan Chen, Shan Liang, Ya Li,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "analysis.\nIn Proceedings of the AAAI conference on artifi-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Jiangyan Yi, Bin Liu, and Jianhua Tao. Explainable mul-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "cial intelligence, volume 35, pages 10790–10797, 2021."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "timodal emotion recognition, 2024.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Yu et al., 2023] Yakun Yu, Mingjun\nZhao,\nShi-ang Qi,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Liu et al., 2024] Haotian Liu, Chunyuan Li, Qingyang Wu,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Feiran Sun, Baoxun Wang, Weidong Guo, Xiaoli Wang,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "and Yong Jae Lee. Visual instruction tuning. Advances in",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Lei Yang, and Di Niu. Conki: Contrastive knowledge in-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "neural information processing systems, 36, 2024.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "jection for multimodal sentiment analysis. arXiv preprint"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Lv et al., 2021] Fengmao Lv, Xiang Chen, Yanyong Huang,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "arXiv:2306.15796, 2023."
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Lixin Duan, and Guosheng Lin. Progressive modality re-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "[Zadeh et al., 2018] AmirAli Bagher Zadeh, Paul Pu Liang,"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "inforcement\nfor human multimodal emotion recognition",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Soujanya\nPoria,\nErik\nCambria,\nand\nLouis-Philippe"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "from unaligned multimodal sequences.\nIn Proceedings of",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Morency. Multimodal language analysis in the wild: Cmu-"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "the IEEE/CVF Conference on Computer Vision and Pat-",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "mosei dataset and interpretable dynamic fusion graph.\nIn"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "tern Recognition, pages 2554–2562, 2021.",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": ""
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "Proceedings of the 56th Annual Meeting of the Association"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "[Poria et al., 2018] Soujanya Poria, Devamanyu Hazarika,",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
          "[Hu et al., 2022a] Dou Hu, Xiaolong Hou, Lingwei Wei,": "Navonil Majumder, Gautam Naik, Erik Cambria,\nand",
          "Rada Mihalcea. Meld: A multimodal multi-party dataset": "pages 2236–2246, 2018."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Liu, Lu Rong, Qian Zheng, Dawei Song, Prayag Tiwari,"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "and Jing Qin. A multitask learning model for multimodal"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "sarcasm, sentiment and emotion recognition in conversa-"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "tions.\nInf. Fusion, 93(C):282–301, May 2023."
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "[Zhang et al., 2024a] Hua Zhang, Yongjian Yan, Zijing Cai,"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Peiqian Zhan, Bi Chen, Bo Jiang, and Bo Xie.\nRecon-"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "structing representations using diffusion models for multi-"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "modal sentiment analysis through reading comprehension."
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Applied Soft Computing, 167:112346, 2024."
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "[Zhang et al., 2024b] Yuanhan Zhang, Jinming Wu, Wei Li,"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li.\nVideo"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "arXiv preprint\ninstruction tuning with synthetic data."
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "arXiv:2410.02713, 2024."
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "[Zheng et al., 2023] Wenjie Zheng, Jianfei Yu, Rui Xia, and"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Shijin Wang. A facial expression-aware multimodal multi-"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "task learning framework for emotion recognition in multi-"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "the 61st Annual\nparty conversations.\nIn Proceedings of"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "Meeting of\nthe Association for Computational Linguistics"
        },
        {
          "[Zhang et al., 2023] Yazhou Zhang, Jinglin Wang, Yaochen": "(Volume 1: Long Papers), pages 15445–15459, 2023."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone",
      "authors": [
        "Abdin"
      ],
      "year": "2024",
      "venue": "Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone",
      "arxiv": "arXiv:2404.14219"
    },
    {
      "citation_id": "2",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "year": "2023",
      "venue": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "3",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks",
      "year": "2024",
      "venue": "Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks"
    },
    {
      "citation_id": "4",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Cheng"
      ],
      "year": "2023",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "5",
      "title": "Advancing spatial-temporal modeling and audio understanding in video-llms",
      "authors": [
        "Cheng"
      ],
      "year": "2024",
      "venue": "Advancing spatial-temporal modeling and audio understanding in video-llms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Dosovitskiy"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "7",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "Han"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 international conference on multimodal interaction"
    },
    {
      "citation_id": "8",
      "title": "Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Han"
      ],
      "year": "2020",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2109.00412"
    },
    {
      "citation_id": "9",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "10",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Hu"
      ],
      "year": "2020",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "11",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Lei"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "12",
      "title": "Ga2mif: graph and attention based twostage multi-source information fusion for conversational emotion detection",
      "authors": [
        "Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "13",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "15",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Shen"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "16",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Sun"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "17",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Tsai"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "18",
      "title": "SemEval-2024 task 3: Multimodal emotion cause analysis in conversations",
      "authors": [
        "Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)"
    },
    {
      "citation_id": "19",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Wang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Confede: Contrastive feature decomposition for multimodal sentiment analysis",
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Learning modality-specific representations with selfsupervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "Yu"
      ],
      "year": "2021",
      "venue": "Contrastive knowledge injection for multimodal sentiment analysis",
      "arxiv": "arXiv:2306.15796"
    },
    {
      "citation_id": "22",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Zadeh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Reconstructing representations using diffusion models for multimodal sentiment analysis through reading comprehension",
      "authors": [
        "Zhang"
      ],
      "year": "2023",
      "venue": "Video instruction tuning with synthetic data",
      "arxiv": "arXiv:2410.02713"
    },
    {
      "citation_id": "24",
      "title": "A facial expression-aware multimodal multitask learning framework for emotion recognition in multiparty conversations",
      "authors": [
        "Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}