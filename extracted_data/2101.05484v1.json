{
  "paper_id": "2101.05484v1",
  "title": "4D Attention-Based Neural Network For Eeg Emotion Recognition",
  "published": "2021-01-14T07:41:48Z",
  "authors": [
    "Guowen Xiao",
    "Mengwen Ye",
    "Bowen Xu",
    "Zhendi Chen",
    "Quansheng Ren"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "attention mechanism",
    "convolutional recurrent neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalograph (EEG) emotion recognition is a significant task in the brain-computer interface field. Although many deep learning methods are proposed recently, it is still challenging to make full use of the information contained in different domains of EEG signals. In this paper, we present a novel method, called four-dimensional attention-based neural network (4D-aNN) for EEG emotion recognition. First, raw EEG signals are transformed into 4D spatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts spectral and spatial attention mechanisms to adaptively assign the weights of different brain regions and frequency bands, and a convolutional neural network (CNN) is utilized to deal with the spectral and spatial information of the 4D representations. Moreover, a temporal attention mechanism is integrated into a bidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of the 4D representations. Our model achieves state-of-the-art performance on the SEED dataset under intra-subject splitting. The experimental results have shown the effectiveness of the attention mechanisms in different domains for EEG emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays an important role in daily life and is closely related to human behavior and cognition  (Dolan 2002) . As one of the most significant research topics of affective computing, emotion recognition has received increasing attention in recent years for its applications of disease detection  (Bamdad et al. 2015; Figueiredo et al. 2019) , human-computer interaction  (Fiorinia et al. 2020; Katsigiannis and Ramzan 2017) , and workload estimation  (Blankertz et al. 2016) . In general, emotion recognition methods can be divided into two categories  (MÃ¼hl et al. 2014) . One is based on external emotion responses including facial expressions and gestures  (Yan et al. 2016) , and the other is based on internal emotion responses including electroencephalograph (EEG) and electrocardiography (ECG)  (Zheng et al. 2017) . Neuroscientific researches have shown that some major brain cortex regions are closely related to emotions, making it possible to decode emotions based on EEG  (Brittona et al. 2006; Lotfia and Akbarzadeh-T 2014) . EEG is non-invasive, portable, and inexpensive so that it has been widely used in the field of brain-computer interfaces (BCIs)  (Pfurtscheller et al. 2010) . Besides, EEG signals contain various spatial, spectral, and temporal information about emotions evoked by specific stimulation patterns. Therefore, more and more researchers concentrate on EEG emotion recognition recently  (Alhagry et al. 2017; Li and Lu 2009) .\n\nTraditional EEG emotion recognition methods usually extract hand-crafted features from EEG signals first and then adopt shallow models to classify the emotion features. EEG emotion features can be extracted from the time domain, frequency domain, and time-frequency domain. Jenke et al. conduct a comprehensive survey on EEG feature extraction methods by using machine learning techniques on a selfrecorded dataset  (Jenke et al. 2014) . For classifying the extracted emotion features, many researchers have adopted machine learning methods over the past few years  (Kim et al. 2013) . Li et al. apply a linear support vector machine (SVM) to classify emotion features extracted from the gamma frequency band  (Li and Lu 2009) .  Duan et al. extract differential entropy (DE)  features, which are superior to representing emotion states in EEG signals  (Shi et al. 2013) , from multichannel EEG data and combine a k-Nearest Neighbor (KNN) with SVM to classify the DE features  (Duan et al. 2013 ). However, shallow models require lots of expert knowledge to design and select emotion features, limiting their performance on EEG emotion classification.\n\nDeep learning methods have been demonstrated to outperform traditional machine learning methods in many fields such as computer vision, natural language processing, and biomedical signal processing  (Abbass et al. 2018; Craik et al. 2019)  for the ability to learn high-level features from data automatically  (Krizhevsky et al. 2012)    (Li et al. 2020) . All those deep learning methods outperform the shallow models.\n\nAlthough deep learning emotion recognition models have achieved higher accuracy than shallow models, it is still challenging to fuse more important information on different domains and capture discriminative local patterns in EEG signals. In the past decades, many researchers have investigated the critical frequency bands and channels for EEG emotion recognition. Zheng et al. demonstrate that Î²[14~31 Hz] and Î³[31~51 Hz] bands are more related to emotion recognition than other bands, and their model achieves the best performance when combining all frequency bands. They also conduct experiments to select critical channels and propose the minimum pools of electrode sets for emotion recognition  (Zheng and Lu 2015) . To utilize the spatial information of EEG signals,  Li et al.  propose a 2D sparse map to maintain the information hidden in the electrode placement  (Li et al. 2018 ). Zhong et al. introduce a regularized graph neural network (RGNN) to capture both local and global relations among different EEG channels for emotion recognition  (Zhong et al. 2020) . The temporal dependencies in EEG signals are also important to emotion recognition. For example,  Ma et al. (Jiaxin Ma et al. 2019)  apply LSTMs in their models to extract temporal features for emotion recognition.  Shen et al. transform  the DE features of different channels into 4D structures to integrate the spectral, spatial, and temporal information simultaneously and then use a fourdimensional convolutional recurrent neural network (4D-CRNN) to recognize different emotions  (Shen et al. 2020 ). However, the differences among brain regions and frequency bands are not fully utilized in their work. To adaptively capture discriminative patterns in EEG signals, attention mechanisms have been applied to EEG emotion recognition. For instance, Tao et al. introduce a channel-wise attention mechanism, assigning the weights of different channels adaptively, along with an extended self-attention to explore the temporal dependencies of EEG signals  (Tao et al. 2020) .  Jia et al.  propose a two-stream network with attention mechanisms to adaptively focus on important patterns  (Jia et al. 2020 ). From the above, it can be observed that it is critical to integrate information on different domains and adaptively capture important brain regions, frequency bands, and timestamps in a unified network for EEG emotion recognition.\n\nIn this paper, we propose a four-dimensional attentionbased neural network named 4D-aNN for EEG emotion recognition. First, we transform raw EEG signals into 4D spatial-spectral-temporal representations which consist of several temporal slices. Different brain regions and frequency bands vary in the contributions to EEG emotion recognition, and the temporal dependencies of 4D representations should also be considered. Therefore, we employ attention mechanisms on both a CNN and a bidirectional LSTM network to adaptively capture discriminative patterns. For the CNN model, the attention mechanism is applied to the spatial and spectral dimensions of each temporal slice so that the important brain regions and frequency bands could be captured. As for the bidirectional LSTM model, the attention mechanism is applied to utilize long-range temporal dependencies so that the importance of different temporal slices in one 4D representation could be fully explored.\n\nThe primary contribution of this paper are summarized as follows: a) We propose a four-dimensional attention-based neural network, which fuses information on different domains and captures discriminative patterns in EEG signals based on the 4D spatial-spectral-temporal representation. b) We conduct experiments on the SEED dataset, and the experimental results indicate that our model achieves state-ofthe-art performance under intra-subject splitting.\n\nThe remainder of this paper is organized as follows. We describe our proposed method in the Method section. Dataset, experiment settings, results, ablation studies, and discussion are presented in the Experiment section. Finally, conclusions are given in the Conclusion section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Figure  1  illustrates the overall structure of 4D-aNN for EEG emotion recognition. It consists of the 4D spatial-spectraltemporal representation, the attention-based CNN, the attention-based bidirectional LSTM, and the classifier. We will describe the details of each part in sequence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "4D Spatial-Spectral-Temporal Representation",
      "text": "The process of generating 4D representation is depicted in PSD is defined as\n\nwhere ğ‘¥ is formally a random variable and in this context, the signal acquired from a certain frequency band on a certain EEG channel. DE feature is capable of discriminating EEG patterns between low and high frequency energy, which is defined as\n\nwhere ğ‘“(ğ‘¥) is the probability density function of ğ‘¥ . If ğ‘¥ obeys the Gaussian distribution ğ‘(ğœ‡, ğœ 2 ), DE can simply be calculated by the following formulation:\n\nwhere ğ‘’ and ğœ are Euler's constant and standard deviation of ğ‘¥, respectively. Thus, We extract a 3D feature tensor ğ¹ ğ‘› ïƒ ğ‘… ğ‘ï‚´2ğ‘“ï‚´2ğ‘‡ , ğ‘› = 1, 2, . . . , ğ‘ from each segment, where ğ‘ is the number of total segments, ğ‘ is the number of EEG channels, 2ğ‘“ represents DE and PSD features of ğ‘“ frequency bands, and 2ğ‘‡ is 4 s atial s ectral te oral re resentation ttention ased ttention ased idirectional lassi ier derived by the 0.5s window without overlapping. To utilize the spatial information of electrodes, we organize all the ğ‘ channels as a 2D sparse map so that the 3D feature tensor ğ¹ ğ‘› is transformed into a 4D representation ğ‘‹ ğ‘› ïƒ ğ‘… â„ï‚´ğ‘¤ï‚´2ğ‘“ï‚´2ğ‘‡ , where â„ and ğ‘¤ are the height and width of the 2D sparse map, respectively. The 2D sparse map of all the c channels with zero-padding is shown in Fig.  3 , which preserves the topology of different electrodes. In this paper, we set â„ = 19, ğ‘¤ = 19, and ğ‘“ = 5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fig. 2",
      "text": "The generation of 4D spatial-spectral-temporal representation. For each Ts EEG signal segment, we extract DE and PSD features from different channels and frequency bands with a 0.5s window. Then, the features are transformed into a 4D representation which consists of 2T temporal slices.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attention-Based Cnn",
      "text": "For a 4D spatial-spectral-temporal representation ğ‘‹ ğ‘› , we extract the spatial and spectral information from each temporal slice ğ‘† ğ‘– ïƒ ğ‘… â„ï‚´ğ‘¤ï‚´2ğ‘“ , ğ‘– = 1, 2, . . . , 2ğ‘‡ with a CNN, explore the discriminative local patterns in spatial and spectral domains with a convolutional attention module, and finally get its spatial and spectral representation. The attention module here is similar to what Woo et al. propose  (Woo et al. 2018) , which is originally used to improve the representation power of CNN networks.\n\nThe structure of the attention-based CNN is shown in Fig.  4 . It contains four convolutional layers, four convolutional attention modules, one max-pooling layer, and one fullyconnected layer. The four convolutional layers have 64, 128, 256, and 64 feature maps with the filter size of 5 ï‚´ 5, 5 ï‚´ 5, 5 ï‚´ 5, and 3 ï‚´ 3, respectively. Specifically, a convolutional attention module is used after each convolutional layer to utilize the spatial and spectral attention mechanisms, and the details will be given later. We only use one max-pooling layer with a filter size of 2 ï‚´ 2 after the last convolutional attention module to preserve more information and enhance the robustness of the network. Finally, outputs of the max-pooling layer are flattened and fed to the fully-connected layer with 150 units. Thus, for each temporal slice ğ‘† ğ‘– , we take the final output ğ‘ƒ ğ‘– ïƒ ğ‘… 150 as its spatial and spectral representation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Convolutional Attention Module",
      "text": "The convolutional attention module is applied after each convolutional layer to adaptively capture important brain regions and frequency bands. The structure of the convolutional attention module is shown in Fig.  5 . It consists of two sub-modules, i.e. the spatial attention module and the spectral attention module.\n\nFor each convolutional layer above, its output is a 3D feature tensor ğ‘‰ ïƒ ğ‘… â„ ğ‘£ Ã— ğ‘¤ ğ‘£ Ã— ğ‘ ğ‘£ , where â„ ğ‘£ , ğ‘¤ ğ‘£ , and ğ‘ ğ‘£ are the height of the 2D feature maps of ğ‘‰, the width of the 2D feature maps of ğ‘‰, and the number of the 2D feature maps of ğ‘‰, respectively. We take ğ‘‰ as the input of the convolutional attention module.\n\nThe spectral attention module is applied to identify valuable frequency bands for emotion recognition. The average pooling has been widely used to aggregate spatial information and the maximum pooling has been commonly adopted to gather distinctive features. Therefore, we shrink the spatial dimension of ğ‘‰ by a spatial-wise average pooling and a spatial-wise maximum pooling, which are defined as:\n\nwhere ğ‘‰ ğ‘– ïƒ ğ‘… â„ ğ‘£ Ã— ğ‘¤ ğ‘£ denotes the 2D feature map in the i-th channel of ğ‘‰, ğ¶ ğ‘ğ‘£ğ‘”,ğ‘– represents the element in the i-th channel of the spatial average representation ğ¶ ğ‘ğ‘£ğ‘” ïƒ ğ‘… ğ‘ ğ‘£ , ğ‘šğ‘ğ‘¥(ğ‘) returns the largest element in ğ‘, and ğ¶ ğ‘šğ‘ğ‘¥,ğ‘– is the element in the i-th channel of the spatial maximum representation ğ¶ ğ‘šğ‘ğ‘¥ ïƒ ğ‘… ğ‘ ğ‘£ . Subsequently, we implement the spectral attention by two fully-connected layers, a Relu activation function and a sigmoid activation function, which is defined as:\n\nwhere ğ‘Š 1 ğ‘† and ğ‘Š 2 ğ‘† are learnable parameters, ïƒ… denotes the element-wise addition, and ğ´ ğ‘ ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘™ ïƒ ğ‘… 1Ã—1Ã—ğ‘ ğ‘£ is the spectral attention. The elements of ğ´ ğ‘ ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘™ represent the importance of the corresponding 2D feature maps of the spectral domain. After generating the spectral attention ğ´ ğ‘ ğ‘ğ‘’ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘™ , the output of the spectral attention module can be defined as:\n\nwhere ğ‘‰ â€² denotes the refined 3D feature tensor, and ïƒ„ represents the element-wise multiplication.\n\nThe spatial attention module is applied to identify valuable brain regions for emotion recognition. Firstly, we shrink the spectral dimension of ğ‘‰ â€² by spectral-wise average pooling and spectral-wise maximum pooling, which is defined as: where ğ‘† â„,ğ‘¤ â€² ïƒ ğ‘… ğ‘ ğ‘£ denotes the channel in the h-th row and w-th column of ğ‘‰ â€² , ğ‘†ğ‘ƒğ´ ğ‘ğ‘£ğ‘”,(â„,ğ‘¤) represents the element in the h-th row and w-th column of the spectral average representation ğ‘†ğ‘ƒğ´ ğ‘ğ‘£ğ‘” ïƒ ğ‘… â„ ğ‘£ Ã—ğ‘¤ ğ‘£ Ã—1 and ğ‘†ğ‘ƒğ´ ğ‘šğ‘ğ‘¥,(â„,ğ‘¤) is the element in the hth row and w-th column of the spectral maximum representation ğ‘†ğ‘ƒğ´ ğ‘šğ‘ğ‘¥ ïƒ ğ‘… â„ ğ‘£ Ã—ğ‘¤ ğ‘£ Ã—1 . In the following, we implement the spatial attention with a convolutional layer and a sigmoid activation function, which is defined as:\n\nwhere ğ¶ğ‘ğ‘¡(ğ‘†ğ‘ƒğ´ ğ‘ğ‘£ğ‘” , ğ‘†ğ‘ƒğ´ ğ‘šğ‘ğ‘¥ ) denotes the concatenation of ğ‘†ğ‘ƒğ´ ğ‘ğ‘£ğ‘” and ğ‘†ğ‘ƒğ´ ğ‘šğ‘ğ‘¥ along the spectral dimension, ğ¶ğ‘œğ‘›ğ‘£(ğ‘†ğ‘ƒğ´) represents the convolutional layer for ğ‘†ğ‘ƒğ´, and ğ´ ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ ïƒ ğ‘… â„ ğ‘£ Ã—ğ‘¤ ğ‘£ Ã—1 is the spatial attention. The elements of ğ´ ğ‘ ğ‘ğ‘ğ‘¡ğ‘–ğ‘ğ‘™ represent the importance of the corresponding regions of the spatial domain. Subsequently, the output of the spatial attention module can be defined as:\n\nwhere ğ‘‰ â€²â€² ïƒ ğ‘… â„ ğ‘£ Ã— ğ‘¤ ğ‘£ Ã— ğ‘ ğ‘£ denotes the final output 3D feature tensor of the convolutional attention module.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Attention-Based Bidirectional Lstm",
      "text": "For each temporal slice ğ‘† ğ‘– ïƒ ğ‘… â„ï‚´ğ‘¤ï‚´2ğ‘“ , ğ‘– = 1, 2, . . . , 2ğ‘‡ , the final output of the attention-based CNN is ğ‘ƒ ğ‘– ïƒ ğ‘… 150 . Since the variation between different temporal slices contains temporal information for emotion recognition, we utilize an attentionbased bidirectional LSTM to explore the importance of different slices, as shown in Fig.  6 . A bidirectional LSTM connects two unidirectional LSTMs with opposite directions to the same output. Comparing with a unidirectional LSTM, a bidirectional LSTM preserves information from both past and future, making it understand the context better. In this paper, the bidirectional LSTM comprises two unidirectional LSTMs with 36 memory cells. The unidirectional LSTM for positive time direction, LSTMP takes the output sequence of the attention-based CNN ğ‘ƒ ğ‘ƒ = (ğ‘ƒ 1 , ğ‘ƒ 2 , . . ., ğ‘ƒ 2ğ‘‡ ) as the input sequence, while the other for negative time direction, LSTMN takes the reverse sequence ğ‘ƒ ğ‘ = (ğ‘ƒ 2ğ‘‡ , ğ‘ƒ 2ğ‘‡-1 , . . ., ğ‘ƒ 1 ) as the input sequence. The outputs of the i-th node of the unidirectional LSTMs are ğ‘Œ ğ‘– ğ‘ƒ ïƒ ğ‘… 36\n\nand ğ‘Œ ğ‘– ğ‘ ïƒ ğ‘… 36 , ğ‘– = 1, 2, . . . , 2ğ‘‡ , respectively. Then, we concatenate ğ‘Œ ğ‘– ğ‘ƒ and ğ‘Œ 2ğ‘‡ + 1 -ğ‘– ğ‘ as the output of the i-th node of the bidirectional LSTM ğ‘Œ ğ‘– ïƒ ğ‘… 72 . Different from traditional ways that only use the output of the last node of an LSTM for classification or other applications, we take the outputs of all the bidirectional LSTM nodes ğ‘Œ ïƒ ğ‘… 2ğ‘‡Ã—72 into consideration and explore the importance of different temporal slices by the temporal attention mechanism.\n\nThe temporal attention mechanism is implemented with two fully-connected layers, a Relu activation function, and a softmax activation function, which is defined as:\n\nwhere ğ‘Š 1 ğ‘‡ , ğ‘Š 2 ğ‘‡ , ğ‘ 1 ğ‘‡ , and ğ‘ 2 ğ‘‡ are learnable parameters, ğ‘‡ğ‘’ğ‘š ğ‘– represents the i-th element of ğ‘‡ğ‘’ğ‘š ïƒ ğ‘… 2ğ‘‡Ã—1 which projects ğ‘Œ ïƒ ğ‘… 2ğ‘‡Ã—72 to a lower dimension, and ğ´ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ ïƒ ğ‘… 2ğ‘‡Ã—1 is the temporal attention. The elements of ğ´ ğ‘¡ğ‘’ğ‘šğ‘ğ‘œğ‘Ÿğ‘ğ‘™ represent the importance of the corresponding temporal slices. Subsequently, the high-level representation of the 4D sample ğ‘‹ ğ‘› can be defined as:\n\nwhere ğ‘Œ ğ‘’ ïƒ R 2TÃ—1 denotes the e-th column of ğ‘Œ ïƒ R 2TÃ—72 and ğ¿ ğ‘› (ğ‘’) is the e-th element of the high-level representation ğ¿ ğ‘› ïƒ ğ‘… 72 , which integrates spatial, spectral, and temporal information of ğ‘‹ ğ‘› .\n\nFig.  6  The top block is the structure of the bidirectional LSTM. We concatenate the outputs of LSTMP and LSTMP as the output of the bidirectional LSTM, ğ‘Œ ïƒ ğ‘… 2ğ‘‡Ã—72 . The middle block represents the projection of the outputs of the bidirectional LSTM. The bottom block denotes the generation of temporal attention.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Classifier",
      "text": "Based on the high-level representation ğ¿ ğ‘› of EEG signals, we apply a fully-connected layer and a softmax activation function to predict the label of the 4D sample ğ‘‹ ğ‘› , which can be defined as follows:\n\nwhere ğ‘Š ğ‘ , ğ‘ ğ‘ are learnable parameters and ğ‘ƒğ‘Ÿğ‘’ ïƒ ğ‘… ğ¶ denotes the probability of ğ‘‹ ğ‘› belonging to all the ğ¶ classes. Specifically, the class of the largest probability is the predicted label of 4D-aNN.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiment",
      "text": "In this section, we firstly introduce a widely used dataset. Then, the experiment settings are described. Finally, the results on the dataset are reported and discussed.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Seed Dataset",
      "text": "SEED dataset  (Zheng and Lu 2015)  contains 3 different categories of emotion data: positive, neutral, and negative. For each kind of emotion, 5 film clips that are about 4 minutes long and can elicit the desired target emotion are selected. 15 healthy subjects (7 males and 8 females, with age (23.27 ï‚± 2.37)) take part in the EEG signals collection experiment. 3 groups of experiments are conducted for each subject, and each experiment consists of 15 clips viewing processes. Each clip viewing process can be divided into four stages, including a 5 seconds hint of start, a 4 minutes clip period, a 45 seconds self-assessment, and a 15 seconds rest period. The order of the 15 clips is arranged so that two clips eliciting the same emotion are not shown consecutively. The EEG signals in the experiments are recorded by a 62-channel's E I euro can system and down-sampled to 200 Hz. Besides, the EEG signals seriously contaminated by electromyography (EMG) and electrooculography (EOG) are removed manually. Then, a bandpass filter between 0.3 to 50 Hz is applied to filter the noise.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Settings",
      "text": "The proposed 4D-aNN takes a 4D segment ğ‘‹ ğ‘› ïƒ ğ‘… â„ï‚´ğ‘¤ï‚´2ğ‘“ï‚´2ğ‘‡ as the input. In this paper, we adopt the 2D sparse map with â„ = 19 and ğ‘¤ = 19 to maintain the positional relationship of electrodes. As shown in previous works, the combination of all the 5 bands can contribute to better results so that we set ğ‘“ = 5. For each experiment, we set the length of segments ğ‘‡ as 3, obtaining about 1128 samples per experiment. Then, we conduct a fivefold cross-validation on each experiment and calculate the average classification accuracy (ACC) and standard deviation (STD) of 3 experiments for each subject. The average ACC and STD of all subjects are taken as the final performances of our method. We train the 4D-aNN on an NVIDIA GTX 1080 GPU. The Adam optimization is applied to minimize the loss function. We set the learning rate as 0.0003, the batch size as 12, and the maximum of epochs as 150.\n\nJournal XX (XXXX) XXXXXX Xiao et al 9",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baseline Models",
      "text": "â€¢ HCNN  (Li et al. 2018)",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "We compare our model with 5 baseline models on SEED dataset. Table  1  presents the average ACC and STD of these models for EEG emotion recognition. HCNN uses the hierarchical CNN architecture to classify emotion, but only considers the spatial information of EEG signals, reaching 88.60% on classification accuracy. BiHDM  (Li et al. 2020)  applies four directed RNNs to obtain the deep representation o all the EEG electrodes' signals, reaching 93.12% on classification accuracy. RGNN considers the biological topology among different brain regions, reaching 94.24% on classification accuracy. 4D-CRNN takes 4D DE feature maps containing spatial, spectral, and temporal information as inputs, reaching 94.74% on classification accuracy. SST-EmotionNet uses a two-stream network with the attention mechanisms, reaching 96.02% on classification accuracy. However, the data size of each input sample of SST-EmotionNet is about 4 times larger than 4D-aNN. Comparing with the baseline models, the proposed 4D-aNN achieves the state-of-the-art performance on the SEED dataset under intrasubject splitting. The average ACC of all subjects is 96.10%. The performances on each subject are shown as Fig.  7 , and there are 9 subjects  (#5, #6, #8, #9, #10, #11, #12, #13, and #15)  whose performances are better than the average ACC. Specifically, to make a fair comparison with 4D-CRNN, we conduct experiments on 4D-aNN (DE) and 4D-aNN (PSD), which represents the 4D-aNN only takes DE features as inputs and only takes PSD features as inputs, respectively. The accuracy of 4D-aNN (DE) exceeds that of 4D-CRNN by 0.65%, indicating the superiority of the proposed 4D-aNN. When compared with 4D-aNN (DE) and 4D-aNN (PSD), 4D-aNN displays the best performance, which indicates the effectiveness of the combination of different features.\n\nTable  1  The performance (average ACC and STD (%)) of the compared models.\n\nModel SEED ACC (%) STD (%) HCNN  (Li et al. 2018)  88.60 2.60 BiHDM  (Li et al. 2020)  93.12 6.06 RGNN  (Zhong et al. 2020)  94.24 5.95 4D-CRNN  (Shen et al. 2020)  94.74 2.32 SST-EmotionNet  (Jia et al. 2020)  96.02 2.17 4D-aNN 96.10 2.61 4D-aNN (DE) 95.39 3.05 4D-aNN (PSD) 90.49 7.97 Fig.  7  The performance of 4D-aNN on each subject. In the SEED dataset, 3 experiments are conducted for each subject. We evaluate the performance of each experiment and also present the average classification accuracy for each subject.\n\nTo verify the importance of the attention mechanisms in our model, we conduct an additional experiment for ablation studies on SEED dataset. The experiment is ablation on spatial, spectral, and temporal attention mechanisms. We evaluate the performances of 4D-aNN when spatial, spectral, temporal, and all the attention mechanisms are ablated respectively. As shown in Fig.  8 , when one of the attention mechanisms is ablated, the classification accuracy decreases. 4D-aNN without the spectral attention mechanism decreases by 0.63%, 4D-aNN without the spatial attention mechanism decreases by 0.47%, and 4D-aNN without the temporal attention mechanism decrease by 1.19%. Specifically, 4D-aNN without all the attention mechanisms decreases by 2.17%, which is the worst among the models used for comparison. In conclusion, the results indicate that the attention mechanisms make contributions to EEG emotion recognition for the ability to capture the discriminative local patterns in spatial, spectral, and temporal domains. In particular, to explore the critical brain regions for different emotions, we separately depict the electrode activity heatmaps in Fig.  9 . We draw the heatmaps using Grad-CAM++  (Chattopadhay et al. 2018) , based on the experimental results of subject #15. Grad-CAM++ uses the last convolutional layer feature maps and the class scores of the classifier to generate heatmaps. The heatmaps are able to explain which input regions are important for predictions. In this work, the size of each heatmap is 19ï‚´19, which is the same as the 2D sparse map. The elements in the heatmaps represent the contributions of the corresponding brain regions to the recognition of the target emotions. From Fig.  9 , We can observe the distinct distributions of important brain regions with regard to different emotions: channels FC5, FC3, and C5 are important for recognition of positive emotions, channels CP5, CP3, and CP1 are important for recognition of neutral emotions, and channels PO7, PO5, and P3 are important for recognition of negative emotions for subject #15. In particular, the critical brain regions could vary with different subjects, time, and emotions so that the attention mechanisms that enable 4D-aNN to adaptively capture discriminative patterns make sense for EEG emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "We conduct several experiments to investigate the use of 4D-aNN which fuses the spatial-spectral-temporal information and the effectiveness of the attention mechanisms on different domains for EEG emotion classification. In this section, we discuss three noteworthy points.\n\nFirst, to deal with the spatial-spectral information, we apply an attention-based CNN which consists of a CNN network, a spectral attention module, and a spatial attention module. The CNN network extracts the spatial-spectral representation from inputs first. Then, the spectral attention mechanism is applied to each spectral feature to explore the importance of different frequency bands and features. Besides, the spatial attention mechanism is applied to each 2D feature map to adaptively capture the critical brain regions. The critical brain regions and frequency bands could vary with different individuals, emotions, and time so that the ability to capture discriminative patterns of the attention modules improves the performance of 4D-aNN.\n\nSecond, to explore the temporal dependencies in 4D spatialspectral-temporal representations, we utilize an attentionbased bidirectional LSTM. The bidirectional LSTM extracts high-level representations from the outputs of the attentionbased CNN. Different from traditional ways that only use the output of the last node of an LSTM for classifications or other applications, we consider outputs of all the nodes with the temporal attention mechanism. The temporal attention mechanism adaptively assigns weights of different temporal slices so that the dynamic content of emotions in 4D representations could be captured better.\n\nThird, to address the importance of the attention mechanisms, we conduct ablation studies on different attention modules. 4D-aNN without the spatial, spectral, and temporal attention mechanism decreases by 0.47%, 0.63%, and 1.19% on classification accuracy, respectively. In particular, 4D-aNN without all the attention mechanisms decreases by 2.17%, which is the worst among the models in comparison. The experimental results demonstrate the effectiveness of the attention mechanisms to adaptively capture discriminative patterns.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the 4D-aNN model for EEG emotion recognition. The 4D-aNN takes 4D spatial-spectraltemporal representations containing spatial, spectral, and temporal information of EEG signals as inputs. We integrate the attention mechanisms into the CNN module and the bidirectional LSTM module. The CNN module deals with the spatial and spectral information of EEG signals while the spatial and spectral attention mechanisms capture critical brain regions and frequency bands adaptively. The bidirectional LSTM module extracts temporal dependencies on the outputs of the CNN module while the temporal attention mechanism explores the importance of different temporal slices. The experiments on SEED dataset demonstrate better performance than all baselines. In particular, the ablation studies on different attention modules show the effectiveness of the attention mechanisms in our model for EEG emotion recognition.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall structure of 4D-aNN for EEG",
      "page": 3
    },
    {
      "caption": "Figure 1: The overall structure of 4D-aNN.",
      "page": 3
    },
    {
      "caption": "Figure 2: As previous works do (Shen et al. 2020; Yang et al.",
      "page": 3
    },
    {
      "caption": "Figure 3: , which preserves the",
      "page": 4
    },
    {
      "caption": "Figure 2: The generation of 4D spatial-spectral-temporal representation. For each Ts EEG signal segment, we extract DE and PSD features",
      "page": 4
    },
    {
      "caption": "Figure 3: The 2D sparse map with zero-padding of 62 channels. The purpose of the organization is to preserve the positional relationships among",
      "page": 5
    },
    {
      "caption": "Figure 4: It contains four convolutional layers, four convolutional",
      "page": 5
    },
    {
      "caption": "Figure 4: The structure of the attention-based CNN. The upper half of",
      "page": 5
    },
    {
      "caption": "Figure 5: It consists",
      "page": 6
    },
    {
      "caption": "Figure 5: The top block is the overall structure of the convolutional attention block, it consists of the spectral attention module and",
      "page": 7
    },
    {
      "caption": "Figure 6: A bidirectional LSTM connects two unidirectional LSTMs",
      "page": 7
    },
    {
      "caption": "Figure 6: The top block is the structure of the bidirectional LSTM.",
      "page": 8
    },
    {
      "caption": "Figure 7: The performance of 4D-aNN on each subject. In the SEED dataset, 3 experiments are conducted for each subject. We evaluate the",
      "page": 9
    },
    {
      "caption": "Figure 8: , when one of the attention mechanisms is",
      "page": 10
    },
    {
      "caption": "Figure 8: Ablation studies on different input features and attention",
      "page": 10
    },
    {
      "caption": "Figure 9: We draw the heatmaps using Grad-",
      "page": 10
    },
    {
      "caption": "Figure 9: The electrode activity heatmaps based on the experimental",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The performance (average ACC and STD (%)) of the",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "NN\nand",
          "Column_4": "",
          "Column_5": "ac\npr",
          "Column_6": "",
          "Column_7": "jec\nth",
          "Column_8": "",
          "Column_9": "th\nrag",
          "Column_10": "",
          "Column_11": "ED\nssi",
          "Column_12": "",
          "Column_13": "set,\non",
          "Column_14": "",
          "Column_15": "pe\nrac",
          "Column_16": "",
          "Column_17": "nts\nea",
          "Column_18": "",
          "Column_19": "on\nbje",
          "Column_20": "",
          "Column_21": "d f",
          "Column_22": "",
          "Column_23": "ch",
          "Column_24": "",
          "Column_25": "ect",
          "Column_26": "",
          "Column_27": "ev"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "tion\n-a\nlar,\ntio\nFi\ntto\nsub\nl la\ngen\nh in\ne si\n2D\ncon\nitio\ndist\ndi\nfo\nnd\nd c\nf ne",
          "Column_3": "",
          "Column_4": "die\nâ€œâˆ’\ne\nwe\n9.\nhay\nt\nfe\nte\nt re\nof\nars\nbut\nf th\nt di\nren\neco\n1 a\nnel\nive",
          "Column_5": "",
          "Column_6": "n d\neno\nore\npar\nd\nal.\n5.\nre\natm\nns\nh\nap\ns o\nrg\nibu\not\ntio\nmp\nO7\noti",
          "Column_7": "",
          "Column_8": "rent\nthe\ne\ny d\nth\n18),\nad\ns a\n. T\nim\ntma\nhe\ne c\nmo\ns o\ns: c\nf p\nant\nO5\ns fo",
          "Column_9": "",
          "Column_10": "put\natio\nical\nct t\nhea\nsed\nM\nth\nhe\nrtan\ns 1\nme\nsp\nns.\nmp\nne\nive\nr re\nd\nbje",
          "Column_11": "",
          "Column_12": "ture\nn c\nrai\nele\naps\nth\nu\nass\nmap\nor\n19\nin\ning\nm\nant\nC5\noti\ngnit\nare\n#15",
          "Column_13": "",
          "Column_14": "nd\nain\negi\nde\ning\nper\nt\nore\nre\ndict\nhic\nh\nain\n. 9,\nain\nC3,\ns, c\nof\npo\npa"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Spatio-Spectral Representation Learning for Electroencephalographic Gait",
      "authors": [
        "Skgha Abbass",
        "K Tan",
        "A Al-Mamun",
        "N Thakor",
        "A Bezerianos",
        "J Li"
      ],
      "year": "2018",
      "venue": "Pattern Classification Ieee T Neur Sys Reh",
      "doi": "10.1109/TNSRE.2018.2864119"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition based on EEG using LSTM recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications",
      "doi": "10.14569/IJACSA.2017.081046"
    },
    {
      "citation_id": "3",
      "title": "Application of BCI systems in neurorehabilitation: a scoping review",
      "authors": [
        "M Bamdad",
        "H Zarshenas",
        "M Auais"
      ],
      "year": "2015",
      "venue": "Disability and Rehabilitation: Assistive Technology",
      "doi": "10.3109/17483107.2014.961569"
    },
    {
      "citation_id": "4",
      "title": "The Berlin brain-computer interface: progress beyond communication and control",
      "authors": [
        "B Blankertz"
      ],
      "year": "2016",
      "venue": "Front Neurosci-Switz",
      "doi": "10.3389/fnins.2016.00530"
    },
    {
      "citation_id": "5",
      "title": "Neural correlates of social and nonsocial emotions: An fMRI study",
      "authors": [
        "J Brittona",
        "K Phan",
        "S Taylor",
        "R Welsh",
        "K Berridge",
        "I Liberzon"
      ],
      "year": "2006",
      "venue": "Neuroimage",
      "doi": "10.1016/j.neuroimage.2005.11.027"
    },
    {
      "citation_id": "6",
      "title": "Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks",
      "authors": [
        "A Chattopadhay",
        "A Sarkar",
        "P Howlader",
        "V Balasubramanian"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for electroencephalogram (EEG) classification tasks: a review",
      "authors": [
        "A Craik",
        "Y He",
        "J Contreras-Vidal"
      ],
      "year": "2019",
      "venue": "J Neural Eng",
      "doi": "10.1088/1741-2552/ab0ab5"
    },
    {
      "citation_id": "8",
      "title": "Emotion, cognition, and behavior Science",
      "authors": [
        "R Dolan"
      ],
      "year": "2002",
      "venue": "Emotion, cognition, and behavior Science",
      "doi": "10.1126/science.1076358"
    },
    {
      "citation_id": "9",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R-N Duan",
        "J-Y Zhu",
        "B-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "10",
      "title": "Attentional bias for emotional faces in depressed and nondepressed individuals: an eye-tracking study",
      "authors": [
        "G Figueiredo",
        "W Ripka",
        "Efr Romaneli",
        "L Ulbricht"
      ],
      "year": "2019",
      "venue": "st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised emotional state classification through physiological parameters for social robotics applications Knowledge-Based Systems",
      "authors": [
        "L Fiorinia",
        "G Mancioppi",
        "F Semeraro",
        "H Fujita",
        "F Cavallo"
      ],
      "year": "0190",
      "venue": "Unsupervised emotional state classification through physiological parameters for social robotics applications Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2019.105217"
    },
    {
      "citation_id": "12",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2339834"
    },
    {
      "citation_id": "13",
      "title": "Spatial-Spectral-Temporal based Attention 3D Dense Network for EEG Emotion Recognition",
      "authors": [
        "Z Jia",
        "Y Lin",
        "X Cai",
        "H Chen",
        "H Gou",
        "J Wang",
        "Sst-Emotionnet"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia",
      "doi": "10.1145/3394171.3413724"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition using multimodal residual LSTM network",
      "authors": [
        "Jiaxin Ma",
        "H Tang",
        "W-L Zheng",
        "B-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia",
      "doi": "10.1145/3343031.3350871"
    },
    {
      "citation_id": "15",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "Ieee J Biomed Health",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "16",
      "title": "A review on the computational methods for emotional state estimation from the human eeg",
      "authors": [
        "M-K Kim",
        "M Kim",
        "E Oh",
        "S-P Kim"
      ],
      "year": "2013",
      "venue": "Comput Math Method M",
      "doi": "10.1155/2013/573734"
    },
    {
      "citation_id": "17",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Hierarchical convolutional neural networks for EEG-based emotion recognition",
      "authors": [
        "J Li",
        "Z Zhang",
        "H He",
        "M ; X Li",
        "B-L Lu"
      ],
      "year": "2009",
      "venue": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "doi": "10.1007/s12559-017-9533-"
    },
    {
      "citation_id": "19",
      "title": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition Ieee",
      "authors": [
        "Y Li"
      ],
      "year": "2020",
      "venue": "Cogn Dev Syst",
      "doi": "10.1109/TCDS.2020.2999337"
    },
    {
      "citation_id": "20",
      "title": "Practical emotional neural networks",
      "authors": [
        "E Lotfia"
      ],
      "year": "2014",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2014.06.012"
    },
    {
      "citation_id": "21",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "C MÃ¼hl",
        "Baa Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces",
      "doi": "10.1080/2326263X.2014.912881"
    },
    {
      "citation_id": "22",
      "title": "The hybrid BCI Front Neurosci-Switz",
      "authors": [
        "G Pfurtscheller"
      ],
      "year": "2010",
      "venue": "The hybrid BCI Front Neurosci-Switz",
      "doi": "10.3389/fnpro.2010.00003"
    },
    {
      "citation_id": "23",
      "title": "EEGbased emotion recognition using 4D convolutional recurrent neural network Cogn Neurodynamics",
      "authors": [
        "F Shen",
        "G Dai",
        "G Lin",
        "J Zhang",
        "W Kong",
        "H Zeng"
      ],
      "year": "2020",
      "venue": "EEGbased emotion recognition using 4D convolutional recurrent neural network Cogn Neurodynamics",
      "doi": "10.1007/s11571-020-09634-1"
    },
    {
      "citation_id": "24",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L-C Shi",
        "Y-Y Jiao",
        "B-L Lu"
      ],
      "year": "2013",
      "venue": "35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "25",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "26",
      "title": "EEG-based Emotion Recognition via Channel-wise Attention and Self Attention IEEE",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "EEG-based Emotion Recognition via Channel-wise Attention and Self Attention IEEE",
      "doi": "10.1109/TAFFC.2020.3025777"
    },
    {
      "citation_id": "27",
      "title": "Cbam: Convolutional block attention module. Computer Vision -ECCV 2018",
      "authors": [
        "S Woo",
        "J Park",
        "J-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Cbam: Convolutional block attention module. Computer Vision -ECCV 2018",
      "doi": "10.1007/978-3-030-01234-2_1"
    },
    {
      "citation_id": "28",
      "title": "Sparse kernel reduced-rank regression for bimodal emotion recognition from facial expression and speech",
      "authors": [
        "J Yan",
        "W Zheng",
        "Q Xu",
        "G Lu",
        "H Li",
        "B Wang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2016.2557721"
    },
    {
      "citation_id": "29",
      "title": "Continuous Convolutional Neural Network with 3D Input for EEG-Based Emotion Recognition",
      "authors": [
        "Y Yang",
        "Q Wu",
        "Y Fu",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Neural Information Processing",
      "doi": "10.1007/978-3-030-04239-4_39"
    },
    {
      "citation_id": "30",
      "title": "EEG-based emotion recognition using hierarchical network with subnetwork nodes Ieee",
      "authors": [
        "Y Yang",
        "Qmj Wu",
        "W-L Zheng",
        "B-L Lu"
      ],
      "year": "2018",
      "venue": "Cogn Dev Syst",
      "doi": "10.1109/TCDS.2017.2685338"
    },
    {
      "citation_id": "31",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W-L Zheng",
        "B-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "doi": "10.1109/TAMD.2015.2431497"
    },
    {
      "citation_id": "32",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W-L Zheng",
        "J-Y Zhu",
        "B-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2712143"
    },
    {
      "citation_id": "33",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2994159"
    }
  ]
}