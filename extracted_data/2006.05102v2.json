{
  "paper_id": "2006.05102v2",
  "title": "Rosbag-Based Multimodal Affective Dataset For Emotional And Cognitive States",
  "published": "2020-06-09T08:09:42Z",
  "authors": [
    "Wonse Jo",
    "Shyam Sundar Kannan",
    "Go-Eum Cha",
    "Ahreum Lee",
    "Byung-Cheol Min"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces a new ROSbag-based multimodal affective dataset for emotional and cognitive states generated using the Robot Operating System (ROS). We utilized images and sounds from the International Affective Pictures System (IAPS) and the International Affective Digitized Sounds (IADS) to stimulate targeted emotions (happiness, sadness, anger, fear, surprise, disgust, and neutral), and a dual N -back game to stimulate different levels of cognitive workload. 30 human subjects participated in the user study; their physiological data were collected using the latest commercial wearable sensors, behavioral data were collected using hardware devices such as cameras, and subjective assessments were carried out through questionnaires. All data were stored in single ROSbag files rather than in conventional Comma-Separated Values (CSV) files. This not only ensures synchronization of signals and videos in a data set, but also allows researchers to easily analyze and verify their algorithms by connecting directly to this dataset through ROS. The generated affective dataset consists of 1,602 ROSbag files, and the size of the dataset is about 787GB. The dataset is made publicly available. We expect that our dataset can be a great resource for many researchers in the fields of affective computing, Human-Computer Interaction (HCI), and Human-Robot Interaction (HRI).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The recent advancements in wearable devices have increased the attention to affective computing and Human-Computer Interaction (HCI). The easy availability of the wearable sensors has allowed for its integration with affective computing and has given rise to intelligent computing devices that can interpret the affective state of users and provide adaptive feedback to them accordingly. For instance, in an autonomous car, the level of autonomy could be dynamically adjusted based on the affective state of the human operator  [1] . In addition to the field of HCI, the affective computing has been deeply influencing the field of robotics too, especially Human-Robot Interaction (HRI). For example, in the social robot interaction system, physical conditions of users extracted from cameras (e.g., facial expression and body gestures) and/or physiological states of users collected from sensors used to flexibly change communication methods to reduce human's antipathy toward the robotics system  [2] ,  [3] .\n\nWith the advent of wireless wearable sensors and other commercially available devices like a smartwatch, there has been an increasing interest in estimating human's state from monitoring physiological signals. In response to this current trend of monitoring human state using wearable sensors, it is becoming more important to build more physiological datasets based on wearable sensors.\n\nMoreover, the development of affective state prediction algorithms and estimation methods using machine learning and neural networks has boosted the availability of publically available annotated affective datasets  [4] . The datasets have focused on recording the physiological responses of the participants using various stimuli. However, in most of the existing datasets, the data were recorded using laboratory type monitoring devices which are using wired technologies, so caused inconvenience for participants' movement  [5] ,  [6] .\n\nIn addition to the physiological sensor dataset, external behavioral information of the human is also useful in the estimation of the affective state  [7] . For example, the facial data are mostly used in affective datasets alongside the physiological sensor data  [8] ,  [9] . Another external modality that is widely used is the body gesture data  [10] . However, there are not many studies considering the relationship between physiological signals and behavioral information, so there are not many datasets including both the physiological data and human behavioral data. Therefore, it is necessary to build multimodal datasets that consist of both physiological and behavioral data.\n\nFurthermore, the estimation of human's affective state for effective HRI has been gaining increased interest in the recent days. The emergence of new robotics middleware (such as Robot Operating System (ROS)  [11] ) has also played a larger role in growing the variety of HRI research to integrate the robotics system with the affective computing. In ROS, the data collected are usually stored as a ROSbag. The ROSbag format has more benefits than the CSV format arXiv:2006.05102v2 [cs.CY] 20 Oct 2020 for collecting and analyzing the dataset. Since the ROS can ensure to synchronize the recording signals and videos, it is available to easily and directly analyze the dataset by replaying both using a single ROSbag file. Also, the ROS supports various program languages and operating systems, so that users can validate the developing algorithm and programs by connecting the dataset as like in real-time experiments. Plus, the dataset is available to convert to CSV format or others via additional ROS packages. Therefore, a dataset that combines both physiological and behavioral data based on ROS can have great advantages.\n\nIn this work, we present a ROSbag-based multimodal dataset comprising physiological data measured using wearable devices and behavioral data recorded using external devices. The data were collected from participants through a user study where various stimuli such as images, audio, and workload tasks were used. Fig.  1  outlines how the dataset was created and organized. During the user study, physiological responses such as Blood Volume Pulse (BVP), Electrocardiography (ECG), Electro-dermal Activity (EDA), Electromyography (EMG), Galvanic Skin Response (GSR), Heart Rate (HR), Interbeat Interval (IBI), Photoplethysmography (PPG), and Skin Temperature (ST) were measured using commercially available wearable devices. In addition to the physiological sensors, a 3D frontal camera and a sideview camera were used to record face and body gestures, respectively. To investigate the implicit behaviors of users, the variations in the keyboard typing and the mouse motion patterns were also recorded. During the study, the participants performed a self-assessment of their affective level using questionnaires at the end of each experiment. These subjective data can be used later for the training of the classifiers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Human affects shape a huge part of the human experience such as attention, learning, memory, and even decisionmaking which are required to complete tasks. Therefore, understanding and measuring human affects in real-time is vital to construct adaptive and context-aware interfaces that could enrich the user experience. To do so, affective computing research investigates how affect sensing and elicitation techniques can build the understanding of affect and contribute to the design of technologies  [12] . Two main methods have been used to estimate human emotion and cognition states  [13] . The first is to analyze internal human changes by monitoring physiological signals such as ECG, GSR, EMG, and so on. The other method involves human physical signals such as facial expression, gesture, voice, and so on. As human affects are too complex to present with a single signal, many researchers have applied multiple sensors to improve accuracy and reliability of the system  [13] ,  [14] .\n\nMost affective computing applications use annotated datasets to train machine learning models that recognize human psychological states  [14] ,  [15] . The majority of the dataset includes multimodal stimuli which were designed to elicit a particular human affect and sensor data that were collected when a subject was exposed to the stimuli. Depending on how the researchers defined the human affects and what types of sensors they used, characteristics of the annotated datasets are different. Although the independence between emotion and cognition is still a controversial topic  [16] , the researchers mainly focused on emotion recognition by providing different dimensions of emotion, so the affective dataset are getting increasingly diversified (such as, DEAP  [5] , DECAF  [6] , AMIGO  [8] , WESAD  [17] , and so on). Most of the existing dataset particularly focused on emotion recognition but did not design a deliberate experimental setting to detect one's cognitive state which could affect one's emotional states.\n\nIn this regard, we present a dataset for detecting emotional and cognitive states which is collected from various wearable devices that can monitor and collect human physiological and behavioral data in an unobtrusive manner.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Design Of User Study",
      "text": "We designed a user study to build a new affective dataset that includes physiological and behavioral data based on the participants' emotional and cognitive states. All participants were asked to perform two tasks: an emotion elicitation task and a cognitive workload task. This study was approved by the Purdue University's Institutional Review Board (Purdue IRB Protocol: #1812021453).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "The user study was conducted in a closed indoor setup as shown in Fig.  2 . The participants were seated in front of a screen with the various wearable sensors and other external sensors connected to a ROS-based monitoring system. Fig.  1  depicts the schematic of the monitoring system for reading physiological and behavioral data, as well as self-assessment ratings. The main laptop behind the screen is used to connect all sensors and devices, as well as to execute the Graphical user interface (GUI) programs for displaying emotion stimulus sets and the memory test game on the screen. The programs are connected with the ROS to synchronize and to save the data to a ROSbag file that is used to track and record all rostopic messages communicated within the ROS.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Participants",
      "text": "For this user study, we recruited 30 participants from the University; the 11 females and 19 males had an age range of 18 to 37 years (mean: 25.1; std: 4.497). It was ensured that none of the participants had any skin allergies to metal or plastic, medical history of brain disorder, or heart diseases and vision or muscle impairment, so that all the wearable devices could be used. The participants were compensated with $10 for their participation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Equipment",
      "text": "As shown in Fig.  2 , the physiological and behavioral sensors used in the monitoring system are wearable and commercial devices, so that the experimental settings do The physiological sensors connected to the monitoring system are as follows:\n\n• Empatica E4 is a wristband with an array of sensors for physiological monitoring: EDA, BVP, IBI HR, and ST  [18] . • Myo is an armband that measures the 8-channel EMG signals. It includes the 8 electrodes placed inside the band to measure the 8-channels EMG signals  [19] . • Polar H10 is worn-chest strap wearable measuring the HR via electrodes attached on a participant's chest  [20] . • Shimmer3 GSR+ measures GSR and the PPG using electrodes that are attached to the fingers [21]. The behavioral sensors included in the monitoring system are as follows:\n\n• Intel RealSense is used to record 3D-depth and 2D\n\ncolor videos, and mounted on the top of the TV screen for capturing participant's face  [22] . • USB camera is a basic camera to monitor the side view of the participants. • Mouse & Keyboard is used to track mouse cursor and monitor pushed keys. • Microphone is used to record the participant's voice.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Stimulus",
      "text": "For the emotion elicitation task, the images and the audio clips were taken from the IAPS and IADS which are widely used and validated in the field of physiology for provoking specific emotions  [23] ,  [24] . We particularly exploited 21 pictures of the International Affective Picture System (IAPS)  [25]  and 21 audio clips of the International Affective Digitized Sound System (IADS)  [26] . We used these visual and auditory stimuli to elicit targeted seven-emotions (e.g., happiness, sadness, anger, fear, surprise, and neutral). Table  I  shows the finally selected stimulus data for this user study. The used images and the number of IAPS and IADS are included on the dataset. For the cognitive workload task, we employed dual N -back games  [27] . To provoke different levels of cognitive workload (e.g., low, medium, and high), we controlled the number of back steps (N ) of games from 1-back to 3-back to adjust the difficulty of the games.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Experimental Protocol",
      "text": "In the user study, participants were given three tasks as illustrated in Fig.  3 . The first and second tasks are for emotional elicitation using IAPS and IADS, respectively. The third task is to stimulate the three-levels cognitive workload using dual N -back game. After finishing each task, the participant took a break until they want to proceed with the next task.\n\nThe first and second tasks were the emotion elicitation task which was composed of 21 rounds for each task. The participants were asked to look at a white cross on the screen for 10 seconds (called a fixation cross), then watch images of IAPS for 6 seconds in the first tasks or listen to short audio clips of IADS for 6 seconds in the second tasks, and then rate their perceived emotion with a 9-point Self-Assessment Manikin (SAM) scale  [28] . The images and the audios were selected such that they can stimulate various human emotions. Fig.  3a  and Fig.  3b  explain the procedures of the emotion elicitation task using the images and sound",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Video Types",
      "text": "Frontal face videos (RGB and depth), side view video stimulus, respectively. The third task was the cognitive workload task which consisted of three rounds by presenting different levels of difficulty, low, medium, and high. The participants were asked to complete the Dual N -back games. During the experiment, the humans' physiological and behavioral conditions were monitored using the proposed monitoring system in section III-C. After they completed each session, they were asked to rate their perceived cognitive workload with NASA-Task Load Index (NASA-TLX)  [29] . Fig.  3c  shows the procedures of the cognitive workload tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Dataset Construction",
      "text": "In this section, we explain the details of the proposed dataset configuration: physiological and behavior sensor data. Table  II  presents the summary of the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Physiological Sensor Data",
      "text": "The dataset includes BVP, ST, EDA, and IBI from Empatica E4 sensor with 30Hz sampling time, BVP and GSR from Shimmer3 GSR unit with 30Hz sampling time, HR from Polar H10 with 1Hz, and 8-channel EMGs from Myo armband with 50 sampling time.\n\nFig.  4  shows an example of physiological data in the dataset (IAPS #1201, P13). The first plot from top is the BVP signals, the second plot is the average of the IBI data, the third plot is the average of the EDA, the fourth plot is the average of ST data. Those data are collected from the Empatica E4 sensor. The fifth and sixth plots are raw PPG and GSR data of the Shimmer3 sensor. The seventh plot is the result of HR data of the Polar H10. The last plot is raw data of 8-channel EMGs of the Myo armband.\n\nIn the figures, the gray area indicates the duration when the stimulus was exposed to the participants during the experiments. The left side of the gray area is a baseline section where the participant lies in the fixation section. The right side of the gray area is a self-assessment reporting section for participants to fill the subjective questionnaires out.\n\nTable  III  summarizes the rostopic message information of the physiological data in the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Behavioral Sensor Data",
      "text": "The dataset includes three different kinds of image sequences taken by two cameras. The Intel RealSense camera located at the front captured facial expressions and upper body gestures in 30 frames per second (fps). At the same time, depth camera results separately were recorded in 30 fps. The USB camera at the side of participants obtained induced behavioral responses in 10 fps. As well, the participant's speech was recorded via a microphone mounted on the participant's neck for the user study.\n\nThe collected experimental data showed that the tasks elicited participants' emotional and cognitive states. For  example, a piece of the proposed dataset with the participant P13 and visual stimulus IAPS#1201 is shown in Fig.  5 . Given the recorded stream of participants, as presented in Fig.  5a , 5b and 5c, the behavioral data include facial expressions and body movements, which may imply emotional reactions.\n\nTable  IV  summarizes the rostopic messages information of the behavioral data in the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Subjective Rating Analysis A. The Sam Rating In The Emotion Elicitation Task",
      "text": "All participants' subjective measures (e.g., arousal, valence, and dominance) in each emotion elicitation task are compared to the reference values published in  [30] ,  [31] . The comparison results were plotted on a grid map image as shown in Fig.  6 , where we used Root-Mean Square Error (RMSE). Fig.  6a  shows the result of the comparison analysis in the emotion elicitation task using IAPS. Fig.  6b  shows the result of the comparison analysis in the emotion elicitation task using IADS. In both figures, the x-axis is the participant's number from P1 to P30 and the y-axis is the number of the dataset. In order to show the overall results of the comparison analysis of the self-assessments, we displayed the results using gradual colors from blue to red. The closer the index value to 0 (blue) means that the more similar it is to the reference value. On the other hand, the closer the index value to 45 (red) means that the more different it is is from the reference value.\n\nFor the results of the SAM scales in the emotion elicitation task using IAPS, the lowest similarity of the dataset is #3350 of P3 with RMSE 42.69, and the highest similarity of the dataset is IAPS#3022 of P26 with RMSE 0.04. P25 produced the highest similarity with mean RMSE 1.66, and P28 produced the lowest similarity with mean RMSE 6.81. The overall average and standard deviation of RMSE are 4.26 and 3.90, respectively.\n\nFor the SAM scales in the emotion elicitation task using IADS, the lowest quality of the dataset is IADS#286 of P2 with RMSE 44.28, and the highest quality of the dataset is # 820 of P26 with RMSE 0.01. P15 produced the highest similarity with mean RMSE 1.69, and P14 produced the lowest similarity with mean RMSE 10.02. The overall average and standard deviation of RMSE are 4.31 and 4.41, respectively, excepting lost data (P3's data and P4's #278, #360, and #425).    We analyzed the results of the NASA-TLX rating scales and scores of the dual N -back game to monitor the change of the participant's workload. Fig.  7  shows the overall results of the NASA-TLX and dual N -back game. The blue bar means the score of the dual N -back game, and orange, yellow, purple, green, sky-blue, and red bars mean each subscale ratings of the NASA-TLX: mental demand, physical demand, temporal demand, overall performance, effort, and frustration level that are rated within a 100-points range. In the dual 1back, most participants obtained 100 point scores in the dual N -back game, and also acquired the lowest rating of the subscales in the NASA-TLX (median of each the subscales: 40/40/35/40/40/50). In the dual 2-back, the participants' game scores decreased 55.56 points compared to the result of the dual 1-back. On the other hand, the subscales of the NASA-TLX increased; (median: 60/65/55/60/65/60). In the dual 3-back, the game score is 40-points that is the lowest score and all subscales of the NASA-TLX are highest scores compared to others; (median: 70/75/65/70/75/70).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Access To Dataset And Application",
      "text": "To obtain the permission for accessing the dataset presented in this paper, researchers should contact us via email; info@smart-laboratory.org. We will also provide source codes (such as ROS package and Matlab codes) to replay the dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Examples Of Replaying The Dataset",
      "text": "Since the dataset is encapsulated into the ROSbag files, the dataset can be easily played back in in any ROS-compatible robot system, such as ROS system in Linux system, and Matlab.\n\nFor using the ROS system, users should install the ROS on Linux, and then decompress the compressed dataset. An example of reading a ROSbag file on Linux system is below:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Applications: Facial Emotion Analysis",
      "text": "In this subsection, we demonstrate an example of applications using the proposed affective dataset. The application is to estimate human facial expression from the facial video of the affective dataset using open source-based Face Emotion Recognition (FER) libraries  [32] .\n\nFig.  8b  shows computed emotion based on the facial expressions (P13's IAPS#1201). The gray area in Fig.  8b  indicates the exposure duration of visual or auditory stimuli. The left side of the gray area is the exposure time with 10 seconds fixation cross. The right side of the gray area indicates the period during the self-assessment. The participant P13 rated the emotion response as word-emotion rating 'Disgust' with the SAM scale assessment level. Compared to the highest emotion probability of 'Happiness' from the emotion recognition library in Fig.  8b , not only is the calculated emotion different from self-assessed one, but the facial expressions are also not matched with the SAM scale assessment. This implies that only analyzing facial expressions may not be enough to fully estimate human emotions and that other behavioral or physiological features and analysis may need to be combined.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii. Conclusion And Future Works",
      "text": "In this paper, we have introduced a new ROSbag-based affective dataset that shows how one's emotional and cognitive states affect physiological and behavioral data.\n\nFor building the affective dataset, we designed a user study to stimulate the targeted emotions using IAPS and IADS datasets and different levels of the cognitive workload using dual N -back games, and executed the study by recruiting 30 participants. In the user study, we recorded the particiapnts status that includes physiological data from commercial wearable devices and the behavioral data using hardware devices, as well as the results of the subjective questionnaires using SAM and NASA-TLX. All data were saved in single ROSbag files rather than CSV files. This not only ensures synchronization of signals and videos in the dataset, but also allows researchers to easily analyze and verify their algorithms by connecting directly to this dataset through ROS. The generated dataset consists of 1,602 ROSbag files, and the size of the dataset is about 787GB. We expect that our dataset can be a great resource for many researchers in the fields of affective computing, HCI, and HRI.\n\nIn the future, we will utilize more (and latest) physiological sensors and hardware devices and develop additional psychological experiments related to workload, in order to update the affective dataset. We also plan to analyze more details of the dataset by extracting features from the collected data and validate the dataset using advanced machine learning techniques to estimate human's emotional and cognitive states.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Outline showing how a new ROSbag-based multi-",
      "page": 1
    },
    {
      "caption": "Figure 1: outlines how the",
      "page": 2
    },
    {
      "caption": "Figure 2: The participants were seated in front of a",
      "page": 2
    },
    {
      "caption": "Figure 1: depicts the schematic of the monitoring system for reading",
      "page": 2
    },
    {
      "caption": "Figure 2: , the physiological and behavioral",
      "page": 2
    },
    {
      "caption": "Figure 2: A user study setting. Commercial wearable sensors including Empatica E4, Shimmer3 GSR and PPG, Polar H10,",
      "page": 3
    },
    {
      "caption": "Figure 3: The ﬁrst and second tasks are for",
      "page": 3
    },
    {
      "caption": "Figure 3: a and Fig.3b explain the procedures",
      "page": 3
    },
    {
      "caption": "Figure 3: Details of the procedures for emotion elicitation tasks and cognitive workload tasks in the user study; (a) using",
      "page": 4
    },
    {
      "caption": "Figure 3: c shows the procedures",
      "page": 4
    },
    {
      "caption": "Figure 4: shows an example of physiological data in the",
      "page": 4
    },
    {
      "caption": "Figure 4: Example of the physiological signals from the dataset",
      "page": 5
    },
    {
      "caption": "Figure 6: , where we used Root-Mean Square",
      "page": 5
    },
    {
      "caption": "Figure 6: a shows the result of the comparison",
      "page": 5
    },
    {
      "caption": "Figure 5: Example of the behavioral data from the dataset",
      "page": 5
    },
    {
      "caption": "Figure 6: Color map to display the comparing results of root",
      "page": 6
    },
    {
      "caption": "Figure 7: shows the overall results of",
      "page": 6
    },
    {
      "caption": "Figure 7: The results of the Dual N-Back game score and",
      "page": 6
    },
    {
      "caption": "Figure 8: b shows computed emotion based on the facial",
      "page": 6
    },
    {
      "caption": "Figure 8: An example of emotion analysis from the collected",
      "page": 7
    },
    {
      "caption": "Figure 8: b, not only is the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fixation cross\n6 sec. \ndual N-back games": "NASA-TLX questionnaire"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion monitoring system for drivers",
      "authors": [
        "Z Kowalczuk",
        "M Czubenko",
        "T Merta"
      ],
      "year": "2019",
      "venue": "IFAC-PapersOnLine"
    },
    {
      "citation_id": "2",
      "title": "Sociosense: Robot navigation amongst pedestrians with social and psychological constraints",
      "authors": [
        "A Bera",
        "T Randhavane",
        "R Prinja",
        "D Manocha"
      ],
      "year": "2017",
      "venue": "2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "3",
      "title": "The emotionally intelligent robot: Improving social navigation in crowded environments",
      "authors": [
        "A Bera",
        "T Randhavane",
        "R Prinja",
        "K Kapsaskis",
        "A Wang",
        "K Gray",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "The emotionally intelligent robot: Improving social navigation in crowded environments",
      "arxiv": "arXiv:1903.03217"
    },
    {
      "citation_id": "4",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Decaf: Meg-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Learning Human Emotion from Body Gesture",
      "authors": [
        "C Shan"
      ],
      "year": "2012",
      "venue": "Learning Human Emotion from Body Gesture",
      "doi": "10.1007/978-1-4419-1428-6_1905"
    },
    {
      "citation_id": "8",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "Ros: an open-source robot operating system",
      "authors": [
        "M Quigley",
        "K Conley",
        "B Gerkey",
        "J Faust",
        "T Foote",
        "J Leibs",
        "R Wheeler",
        "A Ng"
      ],
      "year": "2009",
      "venue": "ICRA workshop on open source software"
    },
    {
      "citation_id": "12",
      "title": "Emotions and personality in adaptive e-learning systems: an affective computing perspective",
      "authors": [
        "O Santos"
      ],
      "year": "2016",
      "venue": "Emotions and personality in personalized services"
    },
    {
      "citation_id": "13",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "15",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "The relationship of emotion to cognition: A functional approach to a semantic controversy",
      "authors": [
        "H Leventhal",
        "K Scherer"
      ],
      "year": "1987",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "17",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "Real-time physiological signals -e4 eda/gsr sensor",
      "venue": "Real-time physiological signals -e4 eda/gsr sensor"
    },
    {
      "citation_id": "19",
      "title": "Myo armband",
      "venue": "Myo armband"
    },
    {
      "citation_id": "20",
      "title": "Polar h10: Heart rate monitor chest strap",
      "venue": "Polar h10: Heart rate monitor chest strap"
    },
    {
      "citation_id": "21",
      "title": "Intel® realsense™ depth and tracking cameras",
      "venue": "Intel® realsense™ depth and tracking cameras"
    },
    {
      "citation_id": "22",
      "title": "Artificial neural networks to assess emotional states from braincomputer interface",
      "authors": [
        "R Sánchez-Reolid",
        "A García",
        "M Vicente-Querol",
        "L Fernández-Aguilar",
        "M López",
        "A Fernández-Caballero",
        "P González"
      ],
      "year": "2018",
      "venue": "Electronics"
    },
    {
      "citation_id": "23",
      "title": "Affective content analysis of music emotion through eeg",
      "authors": [
        "J.-L Hsu",
        "Y.-L Zhen",
        "T.-C Lin",
        "Y.-S Chiu"
      ],
      "year": "2018",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "24",
      "title": "International affective picture system (iaps): Technical manual and affective ratings",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "1997",
      "venue": "International affective picture system (iaps): Technical manual and affective ratings"
    },
    {
      "citation_id": "25",
      "title": "The International affective digitized sounds (IADS): stimuli, instruction manual and affective ratings. NIMH Center for the Study of Emotion and Attention",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1999",
      "venue": "The International affective digitized sounds (IADS): stimuli, instruction manual and affective ratings. NIMH Center for the Study of Emotion and Attention"
    },
    {
      "citation_id": "26",
      "title": "Brain connectivity related to working memory performance",
      "authors": [
        "M Hampson",
        "N Driesen",
        "P Skudlarski",
        "J Gore"
      ],
      "year": "2006",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "27",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "28",
      "title": "Development of nasa-tlx (task load index): Results of empirical and theoretical research",
      "authors": [
        "S Hart",
        "L Staveland"
      ],
      "year": "1988",
      "venue": "Advances in psychology"
    },
    {
      "citation_id": "29",
      "title": "Technical report a-8, international affective picture system (iaps): affective ratings of pictures and instruction manual (university of florida",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2008",
      "venue": "Technical report a-8, international affective picture system (iaps): affective ratings of pictures and instruction manual (university of florida"
    },
    {
      "citation_id": "30",
      "title": "The international affective digitized sounds (; iads-2): Affective ratings of sounds and instruction manual",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "2007",
      "venue": "The international affective digitized sounds (; iads-2): Affective ratings of sounds and instruction manual"
    }
  ]
}