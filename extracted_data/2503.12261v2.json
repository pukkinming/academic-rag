{
  "paper_id": "2503.12261v2",
  "title": "United We Stand, Divided We Fall: Handling Weak Complementary Relationships For Audio-Visual Emotion Recognition In Valence-Arousal Space",
  "published": "2025-03-15T21:03:20Z",
  "authors": [
    "R. Gnana Praveen",
    "Jahangir Alam",
    "Eric Charton"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Audio and visual modalities are two predominant contactfree channels in videos, which are often expected to carry a complementary relationship with each other. However, they may not always complement each other, resulting in poor audio-visual feature representations. In this paper, we introduce Gated Recursive Joint Cross Attention (GR-JCA) using a gating mechanism that can adaptively choose the most relevant features to effectively capture the synergic relationships across audio and visual modalities. Specifically, we improve the performance of Recursive Joint Cross-Attention (RJCA) by introducing a gating mechanism to control the flow of information between the input features and the attended features of multiple iterations depending on the strength of their complementary relationship. For instance, if the modalities exhibit strong complementary relationships, the gating mechanism emphasizes cross-attended features, otherwise non-attended features. To further improve the performance of the system, we also explored a hierarchical gating approach by introducing a gating mechanism at every iteration, followed by high-level gating across the gated outputs of each iteration. The proposed approach improves the performance of RJCA model by adding more flexibility to deal with weak complementary relationships across audio and visual modalities. Extensive experiments are conducted on the challenging Affwild2 dataset to demonstrate the robustness of the proposed approach. By effectively handling the weak complementary relationships across the audio and visual modalities, the proposed model achieves a Concordance Correlation Coefficient (CCC) of 0.561 (0.623) and 0.620 (0.660) for valence and arousal respectively on the test set (validation set). This shows a remarkable improvement over the baseline of 0.211 (0.240) and 0.191 (0.200) for valence and arousal, respectively, in the test set (validation set), achieving competitive performance in the valence-arousal challenge of the 8th Affective Behavior Analysis in-the-Wild (ABAW) competition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the advancement of technology, empowering machines with affective capabilities is a fundamental requirement for Human-Computer Interaction (HCI). It has a lot of applications in a wide range of environments such as healthcare  [45] , autonomous driving  [10] , e-learning  [14] , etc. Automatic recognition of human emotions is a challenging problem as the expressions pertinent to the emotional states are quite complex and extremely diverse across individuals and cultures  [2] . Emotion Recognition (ER) can be formulated as the problem of classification or regression of emotions. In recent years, regression of expressions is gaining attention as it has the potential to capture a wide range of expressions, which can be useful in many applications such as estimation of pain intensity levels in the healthcare domain  [16] , engagement intensity levels in business or educational settings  [20] , etc. Depending on the granularity of labels, the regression of emotions can be formulated as ordinal regression or continuous regression. Compared to ordinal regression, continuous (or dimensional) regression is even more challenging due to the complex process of obtaining annotations in continuous dimensions. Valence and arousal are widely used dimensions to estimate emotion intensities in a continuous domain. Valence reflects the wide range of emotions in the dimension of pleasantness, from being negative (sad) to positive (happy). In contrast, arousal spans a range of intensities from passive (sleepiness) to active (high excitement).\n\nHuman emotions can be often conveyed through various modalities such as face, voice, text, physiology, etc. Out of all the modalities, facial and vocal expressions are the widely explored modalities in videos for multimodal ER  [11] . Cross Attention (CA) has been widely explored to capture the complementary relationships across audio and visual modalities in several applications such as event localization  [12] , action localization  [30] , and emotion recognition  [40] . Recently, sophisticated cross attention models have been proposed by introducing joint feature representation in the cross attentional framework to enhance the fea-ture representations by simultaneously capturing intra-and inter-modal relationships  [41, 42, 44] . Even though these models show promising performance, they rely on the assumption that audio and visual modalities always exhibit complementary relationship. However, the audio and visual modalities may not always be complementary to each other, they may also conflict with each other  [51] . Praveen et al.  [37]  investigated the problem of weak complementary relationships for ER with visual analysis and showed that weak complementary relationships degrade the crossattended features, resulting in poor audio-visual feature representations. To tackle this problem, Praveen et al.  [38]  proposed a Dynamic Cross Attention (DCA) model to adaptively choose the most relevant features among the cross attended and unattended features based on gating mechanism. In this work, we further extend their approach on the recently proposed RJCA  [44]  model by introducing the gating mechanism to control the flow of information across the original features and cross attended features of every iteration. By controlling the flow of information across the features of all iterations of RJCA model, we have more flexibility to capture the most relevant information across all the iterations. To further improve the system, we also explored hierarchical gating, where the gating mechanism is employed in every iteration, followed by high-level gating mechanism across the gated outputs of every iteration. The proposed approach demonstrates superior performance over the DCA  [37]  model as it can leverage the attended features of all iterations instead of only the final attended features. The major contributions of the paper are as follows.\n\n• We improve the performance of RJCA model by handling weak complementary relationships across the audio and visual modalities by controlling the flow of information across the features of all iterations and original features. • The proposed model has been further enhanced by introducing a hierarchical gating mechanism to capture more refined semantic features. • A detailed set of experiments are conducted to evaluate the contribution of the proposed model on the Affwild2 dataset, achieving better generalization ability and superior performance over prior state-of-the-art models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A-V Fusion For Dimensional Er",
      "text": "Atmaja et al  [5]  proposed a weighted summation of the loss functions in a multi-task learning framework, where the weights of the individual loss components are adjusted to improve the performance. Enrique et al also  [46]  explored the multi-stage fusion of predictions obtained from the multiple features of each modality in a hierarchical fashion. Haifeng et al  [9]  extracted deep spatio-temporal feature vectors of the images and spectrograms by combining pre-trained 2D-CNNs with 1D-CNNs and also exploited spatialtemporal graph convolutional network (STGCN) to obtain the geometric features based on facial landmarks. They have employed hybrid fusion using BLSTMs to obtain final predictions. Mihalis et al  [35]  studied the correlation across the emotional dimensions and found that each emotional dimension is more correlated with other emotional dimensions than with the features of individual modalities. Ehab et al  [1]  investigated the optimal trade-off between continuous and discrete emotion representations and found that jointly modeling the discrete and continuous emotion representations yields better performance. Though the abovementioned approaches have shown improvement in dimensional ER, they fail to capture the inter-modal relationships and relevant salient features specific to the task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attention For A-V Fusion",
      "text": "Huang et al  [19]  explored multimodal transformers to obtain effective multimodal feature representations by latently adapting audio to visual modality into a common feature space via cross-modal transformers. Srinivas et al  [36]  also explored multimodal transformers, where the crossattention module is integrated with the self-attention module to obtain the A-V cross-modal feature representations. Tran et al  [48]  investigated the potential of using pre-trained audio-visual transformers for ER and showed that finetuning pre-trained models improves performance. Praveen et al  [42]  proposed a cross-attention model to effectively capture the complementary relationships across the audio and visual modalities. They further extended their approach by introducing joint feature representations in the cross-attentional framework  [41, 42]  and recursive attention  [39, 44] . Although these sophisticated cross attention models have shown promising performance, they often rely on the assumption that audio and visual modalities always exhibit strong complementary relationships. When the audio and visual modalities exhibit weak complementary relationship, they fail to retain their superior performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Gating-Based Attention For A-V Fusion",
      "text": "Gating mechanisms have been explored for multimodal fusion to control the importance of each modality in the fusion mechanism  [3] . Decky et al  [4]  proposed a gated-sequence neural network for dimensional ER, where the gating mechanism is explored along with simple feature concatenation to adaptively fuse the modalities based on their relative importance. Lee et al  [31]  addressed the problem of noisy or corrupted modalities using a leaky gated cross attention by adaptively emphasizing the salient features of A and V modalities for action localization. Ayush et al  [29]  explored gated cross-attention along with self-attention for sentiment analysis and showed that exploiting the gating mechanism using a nonlinear transformation with cross-attention helps to control the impact of imperfect modalities. To deal with complementary relationships, Praveen et al.  [37, 38]  proposed Dynamic Cross Attention (DCA) by employing a gating mechanism to choose the relevant cross-attended or unattended features based on the strength of the complementary relationship. However, they rely only on the final attended features and original features, which may not capture the relevant information available in the features of individual iterations, resulting in suboptimal performance. In this work, we further extend the idea of DCA by employing the gating mechanism over the features of all the iterations of RJCA model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this section, we briefly introduce our baseline fusion model, RJCA, as a preliminary followed by the proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Notations",
      "text": "Given the video subsequence S of L frames, the feature representations of audio and visual modalities are denoted as\n\n, where d a and d v are dimensions of audio and visual modalities. x l a and x l v represent the feature vectors of the individual corresponding frames of audio and visual modalities. The joint feature representation (J ) is obtained by concatenating the feature vectors of audio and visual modalities, followed by fully connected layer, which is given by\n\nthe dimensionality of J and F C denote fully connected layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Recursive Joint Cross Attention (Rjca)",
      "text": "To capture the semantic relevance across and within the modalities, cross-correlation is computed between the joint feature representation J and the feature representations of individual modalities (X a and X v ), which is given by\n\nwhere C a and C v represent the joint cross-correlation matrices, W ja ∈ R da×d , W jv ∈ R dv×d represents learnable weight matrices of audio and visual modalities respectively. Now the joint cross-correlation matrices are used to compute the attention maps of the individual modalities as\n\nwhere W ca ∈ R L×L , W cv ∈ R L×L denote learnable weight matrices for audio and visual modalities.\n\nThe attention maps are further used to compute the attended features of the individual modalities as:\n\nwhere W ha ∈ R L×L and W hv ∈ R L×L denote the learnable weight matrices for audio and visual respectively.\n\nTo obtain more refined feature representations, the attended features of the individual modalities are again fed as input to the joint cross-attention fusion model in a recursive fashion, which is given by\n\nwhere\n\nhv ∈ R L×L denote the learnable weight matrices of audio and visual modalities, respectively, and t refers to the recursive step.\n\nThe attended features of the individual modalities at the last iteration t e are concatenated to obtain the multimodal feature representation X att .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gated Recursive Joint Cross Attention (Gr-Jca)",
      "text": "Though audio and visual modalities are expected to be complementary to each other  [50] , they may not always complement each other  [51] . Therefore, we present a gated attention mechanism to adaptively fuse the audio and visual features, based on the compatibility of the modalities with each other i.e., the gated attention emphasizes the cross-attended features when the features exhibit a strong complementary relationship, otherwise non-attended features. Specifically, we employ the gating mechanism to control the flow of information across the original features and the attended features of all iterations to leverage the semantic information across all the iterations. By adaptively emphasizing the semantic features across all the iterations and the original unattended features, the proposed model is able to effectively handle the weak complementary relationships across audio and visual modalities. Given the attended features of multiple iterations and non-attended features from audio and visual modalities, we design a gating layer using a fully connected layer for every iteration of each modality separately to obtain the attention weights for the attended and non-attended features, which are given by\n\nW go,a = X\n\nwhere W gl,a ∈ R da×(M +1) , W gl,v ∈ R dv×(M +1) are the learnable weights of the gating layers and W (t) go,a ∈ R L×(M +1) , W (t) go,v ∈ R L×(M +1) are outputs of gating layers of audio and visual modalities respectively. The number of output units of the gating layer is determined by the number of iterations and the original unattended features i.e., (M + 1) where M denotes the total number of iterations. To obtain probabilistic attention scores, the output of the gating layers is activated using a softmax function with a small temperature T , as given by\n\nwhere\n\n) denotes the probabilistic attention scores of audio and visual modalities. K denotes the number of output units of the gating layer, which is M + 1. In our experiments, we have empirically set the value of T to 0.1.\n\nThe probabilistic attention scores of the gating layer help to estimate the semantic relevance of attended features of every iteration and the original features. The columns of G a correspond to the probabilistic attention scores of each iteration t and the original unattended features. To multiply with the corresponding feature vectors, each column is replicated to match the dimension of the corresponding feature vectors. Now, the replicated attention scores are multiplied with the corresponding features of the respective modalities, which is further fed to the ReLU activation function as:\n\nwhere ⊗ denotes element-wise multiplication. Here t = 0 represents the original unattended features. X att,ga and X att,gv denote the final attended features of audio and visual modalities, which is concatenated and fed to Multi Layer Perceptron (MLP) for predicting valence and arousal.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Hierarchical Gated Recursive Joint Cross Attention (Hgrjca)",
      "text": "We also explored hierarchical gating approach, where the gating mechanism is employed at every iteration, followed by a high level gating mechanism to control the flow of information over the gated outputs of individual iteration.\n\nGiven the attended features of multiple iterations and nonattended features from audio and visual modalities, we design a gating layer for every iteration of each modality separately, which is given by\n\nwhere\n\ngl,v ∈ R dv×2 are the learnable weights of the gating layers of t-th iteration and W (t)  go,a ∈ R L×2 , W (t)  go,v ∈ R L×2 are outputs of gating layers of audio and visual modalities of t-th iteration respectively. The probabilistic attention scores of the outputs of the gating layers is obtained using a softmax function with a small temperature T , as given by\n\ngo,a /T and\n\ngo,v /T  (16)  where\n\ndenotes the probabilistic attention scores of audio and visual modalities. K denotes the number of output units of the gating layer, which is 2. Note that the two output units correspond to the input and output features of the fusion model at the t th iteration.\n\nThe two columns of G (t) a correspond to the probabilistic attention scores of iteration t (first column) and iteration t -1 (second column). Similar to GRJCA, each column is replicated to match the dimension of the corresponding feature vectors, which is denoted by\n\nfor audio and visual modalities respectively. Now, the replicated attention scores are multiplied with the corresponding features of iteration t and (t -1) of the respective modalities, which is further fed to the ReLU activation function as:\n\nwhere ⊗ denotes element-wise multiplication. X (t) att,ga and\n\natt,gv denote the attended features of audio and visual modalities of iteration t respectively.\n\nTo further enhance the flow of information across all the iterations of the RJCA model, we employ a high-level gating mechanism that can obtain semantic features by controlling the flow of information across the gated features of all iterations. Given the features obtained from the gating layers of every iteration, the final features for audio and visual modalities can be obtained as\n\nwhere M denotes the number of iterations, X\n\natt,ga and X",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "Affwild2 is the largest dataset in the field of affective computing, consisting of 594 videos collected from YouTube, all captured in-the-wild  [13] . The dataset has been extensively used to conduct a series of challenges on affective analysis including, expression recognition, action unit recognition, valence and arousal estimation  [21-23, 25-28, 53] . In this work, we have focused on the task of estimating valence and arousal for the 8th ABAW competition  [24] . In particular, for the valence-arousal track, the dataset is provided with 594 videos of around 2,993,081 frames obtained from 584 subjects. Sixteen of these videos display two subjects, both of which have been annotated. The annotations for valence and arousal are provided continuously in the range of [-1, 1]. The dataset is divided into training, validation, and test sets of 356, 76, and 162 videos respectively. The partitioning is done in a subject-independent manner so that every subject's data will be present in only one subset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metric",
      "text": "The Concordance Correlation Coefficient (CCC) is the widely used evaluation metric in the literature of dimensional ER to measure the level of agreement between the predictions ( y) and ground truth (y) of valence and arousal. Let µ y and σ 2 y represent the mean and variance of predictions respectively. Similarly, µ y and σ 2 y denote the mean and variance of ground truth, respectively, the CCC between the predictions and ground truth can be obtained as\n\nwhere σ 2 yy denotes the covariance of predictions and ground truth.\n\nAlthough Mean Square Error (MSE) has been widely used as a loss function for regression models, CCC was found to be more relevant for continuous regression labels  [6] . So we also used CCC-based loss function, following the literature of dimensional ER  [42, 49] , which is given by\n\n4.3. Implementation Details",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Preprocessing",
      "text": "For visual modality, we have used the cropped and aligned images provided by the challenge organizers  [23] . All the faces are resized to 48x48 and missing faces in the video frames are replaced as black frames (i.e., zero pixels). Some of the frames are not annotated and we discard those frames. The video sequence is divided into sub-sequences of length 300 (i.e., L=300) with a stride 200 frames, resulting in 33% overlap, thereby providing 33% more data. For audio modality, the audio stream is extracted from the videos and sampled with a sampling rate of 16KHz. Now log melspectrograms are computed from the sampled audio stream using the code provided by vggish repository 1 .\n\n1 https://github.com/harritaylor/torchvggish Note that the audio modality is properly synchronized with the corresponding subsequences of visual modality using a hop length of 1/f ps of the raw videos, while extracting the spectrograms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Details",
      "text": "For visual modality, we have used Resnet-50  [18]  pretrained on MS-CELEB-1M dataset  [17] , which is further finetuned on FER+ dataset  [7] . For audio modality, we used VGG-Net architecture pretrained on large-scale audioset dataset  [15] . For both audio and visual modalities, TCNs are used to capture the temporal dynamics of frame-level embeddings. Data augmentation is used for the visual modality, where random flipping and random crop of size 40 is used for training images, while only center crop is used for validation images. Both audio and visual features are normalized in order to have a mean and standard deviation of 0.5. The models are trained separately for valence and arousal. To regularize the network, a weight decay of 0.001 is used with Adam optimizer. The batch size is set to be 12. To avoid overfitting, we have employed early stopping and the maximum number of epochs are set to be 100. The hyperparameters of the initial learning rate and minimum learning rate are set to be 1e -5 and 1e -8 respectively. In our training mechanism, we also deployed a warmup scheme using ReduceLRonP lateau scheduler with a patience of 5 and a factor of 0.1 based on CCC score of the validation set. Inspired by the performance of  [55] , we also employ gradual finetuning of backbones, where three groups of layers for visual (Resnet-50) and audio (VGG) backbones are progressively selected for finetuning. More specifically, the first group is unfrozen at epoch 0 and the learning rate is linearly warmed up to 1e -5 within an epoch. Then repetitive warm-up is employed until epoch 5, after which ReduceLRonP lateau scheduler is used to update the learning rate. The learning rate is gradually dropped with a factor 0.1 until validation CCC does not improve over 5 consecutive epochs. Now the second group is finetuned and the learning rate is set to 1e -5, followed by warm-up scheme with ReduceLRonP lateau. The process is repeated until all the layers are finetuned for audio and visual backbones. To mitigate the problem of overfitting, the best model state dictionary over prior epochs is loaded at the end of each epoch. We also employed 6-fold crossvalidation, where fold-0 is the official partition of training and validation sets provided by the organizers  [24] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "Table  2  presents the results of the experiments conducted on the validation set to analyze the impact of the number of recursions of the RJCA model with the proposed GRJCA and HGRJCA models. First, we conducted a series of experi-Table  1 . CCC of the proposed approach compared to state-of-the-art methods for multimodal fusion on the original Affwild2 validation set (fold 0). ⋆ indicates that results are presented with the implementation of our experimental setup. Highest scores are shown in bold.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Method",
      "text": "Type   [42] . Now the number of recursions are gradually increased and found that the performance of the system also improves gradually, achieving best results at 3 iterations. Now, we conducted another series of experiments to understand the impact of the GRJCA model by varying the number of recursions. Though RJCA model helps in obtaining more refined feature representations, they fail to deal with weak complementary relationships. By introducing GRJCA model, we can observe that the relative performance has been consistently improved over that of RJCA model as we increase the number of iterations. This demonstrates that handling weak complementary relationships across audio and visual modalities plays a key role in effectively modeling the synergic inter-modal relationships. Similar to that of RJCA model, we achieved best results for GRJCA model with 3 iterations. Beyond that, the system performance declines, which can be attributed to overfitting. Finally, we conducted another series of experiments with HGRJCA model by varying the number of recursions.\n\nWe can observe that HGRJCA model offers slight improvement in performance by applying the gating mechanism in a hierarchical fashion. We hypothesize that employing gating mechanism at the individual iterations followed by high level gating helps to capture more relevant information with finer granularity.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "Table  1  shows the performance of the proposed approach against the relevant state-of-the-art audio-visual fusion models on the official validation set of the Affwild2 dataset.\n\nMost of the related work on the Affwild2 dataset has been submitted to the Affective Analysis in-the-wild (ABAW) challenges  [21, 23] . Therefore we compare our proposed approach with the relevant state-of-the-art models of ABAW challenges for A-V fusion in dimensional ER. One the strategies employed by majority of these approaches to improve the generalization ability is to increase the training dataset by leveraging additional datasets. Some of these approaches also explored multiple backbones for each modality to obtain diverse feature representations to improve the performance on test set  [33, 58] . Though leveraging ad-   [54]  proposed a leader-follower attention mechanism by considering the visual modality as the primary channel, while the audio modality is used as a supplementary channel to improve the fusion performance. Zhang et al.  [56]  showed that employing Masked-autoencoders on the visual modality, followed by fusion with audio modality can achieve better generalization and showed consistent improvement on both valence and arousal. Praveen et al.  [42]  introduced joint cross attention model to encode the intra-modal relationships along with the inter-modal relationships simultaneously, achieving significant improvement in the validation set, especially for valence. They further improved their approach by introducing recursive mechanism into the joint cross attention framework, and demonstrated that recursive fusion helps in obtaining more refined feature representations. However, all these methods overlook the problem of weak complementary relationships, assuming audio and visual modalities always complement with each other. Praveen et al.  [38]  investigated this problem and proposed DCA model to handle the weak complementary relationships by controlling the flow of information between original unattended and final attended features. In this work, we can observe that the proposed GRJCA model further improves the performance over DCA model by employing gating mechanism on the attended features of all the iterations. We also showed further improvement with HGRJCA model by employing gating mechanism in a hierarchical fashion, which helps to control the flow of subtle information across the iterations at fine-grained level. Note that even though the proposed models show lower performance on the official validation set, they achieved better performance eon other folds of cross-validation set, which is used to generate test set predictions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results On Test Set",
      "text": "We have implemented 6-fold cross validation on both GR-JCA and HGRJCA models and chose the best performing models on the validation set for generating the test set predictions. All the approaches that surpassed the baseline performance on the valence-arousal challenge of the 8th ABAW competition  [24]  are based on visual modality, while three approaches leveraged both audio and visual modalities. USTC-IAT-United  [52]  also explored Resnet-50 and VGGish backbone for visual and audio modalities similar to our approach, whereas they have improved the temporal modeling of the modalities using multiscale TCNs, followed by cross-modal transformers. It is worth mentioning that the performance of the winner is close to our performance as they also employ similar backbones with different fusion models. CtyunAI  [59]  and AIWELL-UOC  [8]  explored CLIP encoder to improve the visual feature representations and showed that CLIP encoders can help to achieve more generalized visual feature representations. Unlike other approaches, the proposed approach tackles the under-explored problem of weak complementary relationships across audio and visual modalities. We have shown that handling weak complementary relationships using a simple gating mechanism to control the flow of information across the attended features of multiple iterations and unattended features can significantly improve the performance of the system.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have introduced two audio-visual fusion models with RJCA as a baseline fusion model using a simple, yet efficient gating mechanism to effectively capture the inter-modal relationships by handling weak complementary relationships of audio and visual modalities. Even though several approaches have been proposed for dimensional Emotion Recognition (ER) based on cross-modal interactions, the problem of weak complementary relationships is less explored in the literature. Although cross-attention based approaches have shown significant improvement in the fusion performance, we have shown that deploying a gated attention model further improves the performance of the system. Specifically, the proposed approach emphasizes the most relevant features by leveraging the most semantic information across the original features as well as the attended features of all iterations of the RJCA model. Extensive experiments conducted on the challenging Affwild2 dataset demonstrate the robust performanc eof the proposed approach. We have shown that the problem of weak complementary relationships is a promising line of reaseach, that can foster the performance of the model by effectively modliung the synergic relationships across the audio and visual modalities for emotion recognition.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the proposed GRJCA model for two iterations. Different colorized arrows are used to denote the gating mechanism.",
      "page": 4
    },
    {
      "caption": "Figure 2: Illustration of the proposed HGRJCA model with two iterations. Different colorized arrows are used to denote different gating",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the performance of the proposed approach",
      "data": [
        {
          "Zhang et al. [54]": "Zhang et al. [56]",
          "Leader-Follower": "Transformers",
          "0.469": "0.464",
          "0.649": "0.640",
          "0.463": "0.648",
          "0.492": "0.625"
        },
        {
          "Zhang et al. [54]": "Zhou et al. [58]",
          "Leader-Follower": "Transformers",
          "0.469": "0.550",
          "0.649": "0.681",
          "0.463": "0.500",
          "0.492": "0.632"
        },
        {
          "Zhang et al. [54]": "Zhang et al. [57]",
          "Leader-Follower": "Transformers",
          "0.469": "0.554",
          "0.649": "0.659",
          "0.463": "0.523",
          "0.492": "0.545"
        },
        {
          "Zhang et al. [54]": "Meng et al. [34]",
          "Leader-Follower": "Transformers",
          "0.469": "0.588",
          "0.649": "0.668",
          "0.463": "0.606",
          "0.492": "0.596"
        },
        {
          "Zhang et al. [54]": "Praveen et al [41]",
          "Leader-Follower": "JCA",
          "0.469": "0.663",
          "0.649": "0.584",
          "0.463": "0.374",
          "0.492": "0.363"
        },
        {
          "Zhang et al. [54]": "Praveen et al [43]⋆",
          "Leader-Follower": "RJCA",
          "0.469": "0.443",
          "0.649": "0.639",
          "0.463": "0.537",
          "0.492": "0.576"
        },
        {
          "Zhang et al. [54]": "Praveen et al [38]⋆",
          "Leader-Follower": "DCA",
          "0.469": "0.451",
          "0.649": "0.647",
          "0.463": "0.549",
          "0.492": "0.585"
        },
        {
          "Zhang et al. [54]": "GRJCA (Ours)",
          "Leader-Follower": "GRJCA",
          "0.469": "0.459",
          "0.649": "0.652",
          "0.463": "0.556",
          "0.492": "0.605"
        },
        {
          "Zhang et al. [54]": "HGRJCA (Ours)",
          "Leader-Follower": "HGRJCA",
          "0.469": "0.464",
          "0.649": "0.660",
          "0.463": "0.561",
          "0.492": "0.620"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: shows the performance of the proposed approach",
      "data": [
        {
          "Number of\nrecursions (t)": "",
          "Valence": "RJCA",
          "Arousal": "RJCA"
        },
        {
          "Number of\nrecursions (t)": "l = 1",
          "Valence": "0.571",
          "Arousal": "0.649"
        },
        {
          "Number of\nrecursions (t)": "l = 2",
          "Valence": "0.575",
          "Arousal": "0.653"
        },
        {
          "Number of\nrecursions (t)": "l = 3",
          "Valence": "0.582",
          "Arousal": "0.659"
        },
        {
          "Number of\nrecursions (t)": "l = 4",
          "Valence": "0.580",
          "Arousal": "0.652"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "USTC-IAT-United [52]": "GRJCA (Ours)",
          "0.577": "0.561",
          "0.623": "0.620",
          "0.600": "0.590"
        },
        {
          "USTC-IAT-United [52]": "CtyunAI [59]",
          "0.577": "0.546",
          "0.623": "0.611",
          "0.600": "0.578"
        },
        {
          "USTC-IAT-United [52]": "HSEmotion [47]",
          "0.577": "0.494",
          "0.623": "0.551",
          "0.600": "0.522"
        },
        {
          "USTC-IAT-United [52]": "AIWELL-UOC [8]",
          "0.577": "0.468",
          "0.623": "0.492",
          "0.600": "0.480"
        },
        {
          "USTC-IAT-United [52]": "Charon [32]",
          "0.577": "0.504",
          "0.623": "0.412",
          "0.600": "0.458"
        },
        {
          "USTC-IAT-United [52]": "CAS-MAIS",
          "0.577": "0.327",
          "0.623": "0.304",
          "0.600": "0.316"
        },
        {
          "USTC-IAT-United [52]": "Baseline [27]",
          "0.577": "0.211",
          "0.623": "0.191",
          "0.600": "0.201"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Joint discrete and continuous emotion prediction using ensemble and end-to-end approaches",
      "authors": [
        "A Ehab",
        "Yelin Albadawy",
        "Kim"
      ],
      "year": "2018",
      "venue": "Proc. of the ACM ICMI"
    },
    {
      "citation_id": "2",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "T Cn. Anagnostopoulos",
        "I Iliou",
        "Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif Intell Rev"
    },
    {
      "citation_id": "3",
      "title": "Gated multimodal networks",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gómez",
        "Fabio González"
      ],
      "year": "2020",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "4",
      "title": "Audio-visual gated-sequenced neural networks for affect recognition",
      "authors": [
        "Decky Aspandi",
        "Federico Sukno",
        "Bjorn Schuller",
        "Xavier Binefa"
      ],
      "year": "2022",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "5",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Evaluation of errorand correlation-based loss functions for multitask learning dimensional speech emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series, page 012004"
    },
    {
      "citation_id": "7",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "8",
      "title": "Enhancing facial expression recognition through dual-direction attention mixed feature networks",
      "authors": [
        "Josep Cabacas-Maso",
        "Elena Ortega-Beltrán",
        "Ismael Benito-Altamirano",
        "Carles Ventura"
      ],
      "year": "2024",
      "venue": "Application to 7th abaw challenge",
      "arxiv": "arXiv:2407.12390"
    },
    {
      "citation_id": "9",
      "title": "Efficient spatial temporal convolutional features for audiovisual continuous affect recognition",
      "authors": [
        "Haifeng Chen",
        "Yifan Deng",
        "Shiwen Cheng",
        "Yixuan Wang",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2019",
      "venue": "Proc. of AVEC Workshop"
    },
    {
      "citation_id": "10",
      "title": "Emotion detection and face recognition of drivers in autonomous vehicles in iot platform. Image and Vision Computing",
      "authors": [
        "Zhongshan Chen",
        "Xinning Feng",
        "Shengwei Zhang"
      ],
      "year": "2022",
      "venue": "Emotion detection and face recognition of drivers in autonomous vehicles in iot platform. Image and Vision Computing"
    },
    {
      "citation_id": "11",
      "title": "A review and metaanalysis of multimodal affect detection systems",
      "authors": [
        "Jacqueline Kory"
      ],
      "year": "2015",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "12",
      "title": "Audio-visual event localization via recursive fusion by joint co-attention",
      "authors": [
        "Bin Duan",
        "Hao Tang",
        "Wei Wang",
        "Ziliang Zong",
        "Guowei Yang",
        "Yan Yan"
      ],
      "year": "2021",
      "venue": "IEEE WACV"
    },
    {
      "citation_id": "13",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "14",
      "title": "Facial emotion detection to assess learner's state of mind in an online learning system",
      "authors": [
        "Moutan Mukhopadhyay"
      ],
      "year": "2020",
      "venue": "Proc. of the ICIIT"
    },
    {
      "citation_id": "15",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey"
      ],
      "venue": "Cnn architectures for large-scale audio classification"
    },
    {
      "citation_id": "16",
      "title": "Deep weakly supervised domain adaptation for pain localization in videos",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2020",
      "venue": "Proc. of IEEE FG"
    },
    {
      "citation_id": "17",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Yandong Guo",
        "Lei Zhang",
        "Yuxiao Hu",
        "Xiaodong He",
        "Jianfeng Gao"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Prediction and localization of student engagement in the wild",
      "authors": [
        "Amanjot Kaur",
        "Aamir Mustafa",
        "Love Mehta",
        "Abhinav Dhall"
      ],
      "year": "2018",
      "venue": "Proc. of the DICTA"
    },
    {
      "citation_id": "21",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proc. of the IEEE/CVF CVPR"
    },
    {
      "citation_id": "22",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "24",
      "title": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Irene Kotsia",
        "Eric Cogitat",
        "Marco Granger",
        "Simon Pedersoli",
        "Alice Bacon",
        "Chunchang Baird",
        "Shao"
      ],
      "venue": "Advancements in affective and behavior analysis: The 8th abaw workshop and competition"
    },
    {
      "citation_id": "25",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "FG"
    },
    {
      "citation_id": "26",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "28",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "29",
      "title": "Gated mechanism for attention based multi modal sentiment analysis",
      "authors": [
        "Ayush Kumar",
        "Jithendra Vepa"
      ],
      "year": "2020",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "30",
      "title": "Cross-attentional audio-visual fusion for weaklysupervised action localization",
      "authors": [
        "Jun-Tae Lee",
        "Mihir Jain",
        "Hyoungwoo Park",
        "Sungrack Yun"
      ],
      "venue": "Cross-attentional audio-visual fusion for weaklysupervised action localization"
    },
    {
      "citation_id": "31",
      "title": "Leaky gated cross-attention for weakly supervised multi-modal temporal action localization",
      "authors": [
        "Jun-Tae Lee",
        "Sungrack Yun",
        "Mihir Jain"
      ],
      "year": "2022",
      "venue": "IEEE/CVF WACV"
    },
    {
      "citation_id": "32",
      "title": "Mamba-va: A mamba-based approach for continuous emotion recognition in valence-arousal space",
      "authors": [
        "Yuheng Liang",
        "Zheyu Wang",
        "Feng Liu",
        "Mingzhou Liu",
        "Yu Yao"
      ],
      "year": "2025",
      "venue": "Mamba-va: A mamba-based approach for continuous emotion recognition in valence-arousal space",
      "arxiv": "arXiv:2503.10104"
    },
    {
      "citation_id": "33",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "IEEE/CVF CVPR Workshop"
    },
    {
      "citation_id": "34",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "Liyu Meng",
        "Yuchen Liu",
        "Xiaolong Liu",
        "Zhaopei Huang",
        "Wenqiang Jiang",
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Qin Jin"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "35",
      "title": "Correlated-spaces regression for learning continuous emotion dimensions",
      "authors": [
        "A Mihalis",
        "Stefanos Nicolaou",
        "Maja Zafeiriou",
        "Pantic"
      ],
      "year": "2013",
      "venue": "Proc. of the ACMMM"
    },
    {
      "citation_id": "36",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "STL 2021"
    },
    {
      "citation_id": "37",
      "title": "Incongruity-aware cross-modal attention for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Gnana Praveen",
        "Jahangir Alam"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Cross-attention is not always needed: Dynamic cross-attention for audio-visual dimensional emotion recognition",
      "authors": [
        "Gnana Praveen",
        "Jahangir Alam"
      ],
      "year": "2008",
      "venue": "2024 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "39",
      "title": "Recursive joint crossmodal attention for multimodal fusion in dimensional emotion recognition",
      "authors": [
        "Gnana Praveen",
        "Jahangir Alam"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "40",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "R Gnana Praveen",
        "Eric Granger",
        "Patrick Cardinal"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "41",
      "title": "A joint cross-attention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "Wheidima Gnana Praveen",
        "Nasib Carneiro De Melo",
        "Haseeb Ullah",
        "Osama Aslam",
        "Théo Zeeshan",
        "Marco Denorme",
        "Alessandro Pedersoli",
        "Simon Koerich",
        "Patrick Bacon",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2022",
      "venue": "Proc. of the IEEE/CVF CVPR Workshops"
    },
    {
      "citation_id": "42",
      "title": "Audio-visual fusion for emotion recognition in the valencearousal space using joint cross-attention",
      "authors": [
        "Patrick Gnana Praveen",
        "Eric Cardinal",
        "Granger"
      ],
      "year": "2008",
      "venue": "IEEE TBIOM"
    },
    {
      "citation_id": "43",
      "title": "Recursive joint attention for audio-visual fusion in regression based emotion recognition",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "44",
      "title": "Recursive joint attention for audio-visual fusion in regression based emotion recognition",
      "authors": [
        "Eric Gnana Praveen",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2023",
      "venue": "Recursive joint attention for audio-visual fusion in regression based emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "Deep domain adaptation with ordinal regression for pain assessment using weakly-labeled videos",
      "authors": [
        "Eric Gnana Praveen Rajasekhar",
        "Patrick Granger",
        "Cardinal"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "46",
      "title": "Audiovisual three-level fusion for continuous estimation of russell's emotion circumplex",
      "authors": [
        "Enrique Sánchez-Lozano",
        "Paula Lopez-Otero",
        "Laura Docio-Fernandez",
        "Enrique Argones-Rúa",
        "José Luis"
      ],
      "year": "2013",
      "venue": "Proc. of AVEC Workshop"
    },
    {
      "citation_id": "47",
      "title": "Hsemotion team at abaw-8 competition: Audiovisual ambivalence/hesitancy, emotional mimicry intensity and facial expression recognition",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2025",
      "venue": "Hsemotion team at abaw-8 competition: Audiovisual ambivalence/hesitancy, emotional mimicry intensity and facial expression recognition",
      "arxiv": "arXiv:2503.10399"
    },
    {
      "citation_id": "48",
      "title": "A pre-trained audio-visual transformer for emotion recognition",
      "authors": [
        "Minh Tran",
        "Mohammad Soleymani"
      ],
      "year": "2022",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "49",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE JSTSP"
    },
    {
      "citation_id": "50",
      "title": "Deep multimodal complementarity learning",
      "authors": [
        "Daheng Wang",
        "Tong Zhao",
        "Wenhao Yu",
        "Nitesh Chawla",
        "Meng Jiang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "51",
      "title": "M2lens: Visualizing and explaining multimodal models for sentiment analysis",
      "authors": [
        "Xingbo Wang",
        "Jianben He",
        "Zhihua Jin",
        "Muqiao Yang",
        "Yong Wang",
        "Huamin Qu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on VCG"
    },
    {
      "citation_id": "52",
      "title": "Interactive multimodal fusion with temporal modeling",
      "authors": [
        "Jun Yu",
        "Yongqi Wang",
        "Lei Wang",
        "Yang Zheng",
        "Shengfan Xu"
      ],
      "year": "2025",
      "venue": "Interactive multimodal fusion with temporal modeling",
      "arxiv": "arXiv:2503.10523"
    },
    {
      "citation_id": "53",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "54",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Y Zhang",
        "Z Ding",
        "C Wei",
        "Guan"
      ],
      "year": "2021",
      "venue": "ICCV Workshop"
    },
    {
      "citation_id": "55",
      "title": "Multimodal continuous emotion recognition: A technical report for abaw5",
      "authors": [
        "Su Zhang",
        "Ziyuan Zhao",
        "Cuntai Guan"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "56",
      "title": "Multimodal facial affective analysis based on masked autoencoder",
      "authors": [
        "Wei Zhang",
        "Bowen Ma",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "57",
      "title": "Abaw5 challenge: A facial affect recognition approach utilizing transformer encoder and audiovisual fusion",
      "authors": [
        "Ziyang Zhang",
        "Liuwei An",
        "Zishun Cui",
        "Ao Xu",
        "Tengteng Dong",
        "Yueqi Jiang",
        "Jingyi Shi",
        "Xin Liu",
        "Xiao Sun",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "58",
      "title": "Leveraging tcn and transformer for effective visualaudio fusion in continuous emotion recognition",
      "authors": [
        "Weiwei Zhou",
        "Jiada Lu",
        "Zhaolong Xiong",
        "Weifeng Wang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "59",
      "title": "Emotion recognition with clip and sequential learning",
      "authors": [
        "Weiwei Zhou",
        "Chenkun Ling",
        "Zefeng Cai"
      ],
      "year": "2025",
      "venue": "Emotion recognition with clip and sequential learning",
      "arxiv": "arXiv:2503.09929"
    }
  ]
}