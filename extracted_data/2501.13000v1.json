{
  "paper_id": "2501.13000v1",
  "title": "Why Disentanglement-Based Speaker Anonymization Systems Fail At Preserving Emotions?",
  "published": "2025-01-22T16:37:56Z",
  "authors": [
    "Ünal Ege Gaznepoglu",
    "Nils Peters"
  ],
  "keywords": [
    "speaker anonymization",
    "neural vocoders",
    "speaker embeddings",
    "speech foundation models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Disentanglement-based speaker anonymization involves decomposing speech into a semantically meaningful representation, altering the speaker embedding, and resynthesizing a waveform using a neural vocoder. State-of-the-art systems of this kind are known to remove emotion information. Possible reasons include mode collapse in GANbased vocoders, unintended modeling and modification of emotions through speaker embeddings, or excessive sanitization of the intermediate representation (IR). In this paper, we conduct a comprehensive evaluation of a state-of-the-art speaker anonymization system to understand the underlying causes. We conclude that the main reason is the lack of emotion-related information in the IR. The speaker embeddings also have a high impact, if they are learned in a generative context. The vocoder's out-of-distribution performance has a smaller impact. Additionally, we discovered that synthesis artifacts increase spectral kurtosis, biasing emotion recognition evaluation towards classifying utterances as angry. Therefore, we conclude that reporting unweighted average recall alone for emotion recognition performance is suboptimal.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speaker anonymization is the task of modifying a speech signal such that the speaker identity, i.e, timbre, is changed yet other information such as linguistic content and emotions are preserved. Disentanglement-based speaker anonymization consists of obtaining a semantically meaningful speech representation, modifying it to alter the identity, and re-synthesizing a waveform. Numerous systems developed in this spirit have been shown to be successful, in terms of ensuring privacy meanwhile maintaining intelligibility in the main event of this discipline, the VoicePrivacy Challenge (VPC)  [1] .\n\nThe VPC 2024 evaluation plan  [1]  shows that the most successful systems in the literature, such as STTTS (B3)  [2] , the neural audio codec-based system (B4)  [3] , and the vector quantization-based system (B5)  [4]  have a common shortcoming: the loss of emotion information throughout the process, outlined by the gap between emotion recognition performance (UAR, see Sec. III-C for details) on original and anonymized utterances. SSL-SAS, a system that uses self-supervised speech representations, also suffers from this problem  [5] . Loss of emotion information during speaker anonymization has significant implications for, e.g., medical and legal use cases, where emotional cues may be detrimental for diagnosis or evidence. By understanding the underlying causes, we can develop anonymization systems that preserve emotions, enabling more reliable applications.\n\nCurrent evaluation methodologies, e.g., the VPC 2024 evaluation plan  [1] , are not specifically designed to identify problematic block(s) of disentanglement-based anonymization systems. In particular, the interplay between the intermediate representation (IR) and the vocoder is challenging to characterize. By performing ablation studies, we can gain further insights into the system design. For example, bypassing the anonymization block has shown that the The International Audio Laboratories Erlangen are a joint institution of the Friedrich-Alexander-Universtität Erlangen-Nürnberg and Fraunhofer IIS. Corresponding author: ege.gaznepoglu@audiolabs-erlangen.de reconstructions do not necessarily preserve pitch, and contain artifacts  [6] . The authors of  [7]  discovered that unified vocoders largely ignore speaker conditioning and cause a phenomenon called vocoder drift, by measuring the discrepancy between the intended pseudo-speaker and the speaker embedding extracted from the output speech. In a similar spirit, in this paper, we perform a comprehensive ablation study on SSL-SAS, a state-of-the-art speaker anonymization system, to understand why emotion information is lost.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Hypotheses",
      "text": "Based on our literature review, we identified three potential culprits that cause emotion information to be lost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Mode Collapse Of The Gan-Based Vocoders",
      "text": "The first possibility is the design and training of the vocoders, which convert the IR to a speech waveform. Systems such as  [4]  and  [5]  use a joint HiFi-GAN, trained on the reconstruction of read speech (LibriTTS). The adopted methodology (generative adversarial networks) is known to suffer from mode collapse, where the model learns to produce a limited subset of possible outputs. The composition of the training dataset, the lack of inductive biases in the system, and the utilized reconstruction loss might hinder the system from generalizing to out-of-distribution (OOD) samples, such as expressive speech. Several works  [8] -  [12]  identified shortcomings and proposed mitigations to improve OOD performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Speaker Embeddings Capturing Emotion Data",
      "text": "The penultimate layer of automated speaker verification (ASV) systems (e.g., ECAPA-TDNN  [14] ) yields a speaker embedding. Speaker anonymization systems then generate a new embedding, called a pseudo-identity, to condition the speech synthesis. Recent work  [15] ,  [16]  has identified that the speaker embeddings obtained in this fashion do not serve well as conditioning information for speech synthesis. Limitations include a tendency to capture undesired emotion information and an indifference to intra-speaker variations. The reported attempts to compensate the speaker embeddings for emotion information were only partially able to bridge the gap between the unweighted average recall (UAR) scores of the original and anonymized utterances  [5] . In particular, the reported recall for sad utterances is still very low, motivating further research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "C. Discretization Of The Intermediate Representation",
      "text": "The components of IR (see Tab. I) other than the speaker embeddings may be the culprit. We observe that speaker information is removed from the IR by discretizing, either via quantization  [3] ,  [4] , phonetic transcriptions [2], or soft-encoding  [5] . On the contrary, works such as  [17]  discovered that indirect emotion supervision, e.g., by controlling emotions through some continuous acoustic features such as spectral centroids, spectral kurtosis, loudness, or ∆F0 is more effective than providing explicit supervision via discrete labels. GST  [13]  Phonetic transcriptions (ling.) + Prosody representation (prosody) VPC 2024 B4  [3]  Acoustic prompts Semantic tokens (ling.) VPC 2024 B5  [4]  One-hot encoding Vector-quantized and post-processed wav2vec embeddings (ling.) + F0 (prosody) SSL-SAS  [5]  ECAPA-TDNN  [14]  HuBERT-based soft-encoded embeddings (ling.) + F0 (prosody)\n\nBesides, studies probing HuBERT discovered that earlier transformer layers (up to T3) encode prosodic information  [18] . In contrast, SSL-SAS uses either T6 or T12 as they encode the linguistic content  [19] , hinting that even the unsanitized IR has a low availability of prosodic information. To summarize, while striving for better privacy, the excessive sanitization might be causing the already scarce emotion information to be lost. The system under test, SSL-SAS  [20] , is depicted in Fig.  1 . First, three feature extractors encode distinct components of speech. ECAPA-TDNN embeddings  [14]  encode the speaker identity. The linguistic content is encoded using HuBERT  [21] , followed by softencoding  [22] . For prosody YAAPT  [23]  extracts F0. YAAPT yields a pitch track with a frame rate (per second) of 100, whereas HuBERT outputs have a frame rate of 50, which is upsampled to 100 by repetition as in  [20] . Utterance-level speaker embeddings are also repeated. Then, a pseudo-speaker embedding replaces the input speaker embedding. In  [24] , the authors of SSL-SAS introduced a novel anonymizer called orthogonal Householder neural network (OHNN) but since the source code is not released, we use the anonymizer they refer to as selec-anon. First proposed by  [25] , it uses a speaker pool LibriTTS train-other-500 to select speakers away from the original speaker, then averages a random subset of these candidates to generate a pseudo-identity. Finally, a HiFiGAN-variant, trained on the reconstruction of LibriTTS train-clean-100 from the concatenated IR, synthesizes the final waveform.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. System Under Test",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. The Experiments",
      "text": "We manipulate the IR of SSL-SAS to attribute the loss of emotion information to any of the hypotheses. Table  II  provides a summary of our experiments. We change the chosen HuBERT transformer layer, and the applied post-processing, to grasp the impact of the downstream-related IR. T6 without post-processing and T12 with soft-encoding were the choices in  [20] . T1 without post-processing is motivated by  [18] , as it has been shown to encode prosodic information. In addition, we run some experiments to quantify the effect of the speaker embeddings. We substitute ECAPA-TDNN with global style tokens (GST) to see the impact of speaker embeddings learned in a generative context. Next, we omit them altogether, expecting the vocoder to rely on the information in downstream-related IR to reconstruct the utterances. Finally, we use selec-anon to check if anonymizers have any impact on the emotions.\n\nIn our experiments, we always use the implementation and checkpoints for ECAPA-TDNN and HuBERT (incl. soft-encoder if applicable) available on 1  . We use the GST implementation and the checkpoint within VPC 2024 B3 (STTTS), available on 2  . For each configuration, we train the vocoder to 100k steps with same losses and hyperparameters as in  [20] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "We use the pretrained part of the VPC 2024 evaluation protocol [1] as made available on 2 . To measure privacy benefits, a pretrained ASV eval (see Fig.  2 ) performs an attack according to attack models of increasing strengths  [1] . VPC 2024 evaluation includes training an ASV anon eval on anonymized speech, to assess the privacy in semiinformed attack scenarios. Since our primary aim is to evaluate the emotion preservation, we instead focus on ASV evaluation per the lazy-informed attack model. For any attack model, anonymization is expected to increase the equal error rate (EER), hinting that the original speakers of the anonymized utterances could not be identified.\n\nFor utility evaluation (see Fig.  3 ), we use VPC2024 ASR eval to compute word error rates (WERs), which are then compared to the performance on the original data. For these experiments, LibriSpeech  [26]  subsets are used. To assess emotion preservation, we use the VPC2024 SER eval along with the IEMOCAP dataset  [27] . For both metrics, lower values are desired. IV. RESULTS",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Discretization Of The Intermediate Representation",
      "text": "We summarize the results of our experiments in Table  II . The results for SSL-SAS (lowest row, mnemonic ECAPA-Anon-T12-soft) align with  [5] , and appear more promising than for VPC baselines B3-B5  [1] . Models that apply quantization to IR, such as B5, perform poorer in terms of UAR, compared to SSL-SAS. Switching from T12-soft to T6 causes the most noticeable increase in UAR, showing that emotion information is sensitive, more so even than linguistic content, to quantization. Similar behavior is observed for ECAPA-Anon (+11.07 points) as well as ECAPA-Orig (+5.61 points).\n\nFocusing on per-emotion breakdown of UAR, we see that there is a huge variation between different emotion classes, unlike the performance on the original data. Overall, SER eval fails to identify sad and neutral utterances once they are processed. Upon checking the confusion matrices, we observe that 49% of the sad utterances are classified as angry after they are anonymized with SSL-SAS. Furthermore, the recall of angry samples is higher for SSL-SAS (77.65%) than for the original data (72.82%), corroborating the observation that anonymization causes the utterances to be perceived as angry by SER eval .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Speaker Embeddings Capturing Emotion Data",
      "text": "Changing the identity using selec-anon has a smaller impact on the emotion recognition results than the soft-encoding approach (T6: -2.85 points, T12-soft: -8.31). None-T6, the experiment without any speaker embedding in the IR, results in a UAR and WER worse than ECAPA-Orig-T6 (-5.38 points) and ECAPA-Anon-T6 (-2.53 points). This shows that HuBERT T6 and F0 alone are not sufficient for high-quality resynthesis. Interestingly, None-T6 attains the best recall for happy utterances among all experiments.\n\nIncluding the corresponding utterance-level GST embedding yields a similar UAR to the ECAPA experiment, showing that speaker embeddings of both generative and discriminative kind contain some emotion information. However, GST-Orig-T12-soft exhibits a slightly more balanced emotion recognition performance than ECAPA-Orig-T12-soft, which attains recall on par with the original data for angry, happy and neutral samples, yet performs far worse for sad utterances (ECAPA: 12.86% vs. GST: 29.28%).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Mode Collapse Of The Gan-Based Vocoders",
      "text": "Notably, the WER results on IEMOCAP-test reveal a significant difference in performance, compared to Libri-test (25% vs. 1.84%). This \"base effect\" is explainable by two factors: 1) domain mismatch, since ASR eval is trained on reading speech; and 2) the presence of interfering speakers and noise in IEMOCAP dataset. However, the additional 10 point difference hints that processing deteriorates the intelligibility of IEMOCAP utterances in a way that does not occur for Libri utterances. Nevertheless, further research is necessary to attribute this difference to underlying reason(s).\n\nTurning to emotion recognition, using a different HuBERT layer, namely, T1 instead of T6 or T12-soft, achieves a UAR of 64.78%. The recalls of sad and neutral utterances are still lower than that for happy and angry utterances, which are on par with the performance on original data. This shows that the mode collapse hypothesis has a somewhat limited effect on the emotion recognition performance, since HiFiGAN with original training loss as in  [28]  can achieve decent UAR and per-class recall, if an IR that contains emotionrelated information is used.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. What Happens To Emotion-Related Acoustic Features?",
      "text": "In this section, we investigate if there is a systematic change in the emotion-related acoustic features (see Sec. II-C). We use the following methodology:\n\n1) We identify the voice-active segments of the original IEMOCAP-test and Libri-test samples, using  [29] . 2) We compute the following acoustic features on the 'oracle' voice active segments for each experiment:\n\n• Spectral Centroid • Spectral Kurtosis • Loudness per ITU-R BS.1770-4 as implemented in  [30]  Due to space constraints, we only provide an aggregated summary of the acoustic features in Fig.  4 . For randomly selected individual audio samples and their corresponding acoustic feature plots, see our complementary website 3 .\n\n1) Spectral Centroid: For Libri-test samples, median spectral centroids are found to be approximately 1100 Hz. For all experiments, the higher tail of the distribution moved up, to around 6 kHz, likely due to vocoder artifacts. The experiments except ECAPA-Anon-T12-soft, i.e., SSL-SAS, still have a single modality with comparable median values, whereas for that experiment we see that the distribution has a greater elongation and a second modality is barely identifiable. IEMOCAP samples exhibit a similar pattern, except the lower end of the tail does not go as low as 60 Hz, likely due to increased microphone noise in IEMOCAP utterances.\n\n2) Spectral Kurtosis: This acoustic feature reveals the most interesting insights. For original Libri-test samples, median spectral kurtoses are found to be 6.3. The experiments with mnemonics None-T6 and ECAPA-Anon are found to have slightly higher kurtosis values, while the rest have comparable behavior to the original. In contrast, original IEMOCAP-test samples have a lower median spectral kurtosis (5.5), and lower high outliers (80) than the original Libri-test samples. Experiments except None-T1 result in a greater medians and greater high outliers. Using HuBERT T12-soft in IR introduces non-smooth behavior for both datasets.\n\nPlotting the spectral kurtosis distributions across different emotion classes, we see that for None-T1, spectral kurtosis medians are ordered from highest to lowest as angry, happy, neutral, and sad. For other experiments, the shapes and the median values become similar and the ordering between medians differ from experiment to experiment. We interpret this as a sign of lost emotion-related acoustic information due to further processing in other experiments. This figure is included in the supplementary material, available on 3 .\n\n3) Loudness: Original Libri-test samples have a smaller loudness variation than IEMOCAP samples. For both datasets, using HuBERT T12-soft reduces the median loudness.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Limitations Of Our Study",
      "text": "We have identified 16 samples in IEMOCAP-test, for which the VAD did not yield any segments with speech activity. We noticed that these samples either have a lot of noise, interfering speech, or there is no active speech. A collection of the problematic samples is available on our complementary website 3 . For a more reliable evaluation we suggest the VPC organizers to publish a cleaned dataset. Alternatively, researchers could validate on a different dataset such as RAVDESS  [31] .\n\nIn addition, we used GST in the way it is introduced in  [13] , i.e., trained together with Tacotron, so it learns to encode what is missing from Tacotron's inputs. One could also train or fine-tune GST along with SSL-SAS, which should then improve the performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "We have investigated the impact of numerous design choices in speaker anonymization systems on emotion preservation. To grasp the underlying problems, we used a more comprehensive evaluation than the official VPC 2024 evaluation protocol, where we gradually reduced the available information in the IR. Insufficient emotion information in the IR has been identified as the main problem. Quantization, or other approaches such as soft-encoding, limit the emotion information in the IR. Moreover, speaker embeddings, especially if learned in a generative context, contain emotion information. The impact of the vocoder (HiFiGAN) is limited compared to the other two hypotheses.\n\nBased on our results, we think that finding ways to distill emotion information from self-supervised speech representations while preserving the privacy benefits is the main challenge. Compensating or sanitizing the speaker embeddings such that they do not influence the emotion aspect of speech signals, and improving the OOD performance of the vocoder is desirable to reach the full potential.\n\nFurthermore, our findings indicate evaluating on a different emotional speech dataset may be more appropriate, as numerous sources of noise and interference are present in IEMOCAP utterances and thus confound the experiment results. In future work, we aim to propose an improved speaker anonymization utilizing the findings of this study.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. References",
      "text": "[1] Pierre Champion et al., 3rd VoicePrivacy challenge evaluation plan (version 2.0), 2024.\n\n[2] S. Meyer, F. Lux, J. Koch, P. Denisov, P. Tilli, and N. T. Vu, \"Prosody is not identity: A speaker anonymization approach using prosody cloning,\" in Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2023.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The signal flow diagram of the SSL-SAS system.",
      "page": 2
    },
    {
      "caption": "Figure 1: First, three feature extractors encode distinct components of speech.",
      "page": 2
    },
    {
      "caption": "Figure 2: Privacy evaluation for speaker anonymization. Top: Unprotected",
      "page": 2
    },
    {
      "caption": "Figure 3: Utility evaluation for speaker anonymization. Left: Emotion recogni-",
      "page": 2
    },
    {
      "caption": "Figure 2: ) performs an attack according to attack models",
      "page": 2
    },
    {
      "caption": "Figure 3: ), we use VPC2024 ASReval to",
      "page": 2
    },
    {
      "caption": "Figure 4: For randomly selected individual",
      "page": 3
    },
    {
      "caption": "Figure 4: Distributions of emotion-related acoustic features per experiment. Spectral centroid and spectral kurtosis are plotted in log-scale. Top: on IEMOCAP-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "† Trinity College Dublin,\nIreland"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Abstract—Disentanglement-based speaker anonymization involves de-\nreconstructions do not necessarily preserve pitch, and contain artifacts"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "composing speech into a semantically meaningful representation, altering"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "[6]. The authors of [7] discovered that unified vocoders largely ignore"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "the speaker embedding, and resynthesizing a waveform using a neural"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "speaker conditioning and cause a phenomenon called vocoder drift,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "vocoder.\nState-of-the-art\nsystems\nof\nthis\nkind\nare\nknown\nto\nremove"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "by measuring the discrepancy between the intended pseudo-speaker"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion information. Possible\nreasons\ninclude mode\ncollapse\nin GAN-"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "and the speaker embedding extracted from the output\nspeech.\nIn a\nbased\nvocoders,\nunintended modeling\nand modification\nof\nemotions"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "through speaker embeddings, or excessive sanitization of the intermediate\nsimilar\nspirit,\nin this paper, we perform a\ncomprehensive\nablation"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "representation (IR). In this paper, we conduct a comprehensive evaluation"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "study on SSL-SAS, a state-of-the-art speaker anonymization system,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "of\na\nstate-of-the-art\nspeaker\nanonymization system to understand the"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "to understand why emotion information is lost."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "underlying\ncauses. We\nconclude\nthat\nthe main reason is\nthe\nlack of"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion-related information in the IR. The speaker embeddings also have"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "II. HYPOTHESES"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "a high impact,\nif\nthey are learned in a generative context. The vocoder’s"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "out-of-distribution performance has a smaller impact. Additionally, we"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Based on our literature review, we identified three potential culprits"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "discovered\nthat\nsynthesis\nartifacts\nincrease\nspectral\nkurtosis,\nbiasing"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "that cause emotion information to be lost."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion recognition evaluation towards classifying utterances as angry."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Therefore, we conclude that reporting unweighted average recall alone"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "A. Mode collapse of\nthe GAN-based vocoders"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "for emotion recognition performance is suboptimal."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Index Terms—speaker anonymization, neural vocoders,\nspeaker\nem-"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The first possibility is\nthe design and training of\nthe vocoders,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "beddings, speech foundation models"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "which\nconvert\nthe\nIR to\na\nspeech waveform.\nSystems\nsuch\nas"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "[4]\nand\n[5]\nuse\na\njoint HiFi-GAN,\ntrained\non\nthe\nreconstruction"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "I.\nINTRODUCTION"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "of\nread\nspeech\n(LibriTTS). The\nadopted methodology\n(generative"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Speaker anonymization is\nthe task of modifying a speech signal"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "adversarial networks)\nis known to suffer\nfrom mode collapse, where"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "such\nthat\nthe\nspeaker\nidentity,\ni.e,\ntimbre,\nis\nchanged\nyet\nother"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "the model\nlearns\nto produce\na\nlimited subset of possible outputs."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "information such as\nlinguistic content and emotions are preserved."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The composition of\nthe training dataset,\nthe lack of\ninductive biases"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Disentanglement-based speaker anonymization consists of obtaining"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "in the system, and the utilized reconstruction loss might hinder\nthe"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "a\nsemantically meaningful\nspeech\nrepresentation, modifying\nit\nto"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "system from generalizing to out-of-distribution (OOD) samples, such"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "alter the identity, and re-synthesizing a waveform. Numerous systems"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "as expressive speech. Several works [8]–[12] identified shortcomings"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "developed in this spirit have been shown to be successful,\nin terms"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "and proposed mitigations to improve OOD performance."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "of ensuring privacy meanwhile maintaining intelligibility in the main"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "B.\nSpeaker embeddings capturing emotion data\nevent of\nthis discipline,\nthe VoicePrivacy Challenge (VPC)\n[1]."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The VPC 2024 evaluation plan [1] shows that\nthe most successful"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The penultimate\nlayer of\nautomated speaker verification (ASV)"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "systems in the literature, such as STTTS (B3)\n[2],\nthe neural audio"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "systems\n(e.g., ECAPA-TDNN [14])\nyields\na\nspeaker\nembedding."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "codec-based\nsystem (B4)\n[3],\nand\nthe\nvector\nquantization-based"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Speaker\nanonymization\nsystems\nthen\ngenerate\na\nnew embedding,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "system (B5)\n[4] have a common shortcoming:\nthe loss of emotion"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "called a pseudo-identity,\nto condition the\nspeech synthesis. Recent"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "information\nthroughout\nthe\nprocess,\noutlined\nby\nthe\ngap\nbetween"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "work [15],\n[16] has identified that\nthe speaker embeddings obtained"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion recognition performance (UAR,\nsee Sec.\nIII-C for details)"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "in this\nfashion do not\nserve well\nas\nconditioning information for"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "on original and anonymized utterances. SSL-SAS, a system that uses"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "speech synthesis. Limitations include a tendency to capture undesired"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "self-supervised speech representations, also suffers from this problem"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion information and an indifference to intra-speaker variations."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "[5]. Loss of emotion information during speaker anonymization has"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The\nreported attempts\nto compensate\nthe\nspeaker\nembeddings\nfor"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "significant\nimplications for, e.g., medical and legal use cases, where"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotion\ninformation were\nonly\npartially\nable\nto\nbridge\nthe\ngap"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "emotional\ncues may be detrimental\nfor diagnosis or\nevidence. By"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "between the unweighted average recall\n(UAR) scores of\nthe original"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "understanding the underlying causes, we can develop anonymization"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "and anonymized utterances [5].\nIn particular,\nthe reported recall\nfor"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "systems that preserve emotions, enabling more reliable applications."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "sad utterances is still very low, motivating further\nresearch."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Current\nevaluation methodologies,\ne.g.,\nthe VPC 2024\nevalua-"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "C. Discretization of\nthe intermediate representation\ntion plan [1],\nare not\nspecifically designed to identify problematic"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "block(s) of disentanglement-based anonymization systems. In partic-"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The\ncomponents of\nIR (see Tab.\nI) other\nthan the\nspeaker\nem-"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "ular,\nthe interplay between the intermediate representation (IR) and"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "beddings may be the culprit. We observe that\nspeaker\ninformation"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "the vocoder\nis challenging to characterize. By performing ablation"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "is\nremoved from the IR by discretizing, either via quantization [3],"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "studies, we\ncan\ngain\nfurther\ninsights\ninto\nthe\nsystem design. For"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "[4], phonetic transcriptions [2], or soft-encoding [5]. On the contrary,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "example,\nbypassing\nthe\nanonymization\nblock\nhas\nshown\nthat\nthe"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "works such as [17] discovered that\nindirect emotion supervision, e.g.,"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "by controlling emotions\nthrough some continuous acoustic features"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "The\nInternational Audio Laboratories Erlangen are\na\njoint\ninstitution of"
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "such as spectral centroids, spectral kurtosis, loudness, or ∆F0 is more\nthe Friedrich-Alexander-Universtit¨at Erlangen-N¨urnberg and Fraunhofer\nIIS."
        },
        {
          "∗ International Audio Laboratories Erlangen, Friedrich-Alexander-University Erlangen-N¨urnberg, Germany": "Corresponding author: ege.gaznepoglu@audiolabs-erlangen.de\neffective than providing explicit supervision via discrete labels."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Downstream-related representation"
        },
        {
          "TABLE I": "Phonetic transcriptions (ling.) + Prosody representation (prosody)"
        },
        {
          "TABLE I": "Semantic tokens (ling.)"
        },
        {
          "TABLE I": "Vector-quantized and post-processed wav2vec embeddings (ling.) + F0 (prosody)"
        },
        {
          "TABLE I": "HuBERT-based soft-encoded embeddings (ling.) + F0 (prosody)"
        },
        {
          "TABLE I": "trans-"
        },
        {
          "TABLE I": "ASVeval"
        },
        {
          "TABLE I": "[18].\nIn"
        },
        {
          "TABLE I": "Original speech\nOriginal speech\nthe\nlin-"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Anon.(a)\nAnon.(b)\nASVeval"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Original speech\nOriginal speech"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Fig.\n2.\nPrivacy\nevaluation\nfor\nspeaker\nanonymization. Top: Unprotected"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "case,\nfor\nreference. Bottom: The lazy-informed attack model, comparing the"
        },
        {
          "TABLE I": "resulting speech for\ntwo separate invocations of\nthe anonymization system."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "VALUES INDICATE TOP-SCORING EXPERIMENTS. RESULTS FOR ORIGINAL DATA AND VPC2024 BASELINES B3-B5 ARE INCLUDED FOR REFERENCE."
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": ""
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": ""
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "HuBERT Layer"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "Original data"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "B3"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "B4"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "B5"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T1"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T6"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T12"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T12"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T6"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T6"
        },
        {
          "VPC2024 EVALUATION RESULTS OF OUR EXPERIMENTS ON LIBRI-TEST (EER, WER) AND IEMOCAP-TEST (WER, RECALL, UAR). BOLD-FACE": "T12"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "IV. RESULTS",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "C. Mode collapse of\nthe GAN-based vocoders"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "Notably,\nthe WER results on IEMOCAP-test\nreveal a significant"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "A. Discretization of\nthe intermediate representation",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "difference in performance, compared to Libri-test\n(25% vs. 1.84%)."
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "This ”base effect” is explainable by two factors: 1) domain mismatch,"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "We\nsummarize\nthe\nresults\nof\nour\nexperiments\nin\nTable",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "is trained on reading speech; and 2)\nthe presence of\nsince ASReval"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "II.\nThe\nresults\nfor\nSSL-SAS\n(lowest\nrow,\nmnemonic",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "interfering speakers and noise in IEMOCAP dataset. However,\nthe"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "ECAPA-Anon-T12-soft)\nalign\nwith\n[5],\nand\nappear\nmore",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "additional 10 point difference hints\nthat processing deteriorates\nthe"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "promising\nthan\nfor VPC baselines B3-B5\n[1]. Models\nthat\napply",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "intelligibility of\nIEMOCAP utterances in a way that does not occur"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "quantization to IR,\nsuch as B5, perform poorer\nin terms of UAR,",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "for Libri utterances. Nevertheless,\nfurther\nresearch is necessary to"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "compared to SSL-SAS. Switching from T12-soft\nto T6 causes\nthe",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "attribute this difference to underlying reason(s)."
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "most noticeable increase in UAR, showing that emotion information",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "Turning to emotion recognition, using a different HuBERT layer,"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "is\nsensitive, more\nso even than linguistic\ncontent,\nto quantization.",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "namely, T1 instead of T6 or T12-soft, achieves a UAR of 64.78%."
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "Similar behavior\nis observed for ECAPA-Anon (+11.07 points)\nas",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "The recalls of sad and neutral utterances are still\nlower\nthan that\nfor"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "well as ECAPA-Orig (+5.61 points).",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "happy and angry utterances, which are on par with the performance"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "Focusing on per-emotion breakdown of UAR, we see that\nthere",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "on original data. This\nshows\nthat\nthe mode collapse hypothesis has"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "is\na\nhuge\nvariation\nbetween\ndifferent\nemotion\nclasses,\nunlike\nthe",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "a somewhat\nlimited effect on the emotion recognition performance,"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "fails to identify\nperformance on the original data. Overall, SEReval",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "since HiFiGAN with original\ntraining loss\nas\nin [28]\ncan achieve"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "sad and neutral utterances once they are processed. Upon checking",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "decent UAR and per-class\nrecall,\nif\nan IR that\ncontains\nemotion-"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "the confusion matrices, we observe that 49% of\nthe sad utterances",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "related information is used."
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "are\nclassified as\nangry after\nthey are\nanonymized with SSL-SAS.",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "Furthermore,\nthe\nrecall\nof\nangry\nsamples\nis\nhigher\nfor SSL-SAS",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "D. What happens to emotion-related acoustic features?"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "(77.65%)\nthan\nfor\nthe\noriginal\ndata\n(72.82%),\ncorroborating\nthe",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "In this\nsection, we investigate if\nthere is a systematic change in"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "observation that anonymization causes the utterances to be perceived",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": ""
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "the\nemotion-related\nacoustic\nfeatures\n(see Sec.\nII-C). We\nuse\nthe"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "as angry by SEReval.",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "following methodology:"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "1) We\nidentify\nthe\nvoice-active\nsegments\nof\nthe\noriginal"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "B.\nSpeaker embeddings capturing emotion data",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "IEMOCAP-test and Libri-test samples, using [29]."
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "2) We compute the following acoustic features on the ’oracle’ voice"
        },
        {
          "24.27\n27.17\nECAPA-Anon\nT12\nsoft-label": "Changing the identity using selec-anon has a smaller\nimpact",
          "77.65\n2.42\n38.85\n3.42\n46.71\n52.98\n45.19": "active segments for each experiment:"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "Spectral Centroid [Hz]",
          "Spectral Kurtosis": "Spectral Kurtosis",
          "Loudness [LKFS]": "Loudness [LKFS]"
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "",
          "Spectral Kurtosis": "",
          "Loudness [LKFS]": ""
        },
        {
          "Spectral Centroid [Hz]": "None-T6",
          "Spectral Kurtosis": "None-T6",
          "Loudness [LKFS]": "None-T6"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": ""
        },
        {
          "thus": "propose",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "an improved speaker",
          "aim to": "anonymization utilizing the findings"
        },
        {
          "thus": "of",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "this study.",
          "aim to": ""
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": ""
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "VI. REFERENCES",
          "aim to": ""
        },
        {
          "thus": "[1]",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": "Pierre Champion et al., 3rd VoicePrivacy challenge evaluation"
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "plan (version 2.0), 2024.",
          "aim to": ""
        },
        {
          "thus": "[2]",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": "S. Meyer, F. Lux, J. Koch, P. Denisov, P. Tilli, and N. T. Vu,"
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "“Prosody is not",
          "aim to": "identity: A speaker anonymization approach"
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": "using prosody cloning,” in Proc. IEEE Intl. Conf. on Acoustics,"
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "Speech and Signal Processing (ICASSP), 2023.",
          "aim to": ""
        },
        {
          "thus": "",
          "confound\nthe\nexperiment\nresults.\nIn\nfuture work, we": "",
          "aim to": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "i\ni\nr\nr": "g N\ng N\no\no",
          "i\nr": "g N\no"
        },
        {
          "i\ni\nr\nr": "GST-Orig-T12-soft\nGST-Orig-T12-soft\nECAPA-Orig-T12-soft\nECAPA-Orig-T12-soft\nECAPA-Anon-T12-soft",
          "i\nr": "GST-Orig-T12-soft\nECAPA-Orig-T12-soft\nECAPA-Anon-T12-soft\nECAPA-Anon-T12-soft"
        },
        {
          "i\ni\nr\nr": "Fig. 4. Distributions of emotion-related acoustic features per experiment. Spectral centroid and spectral kurtosis are plotted in log-scale. Top: on IEMOCAP-",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "test, Bottom: on Libri-test",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "we see that\nthe distribution has a greater elongation and a second",
          "i\nr": "from Tacotron’s inputs. One could also train or fine-tune GST along"
        },
        {
          "i\ni\nr\nr": "modality is barely identifiable.\nIEMOCAP samples exhibit a similar",
          "i\nr": "with SSL-SAS, which should then improve the performance."
        },
        {
          "i\ni\nr\nr": "pattern, except\nthe lower end of the tail does not go as low as 60 Hz,",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "V. CONCLUSION"
        },
        {
          "i\ni\nr\nr": "likely due to increased microphone noise in IEMOCAP utterances.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "We have investigated the impact of numerous design choices\nin"
        },
        {
          "i\ni\nr\nr": "2)\nSpectral Kurtosis:\nThis\nacoustic\nfeature\nreveals\nthe most\nin-",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "speaker\nanonymization systems on emotion preservation. To grasp"
        },
        {
          "i\ni\nr\nr": "teresting insights. For original Libri-test\nsamples, median spectral",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "the underlying problems, we used a more comprehensive evaluation"
        },
        {
          "i\ni\nr\nr": "kurtoses\nare\nfound\nto\nbe\n6.3. The\nexperiments with mnemonics",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "than the official VPC 2024 evaluation protocol, where we gradually"
        },
        {
          "i\ni\nr\nr": "None-T6 and ECAPA-Anon are\nfound\nto\nhave\nslightly\nhigher",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "reduced\nthe\navailable\ninformation\nin\nthe\nIR.\nInsufficient\nemotion"
        },
        {
          "i\ni\nr\nr": "kurtosis\nvalues, while\nthe\nrest\nhave\ncomparable\nbehavior\nto\nthe",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "information in the IR has been identified as the main problem. Quan-"
        },
        {
          "i\ni\nr\nr": "original.\nIn contrast, original\nIEMOCAP-test\nsamples have a lower",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "tization, or other approaches such as soft-encoding,\nlimit\nthe emotion"
        },
        {
          "i\ni\nr\nr": "median spectral kurtosis (5.5), and lower high outliers (80)\nthan the",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "information in the IR. Moreover,\nspeaker embeddings, especially if"
        },
        {
          "i\ni\nr\nr": "original Libri-test samples. Experiments except None-T1 result\nin a",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "learned in a generative\ncontext,\ncontain emotion information. The"
        },
        {
          "i\ni\nr\nr": "greater medians and greater high outliers. Using HuBERT T12-soft",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "impact of\nthe vocoder\n(HiFiGAN)\nis limited compared to the other"
        },
        {
          "i\ni\nr\nr": "in IR introduces non-smooth behavior\nfor both datasets.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "two hypotheses."
        },
        {
          "i\ni\nr\nr": "Plotting the spectral kurtosis distributions across different emotion",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "Based on our results, we think that finding ways to distill emotion"
        },
        {
          "i\ni\nr\nr": "classes, we\nsee\nthat\nfor None-T1,\nspectral\nkurtosis medians\nare",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "information from self-supervised speech representations while pre-"
        },
        {
          "i\ni\nr\nr": "ordered from highest\nto lowest\nas\nangry, happy, neutral,\nand sad.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "serving the privacy benefits is the main challenge. Compensating or"
        },
        {
          "i\ni\nr\nr": "For other\nexperiments,\nthe\nshapes\nand the median values become",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "sanitizing the\nspeaker\nembeddings\nsuch that\nthey do not\ninfluence"
        },
        {
          "i\ni\nr\nr": "similar\nand the ordering between medians differ\nfrom experiment",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "the\nemotion\naspect\nof\nspeech\nsignals,\nand\nimproving\nthe OOD"
        },
        {
          "i\ni\nr\nr": "to experiment. We\ninterpret\nthis\nas\na\nsign of\nlost\nemotion-related",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "performance of\nthe vocoder\nis desirable to reach the full potential."
        },
        {
          "i\ni\nr\nr": "acoustic information due to further processing in other experiments.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "Furthermore, our findings indicate evaluating on a different emo-"
        },
        {
          "i\ni\nr\nr": "This figure is included in the supplementary material, available on3.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "tional speech dataset may be more appropriate, as numerous sources"
        },
        {
          "i\ni\nr\nr": "3) Loudness: Original Libri-test samples have a smaller\nloudness",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "of noise and interference are present\nin IEMOCAP utterances and"
        },
        {
          "i\ni\nr\nr": "variation than IEMOCAP samples. For both datasets, using HuBERT",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "thus\nconfound\nthe\nexperiment\nresults.\nIn\nfuture work, we\naim to"
        },
        {
          "i\ni\nr\nr": "T12-soft\nreduces the median loudness.",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "propose\nan improved speaker\nanonymization utilizing the findings"
        },
        {
          "i\ni\nr\nr": "",
          "i\nr": "of\nthis study."
        },
        {
          "i\ni\nr\nr": "E. Limitations of our study",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "We have identified 16 samples\nin IEMOCAP-test,\nfor which the",
          "i\nr": "VI. REFERENCES"
        },
        {
          "i\ni\nr\nr": "VAD did not yield any segments with speech activity. We noticed",
          "i\nr": "[1]\nPierre Champion et al., 3rd VoicePrivacy challenge evaluation"
        },
        {
          "i\ni\nr\nr": "that\nthese samples either have a lot of noise,\ninterfering speech, or",
          "i\nr": "plan (version 2.0), 2024."
        },
        {
          "i\ni\nr\nr": "there is no active speech. A collection of\nthe problematic samples",
          "i\nr": "[2]\nS. Meyer, F. Lux, J. Koch, P. Denisov, P. Tilli, and N. T. Vu,"
        },
        {
          "i\ni\nr\nr": "is\navailable\non\nour\ncomplementary website3. For\na more\nreliable",
          "i\nr": "“Prosody is not\nidentity: A speaker anonymization approach"
        },
        {
          "i\ni\nr\nr": "evaluation we\nsuggest\nthe VPC organizers\nto\npublish\na\ncleaned",
          "i\nr": "using prosody cloning,” in Proc. IEEE Intl. Conf. on Acoustics,"
        },
        {
          "i\ni\nr\nr": "dataset. Alternatively, researchers could validate on a different dataset",
          "i\nr": "Speech and Signal Processing (ICASSP), 2023."
        },
        {
          "i\ni\nr\nr": "such as RAVDESS [31].",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "In addition, we used GST in the way it\nis introduced in [13],\ni.e.,",
          "i\nr": ""
        },
        {
          "i\ni\nr\nr": "trained together with Tacotron, so it\nlearns to encode what\nis missing",
          "i\nr": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "anonymization\nusing\nneural\naudio\ncodec\nlanguage models,”",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Spoken Lang. Tech.\nfor prosody-related tasks,” in Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf. on Acoustics, Speech and Signal\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Workshop (SLT), 2023."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Processing (ICASSP), 2024.",
          "[18]": "[19]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "A. Pasad, B. Shi,\nand K. Livescu,\n“Comparative\nlayer-wise"
        },
        {
          "[3]": "[4]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "P. Champion, “Anonymizing speech: Evaluating and designing",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "analysis of self-supervised speech models,” in Proc. IEEE Intl."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker anonymization techniques,” Ph.D. dissertation, Univer-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Conf. on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "site de Lorraine, 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "2023."
        },
        {
          "[3]": "[5]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "X. Miao, Y. Zhang, X. Wang, N. Tomashenko, D. C. L. Soh,",
          "[18]": "[20]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "X. Miao,\nX. Wang,\nE.\nCooper,\nJ.\nYamagishi,\nand\nN."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "and I. Mcloughlin, “Adapting general disentanglement-based",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Tomashenko,\n“Language-independent\nspeaker\nanonymization"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker anonymization for enhanced emotion preservation,” in",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "approach using self-supervised pre-trained models,”\nin Odd-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "arXiv preprint, 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "ysey: Speaker and Lang. Recognition Workshop, 2022."
        },
        {
          "[3]": "[6]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "¨\nU. E. Gaznepoglu and N. Peters,\n“Evaluation of\nthe\nspeech",
          "[18]": "[21]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "resynthesis\ncapabilities of\nthe VoicePrivacy baseline b1,”\nin",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "dinov,\nand A. Mohamed,\n“HuBERT: Self-supervised speech"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Proc. 3rd Symp. on Security and Privacy in Speech Commu-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "representation learning by masked prediction of hidden units,”"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "nication, 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE/ACM Transactions\non Audio,\nSpeech,\nand\nLanguage"
        },
        {
          "[3]": "[7]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "M. Panariello, M. Todisco,\nand N. Evans,\n“Vocoder drift\nin",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Processing, vol. 29, 2021."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Interspeech\nx-vector-based speaker\nanonymization,”\nin Proc.",
          "[18]": "[22]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "B.\nvan Niekerk, M.-A.\nCarbonneau,\nJ.\nZa¨ıdi, M.\nBaas,"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Conf., 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "H. Seut´e,\nand H. Kamper,\n“A comparison\nof\ndiscrete\nand"
        },
        {
          "[3]": "[8]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "J. Pons, S. Pascual, G. Cengarle, and J. Serra, “Upsampling",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "soft\nspeech\nunits\nfor\nimproved\nvoice\nconversion,”\nin Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf.\nartifacts in neural audio synthesis,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "on Acoustics, Speech and Signal Processing (ICASSP), 2021.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "(ICASSP), 2022."
        },
        {
          "[3]": "[9]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "T. Bak,\nJ. Lee, H. Bae,\nJ. Yang,\nJ.-S. Bae,\nand Y.-S.\nJoo,",
          "[18]": "[23]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "S. A. Zahorian and H. Hu, “A spectral/temporal method for"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "“Avocodo: Generative\nadversarial\nnetwork\nfor\nartifact-free",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "robust\nfundamental\nfrequency tracking,” The Journal Acoust."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "vocoder,” in Proc. Conf. AAAI, 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Soc. of America, vol. 123, no. 6, 2008."
        },
        {
          "[3]": "[10]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "A. D´efossez, J. Copet, G. Synnaeve, and Y. Adi, “High fidelity",
          "[18]": "[24]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "X. Miao,\nX. Wang,\nE.\nCooper,\nJ.\nYamagishi,\nand\nN."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "neural audio compression,” Trans. on Machine Learning Re-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Tomashenko,\n“Language-independent\nspeaker\nanonymization"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "search (TMLR), 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE/ACM\nusing\northogonal\nhouseholder\nneural\nnetwork,”"
        },
        {
          "[3]": "[11]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon,",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Trans. Audio, Speech, Lang. Process., 2023."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "“BigVGAN: A universal neural vocoder with large-scale train-",
          "[18]": "[25]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "et\nF. Fang\nal.,\n“Speaker\nanonymization\nusing\nx-vector\nand"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "ing,” in Proc. Intl. Conf. on Learning Representations (ICLR),",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "neural waveform models,” in Proc. 10th ISCA Speech Synthesis"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Workshop, 2019."
        },
        {
          "[3]": "[12]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Y\n. Gu, X. Zhang, L. Xue, and Z. Wu, “Multi-scale sub-band",
          "[18]": "[26]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "V\n. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "constant-q transform discriminator\nfor high-fidelity vocoder,”",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "rispeech: An ASR corpus\nbased\non\npublic\ndomain\naudio"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf. on Acoustics, Speech and Signal\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Intl. Conf. on Acoustics, Speech and\nbooks,”\nin Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Processing (ICASSP), 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Signal Processing (ICASSP), 2015."
        },
        {
          "[3]": "[13]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Y\n. Wang et al., “Style tokens: Unsupervised style modeling,",
          "[18]": "[27]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "C. Busso et al., “IEMOCAP: Interactive emotional dyadic mo-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "control and transfer\nin end-to-end speech synthesis,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "tion capture database,” Language Resources and Evaluation,"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Intl. Conf. on Machine Learning (ICML), 2018.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "vol. 42, no. 4, 2008."
        },
        {
          "[3]": "[14]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-",
          "[18]": "[28]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversar-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "TDNN: Emphasized channel attention, propagation and aggre-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "ial networks\nfor efficient and high fidelity speech synthesis,”"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Inter-\ngation in TDNN based speaker verification,”\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "presented at\nthe Proc. NeurIPS Conf. 2020."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speech Conf., 2020.",
          "[18]": "[29]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Silero Team, Silero VAD: Pre-trained enterprise-grade voice"
        },
        {
          "[3]": "[15]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "I. R. Ulgen, C. Busso, J. H. L. Hansen, and B. Sisman, “We",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "activity detector (vad), number detector and language classi-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "need variations in speech synthesis: Sub-center modelling for",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "fier, https://github.com/snakers4/silero-vad, 2024."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker embeddings,” in arXiv preprint, 2024.",
          "[18]": "[30]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "“Torchaudio.functional.loudness — torchaudio 2.3.0 documen-"
        },
        {
          "[3]": "[16]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "I. R. Ulgen, Z. Du, C. Busso, and B. Sisman, “Revealing emo-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "tation.” (),\n[Online]. Available: https://pytorch.org/audio/2.3"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "tional clusters in speaker embeddings: A contrastive learning",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": ".0/generated/torchaudio.functional.loudness.html\n(visited on"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl.\nstrategy for\nspeech emotion recognition,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "09/10/2024)."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Conf. on Acoustics, Speech and Signal Processing (ICASSP),",
          "[18]": "[31]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "S. R. Livingstone\nand\nF. A. Russo,\n“The\nryerson\naudio-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "visual database of\nemotional\nspeech and song (RAVDESS):"
        },
        {
          "[3]": "[17]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S. Ghosh, A. Das, Y.\nSinha,\nI.\nSiegert,\nT.\nPolzehl,\nand",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "A dynamic, multimodal\nset of\nfacial\nand vocal\nexpressions"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S. Stober,\n“Emo-StarGAN: A semi-supervised\nany-to-many",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "in\nnorth\namerican\nenglish,” PLOS ONE,\nvol.\n13,\nno.\n5,\nJ."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "non-parallel\nemotion-preserving\nvoice\nconversion,”\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Najbauer, Ed., 2018."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Interspeech Conf., 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "anonymization\nusing\nneural\naudio\ncodec\nlanguage models,”",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Spoken Lang. Tech.\nfor prosody-related tasks,” in Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf. on Acoustics, Speech and Signal\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Workshop (SLT), 2023."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Processing (ICASSP), 2024.",
          "[18]": "[19]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "A. Pasad, B. Shi,\nand K. Livescu,\n“Comparative\nlayer-wise"
        },
        {
          "[3]": "[4]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "P. Champion, “Anonymizing speech: Evaluating and designing",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "analysis of self-supervised speech models,” in Proc. IEEE Intl."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker anonymization techniques,” Ph.D. dissertation, Univer-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Conf. on Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "site de Lorraine, 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "2023."
        },
        {
          "[3]": "[5]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "X. Miao, Y. Zhang, X. Wang, N. Tomashenko, D. C. L. Soh,",
          "[18]": "[20]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "X. Miao,\nX. Wang,\nE.\nCooper,\nJ.\nYamagishi,\nand\nN."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "and I. Mcloughlin, “Adapting general disentanglement-based",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Tomashenko,\n“Language-independent\nspeaker\nanonymization"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker anonymization for enhanced emotion preservation,” in",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "approach using self-supervised pre-trained models,”\nin Odd-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "arXiv preprint, 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "ysey: Speaker and Lang. Recognition Workshop, 2022."
        },
        {
          "[3]": "[6]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "¨\nU. E. Gaznepoglu and N. Peters,\n“Evaluation of\nthe\nspeech",
          "[18]": "[21]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "resynthesis\ncapabilities of\nthe VoicePrivacy baseline b1,”\nin",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "dinov,\nand A. Mohamed,\n“HuBERT: Self-supervised speech"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Proc. 3rd Symp. on Security and Privacy in Speech Commu-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "representation learning by masked prediction of hidden units,”"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "nication, 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE/ACM Transactions\non Audio,\nSpeech,\nand\nLanguage"
        },
        {
          "[3]": "[7]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "M. Panariello, M. Todisco,\nand N. Evans,\n“Vocoder drift\nin",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Processing, vol. 29, 2021."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Interspeech\nx-vector-based speaker\nanonymization,”\nin Proc.",
          "[18]": "[22]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "B.\nvan Niekerk, M.-A.\nCarbonneau,\nJ.\nZa¨ıdi, M.\nBaas,"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Conf., 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "H. Seut´e,\nand H. Kamper,\n“A comparison\nof\ndiscrete\nand"
        },
        {
          "[3]": "[8]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "J. Pons, S. Pascual, G. Cengarle, and J. Serra, “Upsampling",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "soft\nspeech\nunits\nfor\nimproved\nvoice\nconversion,”\nin Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf.\nartifacts in neural audio synthesis,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "on Acoustics, Speech and Signal Processing (ICASSP), 2021.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "(ICASSP), 2022."
        },
        {
          "[3]": "[9]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "T. Bak,\nJ. Lee, H. Bae,\nJ. Yang,\nJ.-S. Bae,\nand Y.-S.\nJoo,",
          "[18]": "[23]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "S. A. Zahorian and H. Hu, “A spectral/temporal method for"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "“Avocodo: Generative\nadversarial\nnetwork\nfor\nartifact-free",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "robust\nfundamental\nfrequency tracking,” The Journal Acoust."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "vocoder,” in Proc. Conf. AAAI, 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Soc. of America, vol. 123, no. 6, 2008."
        },
        {
          "[3]": "[10]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "A. D´efossez, J. Copet, G. Synnaeve, and Y. Adi, “High fidelity",
          "[18]": "[24]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "X. Miao,\nX. Wang,\nE.\nCooper,\nJ.\nYamagishi,\nand\nN."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "neural audio compression,” Trans. on Machine Learning Re-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Tomashenko,\n“Language-independent\nspeaker\nanonymization"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "search (TMLR), 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE/ACM\nusing\northogonal\nhouseholder\nneural\nnetwork,”"
        },
        {
          "[3]": "[11]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon,",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Trans. Audio, Speech, Lang. Process., 2023."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "“BigVGAN: A universal neural vocoder with large-scale train-",
          "[18]": "[25]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "et\nF. Fang\nal.,\n“Speaker\nanonymization\nusing\nx-vector\nand"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "ing,” in Proc. Intl. Conf. on Learning Representations (ICLR),",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "neural waveform models,” in Proc. 10th ISCA Speech Synthesis"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Workshop, 2019."
        },
        {
          "[3]": "[12]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Y\n. Gu, X. Zhang, L. Xue, and Z. Wu, “Multi-scale sub-band",
          "[18]": "[26]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "V\n. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "constant-q transform discriminator\nfor high-fidelity vocoder,”",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "rispeech: An ASR corpus\nbased\non\npublic\ndomain\naudio"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl. Conf. on Acoustics, Speech and Signal\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "IEEE Intl. Conf. on Acoustics, Speech and\nbooks,”\nin Proc."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Processing (ICASSP), 2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Signal Processing (ICASSP), 2015."
        },
        {
          "[3]": "[13]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Y\n. Wang et al., “Style tokens: Unsupervised style modeling,",
          "[18]": "[27]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "C. Busso et al., “IEMOCAP: Interactive emotional dyadic mo-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "control and transfer\nin end-to-end speech synthesis,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "tion capture database,” Language Resources and Evaluation,"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Intl. Conf. on Machine Learning (ICML), 2018.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "vol. 42, no. 4, 2008."
        },
        {
          "[3]": "[14]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-",
          "[18]": "[28]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversar-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "TDNN: Emphasized channel attention, propagation and aggre-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "ial networks\nfor efficient and high fidelity speech synthesis,”"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Inter-\ngation in TDNN based speaker verification,”\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "presented at\nthe Proc. NeurIPS Conf. 2020."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speech Conf., 2020.",
          "[18]": "[29]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Silero Team, Silero VAD: Pre-trained enterprise-grade voice"
        },
        {
          "[3]": "[15]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "I. R. Ulgen, C. Busso, J. H. L. Hansen, and B. Sisman, “We",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "activity detector (vad), number detector and language classi-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "need variations in speech synthesis: Sub-center modelling for",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "fier, https://github.com/snakers4/silero-vad, 2024."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "speaker embeddings,” in arXiv preprint, 2024.",
          "[18]": "[30]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "“Torchaudio.functional.loudness — torchaudio 2.3.0 documen-"
        },
        {
          "[3]": "[16]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "I. R. Ulgen, Z. Du, C. Busso, and B. Sisman, “Revealing emo-",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "tation.” (),\n[Online]. Available: https://pytorch.org/audio/2.3"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "tional clusters in speaker embeddings: A contrastive learning",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": ".0/generated/torchaudio.functional.loudness.html\n(visited on"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "IEEE Intl.\nstrategy for\nspeech emotion recognition,” in Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "09/10/2024)."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Conf. on Acoustics, Speech and Signal Processing (ICASSP),",
          "[18]": "[31]",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "S. R. Livingstone\nand\nF. A. Russo,\n“The\nryerson\naudio-"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "2024.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "visual database of\nemotional\nspeech and song (RAVDESS):"
        },
        {
          "[3]": "[17]",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S. Ghosh, A. Das, Y.\nSinha,\nI.\nSiegert,\nT.\nPolzehl,\nand",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "A dynamic, multimodal\nset of\nfacial\nand vocal\nexpressions"
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "S. Stober,\n“Emo-StarGAN: A semi-supervised\nany-to-many",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "in\nnorth\namerican\nenglish,” PLOS ONE,\nvol.\n13,\nno.\n5,\nJ."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "non-parallel\nemotion-preserving\nvoice\nconversion,”\nin Proc.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": "Najbauer, Ed., 2018."
        },
        {
          "[3]": "",
          "M. Panariello, F. Nespoli, M. Todisco, and N. Evans, “Speaker": "Interspeech Conf., 2023.",
          "[18]": "",
          "et\nG.-T. Lin\nal.,\n“On\nthe\nutility\nof\nself-supervised models": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speaker anonymization using neural audio codec language models",
      "authors": [
        "M Panariello",
        "F Nespoli",
        "M Todisco",
        "N Evans"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Anonymizing speech: Evaluating and designing speaker anonymization techniques",
      "authors": [
        "P Champion"
      ],
      "year": "2024",
      "venue": "Anonymizing speech: Evaluating and designing speaker anonymization techniques"
    },
    {
      "citation_id": "3",
      "title": "Adapting general disentanglement-based speaker anonymization for enhanced emotion preservation",
      "authors": [
        "X Miao",
        "Y Zhang",
        "X Wang",
        "N Tomashenko",
        "D Soh",
        "I Mcloughlin"
      ],
      "year": "2024",
      "venue": "Adapting general disentanglement-based speaker anonymization for enhanced emotion preservation"
    },
    {
      "citation_id": "4",
      "title": "Evaluation of the speech resynthesis capabilities of the VoicePrivacy baseline b1",
      "authors": [
        "Ü Gaznepoglu",
        "N Peters"
      ],
      "year": "2023",
      "venue": "Proc. 3rd Symp. on Security and Privacy in Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "Vocoder drift in x-vector-based speaker anonymization",
      "authors": [
        "M Panariello",
        "M Todisco",
        "N Evans"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech Conf"
    },
    {
      "citation_id": "6",
      "title": "Upsampling artifacts in neural audio synthesis",
      "authors": [
        "J Pons",
        "S Pascual",
        "G Cengarle",
        "J Serra"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Avocodo: Generative adversarial network for artifact-free vocoder",
      "authors": [
        "T Bak",
        "J Lee",
        "H Bae",
        "J Yang",
        "J.-S Bae",
        "Y.-S Joo"
      ],
      "year": "2023",
      "venue": "Proc. Conf. AAAI"
    },
    {
      "citation_id": "8",
      "title": "High fidelity neural audio compression",
      "authors": [
        "A Défossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "venue": "Trans. on Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "BigVGAN: A universal neural vocoder with large-scale training",
      "authors": [
        "W S.-G. Lee",
        "B Ping",
        "B Ginsburg",
        "S Catanzaro",
        "Yoon"
      ],
      "venue": "Proc. Intl. Conf. on Learning Representations (ICLR)"
    },
    {
      "citation_id": "10",
      "title": "Multi-scale sub-band constant-q transform discriminator for high-fidelity vocoder",
      "authors": [
        "Y Gu",
        "X Zhang",
        "L Xue",
        "Z Wu"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang"
      ],
      "year": "2018",
      "venue": "Proc. Intl. Conf. on Machine Learning (ICML)"
    },
    {
      "citation_id": "12",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech Conf"
    },
    {
      "citation_id": "13",
      "title": "We need variations in speech synthesis: Sub-center modelling for speaker embeddings",
      "authors": [
        "I Ulgen",
        "C Busso",
        "J Hansen",
        "B Sisman"
      ],
      "year": "2024",
      "venue": "We need variations in speech synthesis: Sub-center modelling for speaker embeddings"
    },
    {
      "citation_id": "14",
      "title": "Revealing emotional clusters in speaker embeddings: A contrastive learning strategy for speech emotion recognition",
      "authors": [
        "I Ulgen",
        "Z Du",
        "C Busso",
        "B Sisman"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Emo-StarGAN: A semi-supervised any-to-many non-parallel emotion-preserving voice conversion",
      "authors": [
        "S Ghosh",
        "A Das",
        "Y Sinha",
        "I Siegert",
        "T Polzehl",
        "S Stober"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech Conf"
    },
    {
      "citation_id": "16",
      "title": "On the utility of self-supervised models for prosody-related tasks",
      "authors": [
        "G.-T Lin"
      ],
      "venue": "Proc. IEEE Spoken Lang. Tech. Workshop (SLT)"
    },
    {
      "citation_id": "17",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "A Pasad",
        "B Shi",
        "K Livescu"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Language-independent speaker anonymization approach using self-supervised pre-trained models",
      "authors": [
        "X Miao",
        "X Wang",
        "E Cooper",
        "J Yamagishi",
        "N Tomashenko"
      ],
      "year": "2022",
      "venue": "Oddysey: Speaker and Lang"
    },
    {
      "citation_id": "19",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "A comparison of discrete and soft speech units for improved voice conversion",
      "authors": [
        "B Van Niekerk",
        "M.-A Carbonneau",
        "J Zaïdi",
        "M Baas",
        "H Seuté",
        "H Kamper"
      ],
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "A spectral/temporal method for robust fundamental frequency tracking",
      "authors": [
        "S Zahorian",
        "H Hu"
      ],
      "year": "2008",
      "venue": "The Journal Acoust. Soc. of America"
    },
    {
      "citation_id": "22",
      "title": "Language-independent speaker anonymization using orthogonal householder neural network",
      "authors": [
        "X Miao",
        "X Wang",
        "E Cooper",
        "J Yamagishi",
        "N Tomashenko"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "23",
      "title": "Speaker anonymization using x-vector and neural waveform models",
      "authors": [
        "F Fang"
      ],
      "year": "2019",
      "venue": "Proc. 10th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "24",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "26",
      "title": "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS Conf"
    },
    {
      "citation_id": "27",
      "title": "Pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier",
      "authors": [
        "Silero Team",
        "Vad Silero"
      ],
      "year": "2024",
      "venue": "Pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier"
    },
    {
      "citation_id": "28",
      "title": "documentation",
      "venue": "documentation"
    },
    {
      "citation_id": "29",
      "title": "The ryerson audiovisual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    }
  ]
}