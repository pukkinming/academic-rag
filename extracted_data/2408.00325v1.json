{
  "paper_id": "2408.00325v1",
  "title": "Iterative Prototype Refinement For Ambiguous Speech Emotion Recognition",
  "published": "2024-08-01T06:52:32Z",
  "authors": [
    "Haoqin Sun",
    "Shiwan Zhao",
    "Xiangyu Kong",
    "Xuechen Wang",
    "Hui Wang",
    "Jiaming Zhou",
    "Yong Qin"
  ],
  "keywords": [
    "speech emotion recognition",
    "iterative prototype refinement",
    "contrastive learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing emotions from speech is a daunting task due to the subtlety and ambiguity of expressions. Traditional speech emotion recognition (SER) systems, which typically rely on a singular, precise emotion label, struggle with this complexity. Therefore, modeling the inherent ambiguity of emotions is an urgent problem. In this paper, we propose an iterative prototype refinement framework (IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive learning and class prototypes. The former provides an efficient way to obtain high-quality representations of ambiguous samples. The latter are dynamically updated based on ambiguous labels-the similarity of the ambiguous data to all prototypes. These refined embeddings yield precise pseudo labels, thus reinforcing representation quality. Experimental evaluations conducted on the IEMOCAP dataset validate the superior performance of IPR over state-of-the-art methods, thus proving the effectiveness of our proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The advancement of affective computing  [1, 2]  has spurred the development of a wide array of emotion recognition corpora  [3, 4, 5] . Despite this progress, the intricate and costly process of data annotation, combined with the inherent ambiguity of emotional expressions, poses significant challenges. Traditional speech emotion recognition (SER) systems commonly rely on the assumption that emotions can be clearly categorized, attributing a precise or singular label to each vocal expression. For instance, Sun et al.  [6]  have engaged in representation learning with IEMOCAP  [7] , while Liu et al.  [8]  have explored metric learning on EmoDB  [9]  and CAISA  [10] . Nevertheless, these approaches may fall short of capturing the full spectrum of emotional ambiguity. This observation is supported by psychological and statistical research  [11, 12] , as well as machine learning studies  [13] , which collectively suggest that the boundaries between different emotional categories are not always distinct. The complexity and nuance of emotional expressions, coupled with the diversity in corpus composition, demand more sophisticated SER methodologies capable of more effectively navigating the intricate emotional landscape. Such advancements would transcend the constraints of singular emotion labeling, thereby enhancing the precision and broad applicability of emotion recognition across a varied range of emotional states.\n\nRecently, researchers have attempted to address the challenges in ambiguous SER by modeling emotion ambiguity. For example, Lotfian et al.  [14]  propose a multitask learning framework to learn the primary and secondary emotions of an utterance, ignoring information about other minor emotions. In contrast, Ando et al.  [15]  propose soft-target label learningestimating the proportion of all emotions, which increases the complexity of soft label learning. Moreover, Fei et al.  [16]  propose a multi-label emotion classification method to represent the ambiguity of emotions. However, these methods are primarily applicable to corpora retaining expert voting information during annotation. In the absence of annotated records, the generalizability of these methods is challenged.\n\nTo address these challenges, Zhou et al.  [17]  propose a Multi-Classifier Interaction Learning (MCIL) framework. This framework emulates the expert annotation process through multiple classifiers. On the one hand, it represents the annotation process using the classification results of multiple classifiers, facilitating soft label learning. On the other hand, it employs the majority voting mechanism to generate a precise label, enabling the study of traditional SER techniques. It is important to note that the efficacy of MCIL is heavily contingent on the performance of individual classifiers. Poor performance of these classifiers may result in unsatisfactory data labeling quality and emotion recognition performance. Conversely, if a particular classifier exhibits exceptional performance, the effectiveness of interactive learning may be diminished. Furthermore, simply using classifiers to annotate ambiguous samples and retraining the model may compromise the discriminability of the model in this type of data.\n\nInspired by the work  [18] , in this paper, we propose an iterative prototype refinement framework (IPR) for ambiguous SER. IPR comprises three key phases: class prototype learning, class prototype updating, and contrastive learning. In the class prototype learning phase, precise samples facilitate the initial acquisition of class prototype embeddings via a warm-up mechanism. Subsequently, during the class prototype updating phase, unlabeled ambiguous samples are introduced to participate in training process. We calculate the distance of the samples from each class prototype to construct ambiguous soft labels and update the class prototypes by the proportion of classes in the soft labels. Finally, contrastive learning is employed to augment the discriminative capability of the model, updating class prototypes when ambiguous samples and their enhanced counterparts points to one prototype. The main contributions are summarized as follows:\n\n( (2) IPR is trained on a small amount of high-quality data, enabling the construction of ambiguous soft labels and single labels for a large amount of data, thereby possessing better generalization capability.\n\n(3) Experimental results on IEMOCAP demonstrate that IPR outperforms the current state-of-the-art methods, achieving an accuracy of 70.75%, with an absolute improvement of 2.00%.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methods",
      "text": "Illustrated in Fig.  1 , our proposed framework, IPR, comprises three key phases: class prototype learning, class prototype updating, and contrastive learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Class Prototype Learning",
      "text": "First, we initially train the encoder using the precise labeled samples x q i . Meanwhile, for each class c ∈ {1, 2, ..., C}, the prototype vectors pc are initialized based on the precise samples as representative embeddings.\n\nwhere Encoder represents the pre-trained model Wav2vec2.0  [19] , which is commonly used as a feature extractor  [20, 21] . θ represents the trainable parameters. n represents the number of samples belonging to class c. Since, when the class prototype is not adequately learned, it has a negative impact in guiding the model to learn unlabeled data. Therefore, we set up the warm-up mechanism so that the encoder learns the better representation of the class prototypes on the precise labeled data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Class Prototype Updating",
      "text": "Once we acquire a prototype representation trained on precise labeled samples, the training process of the model will incorporate ambiguous unlabeled samples x k i . Then, we compute the similarity between ki and each class prototype vector pc to construct ambiguous soft label {s1, s2, ..., sC }, where sc is k ⊤ i pc. The class prototype vector with the largest similarity to ki is the pseudo label zc of that representation.\n\nwhere pc represents the c-th class. max represents maximization operation.\n\nIn the subsequent step, following the assignment of pseudo labels to ambiguous unlabeled samples, we dynamically adjust all class prototype vectors based on the ambiguous labels to incorporate the emotional information introduced by these samples. Traditional methods for calculating prototype embeddings require iterative updates, which is time-consuming and computationally intensive. To avoid this problem, we use moving average as the update method for prototype embedding.\n\nwhere Normalize represents the normalization operation. γ represents a tunable hyperparameter. MLP stands for the classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Learning",
      "text": "To enhance the quality of learned emotion representations in ambiguous samples, we integrate contrastive learning into IPR. For each ambiguous sample x k i , we initiate the process by generating an augmented version, denoted as x aug i , through a mixing method. This method encompasses various techniques such as adding noise, changing volume, adding reverberation, and changing pitch. Subsequently, we employ class prototype embeddings to assign pseudo labels for both x k i and x aug i . In a batch of 2N samples, comprising x k i and x aug i , we differentiate between positive examples, which share the same pseudo label, and negative examples, which possess different pseudo labels. The loss is computed as follows:\n\nwhere τ is a temperature parameter.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training Objective",
      "text": "To train our proposed IPR, three distinct loss functions are employed: cross-entropy loss for precise labeled samples, crossentropy loss for ambiguous samples, and contrastive learning loss. The overall training loss function is formulated as:\n\nwhere α, β, and µ represent the tunable trade-off factors. y and ŷ represent ground-truth and pseudo labels, respectively.\n\n3. Experiments",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "As shown in Table  1 , following the methodology proposed by  [17] , we partition the IEMOCAP dataset into three distinct subsets tailored for addressing ambiguous SER scenarios. Specifically, D1 encompasses precise labeled samples exhibiting minimal ambiguity in emotional classification. D2 comprises unlabeled samples with moderate ambiguity in emotion. D3, on the other hand, includes labeled samples characterized by the highest degree of ambiguity in emotion, serving as the evaluation set. The determination of emotion ambiguity is predicated on the level of consensus among experts during the annotation process. For each utterance in IEMOCAP, multiple experts contribute their assessments, and the consistency of their judgments dictates the attribution of the sample. A high level of agreement among annotators results in D1, whereas increased variability in assessments places a sample within D2. Samples demonstrating substantial disagreement among annotators are designated to D3. In our experimental setup, we establish three baseline systems: a baseline system, a baseline + system, and a supervised baseline system. The baseline system is trained exclusively on D1 and subsequently evaluated on D3. In contrast, the baseline + system utilizes both D1 and unlabeled D2 during training, which employs model-generated soft labels instead of the prototype-assigned pseudo labels employed in our proposed method. Finally, the supervised baseline system is trained on a combination of labeled D1 and D2 data and tested on D3. This system consists of an encoder and a MLP without the incorporation of class prototype and contrastive learning.\n\nOur proposed framework, IPR, is trained on A100 utilizing PyTorch. We adopt a batch size of 8 and set the maximum training epoch to 50, incorporating a warm-up mechanism for the initial 10 epochs. We employ the AdamW optimizer with an initial learning rate of 10 -5 , setting γ to 0.99. The trade-off parameters α and µ are fixed at 1.0 and 0.2, respectively. Initially, β is set to 0.0 and updated according to Eq. 8, gradually increasing based on the current and maximum epoch numbers.\n\nThe increment of β accelerates with each epoch until reaching 0.5.\n\nwhere the weight parameter weightm is set to 0.5, denoting its maximum value. Here, epochm represents the maximum epoch, while epochc denotes the current epoch. The exponential growth factor r governs the rate of increase. Consistent with prior research  [17] , we adopt accuracy (Acc) as the primary metric for assessing the efficacy of our proposed method. The final Acc is the average results across five seeds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "To demonstrate the effectiveness of IPR, we compare IPR with many state-of-the-art (SOTA) methods. Table  2  lists a brief description and accuracy results of these methods, where the first half is SER method, retrained following MCIL training algorithm, and the second half is the classical semi-supervised methods.\n\nWe could find that our method achieves the optimal performance on Acc, reaching 70.75%, which is significantly better than SOTA methods (with an absolute improvement of 2.00%). This performance is mainly attributed to the collaborative effect of class prototype learning and contrastive learning, which reinforce each other, thus allowing the model to capture weak dominant emotions from ambiguous utterances. In comparison with the SER approaches, IPR demonstrates effectiveness and reliability in generating pseudo labels. With the guidance of pseudo labels, IPR achieves a 3.75% improvement on Acc. Furthermore, in the comparison with the semi-supervised methods, we find that IPR is improved by 2.00% on Acc. These semisupervised methods only utilize a large amount of unlabeled data for the purpose of improving recognition performance. It does not seem to tackle the problem of annotation on large amounts of data by learning on a small number of precise samples. The experimental results provide strong evidence that our proposed IPR could provide ideal performance for ambiguous SER and permit more reliable data annotation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline System Analysis",
      "text": "We report the experimental results of IPR with different baseline systems as shown in Table  3 . First, we observe that by integrating these unlabeled samples, the performance of IPR improves significantly, by 6.50% over baseline on Acc. This highlights the importance of using unlabeled ambiguous data to optimize model. Second, IPR outperforms baseline + . This proves that IPR, an iterative prototype refinement framework, outperforms model-generated soft labels in the generation of pseudo labels. Notably, the baseline + also outperforms the majority voting method of MCIL in SOTA. Thus, when faced with a corpus without voting information, our method utilizes a small amount of data for training and could achieve a large number of data annotation, both single and ambiguous soft labels. Finally, by comparing IPR and supervised baseline, we could notice that on Acc, the proposed method reduces 1.55%, while MCIL reduces 5.30%. As a result, IPR is closer to the performance of supervised system, while it produces more precise and reliable pseudo labels when annotating unlabeled samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Class Prototype Analysis",
      "text": "Figure  2  shows the trend of similarity between class prototype embeddings during the training process. In the initial 10 epochs of training, we observe a transient increase in the similarity between the neutral prototype and other class prototype vectors. This phenomenon arises from the localized occurrence of emotional expression within speech segments. Additionally, given the similarity in activation levels between happiness and neutrality, they exhibit the highest prototype similarity, aligning with prior finding  [30] . Intuitively, anger and sadness represent contrasting emotions, thus exhibiting a declining trend in the similarity between their prototypes. In subsequent training stages, prototype-assigned pseudo labels are highly inaccurate, resulting in unlabeled data negatively impacting the model and thus reducing the quality of the learned class prototypes. Furthermore, all prototype embeddings are soft updated according to the proportion of each emotion in ambiguous soft labels, which leads to an increase in the similarity of the emotion prototypes. Ultimately, we observe that the class prototypes stabilize at a relatively steady state with a similarity greater than 0, affirming the inherent ambiguity in emotion expression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pseudo Label Analysis",
      "text": "Figure  3  shows agreement rate of model-generated pseudo labels and prototype-assigned pseudo labels with ground-truth labels. We observe that the upward trend in the agreement rate between prototype-assigned and ground-truth labels is significantly greater than the agreement rate between model-generated  soft labels and ground-truth labels. Additionally, the prototypeassigned pseudo labels exhibit a higher agreement rate with the ground-truth labels. On the one hand, IPR could achieve ambiguous soft label annotation on a large amount of unlabeled data by training on a small amount of precise labeled data. On the other hand, it could assign more precise single labels while maintaining the desired recognition performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel iterative prototype refinement framework leveraging class prototypes and contrastive learning for ambiguous SER. IPR capitalizes on the clustering effect induced by contrastive learning to generate optimal representations. These representations facilitate the refinement and updating of class prototype embeddings, thereby enhancing the precision of assigned pseudo labels. Consequently, improved pseudo labels contribute to the augmentation of representation quality, establishing a positive feedback loop. Through a comprehensive array of comparative experiments, ablation studies, and visualization analyses conducted on the IEMOCAP benchmark dataset, we substantiate the effectiveness of IPR.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of IPR, which consists of class prototype learning phase, class prototype updating phase and",
      "page": 2
    },
    {
      "caption": "Figure 1: , our proposed framework, IPR, comprises",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the trend of similarity between class prototype",
      "page": 4
    },
    {
      "caption": "Figure 3: shows agreement rate of model-generated pseudo la-",
      "page": 4
    },
    {
      "caption": "Figure 2: Similarity between class prototype embeddings during",
      "page": 4
    },
    {
      "caption": "Figure 3: Blue (red) represents the agreement rate be-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "qinyong@nankai.edu.cn": "Recently,\nresearchers have attempted to address the chal-"
        },
        {
          "qinyong@nankai.edu.cn": "lenges in ambiguous SER by modeling emotion ambiguity. For"
        },
        {
          "qinyong@nankai.edu.cn": "example, Lotfian et al. [14] propose a multitask learning frame-"
        },
        {
          "qinyong@nankai.edu.cn": "work to learn the primary and secondary emotions of an ut-"
        },
        {
          "qinyong@nankai.edu.cn": "terance,\nignoring information about other minor emotions.\nIn"
        },
        {
          "qinyong@nankai.edu.cn": "contrast, Ando et al.\n[15] propose soft-target\nlabel\nlearning –"
        },
        {
          "qinyong@nankai.edu.cn": "estimating the proportion of all emotions, which increases the"
        },
        {
          "qinyong@nankai.edu.cn": "complexity of\nsoft\nlabel\nlearning. Moreover, Fei et al.\n[16]"
        },
        {
          "qinyong@nankai.edu.cn": "propose a multi-label emotion classification method to repre-"
        },
        {
          "qinyong@nankai.edu.cn": "sent\nthe ambiguity of emotions. However,\nthese methods are"
        },
        {
          "qinyong@nankai.edu.cn": "primarily applicable to corpora retaining expert voting informa-"
        },
        {
          "qinyong@nankai.edu.cn": "tion during annotation. In the absence of annotated records, the"
        },
        {
          "qinyong@nankai.edu.cn": "generalizability of these methods is challenged."
        },
        {
          "qinyong@nankai.edu.cn": "To address\nthese challenges, Zhou et al.\n[17] propose a"
        },
        {
          "qinyong@nankai.edu.cn": "Multi-Classifier Interaction Learning (MCIL) framework. This"
        },
        {
          "qinyong@nankai.edu.cn": "framework emulates the expert annotation process through mul-"
        },
        {
          "qinyong@nankai.edu.cn": "tiple classifiers. On the one hand,\nit represents the annotation"
        },
        {
          "qinyong@nankai.edu.cn": "process using the classification results of multiple classifiers,"
        },
        {
          "qinyong@nankai.edu.cn": "facilitating soft\nlabel\nlearning. On the other hand,\nit employs"
        },
        {
          "qinyong@nankai.edu.cn": "the majority voting mechanism to generate a precise label, en-"
        },
        {
          "qinyong@nankai.edu.cn": "abling the study of traditional SER techniques. It is important to"
        },
        {
          "qinyong@nankai.edu.cn": "note that the efficacy of MCIL is heavily contingent on the per-"
        },
        {
          "qinyong@nankai.edu.cn": "formance of\nindividual classifiers.\nPoor performance of\nthese"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "classifiers may result in unsatisfactory data labeling quality and"
        },
        {
          "qinyong@nankai.edu.cn": "emotion recognition performance.\nConversely,\nif a particular"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "classifier exhibits exceptional performance, the effectiveness of"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "interactive learning may be diminished.\nFurthermore,\nsimply"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "using classifiers to annotate ambiguous samples and retraining"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "the model may compromise the discriminability of the model in"
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "this type of data."
        },
        {
          "qinyong@nankai.edu.cn": ""
        },
        {
          "qinyong@nankai.edu.cn": "Inspired by the work [18],\nin this paper, we propose an it-"
        },
        {
          "qinyong@nankai.edu.cn": "erative prototype refinement\nframework (IPR)\nfor ambiguous"
        },
        {
          "qinyong@nankai.edu.cn": "SER.\nIPR comprises three key phases:\nclass prototype learn-"
        },
        {
          "qinyong@nankai.edu.cn": "ing, class prototype updating, and contrastive learning.\nIn the"
        },
        {
          "qinyong@nankai.edu.cn": "class prototype learning phase, precise samples\nfacilitate the"
        },
        {
          "qinyong@nankai.edu.cn": "initial acquisition of class prototype embeddings via a warm-up"
        },
        {
          "qinyong@nankai.edu.cn": "mechanism. Subsequently, during the class prototype updating"
        },
        {
          "qinyong@nankai.edu.cn": "phase, unlabeled ambiguous samples are introduced to partici-"
        },
        {
          "qinyong@nankai.edu.cn": "pate in training process. We calculate the distance of the sam-"
        },
        {
          "qinyong@nankai.edu.cn": "ples from each class prototype to construct ambiguous soft\nla-"
        },
        {
          "qinyong@nankai.edu.cn": "bels and update the class prototypes by the proportion of classes"
        },
        {
          "qinyong@nankai.edu.cn": "in the soft\nlabels. Finally, contrastive learning is employed to"
        },
        {
          "qinyong@nankai.edu.cn": "augment\nthe discriminative capability of\nthe model, updating"
        },
        {
          "qinyong@nankai.edu.cn": "class prototypes when ambiguous samples and their enhanced"
        },
        {
          "qinyong@nankai.edu.cn": "counterparts points to one prototype.\nThe main contributions"
        },
        {
          "qinyong@nankai.edu.cn": "are summarized as follows:"
        },
        {
          "qinyong@nankai.edu.cn": "(1) We\npropose\na\nnovel\niterative\nprototype\nrefinement"
        },
        {
          "qinyong@nankai.edu.cn": "framework for ambiguous SER. Contrastive learning aids in the"
        },
        {
          "qinyong@nankai.edu.cn": "learning of ideal representations, improving the quality of class"
        },
        {
          "qinyong@nankai.edu.cn": "prototypes. Subsequently, enhanced class prototypes guide the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Augmented Ambiguous": "xaug",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "Encoder",
          "Contrastive": "Similarity"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "Loss"
        },
        {
          "Augmented Ambiguous": "Unlabeled Samples",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "kaug"
        },
        {
          "Augmented Ambiguous": "The overall architecture of\nFigure 1:",
          "Contrastive": "IPR, which consists of class prototype learning phase, class prototype updating phase and"
        },
        {
          "Augmented Ambiguous": "contrastive learning phase.",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "model in generating high-quality representations.",
          "Contrastive": "similarity between ki and each class prototype vector pc to con-"
        },
        {
          "Augmented Ambiguous": "(2) IPR is trained on a small amount of high-quality data,",
          "Contrastive": "struct ambiguous soft label {s1, s2, ..., sC }, where sc is k⊤\ni pc."
        },
        {
          "Augmented Ambiguous": "enabling the construction of ambiguous soft\nlabels and single",
          "Contrastive": "The class prototype vector with the largest similarity to ki is the"
        },
        {
          "Augmented Ambiguous": "labels for a large amount of data, thereby possessing better gen-",
          "Contrastive": "pseudo label zc of that representation."
        },
        {
          "Augmented Ambiguous": "eralization capability.",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "(3)\nzc = arg max k⊤\ni pc,"
        },
        {
          "Augmented Ambiguous": "(3) Experimental\nresults on IEMOCAP demonstrate that",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "IPR outperforms the current state-of-the-art methods, achiev-",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "where pc represents the c-th class. max represents maximiza-"
        },
        {
          "Augmented Ambiguous": "ing an accuracy of 70.75%, with an absolute improvement of",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "tion operation."
        },
        {
          "Augmented Ambiguous": "2.00%.",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "In the subsequent step, following the assignment of pseudo"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "labels to ambiguous unlabeled samples, we dynamically adjust"
        },
        {
          "Augmented Ambiguous": "2. Proposed Methods",
          "Contrastive": "all class prototype vectors based on the ambiguous labels to in-"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "corporate the emotional\ninformation introduced by these sam-"
        },
        {
          "Augmented Ambiguous": "Illustrated in Fig. 1, our proposed framework,\nIPR, comprises",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "ples. Traditional methods for calculating prototype embeddings"
        },
        {
          "Augmented Ambiguous": "three key phases: class prototype learning, class prototype up-",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "require iterative updates, which is\ntime-consuming and com-"
        },
        {
          "Augmented Ambiguous": "dating, and contrastive learning.",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "putationally intensive. To avoid this problem, we use moving"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "average as the update method for prototype embedding."
        },
        {
          "Augmented Ambiguous": "2.1. Class Prototype Learning",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "(4)\npc = Normalize (γpc + (1 − γ)kisc) ,"
        },
        {
          "Augmented Ambiguous": "First, we initially train the encoder using the precise labeled",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "samples xq\nfor each class c ∈ {1, 2, ..., C},",
          "Contrastive": ") ,\nif c = max MLP (ki) = max MLP (kaug"
        },
        {
          "Augmented Ambiguous": "i . Meanwhile,",
          "Contrastive": "i"
        },
        {
          "Augmented Ambiguous": "prototype vectors pc are initialized based on the precise samples",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "where Normalize represents the normalization operation. γ rep-"
        },
        {
          "Augmented Ambiguous": "as representative embeddings.",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "resents a tunable hyperparameter. MLP stands for the classifier."
        },
        {
          "Augmented Ambiguous": "(1)\nqi = Encoder (xq\ni ; θ) ,",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "2.3. Contrastive learning"
        },
        {
          "Augmented Ambiguous": "1 n\n(cid:88) i\n(2)\npc =\nqi,",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "To enhance the quality of\nlearned emotion representations in"
        },
        {
          "Augmented Ambiguous": "∈c",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "ambiguous samples, we integrate contrastive learning into IPR."
        },
        {
          "Augmented Ambiguous": "where Encoder represents the pre-trained model Wav2vec2.0",
          "Contrastive": "For each ambiguous sample xk\ni , we initiate the process by gen-"
        },
        {
          "Augmented Ambiguous": "[19], which is commonly used as a feature extractor [20, 21]. θ",
          "Contrastive": "erating an augmented version, denoted as xaug\n, through a mix-\ni"
        },
        {
          "Augmented Ambiguous": "represents the trainable parameters. n represents the number of",
          "Contrastive": "ing method. This method encompasses various techniques such"
        },
        {
          "Augmented Ambiguous": "samples belonging to class c.",
          "Contrastive": "as adding noise, changing volume, adding reverberation, and"
        },
        {
          "Augmented Ambiguous": "Since, when the class prototype is not adequately learned,",
          "Contrastive": "changing pitch. Subsequently, we employ class prototype em-"
        },
        {
          "Augmented Ambiguous": "it has a negative impact in guiding the model to learn unlabeled",
          "Contrastive": "beddings to assign pseudo labels for both xk\nand xaug\n.\nIn a\ni"
        },
        {
          "Augmented Ambiguous": "data. Therefore, we set up the warm-up mechanism so that\nthe",
          "Contrastive": "batch of 2N samples, comprising xk\n, we differentiate\ni and xaug"
        },
        {
          "Augmented Ambiguous": "encoder learns the better representation of the class prototypes",
          "Contrastive": "between positive examples, which share the same pseudo label,"
        },
        {
          "Augmented Ambiguous": "on the precise labeled data.",
          "Contrastive": "and negative examples, which possess different pseudo labels."
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "The loss is computed as follows:"
        },
        {
          "Augmented Ambiguous": "2.2. Class Prototype Updating",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "exp (cid:0)(cid:0)xk\n· xaug\n(cid:1) /τ (cid:1)\ni"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "(5)\nLcon = − log"
        },
        {
          "Augmented Ambiguous": "Once we acquire a prototype representation trained on precise",
          "Contrastive": "(cid:80)\n(cid:1) /τ (cid:1) ,\n· xaug"
        },
        {
          "Augmented Ambiguous": "",
          "Contrastive": "j\nj∈{k,aug} exp (cid:0)(cid:0)xk"
        },
        {
          "Augmented Ambiguous": "labeled samples, the training process of the model will incorpo-",
          "Contrastive": ""
        },
        {
          "Augmented Ambiguous": "rate ambiguous unlabeled samples xk\ni . Then, we compute the",
          "Contrastive": "where τ is a temperature parameter."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Performance comparison of our proposed methods",
      "data": [
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "with SOTA approaches on IEMOCAP."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "Dataset\nD1\nD2\nD3",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "IEMOCAP\n1710\n3421\n400",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Method\nDescription\nAcc(%)"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Cummins et al.[22]\nAlexNet\n59.20"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "2.4. Training Objective",
          "Performance comparison of our proposed methods\nTable 2:": "Li et al. [23]\nCNN\n58.00"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Ando et al. [24]\nMulti-label\n57.80"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "To train our proposed IPR, three distinct loss functions are em-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Liu et al. [25]\nCapsNet\n58.55"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "ployed:\ncross-entropy loss for precise labeled samples, cross-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Zhou et al. [17]\nMajority voting\n65.00"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "entropy loss for ambiguous samples, and contrastive learning",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Zhou et al. [17]\nMCIL\n67.00"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "loss. The overall training loss function is formulated as:",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Ghifary et al. [26]\nDaNN\n65.00"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "(cid:16)\n(cid:17)",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "xk, ˆy\n(6)\n+ µLcon,\nL = αLcls (xq, y) + βLcls",
          "Performance comparison of our proposed methods\nTable 2:": "Yu et al. [27]\nDAAN\n65.75"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "Ganin et al. [28]\nDANN\n68.50"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "where α, β, and µ represent the tunable trade-off factors. y and",
          "Performance comparison of our proposed methods\nTable 2:": "Cui et al. [29]\nBNM\n68.75"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "y represent ground-truth and pseudo labels, respectively.",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "IPR\nPrototype learning\n70.75"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "3. Experiments",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "an initial learning rate of 10−5, setting γ to 0.99. The trade-off"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "3.1. Dataset",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "parameters α and µ are fixed at 1.0 and 0.2, respectively.\nIni-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "tially, β is set to 0.0 and updated according to Eq. 8, gradually"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "IEMOCAP [7] stands out as a widely used dataset in SER. De-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "increasing based on the current and maximum epoch numbers."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "veloped by the Signal Analysis and Interpretation Laboratory",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "The increment of β accelerates with each epoch until reaching"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "at the University of Southern California, this dataset comprises",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "0.5."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "emotionally charged dialogues enacted by 10 actors represent-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "ing diverse demographics in terms of gender and age.\nThese",
          "Performance comparison of our proposed methods\nTable 2:": "(cid:18)\n(cid:19)\nepochc"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "epochm/2 −1"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "dialogues are orchestrated using constraint-based scripts crafted",
          "Performance comparison of our proposed methods\nTable 2:": " \n \nweightm ×"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "to mimic authentic interaction scenarios. we focus on four emo-",
          "Performance comparison of our proposed methods\nTable 2:": ",\nβ = min\n(7)\n, weightm"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "r − 1"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "tions: anger, happiness, sadness, and neutral, where excitement",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "class is merged into the happiness class.",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "set\nto 0.5, denoting\nwhere the weight parameter weightm is"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "3.2. Experimental Settings",
          "Performance comparison of our proposed methods\nTable 2:": "its maximum value.\nthe maximum\nHere, epochm represents"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "epoch, while epochc denotes the current epoch. The exponen-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "As shown in Table 1, following the methodology proposed by",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "tial growth factor r governs\nthe rate of\nincrease.\nConsistent"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "[17], we partition the IEMOCAP dataset into three distinct sub-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "with prior\nresearch [17], we adopt accuracy (Acc) as the pri-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "sets tailored for addressing ambiguous SER scenarios. Specifi-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "mary metric for assessing the efficacy of our proposed method."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "cally, D1 encompasses precise labeled samples exhibiting min-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "The final Acc is the average results across five seeds."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "imal ambiguity in emotional classification. D2 comprises un-",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "labeled samples with moderate ambiguity in emotion. D3, on",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "",
          "Performance comparison of our proposed methods\nTable 2:": "3.3. Comparison with State-of-the-Art Methods"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "the other hand,\nincludes labeled samples characterized by the",
          "Performance comparison of our proposed methods\nTable 2:": ""
        },
        {
          "Table 1: The sizes of training and testing datasets.": "highest degree of ambiguity in emotion, serving as the evalua-",
          "Performance comparison of our proposed methods\nTable 2:": "To demonstrate the effectiveness of IPR, we compare IPR with"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "tion set. The determination of emotion ambiguity is predicated",
          "Performance comparison of our proposed methods\nTable 2:": "many state-of-the-art (SOTA) methods. Table 2 lists a brief de-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "on the level of consensus among experts during the annotation",
          "Performance comparison of our proposed methods\nTable 2:": "scription and accuracy results of these methods, where the first"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "process. For each utterance in IEMOCAP, multiple experts con-",
          "Performance comparison of our proposed methods\nTable 2:": "half\nis SER method,\nretrained following MCIL training algo-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "tribute their assessments, and the consistency of their judgments",
          "Performance comparison of our proposed methods\nTable 2:": "rithm, and the second half is the classical semi-supervised meth-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "dictates the attribution of the sample. A high level of agreement",
          "Performance comparison of our proposed methods\nTable 2:": "ods."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "among annotators results in D1, whereas increased variability in",
          "Performance comparison of our proposed methods\nTable 2:": "We could find that our method achieves the optimal perfor-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "assessments places a sample within D2. Samples demonstrat-",
          "Performance comparison of our proposed methods\nTable 2:": "mance on Acc,\nreaching 70.75%, which is significantly better"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "ing substantial disagreement among annotators are designated",
          "Performance comparison of our proposed methods\nTable 2:": "than SOTA methods (with an absolute improvement of 2.00%)."
        },
        {
          "Table 1: The sizes of training and testing datasets.": "to D3.",
          "Performance comparison of our proposed methods\nTable 2:": "This performance is mainly attributed to the collaborative ef-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "In our experimental setup, we establish three baseline sys-",
          "Performance comparison of our proposed methods\nTable 2:": "fect of class prototype learning and contrastive learning, which"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "tems:\na baseline\nsystem,\na baseline+ system,\nand a\nsuper-",
          "Performance comparison of our proposed methods\nTable 2:": "reinforce each other,\nthus allowing the model\nto capture weak"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "vised baseline system.\nThe baseline system is trained exclu-",
          "Performance comparison of our proposed methods\nTable 2:": "dominant emotions from ambiguous utterances.\nIn comparison"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "sively on D1 and subsequently evaluated on D3.\nIn contrast,",
          "Performance comparison of our proposed methods\nTable 2:": "with the SER approaches,\nIPR demonstrates effectiveness and"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "the baseline+ system utilizes both D1 and unlabeled D2 during",
          "Performance comparison of our proposed methods\nTable 2:": "reliability in generating pseudo labels. With the guidance of"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "training, which employs model-generated soft labels instead of",
          "Performance comparison of our proposed methods\nTable 2:": "pseudo labels, IPR achieves a 3.75% improvement on Acc. Fur-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "the prototype-assigned pseudo labels employed in our proposed",
          "Performance comparison of our proposed methods\nTable 2:": "thermore, in the comparison with the semi-supervised methods,"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "method. Finally,\nthe supervised baseline system is trained on a",
          "Performance comparison of our proposed methods\nTable 2:": "we find that\nIPR is improved by 2.00% on Acc. These semi-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "combination of labeled D1 and D2 data and tested on D3. This",
          "Performance comparison of our proposed methods\nTable 2:": "supervised methods only utilize a large amount of unlabeled"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "system consists of an encoder and a MLP without\nthe incorpo-",
          "Performance comparison of our proposed methods\nTable 2:": "data for the purpose of improving recognition performance.\nIt"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "ration of class prototype and contrastive learning.",
          "Performance comparison of our proposed methods\nTable 2:": "does not\nseem to tackle the problem of annotation on large"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "Our proposed framework,\nIPR,\nis trained on A100 utiliz-",
          "Performance comparison of our proposed methods\nTable 2:": "amounts of data by learning on a small number of precise sam-"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "ing PyTorch. We adopt a batch size of 8 and set\nthe maximum",
          "Performance comparison of our proposed methods\nTable 2:": "ples. The experimental results provide strong evidence that our"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "training epoch to 50,\nincorporating a warm-up mechanism for",
          "Performance comparison of our proposed methods\nTable 2:": "proposed IPR could provide ideal performance for ambiguous"
        },
        {
          "Table 1: The sizes of training and testing datasets.": "the initial 10 epochs. We employ the AdamW optimizer with",
          "Performance comparison of our proposed methods\nTable 2:": "SER and permit more reliable data annotation."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: First, we observe that by Figure2:Similaritybetweenclassprototypeembeddingsduring",
      "data": [
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "(compared with baseline). ⋆ indicates"
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "(compared with baseline and baseline+)."
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "Method"
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "baseline"
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "baseline+"
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "IPR"
        },
        {
          "model-generated pseudo labels. ⋆ indicates that p-value < 0.05": "supervised baseline"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "6. References": "[1]\nT. Hu, A. Xu, Z. Liu, Q. You, Y. Guo, V. Sinha,\nJ. Luo,\nand",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "sentations,” Advances in neural\ninformation processing systems,"
        },
        {
          "6. References": "R. Akkiraju, “Touch your heart: A tone-aware chatbot\nfor cus-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "6. References": "tomer care on social media,” in Proceedings of the 2018 CHI con-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "ference on human factors in computing systems, 2018, pp. 1–12.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[20]\nS. Dang,\nT. Matsumoto, Y. Takeuchi, H. Kudo,\nT. Tsuboi,"
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Y\n. Tanaka, and M. Katsuno, “Using self-learning representations"
        },
        {
          "6. References": "[2] H. Arsikere, E. Shriberg,\nand U. Ozertem,\n“Computationally-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "for objective assessment of patient voice in dysphonia,” in 2022"
        },
        {
          "6. References": "efficient endpointing features for natural spoken interaction with",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Asia-Pacific Signal and Information Processing Association An-"
        },
        {
          "6. References": "personal-assistant\nsystems,”\nin 2014 IEEE International Con-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "nual Summit and Conference (APSIPA ASC).\nIEEE, 2022, pp."
        },
        {
          "6. References": "ference on Acoustics, Speech and Signal Processing (ICASSP).",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "359–363."
        },
        {
          "6. References": "IEEE, 2014, pp. 3241–3245.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[21] Y. Gao, H. Shi, C. Chu,\nand T. Kawahara,\n“Enhancing two-"
        },
        {
          "6. References": "[3]\nS. Haq, P.\nJ.\nJackson, and J. Edge, “Speaker-dependent audio-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "stage finetuning for speech emotion recognition using adapters,”"
        },
        {
          "6. References": "visual emotion recognition.” in AVSP, vol. 2009, 2009, pp. 53–58.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "in ICASSP 2024-2024 IEEE International Conference on Acous-"
        },
        {
          "6. References": "[4] C. Busso,\nS.\nParthasarathy, A. Burmania, M. AbdelWahab,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2024, pp."
        },
        {
          "6. References": "N. Sadoughi, and E. M. Provost, “Msp-improv: An acted corpus",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "11 316–11 320."
        },
        {
          "6. References": "of dyadic interactions to study emotion perception,” IEEE Trans-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "actions on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[22] N. Cummins, S. Amiriparian, G. Hagerer, A. Batliner, S. Steidl,"
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "and B. W. Schuller, “An image-based deep spectrum feature repre-"
        },
        {
          "6. References": "[5]\nS. R. Livingstone and F. A. Russo,\n“The ryerson audio-visual",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "sentation for the recognition of emotional speech,” in Proceedings"
        },
        {
          "6. References": "database of emotional\nspeech and song (ravdess): A dynamic,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "of\nthe 25th ACM international conference on Multimedia, 2017,"
        },
        {
          "6. References": "multimodal set of facial and vocal expressions in north american",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "pp. 478–484."
        },
        {
          "6. References": "english,” PloS one, vol. 13, no. 5, p. e0196391, 2018.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[23]\nP. Li, Y. Song,\nI. V. McLoughlin, W. Guo, and L.-R. Dai, “An"
        },
        {
          "6. References": "[6] H. Sun, S. Zhao, X. Wang, W. Zeng, Y. Chen, and Y. Qin, “Fine-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "attention pooling based representation learning method for speech"
        },
        {
          "6. References": "grained disentangled representation learning for multimodal emo-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "emotion recognition,” 2018."
        },
        {
          "6. References": "tion recognition,” arXiv preprint arXiv:2312.13567, 2023.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[24] A. Ando, R. Masumura, H. Kamiyama, S. Kobashikawa,\nand"
        },
        {
          "6. References": "[7] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Y\n. Aono, “Speech emotion recognition based on multi-label emo-"
        },
        {
          "6. References": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tion existence model.” in INTERSPEECH, 2019, pp. 2818–2822."
        },
        {
          "6. References": "Interactive emotional dyadic motion capture database,” Language",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "resources and evaluation, vol. 42, pp. 335–359, 2008.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[25]\nJ. Liu, Z. Liu, L. Wang, L. Guo, and J. Dang, “Speech emotion"
        },
        {
          "6. References": "[8] Y. Liu, H. Sun, W. Guan, Y. Xia, Y. Li, M. Unoki, and Z. Zhao,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "recognition with local-global aware deep representation learning,”"
        },
        {
          "6. References": "“A discriminative feature representation method based on cas-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "in ICASSP 2020-2020 IEEE International Conference on Acous-"
        },
        {
          "6. References": "caded attention network with adversarial strategy for speech emo-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp."
        },
        {
          "6. References": "tion recognition,” IEEE/ACM Transactions on Audio, Speech, and",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "7174–7178."
        },
        {
          "6. References": "Language Processing, vol. 31, pp. 1063–1074, 2023.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[26] M. Ghifary, W. B. Kleijn, and M. Zhang, “Domain adaptive neu-"
        },
        {
          "6. References": "[9]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier,\nand",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "ral networks for object recognition,” in PRICAI 2014: Trends in"
        },
        {
          "6. References": "B. Weiss, “A database of german emotional speech in ninth eu-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Artificial Intelligence: 13th Pacific Rim International Conference"
        },
        {
          "6. References": "ropean conference on speech communication and technology,”",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "on Artificial Intelligence, Gold Coast, QLD, Australia, December"
        },
        {
          "6. References": "2005.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "1-5, 2014. Proceedings 13.\nSpringer, 2014, pp. 898–904."
        },
        {
          "6. References": "[10]\nJ. Zhang and H. Jia, “Design of speech corpus for mandarin text",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[27] C. Yu, J. Wang, Y. Chen, and M. Huang, “Transfer learning with"
        },
        {
          "6. References": "to speech,” in The blizzard challenge 2008 workshop, 2008.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "dynamic adversarial adaptation network,” in 2019 IEEE interna-"
        },
        {
          "6. References": "[11] A. Ortony, G. L. Clore, and A. Collins, The cognitive structure of",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tional conference on data mining (ICDM).\nIEEE, 2019, pp. 778–"
        },
        {
          "6. References": "emotions.\nCambridge university press, 2022.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "786."
        },
        {
          "6. References": "[12]\nJ. Tao, Y. Li, and S. Pan, “A multiple perception model on emo-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[28] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by"
        },
        {
          "6. References": "tional speech,” in 2009 3rd International Conference on Affective",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "backpropagation,” in International conference on machine learn-"
        },
        {
          "6. References": "Computing and Intelligent\nInteraction and Workshops.\nIEEE,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "ing.\nPMLR, 2015, pp. 1180–1189."
        },
        {
          "6. References": "2009, pp. 1–6.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[29]\nS. Cui, S. Wang,\nJ. Zhuo, L. Li, Q. Huang, and Q. Tian, “To-"
        },
        {
          "6. References": "[13] X. Wang, S. Zhao, and Y. Qin, “Supervised Contrastive Learning",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "wards discriminability and diversity: Batch nuclear-norm maxi-"
        },
        {
          "6. References": "with Nearest Neighbor Search for Speech Emotion Recognition,”",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "mization under label insufficient situations,” in Proceedings of the"
        },
        {
          "6. References": "in Proc. INTERSPEECH 2023, 2023, pp. 1913–1917.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "IEEE/CVF conference on computer vision and pattern recogni-"
        },
        {
          "6. References": "",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tion, 2020, pp. 3941–3950."
        },
        {
          "6. References": "[14] R. Lotfian and C. Busso,\n“Predicting categorical\nemotions by",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "jointly learning primary and secondary emotions through multi-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[30] Y. Liu, H. Sun, W. Guan, Y. Xia, and Z. Zhao, “Discriminative"
        },
        {
          "6. References": "task learning,” Interspeech 2018, 2018.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Feature Representation Based on Cascaded Attention Network"
        },
        {
          "6. References": "[15] A. Ando, S. Kobashikawa, H. Kamiyama, R. Masumura, Y. Ijima,",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "with Adversarial Joint Loss for Speech Emotion Recognition,” in"
        },
        {
          "6. References": "and Y. Aono, “Soft-target\ntraining with ambiguous emotional ut-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Proc. Interspeech 2022, 2022, pp. 4750–4754."
        },
        {
          "6. References": "terances\nfor dnn-based speech emotion classification,”\nin 2018",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "IEEE International Conference on Acoustics, Speech and Signal",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "Processing (ICASSP).\nIEEE, 2018, pp. 4964–4968.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "[16] H. Fei, D. Ji, Y. Zhang, and Y. Ren, “Topic-enhanced capsule net-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "work for multi-label emotion classification,” IEEE/ACM Transac-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "tions on Audio, Speech, and Language Processing, vol. 28, pp.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "1839–1848, 2020.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "[17] Y. Zhou, X. Liang, Y. Gu, Y. Yin, and L. Yao, “Multi-classifier",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "interactive learning for ambiguous speech emotion recognition,”",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "cessing, vol. 30, pp. 695–705, 2022.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "[18]\nJ. Zhou,\nS. Zhao, N.\nJiang, G. Zhao,\nand Y. Qin,\n“Madi:",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "Inter-domain matching and intra-domain discrimination for cross-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "domain speech recognition,” in ICASSP 2023-2023 IEEE Inter-",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "national Conference on Acoustics, Speech and Signal Processing",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        },
        {
          "6. References": "(ICASSP).\nIEEE, 2023, pp. 1–5.",
          "[19] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Touch your heart: A tone-aware chatbot for customer care on social media",
      "authors": [
        "T Hu",
        "A Xu",
        "Z Liu",
        "Q You",
        "Y Guo",
        "V Sinha",
        "J Luo",
        "R Akkiraju"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "3",
      "title": "Computationallyefficient endpointing features for natural spoken interaction with personal-assistant systems",
      "authors": [
        "H Arsikere",
        "E Shriberg",
        "U Ozertem"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Speaker-dependent audiovisual emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2009",
      "venue": "AVSP"
    },
    {
      "citation_id": "5",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "7",
      "title": "Finegrained disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "H Sun",
        "S Zhao",
        "X Wang",
        "W Zeng",
        "Y Chen",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Finegrained disentangled representation learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.13567"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "A discriminative feature representation method based on cascaded attention network with adversarial strategy for speech emotion recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Y Li",
        "M Unoki",
        "Z Zhao"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "A database of german emotional speech in ninth european conference on speech communication and technology",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech in ninth european conference on speech communication and technology"
    },
    {
      "citation_id": "11",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The blizzard challenge 2008 workshop"
    },
    {
      "citation_id": "12",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "2022",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "13",
      "title": "A multiple perception model on emotional speech",
      "authors": [
        "J Tao",
        "Y Li",
        "S Pan"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "14",
      "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "15",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning"
    },
    {
      "citation_id": "16",
      "title": "Soft-target training with ambiguous emotional utterances for dnn-based speech emotion classification",
      "authors": [
        "A Ando",
        "S Kobashikawa",
        "H Kamiyama",
        "R Masumura",
        "Y Ijima",
        "Y Aono"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Topic-enhanced capsule network for multi-label emotion classification",
      "authors": [
        "H Fei",
        "D Ji",
        "Y Zhang",
        "Y Ren"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Multi-classifier interactive learning for ambiguous speech emotion recognition",
      "authors": [
        "Y Zhou",
        "X Liang",
        "Y Gu",
        "Y Yin",
        "L Yao"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Madi: Inter-domain matching and intra-domain discrimination for crossdomain speech recognition",
      "authors": [
        "J Zhou",
        "S Zhao",
        "N Jiang",
        "G Zhao",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Using self-learning representations for objective assessment of patient voice in dysphonia",
      "authors": [
        "S Dang",
        "T Matsumoto",
        "Y Takeuchi",
        "H Kudo",
        "T Tsuboi",
        "Y Tanaka",
        "M Katsuno"
      ],
      "venue": "Using self-learning representations for objective assessment of patient voice in dysphonia"
    },
    {
      "citation_id": "22",
      "title": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
      "year": "2022",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "23",
      "title": "Enhancing twostage finetuning for speech emotion recognition using adapters",
      "authors": [
        "Y Gao",
        "H Shi",
        "C Chu",
        "T Kawahara"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "An image-based deep spectrum feature representation for the recognition of emotional speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L.-R Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition based on multi-label emotion existence model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama",
        "S Kobashikawa",
        "Y Aono"
      ],
      "year": "2019",
      "venue": "Speech emotion recognition based on multi-label emotion existence model"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "L Guo",
        "J Dang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Domain adaptive neural networks for object recognition",
      "authors": [
        "M Ghifary",
        "W Kleijn",
        "M Zhang"
      ],
      "year": "2014",
      "venue": "PRICAI 2014: Trends in Artificial Intelligence: 13th Pacific Rim International Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Transfer learning with dynamic adversarial adaptation network",
      "authors": [
        "C Yu",
        "J Wang",
        "Y Chen",
        "M Huang"
      ],
      "year": "2019",
      "venue": "2019 IEEE international conference on data mining (ICDM)"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations",
      "authors": [
        "S Cui",
        "S Wang",
        "J Zhuo",
        "L Li",
        "Q Huang",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "32",
      "title": "Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    }
  ]
}