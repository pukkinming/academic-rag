{
  "paper_id": "2308.14894v1",
  "title": "Multiscale Contextual Learning For Speech Emotion Recognition In Emergency Call Center Conversations",
  "published": "2023-08-28T20:31:45Z",
  "authors": [
    "Th√©o Deschamps-Berger",
    "Lori Lamel",
    "Laurence Devillers"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Multiscale contextual learning",
    "Emotion Recognition in Conversation",
    "Transformers",
    "Emergency call center"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Figure 1: The Multiscale Contextual Learning has been applied for Speech Emotion Recognition on a French emergency call center conversations corpus named CEMO [7]. A subset of this corpus with 4 emotions (Anger, Fear, Neutral and Positive state), described in the left table has been used for this contextual study. The right figure shows the distribution of the previous emotion segments for the 4 target emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Recent Work",
      "text": "In recent years novel methods and techniques have been applied to speech-based downstream applications with a focus on the potential benefits of incorporating conversational information into such systems. This contextual information is usually derived from previous and subsequent utterances in the form of speech transcriptions or acoustic contexts.\n\nAn early significant approach by  [17] , utilized a bidirectional LSTM to assimilate context without distinguish speakers. Extending this methodology,  [12]  incorporated a GRU structure within their ICON model to identify speaker's relationships. Later,  [10] , converted conversations into a graph, employing a graph convolutional neural network for emotion classification. This work was further developed by (almost) the same team, who integrated common-sense knowledge to understand interlocutors' interactions  [9] .\n\nRecent work by  [18]  has used new neural network structures for context understanding. An extension of this approach was proposed in  [13]  which introduced DialogueCRN to fully capture conversational context from a cognitive point of view. These papers illustrate the ongoing evolution of the field.\n\nOngoing research about conversational context in speech task has paralleled the rise of self-supervised pre-training models, which are now popular for handling downstream tasks. These models has shown strong results across various speech tasks benchmarks as highlighted in  [21] . Our paper proposes context-aware fine-tuning, which utilizes surrounding speech segments during fine-tuning to improve performance on downstream speech tasks and enrich Transformer embeddings through the integration of auxiliary context module, as illustrated by  [19]  and by  [15]  with their emotionaware Transformer Emoformer.\n\nIn the field of Speech Emotion Recognition, advances with Transformer models in deep learning have reached state-of-the-art performance on acted speech  [16]  and on widely-known open-source research database like  [2] . Upon appropriate fine-tuning Transformers are able to learn efficient representations of the inputs.\n\nHowever recognizing spontaneous emotions remains a challenge. But remarkably, Transformer encoder models shown significant results over classical approaches on spontaneous emotion recordings  [4] . Through a specific integration of multimodal fusion mechanisms, these models are highly capable of gathering efficient emotional cues across modalities,  [6] . This paper leverages the French CEMO corpus which consists of real-life conversational data collected in an emergency call center  [7] . This corpus provides an excellent opportunity to tackle the challenge of integrating conversation context in a realistic emergency context.\n\nDespite the effectiveness of Transformer models, their standard self-attention mechanism's quadratic complexity limits application to relatively small windows  [3] . Cutting-edge research has focused on optimizing the attention mechanisms to a lower complexity like FlashAttention  [3] , addressing this limitation by lowering the attention complexity paves the way for future models to be trained from scratch on huge datasets with wider context.\n\nIn this work we propose a multi-scale hierarchical training system adapted to pre-trained standard attention models which are available by the French community. The proposed approach draws inspiration from recent work by  [19] . We evaluate the impact of different types of contextual information for acoustic level and manual speech transcription. Integrating the acoustic and linguistic context of dialogue into an emotion detection system remains a challenge, but this work aims to contribute to these ongoing efforts and explain the impact of such a system and their limitations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Conversational Corpus: Cemo",
      "text": "The emergency call center corpus presents a unique opportunity to examine real-world emotional expression. This rich 20+ hour dataset captures naturalistic interactions between callers in crisis and operators. As described by  [7, 20] , it contains emotional annotations across situations ranging from medical emergencies to psychiatric distress. Segments were coded for major and minor emotions with fine-grained labels from 7 macro-classes.\n\nThe caller can be either the patient or a third party (family, friend, colleague, neighbor, stranger). The wide range of caller types (age, gender, origin), accents (regional, foreign), different vocal qualities (alterations due to alcohol/medication, a cold, etc.) also makes it an extremely diverse corpus. As shown in Table  1 , the Caller and Agent emotional profiles differ. Callers expressed intense emotions like fear, anger, and sadness, given their crisis state. In contrast,  agents maintained a regulated presence, with more positive and neutral states, reflecting their professional role.\n\nInter-rater reliability highlights differences between callers and agents. Agreement on emotions was higher for callers than agents (Kappa 0.54 vs 0.35). This suggests agents regulate emotions, producing subtle expressions that are challenging to consistently code. Refining annotation schemes could better capture the complexity of agents' emotional states.\n\nData preparation is key for performance and robustness. As detailed in Table  2 , a balanced CEMO subset (2h40) of 4224 segments was selected for training/validation/testing. The 4 classes were equally distributed with 1056 samples each. Fear and Neutral were subsampled, prioritizing speaker diversity. Anger was completed with agent segments of annoyance/impatience resulting in a class with less speakers diversity and possible bias. Positive had the most speakers and dialogues, suggesting heterogeneity. Manual transcriptions were performed with guidelines similar to the Amities project  [11] .\n\nThe transcriptions contain about 2499 nonspeech markers, primarily pauses, breath, and other mouth noises. The vocabulary size is 2.6k, with a mean and median of about 10 words per segment (min 1, max 47).\n\nFigure  2  represents the transition probabilities between the emotion expressed in the previous speech turn and the target segment. The diagram illustrates the likelihood of moving from each prior emotion category (rows) to each target emotion (columns). Anger persists across turns at a 68% probability. Asymmetry exists between Anger and Fear, with Fear more often following Anger. Surprise is surprisingly followed by Anger, without any wordplay intended.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Our approach aims at recognizing emotions from speech. The systems presented in this article are based on the incorporation of conversational context via pre-trained transformative attention mechanisms. We have divided this section into two main parts, devoted to single modalities (acoustic and textual). Our aim is to better understand the impact of context in these systems.\n\nFirst, we tackled the textual modality, i.e. manual transcriptions of dialogues incorporating the context in a \"blind\" way a defined number of conversational elements (named tokens in pre-trained models). Then, we modified the scale of the contextual window as a function of speech turns, and conducted experiments on specific conversational segments.\n\nIn a second phase, we focused on the acoustic modality, where we exploited the context of speech turns that had been supported by the textual approach. We then extended this to hierarchical training, on the assumption that low-level cues for emotion prediction would be learned by the model during initial context-free training, and that incorporating conversational context in a second phase would enable higher-level information to be learned.\n\nOur methodology is based on the application of specific Transformer encoder models: FlauBERT large  [14]  and wav2vec2.0 large  [1] . These models use self-supervised learning to create meaningful abstractions from text and raw audio data. Prior research  [5]  showed the successful adaptation of pre-trained models to detect discrete speech emotion labels from the CEMO corpus  [7] . From the available models, we chose to use the leBenchmark model (Wav2Vec2-FR-3K)  [8] , trained on 3,000 hours of French language data. This decision was guided by the model's performance on the CEMO corpus  [5] .\n\nThe training database for the wav2vec2-FR-3K model is comprised of spontaneous dialogues recorded by telephone, some with emotional content, thus mirroring the characteristics of the CEMO corpus. The multi-head attention layers were fine-tuned for speech emotion recognition using the CEMO corpus. This was done under the assumption that the initial layers of the model (Convolutional layers and Embedding) are robust to this task  [5, 21] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Contextual Exploration Of Textual Modality",
      "text": "In this research, we propose a fine-tuned system for detecting emotions on the CEMO dataset by incorporating semantic information from the anterior or posterior parts of speech. During training, the context is concatenated with speech inputs to be fed into a Transformer. The proposed system relies on the pre-trained multihead attention layers of the FlauBERT model  [14] , to learn the relationships between the latent states of the current segment and its context. The multi-head attention mechanism allows the model to learn relevant parts of the segment to predict, within its conversational context. To emphasis this weighting, we mask the embeddings yielded by the Transformer corresponding to the context. The rest of the embeddings are fed into an attention pooling layer and classified into discrete emotions.\n\nWe firstly focused on a \"blind\" semantic approach where the context was selected by the amount of tokens. The average number of seconds for one token in the CEMO dataset is equivalent to 0.2s, then we have an average of 5 tokens per second. We performed some experiments with a window of tokens' number from 0 to 100. The results are displayed in the Figure . 5 which shows the UA scores obtained in the prediction of the four discrete emotions. Two regression lines pass through the origin 0 which correspond to the baseline experiment without context.  There is a positive impact of context unevenly distributed between the anterior and posterior conversational contexts. The vious tokens in our tokens are more useful to enrich the segment embeddings to be predicted. Limits to the interpretability of this approach may arise from the semantic perspective, where we are uncertain whether the number of tokens will be extracted from the middle of a sentence or a speech turn.\n\nTo address this hypothesis, we conducted experiments at the speech turn level, using the previous or next segment of speech. We also extended the experiments to speaker type, which could have an impact on how the context is learned by the Transformer.\n\nThe results in Table . 3 detailed the different configurations we used. From the results, it seems that incorporating context from the same speaker outperforms the opposite speaker approach, suggesting that the emotion of a sentence may be more influenced by the speaker's previous sentences rather than the other speaker's. This makes intuitive sense as people's emotions tend to be consistent within a short time frame and are likely to be less influenced by the immediate response from others. The Anger and Fear classes fluctuate the most with context, which may indicate that these emotional states are more complex or nuanced, and may be more influenced by context and speaker.\n\nContextual experiments on the speech turns scale produced better or similar results to those obtained on the token scale, see Fig.  5  and Table . 3. Even with a large token window, up to 100 tokens (sub-words for FlauBERT), equivalent to around 20 seconds of speech, it fails to achieve the best scores, regardless the turn before or after the segment to be predicted.\n\nIn comparison the average context speech turn segments last 1 seconds, thus, the right positioning and semantic meaning of the text is one of Speech Emotion Recognition's keys performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Contextual Exploration Of Acoustic Modality",
      "text": "Our approach to predict emotions from acoustic is similar to the text modality, we concatenate raw audio as input to the acoustic Transformer and mask the embeddings specific to the context produced by the Transformer. At this stage, the wav2vec2 model applies a multi-head attention mechanism on both the surrounding segments and the target segment. This mechanism allows the model to focus on different features in the segment and its surrounding context, potentially improving the emotional relevance of the embeddings produced.\n\nTo adjust the wav2vec2-FR-3K model to our needs, we added an attention pooling layer and a classifier. One drawback of this approach is the higher computational cost of the Transformer acoustic model compared to the textual one. Due to the specifications of our computational clusters, we are limited to a maximum length of around 6.5 seconds for the large wav2vec2 model.\n\nFollowing the results obtained on context at a speech turns level with the text modality, we incorporate the context from the previous or next turn of the target segment. Furthermore we implemented a novel way to enrich the yielded wav2vec2 embeddings through a dedicated auxiliary context module influenced by  [19] . The auxiliary module is detailed in Fig.  5 , it gathers the embeddings from the surrounding segments into a context attention pooling layer. This pool, together with a fully connected network, generates a context vector that provides a compact, informative representation of the surrounding context.\n\nIn the equation above, ùê∂ ùëñ is the context vector for the ùëñ-th segment, ùê∏ ùëñ signifies the embeddings of the target segment, and ùëÜ ùëñ is the input segment. The context vector is then concatenated with each of the embeddings of the target segment, effectively underline the contextual information into the final classifier prediction.\n\nThe Table  4  presents results evaluating the incorporation of contextual acoustic information to enhance emotion recognition performance of wav2vec2 embeddings. Across conditions, two proposed context integration methods were examined -masking the context embedding (MWCE) and concatenating context features with target embeddings (CCFTE) -using either previous or next utterances as context.\n\nNotably, the baseline wav2vec2 model with no context elicited the highest total unweighted accuracy (UA) of 75.6%, exceeding all context-enhanced models. This suggests intrinsic limitations of the concatenation-based context integration approaches assessed. Both MWCE and CCFTE concatenation utilizing prior context modestly boosted performance to 75.4% UA. However, next context yielded negligible gains, indicating contextual benefits may be asymmetric.\n\nDespite the disappointing results of our preliminary experiments using acoustic models trained on isolated utterances, we continue to further explore this approach building on prior textual results. We were seeking of a way to leverage the meaningful contextual signals that could be present in adjacent turns. We shifted the method in a hierarchical training framework where first, acoustic models were trained on the target segments using isolated utterances without conversational context. Subsequently, we fine-tuned the model to adapt to the surrounding conversation segments, thereby learning higher-level emotional cues that are context-dependent. Simultaneously, we train a parallel model from the same baseline checkpoint to serve as a comparison, ensuring our fine-tuning process contributes positively to the emotion prediction task.\n\nThe obtained results, detailed in Table  5  demonstrate the limited gains achieved through hierarchical fine-tuning with concatenated context. Critically, all context-enhanced models fail to improve over the baseline wav2vec2 model at 76.2% UA. This implies significant shortcomings in the concatenation-based context integration paradigm.\n\nAlthough small improvements are achieved using the previous context with MWCE+CCFTE, the global hierarchical learning methodology provides insignificant improvements to acoustic modeling. These results reveal shortcomings compared to text-based modeling approaches.\n\nIn particular, the minimal gains from concatenating context features (CCFTE) reveal this technique inadequately incorporates conversational patterns. The embedding masking (MWCE) is somewhat more beneficial, but the context integration remains insufficient.\n\nWe furthermore tried other experiments which not yielded better results, these experiments where based on MFCC cues of the surroundings segments. The Figure  6 , illustrates the distribution of previous emotion labels of the 4 targeted emotions. To compare the results obtained with conversational context, we took the same configuration with context taken from previous segments (whatever the speakers) for two different sets of predictions: speech transcriptions (T) and acoustic (A). Both models performance are respectively 71.2% (T) and 76.2% (A) UA (see Table  3  and 4 ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis Of Prediction Accuracy Based On The Previous Segment'S Emotion",
      "text": "Across both experiments, the Positive emotion and Neutral state segments seem to be predicted most accurately when the previous emotion is also Positive, results from 86.7% to 88.6% UA for both acoustic and transcriptions. The best results for Fear is obtained from Anger previous segment, 77.8% (A) and 70.8% (T). For Anger class an high UA is obtained for the segments with anterior Fear emotion expressed. The acoustic and textual models results are heterogeneous for the Anger class, the acoustic model is outperforming textual model when the previous segment was Fear (89.5% (A) vs. 71.5% (T)), on the other hand when the previous segment was Anger, the textual model had great results over the acoustic model (68.7% (T) vs. 58.7% (A)).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper explored Multiscale Contextual Learning for Speech Emotion Recognition in emergency call center conversations using the CEMO corpus collected in-the-wild. We conducted experiments incorporating contextual information from both speech transcriptions and acoustic signals with varying scales of the context. Overall, acoustic models demonstrate superior performance compared to text models, Table  3, 5 .\n\nFor text modeling with FlauBERT's Transformer embeddings, the context derived from previous segment has a more significant influence on accurate prediction than the following segment. Furthermore, taking the last speech turn of the same speaker in the conversation leads to better results in Table  3 .\n\nFor acoustic modeling with wav2vec2.0 Transformer embeddings, we did not improve our results by using contextual information, Table  4 . Despite pursuing a hierarchical training framework, Table  5 , the results are disappointing and reveal challenges in effectively modeling sequential unimodal acoustic context using feature concatenation.\n\nWe also conducted an in-depth analysis of the impact of the previous emotions on the predictions. While multi-scale conversational context learning using Transformers can enhance performance in the textual modality for emergency call recordings, incorporating acoustic context is more challenging, see Table  4 . Advanced context modeling techniques are needed to fully leverage conversational dependencies in speech emotion recognition. Extending the context to model inter-speaker dynamics and relationships throughout full conversations is an important direction. Advances in attention mechanisms to handle wider contexts will also enable further progress on context-aware speech emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ethics And Reproducibility",
      "text": "The use of the CEMO database or any subsets of it, carefully respected ethical conventions and agreements ensuring the anonymity of the callers. All evaluations are performed on 5 folds with a classical cross-speaker folding strategy that is speaker independent between training, validation and test sets. During each fold, system training is optimized on the best Unweighted Accuracy (UA) of the validation set. The outputs of each fold are combined for the final results. The experiments were carried out using Pytorch on GPUs (Tesla V100 with 32 Gbytes of RAM). To ensure the reproducibility of the runs, we set a random seed to 0 and prevent our system from using non-deterministic algorithms. This work was performed using HPC resources from GENCI-IDRIS (Grant 2022-AD011011844R1).",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Multiscale Contextual Learning has been applied for Speech Emotion Recognition on a French emergency call",
      "page": 1
    },
    {
      "caption": "Figure 2: represents the transition probabilities between the emo-",
      "page": 2
    },
    {
      "caption": "Figure 2: Transition between the previous and current emo-",
      "page": 3
    },
    {
      "caption": "Figure 3: Histogram of gap duration between context segment",
      "page": 3
    },
    {
      "caption": "Figure 3: displays a histogram which illustrates the distribution",
      "page": 3
    },
    {
      "caption": "Figure 4: Prediction Accuracy vs. Context Token Count: To-",
      "page": 4
    },
    {
      "caption": "Figure 5: and Table. 3. Even with a large token window, up to 100",
      "page": 4
    },
    {
      "caption": "Figure 5: Illustration of CCFTE Concatenation of Context",
      "page": 4
    },
    {
      "caption": "Figure 5: , it gathers the embeddings from the",
      "page": 4
    },
    {
      "caption": "Figure 6: Prediction accuracy of the target emotions based",
      "page": 5
    },
    {
      "caption": "Figure 6: , illustrates the distribution of previous emotion",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , the Caller and",
      "page": 2
    },
    {
      "caption": "Table 1: The 10 most represented emotions and mixtures",
      "page": 2
    },
    {
      "caption": "Table 2: Details of the CEMO subset of speech signals and",
      "page": 2
    },
    {
      "caption": "Table 2: , a balanced CEMO subset (2h40) of 4224 segments",
      "page": 2
    },
    {
      "caption": "Table 3: Comparison of Textual Models (on manual tran-",
      "page": 4
    },
    {
      "caption": "Table 4: Comparison of Acoustic Models Using wav2vec2 Em-",
      "page": 5
    },
    {
      "caption": "Table 5: Hierarchical Training: Fine-tuning of Models with",
      "page": 5
    },
    {
      "caption": "Table 4: presents results evaluating the incorporation of",
      "page": 5
    },
    {
      "caption": "Table 5: demonstrate the limited",
      "page": 5
    },
    {
      "caption": "Table 3: For acoustic modeling with wav2vec2.0 Transformer embed-",
      "page": 6
    },
    {
      "caption": "Table 4: Despite pursuing a hierarchical training framework,",
      "page": 6
    },
    {
      "caption": "Table 5: , the results are disappointing and reveal challenges in effec-",
      "page": 6
    },
    {
      "caption": "Table 4: Advanced context",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "Advances in Neural Inform. Process. Systems"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive Emotional Dyadic Motion Capture Database",
      "authors": [
        "C Busso",
        "M Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "3",
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "authors": [
        "Tri Dao",
        "Daniel Fu",
        "Stefano Ermon",
        "Atri Rudra",
        "Christopher R√©"
      ],
      "year": "2022",
      "venue": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "arxiv": "arXiv:2205.14135"
    },
    {
      "citation_id": "4",
      "title": "End-to-End Speech Emotion Recognition: Challenges of Real-Life Emergency Call Centers Data Recordings",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2021",
      "venue": "ACII"
    },
    {
      "citation_id": "5",
      "title": "Investigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2022",
      "venue": "Investigating Transformer Encoders and Fusion Strategies for Speech Emotion Recognition in Emergency Call Center Conversations"
    },
    {
      "citation_id": "6",
      "title": "Exploring Attention Mechanisms for Multimodal Emotion Recognition in an Emergency Call Center Corpus",
      "authors": [
        "Theo Deschamps-Berger",
        "Lori Lamel",
        "Laurence Devillers"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49357.2023.10096112"
    },
    {
      "citation_id": "7",
      "title": "Challenges in Real-Life Emotion Annotation and Machine Learning Based Detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural networks: INNS",
      "doi": "10.1016/j.neunet.2005.03.007"
    },
    {
      "citation_id": "8",
      "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech",
      "authors": [
        "S Evain",
        "H Nguyen",
        "H Le"
      ],
      "year": "2021",
      "venue": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech"
    },
    {
      "citation_id": "9",
      "title": "COSMIC: COmmonSense Knowledge for eMotion Identification in Conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "10",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "11",
      "title": "Multi-Layer Dialogue Annotation for Automated Multilingual Customer Service",
      "authors": [
        "H Hardy",
        "K Baker",
        "L Devillers"
      ],
      "year": "2003",
      "venue": "ISLE"
    },
    {
      "citation_id": "12",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "13",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "14",
      "title": "FlauBERT: Unsupervised Lang. Model Pretraining for French",
      "authors": [
        "H Le",
        "L Vial",
        "J Frej"
      ],
      "year": "2020",
      "venue": "Twelfth Lang. Resources and Evaluation Conf"
    },
    {
      "citation_id": "15",
      "title": "EmoCaps: Emotion Capsule Based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "EmoCaps: Emotion Capsule Based Model for Conversational Emotion Recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "16",
      "title": "A Survey of Transformers",
      "authors": [
        "Tianyang Lin",
        "Yuxin Wang",
        "Xiangyang Liu",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "AI Open",
      "doi": "10.1016/j.aiopen.2022.10.001"
    },
    {
      "citation_id": "17",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "18",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "19",
      "title": "Context-Aware Fine-Tuning of Self-Supervised Speech Models",
      "authors": [
        "Suwon Shon",
        "Felix Wu",
        "Kwangyoun Kim",
        "Prashant Sridhar",
        "Karen Livescu",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP49357.2023.10094687"
    },
    {
      "citation_id": "20",
      "title": "Detection of Real-Life Emotions in Call Centers",
      "authors": [
        "Laurence Vidrascu",
        "Laurence Devillers"
      ],
      "year": "2005",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "21",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf"
      ],
      "year": "2022",
      "venue": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap"
    }
  ]
}