{
  "paper_id": "2208.08909v2",
  "title": "\"Are You Okay, Honey?\": Recognizing Emotions Among Couples Managing Diabetes In Daily Life Using Multimodal Real-World Smartwatch Data",
  "published": "2022-08-16T22:04:12Z",
  "authors": [
    "George Boateng",
    "Xiangyu Zhao",
    "Malgorzata Speichert",
    "Elgar Fleisch",
    "Janina Lüscher",
    "Theresa Pauly",
    "Urte Scholz",
    "Guy Bodenmann",
    "Tobias Kowatsch"
  ],
  "keywords": [
    "Affective Computing",
    "Emotion Recognition",
    "Multimodal Sensor Data",
    "Couples",
    "Smartwatches",
    "Wearable Computing",
    "Speech Processing",
    "Natural Language Processing",
    "Machine Learning",
    "Deep Learning",
    "Transfer Learning",
    "BERT",
    "Chronic Disease Management"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Couples generally manage chronic diseases together and the management takes an emotional toll on both patients and their romantic partners. Consequently, recognizing the emotions of each partner in daily life could provide an insight into their emotional well-being in chronic disease management. Currently, the process of assessing each partner's emotions is manual, time-intensive, and costly. Despite the existence of works on emotion recognition among couples, none of these works have used data collected from couples' interactions in daily life. In this work, we collected 85 hours (1,021 5-minute samples) of real-world multimodal smartwatch sensor data (speech, heart rate, accelerometer, and gyroscope) and self-reported emotion data (n=612) from 26 partners (13 couples) managing diabetes mellitus type 2 in daily life. We extracted physiological, movement, acoustic, and linguistic features, and trained machine learning models (support vector machine and random forest) to recognize each partner's self-reported emotions (valence and arousal). Our results from the best models -balanced accuracies of 63.8% and 78.1% for arousal and valence respectively -are better than chance and our prior work that also used data from German-speaking, Swiss-based couples, albeit, in the lab. This work contributes toward building automated emotion recognition systems that would eventually enable partners to monitor their emotions in daily life and enable the delivery of interventions to improve their emotional well-being. CCS Concepts: • Human-centered computing → Ubiquitous and mobile computing; • Applied computing → Consumer health; Psychology.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "For couples in which one partner has a chronic disease such as cancer and diabetes, their relationship plays a key role in the disease management if partners share the responsibility of its management  [44, 51] . Such joint disease management, also called dyadic coping  [5, 16, 43]  takes an emotional toll on both patients and spouses  [52] . Consequently, understanding each partner's emotion within the context of their interactions and disease Fig.  1 . Russell's circumplex model of emotions  [57]  work is an extension of the work  [10]  and implements the research plan that was proposed in that work, along with machine learning experiments and reported results.\n\nIn the rest of this paper, we discuss background and related work in Section 2, methodology in Section 3, experiments and evaluation in Section 4 and results and discussion in Section 5, limitations and future work in Section 6, and we conclude in Section 7.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background And Related Work",
      "text": "In this section, we describe various emotion models, multimodal emotion recognition, and works that have been done to recognize emotions among couples.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Models",
      "text": "There are mainly two models of emotions used in the literature in emotion recognition: categorical and dimensional. Categorical emotions are based on the six basic emotions proposed by Ekman: happiness, sadness, fear, anger, disgust, and surprise  [26] . Dimensional approaches mainly use two dimensions: valence (pleasure) and arousal which are based on Russell's circumplex model of emotions  [47] . Valence refers to how negative to positive a person feels and arousal refers to how sleepy to active a person feels. Using these two dimensions, several categorical emotions can be placed and grouped into the four quadrants: high arousal and negative valence (e.g., angry), low arousal and negative valence (e.g., depressed), low arousal and positive valence (e.g., relaxed), and high arousal and positive valence (e.g., excited)  [47] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal fusion entails combining data collected from various modalities and leverages the idea that data contained in different modalities could provide a better understanding of a certain context. Various works have employed multimodal fusion approaches for emotion recognition and they have been shown to give better results than unimodal approaches  [25, 39] . There are two main fusion approaches -fusion at the feature level (early fusion) and at the decision level (late fusion). Early fusion entails combining features from different data modalities, for example, through concatenation and feeding them into the same machine learning algorithm. For late fusion, a separate algorithm is used for each data modality and then the predictions of the individual algorithms are combined using, for example, majority voting. Additional approaches include some hybrid of early and late fusion  [56]  and model-level fusion which leverages interactions between different modalities at the model level e.g  [30] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition Among Couples",
      "text": "Emotion recognition among couples is the task of recognizing the emotion of each romantic partner based on the context of their interaction /conversation  [11] . Specifically, it entails recognizing each partner's emotions for every utterance/speaker turn, every few seconds, or for the whole conversation. It differs from other kinds of emotion recognition tasks mainly by the kind of stimuli that induces emotions. Some stimuli are driving  [58] , listening to music or watching a movie  [2] , and conversation between people  [40] . Couples' emotion recognition is similar to emotion recognition tasks whose stimuli are conversations since it uses a conversational context. However, its uniqueness lies in the fact that the two interacting individuals are in a romantic relationship. Consequently, various insights from psychology about couples' interaction dynamics could be leveraged to recognize each partner's emotions. For example, romantic partners influence each other when interacting, and that insight has been used for couples' emotion recognition (e.g.,  [12, 21] ).\n\nThere are several works that have developed machine learning systems to recognize the emotions among couples (see  [11]  for a detailed overview of the research field). Most of these works have been done by the Signal Analysis and Interpretation Laboratory (SAIL) team at the University of Southern California  [11] . The works have mainly used emotion labels from external raters, support vector machines as the algorithm, the following three modalities -acoustic, lexical, and visual -with acoustic being the most used modality, and feature-level fusion of acoustic and lexical modalities  [11] . There are other related work focused on recognizing behaviors among couples other than emotions such as level of blame  [8, 9] , conflict  [53] , and suicidal risk  [23] .\n\nMost of these works have used observer ratings (perceived emotions) rather than self-reports (one's actual emotions) as labels. Consequently, the emotion recognition task essentially becomes recognizing external individuals' perception of each partner's emotion rather than each partner's emotion per their own assessment. Though similar, the latter is more challenging. For observer ratings, coders are generally trained over several weeks, and various approaches are used to resolve ratings that are not in agreement and ensure the validity of the labels. Also, the self-reported emotion may not be reflected in that partner's behavior in comparison to observer ratings which are purely based on behavioral observation.\n\nAlso, several of these works have used data from English-speaking couples in the U.S. with a few using data from German-Speaking couples in Switzerland  [7, 12]  and Dutch-speaking couples in Belgium  [15] . Additionally, several modalities such as physiological data, hand gestures, and body movement have not been explored. More importantly, none of these works have used data collected from couples' interactions in daily life. Our work fills the current research gap by performing emotion recognition using multimodal real-world smartwatch dataspeech, accelerometer, gyroscope, heart rate -and self-reported emotion data collected from German-speaking, Swiss-based couples.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition Using Smartwatch Data",
      "text": "There are a number of works that have performed emotion recognition using smartwatch data. AlHanai et al.  [3]  trained neural network models to recognize emotions using smartwatch and smartphone data collected from 10 subjects who told 31 personal stories (15 happy) in a lab. They used an iPhone to collect audio which they transcribed. They also collected physiological and movement data with the smartwatch. They extracted 386 acoustic (functionals over low-level descriptors), linguistic (average positive and negative sentiment of words), physiological, and movement features (mean, median, variance of electrocardiogram, photoplethysmogram, accelerometer, gyroscope, bioimpedance, electric tissue impedance, galvanic skin response, and skin temperature) and selected 10 features for use using sequential forward features selection. They classified the whole narration as happy or sad (indicated by the subject) and 5-sec segments as positive, negative, or neutral (annotated by a research assistant with balanced distribution). Though they used naturalistic data (personal narratives), this work did not use data collected from an uncontrolled, real-world context such as in daily life.\n\nBudner et al.  [18]  trained Random Forest models to recognize moods using smartwatch data collected from 60 subjects in daily life. They classified 9 mood states (angry, sad, tired, excited, happy, quiet, elated, very happy, relaxed) and 3 levels of pleasure and activation. They extracted the following features related to body sensor data: vector magnitude counts (a measure of the total amount of movement), heart rate, and external influences: light level and GPS coordinates (variance), weather, (humidity, temperature, cloudiness, windiness, air pressure), the hour of the day, whether it was the weekend and day of the week. Arano et al.  [4]  built upon the work by Budner et al.  [18]  and proposed the use of the smartwatch-based system to measure emotions in a real-world scenario: classroom. They were able to collect data from 30 subjects related to body sensor data: accelerometer, light, audio, heart rate (from a smartwatch), GPS data (from a smartphone), and environmental variables (e.g., weather, longitude, latitude, altitude, room temperature, humidity, pressure, wind level, clouds level, noise level). Subjects indicated their level of Activation, Tiredness, Pleasance, Quality (of lecturer's presentation), and Understanding (of lecturer's presentation) on a scale from 0-2. They extracted statistical features and used 9 models: K-Nearest Neighbor, Decision Trees, Support Vector Machines, Multilayer perceptron, logistic regression, Gradient Boost, XGBoost, and LSTM.\n\nKanjo et al  [31]  developed models to recognize emotions using body sensor and environmental data collected from the wild. They collected data from 40 females walking around Nottingham city center, UK for 45 mins: body movement, activity, heart rate, Electrodermal activities and body temperature and, environmental data including noise level (Env-noise), air pressure and ambient light levels, and GPS data. User emotions labels are collected using self-report input, based on a scale for valence (1-5). They used the Microsoft Band and Android phones (to collect noise, GPS, and self-report). They extracted 87 features: mean, median, max, min, range, and standard deviation and quartiles and selected 21 after feature selection. They trained ensemble models (stacking) to perform classification of the 5 levels of valence. They had a base model for each modality and a stacking model which fused the results of both models. They used the following models: Support Vector Machine, Random Forest, and K Nearest Neighbour as the base models, and Naive Bayes as the stacking model Learner which fused the base models' predictions.\n\nQuiroz et al.  [41]  developed a smartwatch-based method to recognize emotions based on movement data. They collected data from 50 subjects: (43 females; mean age  23.18 [SD 4 .87] years), North-West, UK. They collected emotion data with the PANAS before and after emotion elicitation; happy, sad, and neutral. They used audiovisual movie clips and audio music clips to elicit emotions. They asked the subject to walk for 250 meters while wearing a smartwatch and heart rate monitor strap on the chest. 18 were assigned to the audiovisual condition and watched the movie before walking. Out of the 32 assigned to audio, half of them listened while walking. It took 20 mins for each subject. They extracted 107 features over 1-second sliding windows with 50% overlap over filtered signal (accelerometer and gyroscope, and heart rate). They trained personalized models from 44 subjects to classify happy vs sad and happy vs sad vs neutral. The data was balanced. They used 10-fold stratified cross-validation with logistic regression and random forest.\n\nSchmidt et al, 2019  [48]  trained a convolutional neural network (CNN) model to predict emotions (arousal, valence), anxiety, and stress from real-world smartwatch-based physiological and motion data. They used an Empatica E4 to collect 1,400 hours of data; accelerometer, photoplethysmogram (PPG), EDA, and skin-temperature data from 12 subjects (7 male). Subjects received EMA prompts every 2-2.5 hours or triggered manually: 1) selfassessment mannequins assessing valence and arousal 2) State-Trait Anxiety Inventory (STAI) on 6 levels and 3) Stress level scored on a four-point Likert scale. The data was skewed for all the labels. They preprocessed the data resulting in 1083 valid windows/questionnaires. The data was split between 3 levels for all the labels except stress which was binarized. They extracted 62 features (e.g., mean, standard deviation, heart rate, heart rate variability). They used leave-one-subject-out cross-validation(LOSO) and leave-target-questionnaires-out (LTQO). As baseline models, they used different tree-based classifiers (decision-tree (DT), randomized decision trees (ET), and random forest (RF)). They used a single-task and multi-task CNN which takes the raw sensor data as the main model with late fusion.  Park et al., 2020 [38]  developed WellBeat, a smartwatch-based system for assessing the emotional well-being of individuals. They used a Samsung smartwatch to collect PPG and heart rate data from 12 subjects (3 female) continuously throughout the day: 1121 hours of data from a 3-week study (about 445 hrs eliminated) and 1032 self-report labels related to happiness, awakeness, and relaxedness levels (1-5). Subjects were asked to complete the self-report 3 times a day at random times during waking hours. They performed data preprocessing by removing samples where the watch was not worn, partitioned data into consecutive 5-min slices, filtered out signals without heart rate signals, extracted heart rate and RR intervals, and HRV parameters such as RMSSD, and estimated their validity. The label was matched to the day -10 mins to +10 mins around the label timestamp similar to  Schmidt et al (2019) . They performed classification with logistic regression and 10-fold cross-validation.\n\nOur work builds upon some of these works by using similar preprocessing approaches and features (for heart rate, accelerometer data, speech data, gyroscope), algorithms and evaluation approach. One modality that is missing in most of these works is the linguistic modality. We leveraged recent advances in deep learning and natural language processing to extract linguistic features from speech. Also, we systematically evaluated the performance of individual modalities and various modality combinations. The key way our work differs from these works though is our use of the context of couples' interactions in daily life to recognize each partner's emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we describe how we collected and preprocessed the data and the features that we extracted.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Collection",
      "text": "We developed DyMand, an open-source smartwatch and smartphone system which we used to collect data from couples in daily life in a user study (see  [13]  for a detailed description). The DyMand system (Figure  2 ) consists of a smartwatch app, and a smartphone app built on top of the MobileCoach platform  [28, 34]  that consists of a web-based intervention designer and backend.\n\nWe ran the DyMand study between 2019 and 2021 with heterosexual romantic couples from the Germanspeaking part of Switzerland in which one partner had type 2 diabetes  [35] . In total, we collected 85 hours of sensor and self-report data from 13 couples aged 47 to 81 years, with a mean age of 68 (SD = 9).\n\nThe study was advertised in various places including hospitals, magazines, local newspapers, and the diabetes association in Switzerland. Interested couples completed a web-based questionnaire to screen them for the inclusion and exclusion criteria, and collect socio-demographic information. Those who met the eligibility criteria were able to pick a date for a baseline assessment. During this session, both partners received comprehensive information about the study, signed the informed consent form, and completed a web-based questionnaire that captured constructs of interest at baseline that were not assessed daily.\n\nEach partner was given a smartwatch (Polar M600 running Wear OS) and a smartphone (Nokia 6.1 running Android 9.0), both paired and running the DyMand apps. They also received instructions on the study and then trained research assistants helped them to set up their devices and pair the corresponding smartphone and smartwatch. They were instructed to have all devices with them every day for 7 days from getting up until going to bed. To prevent mistakes from one partner accidentally using the other partner's watch and phone, one set of phones and watches had black covers and the other set had white covers. The patient was given the white set and the supporting partner was given the black set. The partners picked the hours during which we could collect Fig.  2 . Overview of the DyMand system data from them. During the week, they could choose a period for the morning hours (any time between 4 am to 11 am, at least 2 hrs) and a period for the evening hours (any time between 4 pm to 11 pm, at least 2 hrs). During the weekend, data was collected all day and couples chose a start time in the early morning hours and an end time in the late evening hours (e.g., from 6 am to 10 pm). With this procedure, privacy aspects were addressed by primarily focusing on situations, in which the couples spent time together and thus reducing the number of audio recordings during the day of weekdays when chances are higher that subjects are working or moving around in public places.\n\nWe collected data from their daily life for 7 consecutive days starting the next Monday after their visit until the following Sunday night. The DyMand system triggered the collection of sensor and self-report data for 5 minutes each hour during the hours that partners pick. We collected the following sensor data from the smartwatch: audio, heart rate, accelerometer, gyroscope, Bluetooth low energy (BLE) signal strength between watches, and ambient light.\n\nWe collected a maximum of 5 minutes of data per hour for privacy reasons. Hence, to optimize the quality of data collected within that hour and to ensure that we recorded the most relevant 5 minutes of data (when partners are interacting), rather than triggering data collection at random or scheduled times which is the norm (  [36, 45] ), the app on each of the two smartwatches collected data when 1) the partners were physically close and 2) when there was speech (see  [13]  for the full details).\n\nOur algorithm used a two-step process. First, the app determines physical closeness using the BLE signal strength between the two smartwatches with one acting as the central and the other acting as the peripheral. The central smartwatch scans for the peripheral device, and checks if the signal strength between them is within a certain threshold, which corresponds to a distance estimate. If this condition is met, the app on the central device determines if the partners are speaking by using a voice activity detection (VAD) machine-learning algorithm, which is implemented on the smartwatch  [14] . If these two conditions are met, the central device connects with the peripheral smartwatch, starts recording, and also sends a signal to the peripheral watch to also start recording. Consequently, each smartwatch records the same interaction, albeit, sometimes with a start delay of a few seconds Fig.  3 . Emotion rating with Affective Slider on the peripheral smartwatch. It is important to note that depending on the proximity of the partners, and the presence of other individuals, parts of ongoing conversations were captured to different degrees separately on each of the smartwatches even for the same recording time period. Hence it cannot be assumed that the two recordings at the same hour and minute are exact duplicates. For example, there was a case where both partners were together with two friends with all four being in proximity, having conversations. Yet, the male partner was talking directly with the male friend, and the female partner was talking directly with the female friend, and though the two smartwatches were recorded at the same time, they captured different conversations.\n\nIn the case in which the condition of physical closeness and speaking is not met in the hour, the app triggers a backup recording in the last 15 minutes of the hour. Our evaluation showed this approach for triggering data collection to capture conversation moments between partners performed better than the backup recording  [13] . The app also ensured that there were at least 20 minutes between subsequent data collection to reduce the burden of the partners completing the self-reports.\n\nAfter the 5-minute sensor data collection, the smartwatch vibrates and triggers a self-report on the smartphone for that partner to complete. The self-report asks about emotions over the last 5 minutes using the Affective Slider, a digital affect measuring tool that assesses the valence and arousal dimensions of their emotions  [6] . In particular, they responded to \"how unhappy vs. happy did you feel in the last 5 minutes?\" and \"how tired vs. awake did you feel in the last 5 minutes\" by moving a slider from 0 to 100 on a visual scale -the Affective Slider (Figure  3 ). If the smartwatch does not receive a message from the smartphone app within 2 mins indicating that the self-report has been started, it gives another vibration alert. If once more, within the next 2 minutes, there is still no response about the start or completion of the self-report, it implies the self-report was not completed. The self-report is then dismissed. Doing this ensured that we collected data with matching sensor and self-report samples. For privacy reasons, the app deletes that audio sample if the self-report is not completed and attempts to trigger another sensor data collection and self-report later in the hour, optimizing for the case detection partner's Fig.  4 . Screenshot of the annotation process of the audio interactions. Other sensor samples are still kept which could result in several sensor samples per hour without audio. If a backup recording is done, which implies that it was the last recording in that hour, the audio is not deleted even if the self-report is not completed. Doing this ensures we have at least one audio recording per hour. This approach resulted in a significant number of sensor recordings without labels. At the end of the day, the system triggered the Affective Slider, and also a short form of the PANAS self-report  [55]  for the couples to report their emotions over the whole day.\n\nThere are significant ethical and privacy concerns of such a system and study since we collect audio which is sensitive data, and more so in the context of couples' interactions with the likelihood of speech about private topics. We took several measures as follows. First, our study received ethical clearance from the cantonal ethics committee of the Canton of Zurich, Switzerland (Req-2017_00430). Second, we ensured that we collected a maximum of 5 minutes of audio per hour in order not to record a significant percentage of the couples' everyday life. Consequently, even if the system triggered multiple recordings in the hour, the app always deleted all but the last one before the end of the hour. Third, to protect the privacy of subjects not taking part in the study, we asked subjects to wear a tag that we give them to indicate to others around that recording may be happening and that they may be recorded. Finally, after subjects returned their devices, we gave them the option to listen to and request the deletion of any audio samples without any explanation before the study team could listen to the audio files. Similar measures have been used in previous studies  [36, 45]  and have proven adequate to safeguard the privacy of study subjects and others not taking part in the study.",
      "page_start": 6,
      "page_end": 9
    },
    {
      "section_name": "Data Annotation, Transcription And Coding",
      "text": "Four trained research assistants (RA) annotated, transcribed, and coded the audios. Using the software Audacity, each 5-minute audio was annotated with the start and end times of the speaker turns of each partner (m, f), unknown speakers (u), cross-talk between partners (c), vocalizations such as laughs, sighs (v) and the context (e.g., TV, radio), silence with no one speaking (p), noise such as music, movements of the watch, vehicles, etc. (n) and speech from radio or tv (u-tv/radio) (Figure  4 ).\n\nThe speech of both partners within each audio was transcribed in separate documents. In particular, each Microsoft Word document was used for each partner and RAs wrote the transcript in 15 secs chunks with \"//\" to separate the chunks. RAs wrote the German equivalent of any Swiss German words that were used since Swiss German is not a written language and there are different dialects of Swiss German spoken in Switzerland. Words that were not intelligible were written as \"XY\" in the document.\n\nRAs coded the context of each audio in a spreadsheet as they listened to the audio using a protocol based on Mehl et al.  [36] . They indicated if the audio contained speech, each partner spoke, and there was a conversation and a conversation between both partners. They also provided information about the conversational context (what was going on in the audio), location, interaction partners, conversation type, activity, and emotional expression (Figure  5 )\n\nThe real-world nature of the data posed challenges for our RAs. There were cases where the partners were having conversations with friends which made it difficult to distinguish the voices. For other cases, one partner was far away from the smartwatch while speaking, making it difficult to hear their voice. Given the challenging nature of the annotation, transcription, and coding tasks using real-world data (85 hours of audio) and the susceptibility to error, we implemented several manual and automatic approaches to perform sanity checks. We reviewed the codes to make sure the entries for different fields were consistent. For example, we cross-checked that if it is indicated that both partners spoke, then the field \"interaction partner\" should be \"romantic partner\". Also, for each audio file, we automatically checked if there existed a non-empty transcription file and an annotation file with 'm' or 'f' if there was a \"yes\" for \"male spoke\" or \"female spoke\". We also verified the accuracy of the annotations by automatically checking that for each 15-second chunk in the transcript file that contained text for the male or female partner, there existed an 'm' or 'f' in the corresponding 15-sec time period in the annotation file. We computed a percentage overlap for 'm' or 'f' with the corresponding transcript as a proxy for the quality of the annotation of that audio. Furthermore, we computed the percentage of \"XY\"s -inaudible words -for each audio that had speech which was a proxy for the quality of the audio and difficulty of the transcription task for that audio. RAs were given a list of files that failed these checks to then fix.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Data Preprocessing",
      "text": "We had a total of 612 self-report samples consisting of arousal and valence ratings (0 -100) of each partner collected using the Affective Slider after the sensor data collection. We had a total of 1021 5-minute samples of sensor data (85 hours) consisting of audio, heart rate, accelerometer, gyroscope, and ambient light collected from each partner's smartwatch. Some of the 5-minute samples were without audio data since the data collection protocol resulted in the deletion of audio samples without completed self-reports due to privacy reasons. Furthermore, because of software errors, a few of the sensor data were collected outside the data collection window and some audios were corrupted and hence could not be played or processed. We inferred these audios by eliminating audios whose size was smaller than the expected file size for 5-minute audio. Consequently, we automatically selected 5-minute samples that met the following conditions: 1) had both audio (non-corrupted) and other sensor data, and 2) were within the data collection hours specified by the partners. In total, that resulted in 1014 5-minute samples. Given our task is a supervised learning task, we also filtered and selected sensor samples that had a corresponding completed self-report.\n\nWe binarized the arousal and valence data into high (above 50) and low (less than or equal to 50) for arousal and negative (less than or equal to 50) and positive (above 50) for valence similar to the approach by previous  works  [7, 12, 15] . With binarization, the binarized arousal and valence labels can be mapped to one of the four quadrants of Russell's circumplex model of emotions, enabling its usefulness in the real world since we can tell which group of emotions are being felt by each partner. We split at 50 because with the design of the Affective Slider, 50 was understood to be the midpoint for the labels while partners responded to it. Furthermore, taking a per subject median rating as the midpoint would be problematic if there is not a good distribution of ratings for that partner (e.g., if there are just 3 ratings which are all 80, 90, and 100, it will not be correct to assume that 80 implies negative emotions for them). Next, we filtered for samples for which we had 'yes' for 'male spoke' and 'female spoke' as a proxy for the context of a conversation between both partners. This filtering resulted in 380 sensor-self-report samples: 20 negative valence and 360 positive valence, 97 low arousal, and 283 high arousal. The data is highly imbalanced which is typical of real-world emotion data. Table  1  shows the sensor-self-report samples per gender. Figures  6,  and 7  show the distributions of low and high arousal and negative and positive valence per couple per gender. We observe the skewness of the labels per couple with some couples' data not containing any negative valence samples (1, 2, 5, 11, 13) and any low arousal samples  (2, 11) . We filtered the audio using a low-pass filter with a cut-off frequency of 4 kHz given it was collected as raw audio at a sampling rate of 44.1KHz and human speech is less than 4 kHz. For each 5-minute data for all the modalities, we removed outliers (data points that were more than 2 standard deviations from the mean). We resampled the data points at 50 Hz for accelerometer and gyroscope and 1 Hz for heart rate given that the Wear OS platform did not sample the signal at our specified frequency in the app. We additionally preprocessed the heart rate data to remove outlier samples such as heart rate values outside the normal range (30 to 200 beats per minute). We automatically inferred samples for which the watch was not worn. For each physiological and movement sample, we logged a value (between 0 and 3) that provides a confidence estimate of the wear state of the watch. If 50% of the data points within the 5-minute sample had a value of 0 for the wear state, we marked the same as non-worn.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Features Extraction",
      "text": "We describe the extraction of physiological, movement, context, linguistic and acoustic features.  [31] , we extracted the following statistical features from the heart rate data: mean, median, max, min, 25th percentile, 75th percentile, standard deviation, range, skewness, and kurtosis. The extraction resulted in a 10-dimensional feature vector.  [41, 48] , we extracted the following statistical features from the accelerometer and gyroscope: mean, median, max, min, 25th percentile, 75th percentile, standard deviation, range, skewness, and kurtosis. For the accelerometer and gyroscope data, we computed the magnitude of the x, y, and z axes before using them to compute the features. We did this so that the orientation of the device does not affect the results. The extraction resulted in a 10-dimensional feature vector.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Physiological. Similar To Prior Work",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Movement. Similar To Prior Work",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Acoustic Features.",
      "text": "For speech, we used openSMILE to extract the 88 eGeMAPS features using the annotations corresponding to the sections of the audio where each partner spoke similar to prior work on couples' emotion recognition  [7, 12, 22] . These features have been shown to be a minimalist set of features adequate for emotion recognition  [27] . Researchers use the extraction of acoustic parameters from the speech signal as a method to understand the patterning of the vocal expression of different emotions and other affective dispositions and processes. They used a number of acoustic parameters, including parameters: time-domain (e.g., speech rate), frequency domain (e.g., fundamental frequency or formant frequencies), amplitude domain (e.g., intensity or energy), distribution domain (e.g., relative energy in different frequency bands).\n\nThe use of machine learning led to the increase in the variety and quantity of acoustic features employed: basic (low-level ones) and derived (functionals). Therefore, finding relevant acoustic parameters is crucial in order to understand the mechanism of production and perception of emotions. Minimalistic standard parameters set for acoustic analysis of speech and other vocal sounds might lead to better generalization in real-world scenarios. There are three criteria that guided the choice of parameters: the potential of an acoustic parameter to index physiological changes in voice production during affective processes; the frequency and success with which the parameter has been used in the past literature; its theoretical significance The minimalistic acoustic parameter set contains 18 Low-level descriptors (LLD), which are grouped into frequency-related parameters, energy/amplitude-related parameters, and spectral (balance) parameters. They are smoothed over time with symmetric moving average filters 3 frames long (for pitch, jitter, and shimmering, only performed within voiced regions). The following functionals are applied:\n\n• arithmetic mean and coefficient of variation (standard deviation normalized by arithmetic mean) to all 18 LDD • 20th, 50th, and 80th percentiles, range of 20th to 80th percentile, and mean and std of the slope of rising/falling signal parts are added to loudness and pitch • arithmetic mean of Alpha Ratio, Hammarberg Index, spectral slopes from 0-500Hz and 500-1500Hz over all unvoiced segments • rate of loudness peaks, mean length & standard deviation of continuously voiced regions, mean length and std of unvoiced regions, number of continuous voiced regions per second The above functionals yield 62 parameters in the Geneva Minimalistic Standard Parameter Set. For extended Geneva Minimalistic Standard Parameter Set (eGeMAPS) the following are added so the final set contains 88 parameters: arithmetic means and coefficients of variation are applied to 7 additional LLD to all segments; arithmetic mean of the spectral flux in unvoiced regions, arithmetic mean and coefficient of variation of the spectral flux and MFCC 1-4 in voices regions + equivalent sound level. In evaluations, eGeMAPS was shown to be superior or equal to the GeMAPS  [27] . Hence, we extracted the eGeMAPS features resulting in an 88-dimensional feature vector.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Linguistic Features.",
      "text": "We extracted linguistic features from the transcripts of the whole 5-minute interaction using a pre-trained model -Sentence-BERT (SBERT)  [42]  Similar to prior work  [7, 12] . Sentence-BERT is a modification of the BERT architecture with siamese and triplet networks to compute sentence embeddings such that semantically similar sentences are close in vector space. Sentence-BERT has been shown to outperform the mean and CLS token outputs of regular BERT models for semantic similarity and sentiment classification tasks. Given that the transcripts are in German, we used the German BERT model [1] as SBERT's Transformer model and the mean pooling setting. The German BERT model was pre-trained using the German Wikipedia dump, the OpenLegalData dump, and German news articles. The extraction resulted in a 768-dimensional feature vector.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Unimodal And Multimodal Fusion",
      "text": "We used the features of each modality separately as input for our machine learning experiments: physiological (heart rate), movement (accelerometer and gyroscope), acoustic and linguistic. We also used a multimodal approach with feature-level fusion  [12, 22, 54] . We compared performance for individual modalities and various modality combinations as follows to answer our two research questions: physiological and movement, acoustic and linguistic, and physiological, movement, acoustic and linguistic.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiments And Evaluation",
      "text": "We trained models for each gender to perform binary classification for arousal and valence. We trained separate models for each gender since gender differences affect how people express their emotions  [17] . Hence, building gender-specific models  [12, 15]  may benefit the emotion recognition task. Similar to prior work  [11] , we performed couple disjoint cross-validation in which data from the same couples are never in both the train and test sets. This evaluation approach is a specific form of the subject independent evaluation but more robust as it accounts for the situation in which data from one partner (e.g., speech) may be contained in the data of the other partner  [11] . We did not perform leave-one-couple-out cross-validation which is the most used evaluation approach in couples' emotion recognition tasks  [11] . Given that most couples did not have negative samples, using this evaluation approach could lead to inflated results since the model could just predict all positive results without any learning. Rather, we performed 3-fold couple disjoint stratified cross-validation. In this setup, we trained on two folds, perform prediction on the third fold as a test set, and repeated this process with each fold serving as the test fold. The stratification aspect ensures that the same ratio for classes is maintained in the train and test splits, guaranteeing that each test fold will have some negative samples. The predicted labels of each test fold are combined and the evaluation metric is computed. We used the metric balanced accuracy / unweighted average recall (UAR) due to data imbalance and confusion matrices to perform an evaluation of the predictions. We also performed hyperparameter tuning within the train split using 2-fold stratified cross-validation. We used the following machine learning models: random forest (RF), support vector machines -linear, and radial basis function (RBF) with the 'weight' hyperparameter set to 'balanced' to account for the class imbalance. We used a random baseline of 50% for comparison.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Results And Discussion",
      "text": "The results of the best models are shown in Table  2  to answer the research questions. Among the unimodal models, for arousal, movement, and linguistic modalities performed the best for male partners (59.6%) and female partners (63.2%) respectively, and for valence, acoustic and linguistic performed the best for male partners (78.1%) and female partners (64.8%) respectively. Among the multimodal models, for arousal, \"Linguistic and Acoustic\" and \"Physiological and Movement\" performed the best for male partners (63.8%) and female partners (62.3%) respectively, and for valence, \"Linguistic and Acoustic\" performed the best for valence for both male partners (62.6%) and female partners (64.9%), with \"Physiological, Movement, Linguistic and Acoustic\" also performing the same for female partners. Figure  8  shows the confusion matrices for valence's and arousal's best models. The linguistic and acoustic modalities produced most of the best results alone or in combination, particularly for valence, which indicates that what partners say and how they speak during their conversations are the most informative for recognizing how negative or positive they feel. This result is in line with the use of these two modalities in several couples' emotion recognition works  [11] . Furthermore, the movement modality alone or in combination with physiological modality performed the best for arousal. This result is consistent with the intuition that the greater body and hand movement are expected the more active a person feels -the arousal dimension of emotion. We compare our results to the best results of our prior work that similarly performed global emotion recognition of positive vs negative valence from German-speaking, Swiss-based couples, albeit, with lab data. The best results (UAR) were 64.8% (female) and 56.1% (male) using fusion of acoustic and linguistic modality  [12] . Our best valence results of 64.9% (female) and 78.1% (male) outperform that work, albeit, only slightly for female partners. Also, as a reference, the partner-perceived emotion results reported in Boateng et al.  [15]  that indicate how well partner A could tell the emotions of their partner B were 73.2% (for male partners) and 74.3% (for female partners). We did not collect such perception data from partners in this work. Hence, a direct comparison is not possible. Nonetheless, it is worth noting that our results for male partners slightly outperform female partners' perceptions of their male partners' emotions from that work.\n\nThere are significant privacy concerns with a system that recognizes the emotion of romantic couples especially considering such interactions have a high tendency to entail sensitive information. We argue that a smartwatchbased emotion recognition system such as ours has a better potential to be privacy-preserving. Compared to a facial-based emotion recognition system which can infer people's emotions without their consent (e.g., via CCTV camera), the smartwatch system can be designed to work only with the consent of the partner. For example, the device would need to be worn to be able to collect the relevant physiological and movement data needed for emotion recognition. The speech processing component would need to work only for the speech of the partner and hence would require a speech sample from that partner to work. Furthermore, the system using partner A's smartwatch could be designed to require a confirmation from partner B to allow their watch to share data (e.g., BLE signal or physiological data) which is needed to be able to recognize partner B's emotion from partner A's smartwatch. Hence, partner A could be prevented from recognizing the emotion of partner B without their consent. Furthermore, all processing of signals could be implemented to run on the device further restricting potentially sensitive data such as audio from ever leaving the smartwatch.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Limitation And Future Work",
      "text": "The biggest limitation of this work is how highly skewed the data is especially for valence. The count for negative valence is 20 vs positive valence which is 360 with 5 out of the 13 couples having no negative valence labels.\n\nThere is a self-selection bias for this study which may have resulted in having couples that are less likely to have negatively rated interactions. Future studies could target couples in therapy who may have more negative conversation moments. Furthermore, collecting data for longer than 7 days may potentially capture conversation moments with negative emotion ratings.\n\nAdditionally, data was collected from only 13 couples (26 partners). Though small, for reference, two of the most popular public emotion datasets used in emotion recognition works -IEMOCAP  [19]  and MSP Improv  [20]  -contain data from 10 individuals (12 hours) and 12 individuals (9 hours) respectively, all collected from actors in the lab. Hence, our dataset has a greater variety with reference to subjects.\n\nFuture work would explore other fusion approaches such as decision-level fusion or some hybrid approach, multitask learning since the prediction entails two target variables -valence and arousal -, and pretraining on a related dataset and then fine-tuning on this dataset. Also, further work is needed to understand the conditions under which the model performs poorly e.g., indoors vs outdoors, when the partners are together alone or with other individuals. Such analysis could provide insight into potential changes that could improve the results (e.g., additional preprocessing of the dataset). It is also critical to better understand the conditions that could degrade performance before deploying for use in the real world.\n\nGiven that we had several audio samples without labels, we had three research assistants code all the audios with emotion labels so we could have more labeled data. Unfortunately, the inter-rater agreements were poor with an intraclass correlation coefficient of 0.21 -average for all the emotions coded. The poor agreement further demonstrates the difficulty even for humans in recognizing the emotions of romantic partners. Hence, we did not use those labels for our emotion recognition experiments. This agreement could potentially become better by improving the quality of the annotation instructions and having several rounds of annotation to ensure consistency in the annotation.\n\nOur emotion recognition system used manual speaker annotations and transcription data. Hence, there are several steps needed in the future for this system to be usable in the real world such as implementing an automatic speaker diarization (detecting when each person spoke) and a speech recognition system. In particular, current speech recognition systems do not work for this unique dataset given that the couples spoke Swiss German, which is (1) a spoken dialect and not written, and (2) varies across different parts of the German-speaking regions of Switzerland. Hence, further work is needed to develop automatic speech recognition systems for Swiss German. Also, the machine learning system needs to be implemented on the smartwatch and evaluated in real-time in the real world. The pipeline of preprocessing, feature extraction, and machine learning classification would have to be implemented using libraries and frameworks that run on smartwatch platforms such as Google's Wear OS and or Apple's Watch OS. Then, the system would need to be validated in a field study to evaluate the algorithm in a new, unseen context.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we trained machine learning models to predict the emotions of romantic partners using multimodal smartwatch data collected from daily life. We used the following sensor data: heart rate, accelerometer, gyroscope, and ambient light. We performed binary classification of valence and arousal using linear SVM, RBF SVM, and random forest. We used individual modalities and explored various combinations of modalities using feature-level fusion. Our results from the best models -balanced accuracies of 63.8% and 78.1% for arousal and valence respectively -are better than chance and our prior work that also used data from German-speaking, Swiss-based couples, albeit, in the lab. This work contributes toward building automated emotion recognition systems that would eventually enable couples to monitor their emotions in daily life and enable the delivery of interventions to improve their emotional well-being. This approach could also be useful in couple therapy.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Russell’s circumplex model of emotions[57]",
      "page": 3
    },
    {
      "caption": "Figure 2: ) consists",
      "page": 6
    },
    {
      "caption": "Figure 2: Overview of the DyMand system",
      "page": 7
    },
    {
      "caption": "Figure 3: Emotion rating with Affective Slider",
      "page": 8
    },
    {
      "caption": "Figure 3: ). If the smartwatch does not receive a message from the smartphone app within 2 mins indicating that",
      "page": 8
    },
    {
      "caption": "Figure 4: Screenshot of the annotation process of the audio",
      "page": 9
    },
    {
      "caption": "Figure 5: Screenshot of the coding context options",
      "page": 10
    },
    {
      "caption": "Figure 6: Distributions (bar chart) of data for arousal (left) and valence (right) per couple per gender",
      "page": 11
    },
    {
      "caption": "Figure 7: Total distribution (histogram/bar chart) of data for all couples per gender for arousal (left) and valence (right)",
      "page": 12
    },
    {
      "caption": "Figure 8: shows the confusion matrices for valence’s and arousal’s best models. The",
      "page": 14
    },
    {
      "caption": "Figure 8: Confusion matrix for the best model models for arousal (left) and valence (right). Linear SVC = Linear Support Vector",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "Negative\nPositive",
          "Arousal": "Low\nHigh"
        },
        {
          "Valence": "8\n191",
          "Arousal": "43\n156"
        },
        {
          "Valence": "12\n169",
          "Arousal": "54\n127"
        },
        {
          "Valence": "20\n360",
          "Arousal": "97\n283"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modalities": "Unimodal",
          "Arousal (%)": "Male",
          "Valence (%)": "Male"
        },
        {
          "Modalities": "Physiological",
          "Arousal (%)": "51.7",
          "Valence (%)": "62.9"
        },
        {
          "Modalities": "Movement",
          "Arousal (%)": "59.6",
          "Valence (%)": "58.2"
        },
        {
          "Modalities": "Linguistic",
          "Arousal (%)": "58.2",
          "Valence (%)": "59.9"
        },
        {
          "Modalities": "Acoustic",
          "Arousal (%)": "54.8",
          "Valence (%)": "78.1"
        },
        {
          "Modalities": "Multimodal",
          "Arousal (%)": "",
          "Valence (%)": ""
        },
        {
          "Modalities": "Physiological and Movement\n54\n62.3\n48\n62.3",
          "Arousal (%)": "",
          "Valence (%)": ""
        },
        {
          "Modalities": "Linguistic and Acoustic\n55.9\n63.8\n62.6\n64.9",
          "Arousal (%)": "",
          "Valence (%)": ""
        },
        {
          "Modalities": "Physiological, Movement, Linguistic and Acoustic\n59.7\n59.1\n59.6\n64.9",
          "Arousal (%)": "",
          "Valence (%)": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Open Sourcing German"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "Mojtaba Khomami",
        "Ramanathan Subramanian",
        "Seyed Mostafa Kia",
        "Paolo Avesani"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Predicting latent narrative mood using audio and physiologic data",
      "authors": [
        "Tuka Alhanai",
        "Mohammad Ghassemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Emotions are the Great Captains of our Lives\": Measuring Moods through the Power of Physiological and Environmental Sensing",
      "authors": [
        "April Keith",
        "Peter Arano",
        "Carlotta Gloor",
        "Carlo Orsenigo",
        "Vercellis"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Re-thinking dyadic coping in the context of chronic illness",
      "authors": [
        "Hoda Badr",
        "Linda Acitelli"
      ],
      "year": "2017",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "6",
      "title": "The affective slider: A digital self-assessment scale for the measurement of human emotions",
      "authors": [
        "Alberto Betella",
        "Paul",
        "Verschure"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "7",
      "title": "BERT Meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions",
      "authors": [
        "Jacopo Biggiogera",
        "George Boateng",
        "Peter Hilpert",
        "Matthew Vowels",
        "Guy Bodenmann",
        "Mona Neysari",
        "Fridtjof Nussbeck",
        "Tobias Kowatsch"
      ],
      "year": "2021",
      "venue": "BERT Meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions",
      "doi": "10.1145/3461615.3485423"
    },
    {
      "citation_id": "8",
      "title": "Automatic classification of married couples' behavior using audio features",
      "authors": [
        "Matthew Black",
        "Athanasios Katsamanis",
        "Chi-Chun Lee",
        "Adam Lammert",
        "Brian Baucom",
        "Andrew Christensen",
        "Panayiotis Georgiou",
        "Shrikanth S Narayanan"
      ],
      "year": "2010",
      "venue": "Automatic classification of married couples' behavior using audio features"
    },
    {
      "citation_id": "9",
      "title": "Classification of Blame in Married Couples' Interactions by Fusing Automatically Derived Speech and Language Information",
      "authors": [
        "Matthew P Black",
        "G Panayiotis",
        "Athanasios Georgiou",
        "Brian Katsamanis",
        "Shrikanth Baucom",
        "Narayanan"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "10",
      "title": "Towards Real-Time Multimodal Emotion Recognition among Couples",
      "authors": [
        "George Boateng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition among Couples: A Survey",
      "authors": [
        "George Boateng",
        "Elgar Fleisch",
        "Tobias Kowatsch"
      ],
      "year": "2022",
      "venue": "Emotion Recognition among Couples: A Survey",
      "arxiv": "arXiv:2202.08430"
    },
    {
      "citation_id": "12",
      "title": "You Made Me Feel This Way\": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions Using Speech Data",
      "authors": [
        "George Boateng",
        "Peter Hilpert",
        "Guy Bodenmann",
        "Mona Neysari",
        "Tobias Kowatsch"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction",
      "doi": "10.1145/3461615.3485424"
    },
    {
      "citation_id": "13",
      "title": "Development, Deployment, and Evaluation of DyMand-An Open-Source Smartwatch and Smartphone System for Capturing Couples' Dyadic Interactions in Chronic Disease Management in Daily Life",
      "authors": [
        "George Boateng",
        "Prabhakaran Santhanam",
        "Elgar Fleisch",
        "Janina Lüscher",
        "Theresa Pauly",
        "Urte Scholz",
        "Tobias Kowatsch"
      ],
      "year": "2022",
      "venue": "Development, Deployment, and Evaluation of DyMand-An Open-Source Smartwatch and Smartphone System for Capturing Couples' Dyadic Interactions in Chronic Disease Management in Daily Life",
      "arxiv": "arXiv:2205.07671"
    },
    {
      "citation_id": "14",
      "title": "VADLite: an open-source lightweight system for real-time voice activity detection on smartwatches",
      "authors": [
        "George Boateng",
        "Prabhakaran Santhanam",
        "Janina Lüscher",
        "Urte Scholz",
        "Tobias Kowatsch"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "15",
      "title": "Speech Emotion Recognition among Couples using the Peak-End Rule and Transfer Learning",
      "authors": [
        "George Boateng",
        "Laura Sels",
        "Peter Kuppens",
        "Peter Hilpert",
        "Urte Scholz",
        "Tobias Kowatsch"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction (ICMI '20 Companion)"
    },
    {
      "citation_id": "16",
      "title": "Dyadic coping-a systematic-transactional view of stress and coping among couples: Theory and empirical findings",
      "authors": [
        "Guy Bodenmann"
      ],
      "year": "1997",
      "venue": "European Review of Applied Psychology"
    },
    {
      "citation_id": "17",
      "title": "On understanding gender differences in the expression of emotion",
      "authors": [
        "Leslie R Brody"
      ],
      "year": "1993",
      "venue": "Human feelings: Explorations in affect development and meaning"
    },
    {
      "citation_id": "18",
      "title": "Making you happy makes me happy",
      "authors": [
        "Pascal Budner",
        "Joscha Eirich",
        "Peter Gloor"
      ],
      "year": "2017",
      "venue": "Measuring Individual Mood with Smartwatches",
      "arxiv": "arXiv:1711.06134"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions",
      "authors": [
        "Brian Sandeep Nallan Chakravarthula",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "2018",
      "venue": "Modeling Interpersonal Influence of Verbal Behavior in Couples Therapy Dyadic Interactions",
      "arxiv": "arXiv:1805.09436"
    },
    {
      "citation_id": "22",
      "title": "Predicting Behavior in Cancer-Afflicted Patient and Spouse Interactions Using Speech and Language",
      "authors": [
        "Haoqi Sandeep Nallan Chakravarthula",
        "Li",
        "Shao-Yen",
        "Maija Tseng",
        "Panayiotis Reblin",
        "Georgiou"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Automatic Prediction of Suicidal Risk in Military Couples Using Multimodal Interaction Cues from Couples Conversations",
      "authors": [
        "Md Sandeep Nallan Chakravarthula",
        "Nasir",
        "Shao-Yen",
        "Haoqi Tseng",
        "Li",
        "Jin Tae",
        "Brian Park",
        "Craig Baucom",
        "Shrikanth Bryan",
        "Panayiotis Narayanan",
        "Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "The specific affect coding system (SPAFF). Handbook of emotion elicitation and assessment",
      "authors": [
        "A James",
        "John Coan",
        "Gottman"
      ],
      "year": "2007",
      "venue": "The specific affect coding system (SPAFF). Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "25",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "K Sidney",
        "Jacqueline Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "26",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "Paul Ekman",
        "Dacher Keltner"
      ],
      "year": "1997",
      "venue": "Nonverbal communication: Where nature meets culture"
    },
    {
      "citation_id": "27",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "28",
      "title": "MobileCoach: A novel open source platform for the design of evidence-based, scalable and low-cost behavioral health interventions: overview and preliminary evaluation in the public health context",
      "authors": [
        "Andreas Filler",
        "Tobias Kowatsch",
        "Severin Haug",
        "Fabian Wahle",
        "Thorsten Staake",
        "Elgar Fleisch"
      ],
      "year": "2015",
      "venue": "Wireless Telecommunications Symposium (WTS)"
    },
    {
      "citation_id": "29",
      "title": "Observation of couple conflicts: Clinical assessment applications, stubborn truths, and shaky foundations",
      "authors": [
        "Richard E Heyman"
      ],
      "year": "2001",
      "venue": "Observation of couple conflicts: Clinical assessment applications, stubborn truths, and shaky foundations"
    },
    {
      "citation_id": "30",
      "title": "Multimodal Transformer Fusion for Continuous Emotion Recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Towards unravelling the relationship between on-body, environmental and emotion data using sensor information fusion approach",
      "authors": [
        "Eiman Kanjo",
        "M Eman",
        "Nasser Younis",
        "Sherkat"
      ],
      "year": "2018",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "Couple observational coding systems",
      "authors": [
        "K Patricia",
        "Donald Kerig",
        "Baucom"
      ],
      "year": "2004",
      "venue": "Couple observational coding systems"
    },
    {
      "citation_id": "33",
      "title": "The Me in We dyadic communication intervention is feasible and acceptable among advanced cancer patients and their family caregivers",
      "authors": [
        "Dana Ketcher",
        "Casidee Thompson",
        "Amy Otto",
        "Maija Reblin",
        "Kristin Cloyes",
        "Margaret Clayton",
        "Brian Baucom",
        "Lee Ellington"
      ],
      "year": "2021",
      "venue": "Palliative Medicine"
    },
    {
      "citation_id": "34",
      "title": "Design and evaluation of a mobile chat app for the open source behavioral health intervention platform MobileCoach",
      "authors": [
        "Tobias Kowatsch",
        "Dirk Volland",
        "Iris Shih",
        "Dominik Rüegger",
        "Florian Künzler",
        "Filipe Barata",
        "Andreas Filler",
        "Dirk Büchter",
        "Björn Brogle",
        "Katrin Heldt"
      ],
      "year": "2017",
      "venue": "International Conference on Design Science Research in Information System and Technology"
    },
    {
      "citation_id": "35",
      "title": "Social Support and Common Dyadic Coping in Couples' Dyadic Management of Type II Diabetes: Protocol for an Ambulatory Assessment Application",
      "authors": [
        "Janina Lüscher",
        "Tobias Kowatsch",
        "George Boateng",
        "Prabhakaran Santhanam",
        "Guy Bodenmann",
        "Urte Scholz"
      ],
      "year": "2019",
      "venue": "JMIR research protocols"
    },
    {
      "citation_id": "36",
      "title": "Naturalistic observation of health-relevant social processes: The Electronically Activated Recorder (EAR) methodology in psychosomatics",
      "authors": [
        "Megan Matthias R Mehl",
        "Fenne Robbins",
        "Deters"
      ],
      "year": "2012",
      "venue": "Psychosomatic Medicine"
    },
    {
      "citation_id": "37",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "Angeliki Metallinou",
        "Shrikanth Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "38",
      "title": "Wellbeat: A framework for tracking daily well-being using smartwatches",
      "authors": [
        "Sungkyu Park",
        "Marios Constantinides",
        "Maria Luca",
        "Daniele Aiello",
        "Paul Quercia",
        "Van Gent"
      ],
      "year": "2020",
      "venue": "IEEE Internet Computing"
    },
    {
      "citation_id": "39",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition using smart watch sensor data: Mixed-design study",
      "authors": [
        "Juan Quiroz",
        "Elena Geangu",
        "Min Hooi"
      ],
      "year": "2018",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "42",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "43",
      "title": "Couples coping with chronic illness",
      "authors": [
        "A Tracey",
        "Anita Revenson",
        "Delongis"
      ],
      "year": "2011",
      "venue": "Couples coping with chronic illness"
    },
    {
      "citation_id": "44",
      "title": "Interrelation between adult persons with diabetes and their family: a systematic review of the literature",
      "authors": [
        "Tuula-Maria Rintala",
        "Pia Jaatinen",
        "Eija Paavilainen",
        "Päivi Åstedt-Kurki"
      ],
      "year": "2013",
      "venue": "Journal of family nursing"
    },
    {
      "citation_id": "45",
      "title": "Cancer conversations in context: naturalistic observation of couples coping with breast cancer",
      "authors": [
        "Megan Robbins",
        "Ana López",
        "Karen Weihs",
        "Matthias Mehl"
      ],
      "year": "2014",
      "venue": "Journal of Family Psychology"
    },
    {
      "citation_id": "46",
      "title": "Emotion elicitation using dyadic interaction tasks. Handbook of emotion elicitation and assessment",
      "authors": [
        "Nicole Roberts",
        "Jeanne Tsai",
        "James Coan"
      ],
      "year": "2007",
      "venue": "Emotion elicitation using dyadic interaction tasks. Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "47",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "48",
      "title": "Wearable-Based Affect Recognition-A Review",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Dürichen",
        "Kristof Van Laerhoven"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "49",
      "title": "The coregulation of daily affect in marital relationships",
      "authors": [
        "Dominik Schoebi"
      ],
      "year": "2008",
      "venue": "Journal of Family Psychology"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "51",
      "title": "Spouse control and type 2 diabetes management: moderating effects of dyadic expectations for spouse involvement",
      "authors": [
        "Amber Seidel",
        "Melissa Franks",
        "Ann Stephens",
        "Karen Rook"
      ],
      "year": "2012",
      "venue": "Family relations"
    },
    {
      "citation_id": "52",
      "title": "Caregiver's burden and quality of life: Caring for physical and mental illness",
      "authors": [
        "Salvatore Settineri",
        "Amelia Rizzo",
        "Marco Liotta",
        "Carmela Mento"
      ],
      "year": "2014",
      "venue": "International Journal of Psychological Research"
    },
    {
      "citation_id": "53",
      "title": "Using multimodal wearable technology to detect conflict among couples",
      "authors": [
        "Adela Timmons",
        "Theodora Chaspari",
        "C Sohyun",
        "Laura Han",
        "Perrone",
        "Gayla Shrikanth S Narayanan",
        "Margolin"
      ],
      "year": "2017",
      "venue": "Computer"
    },
    {
      "citation_id": "54",
      "title": "Multimodal Fusion for Behavior Analysis",
      "authors": [
        "Shao-Yen",
        "Haoqi Tseng",
        "Brian Li",
        "Panayiotis Baucom",
        "Georgiou"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "55",
      "title": "Development and validation of brief measures of positive and negative affect: the PANAS scales",
      "authors": [
        "David Watson",
        "Anna Clark",
        "Auke Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "56",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "Martin Wöllmer",
        "Felix Weninger",
        "Tobias Knaup",
        "Björn Schuller",
        "Congkai Sun",
        "Kenji Sagae",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "57",
      "title": "Building Chinese affective resources in valence-arousal dimensions",
      "authors": [
        "Liang-Chih Yu",
        "Lung-Hao Lee",
        "Shuai Hao",
        "Jin Wang",
        "Yunchao He",
        "Jun Hu",
        "Robert Lai",
        "Xuejie Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter"
    },
    {
      "citation_id": "58",
      "title": "Driver emotion recognition for intelligent vehicles: a survey",
      "authors": [
        "Sebastian Zepf",
        "Javier Hernandez",
        "Alexander Schmitt",
        "Wolfgang Minker",
        "Rosalind Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    }
  ]
}