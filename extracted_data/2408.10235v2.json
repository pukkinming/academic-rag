{
  "paper_id": "2408.10235v2",
  "title": "Multi-Source Eeg Emotion Recognition Via Dynamic Contrastive Domain Adaptation",
  "published": "2024-08-04T03:51:35Z",
  "authors": [
    "Yun Xiao",
    "Yimeng Zhang",
    "Xiaopeng Peng",
    "Shuzheng Han",
    "Xia Zheng",
    "Dingyi Fang",
    "Xiaojiang Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) provides reliable indications of human cognition and mental states. Accurate emotion recognition from EEG remains challenging due to signal variations among individuals and across measurement sessions. We introduce a multi-source dynamic contrastive domain adaptation method (MS-DCDA) based on differential entropy (DE) features, in which coarse-grained inter-domain and fine-grained intra-class adaptations are modeled through a multi-branch contrastive neural network and contrastive sub-domain discrepancy learning. Leveraging domain knowledge from each individual source and a complementary source ensemble, our model uses dynamically weighted learning to achieve an optimal tradeoff between domain transferability and discriminability. The proposed MS-DCDA model was evaluated using the SEED and SEED-IV datasets, achieving respectively the highest mean accuracies of 90.84% and 78.49% in cross-subject experiments as well as 95.82% and 82.25% in cross-session experiments. Our model outperforms several alternative domain adaptation methods in recognition accuracy, inter-class margin, and intra-class compactness. Our study also suggests greater emotional sensitivity in the frontal and parietal brain lobes, providing insights for mental health interventions, personalized medicine, and preventive strategies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are fundamental to human experience and play a significant role in well-being, behavior, social interaction, decision-making, and cognitive function  [1] [2] [3] [4] . Recent advances in electrode technology  [5] [6] [7]  and machine learning have led to improved electroencephalogram (EEG) analysis for emotional recognition  [8] [9] [10] . However, the inherent non-stationarity of EEG signals presents challenges in generalizing an emotion recognition method for accurate predictions between individuals or over time  [11] [12] [13] .\n\nDomain adaptation transfers knowledge from the source domain to the target domain, thereby minimizing the need for extensive data labeling  [14] [15] [16] . Unlike single-source domain adaptation (SS-DA), which treats EEG data from different subjects as a single source (see Fig.  1(a) ), multi-source domain adaptation (MS-DA) considers each subject as an individual source domain (see Fig.  1(b) ). The assumption of diverse data in the MS-DA may reduce domain bias and model overfitting  [17, 18] . However, domain shifts occur not only between each source and target, but also among different sources, potentially complicating the learning. Two common MS-DA include discrepancy-based and adversarial-based methods  [19] . The adversarial discriminative methods align target and source features through domain discriminator and adversarial objectives, whereas the discrepancy-based methods align features in latent space explicitly through discrepancy measures. Existing MS-DA approaches tend to assume shared features across domains, and their domain discriminability is limited to such coarse-grained global alignment. In addition, the typical static loss-weighting tends to produce suboptimal balance between the domain alignment and domain discriminability.\n\nTo address these challenges, we present a multi-source dynamic contrastive domain adaptation (MS-DCDA) method for EEG-based emotion recognition. Our model learns domain knowledge from both the source and target, where we consider not only the data contribution of each individual source but also a complementary source ensemble (see Fig.  1(c) ). Additionally, our model also learns domain-variant features using fine-grained class-aware contrastive sub-domain adaptation. What is more, an optimal domain transferability and domain discrimination are achieved through a dynamically weighted loss function. Extensive evaluation of the proposed MS-DCDA is conducted on the SEED and SEED-IV dataset, and state-of-the-art performances are achieved for the cross-subject and the cross-session recognition respectively. In addition, the generalization performance of our model is evaluated through a dataset transfer study where key brain lobes involved in EEG emotion recognition are also examined. The specific contributions of our work are summarized as follows:\n\nâ€¢ We introduce a multi-source dynamic contrastive domain adaptation method, which models coarse-grained inter-domain and fine-grained intra-class adaptations through a multi-branch contrastive network and class-aware contrastive sub-domain discrepancy learning. â€¢ Our model leverages domain knowledge from each individual source and a complementary source ensemble uses dynamically weighted learning to achieve optimal domain transferability and discriminability, significant improvements in inter-class margin and intra-class compactness are demonstrated.\n\nâ€¢ State-of-the-art performance is achieved on SEED and SEED-IV datasets, and our model outperforms the second best approach respectively by 1.5% and 2.4% in the cross-subject experiments as well as 2.2% and 3.6% in the cross-session experiment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "EEG provides a non-invasive measurement of the brain's electrophysiological activities. EEG signals mainly come from cortical pyramidal neurons perpendicular to the surface of the brain in the cerebral cortex. These signals offer numerous valuable clinical indications, including those related to higher cognitive functions such as emotions  [20] .\n\nTwo types of commonly used features for emotion classification include the time-domain features (e.g., standard deviation, mean, variance, and differential entropy) and the frequency-domain features (e.g., spectral power and power spectral density). In both feature domains, the use of differential entropy has demonstrated higher accuracy and greater stability in emotion classification than the power spectral density  [21] . Traditional machine learning methods such as support vector machine  [22] [23] [24] [25] , K-nearest neighbor  [26] , and random forest  [27]  make use of these manually crafted features for recognition. In recent years, machine learning and deep learning has achieved remarkable advancement in many areas  [28] [29] [30] [31] [32] , including affection computing and emotion recognition  [10] . Deep learning methods learn features from data without manual feature extractions and selections. For example, convolutional neural networks  [33]  have proven effective for lesion classification. CNNs have also been coupled with local information for emotion classification. By exploring frequency bands and channels via deep belief networks  [34] , the accuracy has been improved on the SEED dataset. Additionally, using attention-based convolutional recurrent neural networks  [35]  for discriminative features extraction further boosting the performance of EEG based emotion recognition. Pre-trained Transformers are also optimized through random feature projection and head augmentation  [36] , demonstrating improved efficiency, accuracy, and generalization in classification tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Single-Source Domain Adaptation",
      "text": "Domain adaptation is a type of transductive transfer learning  [16] , which adapts a model learned on source domain to classify data in target domain. Depending on the availability of class labels of samples in target domain during the training, deep visual domain adaptation are typically categorized into: supervised, semi-supervised, and unsupervised approaches. The case where the label is unavailable in target domain is considered as unsupervised domain adaptation (UDA)  [37]  [38]  [39] . Unsupervised domain adaptation has proven effective in addressing individual differences in EEG signals for emotion recognition, even in cases where the distribution of source and target domains are entirely different. To transfer cross-subject knowledge, maximum mean discrepancy (MMD)  [40]  has been widely adopted as a metric to describe the discrepancies between the distributions of source and target domains in UDA. For example, MMD preserves the domain invariance in deep adaptation network  [41]  while addressing the individual differences. However, the naive MMD is not capable of addressing the class prior distributions, resulting in degraded domain adaptation accuracies. Class-aware subdomain adaptation network have been introduced  [42, 43]  to address the class weight bias. In addition, contrastive domain discrepancy metric  [44]  is introduced for cross-subject and cross-session emotion recognition. The approximated joint marginal and conditional distributions  [45]  also shows improved clustering and recognition accuracy on the SEED dataset. A deep subdomain adaptation network  [46]  captures fine-grained information from each class, improving the alignment of subdomains. During training, the îˆ¸ ğ‘€ğ‘€ğ· and îˆ¸ ğ‘†ğ¶ğ· losses encourage class-independent and class-aware alignment respectively. The classification loss is comprised of the cross entropy loss îˆ¸ ğ¶ğ¸ and a complementary îˆ¸ ğ·ğ¼ğ‘†ğ¶ loss, which encourages the predictions consistency across classifiers. Additionally, a dynamic coefficient ğœ optimizes the domain transferability and discriminability. During the test, the predicted target class probabilities are averaged across the classifiers, and the target emotion Ãªğ‘¡ is determined by the maximum mean class probability. We train our model in an unsupervised setting, where the source domain is labeled whereas the labels are unavailable for samples in the target domain.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Source Domain Adaptation",
      "text": "Single-source DA methods are limited to single-source domain distributions and relatively simple datasets. Multisource domain adaptation algorithms are introduced to address more complicated data distributions. Unified convergent learning bound has been investigated for multi-source data  [47] . The deep multi-source adaptation transfer network  [48]  combines an adaptation network  [41]  with a discriminator, allowing nonuniform distribution in cross-subject emotion recognition. The multi-source marginal distribution adaptation method  [21]  maps domains to a shared feature space using multi-layer perceptrons to extract unique domain-invariant features from each individual domain. The multi-kernel and multi-source MMD method  [49]  extends the single-source MMD to measure the differences between domains. Joint distribution was also investigated in multi-source domain adaptation  [50]  for fine-grained domain adaptation using reinforcement learning. Dynamic transfer learning  [51]  is also explored to reduce multi-source to single-source adaptation thorough dynamic routing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "We introduce a multi-source dynamic contrastive domain adaptation (MS-DCDA) method for cross-subject and cross-session EEG-based emotion recognition. Our framework makes use of multi-branch contrastive neural net for class-aware domain alignment, which allows discriminative adjustment of marginal and conditional distributions of domains. The multi-source MMD is combined with a class-aware subdomain contrastive discrepancy (SCD) metric to tradeoff the coarse-and fine-grained domain alignments between source and target domains. Additionally, domain transferability and discriminability are optimally balanced through dynamically weighted domain alignment and classification penalties. The pipeline of EEG-based emotion recognition and the schematic of our MS-DCDA model are illustrated in Fig.  2 . We learn transferable EEG features in an unsupervised domain adaptation (UDA) setting, where the samples are labeled in the source domain but remain unlabeled in the target domain during the training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing",
      "text": "The pipeline of the emotion recognition starts with EEG data preprocessing, where the measured EEG signals are segmented and prefiltered. Differential entropy (DE)  [52]  features, which are effective in distinguishing low-and high-frequencies in EEG signals, are extracted from the EEG segments of each participant for five frequency bands (see Section 4.1). For an EEG segment having a Gaussian distributed frequency spectrum îˆº (ğœ‡, ğœ 2 ), its DE is given by:\n\n(1)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ms-Dcda Architecture",
      "text": "The proposed MS-DCDA model consists of three modules: the common feature extractor, the multi-branch contrastive module, and the multi-branch domain classifier. The details of these modules are introduced below.\n\nwhere ğ‘Š 2 is the total number of samples in the combined domain, which is the total number of samples in the first n-1 source domains. The common target feature is given by ğ¹ ğ‘¡ = {ğ‘“ ğ‘¡ |ğ‘“ ğ‘¡ = ğ¶ğ¹ ğ¸(ğ‘¥ ğ‘¡ )}. The common source features are extracted not only from each individual source but also from the source ensemble\n\nHere we employ multi-layer perceptron (MLP) for the CFE module, where the structure of each of the three layers is given respectively by Linear310/256/128-BatchNorm1D-LeakyReLU(0.01).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Branch Contrastive",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ms-Dcda Learning",
      "text": "The learning of the proposed MS-DCDA model involves four aspects: 1) coarse-grained domain aligment; 2) fine-grained sub-domain domain alignment with class awareness; and 3) fine-grained classification. 4) Dynamically weighted loss function for optimally balanced domain transferability and discrimination performance.\n\nDomain Alignment. The coarse-grained alignment of the source and target involves using the multi-source MMD to measure the difference between the MBC features of the pair-wise source and the target {ğ‘“ ğ‘– ğ‘ ,ğ· , ğ‘“ ğ‘– ğ‘¡,ğ· }. Mathematically, the MMD loss measures the difference between two distributions using their mean embeddings in the reproducing kernel Hilbert space. In practice, the squared value of MMD is typically estimated by kernel embedding:\n\nwhere ğ‘ ğ‘  and ğ‘ ğ‘¡ denote the number of mini-batch features sampled respectively from features of individual source and the target, ğ‘ represent the total number of sources-target feature pairs and ğ‘˜ is the Gaussian kernel function:\n\nContrastive Sub-Domain Alignment. To improve the inter-class margin and improve the intra-class compactness of the alignment, we extend the contrastive domain discrepancy  [42]  to a sub-domain contrastive discrepancy (SCD) for the alignment of our multi-branch contrastive features. The SCD measures feature differences of pair-wise MBC features {ğ‘“ ğ‘– ğ‘ ,ğ· , ğ‘“ ğ‘– ğ‘¡,ğ· } in class level, which not only aligns inter-domain features in general but also aligns the intra-domain features with class-level accuracies:\n\nwhere M represents the number of emotional categories. Minimizing this loss encourages minimizing the intra-class discrepancies while maximizing the inter-class differences.\n\nFor two classes c1 and c2 in a particular domain, the distance between the two classes is written as:\n\nHere MMD is employed to measure differences within the same subdomain. The MMD is not limited by data distribution types such as edge distribution or conditional distribution, making it favorable for class-level optimization:\n\nwhere ğ‘˜(â‹…, â‹…) is the Gaussian kernel function (see Eq.3). The class masking function is given by:\n\nIn cases where c1 = c2, the above equation measures the discrepancies within the same emotion class; for cases where c1 â‰  c2, it measures the discrepancies between the two classes.\n\nPenalizing the SCD loss minimizes the intra-class discrepancies while maximizing the inter-class discrepancies. Due to the lack of target label, here we use pseudo labels Å·ğ‘– ğ‘¡ as alternatives to estimate the SCD. Although the approximated pseudo labels might not be perfect, the impact of label noise on the SCD loss is typically minimal compared to a large dataset  [42] . The model robustness to pseudo label errors may attribute to the the mean embedding of the distribution within a reproducing kernel Hilbert space of MMD.\n\nClassification and Feature Discrimination. Given the pair-wise multi-branch features {ğ‘“ ğ‘– ğ‘ ,ğ· , ğ‘“ ğ‘– ğ‘¡,ğ· }, we train the classifier using cross entropy loss:\n\nwhere ğ‘ƒ (â€¢|â€¢) represents the probability distribution of the predicted label. Using the cross entropy loss alone typically leads to imbalanced prediction across different classifiers. To encourage consistent prediction of the target and accelerate the convergence rate of our multi-branch domain classifiers, a complementary classification loss ğ¿ ğ·ğ¼ğ‘†ğ¶ is introduced:\n\nwhere ğ¸ ğ‘¥âˆ¼ğ‘¥ ğ‘¡ represents randomly selection of samples from the target features. To achieve an optimal tradeoff between the domain transferability and discriminability, we adopt a dynamic weighted learning method  [25] , where a dynamic coefficient is used to adjust the losses of domain feature alignment and domain feature discrimination. In the dynamic learning, the multi-source MMD (see Eq.2) is employed as an indicator of the cross-domain alignment. Using the linear discriminant analysis, domain discriminability is measured from features extracted by the CFE module:\n\nwhere ğ‘† ğ‘ and ğ‘† ğ‘Š are respectively the inter and intra class variance of the CFE features, and ğ‘‡ ğ‘Ÿ(â‹…) represents trace of the two variance matrices  [53] . A larger ğ½ (ğ‘Š ) value indicates a better discriminability, while a smaller ğ¿ ğ‘€ğ‘€ğ· value indicates better a domain transferability. The dynamic coefficient ğœ which balances the domain discriminability and transferability is given by:\n\nwhere the values of ğ¿ ğ‘€ğ‘€ğ· and ğ½ (ğ‘Š ) are normalized to the range of [0,1] with their respective maximum and minimum.\n\nThe dynamic coefficient ğœ varies between 0 and 1. A large ğœ indicates that the model favors domain alignment, whereas a smaller ğœ reflects a preference for class discriminability.\n\nThe training objective for MS-DCDA consists of the domain alignment learning, class-level domain alignment learning, class discrimination learning, and classifier learning, where the total loss is dynamically weighted as follows:\n\nIn addition to the dynamic coefficient ğœ, another two dynamically changing coefficients include ğ›¼=(  2 1+ğ‘’ -10Ã—ğ‘–ğ‘¡ğ‘’ğ‘Ÿâˆ•ğ‘’ğ‘ğ‘œğ‘â„ )-1 and ğ›½= ğ›¼  10 , where ğ‘–ğ‘¡ğ‘’ğ‘Ÿ is the index of iteration of training. The values of ğ›¼ and ğ›½ increase from 0 to 1 at different speeds as the number of iterations increases. The MS-DCDA algorithm is summarized in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "This section introduces the datasets, experimental setup, and training details of the proposed MS-DCDA model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate baseline methods and our MS-DCDA model on the Shanghai Jiao Tong University (SJTU) Emotion EEG Dataset (SEED)  [34] [54] and the SJTU Emotion EEG Dataset IV (SEED-IV)  [55] . Fifteen healthy participants (subjects), including seven males and eight females with an average age of 23 years, participated in data collection. The SEED dataset is collected by having participants watch 15 movie clips, which incite positive, neutral, and negative emotions. While subjects were watching movie clips, EEG data were obtained through a 62-channel ESI NeuroScan system. The collected data was downsampled to 200 Hz. The SEED signals are filtered with bandpass filter of 0-75Hz and segmented into non-overlapping intervals of 1 second each. The SEED-IV dataset is collected by having the same participants watch 24 movie clips, which incite neutral, sad, fear, and happy emotions. The SEED-IV signals are prefiltered by a 1-75 Hz bandpass filter and segmented into non-overlapping intervals of 4 seconds each. For both the SEED and SEED-IV dataset, signals are collected in three sessions for each subject and each segment.\n\nData Preprocessing. Each of the filtered EEG segments is considered as a data sample in training. DE features are extracted from each segment for five frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-31 Hz), and gamma (31-50 Hz), resulting in a feature dimension of 62 channels Ã— 5 frequency bands. A total of 3394 EEG samples are collected for each participant per session for the SEED set. The SEED-IV set consists of 851, 832, 822 samples per participant for each of the three sessions respectively. Labels are generated for both datasets, and pre-processed EEG data of each participant are normalized electrode-wise  [56]  based on individual mean and standard deviation values.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "To evaluate the performance of the proposed MS-DCDA network, we performed three round leave-one-out (LOO) cross validations for cross-subject and cross-session experiments respectively. In the Cross-Subject experiment, a total number of 15 subjects is evaluated, where EEG of one subject is selected as the target and EEG of the remaining 14 subjects are considered as sources. EEG signals of subjects measured in one session are selected as target, and EEG signals of subjects measured from the rest two sessions are considered as source.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training",
      "text": "The proposed MS-DCDA model is trained with Adam optimizer for 50 epochs. We use a batch size of 32 and 16 respectively for the SEED and SEED-IV dataset. The learning rate remains at 5 Ã— 10 -3 for both. The dynamic coefficients are given by ğ›¼=(  2 1+ğ‘’ -10Ã—ğ‘–ğ‘¡ğ‘’ğ‘Ÿâˆ•ğ‘’ğ‘ğ‘œğ‘â„ )-1 and ğ›½= ğ›¼ 10 respectively, where ğ‘–ğ‘¡ğ‘’ğ‘Ÿ is the index of training iterations. All the training tasks are conducted on a NVIDIA GeForce RTX 3060 GPU. The implementation is based on Python 3.9.18, PyTorch 2.0.0, and Torchvision 0.15.0.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Three rounds of LOO cross-validations (see Section 4.2) are conducted for cross-subject and cross-session experiments respectively. The same validations are repeated for the SEED and SEED-IV datasets respectively. For each dataset and experiment, we evaluated the mean recognition accuracy (ACC-mean) and the best recognition accuracy (ACC-best) across the three LOO validation rounds for the target domain. The best recognition accuracies are given in the form of mean Â± standard deviation for the three rounds of LOO.\n\nWe compare the proposed MS-DCDA with several advanced methods for both cross-subject and cross-session experiments. They include four single-source domain adaptation methods: deep domain confusion (DDC)  [57] , DAN  [41] , DANN  [42] , deep correlation alignment (DCORAL)  [58] ; Two multi-source domain adaptation models: multisource marginal distribution adaptation (MS-MDA)  [56] , and multi-source associate domain adaptation (MS-ADA)  [50] . The DDC method minimizes the domain distribution discrepancies between the source and the target using MMD. The DCORAL model aligns the second-order statistics of domain distributions using linear transformations. The DANN aligns domain feature distributions through backpropagation. The DAN method aligns high-level domain features through the use of deep neural networks and multi-kernel MMD. In the MS-MDA method, multi-branch networks and MMD are combined for the multi-source alignment. The MS-ADA aligns the edge distributions and through MMD and reinforcement learning respectively. In addition, we compare our MS-DCDA against several more models for the cross-subject experiment. They include BiDANN  [59] , RGNN  [60] , TANN  [61] , PPDA  [62] , GMSS  [63] , HVF 2 N-DBR  [64] , MWACN  [15] , PCDG  [65] , and MFA-LR  [39] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Cross-Subject Results",
      "text": "The comparisons of the emotion recognition results of our MS-DCDA model and the baseline algorithms are presented in Tables 1, where the best results are highlighted in bold font. The alternative methods are tuned to their best performances. If the relevant parameters are published, we will use them. If not, the parameters will remain consistent with our method. The cross-subject experimental results on SEED and SEED-IV, which are based on both the   [60]  85.30Â±6.72 73.84Â±8.02 TANN  [61]  84.41Â±8.75 68.00Â±8.35 PPDA  [62]  86.70Â±7.10 -GMSS  [63]  86.52Â±6.22 73.48Â±7.41 HVF 2 N-DBR  [64]  89.33Â±10.13 73.60Â±2.91 MWACN  [15]  89.30Â±9.18 74.60Â±10.77 PCDG  [65]  87.30Â±2.10 73.60Â±5.10 MFA-LR  [39]  89.11Â±7.72 74.99Â±12.1 MS-DCDA (ours) 90.84Â±8.31 78.49Â±7.  36  state-of-the-art methods and our proposed method, are presented in Table  2 . On the SEED dataset, the constructed MS-DCDA model significantly improves recognition performance, achieving 90.84% for the third session and 88.75% for the average of three sessions. In comparison to other methods, such as MS-MDA and MS-ADA, both being multiple source domain adaptation methods, our method demonstrates superior performance. Compared to MS-MDA which achieves the highest average accuracy, our method has improved accuracy by 3.01%. Compared to MS-ADA which achieves the highest average accuracy, our method has improved accuracy by 3.77%. On the SEED-IV dataset, our method achieved the highest accuracy 78.49% for the second session, which is 2.38% higher than MS-MDA, 74.34% for the average of three sessions, which is 3.12% higher than MS-MDA. Additionally, both F1 and Kappa scores of our model outperform the alternative methods.\n\nTo further validate the superiority of our method, we compared it with another set of state-of-the-art methods. As illustrated in Table  3 , our method outperforms other methods in mean accuracy for both the SEED and SEED-IV   [58]  85.57 87.30Â±6.00 0.87 0.81 DANN  [42]  88.14 89.43Â±6.74 0.88 0.81 DAN  [41]  90.60 92.07Â±6.66 0.94 0.90 MS-MDA  [56]  91.80 93.61Â±4.95 0.94 0.91 MS-ADA  [50]  92.02 93.28Â±6.72 0.93 0.89 MS-DCDA(ours) 93.92 95.82Â±4.81 0.95 0.93",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Seed-Iv",
      "text": "DDC  [57]  75.30 78.02Â±12.11 0.77 0.70 DCORAL  [58]  72.86 76.01Â±11.44 0.74 0.67 DANN  [42]  72.97 75.32Â±11.23 0.74 0.67 DAN  [41]  72.79 76.09Â±14.12 0.74 0.68 MS-MDA  [56]  75.78 78.60Â±9.61 0.73 0.66 MS-ADA  [50]  72.22 76.17Â±13.91 0.77 0.69 MS-DCDA (ours) 77.53 82.25Â±11.00 0.83 0.77 datasets. The multi-source domain adaptation algorithm MFA-LR, HVF 2 N-DBR, and MWACN also work well in the cross-subject experiments. Our algorithm dynamically balance the model domain transferability and discriminability and achieved higher accuracy than MFA-LR (uses class-level alignment loss) and MWACN (uses association reinforcement to adapt conditional distribution). The recognition accuracy is respectively increased by 1.51% and 3.50% in cross-subject experiments compared to the alternative methods (HVF 2 N-DBR for SEED dataset and MFA-LR for SEED-IV dataset).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross-Session Results",
      "text": "The specific subject results of the best session in cross-session experiments are shown in Tables 4. Our algorithm, which integrates dynamic multi-source domain adaptation and class alignment techniques, achieved the highest accuracy across most subjects. On the SEED dataset, some subjects achieved the highest accuracy of 100% in cross-session experiments. On the SEED-IV dataset, the subject achieved the highest accuracy of 95.55% and 94.98% in cross-session experiments. Notably, the results on the SEED dataset are significantly better than the results on the SEED-IV dataset. On the one hand, this may be because the SEED-IV dataset has more sentiment classification than the SEED dataset, which increases the difficulty of class feature recognition. On the other hand, it may be because the sample size of SEED-IV is much smaller than that of SEED. The cross-session experimental results of SEED and SEED-IV are provide in Table  5 . Our MS-DCDA model outperformed the rest models for all four metrics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "Ablation on Loss Terms In this section, we conduct ablation experiments to investigate the different components and equilibrium factors of the model on performance. For the sake of conciseness and consistency, all ablation experiments were conducted on the SEED. The results of the ablation experiment for the model components in cross-subject and cross-session experiments are shown in Table  6 . Compared to the accuracy of 88.75% for full model components in cross-subject experiments, the removal of the MMD loss leads to a decrease in accuracy by 0.74%, omitting the disc loss decreases accuracy by 2.1%, and excluding the SCD loss results in a decrease of 4.49%. It can be seen that SCD has the greatest performance improvement, while MMD has the smallest performance improvement. Even without using SCD, the accuracy remains higher than the baseline model by 0.13%. Compared to the accuracy of 93.92% for full model components in cross-session experiments, excluding the MMD loss results in a decreased accuracy of 0.51%, removing the disc loss reduces accuracy by 1.76%, and omitting the SCD loss leads to a decrease of 2.97%. Similar to the cross-subject experiments, SCD shows the most substantial performance improvement, followed by disc, and MMD. The accuracy of not using SCD is still higher than that of the baseline model by 0.27%. These results emphasize the indispensability of each loss component.  7  presents the comparison of using dynamic weighting factor ğœ in Eq. 14 against several combinations of static weights. It can be seen that using dynamic weighting factor shows better performance. Using dynamic weighting factor ğœ promoting dynamic balance between domain alignment and class discrimination effectively avoids performance degradation caused by excessive pursuit of alignment or discriminability.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablation On Learning Strategies Table",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation On Layer Normalization",
      "text": "The proposed MS-DCDA preserves the coarse-grained inter-domain alignment while improving the intra-domain and intra-class alignment through the fine-grained class-level adaptation. With the use of the SCD penalty, the increases in recognition accuracy of 4.49% and 2.97% are observed in the cross-subject and cross-session experiments respectively. Fine-granularity alignment encourages wider inter-class margin. As shown in Table  8 , the performance of domain alignment also depends on feature normalization, where the removal of the batch normalization layer leads to decreased performance. To improve the quality of pseudo class labels the fine-grained alignment relies on, unreliable labels are pre-filtered by an empirical threshold. The fine-grained adaptation is dynamically weighted using a dynamic coefficient, which is proportionate to the number of training iterations. By improving the",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation On Brain Lobes",
      "text": "To examine impact of different brain lobes on emotion recognition, the 62 EEG electrodes are divided into four regions based on their location: front, parietal, temporal, and occidental. As shown in Table  9 , the frontal and parietal lobes appear to be the most active brain lobes while watching emotional movie clips, followed by the occipital and temporal lobes. Although only six electrodes are allocated for the temporal lobe, its high activity suggests that the expression of emotions is likely independent of the number of recognized electrode signals. The combination of frontal and parietal lobes consistently outperformed individual lobes for emotion recognition on both the SEED and SEED-IV datasets, reaffirming the important role of frontal and parietal lobes in emotional expression. In addition, as the temporal or occipital signals continue to increase and exceed a certain threshold, the recognition accuracy stops improving, and some may even appear to decrease. One possible explanation is that EEG signals measured from the occipital and temporal lobes exhibit relatively lower sensitivity to emotional states compared to those from the frontal and parietal lobes. Including the less sensitive occipital and temporal lobes may reduce the emotion recognition results. Despite of the adverse effect of the occipital and temporal lobe, the recognition accuracies remain higher in combined brain lobes compared to a single brain lobe. Our studies indicate that brain regions exhibit different sensitivities to emotional states, reaffirming the need to consider the combination of different lobes during emotion recognition.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Visual Analysis",
      "text": "Confusion Matrix Analysis. The confusion matrices of cross-subject and cross-session experiments for three sessions on SEED and SEED-IV are provide in Fig.  3 . As can be seen in Fig.  3 (a) and 3(b), our MS-DCDA method exhibits strongest sensitive to the positive emotion on the SEED data, with slightly reduced sensitivity in neutral and negative emtions. It is observed in Fig.  3(c ) and 3(d) that neutral emotion is less likely being confused than fear and sadness on SEED-IV by our model, which may be explain by the fact that both fear and sadness are particularly negative and exhibit similarities in the activated electrode signals.\n\nT-SNE Analysis. The T-SNE plots  [66]  of feature distributions are provide in Fig.  4  for SEED dataset, which is richer in source diversity. We randomly picked 100 EEG samples from each subject for visualization to display changes in feature distribution caused by model training. As shown in Fig.  4 (a), the distribution of most of the samples of original features are observed to be concentrated in a singular area, with a few outliers among individual subjects. This distribution affirms that the distribution EEG features may overlap in low-level feature space. This observation becomes more clear with normalization, where a more uniform and centralized distribution appears",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "The presented MS-DCDA framework demonstrated improved emotion recognition performance on both SEED and SEED-IV datasets. In cross-subject experiments, we achieved an accuracy of 90.48% and 90.85% for S1 and S3 on the SEED dataset, with a relatively lower accuracy of 84.98% in S2. On the SEED-IV dataset, our MS-DCDA performs better in S2 with a leading 78.49% accuracy, while the accuracies are slightly reduced to 72.36% and 72.16% for S1 and S3 due to more complicated data dependencies in that session. The cross-session variations can result in variations in recognition accuracies if data are obtained through repetitive experiments. The robustness of the proposed framework against such variations was demonstrated through extensive cross-validation. The observation that cross-subject results appear to be less robust compared to the corresponding cross-session results may be explained by the individual subject disparities in EEG being more complicated than its non-stationary problems. As demonstrated in Tables 2, 3 and 5, multisource domain adaptation generally achieves superior performance than single-source methods for EEG-based emotion recognition tasks. The ablation studies on various   6 , where the accuracy of our model improved from 84.13% and 90.68% by incorporating multi-source loss penalties. The use of pair-wise neural network branches allows adaptation of target to different source distributions through distinctive feature representations and improves the accuracy and generalization of our model. However, computational complexity may increase linearly with the number of sources. Dynamic routing among branches using a mixture-of-expert architecture may improve the recognition accuracy at a reduced training cost.\n\nWhile our MS-DCDA model demonstrated improved emotion recognition performance with the use of frequency information of EEG signals, the accuracies of the emotion recognition are found to be significantly reduced without the use of DE features regardless of the recognition models. This observation reaffirms the advantage of DE features as an effective representation of EEG signals for emotion recognition. Future work may benefit from more advanced and learnable feature representations based on neural networks. End-to-end learning of EEG representation with subsequent emotion recognition models may further improve accuracy. In addition, the spatial features of the EEG data have not yet been fully explored due to the lack of spatial information of the electrodes in the current dataset. Collecting the spatial information of electrodes without significantly increasing the experiment cost may require further investigation. Multi-modality learning that combines frequency and spatial EEG features also warrants a future study. Future work may involve extending the examination of EEG features in the time domain. The accuracies, generalization, and robustness of our model may also benefit from exploring the attention and gating mechanisms.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In summary, a multi-source dynamic contrastive domain adaptation (MS-DCDA) method was introduced for EEG based emotion recognition. The proposed combines coarse-and fine-grained domain alignments and dynamically optimize the domain transferability and discriminability, leading to improved model generalization, classification accuracies, wider inter-class margin, and higher intra-class compactness. The proposed MS-DCDA outperformed both the classical and the state-of-the-art methods on both the SEED and SEED-IV datasets by a large margin. In addition, Our study also suggests greater emotional sensitivity in the frontal and parietal brain lobes, providing insights for mental health care. The broader significance of this research also lies in its potential applications in human-computer and brain-computer interfaces, allowing systems to respond more precisely to the emotional states of users through enhanced emotion recognition capabilities.   [58] , DAN  [41] , DANN  [42] , DDC  [57] . The performance of our model also exceeds two multi-source methods: MS-MDA  [56] , and MS-ADA  [50] .",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a)), multi-source",
      "page": 1
    },
    {
      "caption": "Figure 1: (b)). The assumption",
      "page": 1
    },
    {
      "caption": "Figure 1: (c)). Additionally, our model also learns",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison of traditional discrepancy-based domain adap-",
      "page": 2
    },
    {
      "caption": "Figure 2: The pipeline of EEG-based emotion recognition and the schematics of the proposed MS-DCDA model. The multi-source EEG",
      "page": 3
    },
    {
      "caption": "Figure 2: We learn transferable EEG features in an",
      "page": 3
    },
    {
      "caption": "Figure 3: Confusion matrices analysis of our MS-DCDA model for (a) Cross-subject experiment on SEED dataset; (b) Cross-subject exper-",
      "page": 10
    },
    {
      "caption": "Figure 3: As can be",
      "page": 10
    },
    {
      "caption": "Figure 3: (a) and 3(b), our MS-DCDA method exhibits",
      "page": 10
    },
    {
      "caption": "Figure 3: (c) and 3(d) that neutral emo-",
      "page": 10
    },
    {
      "caption": "Figure 4: for SEED dataset, which",
      "page": 10
    },
    {
      "caption": "Figure 4: (a), the distribution of most of the samples",
      "page": 10
    },
    {
      "caption": "Figure 4: (b) and Fig. 4 (c) respectively.",
      "page": 10
    },
    {
      "caption": "Figure 4: (d) shows the initial distribution of combined source do-",
      "page": 10
    },
    {
      "caption": "Figure 4: (e) that DAN does not show a clear trend of class clut-",
      "page": 10
    },
    {
      "caption": "Figure 5: It is observed that our MS-DCDA model",
      "page": 10
    },
    {
      "caption": "Figure 5: (a)) and cross-session experi-",
      "page": 10
    },
    {
      "caption": "Figure 5: (b)), demonstrating an improved general-",
      "page": 10
    },
    {
      "caption": "Figure 4: T-SNE illustration of domain adaptation and emotion recognition on SEED dataset. (a) Distribution of 14 individual source subjects",
      "page": 11
    },
    {
      "caption": "Figure 5: Comparison of emotion recognition accuracies of different domain adaptation algorithms on SEED and SEED-IV datasets respec-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 2: On the SEED dataset, the constructed",
      "data": [
        {
          "Column_1": "98.70",
          "100.00": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: , our method outperforms other meth-",
      "data": [
        {
          "95.49": "",
          "Column_2": "100.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: On the SEED dataset, the constructed",
      "data": [
        {
          "Column_1": "88.21",
          "Column_2": "100.00",
          "98.68": "88.80",
          "Column_4": "95.58",
          "Column_5": "99.29",
          "Column_6": "92.55",
          "Column_7": "94.87",
          "Column_8": "99.00",
          "Column_9": "90.85"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: On the SEED dataset, the constructed",
      "data": [
        {
          "68.99": "",
          "Column_2": "91.47"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: On the SEED dataset, the constructed",
      "data": [
        {
          "91.47": "",
          "91.11": "",
          "Column_3": "91.47",
          "Column_4": "",
          "Column_5": "77.04"
        },
        {
          "91.47": "",
          "91.11": "",
          "Column_3": "",
          "Column_4": "71.75",
          "Column_5": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "94.43",
          "100.00": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "100.00",
          "Column_2": "100.00",
          "100.00": "100.00",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "100.00": "100.00",
          "Column_4": "",
          "Column_5": "100.00",
          "Column_6": "98.11",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "Column_1": "",
          "Column_2": "100.00",
          "100.00": "93.96",
          "Column_4": "100.00",
          "Column_5": "99.73 84.68",
          "Column_6": "",
          "Column_7": "95.82",
          "Column_8": "99.53"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "99.03",
          "100.00": "98.59",
          "Column_4": "95.82"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "85.46",
          "82.88": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "98.00": "",
          "Column_2": "92.65"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "100.00": "",
          "Column_2": "94.85",
          "77.63": "72.30",
          "Column_4": "82.25"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The emerging view of emotion as social information",
      "authors": [
        "G Van Kleef"
      ],
      "year": "2010",
      "venue": "Social and Personality Psychology Compass"
    },
    {
      "citation_id": "2",
      "title": "From mindful attention to social connection: The key role of emotion regulation",
      "authors": [
        "J Quaglia",
        "R Goodman",
        "K Brown"
      ],
      "year": "2015",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "3",
      "title": "Emotional well-being: What it is and why it matters",
      "authors": [
        "C Park",
        "L Kubzansky",
        "S Chafouleas",
        "R Davidson",
        "D Keltner",
        "P Parsafar",
        "Y Conwell",
        "M Martin",
        "J Hanmer",
        "K Wang"
      ],
      "year": "2023",
      "venue": "Emotional well-being: What it is and why it matters"
    },
    {
      "citation_id": "4",
      "title": "Emotion, motivation, decision-making, the orbitofrontal cortex, anterior cingulate cortex, and the amygdala",
      "authors": [
        "E Rolls"
      ],
      "year": "2023",
      "venue": "Brain Structure and Function"
    },
    {
      "citation_id": "5",
      "title": "The electroencephalogram as a biometric",
      "authors": [
        "R Paranjape",
        "J Mahovsky",
        "L Benedicenti",
        "Z Koles"
      ],
      "year": "2001",
      "venue": "Canadian Conference on Electrical and Computer Engineering"
    },
    {
      "citation_id": "6",
      "title": "Validity and reliability of quantitative electroencephalography",
      "authors": [
        "R Thatcher"
      ],
      "year": "2010",
      "venue": "Journal of Neurotherapy"
    },
    {
      "citation_id": "7",
      "title": "Advances in electrode materials for scalp, forehead, and ear eeg: a mini-review",
      "authors": [
        "G Petrossian",
        "P Kateb",
        "F Miquet-Westphal",
        "F Cicoira"
      ],
      "year": "2023",
      "venue": "ACS Applied Bio Materials"
    },
    {
      "citation_id": "8",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "9",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars",
        "S Bagherzadeh",
        "A Shalbaf",
        "D GarcÃ­a",
        "J Gorriz",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "11",
      "title": "Transfer learning for eeg-based brain-computer interfaces: A review of progress made since",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "12",
      "title": "A review on transfer learning in eeg signal analysis",
      "authors": [
        "Z Wan",
        "R Yang",
        "M Huang",
        "N Zeng",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Self-supervised learning for electroencephalogram",
      "authors": [
        "W Weng",
        "Y Gu",
        "S Guo",
        "Y Ma",
        "Z Yang",
        "Y Liu",
        "Y Chen"
      ],
      "year": "2024",
      "venue": "A systematic survey",
      "arxiv": "arXiv:2401.05446"
    },
    {
      "citation_id": "14",
      "title": "Multi-source fusion domain adaptation using resting-state knowledge for motor imagery classification tasks",
      "authors": [
        "L Zhu",
        "J Yang",
        "W Ding",
        "J Zhu",
        "P Xu",
        "N Ying",
        "J Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "15",
      "title": "Multisource wasserstein adaptation coding network for eeg emotion recognition",
      "authors": [
        "L Zhu",
        "W Ding",
        "J Zhu",
        "P Xu",
        "Y Liu",
        "M Yan",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "16",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "17",
      "title": "Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "D Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "18",
      "title": "Deep source semi-supervised transfer learning (ds3tl) for cross-subject eeg classification",
      "authors": [
        "X Jiang",
        "L Meng",
        "Z Wang",
        "D Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "19",
      "title": "Multi-source domain adaptation in the deep learning era: A systematic survey",
      "authors": [
        "S Zhao",
        "B Li",
        "P Xu",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "Multi-source domain adaptation in the deep learning era: A systematic survey",
      "arxiv": "arXiv:2002.12169"
    },
    {
      "citation_id": "20",
      "title": "Electroencephalography (eeg): An introductory text and atlas of normal and abnormal findings in adults, children, and infants",
      "authors": [
        "J Britton",
        "L Frey",
        "J Hopp",
        "P Korb",
        "M Koubeissi",
        "W Lievens",
        "E Pestana-Knight",
        "E St"
      ],
      "year": "2016",
      "venue": "Electroencephalography (eeg): An introductory text and atlas of normal and abnormal findings in adults, children, and infants"
    },
    {
      "citation_id": "21",
      "title": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "22",
      "title": "Computer-aided diagnosis: a support-vector-machine-based approach of automatic pulmonary nodule detection in chest radiographs",
      "authors": [
        "Q Weng",
        "Y Sun",
        "X Peng",
        "S Wang",
        "L Gu",
        "L Qian",
        "J Xu"
      ],
      "year": "2009",
      "venue": "Proceedings of the 2009 International Symposium on Bioelectronics and Bioinformatics"
    },
    {
      "citation_id": "23",
      "title": "A saliency measure constraint multi-level immersion watershed transformation for medical image segmentation",
      "authors": [
        "X Peng",
        "L Gu",
        "L Pan",
        "Q Weng"
      ],
      "year": "2009",
      "venue": "Proceedings of the 2009 International Symposium on Bioelectronics and Bioinformatics"
    },
    {
      "citation_id": "24",
      "title": "Randomized apertures: high resolution imaging in far field",
      "authors": [
        "X Peng",
        "G Ruane",
        "M Quadrelli",
        "G Swartzlander"
      ],
      "year": "2017",
      "venue": "Optics express"
    },
    {
      "citation_id": "25",
      "title": "Dynamic weighted learning for unsupervised domain adaptation",
      "authors": [
        "N Xiao",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition from multichannel eeg signals using k-nearest neighbor classification",
      "authors": [
        "M Li",
        "H Xu",
        "X Liu",
        "S Lu"
      ],
      "year": "2018",
      "venue": "Technology and health care"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition from multi-channel eeg via deep forest",
      "authors": [
        "J Cheng",
        "M Chen",
        "C Li",
        "Y Liu",
        "R Song",
        "A Liu",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "28",
      "title": "Machine learning and deep learning: A review of methods and applications",
      "authors": [
        "K Sharifani",
        "M Amini"
      ],
      "year": "2023",
      "venue": "World Information Technology and Engineering Journal"
    },
    {
      "citation_id": "29",
      "title": "Learning to see through dazzle",
      "authors": [
        "X Peng",
        "E Fleet",
        "A Watnik",
        "G Swartzlander"
      ],
      "year": "2024",
      "venue": "Learning to see through dazzle",
      "arxiv": "arXiv:2402.15919"
    },
    {
      "citation_id": "30",
      "title": "Cnn-based real-time image restoration in laser suppression imaging, in: Imaging and Sensing Congress",
      "authors": [
        "X Peng",
        "P Srivastava",
        "G Swartzlander"
      ],
      "year": "2021",
      "venue": "Cnn-based real-time image restoration in laser suppression imaging, in: Imaging and Sensing Congress"
    },
    {
      "citation_id": "31",
      "title": "Computational Imaging and Its Applications, Rochester Institute of Technology",
      "authors": [
        "H Peng"
      ],
      "year": "2022",
      "venue": "Computational Imaging and Its Applications, Rochester Institute of Technology"
    },
    {
      "citation_id": "32",
      "title": "Temporal action detection with adaptive context aggregation",
      "authors": [
        "N Wang",
        "Y Xiao",
        "X Peng",
        "X Chang",
        "X Wang",
        "D Fang",
        "Contextdet"
      ],
      "year": "2024",
      "venue": "Temporal action detection with adaptive context aggregation",
      "arxiv": "arXiv:2410.15279"
    },
    {
      "citation_id": "33",
      "title": "A survey of convolutional neural network in breast cancer",
      "authors": [
        "Z Zhu",
        "S Wang",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Cmes-computer Modeling in Engineering & Sciences"
    },
    {
      "citation_id": "34",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "35",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Opt-co: Optimizing pre-trained transformer models for efficient covid-19 classification with stochastic configuration networks",
      "authors": [
        "Z Zhu",
        "L Liu",
        "R Free",
        "A Anjum",
        "J Panneerselvam"
      ],
      "year": "2024",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "37",
      "title": "Deep visual domain adaptation: A survey",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "38",
      "title": "Video unsupervised domain adaptation with deep learning: A comprehensive survey",
      "authors": [
        "Y Xu",
        "H Cao",
        "L Xie",
        "X.-L Li",
        "Z Chen",
        "J Yang"
      ],
      "year": "2022",
      "venue": "Video unsupervised domain adaptation with deep learning: A comprehensive survey"
    },
    {
      "citation_id": "39",
      "title": "Learning a robust unified domain adaptation framework for cross-subject eeg-based emotion recognition",
      "authors": [
        "M JimÃ©nez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "40",
      "title": "A kernel method for the two-sample-problem, Advances in neural information processing systems",
      "authors": [
        "A Gretton",
        "K Borgwardt",
        "M Rasch",
        "B SchÃ¶lkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "A kernel method for the two-sample-problem, Advances in neural information processing systems"
    },
    {
      "citation_id": "41",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "42",
      "title": "Contrastive adaptation network for single-and multi-source domain adaptation",
      "authors": [
        "G Kang",
        "L Jiang",
        "Y Wei",
        "Y Yang",
        "A Hauptmann"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "43",
      "title": "Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation",
      "authors": [
        "H Yan",
        "Y Ding",
        "P Li",
        "Q Wang",
        "Y Xu",
        "W Zuo"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "45",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "46",
      "title": "A deep subdomain associate adaptation network for cross-session and cross-subject eeg emotion recognition",
      "authors": [
        "M Meng",
        "J Hu",
        "Y Gao",
        "W Kong",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "47",
      "title": "A theory of learning from different domains",
      "authors": [
        "S Ben-David",
        "J Blitzer",
        "K Crammer",
        "A Kulesza",
        "F Pereira",
        "J Vaughan"
      ],
      "year": "2010",
      "venue": "Machine learning"
    },
    {
      "citation_id": "48",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "F Wang",
        "W Zhang",
        "Z Xu",
        "J Ping",
        "H Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "49",
      "title": "Optimal kernel choice for large-scale two-sample tests, Advances in neural information processing systems",
      "authors": [
        "A Gretton",
        "D Sejdinovic",
        "H Strathmann",
        "S Balakrishnan",
        "M Pontil",
        "K Fukumizu",
        "B Sriperumbudur"
      ],
      "year": "2012",
      "venue": "Optimal kernel choice for large-scale two-sample tests, Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Q She",
        "C Zhang",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "51",
      "title": "Dynamic transfer for multi-source domain adaptation",
      "authors": [
        "Y Li",
        "L Yuan",
        "Y Chen",
        "P Wang",
        "N Vasconcelos"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L.-C Shi",
        "Y.-Y Jiao",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "53",
      "title": "Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation",
      "authors": [
        "X Chen",
        "S Wang",
        "M Long",
        "J Wang"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "54",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for eeg-based emotion classification"
    },
    {
      "citation_id": "55",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "56",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "57",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "58",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops: Amsterdam"
    },
    {
      "citation_id": "59",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "61",
      "title": "A novel transferability attention neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "B Fu",
        "F Li",
        "G Shi",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "62",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "63",
      "title": "Gmss: Graph-based multi-task self-supervised learning for eeg emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        "Y Zhou",
        "Y Niu",
        "G Shi",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "64",
      "title": "Horizontal and vertical features fusion network based on different brain regions for emotion recognition",
      "authors": [
        "W Guo",
        "G Xu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "65",
      "title": "Two-phase prototypical contrastive domain generalization for cross-subject eeg-based emotion recognition",
      "authors": [
        "H Cai",
        "J Pan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "66",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}