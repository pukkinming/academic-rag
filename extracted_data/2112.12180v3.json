{
  "paper_id": "2112.12180v3",
  "title": "Multimodal Personality Recognition Using Cross-Attention Transformer And Behaviour Encoding",
  "published": "2021-12-22T19:14:55Z",
  "authors": [
    "Tanay Agrawal",
    "Dhruv Agarwal",
    "Michal Balazia",
    "Neelabh Sinha",
    "Francois Bremond"
  ],
  "keywords": [
    "Multimodal Transformer",
    "Multimodal Data",
    "Feature Engineering",
    "Personality Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personality computing and affective computing have gained recent interest in many research areas. The datasets for the task generally have multiple modalities like video, audio, language and bio-signals. In this paper, we propose a flexible model for the task which exploits all available data. The task involves complex relations and to avoid using a large model for video processing specifically, we propose the use of behaviour encoding which boosts performance with minimal change to the model. Cross-attention using transformers has become popular in recent times and is utilised for fusion of different modalities. Since long term relations may exist, breaking the input into chunks is not desirable, thus the proposed model processes the entire input together. Our experiments show the importance of each of the above contributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Personality is a combination of behavior, emotion, motivation, and thought patterns. Our personality greatly impacts our lives, defining choices, health along with our preferences and desires. Personality traits define a particular way of thinking, feeling, and behaving. Specifically, personality traits have been defined pertaining to individual well being and socialinstitutional outcomes like occupational choices, interpersonal relations, and success in various scenarios.\n\nWe make decisions using a two system model: rational and emotional. So, modelling the latter will help us build more accurate AI systems when it is coupled with the vast amount of research done on the former. The problem of personality recognition is complex and thus would require a lot of training data to get models usable in real-life. This is one reason that multimodal learning is very popular in this domain. First Impressions v2  [Junior et al., 2021]  is a multimodal dataset for personality recognition and is used in this work. We utilise all the information available in the dataset -speech, body language, expressions and their surroundings along with their demographic information -and define a new behaviour encoding to facilitate learning. Deep learning backbones have been found to extract meaningful features. Generally, larger models give better features. Due to the high number of inputs, it is not possible to have large backbones for all. So we decide to compute the additional behaviour encoding to have better features even with a smaller backbone. In multimodal learning, we need to process each modality individually and also find how they are correlated. Thus, we also show how to merge the behaviour encoding with an existing baseline  [Palmero et al., 2021] . Temporal processing is also important, we use LSTMs to have a higher temporal resolution. Even a simple temporal processing model helps as there are multiple modalities and the embedding input to the temporal processor is very rich in information.\n\nFor defining personality, the big five personality traits are used. They are often referred to as OCEAN: Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism. These five traits represent broad domains of human behaviour and account for differences in both personality and decision making. Today, a major use of this model is by Human Resource practitioners to evaluate potential employees and marketers to understand the audiences of their products.\n\nThe focus on this problem is relatively new and there is limited work done till date. As discussed above, the task is complex so it requires careful processing of each input modality and their relations. Previous works either involves using a subset of modalities or only simple fusion techniques to fuse the modalities  [Aslan and Güdükbay, 2019] .\n\nAnother challenge in this domain is that annotations are generally provided by the participants through questionnaires or an online answering platform during or after the experiment sessions. They may also be annotated by third-party annotators, but personality is subjective so they are not always perfect. This makes the task even harder and further requires a method that can utilise all available modalities and formulate complex relations not only for each modality but also across modalities. Defining handcrafted inputs increases performance as there might be more direct correlation between them and other modalities or even the output as compared to the original inputs.\n\nAs stated above, ChaLearn First Impressions V2 challenge dataset  [Ponce-López et al., 2016]  which is publicly available is used  [Palmero et al., 2021]  for this work. The dataset consists of 10,000 videos of people facing and speaking to a camera. Videos are extracted from YouTube, they are mostly in highdefinition (1280×720 pixels), and, in general, they have an average duration of 15 seconds with 30 frames per second. In the videos, people talk to the camera in a self-presentation context and there is a diversity in terms of age, ethnicity, gender, and nationality. The videos are labeled with personality factors using Amazon Mechanical Turk (AMT), so the ground truth values are obtained by using human judgment. For the challenge, videos are split into training, validation and test sets with a 3:1:1 ratio and we choose to use the same to compare the results.\n\nSummarising our contributions in this work, we introduce a handcrafted behaviour embedding that improves performance and reduces convergence time of the model. We modify the chosen baseline to incorporate new modalities (transcript and behaviour encoding) and also address missing temporal relations in it. We also achieve state of the art results for personality recognition on the chosen dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section discusses the work done on personality recognition using different techniques and modalities. They can be broadly classified into the following categories.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Using Video",
      "text": "As in the case of most visual deep learning tasks, Convolutional Neural Networks (CNNs) are the most commonly used in the field of personality detection. Facial attributes can be an important factor in predicting social traits  [Qin et al., 2016 , Vernon et al., 2014] . Impressions that influence people's behavior towards other individuals can be accurately predicted from videos  [Gürpınar et al., 2016] . Many researchers have experimented with different ways of capturing facial features such as in the form of Facial Action Coding System (FACS) which extracts action units such as raised eyebrows or blinking, and morphological features  [Güc ¸lütürk et al., 2017] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Using Audio",
      "text": "Using audio as the only input modality is not a popular choice for personality recognition. It is combined with video in most of the cases resulting in bimodal approaches. In the existing ones, audio features like Mel-Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Logfbank, other cepstral and spectral ones serve as inputs into regressors. Analyzing conversations  [Valente et al., 2012]  and the pitch, timber, tune and rhythm of the voice  [Madzlan et al., 2014] , it is possible to recognize the personality traits or predict the speaker attitudes automatically. These approaches demonstrate that audio information is important for personality.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Using Text",
      "text": "Looking at the textual modality, preprocessing is an important step. Generally, extracted features include Linguistic Inquiry and Word Count (LIWC)  [Mikolov et al., 2013] , Mairesse, Medical Research Council (MRC), which are then fed into standard classifiers or regressors. Learning word embeddings and representing them as vectors, either with GloVe, Word2Vec, GRU, LSTM or recently BERT, is also a very commonly followed approach. It was observed that combining text features with something else such as metadata and convolutions results in better performance paving the path to multimodal approaches.\n\nSocial networks provide rich textual data for the recognition of personality traits  [Alam et al., 2013 , Farnadi et al., 2016] . Transcribed videos blogs and dialogues also provide useful information for this task  [Nowson and Gill, 2014] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Approaches",
      "text": "Personality traits can be detected in self presentation videos based on the acoustic and visual, non-verbal features such as pitch, intensity, movement, head orientation, posture, fidgeting and eye-gaze. Zheng et al.  [Zeng et al., 2009]  shows body gestures, head movements, expressions, and speech lead to effective assessment of personality and emotion. According to Sarkar et al.  [Sarkar et al., 2014] , features such as audiovisual, text, demographic and sentiment features are important for our task.\n\nAlthough multimodal approaches are commonly used to recognize personality traits, there does not exist a comprehensive method utilizing a considerable amount of informative features. Most of the multimodal approaches perform late fusion. Deep bimodal regression give state of the art results  [He et al., 2015] . Some other approaches with good results are  [Gürpinar et al., 2016 , Zhang et al., 2016]  and  [Wei et al., 2018] .\n\nEach modality features may be used together for personality prediction, this approach is called early fusion. Present research in the field aims to find efficient ways of feature extraction and combination. Few models which have dealt with trimodal fusion of features  [Aslan and Güdükbay, 2019, Palmero et al., 2021] . Emotion recognition is a closely related problem and has interesting approaches for multimodal data processing  [Dai et al., 2021 , Tsai et al., 2019] . Our approach aims to utilise all possible information available and also some extra features computed similar to the ideas discussed in the beginning of this subsection.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Proposed Framework",
      "text": "The approach uses face crops of the target person and relates it to body language, surroundings and speech using a transformer based architecture. Shortterm temporal relations are processed in this way and longer temporal relations are established using LSTM. For transcript analysis, short term temporal relations are not very meaningful so the features for the entire input sequences are extracted using BERT  [Devlin et al., 2019] . Late fusion is then finally used for inferring the OCEAN personality traits. There are several stages in the proposed method and they are discussed in the following sections. Figure  1  shows the overview of the entire architecture.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preparing The Input And Feature Extraction",
      "text": "The audiovisual data is pre-processed in a similar manner as in  [Palmero et al., 2021] . 32 frames with a stride of 2 are taken for video based inputs and R(2+1)D  [Tran et al., 2018]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Behaviour Encoding",
      "text": "We compute behaviour encoding for 13 actions: head tilt, thrust, bob, lips in, mouth corner, frown, small mouth, wrinkle, crouch, lean forward, fold arms, hand to face, hand to mouth. For detecting the individual behaviors, we use a rule based approach on the skeleton and facial key points.\n\nFor extracting the key points (skeleton and face), we use LCRNet  [Rogez et al., 2019]  and OpenFace  [Baltrusaitis et al., 2016] .\n\nIn each frame, we infer a detection confidence for all behaviors in the scale of 0-1, where 1 represents complete confidence in presence of the behavior and 0 represents complete confidence in absence of the behavior. This is done by extracting a specific feature x and transforming it through a sigmoid function with parameters of center c σ and multiplier λ σ . As shown in Table  3 , each behavior is characterized by a specific extracted feature and the two sigmoid function parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Positional Encoding For The Transformer",
      "text": "Positional encodings are important to be added to the input with transformer based models as they make the model order invariant. Sinusoidal encodings are common, but we choose to use learned encodings in our experiments. As we need to process in both space and time, we need an encoding for both. We initialize encodings for both and use a two layer fully connected network for learning them. Then they are broadcast concatenated to each other resulting in an encoding which can be concatenated to the input.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preparing Inputs For The Transformer",
      "text": "Features extracted from face crops of the complete frame input are further processed and are used as the query for the transformer. To factor its relation with the rest of the information in the complete frame and audio inputs, they are processed to be used as key and value. The face features are passed through the following layers to get the input query:\n\n1. 3D max pooling layer with a kernel size and stride of 2 for height and width dimensions, and 1 for the temporal dimension.\n\n2. 3D convolution layer with kernel size of 1 for all dimensions and 16 kernels.\n\n3. ReLU activation followed by reshaping to merge temporal and channel dimensions. 4. 2D max pooling layer with a kernel size and stride of 2 for height and width dimension.\n\n5. 2D convolution layer with kernel size of 2 for all dimensions and 128 kernels.\n\n6. ReLU activation followed by flattening.\n\n7. A fully connected layer to change the shape to 128, followed by ReLU activation and dropout p = 0.2 layer.\n\nDemographic metadata is concatenated to the obtained feature vector and is passed through a fully connected and ReLU layer to obtain a 128 dimensional query vector.\n\nBehaviour encoding is broadcast concatenated to complete frame features which already contain spatio-temporal positional encoding. The audio features are projected into a 100 sized feature vector using a fully connected layer and broadcast concatenated with the above obtained complete frame fea-tures. These are passed through separate fully connected and ReLU layers to obtain keys and values for the transformer.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Transformer, Temporal Processing And Fusion With Transcript Features",
      "text": "The transformer consists of only the encoder with 2 attention heads and stacked 3 times, that is, 3 layers. The hidden dimension is 128. The transformer processes roughly 2.5 seconds of the input in one forward pass. These chunks are passed through two stacked LSTM blocks to find long-term temporal relations.\n\nThe hidden state after the last chunk is passed, is concatenated with transcript features and passed through linear, ReLU and dropout layers to obtain the 5 personality trait values.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Details",
      "text": "To reduce training time, the parameters of backbones R(2+1)D, VGGish and BERT are frozen and are not updated during backpropagation. There is one exception to this, we finetune our model with the weights unfrozen as behaviour encoding helps improve the performance of the backbone also and to exploit that, we finetune our model for 20 epochs. One RTX 6000 GPU is used for training and the batch size is set to 8. Learning rate used is 10 -5 with the scheduler \"Re-duceLRonPlateau\", patience 5 and factor 0.5. Further details of the experiments are given in section 4.3. Figure  2  and 3 show training graphs of two different experiments, an ablation study with the proposed framework without transcript and the proposed framework, respectively. It is interesting to note that adding modalities decreases the number of epochs required for convergence as shown by these two figures.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "The evaluation metric is chosen to be the same as that of the ChaLearn challenge where the dataset was released. The OCEAN traits have five classes which are rated in the range [0, 1]. The challenge  [Ponce-López et al., 2016]  defines mean accuracy A over all predicted personality trait values as where t i are the ground truth scores and p i are the predicted scores for personality traits summed over N videos.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Ablation Studies",
      "text": "We compare our result to the previous state of the art and also perform ablation studies to show the need for all modalities present. Table  3  enumerates these results.\n\nWe achieve state of the art results as we utilise all the available information and also compute an additional behaviour embedding to facilitate learning. This method of computing a behaviour encoding can be utilised in a variety of use-cases and we predict that it will help in reducing training time and improving results in other areas, such as action recognition also.\n\nThe ablation study proves the efficacy of our approach, showing the importance of using different in- .  9166 .9214 .9208 .9189 .9162 .9188 DCC [Güc ¸lütürk et al., 2016]  .  9117 .9113 .9110 .9158 .9091 .9122  evolgen  [Subramaniam et al., 2016]  .  9130 .9136 .9145 .9157 .9098 .9138 Gurpinar et al. [Gürpinar et al., 2016]  .  9141 .9141 .9186 .9143 .9123 .9147  PML  [Bekhouche et al., 2017]  .  9138 .9166 .9175 .9166 .9130 .9170  BU-NKU  [Kaya et al., 2017]  .  9169 .9166 .9206 .9161 .9149   put modalities and the difference in results is significant. All the different models discussed below are trained in parallel and not sequentially, that is, the later models were not finetuned from the initial ones.\n\nThe first approach includes the baseline model and has the same inputs modified as per the dataset details.\n\nThe baseline has all the inputs except the behaviour encoding and the transcript. This experiment is to establish our own baseline results to compare against. We see the results of the model without behaviour encoding. There is roughly 1.8% decrease in accuracy which shows that behaviour encoding facilitates in the prediction of personality.\n\nWe also observe the performance of the model without transcript. This shows a similar trend as behaviour encoding -there is a slightly less decrease in accuracy but the difference is minute.\n\nFor finding the performance of the model with LSTM, we keep everything the same but take the median value across chunks for each video to get the output. Without LSTM, the model behaves erratically. As expected the accuracy decreases for most classes. But, for the class neuroticism, the best results are without LSTM. One explanation is the high variability in the inputs where neuroticism is high and the LSTM which tries to identify a pattern across chunks does not perform very well.\n\nThe last experiment is without metadata about the target person. There is not much difference in results as compared to the other inputs, but we still see a reduction in performance. So, demographic data about a person affects personality too. Some bias in the data is the most probable cause but since the dataset is large, in our opinion this is not the case and the inference drawn holds.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we show that a model for personality recognition will benefit from more modalities and data as input. We propose a new handcrafted behaviour encoding where each element is the probability of a low level action relevant to the task. We show the effectiveness of all the inputs in the data through ablation studies. We also give our opinion on the trends shown in the ablation studies. Owing to the interdisciplinary nature of the project, there are numerous additions that will further improve performance. From intuition, there are some which might improve performance by a higher margin than others. Using better backbones for feature extraction would be interesting. We use the same ones as in the baseline we choose but there are existing models with better performance for similar tasks that can be utilised. Transformers have been shown to perform better than LSTMs. In the future, we will try to increase temporal scale of attention in the transformer rather than using a separate module for combining information across chunks. This might tackle the problem that is seen with neuroticism as discussed in section 4.3. One of the major drawbacks of multimodal data is that preprocessing takes a lot of time. Thus, it will be interesting to explore Knowledge Distillation to allow the model to utilise one or a subset of modalities and give a similar performance but with lesser inputs. We would also like to test our approach on other big scale multimodal datasets, when they are available in the future. This area of work has a lot of applications in healthcare which we are exploring and hope that this work leads to advancement in the area. We also hope that it motivates other people to work on this interesting problem.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed method to infer self-reported personality (OCEAN) traits from multimodal data. Input consists of visual",
      "page": 4
    },
    {
      "caption": "Figure 2: and 3 show training graphs of two differ-",
      "page": 5
    },
    {
      "caption": "Figure 2: MSE Loss curves for w/o Transcript Ablation ex-",
      "page": 6
    },
    {
      "caption": "Figure 3: MSE Loss curves for our proposed approach",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Demographic variable": "Ethnicity\nGender\nAge\nAttractiveness",
          "Dimension": "3D (one hot encoding)\n2D (one hot encoding)\n1D\n1D"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "behavior": "head tilt",
          "extracted feature x": "head roll angle",
          "cσ": "10",
          "λσ": "1"
        },
        {
          "behavior": "thrust",
          "extracted feature x": "derivative\nof\ntranslation\nz\nvector\nalong\naxis when\nderivative\nalong\nother\ndirections\nis\nless\nthan\n10\nand direction of derivative\nin previous and next frame\nis the same",
          "cσ": "−25cm/s",
          "λσ": "1"
        },
        {
          "behavior": "bob",
          "extracted feature x": "derivative\nof\npitch\nangle\nwhen derivative of yaw an-\ngle is\nless\nthan 20 and di-\nrection of derivative in pre-\nvious and next frame is the\nsame",
          "cσ": "−50deg/s 1",
          "λσ": ""
        },
        {
          "behavior": "lips in",
          "extracted feature x": "FACS action unit Lip Suck",
          "cσ": "-",
          "λσ": "-"
        },
        {
          "behavior": "mouth corner",
          "extracted feature x": "FACS\naction\nunit\nLip\nStretcher",
          "cσ": "1.2",
          "λσ": "6"
        },
        {
          "behavior": "frown",
          "extracted feature x": "FACS\naction\nunit\nBrow\nLowerer",
          "cσ": "1.2",
          "λσ": "6"
        },
        {
          "behavior": "small mouth",
          "extracted feature x": "FACS\naction\nunit\nLip\nTightener",
          "cσ": "1.2",
          "λσ": "6"
        },
        {
          "behavior": "wrinkle",
          "extracted feature x": "FACS\naction\nunit\nNose\nWrinkler",
          "cσ": "1.2",
          "λσ": "6"
        },
        {
          "behavior": "crouch",
          "extracted feature x": "distance between knees and\nhead",
          "cσ": "30cm",
          "λσ": "−0.35"
        },
        {
          "behavior": "lean forward",
          "extracted feature x": "z coordinate on distance be-\ntween root and shoulders",
          "cσ": "10cm",
          "λσ": "4"
        },
        {
          "behavior": "fold arms",
          "extracted feature x": "alternate distance between\nelbows and wrists when y\ncoordinate of both elbows\nare less than 10cm",
          "cσ": "20cm",
          "λσ": "−0.5"
        },
        {
          "behavior": "hand to face",
          "extracted feature x": "distance\nbetween\nwrists\nand head",
          "cσ": "35cm",
          "λσ": "−0.5"
        },
        {
          "behavior": "hand to mouth distance",
          "extracted feature x": "between\nwrists\nand head minus 10cm on y\naxis",
          "cσ": "25cm",
          "λσ": "−0.5"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Experiments and Results; O: Openness, C: Con-",
      "data": [
        {
          "O": "[Aslan and G¨ud¨ukbay, 2019]",
          "C": ".9166 .9214 .9208 .9189 .9162 .9188",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "",
          "C": ".9117 .9113 .9110 .9158 .9091 .9122",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "[Subramaniam et al., 2016]",
          "C": ".9130 .9136 .9145 .9157 .9098 .9138",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "",
          "C": ".9141 .9141 .9186 .9143 .9123 .9147",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "",
          "C": ".9138 .9166 .9175 .9166 .9130 .9170",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "",
          "C": ".9169 .9166 .9206 .9161 .9149 .9170",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        },
        {
          "O": "",
          "C": ".9291 .9258 .9272 .9288 .9210 .9263",
          "E": "",
          "A": "",
          "N": "",
          "Mean": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Experiments and Results; O: Openness, C: Con-",
      "data": [
        {
          "Baseline:\nw/o be-\nhaviour\nencoding\nand transcript": "w/o behaviour\nen-\ncoding",
          ".8959 .8996 .8987 .8938 .8932 .8962": ".9095 .9094 .9112 .9133 .9041 .9095"
        },
        {
          "Baseline:\nw/o be-\nhaviour\nencoding\nand transcript": "w/o transcript",
          ".8959 .8996 .8987 .8938 .8932 .8962": ".9013 .8992 .8988 .9041 .8996 .9006"
        },
        {
          "Baseline:\nw/o be-\nhaviour\nencoding\nand transcript": "w/o LSTM",
          ".8959 .8996 .8987 .8938 .8932 .8962": ".8892 .8532 .9131 .9024 .9315 .8978"
        },
        {
          "Baseline:\nw/o be-\nhaviour\nencoding\nand transcript": "w/o metadata",
          ".8959 .8996 .8987 .8938 .8932 .8962": ".9260 .9212 .9234 .9249 .9168 .9225"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Personality traits recognition on social network -facebook",
      "authors": [
        "Alam"
      ],
      "year": "2013",
      "venue": "Personality traits recognition on social network -facebook"
    },
    {
      "citation_id": "2",
      "title": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks",
      "authors": [
        "Aslan Güdükbay ; Aslan",
        "S Güdükbay"
      ],
      "year": "2019",
      "venue": "Multimodal video-based apparent personality recognition using long short-term memory and convolutional neural networks"
    },
    {
      "citation_id": "3",
      "title": "Openface: An open source facial behavior analysis toolkit",
      "authors": [
        "Baltrusaitis"
      ],
      "year": "2016",
      "venue": "Openface: An open source facial behavior analysis toolkit"
    },
    {
      "citation_id": "4",
      "title": "Personality traits and job candidate screening via analyzing facial videos",
      "authors": [
        "Bekhouche"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "5",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Dai"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "7",
      "title": "Computational personality recognition in social media",
      "authors": [
        "Farnadi"
      ],
      "year": "2016",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "8",
      "title": "Multimodal fusion of audio, scene, and face features for first impression estimation",
      "authors": [
        "Gürpınar"
      ],
      "year": "2016",
      "venue": "rd International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "9",
      "title": "Visualizing apparent personality analysis with deep residual networks",
      "authors": [
        "Güc ¸lütürk"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "10",
      "title": "Deep impression: Audiovisual deep residual networks for multimodal apparent personality trait recognition",
      "authors": [
        "Güc ¸lütürk"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "11",
      "title": "Cnn architectures for largescale audio classification",
      "authors": [
        "He"
      ],
      "year": "2015",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Multi-modal score fusion and decision trees for explainable automatic job candidate screening from video cvs",
      "authors": [
        "Kaya"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "13",
      "title": "Automatic recognition of attitudes in video blogs -prosodic and visual feature analysis",
      "authors": [
        "Madzlan"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition, WCPR '14"
    },
    {
      "citation_id": "14",
      "title": "Chalearn lap 2016: First round challenge on first impressions -dataset and results",
      "authors": [
        "Ponce-López"
      ],
      "year": "2016",
      "venue": "Modern physiognomy: An investigation on predicting personality traits and intelligence from the human face"
    },
    {
      "citation_id": "15",
      "title": "Lcr-net++: Multi-person 2d and 3d pose detection in natural images",
      "authors": [
        "Rogez"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Feature analysis for computational personality recognition using youtube personality data set",
      "authors": [
        "Sarkar"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition, WCPR '14"
    },
    {
      "citation_id": "17",
      "title": "Bi-modal first impressions recognition using temporally ordered deep audio and stochastic visual features",
      "authors": [
        "Subramaniam"
      ],
      "year": "2016",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Tsai"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Annotation and recognition of personality traits in spoken conversations from the AMI meetings corpus",
      "authors": [
        "Valente"
      ],
      "year": "2012",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Modeling first impressions from highly variable facial images",
      "authors": [
        "Vernon"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "21",
      "title": "Deep bimodal regression of apparent personality traits from short video sequences",
      "authors": [
        "Wei"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Zeng"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Deep Bimodal Regression for Apparent Personality Analysis",
      "authors": [
        "Zhang"
      ],
      "year": "2016",
      "venue": "Deep Bimodal Regression for Apparent Personality Analysis"
    }
  ]
}