{
  "paper_id": "2104.08857v1",
  "title": "Emotion-Regularized Conditional Variational Autoencoder For Emotional Response Generation",
  "published": "2021-04-18T13:53:20Z",
  "authors": [
    "Yu-Ping Ruan",
    "Zhen-Hua Ling"
  ],
  "keywords": [
    "Emotional Response Generation",
    "Latent Variables",
    "Variational Autoencoder"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents an emotion-regularized conditional variational autoencoder (Emo-CVAE) model for generating emotional conversation responses. In conventional CVAE-based emotional response generation, emotion labels are simply used as additional conditions in prior, posterior and decoder networks. Considering that emotion styles are naturally entangled with semantic contents in the language space, the Emo-CVAE model utilizes emotion labels to regularize the CVAE latent space by introducing an extra emotion prediction network. In the training stage, the estimated latent variables are required to predict the emotion labels and token sequences of the input responses simultaneously. Experimental results show that our Emo-CVAE model can learn a more informative and structured latent space than a conventional CVAE model and output responses with better content and emotion performance than baseline CVAE and sequence-to-sequence (Seq2Seq) models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "T HERE has been growing interest in building conversation agents that can chat with humans directly in natural language. Because emotion is a vital part of human intelligence  [1] , generating responses with specific emotions is one of the key steps towards making conversation agents more human-like. Existing studies have provided much evidence that systems capable of expressing emotions in their responses can improve user satisfaction  [2, 3] .\n\nTraditionally, dialog systems depend on manual efforts to establish rules, design features, or build models with particular learning algorithms  [4, 5, 6] . In early studies on emotional dialog systems, manually designed rules were used to deliberately select the desired \"emotional\" responses from a conversation corpus  [7, 8] ; however, this approach is difficult or impossible to extend to unseen situations and large-scale datasets.\n\nBenefiting from the development of deep learning techniques, neural-network-based models, e.g., sequence-to-sequence (Seq2Seq) models  [9, 10, 11, 12]  and variational generative models  [13, 14, 15, 16] , have achieved impressive results in training end-toend generative chatting machines. Several studies based on Seq2Seq and CVAE models have been devoted to generating emotional responses  [17, 18, 19] ; these studies all adopted RNNs as building blocks and did not take advantage of pretrained language models. Many recent studies have proven that language models pretrained on large-scale corpora can achieve state-of-the-art results on both language understanding and generation tasks  [20, 21, 22, 23] . Thus, it is also worthwhile to utilize pretrained language models as building blocks for emotional response generation.\n\nIn this paper, we propose an emotion-regularized conditional variational autoencoder (Emo-CVAE) model for emotional response generation. The Emo-CVAE model uses BERT  [21] , one of the most popular pretrained language models, as the basis for its encoder and decoder. Different from the conventional CVAE model adopted in a previous study  [19] , which simply used the emotion label e as an extra input condition, the Emo-CVAE model utilizes the emotion label e to regularize the latent distribution. Specifically, the estimated latent variable z in Emo-CVAE is used to recover the emotion label and token sequence of the input response y simultaneously, considering that emotional style and content are naturally entangled in the language space. In experiments on the generation of short text conversations, we have found that the Emo-CVAE model can learn a more informative and structured latent distribution than the conventional CVAE model and output responses with better content and emotion performance than CVAE and Seq2Seq baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Recently, several neural-network-based models have been proposed for emotional response generation. Zhou et al.  [17]  proposed an emotional chatting machine (ECM), in which an external emotion vocabulary and an internal emotion state memory were introduced for better emotional expressiveness. Song et al.  [18]  proposed an emotional dialog system (EmoDS), in which a sentence-level emotion classifier and an external emotion vocabulary were used to guide the response generation process. Both the ECM and EmoDS models were based on the Seq2Seq framework. Considering that CVAE models can perform better than Seq2Seq models for the generation of diverse responses  [13, 14, 16] , a CVAE-based emotional response generation method has also been proposed for Twitter conversations  [19] .\n\nThere are still some deficiencies with existing studies on neuralnetwork-based emotional response generation. First, the ECM  [17] , EmoDS  [18] , and CVAE  [19]  models mentioned above all employ RNNs as building blocks and do not take advantage of pretrained language models. Second, the conventional CVAE model adopted in a previous study  [19]  simply utilized the emotion label e as an extra input condition, ignoring the interrelation between contents and emotional styles in the language space of responses.\n\nTo address these shortcomings, our Emo-CVAE model uses BERT  [21]  as the basis for its encoder and decoder. In addition, considering the difficulty of disentangling the stylistic properties of a sentence from its semantic content  [24, 25] , the Emo-CVAE model constructs an entangled latent space regularized by emotional attributes, which has proven to be beneficial for learning a more informative and structured latent distribution in our experiments.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "From Cvae To Emo-Cvae",
      "text": "Figure  1  shows the directed graphical models of the conventional CVAE approach and our proposed Emo-CVAE approach. As Figure  1 (a) shows, in the CVAE model applied for emotional response generation in  [19] , the input post x and the emotion label e are jointly used as the input conditions for estimating the latent variable z and generating the response y. Specifically, a CVAE is composed of a prior network p θ (z|x, e), a posterior network q φ (z|x, e, y), and a decoder network p θ (y|x, e, z). Both p θ (z|x, e) and q φ (z|x, e, y) are multivariate diagonal Gaussian distributions. Its model parameters can be efficiently trained within the stochastic gradient variational Bayes (SGVB) framework  [26]  by maximizing the lower bound on the conditional log likelihood log p(y|x, e), as follows: L(θ, φ; x, e, y) = -KL(q φ (z|x, e, y)||p θ (z|x, e))\n\n+ E q φ (z|x,e,y) [log p θ (y|x, e, z)] ≤ log p(y|x, e).\n\nOne deficiency of CVAE-based emotional response generation is that the posterior network q φ (z|x, e, y) is conditioned on both e and y, in conflict with the fact that the emotion label e and the semantic content of the response y are entangled in y itself (i.e., e can be inferred from y). Furthermore, e is also used as a condition for decoding y, which causes the latent distribution in the CVAE to be independent of the input condition e.\n\nTherefore, in the proposed Emo-CVAE model, the input condition e is eliminated from both the posterior and decoder networks. Instead, both the emotion label e and the response y are required to be recovered from the latent variable z simultaneously; to this end, an entangled latent distribution is adopted. Specifically, as shown in Figure  1 (b), an Emo-CVAE includes a prior network p θ (z|x, e), a posterior network q φ (z|x, y), and a decoder network p θ (y|x, z). In addition, an emotion prediction network q φ (e|z) is introduced to recover the emotion label e from the sampled latent variable z and to regularize the CVAE latent space.\n\nThe model parameters of an Emo-CVAE are estimated by maximizing L = L sgvb + L emo . Following the training strategy L sgvb (θ, φ; x, e, y) = -KL(q φ (z|x, y)||p θ (z|x, e))\n\nL emo is defined as L emo (θ, φ; x, e, y) = E q φ (z|x,y) [log q φ (e|z)] + E p θ (z|x,e) [log q φ (e|z)]\n\nand acts as an emotion regularizer on the latent variables sampled using either the posterior or prior network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Implementation",
      "text": "The architecture of the Emo-CVAE model implemented in this paper is shown in Figure  2 , and detailed introductions to the model components are given in this subsection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bert-Encoder",
      "text": "The BERT-Encoder is built on the basis of the pretrained BERT model  [21] . Its inputs in the training stage include the post x, the response y, and the emotion label e of y. The input token sequence is shown in Figure  3 (a), in which \"[Emotion]\", \"[ENC posterior ]\" and \"[ENC prior ]\" are tokens standing for the emotion label e, the encoding results for the posterior distribution, and the encoding results for the prior distribution, respectively. {post} and {resp} represent the token sequences of x and y, respectively. The toplayer encoding vectors of \"[ENC posterior ]\" and \"[ENC prior ]\" correspond to the vectors enc posterior and enc prior shown in Figure  2 , which are further sent to the Posterior Network and the Prior Network to produce the distributions q φ (z|x, y) and p θ (z|x, e), respectively. In the testing stage, the input token sequence for the BERT-Encoder is as shown in Figure  3 (b), and only enc prior is output.  As shown in Table  1 , the attention relationships among the tokens in the BERT-Encoder are designed in accordance with the dependencies among the variables in q φ (z|x, y) and p θ (z|x, e). For example, the \"[ENC posterior ]\" token attends to \"[ENC posterior ]\", {post}, and {resp} because enc posterior is used to generate the distribution q φ (z|x, y).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bert-Encoder",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bert-Decoder",
      "text": "The BERT-Decoder is also based on the pretrained BERT model  [21] . Its input token sequence in the training stage is shown in Figure  4 (a), in which \"[z]\", \"[SOS]\", and \"[EOS]\" stand for the sampled latent vector, the start flag token for a response, and the end flag token for a response, respectively. {post} and {resp} represent the token sequences of x and y, respectively. Figure  4 (b) presents the initial input token sequence in the testing stage when generating a response. The attention relationships among the tokens in the BERT-Decoder, as shown in Table  2 , are designed to generate a response from p θ (y|x, z) in a token-by-token manner. Thus, each token in {resp} attends to both the tokens in {post} and its historical tokens in {resp}. The teacher-forcing mode is adopted when training the BERT-Decoder. In the testing stage, once a token in {resp} has been predicted, it is appended to the end of the token sequence and is attended to by subsequent tokens. This process is repeated until the \"[EOS]\" token is predicted.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Other Modules",
      "text": "The Posterior Network is a linear output layer that accepts enc posterior as input and outputs µ and log(σ 2 ) for the posterior distribution q φ (z|x, y) = N (µ, σ 2 I). The Prior Network is structurally identical to the Posterior Network; it accepts enc prior as input and makes predictions for the prior distribution p θ (z|x, e). The Emotion-pred Network is a multilayer perceptron (MLP); in",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Table 2",
      "text": "The attention relationships among the tokens in the BERT-Decoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attended Tokens",
      "text": "tokens in {resp} all except future tokens in {resp} and \"[EOS]\" \"[EOS]\" all tokens our implementation, it has a hidden layer with tanh activation and a linear output layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reranking Multiple Responses",
      "text": "To help model output responses with better content relevance and emotional expressiveness, multiple responses to each post are first generated in the testing stage and are then reranked. Specifically, for variational generative models, we first generate multiple samples of z, and then, a beam search is adopted for each z sample to obtain the best result. For Seq2Seq models, we first use the top-N candidates for the head word as seed words. Then, we continue to generate a response from each seed word through beam search decoding.\n\nThe score used for reranking the multiple responses to each post is defined as\n\nwhere λ is a constant weight and score emo and score rele are explained as follows.\n\n(1) score emo : The emotional expressiveness score is determined by a trained emotion classifier (see Section 4.1). If the predicted emotion label of the generated response is consistent with the corresponding input emotion label, score emo = 1. Otherwise, score emo = 0.\n\n(2) score rele : The topic relevance score is provided by a topic coherence discrimination (TCD) model built by fine-tuning BERT  [21] . Specifically, we concatenate the post and the response to serve as the input token sequence for BERT and define the objective to judge whether the response is a valid response to the given post. The negative samples for training the TCD model are collected by randomly shuffling the mapping between the posts and responses. For a generated response ỹ, we have score rele = p T CD (true|x, ỹ).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "In our experiments, we adopted the short text conversation (STC) dataset from NTCIR-12  [27] , which was crawled from the Chineselanguage platform Sina Weibo 1 , one of the most popular social media platforms in China, and contains 1, 800, 000 post-response pairs 2 . We randomly split the data into 1, 708, 415, 73, 120, and 18, 465 pairs to build the training, development and test sets, respectively. There were no overlapping posts among these three sets.\n\nFollowing previous studies  [17, 18] , we first trained an emotion classifier on the NLPCC dataset 3 and then annotated the responses in the STC dataset using this classifier. Specifically, the NLPCC dataset we used contains 61, 483 sentences collected from Weibo, which were manually annotated into 8 emotion categories, i.e., liking, disgust, happiness, sadness, anger, surprise, fear, and other. The NLPCC dataset was partitioned into training, development, and test sets at a ratio of 8:1:1. The emotion classifier was built by fine-tuning BERT and ultimately achieved an accuracy of 68% on the NLPCC test set. The percentages of responses with different emotion annotations in the STC training set are shown in Table  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baselines",
      "text": "In our experiments, we compared the Emo-CVAE model with four baselines, i.e., EmoDS, ECM, Seq2Seq, and CVAE models.\n\n• EmoDS We implemented the EmoDS model  [18] , which is based on the Seq2Seq framework, using LSTM-RNNs as building blocks.\n\n• ECM We implemented the ECM model  [17] , which is also based on the Seq2Seq framework, using GRU-RNNs as building blocks.\n\n1. https://weibo.com/ 2. This dataset was originally prepared for retrieval models and thus has no standard division for generative models. Here, we filtered the post-response pairs in the raw STC dataset in accordance with the word frequencies to build our dataset.\n\n3  • Seq2Seq To ensure better fairness in the comparisons between the Seq2Seq models and our Emo-CVAE model, we also implemented a Seq2Seq model for emotional response generation based on BERT. Specifically, the Seq2Seq model was structurally identical to the BERT-Decoder in the Emo-CVAE model, but the input token \"[z]\" in the Emo-CVAE model was replaced with the token \"[Emotion]\" for representing emotion labels.\n\n• CVAE The CVAE model used in a previous work [19] was based on GRU-RNNs. To support a fairer comparison between the CVAE and Emo-CVAE approaches, we implemented a CVAE model based on BERT. Specifically, the CVAE implementation was almost the same as the Emo-CVAE implementation, with three differences. First, a token \"[Emotion]\" for emotion labels was concatenated with the head of the input token sequence in the BERT-Decoder. Second, the attention flow settings in the BERT-Encoder and BERT-Decoder were revised to satisfy the requirements of the prior network p θ (z|x, e), the posterior network q φ (z|x, e, y), and the decoder network p θ (y|x, e, z). Third, there was no Emotionpred Network in the CVAE model, and the L emo term in the Emo-CVAE objective function was eliminated accordingly.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Parameter Settings",
      "text": "The PyTorch implementation of BERT with the pretrained model file bert-base-chinese provided by Google 4 was employed. The latent variable z in the CVAE and Emo-CVAE models had 768 dimensions, and the hidden size of the Emotion-pred Network was set to 768. For the fine-tuning process, we mostly followed the default settings. Specifically, the learning rate was 2e-05, and the batch size was set to 128. When training the CVAE and Emo-CVAE models, the KL annealing strategy was adopted to address the issue of latent variable vanishing. The model parameters were pretrained without optimizing the KL divergence term. Additionally, we adopted a training strategy of optimizing the KLD loss term every 15 steps but optimizing the reconstruction nonnegative log likelihood (NLL) loss term every step. As described in Section 3.3, we generated multiple responses to each post. Specifically, 5 responses were generated for each unique post in the test set concerning different emotional inputs, and the beam search size was 5.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Objective Evaluations",
      "text": "The objective evaluation metrics used in our experiments included the following:\n\n1) Emotional accuracy (Emo Acc.), which was determined by the trained emotion classifier introduced in Section 4.1 (i.e., the average of score emo on a test set) and evaluates the models' ability to generate responses expressing specific emotions. 2) Relevance score (Rele.), which was determined by the trained TCD model introduced in Section 3.3 (i.e., the average of score rele on a test set) and evaluates whether the generated responses are relevant to the topic of the input post. 3) Diversity performance scores (distinct-1, distinct-2, and Uniq.), which represent the percentages of distinct unigrams, bigrams  [12] , and sentences  [16]  in the generated responses. The distinct-1/2 and Uniq. scores evaluate the generated responses at the n-gram level and the sentence level, respectively. 4) Fluency performance (PPL on LM), which represents the perplexity of the generated responses evaluated using a BERT language model trained on the same STC dataset. It should be noted that better diversity performance generally leads to higher LM perplexity for a specific model, which implies worse performance in terms of fluency.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Effects Of Different Λ Weights",
      "text": "As introduced in Section 3.3, multiple responses were generated and reranked for each post in our experiments. To investigate the influence of the weight λ in Equation (  4 ) on the reranking results of the different models, we varied the value of λ in increments of 0.1 in the range of [0.2, 1.2], and the average reranking scores of the models' top-1 responses are presented in Figure  5 . There were trade-offs between score emo and score rele for all models, and a larger λ always led to higher accuracy of emotional expression. There was a large performance gap between the RNN-based models (i.e., EmoDS and ECM) and the BERT-based models (i.e., Seq2Seq, CVAE, and Emo-CVAE), which indicates the effectiveness of using the pretrained BERT model  [21]  for emotional response generation. Compared with the baselines, the Emo-CVAE model obtained the highest overall reranking scores.\n\nTo simplify further objective and subjective evaluations, λ was fixed to 0.5 in the following experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overall Performance",
      "text": "Table  4  presents the objective evaluation results for the top-1 responses generated by the different models.\n\nTable  4  shows that the performance of the RNN-based models, i.e., EmoDS and ECM, was obviously worse than that of the BERTbased models in terms of the relevance and diversity metrics. The variational generative models, i.e., CVAE and Emo-CVAE, achieved better performance in generating diverse responses than the models based on the sequence-to-sequence framework, which is consistent with the findings of previous studies  [16] . Specifically, our Emo-CVAE model outperformed the CVAE model and achieved the best performance in terms of both emotional accuracy and relevance. Regarding LM perplexity, both the Emo-CVAE and CVAE models performed worse than the other models. This is reasonable since better diversity generally leads to higher LM perplexity. Compared with the CVAE model, our Emo-CVAE model achieved almost the same diversity performance but lower LM perplexity.\n\nTable  5  presents the detailed emotional accuracies of the top-1 responses generated by the different models. The advantages of Emo-CVAE over other models mainly lie in its expressiveness for the emotions of \"disgust\" and \"anger\".",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Subjective Evaluations",
      "text": "Several groups of subjective ABX preference tests were conducted. To make the human evaluations more efficient, a nonuniform random sampling strategy was adopted for selecting the test samples for human judgment. Table  6  shows the proportions of the test samples conditioned on the emotional correctness of Emo-CVAE and the baselines when applied for response generation. For example, the \"TF\" subset represents the samples for which the responses generated by Emo-CVAE had the correct emotional expression but the responses generated by a baseline model did not. Then, 30 samples from each subset in Table  6  were randomly selected, and six native Chinese speakers with rich Sina Weibo experience were recruited for an evaluation. For each test sample, a pair of responses generated by two of the models were presented in a random order. Based on each of two subjective metrics, emotion and content, the evaluators were asked to judge which response in each pair was preferred or if there was no preference. Here, emotion refers to whether the emotional expression of a response agrees with the given emotion label, and content refers to whether the response is relevant to the topic, informative, and fluent. The p-value of a t-test was adopted to measure the significance of the difference between each pair of models. Several significance levels were examined, including p < 0.05, p < 0.01, and p < 0.001. A level of p > 0.05 indicated that there was no significant difference between the two models. The subjective evaluation results are presented in Table  7 .\n\nTable  7  shows that the Emo-CVAE model significantly outperformed all baseline models in terms of both the emotion and content metrics for the \"TF\" subset. For the \"TT\" subset, our Emo-CVAE model also showed a significant advantage over the baseline models except when compared with the ECM model based on the emotion metric. For the \"FT\" and \"FF\" subsets, some baseline models outperformed the Emo-CVAE model. However, the proportions of responses belonging to the \"FT\" and \"FF\" subsets were much smaller than those belonging to the \"TT\" and \"TF\" subsets (see Table  6 ), which indicates that our Emo-CVAE model can generate responses with better overall content relevance and emotional expressiveness than the baselines.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Importance Of Emotion Regularization",
      "text": "To investigate the importance of emotion regularization in the Emo-CVAE model, we first removed the emotion regularization applied to the prior latent distribution, resulting in the \"Emo-CVAE-M1\" model. Then, the emotion regularization on the posterior latent distribution was also removed, yielding the \"Emo-CVAE-M2\" model. Table  8  presents the objective evaluation results for the top-1 responses generated by the Emo-CVAE models with different emotion regularization settings.\n\nTable  8  shows that although the relevance score is slightly degraded with emotion regularization, emotion regularization on both the prior and posterior distributions is critically important for generating emotional responses of the expected emotional types.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Between Emo-Cvae And Cvae",
      "text": "Several ablation studies were conducted to further assess the advantages of the Emo-CVAE approach over the CVAE approach. We gradually modified the structure of the CVAE model toward that of the Emo-CVAE model. First, we removed the input e for the BERT-Decoder in the CVAE model, resulting in the \"CVAE-M1\" model. Then, the emotion regularization strategy was applied to the latent distributions in CVAE-M1 by introducing the emotion prediction network and the L emo component for model training, yielding the \"CVAE-M2\" model. The only difference between CVAE-M2 and Emo-CVAE lies in their posterior distributions, i.e., q φ (z|x, e, y) and q φ (z|x, y), respectively. Table  9  presents the objective evaluation results of the ablation studies.\n\nTable  9  shows that the emotional accuracy of CVAE-M1 is dramatically degraded, indicating that the latent distribution learned in CVAE-M1 fails to accurately convey emotional information. The CVAE-M2 model achieves better emotional accuracy than either the CVAE-M1 model or the original CVAE model, demonstrating the effectiveness of our proposed emotion regularization strategy. However, CVAE-M2 still does not perform as well as the Emo-CVAE model.  To better analyze the above results, the latent distributions learned by the different models were visualized. Specifically, t-SNE  [28]  was utilized to visualize the distributions of the posterior z samples (i.e., the posterior distributions). As Figure  6 (a) shows, the latent distribution in Emo-CVAE has a clear emotion-clustered structure, although there are blending trends among sample clusters corresponding to different emotions, which reflect the entanglement of emotion and content. The z samples associated with similar emotions, such as \"happiness\" vs. \"liking\" and \"disgust\" vs. \"anger\", are distributed near each other. On the other hand, the latent distributions q φ (z|x, e, y) learned by the CVAE and CVAE-M1 models appear independent of emotion, indicating that the emotional information from the input conditions e and y vanishes in the latent distributions q φ (z|x, e, y) of these models. The CVAE-M2 model can learn a latent distribution with a strong emotion-clustered structure due to the help of emotion regularization. However, the distribution q φ (z|x, e, y) in CVAE-M2 is less informative than the distribution q φ (z|x, y) in Emo-CVAE. The emotional information expressed by q φ (z|x, e, y) in CVAE-M2 can be simply copied from the input condition e, while to obtain q φ (z|x, y) in Emo-CVAE, it is necessary to infer the emotional information from the post x and the response y. Such a posterior distribution with inferred emotional information may be beneficial for learning prior distributions for response generation.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed an emotion-regularized conditional variational autoencoder (Emo-CVAE) model for emotional conversation response generation. In this model, an entangled latent distribution is designed by forcing the estimated latent variable to recover the emotion label and token sequence of a response simultaneously. In experiments on the generation of short text conversations, we have found that the Emo-CVAE model can learn a more informative and structured latent distribution than the conventional CVAE model and generate output responses with better content and emotional expressiveness than CVAE and Seq2Seq baselines.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Graphical models of (a) a CVAE and (b) an Emo-CVAE, where x, y, and e represent the input post (i.e., the conversation context), the",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the directed graphical models of the conventional",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) shows, in the CVAE model applied for emotional",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), an Emo-CVAE includes a prior network",
      "page": 2
    },
    {
      "caption": "Figure 2: , and detailed introductions to the model",
      "page": 2
    },
    {
      "caption": "Figure 3: (a), in which “[Emotion]”, “[ENCposterior]”",
      "page": 2
    },
    {
      "caption": "Figure 2: , which are further sent to the Posterior Network and",
      "page": 2
    },
    {
      "caption": "Figure 2: The architecture of the Emo-CVAE model implemented in this paper.",
      "page": 3
    },
    {
      "caption": "Figure 3: The input token sequences for the BERT-Encoder in (a) the",
      "page": 3
    },
    {
      "caption": "Figure 4: (a), in which “[z]”, “[SOS]”, and “[EOS]” stand for the",
      "page": 3
    },
    {
      "caption": "Figure 4: (b) presents the initial input token sequence in the testing stage",
      "page": 3
    },
    {
      "caption": "Figure 4: (a) The input token sequence of the BERT-Decoder in the training",
      "page": 3
    },
    {
      "caption": "Figure 5: The average emotional expressiveness scores and topic relevance",
      "page": 4
    },
    {
      "caption": "Figure 5: There were",
      "page": 5
    },
    {
      "caption": "Figure 6: t-SNE visualization (in 2D space) of the posterior latent distributions produced by the (a) Emo-CVAE, (b) CVAE, (c) CVAE-M1, and (d)",
      "page": 7
    },
    {
      "caption": "Figure 6: (a) shows, the latent distribution in Emo-CVAE has a clear",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attended Tokens": "“[ENCposterior]”, {post}, {resp}"
        },
        {
          "Attended Tokens": "“[ENCprior]”, “[Emotion]”, {post}"
        },
        {
          "Attended Tokens": "“[Emotion]”"
        },
        {
          "Attended Tokens": "{post}, “[SEP]”(0)"
        },
        {
          "Attended Tokens": "“[SEP]”(0), {post}"
        },
        {
          "Attended Tokens": "{resp}, “[SEP]”(1)"
        },
        {
          "Attended Tokens": "“[SEP]”(1), {resp}"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attended Tokens": "“[z]”, {post}"
        },
        {
          "Attended Tokens": "“[z]”, {post}"
        },
        {
          "Attended Tokens": "“[z]”, {post}, “[SOS]”"
        },
        {
          "Attended Tokens": "all except\nfuture tokens in {resp}\nand “[EOS]”"
        },
        {
          "Attended Tokens": "all\ntokens"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: presents the objective evaluation results for the top-1",
      "data": [
        {
          "all": "89.37\n94.44",
          "other\nliking\ndisgust\nhappiness\nsadness\nanger\nsurprise\nfear": "98.27\n98.27\n68.78\n99.42\n92.40\n64.86\n98.39\n94.59\n99.54\n98.96\n99.31\n99.07\n85.71\n96.65\n80.07\n96.20"
        },
        {
          "all": "91.95\n84.29\n97.89",
          "other\nliking\ndisgust\nhappiness\nsadness\nanger\nsurprise\nfear": "99.77\n99.19\n88.02\n98.16\n93.66\n72.00\n97.58\n87.21\n98.84\n97.46\n75.92\n92.40\n84.10\n51.73\n87.90\n85.94\n99.65\n97.70\n97.58\n94.82\n96.43\n99.65\n99.08\n98.16"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The intelligence of emotional intelligence",
      "authors": [
        "J Mayer",
        "P Salovey"
      ],
      "year": "1993",
      "venue": "INTELLIGENCE"
    },
    {
      "citation_id": "2",
      "title": "The effects of affective interventions in human-computer interaction",
      "authors": [
        "T Partala",
        "V Surakka"
      ],
      "year": "2004",
      "venue": "Interacting with computers"
    },
    {
      "citation_id": "3",
      "title": "The empathic companion: A character-based interface that addresses users'affective states",
      "authors": [
        "H Prendinger",
        "M Ishizuka"
      ],
      "year": "2005",
      "venue": "Applied artificial intelligence"
    },
    {
      "citation_id": "4",
      "title": "Partially observable Markov decision processes for spoken dialog systems",
      "authors": [
        "J Williams",
        "S Young"
      ],
      "year": "2007",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "5",
      "title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies",
      "authors": [
        "J Schatzmann",
        "K Weilhammer",
        "M Stuttle",
        "S Young"
      ],
      "year": "2006",
      "venue": "The knowledge engineering review"
    },
    {
      "citation_id": "6",
      "title": "POMDP-based statistical spoken dialog systems: A review",
      "authors": [
        "S Young",
        "M Gasic",
        "B Thomson",
        "J Williams"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "7",
      "title": "Emotion-sensitive humancomputer interfaces",
      "authors": [
        "T Polzin",
        "A Waibel"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "8",
      "title": "Affect listeners: Acquisition of affective states by means of conversational systems",
      "authors": [
        "M Skowron"
      ],
      "year": "2010",
      "venue": "Development of Multimodal Interfaces: Active Listening and Synchrony"
    },
    {
      "citation_id": "9",
      "title": "A neural conversational model",
      "authors": [
        "O Vinyals",
        "Q Le"
      ],
      "year": "2015",
      "venue": "A neural conversational model",
      "arxiv": "arXiv:1506.05869"
    },
    {
      "citation_id": "10",
      "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "authors": [
        "I Serban",
        "A Sordoni",
        "Y Bengio",
        "A Courville",
        "J Pineau"
      ],
      "year": "2016",
      "venue": "Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Neural responding machine for short-text conversation",
      "authors": [
        "L Shang",
        "Z Lu",
        "H Li"
      ],
      "year": "2015",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "A diversity-promoting objective function for neural conversation models",
      "authors": [
        "J Li",
        "M Galley",
        "C Brockett",
        "J Gao",
        "B Dolan"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter"
    },
    {
      "citation_id": "13",
      "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "authors": [
        "I Serban",
        "A Sordoni",
        "R Lowe",
        "L Charlin",
        "J Pineau",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Learning discourselevel diversity for neural dialog models using conditional variational autoencoders",
      "authors": [
        "T Zhao",
        "R Zhao",
        "M Eskenazi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Latent variable dialogue models and their diversity",
      "authors": [
        "K Cao",
        "S Clark"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "16",
      "title": "Condition-transforming variational autoencoder for conversation response generation",
      "authors": [
        "Y.-P Ruan",
        "Z.-H Ling",
        "Q Liu",
        "Z Chen",
        "N Indurkhya"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Generating responses with a specific emotion in dialog",
      "authors": [
        "Z Song",
        "X Zheng",
        "L Liu",
        "M Xu",
        "X.-J Huang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "MojiTalk: Generating emotional responses at scale",
      "authors": [
        "X Zhou",
        "W Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "21",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "22",
      "title": "XLNet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Unified language model pretraining for natural language understanding and generation",
      "authors": [
        "L Dong",
        "N Yang",
        "W Wang",
        "F Wei",
        "X Liu",
        "Y Wang",
        "J Gao",
        "M Zhou",
        "H.-W Hon"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Adversarial removal of demographic attributes from text data",
      "authors": [
        "Y Elazar",
        "Y Goldberg"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Multiple-attribute text rewriting",
      "authors": [
        "G Lample",
        "S Subramanian",
        "E Smith",
        "L Denoyer",
        "M Ranzato",
        "Y.-L Boureau"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "Overview of the NTCIR-12 short text conversation task",
      "authors": [
        "L Shang",
        "T Sakai",
        "Z Lu",
        "H Li",
        "R Higashinaka",
        "Y Miyao"
      ],
      "year": "2016",
      "venue": "NTCIR"
    },
    {
      "citation_id": "28",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}