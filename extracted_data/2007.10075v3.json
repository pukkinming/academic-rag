{
  "paper_id": "2007.10075v3",
  "title": "Investigating Bias And Fairness In Facial Expression Recognition",
  "published": "2020-07-20T13:12:53Z",
  "authors": [
    "Tian Xu",
    "Jennifer White",
    "Sinan Kalkan",
    "Hatice Gunes"
  ],
  "keywords": [
    "Fairness",
    "Bias Mitigation",
    "Facial Expression Recognition ResNet-18 Image FC_exp Expression Classification FC_gen Gender Classification Gender Confusion FC_race Race Classification Race Confusion FC_age Age Classification Age Confusion (c) ResNet-18 FC_exp Expression Classification Image"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognition of expressions of emotions and affect from facial images is a well-studied research problem in the fields of affective computing and computer vision with a large number of datasets available containing facial images and corresponding expression labels. However, virtually none of these datasets have been acquired with consideration of fair distribution across the human population. Therefore, in this work, we undertake a systematic investigation of bias and fairness in facial expression recognition by comparing three different approaches, namely a baseline, an attribute-aware and a disentangled approach, on two wellknown datasets, RAF-DB and CelebA. Our results indicate that: (i) data augmentation improves the accuracy of the baseline model, but this alone is unable to mitigate the bias effect; (ii) both the attribute-aware and the disentangled approaches equipped with data augmentation perform better than the baseline approach in terms of accuracy and fairness; (iii) the disentangled approach is the best for mitigating demographic bias; and (iv) the bias mitigation strategies are more suitable in the existence of uneven attribute distribution or imbalanced number of subgroup data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatically recognising expressions and affect from facial images has been widely studied in the literature  [27, 33, 42] . Thanks to the unprecedented advances in machine learning field, many techniques for tackling this task now use deep learning approaches  [27]  which require large datasets of facial images labelled with the expression or affect displayed.\n\nAn important limitation of such a data-driven approach to affect recognition is being prone to biases in the datasets against certain demographic groups  [5, 10, 14, 25, 34, 38] . The datasets that these algorithms are trained on do not necessarily contain an even distribution of subjects in terms of demographic attributes such as race, gender and age. Moreover, majority of the existing datasets that are made publicly available for research purposes do not contain information regarding these attributes, making it difficult to assess bias, let alone mitigate it. Machine learning models, unless explicitly modified, are severely impacted by such biases since they are given more opportunities (more training samples) for optimizing their objectives towards the majority group represented in the dataset. This leads to lower performances for the minority groups, i.e., subjects represented with less number of samples  [5, 10, 12, 14, 25, 34, 38, 41] . To address these issues, many solutions have been proposed in the machine learning community over the years, e.g. by addressing the problem at the data level with data generation or sampling approaches  [2, 9, 20, 21, 32, 36, 39, 49] , at the feature level using adversarial learning  [1, 35, 49, 51]  or at the task level using multi-domain/task learning  [11, 49] .\n\nBias and mitigation strategies in facial analysis have attracted increasing attention both from the general public and the research communities. For example, many studies have investigated bias and mitigation strategies for face recognition  [5, 12, 13, 14, 35, 38, 41] , gender recognition  [8, 12, 43, 50] , age estimation  [6, 8, 12, 16, 43] , kinship verification  [12]  and face image quality estimation  [44] . However, bias in facial expression recognition has not been investigated, except for  [9, 49] , that only focussed on the task of smiling/non-smiling using the CelebA dataset.\n\nIn this paper, we undertake a systematic investigation of bias and fairness in facial expression recognition. To this end, we consider three different approaches, namely a baseline deep network, an attribute-aware network and a representation-disentangling network (following  [1, 35, 49, 51] ) under the two conditions of with and without data augmentation. As a proof of concept, we conduct our experiments on RAF-DB and CelebA datasets that contain labels in terms of gender, age and/or race. To the best of our knowledge, this is the first work (i) to perform an extensive analysis of bias and fairness for facial expression recognition, beyond the binary classes of smiling / non-smiling  [9, 49] , (ii) to use the sensitive attribute labels as input to the learning model to address bias, and (iii) to extend the work of  [31]  to the area of facial expression recognition in order to learn fairer representations as a bias mitigation strategy.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Bias in the field of Human-Computer Interaction (HCI), and in particular issues arising from the intersection of gender and HCI have been discussed at length in  [4] . However, studies specifically analysing, evaluating and mitigating race, gender and age biases in affect recognition have been scarce. We therefore provide a summary of related works in other forms of facial analysis including face and gender recognition, and age estimation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bias And Mitigation In Machine Learning",
      "text": "Attention to bias and fairness in machine learning (ML) has been rapidly increasing with the employment of ML applications in everyday life. It is now well accepted that ML models are extremely prone to biases in data  [5, 22] , which has raised substantial concern in public such that regulatory actions are being as preventive measure; e.g. European Commission  [7]  requires training data for such applications to be sufficiently broad, and to reflect all relevant dimensions of gender, ethnicity and other possible grounds of prohibited discrimination.\n\nBias mitigation strategies in ML generally take inspiration from data or class balancing approaches in ML, a very related problem which directly pertains to imbalance in the task labels. Bias can be addressed in an ML model in different ways  [3, 10] : For example, we can balance the dataset in terms of the demographic groups, using under-sampling or over-sampling  [20, 49] , sample weighting  [2, 21, 39] , data generation  [9, 36] , data augmentation  [32]  or directly using a balanced dataset  [46, 47] . However, it has been shown that balancing samples does not guarantee fairness among demographic groups  [48] .\n\nAnother strategy to mitigate bias is to remove the sensitive information (i.e. gender, ethnicity, age) from the input at the data level (a.k.a. \"fairness through unawareness\")  [1, 49, 51] . However, it has been shown that the remaining information might be implicitly correlated with the removed sensitive attributes and therefore, the residuals of the sensitive attributes may still hinder fairness in the predictions  [11, 17] . Alternatively, we can make the ML model more aware of the sensitive attributes by making predictions independently for each sensitive group (a.k.a. \"fairness through awareness\")  [11] . Formulated as a multi-task learning problem, such approaches allow an ML model to separate decision functions for different sensitive groups and therefore prohibit the learning of a dominant demographic group to negatively impact the learning of another one.Needless to say this comes at a cost -it dramatically increases the number of parameters to be learned, as a separate network or a branch needs to be learned for each sensitive group.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Bias In Facial Affect Recognition",
      "text": "It has been long known that humans' judgements of facial expressions of emotion are impeded by the ethnicity of the faces judged  [23] . In the field of automatic affect recognition, systematic analysis of bias and the investigation of mitigation strategies are still in their infancy. A pioneering study by Howard et al.  [19]  investigated how using a cloud-based emotion recognition algorithm applied to images associated with a minority class (children's facial expressions) can be skewed when performing facial expression recognition on the data of that minority class. To remedy this, they proposed a hierarchical approach combining outputs from the cloud-based emotion recognition algorithm with a specialized learner. They reported that this methodology can increase the overall recognition results by 17.3%. Rhue  [40] , using a dataset of 400 NBA player photos, found systematic racial biases in Face++ and Microsoft's Face API. Both systems assigned to African American players more negative emotional scores on average, regardless of how much they smiled.\n\nWhen creating a new dataset is not straightforward, and/or augmentation is insufficient to balance an existing dataset, Generative Adversarial networks (GAN) have been employed for targeted data augmentation. Denton et al. in  [9]  present a simple framework for identifying biases in a smiling attribute classifier. They utilise GANs for a controlled manipulation of specific facial characteristics and investigate the effect of this manipulation on the output of a trained classifier. As a result, they identify which dimensions of variation affect the predictions of a smiling classifier trained on the CelebA dataset. For instance, the smiling classifier is shown to be sensitive to the Young dimensions, and the classification of 7% of the images change from a smiling to not smiling classification as a result of manipulating the images in this direction. Ngxande et al.  [36]  introduce an approach to improve driver drowsiness detection for under-represented ethnicity groups by using GAN for targeted data augmentation based on a population bias The Attribute-aware Approach, (c) The Disentangled Approach visualisation strategy that groups faces with similar facial attributes and highlights where the model is failing. A sampling method then selects faces where the model is not performing well, which are used to fine-tune the CNN. This is shown to improve driver drowsiness detection for the under-represented ethnicity groups. A representative example for non-GAN approaches is by Wang et al.  [49]  who studied the mitigation strategies of data balancing, fairness through blindness, and fairness through awareness, and demonstrated that fairness through awareness provided the best results for smiling/not-smiling classification on the CelebA dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "To investigate whether bias is a problem for the facial expression recognition task, we conduct a comparative study using three different approaches. The first one acts as the baseline approach and we employ two other approaches, namely the Attribute-aware Approach and the Disentangled Approach, to investigate different strategies for mitigating bias. These approaches are illustrated in detail in Fig.  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Definition And Notation",
      "text": "We are provided with a facial image x i with a target label y i . Moreover, each x i is associated with sensitive labels s i =< s 1 , ..., s m >, where each label s j is a member of an attribute group, s j ∈ S j . For our problem, we consider m = 3 attribute groups (race, gender, age) , and S j can be illustratively defined to be {Caucasian, African-American, Asian} for j = race. The goal then is to model p(y i |x i ) without being affected by s i .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Baseline Approach",
      "text": "Our baseline is a Residual Network (ResNet)  [18] , a widely used architecture which achieved high performance for many automatic recognition tasks. We utilise a 18-layer version (ResNet-18) for our analyses. We train this baseline network with a Cross Entropy loss to predict a single expression label y i for each input x i :\n\nwhere p k is the predicted probability for x i being assigned to class k ∈ K; and 1[•] is the indicator function.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "The Attribute-Aware Approach",
      "text": "Inspired by the work described in  [16, 11] , we propose an alternative \"fairness through awareness\" approach. In  [16, 11] , separate networks or branches are trained for each sensitive attribute, which is computationally more expensive.\n\nIn our attribute-aware solution, we provide a representation of the attributes as another input to the classification layer (Fig.  1(b) ). Note that this approach allows us to investigate how explicitly providing the attribute information can affect the expression recognition performance and whether it can mitigate bias.\n\nTo be comparable with the baseline approach, ResNet-18 is used as the backbone network for extracting a feature vector φ(x i ) from image x i . In order to match the size of φ(x i ), which is 512 in the case of ResNet-18, the attribute vector s i is upsampled through a fully-connected layer: φ s (s i ) = W s s i + b s . Then, the addition φ s (s i ) + φ(x i ) is provided as input to the classification layer (Figure  1 ). The network is trained using the Cross Entropy loss in Equation  1 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Disentangled Approach",
      "text": "The main idea for this approach is to make sure the learned representation φ(x i ) does not contain any information about the sensitive attributes s i ; in other words, we cannot predict s i from x i while being able to predict the target label y i . To do that, we utilise the disentangling approach described in  [29, 1] .\n\nFor this purpose, we first extract φ(x i ) using ResNet-18 to be consistent with the first two approaches. Then we split the network into two sets of branches: One primary branch for the primary classification task (expression recognition) with the objective outlined in Equation  1 , and parallel branches designed to ensure that φ(x i ) cannot predict s i .\n\nThe parallel branches use a so-called confusion loss to make sure that sensitive attributes cannot be predicted from φ(x i ):\n\nwhich essentially tries to estimate equal probability (1/|S j |) for each sensitive attribute. If this objective is satisfied, then we can ensure that φ(x i ) cannot predict s i . However, the network can easily learn a trivial solution to not map φ(x i ) to S even when φ(x i ) contains sensitive information. To avoid this trivial solution, an attribute predictive Cross Entropy loss is also used  [29] :\n\nwhich functions as an adversary to L conf in Equation  2 .\n\nThese tasks share all the layers till the final fully-connected layer φ(). At the final fully-connected layer parallel branches are created for specific tasks (Figure  1(c) ). The difference between the training of the primary expression classification task and the sensitive attribute classification tasks is that the gradients of L s are only back-propagated to φ(), but do not update the preceding layers similar to  [29] .\n\nThe overall loss is then defined as:\n\nwhere α the contribution of the confusion loss. By jointly minimizing L exp , L s and L conf , the final shared feature representation φ is forced to distill the facial expression information and dispel the sensitive attribute information. Alvi et al.  [1]  claim that this approach can improve the classification performance when faced with an extreme bias.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "In this section, we provide details about how we evaluate and compare the three methods in terms of their performance for expression recognition and fairness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "Majority of the affect / expression datasets do not contain gender, age and ethnicity labels. Therefore, as a proof of concept we conduct our investigations on two well-known datasets, RAF-DB  [28]  and CelebA  [30] , that meet the following criteria: (1) providing labels for expressions of emotions and/or affect; (2) providing labels for gender, age and/or ethnicity attributes for each sample; and (3) being large enough to enable the training and evaluation of the state-of-the-art deep learning models.\n\nRAF-DB  [28]  is a real-world dataset, with diverse facial images collected from the Internet. The images are manually annotated with expression labels and attribute labels (i.e. race, age and gender of the subjects). For our experiments, we chose a subset of the dataset with basic emotion labels -14,388 images, with 11,512 samples used for training and 2,876 samples used for testing. The task we focus on using this dataset is to recognize the seven categories of facial expressions of emotion (i.e. Anger, Disgust, Fear, Happiness, Sadness, Surprise and Neutral).\n\nCelebA  [30]  is a large-scale and diverse face attribute dataset, which contains 202,599 images of 10,177 identities. There are 40 types of attribute annotations in the dataset. Three attributes are chosen in our experiments, including \"Smiling, \"Male\" and \"Young\", corresponding to \"Facial Expression\", \"Gender\" and \"Age\" information. Although CelebA does not contain full range of expression annotations, it provides \"Gender\" and \"Age\" information that can be utilized to investigate bias and fairness. To the best of our knowledge, there is no other large-scale real-world dataset that can be used for this purpose. Thus, we conduct additional experiments on CelebA as a supplement. Officially, CelebA is partitioned into three parts (training set, validation set and testing set). Our models are trained using the training set and evaluated on the testing set. The task we focus on using this dataset is to recognize the expression of Smiling.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Image Pre-Processing, Augmentation And Implementation",
      "text": "For both RAF-DB and CelebA datasets, images are first aligned and cropped, so that all faces appear in approximately similar positions, and normalized to a size of 100 × 100 pixels. These images are fed into the networks as input.\n\nDeep networks require large amounts of training data to ensure generalization for a given classification task. However, most facial expression datasets available for research purposes do not contain sufficient number of samples to appropriately train of a model, and this may result in overfitting. Therefore, we use data augmentation in this study, which is a commonly employed strategy in many recognition studies when training a network. During the training step, two strategies of image augmentation are applied to the input. For strategy one, the input samples are randomly cropped to a slightly smaller size (i.e. 96 × 96); randomly rotated with a small angle (i.e. range from -15 • to 15 • ); and horizontally mirrored in a randomized manner. For strategy two, histogram equalization is applied to increase the global contrast of the images. Following the suggestion from  [26] , we adopt a weighted summation approach to take advantage of both strategy one and two.\n\nAll three methods are implemented using PyTorch  [37]  and trained with the Adam optimizer  [24] , with a mini-batch size of 64, and an initial learning rate of 0.001. The learning rate decays linearly by a factor of 0.1 every 40 epochs for RAF-DB and every 2 epochs for CelebA. The maximum training epochs is 200, but early stopping is applied if the accuracy does not increase after 30 epochs for RAF-DB and 5 epochs for CelebA.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "When performing a classification task, the most commonly used metrics are accuracy, precision and recall, with most studies reporting comparative results using the accuracy metric. However, these metrics are not sufficient in exposing differences in performance (bias) in terms of gender, age and ethnicity attributes. Therefore, we propose to evaluate the three algorithms introduced using two evaluation metrics: Accuracy and Fairness.\n\nAccuracy is simply the fraction of the predictions that the model correctly predicts. Fairness indicates whether a classifier is fair to the sensitive attributes of gender, age and ethnicity. There are various definitions of fairness  [45] . For this study, we use the Fairness of \"equal opportunity\" as described in  [17] . The main idea is that the classifier should ideally provide similar results across different demographic groups.\n\nLet x, y be the variables denoting the input and the ground truth label targeted by the classifier; let ŷ be the predicted variable; and let s ∈ S i be a sensitive attribute (e.g. S i = {male, female}). Suppose there are C classes: c = 1, ..., C, and let p(ŷ = c|x) denote the probability that the predicted class is c. Then \"equal opportunity\" measures the difference between p(ŷ = c|y = c, s = s 0 , x) and p(ŷ = c|y = c, s = s 1 , x), where s 0 , s 1 are e.g. male and female attributes. Suppose there are only two demographic groups in the sensitive attribute (e.g. 'female' and 'male' in for attribute 'gender') considered, then the fairness measure F can be defined as:\n\nIf the sensitive attribute s is not binary (i.e. s ∈ {s 0 , s 1 , ..., s n } with n > 1), the fairness measure can be defined to measure the largest accuracy gap among all demographic groups. Let d denote the dominant sensitive group (i.e. the group which has the highest overall per-class accuracy). This is calculated by summing up the class-wise accuracy. Then the fairness measure F in a non-binary case is defined as:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments On Raf-Db",
      "text": "We first present a bias analysis on RAF-DB and perform experiments to investigate whether it is possible to mitigate this bias through the three approaches we proposed in the Methodology section.\n\nRAF-DB Bias Analysis RAF-DB contains labels in terms of facial expressions of emotions (Surprise, Fear, Disgust, Happy, Sad, Anger and Neutral) and demographic attribute labels along gender, race and age. The provided attribute labels are as follows -Gender : Male, Female, Unsure; Race: Caucasian, African-American, Asian; and Age: 0-3 years, 4-19 years, 20-39 years, 40-69 years, 70+ years. For simplicity, we exclude images labelled as Unsure for Gender. We performed an assessment of how images in the dataset represent the different race, age and gender categories. Table  1  shows a detailed breakdown for the testing data. Note that the distribution of the training data are kept similar to the testing data. Looking at Table  1 , we observe that with 77.4%, the vast majority of the subjects in the dataset are Caucasian, 15.5% are Asian, and only 7.1% are African-American. 56.3% of the subjects are female, while 43.7% are male. Most subjects are in the 20-39 years age category, with the 70+ years category containing the fewest images. This confirms what we have mentioned earlier, that majority of the affect/expression datasets have been acquired without a consideration for containing images that are evenly distributed across the attributes of gender, age and ethnicity. Therefore the goal of our experiment is to investigate  whether it is possible to mitigate this bias through the three approaches we proposed in the Methodology section: the Baseline Approach, the Attribute-aware Approach, and the Disentangled Approach.\n\nExpression Recognition For each method, we trained two versions, with and without data augmentation. The performance of all six models are presented in Table  2 . We observe that data augmentation increases the accuracy and this applies to almost all the expression categories. The baseline model with data augmentation provides the best performance, but the difference compared to the attribute-aware approach and the disentangled approach with data augmentation are minimal. When comparing the performance across all expression categories, we observe that the accuracy varies and this variation is closely associated with the number of data points available for each expression category (See Table  1 ). The expression category of \"Happiness\" is classified with the highest accuracy, while the categories of \"Fear\" and \"Disgust\" are classified with the lowest accuracy.\n\nThe accuracy breakdown provided in Table  2  by expression labels cannot shed light on the performance variation of the classifiers across different demographic groups. Thus, in Table  3  we provide a detailed comparison of the accuracies broken down by each demographic group. To further shed light on the inter-play between the gender and race attributes, in Table  3  we also include results for joint Gender-Race groups. Note that the accuracy presented in Table  3  is classwise accuracy, which refers to the accuracy for each expression category. In this way, we ensure that the weights of all categories are the same. This enables a more accurate analysis of fairness, not affected by the over-represented classes in the dataset. Note that there are only a few data points for certain subgroups (i.e. Age 0-3 years, Age 70+, African-American), so the results obtained for these groups are likely to be unstable. From Table  3 , we can see that for class-wise accuracy, the disentangled approach with data augmentation provides the best accuracy, with the attribute-aware approach being the runner-up. This suggests that both the attribute-aware and the disentangled approaches can improve the class-wise accuracy when equipped with data augmentation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Assessment Of Fairness",
      "text": "In order to provide a quantitative evaluation of fairness for the sensitive attributes of age, gender and race, we report the estimated fairness measures (obtained using Equation  6 ) for the three models in Table  4 . We observe that, compared to the baseline model, both the attribute-aware and the disentangled approaches demonstrate a great potential for mitigating bias for the unevenly distributed subgroups such as Age and Joint Gender-Race. We note that, the observed effect is not as pronounced when the distribution across (sub)groups is more or less even (e.g., results for gender, which has less gap in comparison -Table  1 ). For the baseline model, applying data augmentation improves the accuracy by approximately 7% (Table  2 ), but this alone can not mitigate the bias effect. Instead, both the attribute-aware and the disen- tangled approaches when equipped with data augmentation achieve further improvements in terms of fairness. We conclude that, among the three approaches compared, the disentangled approach is the best one for mitigating demographic bias.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "We provided a comprehensive analysis on the performance of the three approaches on RAF-DB in Tables  2 3 4 . We observed that although the accuracies achieved by the three models are relatively similar, their abilities in mitigating bias are notably different. The attribute-aware approach utilizes the attribute information to allow the model to classify the expressions according to the subjects sharing similar attributes, rather than drawing information from the whole dataset, which may be misleading. The drawback of this approach is that it requires explicit attribute information apriori (e.g., the age, gender and race of the subject whose expression is being classified) which may not be easy to acquire in real-world applications. Instead, the disentangled approach mitigates bias by enforcing the model to learn a representation that is indistinguishable for different subgroups and does not require attribute labels at test time. This approach is therefore more appropriate and efficient when considering usage in real-world settings. In  [19] , facial expressions of Fear and Disgust have been reported to be less well-recognized than those of other basic emotions and Fear has been reported to have a significantly lower recognition rate than the other classes. Despite the improvements brought along with augmentation and the attribute-aware and disentangled approaches, we observe in Table  2  similar results for RAF-DB.\n\nTo further investigate the performance of all six models, we present a number of challenging cases in Table  5 . We observe that many of these have ambiguities in terms of the expression category they have been labelled with. Essentially, learning the more complex or ambiguous facial expressions that may need to be analysed along/with multiple class-labels, remains an issue for all six models. This also relates to the fact that despite its wide usage, the theory of six-seven basic emotions is known to be problematic in its inability to explain the full range of facial expressions displayed in real-world settings  [15] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Experiments On Celeba Dataset",
      "text": "We follow a similar structure, and first present a bias analysis on CelebA Dataset and subsequently perform experiments using the three approaches we proposed in Section 3.  CelebA Bias Analysis CelebA dataset contains 40 types of attribute annotations, but for consistency between the two studies, here we focus only on the attributes \"Smiling, \"Male\" and \"Young\". Table  6  shows the test data breakdown along these three attribute labels. Compared to RAF-DB, CelebA is a much larger dataset. The target attribute \"Smiling is well balanced, while the sensitive attributes Gender and Age are not evenly distributed.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Expression (Smiling) Recognition",
      "text": "The task on this dataset is to train a binary classifier to distinguish the expression \"Smiling from \"Non-Smiling. The Baseline Approach, the Attribute-aware Approach, and the Disentangled Approach introduced in Section 3 are trained and tested on this task. Again, evaluation is performed with and without data augmentation and performance is reported in Table  7 . As this is a relatively simple task with sufficient samples, the accuracies of all six models do not show significant differences. In Table  8 , all of them provide comparable results for class-wise accuracy broken down by attribute labels. The fairness measures reported in Table  9  are also very close to one other.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Discussion",
      "text": "The results obtained for CelebA-DB could potentially be due to several reasons. Firstly, it is more than ten times larger than RAF-DB, thus the trained models do not benefit as much from data augmentation. Secondly, the bias mitigation approaches are more suitable in the context of an uneven attribute distribution or imbalanced number of data points for certain subgroups, which is not the case for CelebA-DB. Thirdly, the recognition task is a simple binary classification task, and therefore both accuracy and fairness results are already high with little to no potential for improvement. In light of these, presenting visual examples of failure cases does not prove meaningful and does not provide further insights. We did however manually inspect some of the results and observed that the binary labelling strategy may have introduced ambiguities.\n\nIn general, when labelling affective constructs, using a continuous dimensional approach (e.g., labelling Smiling using a Likert scale or continuous values in the range of [-1,+1]) is known to be more appropriate in capturing the full range of the expression displayed  [15] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "To date, there exist a large variety and number of datasets for facial expression recognition tasks  [27, 33] . However, virtually none of these datasets have been acquired with consideration of containing images and videos that are evenly distributed across the human population in terms of sensitive attributes such as gender, age and ethnicity. Therefore, in this paper, we first focused on quantifying how these attributes are distributed across facial expression datasets, and what effect this may have on the performance of the resulting classifiers trained on these datasets. Furthermore, in order to investigate whether bias is a problem for facial expression recognition, we conducted a comparative study using three different approaches, namely a baseline, an attribute-aware and a disentangled approach, under two conditions w/ and w/o data augmentation. As a proof of concept we conducted extensive experiments on two well-known datasets, RAF-DB and CelebA, that contain labels for the sensitive attributes of gender, age and/or race. The bias analysis undertaken for RAF-DB showed that the vast majority of the subjects are Caucasian and most are in the 20-39 years age category. The experimental results suggested that data augmentation improves the accuracy of the baseline model, but this alone is unable to mitigate the bias effect. Both the attribute-aware and the disentangled approach equipped with data augmentation perform better than the baseline approach in terms of accuracy and fairness, and the disentangled approach is the best for mitigating demographic bias. The experiments conducted on the CelebA-DB show that the models employed do not show significant difference in terms of neither accuracy nor fairness. Data augmentation does not contribute much as this is already a large dataset. We therefore conclude that bias mitigation strategies might be more suitable in the existence of uneven attribute distribution or imbalanced number of subgroup data, and in the context of more complex recognition tasks.\n\nWe note that the results obtained and the conclusions reached in all facial bias studies are both data and model-driven. Therefore, the study presented in this paper should be expanded by employing other relevant facial expression and affect datasets and machine learning models to fully determine the veracity of the findings. Utilising inter-relations between other attributes and gender, age and race, as has been done by  [49] , or employing generative counterfactual face attribute augmentation and investigating its impact on the classifier output, as undertaken in  [9] , might be also able to expose other more implicit types of bias encoded in a dataset. However, this requires the research community to invest effort in creating facial expression datasets with explicit labels regarding these attributes.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of the three approaches. (a) The Baseline Approach, (b)",
      "page": 4
    },
    {
      "caption": "Figure 1: (b)). Note that this approach",
      "page": 5
    },
    {
      "caption": "Figure 1: ). The network is trained using the Cross Entropy loss in Equation 1.",
      "page": 5
    },
    {
      "caption": "Figure 1: (c)). The diﬀerence between the training of the primary expression classiﬁcation",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Class-wise accuracy of the models by expression labels on RAF-DB.",
      "data": [
        {
          "Mean": "Surprise\nFear\nDisgust\nHappy\nSad\nAnger\nNeutral",
          "65.3%": "75.8%\n40.5%\n41.1%\n91.6%\n63.2%\n66.5%\n78.7%",
          "66.9%": "79.7%\n47.4%\n41.1%\n92.7%\n64.2%\n62.8%\n80.4%",
          "62.2%": "77.0%\n40.5%\n33.1%\n92.4%\n55.4%\n53.7%\n83.5%",
          "73.8%": "82.8%\n54.4%\n51.6%\n93.6%\n73.1%\n73.8%\n87.6%",
          "74.6%": "82.5%\n55.7%\n53.8%\n92.7%\n80.6%\n74.4%\n82.2%",
          "74.8%": "81.8%\n53.8%\n54.1%\n93.3%\n77.7%\n81.0%\n82.1%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: CelebA data distribution of the test set.",
      "data": [
        {
          "w/o D.A.": "w D.A.",
          "Baseline\nAttri.\nDisen.": "Baseline\nAttri.\nDisen.",
          "Sad": "Happy",
          "Anger\nFear Anger\nHappy Disgust": "Fear Anger Disgust Neutral Disgust Happy\nFear Anger Disgust\nDisgust",
          "Anger Neutral Neutral\nNeutral Happy\nAnger Neutral Surprise Happy Surprise": "Sad\nSad",
          "Surprise\nHappy Surprise": "Fear\nFear\nAnger Surprise",
          "Sad\nSad\nSad": "Neutral\nSad\nSad"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings",
      "authors": [
        "M Alvi",
        "A Zisserman",
        "C Nellåker"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "2",
      "title": "Uncovering and mitigating algorithmic bias through learned latent structure",
      "authors": [
        "A Amini",
        "A Soleimany",
        "W Schwarting",
        "S Bhatia",
        "D Rus"
      ],
      "year": "2019",
      "venue": "AAAI/ACM Conference on AI"
    },
    {
      "citation_id": "3",
      "title": "Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias",
      "authors": [
        "R Bellamy",
        "K Dey",
        "M Hind",
        "S Hoffman",
        "S Houde",
        "K Kannan",
        "P Lohia",
        "J Martino",
        "S Mehta",
        "A Mojsilovic"
      ],
      "year": "2018",
      "venue": "Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias",
      "arxiv": "arXiv:1810.01943"
    },
    {
      "citation_id": "4",
      "title": "Gender and Human-Computer Interaction",
      "authors": [
        "S Breslin",
        "B Wadhwa"
      ],
      "year": "2017",
      "venue": "Gender and Human-Computer Interaction",
      "doi": "10.1002/9781118976005.ch4"
    },
    {
      "citation_id": "5",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "6",
      "title": "From apparent to real age: gender, age, ethnic, makeup, and expression bias analysis in real age estimation",
      "authors": [
        "A Clapes",
        "O Bilici",
        "D Temirova",
        "E Avots",
        "G Anbarjafari",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "7",
      "title": "White paper on artificial intelligence-a european approach to excellence and trust",
      "authors": [
        "E Commission"
      ],
      "year": "2020",
      "venue": "White paper on artificial intelligence-a european approach to excellence and trust"
    },
    {
      "citation_id": "8",
      "title": "Mitigating bias in gender, age and ethnicity classification: a multi-task convolution neural network approach",
      "authors": [
        "A Das",
        "A Dantcheva",
        "F Bremond"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "9",
      "title": "Detecting bias with generative counterfactual face attribute augmentation",
      "authors": [
        "E Denton",
        "B Hutchinson",
        "M Mitchell",
        "T Gebru"
      ],
      "year": "2019",
      "venue": "Detecting bias with generative counterfactual face attribute augmentation"
    },
    {
      "citation_id": "10",
      "title": "Demographic bias in biometrics: A survey on an emerging challenge",
      "authors": [
        "P Drozdowski",
        "C Rathgeb",
        "A Dantcheva",
        "N Damer",
        "C Busch"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Technology and Society"
    },
    {
      "citation_id": "11",
      "title": "Fairness through awareness",
      "authors": [
        "C Dwork",
        "M Hardt",
        "T Pitassi",
        "O Reingold",
        "R Zemel"
      ],
      "year": "2012",
      "venue": "Proceedings of the 3rd innovations in theoretical computer science conference"
    },
    {
      "citation_id": "12",
      "title": "Investigating bias in deep face analysis: The kanface dataset and empirical study. Image and Vision Computing",
      "authors": [
        "M Georgopoulos",
        "Y Panagakis",
        "M Pantic"
      ],
      "year": "2020",
      "venue": "Investigating bias in deep face analysis: The kanface dataset and empirical study. Image and Vision Computing"
    },
    {
      "citation_id": "13",
      "title": "Mitigating face recognition bias via group adaptive classifier",
      "authors": [
        "S Gong",
        "X Liu",
        "A Jain"
      ],
      "year": "2020",
      "venue": "Mitigating face recognition bias via group adaptive classifier",
      "arxiv": "arXiv:2006.07576"
    },
    {
      "citation_id": "14",
      "title": "Ongoing face recognition vendor test (frvt) part 3: Demographic effects",
      "authors": [
        "P Grother",
        "M Ngan",
        "K Hanaoka"
      ],
      "year": "2019",
      "venue": "NISTIR"
    },
    {
      "citation_id": "15",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "16",
      "title": "Human age estimation: What is the influence across race and gender?",
      "authors": [
        "G Guo",
        "G Mu"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops"
    },
    {
      "citation_id": "17",
      "title": "Equality of opportunity in supervised learning",
      "authors": [
        "M Hardt",
        "E Price",
        "N Srebro"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "T Xu"
      ],
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems",
      "authors": [
        "A Howard",
        "C Zhang",
        "E Horvitz"
      ],
      "year": "2017",
      "venue": "IEEE Workshop on Advanced Robotics and its Social Impacts"
    },
    {
      "citation_id": "21",
      "title": "Dealing with bias via data augmentation in supervised learning scenarios",
      "authors": [
        "V Iosifidis",
        "E Ntoutsi"
      ],
      "year": "2018",
      "venue": "Jo Bates Paul D. Clough Robert Jäschke"
    },
    {
      "citation_id": "22",
      "title": "Data preprocessing techniques for classification without discrimination",
      "authors": [
        "F Kamiran",
        "T Calders"
      ],
      "year": "2012",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "23",
      "title": "Face verification subject to varying (age, ethnicity, and gender) demographics using deep learning",
      "authors": [
        "H Khiyari",
        "H Wechsler"
      ],
      "year": "2016",
      "venue": "Journal of Biometrics & Biostatistics",
      "doi": "10.4172/2155-6180.1000323"
    },
    {
      "citation_id": "24",
      "title": "Ethnic bias in the recognition of facial expressions",
      "authors": [
        "J Kilbride",
        "M Yarczower"
      ],
      "year": "1983",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "26",
      "title": "Racial disparities in automated speech recognition",
      "authors": [
        "A Koenecke",
        "A Nam",
        "E Lake",
        "J Nudell",
        "M Quartey",
        "Z Mengesha",
        "C Toups",
        "J Rickford",
        "D Jurafsky",
        "S Goel"
      ],
      "year": "2020",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "27",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C Kuo",
        "S Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "28",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Exploring disentangled feature representation beyond face identification",
      "authors": [
        "Y Liu",
        "F Wei",
        "J Shao",
        "L Sheng",
        "J Yan",
        "X Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "32",
      "title": "On the fairness of disentangled representations",
      "authors": [
        "F Locatello",
        "G Abbati",
        "T Rainforth",
        "S Bauer",
        "B Schölkopf",
        "O Bachem"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Gender bias in neural natural language processing",
      "authors": [
        "K Lu",
        "P Mardziel",
        "F Wu",
        "P Amancharla",
        "A Datta"
      ],
      "year": "2018",
      "venue": "Gender bias in neural natural language processing"
    },
    {
      "citation_id": "34",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar"
      ],
      "year": "2019",
      "venue": "IEEE Tran. on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Bias in, bias out",
      "authors": [
        "S Mayson"
      ],
      "year": "2018",
      "venue": "Bias in, bias out"
    },
    {
      "citation_id": "36",
      "title": "Sensitivenets: Learning agnostic representations with application to face recognition",
      "authors": [
        "A Morales",
        "J Fierrez",
        "R Vera-Rodriguez"
      ],
      "year": "2019",
      "venue": "Sensitivenets: Learning agnostic representations with application to face recognition",
      "arxiv": "arXiv:1902.00334"
    },
    {
      "citation_id": "37",
      "title": "Bias remediation in driver drowsiness detection systems using generative adversarial networks",
      "authors": [
        "M Ngxande",
        "J Tapamo",
        "M Burke"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "39",
      "title": "Face recognition vendor test",
      "authors": [
        "P Phillips",
        "P Grother",
        "R Micheals",
        "D Blackburn",
        "E Tabassi",
        "M Bone"
      ],
      "year": "2002",
      "venue": "IEEE International SOI Conference. Proceedings (Cat. No. 03CH37443"
    },
    {
      "citation_id": "40",
      "title": "Optimized pre-processing for discrimination prevention",
      "authors": [
        "F Du Pin Calmon",
        "D Wei",
        "B Vinzamuri",
        "K Ramamurthy",
        "K Varshney"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "41",
      "title": "Racial influence on automated perceptions of emotions",
      "authors": [
        "L Rhue"
      ],
      "year": "2018",
      "venue": "Racial influence on automated perceptions of emotions",
      "doi": "10.2139/ssrn.3281765"
    },
    {
      "citation_id": "42",
      "title": "Face recognition: too bias, or not too bias?",
      "authors": [
        "J Robinson",
        "G Livitz",
        "Y Henon",
        "C Qin",
        "Y Fu",
        "S Timoner"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "43",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2014.2366127"
    },
    {
      "citation_id": "44",
      "title": "Face image-based age and gender estimation with consideration of ethnic difference",
      "authors": [
        "M Shin",
        "J Seo",
        "D Kwon"
      ],
      "year": "2017",
      "venue": "2017 26th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "45",
      "title": "Face quality estimation and its correlation to demographic and non-demographic bias in face recognition",
      "authors": [
        "P Terhörst",
        "J Kolf",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2020",
      "venue": "Face quality estimation and its correlation to demographic and non-demographic bias in face recognition",
      "arxiv": "arXiv:2004.01019"
    },
    {
      "citation_id": "46",
      "title": "Fairness definitions explained",
      "authors": [
        "S Verma",
        "J Rubin"
      ],
      "year": "2018",
      "venue": "IEEE/ACM International Workshop on Software Fairness (FairWare)"
    },
    {
      "citation_id": "47",
      "title": "Mitigate bias in face recognition using skewness-aware reinforcement learning",
      "authors": [
        "M Wang",
        "W Deng"
      ],
      "year": "2019",
      "venue": "Mitigate bias in face recognition using skewness-aware reinforcement learning"
    },
    {
      "citation_id": "48",
      "title": "Racial faces in the wild: Reducing racial bias by information maximization adaptation network",
      "authors": [
        "M Wang",
        "W Deng",
        "J Hu",
        "X Tao",
        "Y Huang"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision, ICCV 2019"
    },
    {
      "citation_id": "49",
      "title": "",
      "authors": [
        "Ieee"
      ],
      "year": "2019",
      "venue": "",
      "doi": "10.1109/ICCV.2019.00078"
    },
    {
      "citation_id": "50",
      "title": "Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations",
      "authors": [
        "T Wang",
        "J Zhao",
        "M Yatskar",
        "K Chang",
        "V Ordonez"
      ],
      "year": "2019",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "51",
      "title": "Towards fairness in visual recognition: Effective strategies for bias mitigation",
      "authors": [
        "Z Wang",
        "K Qinami",
        "I Karakozis",
        "K Genova",
        "P Nair",
        "K Hata",
        "O Russakovsky"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Gender classification and bias mitigation in facial images",
      "authors": [
        "W Wu",
        "P Michalatos",
        "P Protopapaps",
        "Z Yang"
      ],
      "year": "2020",
      "venue": "12th ACM Conference on Web Science"
    },
    {
      "citation_id": "53",
      "title": "Mitigating unwanted biases with adversarial learning",
      "authors": [
        "B Zhang",
        "B Lemoine",
        "M Mitchell"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society"
    }
  ]
}