{
  "paper_id": "2203.06439v1",
  "title": "An End-User Coding-Based Environment For Programming An Educational Affective Robot",
  "published": "2022-03-12T14:10:42Z",
  "authors": [
    "Cristina Gena",
    "Claudio Mattutino",
    "Enrico Mosca",
    "Alberto Lillo"
  ],
  "keywords": [
    "educational robotics",
    "visual coding",
    "social robots"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper we present an open source educational robot, designed both to engage children in an affective and social interaction, and to be programmable also in its social and affective behaviour. Indeed the robot, in addition to classic programming tasks, can also be programmed as a social robot. In addition to movements, the user can make the robot express emotions and make it say things. The robot can also be left in autonomous mode and greeting the user, recognizing biometric user's features and emotion, etc.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Designing and developing methods that allow a robot to perform certain specific tasks, as similar as possible to the human ones, brings productive and practical advantages. Robots work at constant high speeds without the need to stop the process, reducing costs, waste and risks within a company. When it comes to robotics it is easy to associate it to the industrial context, being one of the first sectors that invested and recognized the great potential in reproducing human work, but robotics is so much more. Being an interdisciplinary science, it finds numerous applications in various contexts, from biomedical, to military, from industry to space. In such artificial and sophisticated robots, a psychological and educational approach may bring several benefits to teachers and children in particular. This is what happens in the educational robotics field  [1] ,  [2] , where man and machine work together, accompanying children towards an engaging, creative, and effective teaching approach. The results achieved by this phenomenon are significant, widespread in primary and secondary schools but in clear growth everywhere. The educational robotics approach is simple and practical and accompanies children, which have a natural predisposition to discover, explore and experiment, in learning through play. Together with educational robotics we find coding, the visual programming approach, typically proposed to children  [3] ), which stimulates mental processes allowing the children to solve problems of various kinds. Sharing the same educational robotics principles, coding allows children to try their hand at new activities such as programming or deepen the basic concepts of other subjects such as science and mathematics. Both activities favor the introduction to a mental process, union of human thought and computer programming, which translates into the ability to tackle problem solving in an algorithmic way: Computational Thinking  [4] , an expression increasingly used and applied to any situation, which allows to plan strategies and solutions to possible obstacles. In short, a real skill that should accompany students from the beginning of their school career. Computational thinking therefore deserves to be introduced and cultivated since primary school, not only to bring children closer to a conscious use of technology, but also to develop different levels of abstraction that allow them to deepen logical aspects and deeper structures in any kind of situation. Design, planning, teamwork, reasoning, in-depth analysis, creativity, imagination are some of the ambitions of this goal: knowing how to think like a machine, or rather knowing how to think like who the machine has programmed it.\n\nIn this paper we present an open source educational robot, designed both to engage children in an affective and social interaction, and to be programmable by the children also in its social and affective behavior. The paper is organized as follows: Section ?? presents a brief overview of the affective interaction, Section 3 presents the robot and its design, Section 4 describes the robot's hardware and software architecture, and Section 5 concludes the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "During the 1990s, a wave of new research on the role of emotions in different areas such as psychology, neurology and sociology ascertained the vital role of emotions in cognitive processes, previously believed to be an interfer-ence with rational thinking  [5] . Above all, such research has challenged the old Cartesian dualistic division that emotional experiences reside only in our mind. Instead, emotions are experienced by our entire body, starting with hormonal changes and nerve signals to tensing or relaxing muscles and facial expressions. Not only that, emotions are also built through the interaction between people, or between people and machines. In the mid-1990s, Picard coined the term Affective Computing  [6] , a specific branch of artificial intelligence that aims to create machines capable of recognizing and expressing emotions. A computer device with the ability to detect and appropriately respond to the user's emotions and other stimuli could collect clues from a variety of sources. Facial expressions, posture, gestures, speech, strength or rhythm of keystrokes, and changes in temperature of the hand on a mouse can mean changes in the user's emotional state, and these can be detected and interpreted by a computer. via sensors, microphones, cameras and / or software logic. This applicative dimension of emotional expression has been an integral part of our project aimed at realizing an educational, social and affective robot, called Wolly, which we will introduce in the next section.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Wolly Robot",
      "text": "At the end of 2017, in our HCI lab we carried out a codesign activity with children aimed at devising an educational robot called Wolly  [7] . The main objective of the robot we have devised is acting as an affective peer for children: hence, it has to be able to execute a standard set of commands, compatible with those used in coding, but also to interact both verbally and affectively with students. Currently, Wolly can be controlled by means of a standard visual block environment, Blockly 1 , see Fig.  3 , which is well known to many children with some experience in coding. However, we also have a simpler set of instructions, see Fig.  2 , we tested with children (as described in  [8] ), so that younger children can use basic commands to control its behavior.\n\nWe are now focusing on its role as an educational and affective robot capable of being controlled by coding instructions and at the same time interacting verbally and affectively with children by providing a mirror of emotions: by this last term we mean the the fact that the robot exhibits an affective mirroring behavior, that is, once it recognizes a certain basic emotion in the child it will replicate it by empathy. At the moment Wolly recognizes and expresses mirror emotions through its facial expressions, showing in this way a kind of emotional intelligence, which can be defined as as the capacity to perceive and understand both one's own and other's emo-1 https://developers.google.com/blockly/ tions, see  [9] . As far as emotion recognition is considered, we have trained a deep neural network written in Python with the use of Pytorch library 2  and the Emotic dataset 3  , also known as EMOTions In Context, which does not only consider the user's face but also the surrounding context to detect the correct emotion. The robot is also able to recognize biometric features thanks to the use of Face Recognition library by Adam Geitgey 4 . The robot can indeed be left in autonomous mode, in which at the moment it is able to carry out both biometric user's features and emotion recognition, and greeting the user. Regarding this last point, Wolly uses Google Cloud Textto-Speech to synthesize natural-sounding speech that has been used to greet a person when his face is detected. We are now also working on speech recognition and generation thanks to the integration of Google Cloud Text-to-Speech 5 and Google Speech to Text API 6 . We would also integrate in the near future the dialogue module described in Gena et al.  [10]  that uses algorithms for recognizing groups of synonyms, which can be easily customized by the user and also allows easy management of the user / robot dialogue in a way similar to Aldebaran's QiChat 7 . In addition to classic programming tasks, the robot may in the future be programmed as a social robot, see Fig.  3 . In addition to movements, the user may make the robot express emotions and make it say things, programming in this way its dialogue and its social and affective behavior.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hardware And Software Architecture",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hardware Architecture",
      "text": "Currently we have developed a second release of the robot (as far as the first release is concerned see  [7] ). We had the requirement of transforming Wolly into a low cost robot that everyone could build regardless of its electronic and technical skills. The idea is to convert the old Arduino-based robot architecture (see details in  [11] ) in a all-in-one device managed by a Raspberry Pi, with a sort of WollyOS (Wolly Operating System) easy to download and update, also for non-technical users, as teachers. In more details, the list of the main hardware",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Software Architecture",
      "text": "As far as the software components of the second release are concerned, Wolly is now controlled both from a desktop web-based interface and from a smartphone/tablet application. The responsive web-based application is developed in HTML/CSS and the commands for the robot are sent and read in Python. The mobile application has been developed in Flutter 8 and presents a webview showing the web pages through which the user can take control of Wolly both in iOS and Android. More in detail, we are using the following software components (see also Figure  1 ).\n\nAppwrite 9 . Appwrite is an end-to-end back-end server that is aimed to abstract the complexity of common, complex, and repetitive tasks required for building a modern app. Appwrite provides the developer with a set of APIs, tools, and a management console User Interface (UI) to help developers build apps a lot faster and in a much  The communication between Wolly and the other components takes place through the Appwrite database. Every action is written on the database and the robot is always listening for changes (commands). Whenever they occur Wolly will execute them, and when the set of commands will be finished, the database will be reset and ready for receiving a new set of commands.\n\nA basic set of operations we have defined are as follows: (i) The user connects to the website via the mobile or desktop app; (ii) To access the web site she has sign up/ sign in, and all the credentials are stored, managed and checked via Appwrite. At the moment Appwrite is currently running onto a server located in our department; (iii) When the login succeeds, the credentials are retrieved via Appwrite, which then returns an object containing the user-id, which will be used as identifier to store the commands given to the robot into the database; (iv) After the login phase the user may insert a sequence of commands (custom blocks) using the Google Blocklybased interface. When the play button is pressed, all the visual commands are translated in Python, processed and inserted into a list sent to Appwrite. In the near future we will also provide the (more advanced) user with a Python editor; (v) The robot reads the first element of the list and executes the action. When the action is done, the first element is dropped from the list and the cycle re-start until the list is empty.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Blockly Developer Tools",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Blockly",
      "text": "Blockly. This library adds an editor representing the coding concepts as interlocking graphical blocks to represent code concepts like variables, logical expressions, loops and, thanks to the customized blocks, to move the robot. As outputs, it returns syntactically correct Python code that is eventually sent to the database.\n\nBlockly is an open-source library, so it's very easy to create and customize your own block. Via the 10 it is possible to design a special block from scratch by choosing the colors, the shape, how it will be translated and in which language, in our case we have chosen Python.\n\nThere are 6 custom designed blocks:\n\nThe expression block has a dropdown menu for choosing one of the eleven different available expressions the robot is able to express. .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In the near future we want to make the robot: be able to carry out a basic vocal interaction through dialogues linked to both the context of the coding exercises and also to its general knowledge about the world and about itself, according to a communication protocol that simulates a natural dialogue.\n\nWe are also focusing on the UX simplification. We believe that it is not only important that the user can program the Wolly Raspberry robot with a simple interface, as Google Blockly, but also to easily download its app, and its update, and save it on a micro-SD card (e.g. from the website learn.wolly.di.unito.it). We will also perform an evaluation in the wild, by bringing the robot at school and trying some educational robotics exercises, as well as some end-user programming tasks, such as having the students programming some basic dialogues and interaction withe robots.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: , which is well known to many children with some",
      "page": 2
    },
    {
      "caption": "Figure 2: , we tested with children (as",
      "page": 2
    },
    {
      "caption": "Figure 3: In addition to movements, the user may make the robot",
      "page": 2
    },
    {
      "caption": "Figure 1: Software architecture",
      "page": 3
    },
    {
      "caption": "Figure 2: The robot can be moved with the 4 arrows in the",
      "page": 3
    },
    {
      "caption": "Figure 3: The Blockly Page",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Special issue on educational robotics, Robotics and Autonomous Systems",
      "authors": [
        "D Alimisis",
        "M Moro"
      ],
      "year": "2016",
      "venue": "Special issue on educational robotics, Robotics and Autonomous Systems",
      "doi": "10.1016/j.robot.2015.12.006"
    },
    {
      "citation_id": "2",
      "title": "Exploring the educational potential of robotics in schools: A systematic review",
      "authors": [
        "F Benitti"
      ],
      "year": "2012",
      "venue": "Computers Education",
      "doi": "10.1016/j.compedu.2011.10.006"
    },
    {
      "citation_id": "3",
      "title": "Behavior constrction kits",
      "authors": [
        "M Resnick"
      ],
      "year": "1993",
      "venue": "Commun. ACM",
      "doi": "10.1145/159544.159593"
    },
    {
      "citation_id": "4",
      "title": "Computational thinking, Commun",
      "authors": [
        "J Wing"
      ],
      "year": "2006",
      "venue": "ACM",
      "doi": "10.1145/1118178.1118215"
    },
    {
      "citation_id": "5",
      "title": "Appraisal theories of emotion: State of the art and future development",
      "authors": [
        "A Moors",
        "P Ellsworth",
        "K Scherer",
        "N Frijda"
      ],
      "year": "2013",
      "venue": "Emotion Review",
      "doi": "10.1177/1754073912468165"
    },
    {
      "citation_id": "6",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Vaudano, Co-designing with kids an educational robot",
      "authors": [
        "V Cietto",
        "C Gena",
        "I Lombardi",
        "C Mattutino"
      ],
      "year": "2018",
      "venue": "2018 IEEE Workshop on Advanced Robotics and its Social Impacts",
      "doi": "10.1109/ARSO.2018.8625810"
    },
    {
      "citation_id": "8",
      "title": "End-user development for the wolly robot",
      "authors": [
        "G Bova",
        "D Cellie",
        "C Gioia",
        "F Vernero",
        "C Mattutino",
        "C Gena"
      ],
      "year": "2019",
      "venue": "End-User Development -7th International Symposium",
      "doi": "10.1007/978-3-030-24781-2\\_19"
    },
    {
      "citation_id": "9",
      "title": "Emotional intelligence, Imagination",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Cognition and Personality",
      "doi": "10.2190/DUGG-P24E-52WK-6CDG"
    },
    {
      "citation_id": "10",
      "title": "Nao_prm: an interactive and affective simulator of the nao robot, in: 30th IEEE International Conference on Robot & Human Interactive Communication, RO-MAN 2021",
      "authors": [
        "C Gena",
        "C Mattutino",
        "W Maltese",
        "G Piazza",
        "E Rizzello"
      ],
      "year": "2021",
      "venue": "Nao_prm: an interactive and affective simulator of the nao robot, in: 30th IEEE International Conference on Robot & Human Interactive Communication, RO-MAN 2021",
      "doi": "10.1109/RO-MAN50785.2021.9515410"
    },
    {
      "citation_id": "11",
      "title": "Design and development of a social, educational and affective robot",
      "authors": [
        "C Gena",
        "C Mattutino",
        "G Perosino",
        "M Trainito",
        "C Vaudano",
        "D Cellie"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems, EAIS 2020",
      "doi": "10.1109/EAIS48028.2020.9122778"
    }
  ]
}