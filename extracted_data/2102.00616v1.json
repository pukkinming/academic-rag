{
  "paper_id": "2102.00616v1",
  "title": "Neural Network Architectures To Classify Emotions In Indian Classical Music",
  "published": "2021-02-01T03:41:25Z",
  "authors": [
    "Uddalok Sarkar",
    "Sayan Nag",
    "Medha Basu",
    "Archi Banerjee",
    "Shankha Sanyal",
    "Ranjan Sengupta",
    "Dipak Ghosh"
  ],
  "keywords": [
    "Indian Classical Music",
    "Emotions",
    "Classification",
    "CNN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music is often considered as the language of emotions. It has long been known to elicit emotions in human being and thus categorizing music based on the type of emotions they induce in human being is a very intriguing topic of research. When the task comes to classify emotions elicited by Indian Classical Music (ICM), it becomes much more challenging because of the inherent ambiguity associated with ICM. The fact that a single musical performance can evoke a variety of emotional response in the audience is implicit to the nature of ICM renditions. With the rapid advancements in the field of Deep Learning, this Music Emotion Recognition (MER) task is becoming more and more relevant and robust, hence can be applied to one of the most challenging test case i.e. classifying emotions elicited from ICM. In this paper we present a new dataset called JUMusEmoDB which presently has 400 audio clips (30 seconds each) where 200 clips correspond to happy emotions and the remaining 200 clips correspond to sad emotion. The initial annotations and emotional classification of the database has been done based on an emotional rating test (5-point Likert scale) performed by 100 participants. The clips have been taken from different conventional 'raga' renditions played in sitar by an eminent maestro of ICM and digitized in 44.1 kHz frequency. The ragas, which are unique to ICM, are described as musical structures capable of inducing different moods or emotions. For supervised classification purposes, we have used 4 existing deep Convolutional Neural Network (CNN) based architectures (resnet18, mobilenet v2.0, squeezenet v1.0 and vgg16) on corresponding music spectrograms of the 2000 sub-clips (where every clip was segmented into 5 sub-clips of about 5 seconds each) which contain both time as well as frequency domain information. The initial results are quite inspiring, and we look forward to setting the baseline values for the dataset using this architecture. This type of CNN based classification algorithm using a rich corpus of Indian Classical Music is unique even in the global perspective and can be replicated in other modalities of music also. This dataset is still under development and we plan to include more data containing other emotional features as well. We plan to make the dataset publicly available soon.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music imposes emotion. Considering the very basic, major scale always creates a happier ambience whereas minor scale creates a bit sad. This response to melody and rhythm is a biological instinct. A non-musician's ear can feel the pain when Garry Moore plays \"Loner\", even a baby responds to different scales and melody lines. While talking about emotions elicited by Indian Classical Music (ICM) it always becomes a matter of huge challenge for its ambiguous emotional response. Studies involving non-linear techniques have been conducted in the recent past to understand this complex behavior of music and its manifestation in the human brain  [13 -17] .\n\nIn the recent years, Machine Learning has made significant advancements in a multitude of fields including computer vision, medical imaging, natural language processing and so on  [18 -37] . Such Machine Learning and Deep Learning approaches have been used to identify different emotions associated with music  [9 -12] . So, Music Emotion Recognition (MER) has always been an interesting task to perform for observing the correlation between the music and perceived emotion. Music Emotion Recognition task was first introduced by Barthet et al  [5] . Since then, several developments have been achieved on this discipline, and hence it has become very important to observe the role of ICM in elicitation of emotion.\n\nTo exploit the significance of emotion induction in ICM, we need a proper database to work with. Previously datasets like Computer Audition Lab 500-song (CAL500)  [6] , CAL500exp  [7] , datasets have been introduced, which is enriched with 500 western music clips. Here in this paper we have introduced a database JUMusEmoDB enriched with 400 music clips from genre of Indian Classical Music, of which 200 clips fall into the category of happy emotion while 200 falls into sad. Each clip is of 30 seconds length which is long enough for introducing an emotional imposition  [8] . All the clips are parts of different 'raga' renditions improvised in Sitar by an eminent maestro of ICM. Each Raga in ICM evokes not a particular emotion (rasa), but a superposition of different emotional states such as joy, sadness, anger, disgust, fear etc. To decipher which particular emotion is conveyed in the chosen 30 sec segment of the raga, an emotion annotation was performed initially by 100 participants based on 5-point Likert scale.\n\nFor an emotion classification task different acoustic features of music are very important. Different acoustic feature consists of (a) Rhythmic Features: Tempo, Silence etc.; (b) Timbral Features: MFCC, Average Energy, Spectral Centroid, Spectral Tilt etc.; (c) Chroma Features. These acoustic features quantify the musicality of a clip which in effect contributes to MER. Study on this knowledge driven approaches results in an efficient model structure; but before this, a validated dataset is very necessary to work with. After the emotion annotation, a data driven approach has been used to validate our dataset.\n\nWe have taken an image processing-oriented approach to classify the dataset into emotion tags. We primarily extracted the spectrogram of a clip and fed the processed spectrogram image into existing deep Convolutional Neural Network (CNN) based architectures. These CNNs were then trained to classify emotions. For this study, we have made use of four different existing CNN architectures: VGG16, ResNet18, MobileNet v2.0, SqueezeNet v1.0 and have received some promising results. Furthermore, the dataset used in our study which we have named as JUMusEmoDB, is a novel dataset comprising of clips from Indian Classical Music genre. This dataset currently has musical clips from two emotions, namely happy and sad, and is still under development. We plan to include more data containing other emotional features eventually making the dataset publicly available shortly. This dataset can be used in future by the scientific community for emotion classification purposes, to investigate the impact of Indian Classical Music on human brain and finally to conduct cross-cultural studies combining both Western Classical and Indian Classical musical clips.\n\nThe paper has been organized as follows: Section 2 contains Data Acquisition, Section 3 comprises of the Methods used, the Experiments and Results of the study have been mentioned in Section 4 and finally Section 5 has the conclusion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Acquisition:",
      "text": "JUMusEmoDB consists of 400 audio clips of 30 second each. The clips have been taken from different conventional raga renditions played in sitar by an eminent maestro of ICM and recorded with a sample frequency of 44.1 kHz. 200 clips correspond to happy emotions and the remaining 200 clips correspond to sad emotion. Initial annotations and emotional classification of the database has been done based on an emotional rating test (5-point Likert scale) performed by 100 participants.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods:",
      "text": "We have labeled our music database JUMusEmoDB into two main classes as stated earlier, i.e., happy, and sad. In this paper we have followed the approach of a data driven method to classify the database into two distinct classes. Now a general question arises regarding why a data driven approach is being followed primarily rather than any novel knowledge-based quantification. To answer this, we have to take into account that a successful classification with high accuracy rate will validate the authenticity of JUMusEmoDB, which can then be used to develop new knowledge-based models to test on this database.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing",
      "text": "The basic input to our CNN based framework should be an image data. Hence, to map our audio database into image paradigm we have made use of Spectrogram. But before obtaining spectrogram for these 30-second-long audio clips, we have sliced all the 30 second clips into individual 5 second clips to augment our dataset and performed STFT on this augmented dataset to obtain spectrograms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectrogram",
      "text": "In our proposed framework, we have performed our classification task on the mel-spectrograms of derived music tracks. To extract the melspectrogram we have made use of short time Fourier transform with a window size of 2048 and hop size of 512 to obtain a spectrogram (Fig.  1 ). Thus we obtained melspectrogram as a dot product of obtained spectrogram with mel filterbanks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "ùë†ùëùùëíùëêùë°(ùë°",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn Models",
      "text": "We have extended our framework into four different established ConvNet models, i.e., VGG  [1] , ResNet  [2] , SqueezeNet  [3] , MobileNet v2  [4] . We thus have obtained four accuracy rates with an average of 99.117%. We have added an extra layer of 2 channels at each end of considered model because of bi-class output of our framework.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vgg",
      "text": "Net is the oldest architecture among the used ConvNet models, proposed by Karen Simonyan and Andrew Zisserman of Oxford Robotics Institute in 2014  [1] . This has sixteen weight layers; thirteen convolution layers divided into five groups, each group followed by pooling layers, and three fully connected (FC) layers at the end of whole network (Fig.  2 ). Convolution layers have a receptive field of 3x3 throughout the whole net, with stride 1. The Maxpooling layers consist of receptive fields of size 2x2 each and with a stride of 2. The network ends with three fully connected layers with first two layers of 4096 channels and last layer of 1000 channels due to 1000 classes of ILSVRC (ImageNet Large-Scale Visual Recognition Challenge).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Resnets Are Residual Learning",
      "text": "framework with substantially deeper network but with lower complexity. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun has proposed a residual network with a depth of up to 152 layers (Fig.  3 4 ) i.e., 8x deeper than VGG-19 nets  [2] . Increasing depth of a network can lead to a very serious problem of vanishing gradient which results in saturation of convergence with a very high training error and low accuracy problems. Kaiming He et al. has beautifully taken care of these facts and modified a very deep network to gain a high accuracy with low training error. They have implemented a \"short-cut connection\" of identity mapping. Their approach was to allow the network to fit the stacked layers to a residual mapping using residual block (Fig.  3 ) instead of fitting them directly to the underlying mapping. So, instead of feeding ùêª(ùë•) (desired underlying mapping) let the stacked network fit, ùêπ(ùë•) ‚à∂= ùêª(ùë•) -ùë• and thus ultimately givesùêª(ùë•) ‚à∂= ùêπ(ùë•) + ùë•.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Squeezenet Is A Lighter Modification Of Deep Convolutional Neural Networks Which Has Achieved An",
      "text": "accuracy near to AlexNet (on ImageNet dataset) with 50 times fewer parameters (Fig.  6 )  [3] . The main concept behind this architecture is introduction of 'fire module\". A fire module is a stacking of a squeeze layer with 1x1 convolution filters and an expand layer which has both 1x1 and 3x3 filters. Number of kernels in squeeze layer should be less than number of kernels in expand layer to limit the number of input channels to 3x3 kernels. Fig.  5  shows an architectural view of fire module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Extraction From Network Output",
      "text": "Output from the fully connected layers is provided to softmax to extract out the classes. In our framework, we are concerned about two distinct classes (i.e., Happy Music clip, Sad Music clip). All the four architectures contain 1000 channels in last fully connected networks by default 1000 classes of ILSVRC (ImageNet Large-Scale Visual Recognition Challenge). So, we added another fully connected layer of 2 channels at the end to cater our purpose. After this last fully connected layer a softmax is performed to extract out the emotion classes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "As a classification task, some criteria are being used to quantify the classification performance. MSE (mean squared error), MAE (mean absolute error) has been used a lot as loss functions for image purposes. In our classification task we have made use of cross entropy loss. Entropy is a measure of uncertainty, and it is measured as,  where ùë¶ ùëñ is the calculated or predicted output and ùë¶ ùëñ ÃÇ is the actual output. Previously we have introduced our used framework and model improvisation. In this section we are illustrating the acquired results. For our work, we used a train/validation split ratio as 85/15. In the training phase we receive a model cost (calculated from loss function) which indicates the model performance. The loss curve for training phase for each model is shown in the Fig.  8 . With time (iterations) convergence is achieved for each of the models used and the performance of SquuezeNetV1 in the training phase also outperforms the other three models used.\n\nThe convergence plot helps to tune the parameters of CNN models. After adjusting the parameters, we obtain the best results for each CNN Models.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "1: Accuracies Of Different Cnn Models On The Validation Dataset",
      "text": "From Table  1 , we can see that SqueezeNetV1 seems to fit best with the acquired dataset. The dataset size (as of now) is not that huge which makes it suitable for light weight models (with fewer parameters) like SqueezeNetV1 which gives the best validation accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed a novel dataset called JUMusEmoDB which presently has 400 audio clips (30 seconds each) where 200 clips correspond to happy emotions and the remaining 200 clips correspond to sad emotion. The initial annotations and emotional classification of the database has been done based on an emotional rating test (5-point Likert scale) performed by 100 participants. We also demonstrated the performances of four deep CNN based architectures namely resnet18, mobilenet v2.0, squeezenet v1.0 and vgg16. Validation accuracy values showed that SqueezeNetV1 performed the best out of the four models. Even though the advantage of employing CNN based architectures for tackling the problem of music emotion recognition include better overall accuracy because of better extraction of useful features from the data compared to other traditional methods, further studies need to be conducted to understand the source of emotions for a given music. Another limitation of this work is the lack of data in our dataset, but this is a pilot study it is still under development. We plan to incorporate more data containing other emotional features as well and eventually make the dataset publicly available shortly.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: -4) i.e., 8x deeper than",
      "page": 4
    },
    {
      "caption": "Figure 6: ) [3]. The main concept behind",
      "page": 4
    },
    {
      "caption": "Figure 5: shows an architectural view of fire module.",
      "page": 5
    },
    {
      "caption": "Figure 7: First layer contains 1x1 kernel with RELU6. Second layer performs the depth-wise",
      "page": 5
    },
    {
      "caption": "Figure 8: With time (iterations) convergence is achieved for each of the models used",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neural Network architectures to classify emotions in": "Indian Classical Music"
        },
        {
          "Neural Network architectures to classify emotions in": "Uddalok Sarkara*, Sayan Nagb, Medha Basua, Archi Banerjeea,c,"
        },
        {
          "Neural Network architectures to classify emotions in": "Shankha Sanyala,d, Ranjan Senguptaa, Dipak Ghosha"
        },
        {
          "Neural Network architectures to classify emotions in": "aSir C.V. Raman Centre for Physics and Music, Jadavpur University, India"
        },
        {
          "Neural Network architectures to classify emotions in": "bDepartment of Medical Biophysics, University of Toronto, Canada"
        },
        {
          "Neural Network architectures to classify emotions in": "cRekhi Centre of Excellence for the Science of Happiness, IIT Kharagpur, India"
        },
        {
          "Neural Network architectures to classify emotions in": "dSchool of Languages and Linguistics, Jadavpur University, India"
        },
        {
          "Neural Network architectures to classify emotions in": "*corresponding author, email: uddaloksarkar@gmail.com"
        },
        {
          "Neural Network architectures to classify emotions in": "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "BSTRACT"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "Music  is  often  considered  as  the  language  of  emotions.  It  has  long  been  known  to  elicit"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "emotions in human being and thus categorizing music based on the type of emotions they"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "induce  in  human  being  is  a  very  intriguing  topic  of  research.  When  the  task  comes  to"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "classify  emotions  elicited  by \nIndian  Classical  Music \n(ICM), \nit  becomes  much  more"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "challenging  because  of  the  inherent  ambiguity  associated  with  ICM.  The  fact  that  a  single"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "musical performance can evoke a variety of emotional response in the audience is implicit to"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "the nature of ICM renditions. With the rapid advancements in the field of Deep Learning,"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "this  Music  Emotion  Recognition  (MER)  task  is  becoming  more  and  more  relevant  and"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "robust, hence can be applied to one of the most challenging test case i.e. classifying emotions"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "elicited  from  ICM.  In  this  paper  we  present  a  new  dataset  called  JUMusEmoDB  which"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "presently  has  400  audio  clips  (30  seconds  each)  where  200  clips  correspond  to  happy"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "emotions  and  the  remaining  200  clips  correspond  to  sad  emotion.  The  initial  annotations"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "and emotional classification of the database has been done based on an emotional rating test"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "(5-point  Likert  scale)  performed  by  100  participants.  The  clips  have  been  taken  from"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "different  conventional  'raga'  renditions  played  in  sitar  by  an  eminent  maestro  of  ICM  and"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "digitized  in  44.1  kHz  frequency.  The  ragas,  which  are  unique  to  ICM,  are  described  as"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "musical  structures  capable  of \ninducing  different  moods  or  emotions.  For  supervised"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "classification  purposes,  we  have  used  4  existing  deep  Convolutional  Neural  Network"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "(CNN)  based  architectures \n(resnet18,  mobilenet  v2.0,  squeezenet  v1.0  and  vgg16)  on"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "corresponding music spectrograms of the 2000 sub-clips (where every clip was  segmented"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "into  5  sub-clips  of  about  5  seconds  each)  which  contain  both  time  as  well  as  frequency"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "domain information. The initial results are quite inspiring, and we look forward to setting"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "the  baseline  values \nfor \nthe  dataset  using \nthis  architecture.  This \ntype  of  CNN  based"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "classification algorithm using a rich corpus of Indian Classical Music is unique even in the"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "global  perspective  and  can  be  replicated  in  other  modalities  of  music  also.  This  dataset  is"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "still  under  development  and  we  plan  to  include  more  data  containing  other  emotional"
        },
        {
          "[Received: 01-12-2020; Revised: 31-12-2020; Accepted: 31-12-2020]": "features as well. We plan to make the dataset publicly available soon."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. INTRODUCTION": "Music \nimposes  emotion.  Considering  the  very  basic,  major  scale  always  creates  a"
        },
        {
          "1. INTRODUCTION": "happier ambience whereas minor scale creates a bit sad. This response to melody and rhythm is"
        },
        {
          "1. INTRODUCTION": "a biological instinct. A non-musician‚Äôs ear can feel the pain when Garry Moore plays ‚ÄúLoner‚Äù,"
        },
        {
          "1. INTRODUCTION": "even a baby responds to different scales and melody lines. While talking about emotions elicited"
        },
        {
          "1. INTRODUCTION": "by Indian Classical Music (ICM) it always becomes a matter of huge challenge for its ambiguous"
        },
        {
          "1. INTRODUCTION": "emotional response. Studies involving non-linear techniques have been conducted in the recent"
        },
        {
          "1. INTRODUCTION": "past to understand this complex behavior of music and its manifestation in the human brain [13"
        },
        {
          "1. INTRODUCTION": "- 17]."
        },
        {
          "1. INTRODUCTION": "In the recent years, Machine Learning has made significant advancements in a multitude"
        },
        {
          "1. INTRODUCTION": "of  fields  including  computer  vision,  medical  imaging,  natural  language  processing  and  so  on"
        },
        {
          "1. INTRODUCTION": "[18  -  37].  Such  Machine  Learning  and  Deep  Learning  approaches  have  been  used  to  identify"
        },
        {
          "1. INTRODUCTION": "different  emotions  associated  with  music  [9  -12].  So,  Music  Emotion  Recognition  (MER)  has"
        },
        {
          "1. INTRODUCTION": "always been an interesting task to perform for observing the correlation between the music and"
        },
        {
          "1. INTRODUCTION": "perceived  emotion.  Music  Emotion  Recognition  task  was  first  introduced  by  Barthet  et  al  [5]."
        },
        {
          "1. INTRODUCTION": "Since  then,  several  developments  have  been  achieved  on  this  discipline,  and  hence  it  has"
        },
        {
          "1. INTRODUCTION": "become very important to observe the role of ICM in elicitation of emotion."
        },
        {
          "1. INTRODUCTION": "To exploit the significance of emotion induction in ICM, we need a proper database to"
        },
        {
          "1. INTRODUCTION": "work with. Previously datasets like Computer Audition Lab 500-song (CAL500) [6], CAL500exp"
        },
        {
          "1. INTRODUCTION": "[7], datasets have been introduced, which is enriched with 500 western music clips. Here in this"
        },
        {
          "1. INTRODUCTION": "paper we have introduced a database JUMusEmoDB enriched with 400 music clips from genre"
        },
        {
          "1. INTRODUCTION": "of Indian Classical Music, of which 200 clips fall into the category of happy emotion while 200"
        },
        {
          "1. INTRODUCTION": "falls  into  sad.  Each  clip  is  of  30  seconds  length  which  is  long  enough  for  introducing  an"
        },
        {
          "1. INTRODUCTION": "emotional imposition [8]. All the clips are parts of different ‚Äòraga‚Äô renditions improvised in Sitar"
        },
        {
          "1. INTRODUCTION": "by an eminent maestro of ICM. Each Raga in ICM evokes not a particular emotion (rasa), but a"
        },
        {
          "1. INTRODUCTION": "superposition  of  different  emotional  states  such  as  joy,  sadness,  anger,  disgust,  fear  etc.  To"
        },
        {
          "1. INTRODUCTION": "decipher  which  particular  emotion  is  conveyed  in  the  chosen  30  sec  segment  of  the  raga,  an"
        },
        {
          "1. INTRODUCTION": "emotion annotation was performed initially by 100 participants based on 5-point Likert scale."
        },
        {
          "1. INTRODUCTION": "For an emotion classification task different acoustic features of music are very important."
        },
        {
          "1. INTRODUCTION": "Different  acoustic  feature  consists  of  (a)  Rhythmic  Features:  Tempo,  Silence  etc.;  (b)  Timbral"
        },
        {
          "1. INTRODUCTION": "Features:  MFCC,  Average  Energy,  Spectral  Centroid,  Spectral  Tilt  etc.;  (c)  Chroma  Features."
        },
        {
          "1. INTRODUCTION": "These  acoustic  features  quantify  the  musicality  of  a  clip  which  in  effect  contributes  to  MER."
        },
        {
          "1. INTRODUCTION": "Study on this knowledge driven approaches results in an efficient model structure; but before"
        },
        {
          "1. INTRODUCTION": "this,  a  validated  dataset  is  very  necessary  to  work  with.  After  the  emotion  annotation,  a  data"
        },
        {
          "1. INTRODUCTION": "driven approach has been used to validate our dataset."
        },
        {
          "1. INTRODUCTION": "We  have  taken  an \nimage  processing-oriented  approach  to  classify  the  dataset \ninto"
        },
        {
          "1. INTRODUCTION": "emotion \ntags.  We  primarily  extracted \nthe  spectrogram  of  a  clip  and \nfed \nthe  processed"
        },
        {
          "1. INTRODUCTION": "spectrogram \nimage \ninto \nexisting \ndeep  Convolutional  Neural  Network \n(CNN) \nbased"
        },
        {
          "1. INTRODUCTION": "architectures. These CNNs were then trained to classify emotions. For this study, we have made"
        },
        {
          "1. INTRODUCTION": "use  of \nfour  different \nexisting  CNN \narchitectures:  VGG16,  ResNet18,  MobileNet  v2.0,"
        },
        {
          "1. INTRODUCTION": "SqueezeNet  v1.0  and  have  received  some  promising  results.  Furthermore,  the  dataset  used  in"
        },
        {
          "1. INTRODUCTION": "our study which we have named as JUMusEmoDB, is a novel dataset comprising of clips from"
        },
        {
          "1. INTRODUCTION": "Indian  Classical  Music  genre.  This  dataset  currently  has  musical  clips  from  two  emotions,"
        },
        {
          "1. INTRODUCTION": "namely  happy  and  sad,  and \nis  still  under  development.  We  plan \nto \ninclude  more  data"
        },
        {
          "1. INTRODUCTION": "containing  other  emotional  features  eventually  making  the  dataset  publicly  available  shortly."
        },
        {
          "1. INTRODUCTION": "This  dataset  can  be  used \nin  future  by  the  scientific  community  for  emotion  classification"
        },
        {
          "1. INTRODUCTION": "purposes,  to  investigate  the  impact  of  Indian  Classical  Music  on  human  brain  and  finally  to"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "clips."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "The  paper  has  been  organized  as  follows:  Section  2  contains  Data  Acquisition,  Section  3"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "comprises of the Methods used, the Experiments and Results of the study have been mentioned"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "in Section 4 and finally Section 5 has the conclusion."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "2.  DATA ACQUISITION:"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "JUMusEmoDB consists of 400 audio clips of 30 second each. The clips have been taken from"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "different  conventional  raga  renditions  played  in  sitar  by  an  eminent  maestro  of  ICM  and"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "recorded with a sample frequency of 44.1 kHz. 200 clips correspond to happy emotions and the"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "remaining 200 clips correspond to sad emotion. Initial annotations and emotional classification"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "of the database has been done based on an emotional rating test (5-point Likert scale) performed"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "by 100 participants."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "3.  METHODS:"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "We have labeled our music database JUMusEmoDB into two main classes as stated earlier,"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "i.e., happy, and sad. In this paper we have followed the approach of a data driven method to"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "classify the database into two distinct classes. Now a general question arises regarding why a"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "data  driven  approach  is  being  followed  primarily  rather  than  any  novel  knowledge-based"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "quantification. To answer this, we have to take into account that a successful classification with"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "high  accuracy  rate will validate  the  authenticity  of  JUMusEmoDB, which  can  then  be used  to"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "develop new knowledge-based models to test on this database."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "3.1. Data Preprocessing"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "The basic input to our CNN based framework should be an image data. Hence, to map"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "our  audio  database \ninto \nimage  paradigm  we  have  made  use  of  Spectrogram.  But  before"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "obtaining  spectrogram  for  these  30-second-long  audio  clips,  we  have  sliced  all  the  30  second"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "clips  into  individual  5  second  clips  to  augment  our  dataset  and  performed  STFT  on  this"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "augmented dataset to obtain spectrograms."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "3.2. Spectrogram"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "In \nour  proposed \nframework,  we \nhave"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "performed  our  classification \ntask  on \nthe"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "mel-spectrograms  of  derived  music  tracks."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "To  extract \nthe  melspectrogram  we  have"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "made  use  of  short  time  Fourier  transform"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "with a window size of 2048 and hop size of"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "512  to  obtain  a  spectrogram  (Fig.  1).  Thus"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "we \nobtained  melspectrogram \nas \na  dot"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "product  of  obtained  spectrogram  with  mel"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "filterbanks."
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "Fig.1: Spectrogram of a HAPPY Music Clipping"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "ùë†ùëùùëíùëêùë°(ùë°, ùëì) = |ùë†ùë°ùëìùë°(ùë°, ùëì)|2"
        },
        {
          "conduct  cross-cultural  studies  combining  both  Western  Classical  and  Indian  Classical  musical": "ùëöùëíùëôùë†ùëùùëíùëêùë°ùëüùëúùëîùëüùëéùëö(ùë°, ùëì‚Ä≤) = ùë†ùëùùëíùëêùë°(ùë°, ùëì) .  ùëöùëíùëôùëìùëñùëôùë°ùëíùëüùëèùëéùëõùëòùë†"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3. CNN Models": "We have extended our framework into four different established ConvNet models, i.e., VGG[1],"
        },
        {
          "3.3. CNN Models": "ResNet[2], \nSqueezeNet[3],  MobileNet \nv2[4].  We \nthus \nhave"
        },
        {
          "3.3. CNN Models": "obtained four accuracy rates with an average of 99.117%. We have"
        },
        {
          "3.3. CNN Models": "added an extra layer of 2 channels at each end of considered model"
        },
        {
          "3.3. CNN Models": "because of bi-class output of our framework."
        },
        {
          "3.3. CNN Models": "3.3.1  VGG  Net  is  the  oldest  architecture  among  the  used"
        },
        {
          "3.3. CNN Models": "ConvNet models, proposed by Karen Simonyan and Andrew"
        },
        {
          "3.3. CNN Models": "in  2014 \n[1].  This  has \nZisserman  of  Oxford  Robotics  Institute"
        },
        {
          "3.3. CNN Models": "sixteen weight layers; thirteen convolution layers divided into five"
        },
        {
          "3.3. CNN Models": "groups,  each  group  followed  by  pooling  layers,  and  three  fully"
        },
        {
          "3.3. CNN Models": "connected \n(FC) \nlayers  at \nthe  end  of  whole  network \n(Fig.  2)."
        },
        {
          "3.3. CNN Models": "Convolution  layers  have  a  receptive  field  of  3x3  throughout  the"
        },
        {
          "3.3. CNN Models": "whole  net,  with \nstride  1.  The  Maxpooling \nlayers \nconsist  of"
        },
        {
          "3.3. CNN Models": "receptive fields of size 2x2 each and with a stride of 2. The network"
        },
        {
          "3.3. CNN Models": "ends  with  three  fully  connected  layers with  first  two  layers  of  4096"
        },
        {
          "3.3. CNN Models": "channels and last layer of 1000 channels due to 1000 classes of ILSVRC (ImageNet Large-Scale"
        },
        {
          "3.3. CNN Models": "Visual Recognition Challenge)."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": "ends  with  three  fully  connected  layers with  first  two  layers  of  4096"
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": "channels and last layer of 1000 channels due to 1000 classes of ILSVRC (ImageNet Large-Scale"
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": "Visual Recognition Challenge)."
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": "Fig.3: Residual Block"
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": ""
        },
        {
          "receptive fields of size 2x2 each and with a stride of 2. The network": "and  modified  a  very  deep  network  to  gain  a  high  accuracy  with  low"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "with 1x1 convolution filters and an expand layer which has both 1x1 and 3x3 filters. Number of"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "kernels  in  squeeze  layer  should  be  less  than  number  of  kernels  in  expand  layer  to  limit  the"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "number of input channels to 3x3 kernels. Fig. 5 shows an architectural view of fire module."
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "3.3.4.  MobileNet,  by  Google,  has"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "replacing traditional convolution layer with ‚ÄúDepth-wise Separable Convolution‚Äù to reduce the"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "model size and complexity. In MobileNetV2 two kinds of blocks are observed [4], stride 1 block"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "(residual block), stride2 block (downsizing). Each block consists of three layers as shown in Fig."
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "7.  First \nlayer"
        },
        {
          "this architecture is introduction of ‚Äòfire module‚Äù. A fire module is a stacking of a squeeze layer": "convolution. Third layer again contains 1x1 kernel without any nonlinear function."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3\nFig.6: SqueezeNet Architecture": ".4. \nEmotion \nExtraction \nfrom  Network"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "output"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "Output \nfrom \nthe \nfully \nconnected \nlayers \nis"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "provided  to  softmax  to extract  out the classes.  In  our"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "framework,  we  are \nconcerned  about \ntwo  distinct"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "classes (i.e., Happy Music clip, Sad Music clip). All the"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "four  architectures  contain  1000  channels  in  last  fully"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "connected networks by default 1000 classes of ILSVRC"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "(ImageNet Large-Scale Visual Recognition Challenge)."
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "So,  we  added  another \nfully  connected \nlayer  of  2"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "channels  at  the  end  to  cater  our  purpose.  After  this"
        },
        {
          "3\nFig.6: SqueezeNet Architecture": "last fully connected layer a softmax is performed to extract"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: shows the validation",
      "data": [
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "Log loss is being calculated using the following equation:"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "ùëñ\nw \nùêø =   ‚àí ‚àë ùë¶ùëñlog (ùë¶ùëñÃÇ)"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "where ùë¶ùëñ is the calculated or predicted output and ùë¶ùëñÃÇ is the actual output."
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "Previously  we  have  introduced  our  used  framework  and  model  improvisation.  In  this"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "section we are illustrating the acquired results. For our work, we used a train/validation split"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "ratio  as  85/15.  In  the  training  phase  we  receive  a  model  cost  (calculated  from  loss  function)"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "which  indicates  the  model  performance.  The  loss  curve  for  training  phase  for  each  model  is"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "shown in the Fig. 8. With time (iterations) convergence is achieved for each of the models used"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "and  the  performance  of SquuezeNetV1  in  the  training  phase  also  outperforms  the  other  three"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "models used."
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "The convergence plot helps to tune the parameters of CNN models. After adjusting the"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "parameters,  we  obtain  the  best  results  for  each  CNN  Models.  Table  1  shows  the  validation"
        },
        {
          "prediction from  actual  output.  Hence  a  0  loss  represents  perfect  model. Cross  entropy  loss  or": "accuracy of the aforementioned models."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5.  CONCLUSION": "In  this  work,  we  proposed  a  novel  dataset  called  JUMusEmoDB  which  presently  has  400"
        },
        {
          "5.  CONCLUSION": "audio clips (30 seconds each) where 200 clips correspond to happy emotions and the remaining"
        },
        {
          "5.  CONCLUSION": "200 clips correspond to sad emotion. The initial annotations and emotional classification of the"
        },
        {
          "5.  CONCLUSION": "database has been done based on an emotional rating test (5-point Likert scale) performed by"
        },
        {
          "5.  CONCLUSION": "100 participants. We also demonstrated the performances of four deep CNN based architectures"
        },
        {
          "5.  CONCLUSION": "namely  resnet18,  mobilenet  v2.0,  squeezenet  v1.0  and  vgg16.  Validation  accuracy  values"
        },
        {
          "5.  CONCLUSION": "showed  that  SqueezeNetV1  performed  the  best  out  of  the  four  models.  Even  though  the"
        },
        {
          "5.  CONCLUSION": "advantage  of  employing  CNN  based  architectures  for  tackling  the  problem  of  music  emotion"
        },
        {
          "5.  CONCLUSION": "recognition include better overall accuracy because of better extraction of useful features from"
        },
        {
          "5.  CONCLUSION": "the  data  compared  to  other  traditional  methods,  further  studies  need  to  be  conducted  to"
        },
        {
          "5.  CONCLUSION": "understand the source of emotions for a given music. Another limitation of this work is the lack"
        },
        {
          "5.  CONCLUSION": "of  data  in  our  dataset,  but  this  is  a  pilot  study  it  is  still  under  development.  We  plan  to"
        },
        {
          "5.  CONCLUSION": "incorporate  more  data  containing  other  emotional  features  as  well  and  eventually  make  the"
        },
        {
          "5.  CONCLUSION": "dataset publicly available shortly."
        },
        {
          "5.  CONCLUSION": "ACKNOWLEDGEMENT"
        },
        {
          "5.  CONCLUSION": "Archi Banerjee acknowledges the Department of Science and Technology (DST), Govt. of India"
        },
        {
          "5.  CONCLUSION": "for providing the DST CSRI Post Doctoral Fellowship (DST/CSRI/PDF-34/2018) to pursue this"
        },
        {
          "5.  CONCLUSION": "research work. Shankha Sanyal acknowledges DST CSRI, Govt of India for providing the funds"
        },
        {
          "5.  CONCLUSION": "related  to  this  Major  Research  Project  (DST/CSRI/2018/78  (G))  and  the  Acoustical  Society  of"
        },
        {
          "5.  CONCLUSION": "America (ASA) for providing the International Students Grant."
        },
        {
          "5.  CONCLUSION": "REFERENCES"
        },
        {
          "5.  CONCLUSION": "[1]  Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-"
        },
        {
          "5.  CONCLUSION": "scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014)."
        },
        {
          "5.  CONCLUSION": "[2]  He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE"
        },
        {
          "5.  CONCLUSION": "conference on computer vision and pattern recognition. 2016."
        },
        {
          "5.  CONCLUSION": "[3]  Iandola, Forrest N., et al. \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters"
        },
        {
          "5.  CONCLUSION": "and< 0.5 MB model size.\" arXiv preprint arXiv:1602.07360 (2016)."
        },
        {
          "5.  CONCLUSION": "[4]  Sandler, Mark, et al. \"Mobilenetv2: Inverted residuals and linear bottlenecks.\" Proceedings of"
        },
        {
          "5.  CONCLUSION": "the IEEE conference on computer vision and pattern recognition. 2018."
        },
        {
          "5.  CONCLUSION": "[5]  Barthet, Mathieu, Gy√∂rgy Fazekas, and Mark Sandler. \"Music emotion recognition: From"
        },
        {
          "5.  CONCLUSION": "content-to context-based models.\" International Symposium on Computer Music Modeling and"
        },
        {
          "5.  CONCLUSION": "Retrieval. Springer, Berlin, Heidelberg, 2012."
        },
        {
          "5.  CONCLUSION": "[6]  Turnbull, Douglas, et al. \"Towards musical query-by-semantic-description using the cal500"
        },
        {
          "5.  CONCLUSION": "data set.\" Proceedings of the 30th annual international ACM SIGIR conference on Research and"
        },
        {
          "5.  CONCLUSION": "development in information retrieval. 2007."
        },
        {
          "5.  CONCLUSION": "[7]  Wang, Shuo-Yang, et al. \"Towards time-varying music auto-tagging based on CAL500"
        },
        {
          "5.  CONCLUSION": "expansion.\" 2014 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2014."
        },
        {
          "5.  CONCLUSION": "[8]  Turnbull, Douglas, et al. \"Towards musical query-by-semantic-description using the cal500"
        },
        {
          "5.  CONCLUSION": "data set.\" Proceedings of the 30th annual international ACM SIGIR conference on Research and"
        },
        {
          "5.  CONCLUSION": "development in information retrieval. 2007."
        },
        {
          "5.  CONCLUSION": "[9]  Juthi J.H., Gomes A., Bhuiyan T., Mahmud I. (2020) Music Emotion Recognition with the"
        },
        {
          "5.  CONCLUSION": "Extraction of Audio Features Using Machine Learning Approaches. In: Singh P., Panigrahi"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Liu, Xin, et al. \"CNN based music emotion classification.\" arXiv preprint"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Liu, Tong, et al. \"Audio-based deep music emotion recognition.\" AIP Conference"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Proceedings. Vol. 1967. No. 1. AIP Publishing LLC, 2018."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Yang, Yi-Hsuan, and Homer H. Chen. \"Machine recognition of music emotion: A"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "review.\" ACM Transactions on Intelligent Systems and Technology (TIST) 3.3 (2012): 1-30."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Sanyal, Shankha, et al. \"Music of brain and music on brain: a novel EEG sonification"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "approach.\" Cognitive neurodynamics 13.1 (2019): 13-31."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Sengupta, Sourya, et al. \"Emotion specification from musical stimuli: An EEG study with"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "AFA and DFA.\" 2017 4th International Conference on Signal Processing and Integrated Networks"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Nag, Sayan, et al. \"Can musical emotion be quantified with neural jitter or shimmer? A"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "novel EEG based study with Hindustani classical music.\" 2017 4th International Conference on"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Signal Processing and Integrated Networks (SPIN). IEEE, 2017."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Sarkar, Uddalok, et al. \"A Simultaneous EEG and EMG Study to Quantify Emotions"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "from Hindustani Classical Music.\" Recent Developments in Acoustics. Springer, Singapore,"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Sayan Nag, Uddalok Sarkar, Shankha Sanyal, Archi Banerjee, Souparno Roy. Samir"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Karmakar, Ranjan Sengupta, Dipak Ghosh. A Fractal Approach to Characterize Emotions in"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Audio and Visual Domain: a Study on Cross-Modal Interaction. Journal of Image Processing"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "& Pattern Recognition Progress. 2019; 6(3): 1‚Äì7p."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Krizhevsky, A., Sutskever, I., & Hinton, G. E. Imagenet classification with deep"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "convolutional neural networks. In Advances in neural information processing systems (pp."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "LeCun, Y. (2015). LeNet-5, convolutional neural networks. URL: http://yann. lecun."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" nature 521.7553"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Goodfellow, Ian, et al. Deep learning. Vol. 1. No. 2. Cambridge: MIT press, 2016."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Xu, Kelvin, et al. \"Show, attend and tell: Neural image caption generation with visual"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "attention.\" International conference on machine learning. 2015."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" arXiv"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": ""
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Baldi, P. Autoencoders, unsupervised learning, and deep architectures. In Proceedings"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "of ICML Workshop on Unsupervised and Transfer Learning (pp. 37-49)(2012, June)."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "for biomedical image segmentation.\" International Conference on Medical image computing"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "and computer-assisted intervention. Springer, Cham, 2015."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Hammernik, Kerstin, et al. \"Learning a variational network for reconstruction of"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "accelerated MRI data.\" Magnetic resonance in medicine 79.6 (2018): 3055-3071."
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "Bhattacharyya, Mayukh, and Sayan Nag. \"Hybrid Style Siamese Network: Incorporating"
        },
        {
          "Electrical Engineering, vol 605. Springer, Cham. https://doi.org/10.1007/978-3-030-30577-": "style loss in complimentary apparels retrieval.\" arXiv preprint arXiv:1912.05014 (2019)."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "captions.\" Advances in neural information processing systems. 2016."
        },
        {
          "[29]": "[30]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Nag, Sayan. \"Lookahead optimizer improves the performance of Convolutional"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Autoencoders for reconstruction of natural images.\" arXiv preprint arXiv:2012.05694 (2020)."
        },
        {
          "[29]": "[31]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "images. Technical report, University of Toronto, 2009"
        },
        {
          "[29]": "[32]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Vincent, Pascal, et al. \"Stacked denoising autoencoders: Learning useful representations"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "in a deep network with a local denoising criterion.\" Journal of machine learning research"
        },
        {
          "[29]": "11.12 (2010).",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": ""
        },
        {
          "[29]": "[33]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "captions.\" Advances in neural information processing systems. 2016."
        },
        {
          "[29]": "[34]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Vincent, Pascal, et al. \"Extracting and composing robust features with denoising"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "autoencoders.\" Proceedings of the 25th international conference on Machine learning. 2008."
        },
        {
          "[29]": "[35]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Hochreiter, Sepp, and J√ºrgen Schmidhuber. \"Long short-term memory.\" Neural"
        },
        {
          "[29]": "computation 9.8 (1997): 1735-1780.",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": ""
        },
        {
          "[29]": "[36]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information"
        },
        {
          "[29]": "",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "processing systems 30 (2017): 5998-6008."
        },
        {
          "[29]": "[37]",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": "Brown, Tom B., et al. \"Language models are few-shot learners.\" arXiv preprint"
        },
        {
          "[29]": "arXiv:2005.14165 (2020).",
          "Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Very deep convolutional networks for largescale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for largescale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "2",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size",
      "authors": [
        "Forrest Iandola"
      ],
      "year": "2016",
      "venue": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size",
      "arxiv": "arXiv:1602.07360"
    },
    {
      "citation_id": "4",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Music emotion recognition: From content-to context-based models",
      "authors": [
        "Barthet",
        "Gy√∂rgy Mathieu",
        "Mark Fazekas",
        "Sandler"
      ],
      "year": "2012",
      "venue": "International Symposium on Computer Music Modeling and Retrieval"
    },
    {
      "citation_id": "6",
      "title": "Towards musical query-by-semantic-description using the cal500 data set",
      "authors": [
        "Douglas Turnbull"
      ],
      "year": "2007",
      "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval"
    },
    {
      "citation_id": "7",
      "title": "Towards time-varying music auto-tagging based on CAL500 expansion",
      "authors": [
        "Shuo- Wang",
        "Yang"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "8",
      "title": "Towards musical query-by-semantic-description using the cal500 data set",
      "authors": [
        "Douglas Turnbull"
      ],
      "year": "2007",
      "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval"
    },
    {
      "citation_id": "9",
      "title": "Music Emotion Recognition with the Extraction of Audio Features Using Machine Learning Approaches",
      "authors": [
        "J Juthi",
        "A Gomes",
        "T Bhuiyan",
        "I Mahmud"
      ],
      "year": "2020",
      "venue": "Proceedings of ICETIT 2019",
      "doi": "10.1007/978-3-030-30577-2_27"
    },
    {
      "citation_id": "10",
      "title": "CNN based music emotion classification",
      "authors": [
        "Xin Liu"
      ],
      "year": "2017",
      "venue": "CNN based music emotion classification",
      "arxiv": "arXiv:1704.05665"
    },
    {
      "citation_id": "11",
      "title": "Audio-based deep music emotion recognition",
      "authors": [
        "Tong Liu"
      ],
      "year": "2018",
      "venue": "AIP Conference Proceedings"
    },
    {
      "citation_id": "12",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "13",
      "title": "Music of brain and music on brain: a novel EEG sonification approach",
      "authors": [
        "Sanyal",
        "Shankha"
      ],
      "year": "2019",
      "venue": "Cognitive neurodynamics"
    },
    {
      "citation_id": "14",
      "title": "Emotion specification from musical stimuli: An EEG study with AFA and DFA",
      "authors": [
        "Sourya Sengupta"
      ],
      "year": "2017",
      "venue": "2017 4th International Conference on Signal Processing and Integrated Networks (SPIN)"
    },
    {
      "citation_id": "15",
      "title": "Can musical emotion be quantified with neural jitter or shimmer? A novel EEG based study with Hindustani classical music",
      "authors": [
        "Nag",
        "Sayan"
      ],
      "year": "2017",
      "venue": "2017 4th International Conference on Signal Processing and Integrated Networks (SPIN)"
    },
    {
      "citation_id": "16",
      "title": "A Simultaneous EEG and EMG Study to Quantify Emotions from Hindustani Classical Music",
      "authors": [
        "Uddalok Sarkar"
      ],
      "year": "2020",
      "venue": "Recent Developments in Acoustics"
    },
    {
      "citation_id": "17",
      "title": "A Fractal Approach to Characterize Emotions in Audio and Visual Domain: a Study on Cross-Modal Interaction",
      "authors": [
        "Sayan Nag",
        "Uddalok Sarkar",
        "Shankha Sanyal",
        "Archi Banerjee",
        "Souparno Roy",
        "Ranjan Samir Karmakar",
        "Dipak Sengupta",
        "Ghosh"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Progress"
    },
    {
      "citation_id": "18",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "LeNet-5, convolutional neural networks",
      "authors": [
        "Y Lecun"
      ],
      "year": "2015",
      "venue": "LeNet-5, convolutional neural networks"
    },
    {
      "citation_id": "20",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "21",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Deep learning",
      "authors": [
        "Ian Goodfellow"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "23",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "24",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "Diederik Kingma",
        "Max Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "25",
      "title": "Autoencoders, unsupervised learning, and deep architectures",
      "authors": [
        "P Baldi"
      ],
      "year": "2012",
      "venue": "Proceedings of ICML Workshop on Unsupervised and Transfer Learning"
    },
    {
      "citation_id": "26",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "Olaf Ronneberger",
        "Philipp Fischer",
        "Thomas Brox"
      ],
      "year": "2015",
      "venue": "International Conference on Medical image computing and computer-assisted intervention"
    },
    {
      "citation_id": "27",
      "title": "Learning a variational network for reconstruction of accelerated MRI data",
      "authors": [
        "Kerstin Hammernik"
      ],
      "year": "2018",
      "venue": "Magnetic resonance in medicine"
    },
    {
      "citation_id": "28",
      "title": "Hybrid Style Siamese Network: Incorporating style loss in complimentary apparels retrieval",
      "authors": [
        "Mayukh Bhattacharyya",
        "Sayan Nag"
      ],
      "year": "2019",
      "venue": "Hybrid Style Siamese Network: Incorporating style loss in complimentary apparels retrieval",
      "arxiv": "arXiv:1912.05014"
    },
    {
      "citation_id": "29",
      "title": "Variational autoencoder for deep learning of images, labels and captions",
      "authors": [
        "Yunchen Pu"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images",
      "authors": [
        "Sayan Nag"
      ],
      "year": "2020",
      "venue": "Lookahead optimizer improves the performance of Convolutional Autoencoders for reconstruction of natural images",
      "arxiv": "arXiv:2012.05694"
    },
    {
      "citation_id": "31",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "32",
      "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "authors": [
        "Pascal Vincent"
      ],
      "year": "2010",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "33",
      "title": "Variational autoencoder for deep learning of images, labels and captions",
      "authors": [
        "Yunchen Pu"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "Pascal Vincent"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "35",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "36",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    }
  ]
}