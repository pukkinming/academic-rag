{
  "paper_id": "2204.09016v1",
  "title": "Benchmarking Domain Generalization On Eeg-Based Emotion Recognition",
  "published": "2022-04-18T07:54:22Z",
  "authors": [
    "Yan Li",
    "Hao Chen",
    "Jake Zhao",
    "Haolan Zhang",
    "Jinpeng Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG)   based emotion recognition has demonstrated tremendous improvement in recent years. Specifically, numerous domain adaptation (DA) algorithms have been exploited in the past five years to enhance the generalization of emotion recognition models across subjects. The DA methods assume that calibration data (although unlabeled) exists in the target domain (new user). However, this assumption conflicts with the application scenario that the model should be deployed without the time-consuming calibration experiments. We argue that domain generalization (DG) is more reasonable than DA in these applications. DG learns how to generalize to unseen target domains by leveraging knowledge from multiple source domains, which provides a new possibility to train general models. In this paper, we for the first time benchmark state-of-the-art DG algorithms on EEG-based emotion recognition. Since convolutional neural network (CNN), deep brief network (DBN) and multilayer perceptron (MLP) have been proved to be effective emotion recognition models, we use these three models as solid baselines. Experimental results show that DG achieves an accuracy of up to 79.41% on the SEED dataset for recognizing three emotions, indicting the potential of DG in zero-training emotion recognition when multiple sources are available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is of great importance for humans in various aspects of daily activities. In human-computer interaction, emotion plays an essential role in fatigue detection and healthcare, and existing studies have confirmed the association between various diseases and emotions  [1] . Human emotions can be detected via facial expression, speech and eye blinking  [2] , etc. However, these methods are always susceptible to subjective influences of the participants, which will affect the accuracy and reliability of the models. Comparatively, recognizing emotions through physiological signals is more objective and reliable. As a bridge between the brain and the computer, brain-computer interfaces (BCIs) allow users to acquire brain signals directly. Invasive BCIs, for example, are prohibitively expensive and need surgery to achieve a better accuracy. On the other hand, non-invasive BCIs using electroencephalography (EEG) are much safer and thus have been commonly employed to collect brain signals  [3] . Typically, feature extraction and classification are employed on the preprocessed EEG data.\n\nNevertheless, the EEG signals acquired from the same subject at the same session can be very biased, and training a general model remains a challenge in EEG-based emotion recognition. In recent years, to tackle this issue, numerous works have applied transfer learning, especially domain adaptation (DA) to transfer knowledge from the labeled source domain (existing subjects) to the unlabeled target domain (new data). For example, Zheng et al. applied transfer component analysis (TCA) to help improve the accuracy in the cross-subject transfer  [4] . Li et al. proposed the multi-source style transfer mapping (MS-STM) for the cross-subject multi-source scenario to reduce the marginal distribution differences  [5] . With the development of deep learning, Li et al. adopted deep adaptation network for crosssubject emotion recognition  [6] . Zheng et al. extended the SEED dataset to SEED-IV with more emotion categories and additional modality of emotion data. They also presented EmotionMeter, which fuses two modalities of EEG data  [7] . Zhao et al. proposed a plug-and-play domain adaptation method for shortening the calibration time while maintaining the accuracy  [8] . Chen et al. focused on the multi-source scenarios in EEG-based emotion recognition and presented MS-MDA  [9] ,  [10]  to take both domain-invariant and domainspecific EEG features into consideration.\n\nMost of the aforementioned works enhanced the performance of the model on the target domain by decreasing the domain shift between the source and target domain. However, the assumption that unlabeled target data exists conflicts with practical application, which does not require additional data acquisition in the target domain. Besides, the cost of training a model using DA for every single target domain is high. Domain generalization (DG), on the other hand, deals with a more challenging setting that several different but related domain(s) are given, and the goal is to learn a zero-training model that can generalize to the unseen test domain  [11] . DG helps extract domain-invariant features by exploiting domain differences across multiple source subjects without acquiring any extra target data. However, there are limited works using DG in EEG-based emotion recognition  [12] . Benchmarking DG methods on EEG-based emotion recognition is necessary to provide insights and references for the community.\n\nIn this paper, we benchmark state-of-the-art DG algorithms on EEG-based emotion recognition, which helps alleviate the differences of multiple domain distributions to achieve better generalization on unseen subjects. For each",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unseen Domains",
      "text": "Model weights",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Label Prediction",
      "text": "Fig.  1 . The overall framework of domain generalization in EEG emotion recognition. Data from each subject is seen as a domain. In the training process, all the source domains are used to train a feature extractor by domain generalization methods. In the testing process, the model trained is used to extract the features from unseen domains and then predict the emotion labels. This figure is best viewed in colors.\n\nDG method, we use strong baselines for training, including convolutional neural network (CNN), deep brief network (DBN) and multilayer perceptron (MLP), which have been demonstrated effective in emotion recognition. We evaluate the performance of these approaches by recognizing three emotions in a zero-training scenario.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Method",
      "text": "The overall framework of the application of domain generalization in EEG-based emotion recognition is presented in Fig.  1 . Data from each subject can be seen as a domain, and the unseen subject data domains are taken as the test sets for the inference stage.\n\nBased on the aspects that are focused on during training, the selected DG methods can be categorized into three types.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Manipulation",
      "text": "We select two data manipulation approaches named Mixup  [13]  and group DRO  [14] . Mixup extends the training distribution by incorporating the prior knowledge that linear interpolations of feature vectors should lead to linear interpolations of the associated targets. The group DRO leverages prior knowledge of spurious correlations to define groups over the training data and define the uncertainly set in terms of these groups.\n\nMixup: It generates virtual feature-target vectors from real feature-target vectors. The whole Mixup method can be described as\n\n(1) where E represents the empirical risk minimization (ERM)  [15] . (x i , y i ) and (x j , y j ) are two feature-target vectors drawn at random from the training data, Œª ‚àà [0, 1]. We minimize the average of the loss function over the data distribution to train the model:\n\nwhere l denotes the loss function and P denotes the data distribution.\n\nGroup DRO: It needs to define m groups P g indexed by G = 1, 2, ..., M and the training distribution P is a mixture of the m groups. The uncertainty set Q is defined by any mixtures of these groups:\n\nwhere m is the (m-1)-dimensional probability simplex. The group DRO method yields the model by minimizing the empirical worst-group risk R(Œ∏):\n\nE represents ERM method and l represents the loss function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Representation Learning",
      "text": "Deep domain confusion (DDC)  [16] , deep adversarial neural network (DANN)  [17] , and deep CORAL  [18]  from selected methods belong to the representation learning. DANN embeds the domain adaptation into the process of learning representation, so that the final classification decisions are made based on features that are both discriminative and invariant to the change of domains. DDC uses an adaption layer along with a domain confusion loss based on maximum mean discrepancy (MMD)  [19]  to automatically learn a representation jointly trained to optimize for classification and domain invariance. The deep CORAL constructs a differentiable loss function that minimizes the difference between source and target correlations.\n\nThe DANN method try to make the two feature distributions as similar as possible and discriminate the two feature distribution at the same time, which can be represented as\n\nwhere Œ∏ f denotes the parameters of the feature mapping that maximize the loss of the domain classifier; Œ∏ d is the parameter of the domain classifier that minimize the loss of the domain classifier; L y is the loss for label prediction and L d is the loss for domain classification. L i y and L i d denote the corresponding loss function evaluated at the i -th training example.\n\nThe DDC method trains the classifier on the labeled source data through minimizing the distance between the source and target distributions, and a standard distribution distance metric MMD is used, which is defined as\n\nwhere X S denotes the source data and X T denotes the target data, and œÜ represents a kernel function. The final loss function can be written as\n\nwhere l c (X L , y) denotes classification loss on the available labeled data X L and the ground truth labely, Œª is a hyperparameter.\n\nThe deep CORAL defines a CORAL loss:\n\nwhere ‚Ä¢ 2 F denotes the squared matrix frobenius norm, and C S , C T are generated by the labeled source-domain data and unlabeled target data. And in order to learn features that work well on the target domain, both the classification and CORAL loss need to be trained at the same time.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Learning Strategy",
      "text": "We adopt the RSC  [20]  to train the models. The RSC method discards the representation associated with the higher gradients at each epoch and forces the model to predict with remaining information during the training process.\n\nRSC first calculates the gradient of upper layers with respect to the representation\n\nwhere denotes an element-wise product; z is the feature representation; h(z, Œ∏top t ) denotes the task component of the model with input z and parameter Œ∏top t ; t denotes the t -th iteration. The gradient can be written as below: gŒ∏ = ‚àÇl(œÉ(h(z; Œ∏top t )), y)/‚àÇ Œ∏t .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "We perform emotion recognition tasks on the SEED dataset  [21] ,  [22]  with all the considered DG methods. We also experiment with different baselines to extract features, including well-tuned CNN, DBN and MLP. The Institution's Ethical Review Board approved all experimental procedures involving human subjects.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Settings",
      "text": "Datasets: The SEED dataset contains EEG signals from 15 healthy subjects (7 males and 8 females) with three emotion categories (negative, neutral, and positive). Each subject performed the signal acquisition three times with an interval of one week, and the trial number of each session is 15. The raw data are recorded with an ESI NeuroScan system with 62 channels, and then down-sampled to 200 Hz sampling rate. After that, a band-pass filter between 0-75 Hz was applied to maintain informative bands. There are several kinds of features extracted from raw data in SEED, among them, differential entropy (DE)  [21]  has been proven to be robust and accurate. It should be noticed that for each emotion category, samples should resized into the same shape to facilitate the input of the model. Here we set the input shape to (62, 250, 5) and add 0 to samples that are smaller than this size. Implementation Details. The Adam optimizer is used and the initial learning rate is set to 1e -2. The batch size is set to 32, the model is trained with 50 Epochs with 5e -4 weight decay. For the lambda of Mixup, we simply set to 0.2. The drop factor of RSC is set to 1/3. All the other parameters that are required for DG methods are set to 1 by default.\n\nWe select one subject as the unseen target domain and the remaining domains are divided into training domains and validation domains according to a ratio of 4:1. During the training process, the model with the highest accuracy on the validation domains is saved, and then we apply it to the target domain to compute the accuracy on the target domain. In order to avoid the contingency of the experiment, each subject is selected to be the target domain in turn. The mean value and the variance of the total 15 results are reported.\n\nTo show the effectiveness of our model, we also compare several different baselines: ResNet",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Results",
      "text": "Table  I  shows the results on SEED with different DG algorithms and baselines. For CNN, it can be seen that with the number of convolutional layers increases, the performance of ResNet series models decreases. As for the MLP models, there is no significant change in performance with increasing number of layers. Besides, all MLP models outperform ResNet series models, which indicates that CNNs may not be the best choice for EEG-based emotion recognition since the input is quite different from images. The results of DBN shows that DBN outperforms ResNet series but not as good as MLPs. Among these results, the highest accuracy is 0.7941 with RSC method using MLP-4 baseline, the best DG method is Mixup with an average The main insufficiency of this work is that we only consider deep learning methods that have been used more often in recent years and do not systematically test the DG algorithms on traditional classifiers such as support vector machines and linear discriminative analysis. When the data amount of source is small, traditional models may have better results. This is the next step of our work plan.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusions",
      "text": "In this paper, we evaluate six representative domain generalization methods on SEED dataset with deep baselines, i.e., CNN, DBN and MLP. Based on experimental evaluations, we find that the well-tuned MLP can reach an accuracy of 79.41% with RSC method when using no target data. Besides, all the results show that DG algorithms combined with specific baselines have the ability to achieve prominent effects on EEG-based emotion recognition for new users. The benchmarked DG method seems a promising routine towards zero-training emotion recognition models. It is suitable to the practical scenario of affective BCIs and serves as an inspiration and reference for subsequent works concerning transfer learning.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall framework of domain generalization in EEG emotion",
      "page": 2
    },
    {
      "caption": "Figure 1: Data from each subject can be seen as a domain, and",
      "page": 2
    },
    {
      "caption": "Figure 2: The inÔ¨Çuences of epoch and batch size on SEED dataset with three",
      "page": 4
    },
    {
      "caption": "Figure 2: In general, these results indicate that DG algorithms com-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Abstract‚Äî Electroencephalography\n(EEG)\nbased\nemotion",
          "IEEE,": "much\nsafer\nand\nthus\nhave\nbeen\ncommonly\nemployed\nto"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "recognition\nhas\ndemonstrated\ntremendous\nimprovement\nin",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "collect\nbrain\nsignals\n[3]. Typically,\nfeature\nextraction\nand"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "recent years. SpeciÔ¨Åcally, numerous domain adaptation (DA)",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "classiÔ¨Åcation are employed on the preprocessed EEG data."
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "algorithms have been exploited in the past Ô¨Åve years to enhance",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "Nevertheless,\nthe EEG signals\nacquired\nfrom the\nsame"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "the generalization of\nemotion recognition models across\nsub-",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "jects. The DA methods assume that calibration data (although",
          "IEEE,": "subject at\nthe same session can be very biased, and train-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "unlabeled)\nexists\nin the\ntarget domain (new user). However,",
          "IEEE,": "ing\na\ngeneral model\nremains\na\nchallenge\nin EEG-based"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "this\nassumption\nconÔ¨Çicts with\nthe\napplication\nscenario\nthat",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "emotion recognition.\nIn recent years,\nto tackle\nthis\nissue,"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "the model\nshould\nbe\ndeployed without\nthe\ntime-consuming",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "numerous works have\napplied transfer\nlearning,\nespecially"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "calibration experiments. We argue that domain generalization",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "domain\nadaptation\n(DA)\nto\ntransfer\nknowledge\nfrom the"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "(DG)\nis more\nreasonable\nthan DA in these applications. DG",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "learns how to generalize to unseen target domains by leveraging",
          "IEEE,": "labeled source domain (existing subjects)\nto the unlabeled"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "knowledge from multiple source domains, which provides a new",
          "IEEE,": "target domain (new data). For example, Zheng et al. applied"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "possibility to train general models. In this paper, we for the Ô¨Årst",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "transfer\ncomponent\nanalysis\n(TCA)\nto\nhelp\nimprove\nthe"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "time benchmark state-of-the-art DG algorithms on EEG-based",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "accuracy in the cross-subject\ntransfer\n[4]. Li et al. proposed"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "emotion recognition. Since convolutional neural network (CNN),",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "the multi-source style transfer mapping (MS-STM)\nfor\nthe"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "deep brief network (DBN) and multilayer perceptron (MLP)",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "have been proved to be effective emotion recognition models,",
          "IEEE,": "cross-subject multi-source\nscenario to reduce\nthe marginal"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "we\nuse\nthese\nthree models\nas\nsolid\nbaselines. Experimental",
          "IEEE,": "distribution differences\n[5]. With the development of deep"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "results\nshow that DG achieves an accuracy of up to 79.41%",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "learning, Li et al. adopted deep adaptation network for cross-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "on the SEED dataset\nfor recognizing three emotions,\nindicting",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "subject emotion recognition [6]. Zheng et al. extended the"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "the potential of DG in zero-training emotion recognition when",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "SEED dataset\nto SEED-IV with more\nemotion categories"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "multiple sources are available.",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "and additional modality of emotion data. They also presented"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "I.\nINTRODUCTION",
          "IEEE,": "EmotionMeter, which fuses two modalities of EEG data [7]."
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "et\nal.\nZhao\nproposed\na\nplug-and-play\ndomain\nadaptation"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Emotion recognition is of great\nimportance\nfor humans",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "method for shortening the calibration time while maintaining"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "in\nvarious\naspects\nof\ndaily\nactivities.\nIn\nhuman-computer",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "the accuracy [8]. Chen et al. focused on the multi-source sce-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "interaction,\nemotion plays\nan essential\nrole\nin fatigue de-",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "narios in EEG-based emotion recognition and presented MS-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "tection and healthcare, and existing studies have conÔ¨Årmed",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "MDA [9],\n[10]\nto take both domain-invariant and domain-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "the association between various diseases and emotions\n[1].",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "speciÔ¨Åc EEG features into consideration."
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Human\nemotions\ncan\nbe\ndetected\nvia\nfacial\nexpression,",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "Most of\nthe aforementioned works enhanced the perfor-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "speech and eye blinking [2],\netc. However,\nthese methods",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "mance of\nthe model on the target domain by decreasing the"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "are\nalways\nsusceptible\nto subjective\ninÔ¨Çuences of\nthe par-",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "domain shift between the source and target domain. However,"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "ticipants, which will\naffect\nthe\naccuracy and reliability of",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "the assumption that unlabeled target data exists conÔ¨Çicts with"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "the models. Comparatively,\nrecognizing\nemotions\nthrough",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "practical application, which does not\nrequire additional data"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "physiological\nsignals\nis more objective\nand reliable. As\na",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "acquisition in the target domain. Besides,\nthe cost of training"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "bridge between the brain and the computer, brain-computer",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "a model using DA for every single target domain is high."
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "interfaces (BCIs) allow users to acquire brain signals directly.",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "Domain generalization (DG), on the other hand, deals with"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Invasive BCIs,\nfor example, are prohibitively expensive and",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "a more challenging setting that several different but\nrelated"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "need surgery to achieve a better accuracy. On the other hand,",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "domain(s) are given, and the goal\nis to learn a zero-training"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "non-invasive BCIs using electroencephalography (EEG) are",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "model that can generalize to the unseen test domain [11]. DG"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "This work was supported in part by National Natural Science Foundation",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "helps extract domain-invariant features by exploiting domain"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "of China (62106248), Zhejiang Provincial Natural Science Foundation of",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "differences across multiple source subjects without acquiring"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "China (LQ20F030013), and Ningbo Public Service Technology Foundation,",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "China (202002N3181).",
          "IEEE,": "any extra target data. However,\nthere are limited works using"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "‚Ä† equal contribution",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "DG in EEG-based emotion recognition [12]. Benchmarking"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "1 School of Computer Science, Zhejiang University, Hangzhou, Zhejiang,",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "DG methods on EEG-based emotion recognition is necessary"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "China.",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "2 HwaMei Hospital, University of Chinese Academy of Sciences, No. 41",
          "IEEE,": "to provide insights and references for\nthe community."
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Northwest Street, Haishu District, Ningbo, Zhejiang, 315010, China.",
          "IEEE,": "In\nthis\npaper, we\nbenchmark\nstate-of-the-art DG algo-"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "3 Ningbo Institute of Life\nand Health Industry, University of Chinese",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "rithms\non\nEEG-based\nemotion\nrecognition, which\nhelps"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "Academy of Sciences, Ningbo, Zhejiang, China.",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "",
          "IEEE,": "alleviate the differences of multiple domain distributions to"
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "4 NIT, Zhejiang University, Ningbo, Zhejiang, China.",
          "IEEE,": ""
        },
        {
          "Yan Li1,‚Ä†, Hao Chen2,3,‚Ä†, Jake Zhao (Junbo)1, Haolan Zhang1,4, and Jinpeng Li2,3,‚àó, Member,": "‚àó Corresponding author: Jinpeng Li\n(E-mail:\nlijinpeng@ucas.ac.cn)",
          "IEEE,": "achieve better generalization on unseen subjects. For\neach"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training process": ""
        },
        {
          "Training process": "Domain generalization training\nùë∫ùüè"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "ùë∫ùüê"
        },
        {
          "Training process": "ùë∫ùüê"
        },
        {
          "Training process": "ùë∫ùüè"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "ùë∫ùüë\n‚Ä¶"
        },
        {
          "Training process": "Source domains"
        },
        {
          "Training process": "ùë∫ùüë"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "ùë∫n"
        },
        {
          "Training process": "‚Ä¶"
        },
        {
          "Training process": ""
        },
        {
          "Training process": ""
        },
        {
          "Training process": "ùë∫n"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Testing process"
        },
        {
          "Training process": "ùë∫j"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Unseen domains"
        },
        {
          "Training process": "Label"
        },
        {
          "Training process": "‚Ä¶"
        },
        {
          "Training process": "prediction"
        },
        {
          "Training process": "Model weights\nùë∫ùíå"
        },
        {
          "Training process": ""
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Fig. 1.\nThe overall framework of domain generalization in EEG emotion"
        },
        {
          "Training process": "recognition. Data from each subject\nis\nseen as a domain.\nIn the training"
        },
        {
          "Training process": "process,\nall\nthe\nsource domains\nare used to train a\nfeature\nextractor by"
        },
        {
          "Training process": "domain generalization methods.\nIn the testing process,\nthe model\ntrained"
        },
        {
          "Training process": "is used to extract\nthe features\nfrom unseen domains and then predict\nthe"
        },
        {
          "Training process": "emotion labels. This Ô¨Ågure is best viewed in colors."
        },
        {
          "Training process": ""
        },
        {
          "Training process": ""
        },
        {
          "Training process": "DG method, we use strong baselines for\ntraining,\nincluding"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "convolutional\nneural\nnetwork\n(CNN),\ndeep\nbrief\nnetwork"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "(DBN) and multilayer perceptron (MLP), which have been"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "demonstrated effective in emotion recognition. We evaluate"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "the performance of\nthese\napproaches by recognizing three"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "emotions in a zero-training scenario."
        },
        {
          "Training process": ""
        },
        {
          "Training process": ""
        },
        {
          "Training process": "II. METHOD"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "The overall\nframework of\nthe application of domain gen-"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "eralization in EEG-based emotion recognition is presented in"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Fig. 1. Data from each subject can be seen as a domain, and"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "the unseen subject data domains are taken as the test sets for"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "the inference stage."
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Based on the aspects that are focused on during training,"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "the selected DG methods can be categorized into three types."
        },
        {
          "Training process": ""
        },
        {
          "Training process": "A. Data Manipulation"
        },
        {
          "Training process": ""
        },
        {
          "Training process": ""
        },
        {
          "Training process": "We\nselect\ntwo\ndata manipulation\napproaches\nnamed"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "Mixup [13] and group DRO [14]. Mixup extends the training"
        },
        {
          "Training process": "distribution by incorporating the prior knowledge that\nlinear"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "interpolations of feature vectors should lead to linear interpo-"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "lations of\nthe associated targets. The group DRO leverages"
        },
        {
          "Training process": "prior knowledge of\nspurious\ncorrelations\nto deÔ¨Åne groups"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "over the training data and deÔ¨Åne the uncertainly set\nin terms"
        },
        {
          "Training process": ""
        },
        {
          "Training process": "of\nthese groups."
        },
        {
          "Training process": "Mixup:\nIt\ngenerates\nvirtual\nfeature-target\nvectors\nfrom"
        },
        {
          "Training process": "real feature-target vectors. The whole Mixup method can be"
        },
        {
          "Training process": "described as"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "average"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "0.0932\n0.1031\n0.1037\n0.0855\n0.1052\n0.1179"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "shape to (62, 250, 5) and add 0 to samples that are smaller"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "than this size."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "Implementation Details.\nThe Adam optimizer\nis used"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "and the initial\nlearning rate is set\nto 1e ‚àí 2. The batch size"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "is set\nto 32,\nthe model\nis trained with 50 Epochs with 5e ‚àí 4"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "weight decay. For the lambda of Mixup, we simply set to 0.2."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "The drop factor of RSC is set to 1/3. All the other parameters"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "that are required for DG methods are set\nto 1 by default."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "We select one subject as the unseen target domain and the"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "remaining domains\nare divided into training domains\nand"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "validation domains according to a ratio of 4:1. During the"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "training process,\nthe model with the highest accuracy on the"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "validation domains\nis\nsaved,\nand then we\napply it\nto the"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "target domain to compute the accuracy on the target domain."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "In order\nto avoid the contingency of\nthe experiment, each"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "subject\nis selected to be the target domain in turn. The mean"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "value and the variance of\nthe total 15 results are reported."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "To show the\neffectiveness of our model, we\nalso com-"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "pare\nseveral\ndifferent\nbaselines: ResNet\n[23]\n(ResNet18,"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "ResNet34, ResNet50). MLP (MLP model with 2, 3, and 4"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "fully connected layers here for experiment). DBN (We use a"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "DBN model with two RBM layers, and the hidden units of"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "the two RBM layers are 23*23*5 and 18*18*2)."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "B. Results"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": ""
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "Table\nI\nshows\nthe\nresults\non SEED with\ndifferent DG"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "algorithms\nand\nbaselines.\nFor CNN,\nit\ncan\nbe\nseen\nthat"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "with the number of convolutional\nlayers\nincreases,\nthe per-"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "formance\nof ResNet\nseries models\ndecreases. As\nfor\nthe"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "MLP models,\nthere is no signiÔ¨Åcant change in performance"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "with increasing number of\nlayers. Besides,\nall MLP mod-"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "els outperform ResNet\nseries models, which indicates\nthat"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "CNNs may not be the best choice for EEG-based emotion"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "recognition since\nthe\ninput\nis quite different\nfrom images."
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "The results of DBN shows\nthat DBN outperforms ResNet"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "series but not as good as MLPs. Among these results,\nthe"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "highest accuracy is 0.7941 with RSC method using MLP-"
        },
        {
          "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961": "4 baseline,\nthe best DG method is Mixup with an average"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "THE COMPARISON OF ACCURACY ON SEED DATASET SHOWN IN"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "MEAN/STD. THE AVERAGE ACCURACY NUMBERS OF EACH BASELINE"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "AND EACH DG METHOD ARE GIVEN. THE BEST RESULTS ARE IN BOLD."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "baseline\nERM\nDANN\nRSC\nMixup\nMMD\nCORAL\naverage"
        },
        {
          "TABLE I": "mean\n0.6429\n0.5733\n0.6178\n0.7378\n0.6859\n0.6889\n0.6578\nResNet-18"
        },
        {
          "TABLE I": "std\n0.1153\n0.1194\n0.1215\n0.1005\n0.0839\n0.1375\n0.1130"
        },
        {
          "TABLE I": "mean\n0.4770\n0.5111\n0.5318\n0.7067\n0.6607\n0.6415\n0.5881"
        },
        {
          "TABLE I": "ResNet-34"
        },
        {
          "TABLE I": "std\n0.0979\n0.1460\n0.1300\n0.0802\n0.1180\n0.1065\n0.1131"
        },
        {
          "TABLE I": "mean\n0.4207\n0.4163\n0.4163\n0.4844\n0.5867\n0.5852\n0.4849\nResNet-50"
        },
        {
          "TABLE I": "std\n0.1030\n0.0967\n0.1068\n0.1070\n0.1251\n0.1292\n0.1113"
        },
        {
          "TABLE I": "mean\n0.7718\n0.7397\n0.7674\n0.7807\n0.7674\n0.7333\n0.7600"
        },
        {
          "TABLE I": "MLP-2"
        },
        {
          "TABLE I": "std\n0.0859\n0.1104\n0.0852\n0.0712\n0.1234\n0.1197\n0.0993"
        },
        {
          "TABLE I": "mean\n0.7703\n0.7572\n0.7585\n0.7600\n0.7304\n0.7363\n0.7521"
        },
        {
          "TABLE I": "MLP-3"
        },
        {
          "TABLE I": "std\n0.0915\n0.0661\n0.0983\n0.0798\n0.1025\n0.1257\n0.0940"
        },
        {
          "TABLE I": "0.7941\n0.7768\nmean\n0.7837\n0.7896\n0.7733\n0.757\n0.763"
        },
        {
          "TABLE I": "MLP-4"
        },
        {
          "TABLE I": "std\n0.0981\n0.1108\n0.1047\n0.1002\n0.1017\n0.1248\n0.1067"
        },
        {
          "TABLE I": "mean\n0.6563\n0.6800\n0.6533\n0.7393\n0.7244\n0.7244\n0.6690"
        },
        {
          "TABLE I": "DBN"
        },
        {
          "TABLE I": "std\n0.0605\n0.0722\n0.0796\n0.0597\n0.0818\n0.0818\n0.0683"
        },
        {
          "TABLE I": "0.7117\n0.6461\n0.6382\n0.6485\n0.7018\n0.6961"
        },
        {
          "TABLE I": "average"
        },
        {
          "TABLE I": "0.0932\n0.1031\n0.1037\n0.0855\n0.1052\n0.1179"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "shape to (62, 250, 5) and add 0 to samples that are smaller"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "than this size."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Implementation Details.\nThe Adam optimizer\nis used"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "and the initial\nlearning rate is set\nto 1e ‚àí 2. The batch size"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "is set\nto 32,\nthe model\nis trained with 50 Epochs with 5e ‚àí 4"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "weight decay. For the lambda of Mixup, we simply set to 0.2."
        },
        {
          "TABLE I": "The drop factor of RSC is set to 1/3. All the other parameters"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "that are required for DG methods are set\nto 1 by default."
        },
        {
          "TABLE I": "We select one subject as the unseen target domain and the"
        },
        {
          "TABLE I": "remaining domains\nare divided into training domains\nand"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "validation domains according to a ratio of 4:1. During the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "training process,\nthe model with the highest accuracy on the"
        },
        {
          "TABLE I": "validation domains\nis\nsaved,\nand then we\napply it\nto the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "target domain to compute the accuracy on the target domain."
        },
        {
          "TABLE I": "In order\nto avoid the contingency of\nthe experiment, each"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "subject\nis selected to be the target domain in turn. The mean"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "value and the variance of\nthe total 15 results are reported."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "To show the\neffectiveness of our model, we\nalso com-"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "pare\nseveral\ndifferent\nbaselines: ResNet\n[23]\n(ResNet18,"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "ResNet34, ResNet50). MLP (MLP model with 2, 3, and 4"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "fully connected layers here for experiment). DBN (We use a"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "DBN model with two RBM layers, and the hidden units of"
        },
        {
          "TABLE I": "the two RBM layers are 23*23*5 and 18*18*2)."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "B. Results"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Table\nI\nshows\nthe\nresults\non SEED with\ndifferent DG"
        },
        {
          "TABLE I": "algorithms\nand\nbaselines.\nFor CNN,\nit\ncan\nbe\nseen\nthat"
        },
        {
          "TABLE I": "with the number of convolutional\nlayers\nincreases,\nthe per-"
        },
        {
          "TABLE I": "formance\nof ResNet\nseries models\ndecreases. As\nfor\nthe"
        },
        {
          "TABLE I": "MLP models,\nthere is no signiÔ¨Åcant change in performance"
        },
        {
          "TABLE I": "with increasing number of\nlayers. Besides,\nall MLP mod-"
        },
        {
          "TABLE I": "els outperform ResNet\nseries models, which indicates\nthat"
        },
        {
          "TABLE I": "CNNs may not be the best choice for EEG-based emotion"
        },
        {
          "TABLE I": "recognition since\nthe\ninput\nis quite different\nfrom images."
        },
        {
          "TABLE I": "The results of DBN shows\nthat DBN outperforms ResNet"
        },
        {
          "TABLE I": "series but not as good as MLPs. Among these results,\nthe"
        },
        {
          "TABLE I": "highest accuracy is 0.7941 with RSC method using MLP-"
        },
        {
          "TABLE I": "4 baseline,\nthe best DG method is Mixup with an average"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.8\n0.8\n0.8\n0.8": "0.75\n0.75\n0.75\n0.75\n0.7\n0.7\n0.7\n0.7"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.65\n0.65\n0.65\n0.65"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.6\n0.6\n0.6\n0.6"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.55\n0.55\n0.55\n0.55"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "(a.1)\n(a.2)\n(a.3)\n(a.4)"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "(a) Mixup"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.8\n0.8\n0.8\n0.8"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.75\n0.75\n0.75\n0.75"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.7\n0.7\n0.7\n0.7"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.65\n0.65\n0.65\n0.65"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.6\n0.6\n0.6\n0.6"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "(b.1)\n(b.2)\n(b.3)\n(b.4)"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "(b) MMD\n0.8\n0.8\n0.8\n0.8"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.75\n0.75\n0.75\n0.75"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.7\n0.7\n0.7\n0.7"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "0.65\n0.65\n0.65\n0.65\n0.6\n0.6\n0.6\n0.6"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n10\n30\n50\n70\n90\n(c.1)\n(c.2)\n(c.3)\n(c.4)"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "(c) CORAL"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "Fig. 2.\nThe inÔ¨Çuences of epoch and batch size on SEED dataset with three"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "representative DG methods (Mixup, MMD, CORAL). Gray bar stands for"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "batch size of 8, green bar stands for batch size of 16, and yellow bar stands"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "for batch size of32. (*.1) is MLP-2 baseline, while (*.2) is MLP-3, (*.3) is"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "MLP-4, and (*.4)\nis DBN."
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "on all baselines of 0.7117,\nand the best baseline\nis MLP-"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "4 with an average on all DG methods of 0.7768. We also"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "investigate the inÔ¨Çuences of epoch and batch size on three"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "representative DG methods\n(Mixup, MMD, CORAL) with"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "four outstanding models\n(MLP-2, MLP-3, MLP-4, DBN),"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "the results are shown in Fig. 2."
        },
        {
          "0.8\n0.8\n0.8\n0.8": "In general,\nthese results indicate that DG algorithms com-"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "bined with suitable baselines can largely reduce the individ-"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "ual differences. The DG algorithms have broad application"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "prospects\nin EEG-based tasks and provide a new direction"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "for\nthe broader application of EEG-based tasks."
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "The main\ninsufÔ¨Åciency\nof\nthis work\nis\nthat we\nonly"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "consider deep learning methods\nthat have been used more"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "often in recent years and do not systematically test\nthe DG"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "algorithms on traditional classiÔ¨Åers\nsuch as\nsupport vector"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "machines and linear discriminative analysis. When the data"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "amount of source is small, traditional models may have better"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "results. This is the next step of our work plan."
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "IV. CONCLUSIONS"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "In this paper, we evaluate six representative domain gener-"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "alization methods on SEED dataset with deep baselines,\ni.e.,"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "CNN, DBN and MLP. Based on experimental evaluations,"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "we Ô¨Ånd\nthat\nthe well-tuned MLP can\nreach\nan\naccuracy"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "of 79.41% with RSC method when using no target data."
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "Besides, all\nthe results\nshow that DG algorithms combined"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "with speciÔ¨Åc baselines have the ability to achieve prominent"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "effects on EEG-based emotion recognition for new users. The"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "benchmarked DG method seems a promising routine towards"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "zero-training emotion recognition models.\nIt\nis\nsuitable\nto"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "the\npractical\nscenario\nof\naffective BCIs\nand\nserves\nas\nan"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "inspiration and reference for\nsubsequent works concerning"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "transfer\nlearning."
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "REFERENCES"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "[1] M. Valstar,\nJ. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. Tor-"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "res Torres, S. Scherer, G. Stratou, R. Cowie, and M. Pantic, ‚ÄúAvec"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "2016: Depression, mood,\nand\nemotion\nrecognition workshop\nand"
        },
        {
          "0.8\n0.8\n0.8\n0.8": ""
        },
        {
          "0.8\n0.8\n0.8\n0.8": "of\nthe\n6th\ninternational workshop\non\nchallenge,‚Äù\nin Proceedings"
        },
        {
          "0.8\n0.8\n0.8\n0.8": "audio/visual emotion challenge, pp. 3‚Äì10, 2016."
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "2",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Non-invasive braincomputer interface system: towards its application as assistive technology",
      "authors": [
        "F Cincotti",
        "D Mattia",
        "F Aloise",
        "S Bufalari",
        "G Schalk",
        "G Oriolo",
        "A Cherubini",
        "M Marciani",
        "F Babiloni"
      ],
      "year": "2008",
      "venue": "Non-invasive braincomputer interface system: towards its application as assistive technology"
    },
    {
      "citation_id": "4",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "5",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "7",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "8",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence, sn"
    },
    {
      "citation_id": "9",
      "title": "Meernet: Multi-source eeg-based emotion recognition network for generalization across subjects and sessions",
      "authors": [
        "H Chen",
        "Z Li",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "10",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "11",
      "title": "Generalizing to unseen domains: A survey on domain generalization",
      "authors": [
        "J Wang",
        "C Lan",
        "C Liu",
        "Y Ouyang",
        "T Qin"
      ],
      "year": "2021",
      "venue": "Generalizing to unseen domains: A survey on domain generalization"
    },
    {
      "citation_id": "12",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "13",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "14",
      "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "authors": [
        "S Sagawa",
        "P Koh",
        "T Hashimoto",
        "P Liang"
      ],
      "year": "2019",
      "venue": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "arxiv": "arXiv:1911.08731"
    },
    {
      "citation_id": "15",
      "title": "The nature of statistical learning theory",
      "authors": [
        "V Vapnik"
      ],
      "year": "1999",
      "venue": "Springer science & business media"
    },
    {
      "citation_id": "16",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "18",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "Integrating structured biological data by kernel maximum mean discrepancy",
      "authors": [
        "K Borgwardt",
        "A Gretton",
        "M Rasch",
        "H.-P Kriegel",
        "B Sch√∂lkopf",
        "A Smola"
      ],
      "year": "2006",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "20",
      "title": "Self-challenging improves cross-domain generalization",
      "authors": [
        "Z Huang",
        "H Wang",
        "E Xing",
        "D Huang"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "21",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "22",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "23",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}