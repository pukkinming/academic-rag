{
  "paper_id": "2508.07625v1",
  "title": "A Trustworthy Method For Multimodal Emotion Recognition",
  "published": "2025-08-11T05:08:31Z",
  "authors": [
    "Junxiao Xue",
    "Xiaozhen Liu",
    "Jie Wang",
    "Xuecheng Wu",
    "Bin Wu"
  ],
  "keywords": [
    "Multimodal deep learning",
    "Emotion recognition",
    "Trusted deep learning",
    "Trusted evaluation criterion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing emotion recognition methods mainly focus on enhancing performance by employing complex deep models, typically resulting in significantly higher model complexity. Although effective, it is also crucial to ensure the reliability of the final decision, especially for noisy, corrupted and out-of-distribution data. To this end, we propose a novel emotion recognition method called trusted emotion recognition (TER), which utilizes uncertainty estimation to calculate the confidence value of predictions. TER combines the results from multiple modalities based on their confidence values to output the trusted predictions. We also provide a new evaluation criterion to assess the reliability of predictions. Specifically, we incorporate trusted precision and trusted recall to determine the trusted threshold and formulate the trusted Acc. and trusted F1 score to evaluate the model's trusted performance. The proposed framework combines the confidence module that accordingly endows the model with reliability and robustness against possible noise or corruption. The extensive experimental results validate the effectiveness of our proposed model. The TER achieves state-of-the-art performance on the Music-video, achieving 82.40% Acc. In terms of trusted performance, TER outperforms other methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511 and 0.9035, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a crucial role in artificial intelligence, enabling machines to move beyond rigid scripts and respond more effectively to human emotions. Generally, emotion recognition methods can be categorized into two types. The first type is based on physiological signals, including electroencephalography (EEG)  [1]  , electromyography (EMG)  [2]  , and electrocardiography (ECG)  [3]  . The second type is based on digital information, such as facial expression images  [4] [5] [6] [7]  , body gestures  [8]  , and speech signals  [9]  . As a data format that contains multiple types of digital information, videos provide a more comprehensive and accurate means of conveying emotion information, making them well-suited for research in emotion recognition. With the increasing availability of video datasets and the rapid development of video processing techniques, there has been a growing interest in analyzing emotion in videos.\n\nIn the field of emotion recognition, common multimodal methods typically model multimodal data by integrating multimodality information with deep neural networks through feature fusion, facilitating emotion classification.\n\nHowever, this approach often overlooks the inherent uncertainties in emotion datasets. To this end, we introduce the concept of confidence to evaluate model performance more accurately from the trusted model  [12]  . Specifically, we replaced the original information fusion module with the Confidence Module and the Belief Combination Module. In their research, Han et al.  [12]  focused on mathematically evaluating the confidence of predictions and adopted a confidence-based fusion method for multimodal recognition results.\n\nFurthermore, while previous research  [12]  has explored Big Data Mining and Analytics, xxxxxxx 20xx, x(x): xxx-xxx confidence methods, there remains a lack of credible evaluation metrics, limiting the research and application of confidence methods. Building on this, we combined the concept of trustworthiness with binary classification criteria  [31]  to propose a trusted method for loss calculation and several trusted evaluation metrics.\n\nIn real-life scenarios, models may generate incorrect predictions due to the high ambiguity of the data. Therefore, it is essential to compute prediction results, perform loss calculation, and evaluate the effectiveness of classification results under high-confidence conditions. To this end, we compared the model training process using the proposed loss method with traditional loss methods on two datasets and evaluated the model performance, including classification and trusted performance as evaluation metrics. The results demonstrated the effectiveness of the proposed loss method in trusted model training and the rationality of the proposed evaluation method. To summarize, our significant contributions are threefold:\n\n• Constructed a Trusted Emotion Recognition(TER) model, a confidence assessment method is also introduced, and the fusion of multimodal recognition results is realized based on this module. The model has achieved classification performance close to the state-of-the-art methods on two datasets.\n\n• Optimized the training method of the model by incorporating confidence assessment into the loss calculation. This can ensure that the model focuses more on the confidence of the results during the training process while maintaining classification performance and improving trusted performance.\n\n• Proposed a trusted evaluation criterion, which incorporates confidence into assessing the model's predictions. This criterion also provides guidelines for selecting a trusted threshold, supporting the application of trusted results. The proposed emotion recognition model has achieved state-ofthe-art trusted performance on two datasets.\n\nThe remaining sections of this paper are organized as follows. Section 2 contains a brief survey of related works. The details of the proposed framework, loss calculation method and trusted evaluation criterion are given in Section 3. Section 4 conducts the ablation experiments of the multimodal model and the loss method, compares the performance with existing models and verifies the generalizability of our proposed model. Finally, Section 5 presents concluding comments and future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "First of all, we discuss the works on utilizing images and audio for emotion recognition and highlight some common challenges. Subsequently, we present studies in deep learning on confidence estimation, including the methods which can be adapted.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "It is a common approach to conduct emotion recognition by extracting image features and audio features. Classical studies  [13, 14]  conducted emotion recognition based on ResNet and VGG models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition Challenges",
      "text": "Based on the existing model structures, we can observe that the primary challenge lies in the temporal dimension. ResNet and VGG are 2D convolutional neural networks that primarily focus on spatial information. However, the most significant difference between video and image is the temporal information. Although the RNNs could extract temporal information, their convolutional kernels cannot extract spatial information. Therefore, the current challenge is constructing a deep model that simultaneously extracts spatio-temporal information from video and audio.\n\nOn the other hand, when using multimodal information, the appropriate fusion method plays a crucial role in enhancing model performance. Compared to early fusion methods, late fusion methods  [15] [16] [17]  can better leverage the excellent feature extraction capabilities of single-modal methods. In these studies, researchers deploy different methods to provide fixed fusion weights to the models. However, the information weights of different modalities in various datasets are disparate, and fixed weight methods can only fit the weights of the overall data while neglecting the outliers. Therefore, adaptive fusion methods are considered the optimal solution for late fusion, and current research mainly focuses on addressing this challenge.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition Methods",
      "text": "With the rapid growth and success of deep learning applications in the past decade, there has been a substantial increase in the number of methods employed to conduct emotion recognition. From emotion recognition in natural language processing (NLP), RNNs are introduced to conduct emotion recognition. Subsequently, CNN-based methods are adopted for affective image content analysis(AICA)  [18]  , which is a commonly used approach in facial emotion recognition. In affective video content analysis (AVCA), Xue et al.  [49]  have introduced advanced methods developed over the past decade for addressing video feature extraction, expressing subjectivity, and multimodal feature fusion.\n\nThe Transformer has gained attention from various research fields due to its outstanding performance and has been combined with methods in various fields to address challenges. The Swin Transformer  [19]  enhanced the feature extraction capability of models by introducing attention mechanisms into twodimensional convolutional networks, achieving state-of-theart results in multiple image classification tasks. Wang et al.  [20]  applied this method to solve AICA and outperform the original models.\n\nThe improvement in performance by incorporating attention mechanisms into three-dimensional convolutional networks has also attracted the interest of more researchers.\n\nLee et al.  [21]  proposed an AVCA architecture that combines facial emotion recognition and attention mechanisms. Recent developments in multimodal emotion recognition include the Frame-SCN framework  [50]  , which utilized framelet transformations to decrease text dependency and boost accuracy.\n\nThe GM2RC model  [51]  enhanced analytical precision through intra-modal refinement and intermodal complementation. Meanwhile, FrameERC  [52]  employd graph framelet transformations and a dual-reminder fusion mechanism, effectively capturing emotional nuances and amplifying the impact of non-textual modalities, leading to superior performance in ERC tasks.\n\nIn multimodal research, researchers have also recognized the impact of attention mechanisms.\n\nApplying attention mechanisms to the fused information enhances the capability of feature extraction. Gupta et al.  [22]  demonstrated the powerful performance of this approach in their proposed model. On the other hand, Priyasad et al.  [17]  applied attention mechanisms to the late fusion process, adjusting the fusion weights during training to achieve optimal performance. Han et al.  [12]  also designed a confidence module based on numerical features of prediction results and proposed an adaptive fusion method that uses confidence as fusion weights. Compared to fixed weight methods, this specific approach, which assigns different fusion weights for different groups, has shown superior performance in multiple performance metrics.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Trusted Result",
      "text": "Deep learning has been widely recognized and has achieved remarkable performance in research fields such as computer vision, natural language processing, and data mining. It has also been successful in practical applications such as object detection, speech recognition, medical diagnostics, and financial fraud detection.\n\nHowever, the inherent lack of interpretability and theoretical support in deep learning methods provided insufficient explanations for the results they produce. Consequently, the application of deep learning has been greatly limited, especially in risk-sensitive scenarios where it has yet to gain widespread practical use. Analyzing the mathematical characteristics of the likelihood-based results generated by deep learning methods has become a significant direction in studying the confidence of deep learning outcomes.\n\nJun Deng et al. proposed to use confidence to evaluate the recognition results of machine learning methods  [45]  , and proposed a CM for a SER system based on semi-supervised learning  [46]  . Research on confidence in deep learning initially stemmed from the work of Guo et al.  [23]  . They proposed to utilize confidence calibration to constrain the confidence of model predictions within an optimal range. The confidence calibration was analyzed for various state-of-the-art models in the computer vision and natural language processing domains across different datasets. Experimental results indicated that the confidence of most current deep learning models tends to be overconfident, with predicted confidence levels exceeding their accuracy. Since then, extensive research has been conducted on the confidence of deep learning models. Mukhoti et al.  [24]  attributed the poor confidence calibration to over-parameterization in deep neural networks, enabling models to memorize the entire training set and maximize confidence for all samples. However, research by Bai et al.  [25]  demonstrated that even the simplest logistic regression models still exhibited overconfidence in datasets, revealing that the confidence calibration ability of models is independent of the number of model parameters. Wang et al.  [26]  focused on practical applications and introduced various methods for confidence calibration in deep models and graph neural networks. Yang Li et al.  [47]  applied the confidence approach on the speech emotion recognition task and proposed EmoConfidNet to obtain excellent model performance.\n\nResearch on confidence in multimodal models primarily focused on the confidence of the final results, employing similar methods for confidence calibration as those used in single-modal models. Han et al.  [12]  proposed a late fusion method based on confidence in their research. The distinct feature of this method was that Han et al. assessed the confidence of different singlemodal results, utilizing these confidences as fusion weights to achieve confidence fusion. They redesigned the confidence module by modeling the confidence of the model results using the Dirichlet distribution. This approach provided confidence that reflected the uncertainty of the model's outputs. Dempster-Shafer evidence theory was introduced for confidence fusion, resulting in fused outcomes that incorporated uncertainty. This fusion method possessed solid theoretical support and a set of adaptive fusion methods based on confidence. Extensive experiments have demonstrated the advantages of this fusion method, validating its accuracy, robustness, and effectiveness.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section, we address a key limitation of conventional multimodal emotion recognition methods, which often neglect the uncertainty inherent in multimodal emotion datasets. To overcome this, we extend the existing trusted model  [12]  to the field of emotion recognition and introduce the Trusted Emotion Recognition (TER) model for affective video content analysis (AVCA). Furthermore, previous trusted models  [12]  did not consider the trustworthiness of loss functions and evaluation metrics. As a result, we present a comprehensive explanation of the loss function method and trusted evaluation criteria developed specifically for this model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Trusted Emotion Recognition Model",
      "text": "In previous work, decision-making typically utilized traditional multimodal information fusion approaches. Although multimodal models using information fusion can provide accurate classification outcomes, they often result in unreliable predictions, especially when there is a conflict between the recognition results of the two modalities. Our method builds upon decision level fusion (late fusion) techniques. Traditional decision level fusion methods integrate different classification results using fixed weights, often overlooking the uncertainties inherent in decision-making. In contrast, our approach considers the confidence of each classification result, dynamically adjusting the weights of these results. Compared to traditional decision level fusion methods, our method achieves higher Trusted accuracy, indicating improved prediction precision under high confidence levels. Therefore, we propose a new confidence-based multimodal architecture for performing affective video content analysis.\n\nWe propose the Trusted Emotion Recognition (TER) model, which consists of four main modules, as shown in Fig.  1 . The first module is the Video Swin-Transformer Module  [29]  , which extracts emotion-related features from the video frame sequence and performs emotion classification for the video modality. The second module is the Multi-VGGish Module  [30]  , which extracts emotion-related features from the audio and performs emotion classification for the audio modality. The third module is the Confidence Module, which measures the confidence of each modality's classification results. The fourth module is the Combining Beliefs Module, which fuses the classification results of each modality based on their confidence to obtain the final multimodal emotion recognition predictions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Video Swin-Transformer Module",
      "text": "Visual information is the primary way humans receive information, encompassing rich emotional expressions such as facial expressions and body movements. However, traditional 2D convolutional methods, such as MAE  [27]  and MAGE  [28]  , tend to lose important temporal features when processing images. To this end, we select frame sequences containing spatial and temporal information in the proposed multimodal architecture as the source of image modality features. We introduce the Video Swin-Transformer  [29]  with the specific attention mechanism to extract spatio-temporal features from frame sequences.\n\nThe structure of the Video Swin-Transformer is illustrated in Fig.  2 . First, the video frame sequence undergoes Patch Partition and Linear Embedding, which divide input into a series of spatiotemporal blocks and embed them. The feature extraction module reduces dimensionality and extracts features through four stages. The Swin Transformer Block employs an attention mechanism within each stage to extract spatio-temporal features from the input. Compared to traditional Transformer models, this block restricts the attention calculation within a window and introduces a 3D relative positional bias matrix\n\nfor each head. The attention calculation equation is as follows:\n\nwhere Q, K, V ∈ R P M 2 ×d are the query, key, and value matrices, respectively; d is the dimension of query and key features. P, M, M represent the size of the 3D window, and",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Vggish Module",
      "text": "Audio is the primary way humans convey information, using language to express ideas and emotions, such as relaxed humming, joyful cheers, angry shouts, fearful screams, and sorrowful sighs. The mature audio feature extraction method VGGish  [30]  has achieved remarkable performance on various kinds of datasets. We employ this model as the baseline for the audio module and construct our Multi-VGGish module that can extract the spatio-temporal features of audio, enabling the classification of multiple seconds of input data.\n\nThe structure of Multi-VGGish is shown in Fig.  3 . In this module, the input is a sequence of mel-spectrograms, which are sliced along the time dimension and fed into the VGGish submodules. After feature extraction in each VGGish sub-module, the extracted features are inputted into the feature fusion submodule and combined along the time dimension. To robustly handle the input, we analyze the data format in the dataset and pad the data that does not meet the time dimension requirements. Therefore, before classification, we add a max pooling layer along the time dimension to reduce dimensions and eliminate the impact of padding data.\n\nSubsequently, we employ a fully connected layer to classify the features, and the structure of the classifier is shown in Fig.  3  (b).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Confidence Module",
      "text": "We need a unimodal confidence method to achieve confidence-based multimodal fusion. Although the output of softmax is often considered as the prediction confidence, existing research has shown that such confidence can lead to overconfidence issues. To obtain unimodal confidence value, Han et al.  [12]  replaced the softmax layer of the model with a softplus layer and obtained the Dirichlet distribution for the model's classification outputs α = {α1, . . . , αC } using the following method, C denotes the total number of classes, and e represents the evidence.:\n\nbased on the proposed confidence theory, the belief mass b and confidence value u are calculated as follows:\n\nThe confidence value u can also be understood as the uncertainty mass. u1 represents the uncertainty of the visual information, and u2 represents the uncertainty of the audio information. The lower the uncertainty, the higher the confidence of the prediction. We implement our confidence module based on this theory. Different from the multimodel fusion method, we combine the belief mass b with the confidence value u as the module's trusted results, as shown in the Confidence module of Fig.  1 . In Fig.  1, for b m  n , the superscript m represents different modalities, where 1 refers to visual information and 2 refers to audio information. The subscript n indicates the various categories in emotion classification. Through the confidence module, the uncertainty u of the results from different modalities is finally obtained.\n\nIn model performance evaluation, confidence is the core of our model, directly participating in calculating various metrics and demonstrating the effectiveness of a trusted multimodal model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Combining Beliefs Module",
      "text": "According to the uncertainty and evidence theory proposed by Han et al.  [12]  , we can obtain unimodal results composed of classification evidence and uncertainty. When fusing these results, we need to use the Evidence Theory. Based on the DS-Combination Rule proposed by Han et al.  [12]  , the fusion of classification results follows the below rules:\n\nwhere k = i̸ =j b 1 i b 2 j is a measure of the amount of conflict between the two mass sets, and the scale factor 1 1-k is used for normalization. Conventional decision-level fusion methods typically use fixed weights to select the analysis results from visual and audio modalities. In contrast, our method integrates uncertainty u into the fusion process using a simplified Dempster-Shafer combination rule. Refer to the Combining Beliefs module in Fig.  1 . This allows for dynamic adjustment of weights across different modalities, better handling inconsistencies between the recognition results of the two modalities, and improving recognition accuracy. The belief mass b and confidence value u are combined into the trusted result.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Trusted Cross-Entropy Loss",
      "text": "We work on obtaining trusted multimodal classification results by joint training neural networks. In traditional neural network classifiers, cross-entropy loss is widely deployed.\n\nIn our model, the output predictions include the belief mass bi, which can be understood as the probability of each class, and ui, which represents the confidence value for the other Fig.  3  The overall illustration of the audio stream in our proposed TER model. Specifically, the prediction of VGGish  [30]  is input to the Fully Connected Classification to output the final predictions. (\n\nConsidering the influence of different modalities during multimodal training, we adopt a multi-task strategy to collect results from all modalities to improve fusion performance. We employ three loss functions to jointly train the TER model in an end-to-end manner:\n\nwhere L v t ce represents the loss for the video branch, L a t ce represents the loss for the audio branch, and L c t ce represents the loss for the fusion results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Trusted Evaluation Criterion",
      "text": "Common evaluation metrics such as accuracy, precision, and F1 score do not take into account the concept of confidence, which can lead to unreliable predictions when modality data is ambiguous. Similarly, when humans classify objects, they also assess the credibility of the classification results. By employing a trusted model that utilizes Trusted Accuracy, it is possible to compute the predictive accuracy under conditions of high confidence, making this approach practically applicable. In the inference process, our proposed evaluation metrics include not only the evaluation metric results but also the uncertainty associated with these results. This uncertainty is compared to a confidence threshold; if the calculated uncertainty is below this threshold, the result is deemed reliable; otherwise, it is considered unreliable. Therefore, we have defined trusted evaluation criteria suitable for trusted models in this subsection. These evaluation criteria will be validated through experiments, demonstrating the superiority of trusted models compared to regular models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Trusted Classification",
      "text": "For classification tasks, the evaluation is typically limited to the correctness of the classification results. However, we consider the classification results and the confidence in the trusted results. When evaluating a trusted model, it is essential to assess these two aspects comprehensively to obtain a more comprehensive evaluation. The trusted results can be divided into true or false classifications, while the confidence of trusted results can be categorized as high or low. Then we consider both aspects and utilize a confusion matrix in Table  1  to represent all trusted results. In this table, HT denotes a trusted result that is correctly classified with high confidence, LT denotes a trusted result that is correctly classified with low confidence, HF denotes a trusted result that is incorrectly classified with high confidence, and LF denotes a trusted result that is incorrectly classified with low confidence. N denotes the total number of samples. In the subsequent evaluation criteria, we will deploy this evaluation result for representation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Trusted Threshold",
      "text": "There are multiple methods to determine the correctness of classification results. When the number of classes is small, the highest probability label consistent with the true label can be used as the criterion. However, when the number of classes is large, Table  1  The confusion matrix we proposed in the trusted classification.\n\nit is common to use whether the true label is included in the top-K classification probability results as the criterion, where K is a small number (e.g., TOP 5). In our experiment, the classes in the dataset are few (less than ten), so we will use the first criterion, whether the highest probability label is consistent with the true label, as the judgment basis.\n\nDetermining the confidence threshold (the degree of uncertainty) requires more consideration and analysis. The trusted results generated by the model only include a confidence value, and the threshold cannot be determined through internal comparison.\n\nThe study of binary classification results  [31]  provides some insights: different classification thresholds for the same model will result in different classification performances. Setting a reasonable classification threshold is necessary to obtain the best binary classification performance. In the trusted results, changes in the confidence threshold will also affect model performance. Therefore, we introduce the concepts of precision and recall to evaluate the performance of trusted results and determine a reasonable confidence threshold by balancing these two indicators, thereby achieving excellent model performance. As the trusted threshold increases, the confidence value u increases, while the trusted precision increases and the trusted recall decreases. Trusted precision (TP) represents the ratio of correctly classified results in high-confidence predictions, which is defined as:\n\nwhere trusted recall (TR) represents the ratio of high-confidence results in the correct predictions, which is defined as:\n\nWe can comprehensively assess the model's performance on trusted results by calculating trusted precision and trusted recall.\n\nAccording to the analysis of Eq.(  7 ) and Eq.(  8 ), we can deduce the relationship between the confidence threshold and trusted precision and trusted recall. When the confidence threshold increases, HT, HF, and HT + HF will decrease, LT will increase, and HT + LT will remain unchanged. In this case, trusted precision may fluctuate slightly, but trusted recall will significantly decrease. Conversely, when the confidence threshold decreases, HT, HF, and HT + HF will increase, LT will decrease, and HT + LT will remain unchanged. In this case, trusted precision may fluctuate slightly, but trusted recall will significantly increase. Excellent overall model performance requires balancing trusted precision and trusted recall, necessitating selecting a suitable confidence threshold. A set of data on the (TR, TP) based on confidence can be obtained Fig.  4  The Trusted P-R curves of video and audio branchs, which is constructed by the sets of (TR, TP) based on confidence.\n\nby adjusting different confidence thresholds. This data can be used to construct a Trusted P-R curve, as shown in Fig.  4 . To balance trusted precision and trusted recall, the y = x line was introduced, and the threshold corresponding to the intersection point with the P-R curve can be selected as the confidence threshold for the training model. The confidence threshold is an essential indicator for evaluating the performance of a trusted model. The lower the confidence threshold of a model, the higher its overall confidence.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Trusted F1 Score",
      "text": "In the common classification models, we can use the harmonic mean of precision and recall to measure the model performance. In the trusted model, trusted precision and trusted recall are equally essential performance metrics. Trusted precision and trusted recall are considered equally important in the trusted model, so we use a coefficient of 1 to average them harmonically. The definition of trusted F1 score is as follows:\n\nthe Trusted F1 score requires the model to achieve a balance between Trusted Precision and Trusted Recall, making it particularly useful for scenarios with imbalanced test samples.\n\nIt not only improves the recognition rate of samples under highconfidence conditions but also increases the proportion of highconfidence correct predictions among all correctly identified samples. A higher F1 score indicates better overall performance of the trusted model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Trusted Accuarcy",
      "text": "For classification models, the most basic performance evaluation metric is classification accuracy, which measures the ratio of correctly classified instances to the total number of test examples. However, for trusted models, we need to consider the impact of confidence on the classification results. Therefore, the concept of trusted accuracy is proposed, which measures the ratio of correctly classified instances with high confidence to the total Big Data Mining and Analytics, xxxxxxx 20xx, x(x): xxx-xxx number of instances classified with high confidence. The trusted accuracy is defined as follows:\n\nTrusted Accuracy = HT HT + HF , (10)   the trusted accuracy allows us to consider both classification accuracy and confidence when evaluating the performance of the trusted model. A higher trusted accuracy indicates that the classification results of the model have higher confidence.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "To evaluate the performance of the proposed TER model, we first evaluate the overall performance of the model using the trusted evaluation criterion proposed in Section 3.3. Additionally, we introduce the accuracy (Acc.) and F1 score to evaluate the classification performance of the TER model. In this section, we first introduce the datasets utilized for model training and testing, as well as the preprocessing methods and the training details of the model. Subsequently, we conduct ablation studies on TER model to validate the effectiveness of the multimodal methods, including the single-modal, fusion, and loss methods. Finally, we compare the performance of the proposed TER model and the confidence of the predictions with the state-of-the-art methods on two datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets And Implementation Details",
      "text": "IEMOCAP  [32]  is the earliest established multimodal emotion recognition dataset, which includes 10 individuals (5 males and 5 females) performing spontaneous dialogues based on predesigned scripts. These performances cover emotions from 10 topics such as neutral, happiness, sadness, etc. The dataset records multiple modalities of information including video, audio, facial motion capture, and textual transcriptions, with an average duration of 4.5 seconds for emotion segments. In our experiment, we selected data from six balanced classes, namely neutral, happy, sad, anger, frustration, and excited. Musicvideo  [33]  is a dataset composed of music videos, containing 3,438 emotional segments from different music videos. The average duration of each segment is 30 seconds. The dataset has coarsegrained discrete labels: exciting, fear, neutral, relaxation, sad, and tension. The recorded data includes both video and audio, and we utilized all available data formats provided by the dataset for model training.\n\nWe performed preprocessing for each data type in the datasets to get the consistent input format. For all datasets, we extracted a frame sequence containing 32 frames from the videos as the input for the video module. During preprocessing, we randomly sampled frames from the videos to ensure data diversity and differences between adjacent frames. We randomly selected a starting frame and added frames to the sequence at a certain interval (in our experiment, we selected one frame every four frames as input data). To meet the model input requirements, for video data, we cropped the frames and applied data augmentation strategies such as flipping and normalization. For audio data, both datasets provide the audio data as the input for the audio module. We extracted Mel spectrogram features from the audio files, with a dimension of 3 × 96 × 64 per second. The duration of the input data for the audio module is consistent with that of the video module. In our experiment, we accept 4 seconds of Mel spectrogram features as input features and padded them with 0 to address cases where the audio is shorter.\n\nThe proposed TER model is implemented in PyTorch  [34]  . We deploy the pre-trained parameters of Video Swin-Transformer and VGGish to initialize the model, and deploy the AdamW  [35]  optimizer with a cosine decay learning rate scheduler and 2.5 epochs of linear warmup. The inital learning rate is 1e-4. All models are trained on a Nvidia Tesla V100 with 32GB. Due to the memory constraint, we set the batch size to 8. To allow the model to learn global features and reduce the risk of overfitting, we use the delayed update strategy, where a backward pass and gradient update are performed every 4 batches. We would stop the training and report the best result when the training reached 30 epochs or the performance does not improve for more than 10 epochs.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Validity Of Submodules",
      "text": "To evaluate the effectiveness of each submodule in our proposed TER model, we designed two variant models:\n\n• Only Video: This model only utilizes the video and confidence modules without incorporating the audio module.\n\n• Only Audio: This model only utilizes the audio and confidence modules without incorporating the video module.\n\nThese variant models are trained in the onece training process as the TER model and generate outputs from both variant models and the TER model during testing. With this design, we can ensure the evaluation of each submodule's effectiveness within the same model and highlight the effectiveness of the multimodal.\n\nThe experimental results of the dissolution research are presented in Table  2 . The results indicate that the proposed TER model outperforms all the variant models in performance. This suggests that incorporating multimodal information in AVCA can enhance the overall model performance. We observe that on the IEMOCAP dataset, our TER model performs mostly better than the only video model but slightly lower in trusted accuracy. It could be attributed to the average duration of emotion segments provided by the dataset being 4 seconds, where shorter audio information cannot meet the training requirements of the model. During multimodal fusion, sufficiently trained video predictions dominate the final outcome, while insufficiently trained audio predictions negatively impact the performance. Our combining beliefs module utilizes the confidence value from each modality's results to suppress this negative impact and mitigate performance degradation. Incorrect results are assigned lower confidence values in the confidence module, whereas correct results are given higher confidence values. When combining the two results according to the DS-Combination Rule, the correct results carry a greater weight to ensure the final result's confidence. Measuring the overall trusted performance of the model should consider the Trusted F1 score. On the IEMOCAP dataset, the TER model's Trusted F1 score is significantly higher than all of the variant models, validating our analysis. The performance on the Musicvideo dataset also validates the effectiveness of the multimodal approach. The sub-modules that learn comprehensive features can mutually enhance each other during fusion, resulting in superior multimodal performance. The TER model outperforms both variant models across all performance metrics. With an average duration of emotion segments of 30 seconds in the Music-video dataset, the data augmentation scheme employed in our preprocessing becomes more effective. The model can learn more diverse feature information from different segments, which is the main reason for the superior performance of the TER model on this dataset.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Validity Of Fusion Module",
      "text": "To evaluate the effectiveness of the combining beliefs module, we designed two commonly used early fusion and late fusion methods as variant methods:\n\n• Early Fusion: This method concatenates the features extracted by the two submodules and then performs dimensionality reduction and classification using a classifier.\n\n• Late Fusion: This method directly combines the output results of the classifiers from the two submodules to obtain the final result.\n\nBoth variant methods utilize the same parameter initialization as the TER model. They are trained on the Music-video using a cosine decay learning rate scheduler and 2.5 epochs of linear warmup. During the testing phase, the output results of the variant methods are fed into the confidence module to obtain trusted results for evaluating the trusted performance of the variant methods.\n\nThe performance comparison of different fusion methods is shown in Table  3 . The experimental results demonstrate that our combining beliefs module outperforms both early fusion and late fusion. Compared to early fusion, our fusion method is more interpretable. Early fusion concatenates features and performs convolution, inheriting the lack of interpretability of deep models. On the other hand, our fusion method combines different classification results based on their confidence value, using the Dempster-Shafer theory to ensure the rationality and effectiveness of fusion. Confidence values are computed using the confidence module based on variational Dirichlet. Our fusion method shares similarities with late fusion in using weighting coefficients to combine multiple classification results but with a distinctive feature. Traditional late fusion methods often seek the weighting coefficients that optimize the overall performance. In contrast, our fusion method finds the most suitable fusion weights for each group of classification results based on their confidence values, resulting in dynamic fusion weights.\n\nTherefore, our fusion method addresses the limitations of existing fusion methods, offering stronger theoretical foundations and interpretability. The dynamic fusion weights enable us to find the optimal fusion strategy for each fusion, thus yielding superior classification performance and confidence compared to existing fusion methods.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Validity Of Trusted Ce Loss",
      "text": "To evaluate the effectiveness of the proposed Loss methods, we have designed commonly used loss calculation methods, as well as several variations incorporating confidence value:\n\n• CE: Calculating the cross-entropy loss.\n\nL( Ŷi; Yi) = -C c=1 yic log ŷic\n\n• Tan(Mul) Trusted: Combining confidence value with the tan function curve to enhance its impact on the results.\n\n• Tan(Add) Trusted: Combining confidence value with the tan function curve, controlling overfitting through addition.\n\n• Exp(Mul) Trusted: Combining confidence value with the exponential function curve to enhance its impact on the results. L( Ŷi, Ûi; Yi) = -C c=1 yic log ŷic + exp( ûi) These loss calculation methods, including the proposed Trusted CE, are trained and tested using an end-to-end TER model with the sum of losses from audio, video, and fusion results. We have performed training and testing on the Music-video dataset with sufficient data volume to evaluate the performance of these loss calculation methods.  It was terminated at 700 steps due to its poor performance. Trust CE achieves the highest accuracy as the number of steps increases.\n\n6. From Fig.  5  and Fig.  6 , it is evident that the Tan(Mul) Trusted method shows a rapid decrease in loss, but its accuracy does not significantly improve with an increase in training steps. However, Trusted CE ranks second in loss reduction, yet it exhibits a notable improvement in accuracy compared to other loss functions. Therefore, Trusted CE is the best method for loss calculation.\n\nThe performance comparison of different methods is presented in Table  4 . From the figures, we first notice that the Tan(Mul) Trusted method exhibits a rapid decrease in loss, followed by a stable trend, while the model's performance shows only minimal fluctuations without significant improvement. This outcome primarily attributes to the choice of the loss function. The curve of the tan function allows the model to fit the minimum loss value quickly, but its impact on model performance is limited, resulting in the obtained model falling short of being effective in classification. The Add Trusted, Tan(Add) Trusted, and Exp(Mul) Trusted methods exhibit similar trends in the training loss, with similar accuracy and final performance. We Table  5  The performance comparison of our proposed TER model and other state-of-the-art methods in terms of Acc., Macro F1, Weighted F1, Trusted Acc., and Trusted F1 on the IEMOCAP. Bold numbers indicate the best performance.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Method",
      "text": "Acc. Macro F1 Weighted F1 Trusted Acc. Trusted F1 bc-LSTM  [36]  0.5860 0.5725 0.5860 0.5612 0.5642 DialogueGCN  [37]  0.6057 0.5719 0.5868 0.5641 0.5742 DAG-ERC  [38]  0.6794 0.6682 0.6782 0.5838 0.5921 CIM  [39]  0.5693 0.5478 0.5614 0.5230 0.5233 COGMEN  [40]  0.6118 0.6097 0.6136 0.5521 0.5520 MMGCN  [41]  0.6562 0.6411 0.6536 0.5424 0.5415 TER(ours) 0.6014 0.5725 0.6007 0.6514 0.7511 observe that simply adding confidence value to the loss function affects the speed of loss decrease, and the range of loss variation in the classification part also decreases, leading to decreased information learned by the model and an inability to achieve excellent performance after stabilization. After observing the training curves of different calculation methods, we incorporate confidence value into the loss function using the cross-entropybased approach. From the training process, we can observe that our proposed Trusted CE exhibits faster loss reduction, lower final fitted loss, quicker improvement in model performance, and results in the TER model's best trusted performance.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Comparison With Sota Methods",
      "text": "This sub-section performs the AVCA on the IEMOCAP and Music-video datasets. The results on the validation sets for the proposed TER model compared with the state-of-the-art methods are shown as Table  5  and Table 6,   the training data is sufficient. However, when the training data is insufficient, the model fails to learn effective features. This observation is validated by the training results for the \"happy\" label, which exhibits significantly lower classification performance due to the limited amount of available data. Compared to the state-of-the-art model performance shown in Table  5 , these high-performing models utilize manually designed feature information as model inputs, whereas our TER model extracts features by the deep model. Under this premise, our proposed TER model achieves classification performance close to the SOTA methods, demonstrating its stronger feature extraction capability. On the other hand, we emphasize the model's trusted performance, which is an important innovation of our method. We make minor modifications to the SOTA models to evaluate their trusted performance. During testing, we change their softmax layer to the softplus layer and input its result into our confidence module to obtain trusted results, then evaluated for trusted performance. We can observe that our TER model achieves SOTA trusted performance in both trusted accuracy and trusted F1 score. We notice that the trust accuracy of all SOTA models is lower than their accuracy, and the same applies to their F1 scores. However, our TER Table  6  The performance comparison of our proposed TER model and other state-of-the-art methods in terms of Acc., Macro F1, Weighted F1, Trusted Acc., and Trusted F1 on the Music-video. Bold numbers indicate the best performance.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Method",
      "text": "Acc. Macro F1 Weighted F1 Trusted Acc. Trusted F1 CIM  [39]  0.4913 0.4787 0.4856 0.4828 0.4836 COGMEN  [40]  0.5223 0.5079 0.5232 0.5041 0.5040 MMGCN  [41]  0.5526 0.5497 0.5502 0.5156 0.5150 TER(ours) 0.8130 0.8231 0.8232 0.8240 0.9035 model not only achieves trusted performance equal to or even higher than its classification performance but also exhibits a significantly higher trusted F1 score than the F1 score. This can be attributed to our proposed Trusted CE Loss, which incorporates confidence value into the loss computation to ensure the model's trusted performance and prevent the model from overfitting to classification performance and neglecting trusted performance during model training. Our combining beliefs module also guarantees the confidence value of the fused results. These proposed trusted methods synergistically enhance the model's trusted performance, enabling TER to achieve SOTA trusted performance on the IEMOCAP dataset. Next, we demonstrate the best performance the TER model can achieve with sufficient data. The Fig.  8  shows the best classification confusion matrix of the TER model on the Music-video dataset. Although the Music-video dataset has fewer videos than IEMOCAP, each video has a longer duration, resulting in more diverse training samples after preprocessing. The TER model exhibits its powerful feature extraction capability and classification performance on the Music-video dataset, showing similar high classification performance across different labels. We compare the performance of the SOTA methods with the TER model by transferring them to the Musicvideo dataset. Since the Music-video dataset does not provide text information, we compare three multimodal models, disable the model's text feature input, and modify specific parameters to complete the model training. We extract the emotional features of the dataset as model inputs according to the requirements of each model using OpenSmile  [42]  . The detailed performance comparison of the proposed TER model and SOTA methods on the Music-video dataset are shown in Table  6 . First of all, we notice that the performance of the three SOTA methods all decreases due to the disabling of the text feature input. The loss of many effective features inevitably leads to a degradation in model performance. In contrast, our TER model remains unaffected and exhibits even better classification and trusted performance due to increased data volume. Therefore, a largescale dataset is indispensable if one wishes to train an endto-end deep model with high performance. Finally, through research on multimodal information and fusion rules, we find that multimodal information can enhance the trusted performance of the overall model. High-confidence modalities can compensate for the shortcomings of low-confidence modalities and improve the overall trusted performance of the model. This advantage of multimodal information in confidence is superior to single-modal information.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Improve Trusted Performence",
      "text": "This sub-section discusses the generality of the proposed Trusted CE Loss method. We have conducted comparative experiments on the IEMOCAP dataset to validate the effectiveness and generality of this method in improving trusted performance.\n\nWe apply the trusted improvement to each SOTA model to obtain the trusted-improved model. The trusted improvement includes model output transformation, calculating confidence value of predictions, and trusted loss calculation. First of all, we replace the softmax layer in the model output layer with a softplus layer and pass its output to the confidence module to obtain trusted results with a specific confidence value. Then, we calculate the trusted loss according to Eq.(  5 ) and perform model training through backpropagation. The trusted-improved model's results are tested for classification and trusted performance during model testing.\n\nThe performance comparison between SOTA and trustedimproved models is presented in Table  7 . Bold font indicates that the trusted improvement model achieved performance enhancement compared to the original model for that evaluation criterion.\n\nFirst, we observe that in terms of trusted evaluation metrics, all the models improved with trustworthiness outperform the original models, effectively validating the generality of our proposed method. We also compared the GFLOPs, parameters, and runtime of our method with those of the late fusion approach on the IEMOCAP dataset, as shown in Table  8 . In Table  8 , we found that while our method matches the late fusion approach in terms of GFLOPs and parameters, it requires slightly more training time (30 epochs). However, as shown in Table  7 , the performance improvement of our method is significant, with an average increase in trusted performance of 20%, further confirming the effectiveness of our approach. On the other hand, in terms of classification performance, trusted-improved models exhibit similar performance to the original models, and even the trust-improved models of DialogueGCN  [37]  and COGMEN  [40]  outperform the original models in classification performance, overall surpassing them. Our method enhances the overall performance of the models, ensuring the preservation of the classification performance while significantly improving the trusted performance.\n\nTable  7  The performance comparison of the state-of-the-art methods and their trusted ones in terms of Acc., Macro F1, Weighted F1, Trusted Acc., and Trusted F1 on the IEMOCAP. Bold numbers indicate better performance of the six existing methods and their trusted variants across six evaluation metrics.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Method",
      "text": "Acc. Macro F1 Weighted F1 Trusted Acc. Trusted F1 Trusted Threshold bc-LSTM  [36]  0.5860 0.5725 0.5860 0.5612 0.5642 0.5341 Trusted bc-LSTM 0.5798 0.5720 0.5826 0.8114 0.8210 0.4608 DialogueGCN  [37]  0.6057 0.5719 0.5868 0.5641 0.5742 0.6481 Trusted DialogueGCN 0.6081 0.5759 0.5870 0.6180 0.6280 0.4620 DAG-ERC  [38]  0.6794 0.6682 0.6782 0.5838 0.5921 0.6520 Trusted DAG-ERC 0.6615 0.6528 0.6619 0.7236 0.7245 0.4647 CIM  [39]  0.5693 0.5478 0.5614 0.5230 0.5233 0.5694 Trusted CIM 0.5619 0.5379 0.5414 0.6101 0.6155 0.4681 COGMEN  [40]  0.6118 0.6097 0.6136 0.5521 0.5520 0.5360 Trusted COGMEN 0.6420 0.6350 0.6438 0.7087 0.7048 0.4399 MMGCN  [41]  0",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "Despite the superior performance in emotion classification demonstrated by previous studies  [38, 41]  , their limitations in practical applications are evident. These models rely on manually designed emotion features as input, different from mainstream end-to-end neural networks that can automatically extract features from raw data. These models suffer from more severe performance degradation issues when dealing with data that does not include textual information. To address the challenges posed by existing models, we propose an end-toend framework called TER for AVCA. The TER model trains the emotion feature extractor and classifier in the end-to-end manner, eliminating the needs for manual feature extraction. When sufficient training data is available, TER exhibits superior classification performance compared to existing SOTA models. Even on the IEMOCAP dataset, TER achieves comparable performance to SOTA models. Therefore, the proposed TER model possesses excellent feature extraction and classification capabilities, and its end-to-end structure is more conducive to practical implementation and deployment.\n\nThe model's trusted performance is another significant contribution, and we realize a trusted deep learning framework by introducing the confidence method proposed by Han et al.  [12]  . Previous research has explored confidence methods but needed more effective evaluation criteria, resulting in limited research and application of confidence methods. This paper proposes a series of trusted evaluation criteria to address this issue and evaluates existing SOTA models and proposed TER model to showcase the advancement of trusted models. In existing methods, model training focuses solely on classification performance, leading to lower trusted performance. However, in the training process of TER, we incorporate confidence value into the loss calculation method, thereby balancing both aspects of performance. The Trusted CE Loss plays a crucial role in improving the confidence of existing methods, significantly enhancing trusted performance while maintaining classification performance. Trusted models, trusted CE loss, and trusted evaluation criterion constitute our trusted approach, and extensive ablation and comparative experiments validate its effectiveness and superiority.\n\nIn future work, larger datasets can be utilized to enhance model performance and generalization, such as the MELD dataset  [48]  , and better performance can be achieved on smaller datasets through transfer learning. Second, more modalities will be added because the flexible combining beliefs module will provide more evidence to improve classification and trusted performance. Finally, using the knowledge distillation method to reduce the parametric number of the model and investigate models with smaller parametric numbers is also an efficient solution.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Acknowledgment",
      "text": "The work of this paper is supported by the Key R&D Program of Zhejiang Province (No. 2023C01181).",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall illustration of our proposed TER model. The model consists of four main modules, which are Video Swin-",
      "page": 4
    },
    {
      "caption": "Figure 2: The overall illustration of the visual stream in our proposed TER model. (a) shows the architecture of Video Swin",
      "page": 5
    },
    {
      "caption": "Figure 1: This allows for dynamic",
      "page": 5
    },
    {
      "caption": "Figure 3: The overall illustration of the audio stream in our proposed TER model. Specifically, the prediction of VGGish[30] is",
      "page": 6
    },
    {
      "caption": "Figure 4: The Trusted P-R curves of video and audio branchs,",
      "page": 7
    },
    {
      "caption": "Figure 5: The loss comparison of our proposed Trusted CE and",
      "page": 10
    },
    {
      "caption": "Figure 5: , while the changes in accuracy are shown in Fig.",
      "page": 10
    },
    {
      "caption": "Figure 6: The performance comparison of our proposed",
      "page": 10
    },
    {
      "caption": "Figure 6: , it is evident that the Tan(Mul)",
      "page": 10
    },
    {
      "caption": "Figure 7: The confusion matrix of our proposed TER model on",
      "page": 11
    },
    {
      "caption": "Figure 8: The confusion matrix of our proposed TER model on",
      "page": 11
    },
    {
      "caption": "Figure 8: shows the",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\n2": ""
        },
        {
          "1\n2": ""
        },
        {
          "1\n2": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "2",
      "title": "A comparison of the affectiva imotions facial expression analysis software with EMG for identifying facial expressions of emotion",
      "authors": [
        "L Kulke",
        "D Feyerabend",
        "A Schacht"
      ],
      "year": "2020",
      "venue": "Front. Psychol"
    },
    {
      "citation_id": "3",
      "title": "Utilizing deep learning towards multi-Big Data Mining and Analytics, xxxxxxx 20xx, x(x): xxx-xxx modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition using facial expressions",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2017",
      "venue": "Procedia Comput. Sci"
    },
    {
      "citation_id": "5",
      "title": "Ease: Robust facial expression recognition via emotion ambiguity-sensitive cooperative networks",
      "authors": [
        "L Wang",
        "G Jia",
        "N Jiang",
        "H Wu",
        "J Yang"
      ],
      "venue": "Proc. 30th ACM Int. Conf. Multimedia, 2022"
    },
    {
      "citation_id": "6",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "7",
      "title": "D 2 S: Dynamic distribution supervision for multi-label facial expression recognition",
      "authors": [
        "L Wang",
        "X Zhang",
        "N Jiang",
        "H Wu",
        "J Yang"
      ],
      "year": "2022",
      "venue": "Proc. IEEE Int. Conf. Multimedia Expo"
    },
    {
      "citation_id": "8",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "F Noroozi",
        "C Corneanu",
        "D Kamińska",
        "T Sapiński",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "9",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "10",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "12",
      "title": "Trusted multi-view classification with dynamic evidential fusion",
      "authors": [
        "Z Han",
        "C Zhang",
        "H Fu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition with identity and emotion joint learning",
      "authors": [
        "M Li",
        "H Xu",
        "X Huang",
        "Z Song",
        "X Liu",
        "X Li"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "14",
      "title": "Eegbased emotion recognition for multi channel fast empirical mode decomposition using vgg-16",
      "authors": [
        "M Asghar",
        "M Khan",
        "Y Amin",
        "A Akram"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Eng. Emerging Technol"
    },
    {
      "citation_id": "15",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "16",
      "title": "Audio-visual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "17",
      "title": "Attention driven fusion for multi-modal emotion recognition",
      "authors": [
        "D Priyasad",
        "T Fernando",
        "S Denman",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "18",
      "title": "Facial emotion recognition: State of the art performance on fer2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on fer2013",
      "arxiv": "arXiv:2105.03588"
    },
    {
      "citation_id": "19",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "20",
      "title": "Twolevel attention with two-stage multi-task learning for facial emotion recognition",
      "authors": [
        "W Xiaohua",
        "P Muzi",
        "P Lijuan",
        "H Min",
        "J Chunhua",
        "R Fuji"
      ],
      "year": "2019",
      "venue": "J. Visual Commun. Image Represent"
    },
    {
      "citation_id": "21",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "22",
      "title": "An attention model for group-level emotion recognition",
      "authors": [
        "A Gupta",
        "D Agrawal",
        "H Chauhan",
        "J Dolz",
        "M Pedersoli"
      ],
      "year": "2018",
      "venue": "Proc. 20th ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "23",
      "title": "On calibration of modern neural networks",
      "authors": [
        "C Guo",
        "G Pleiss",
        "Y Sun",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "24",
      "title": "Calibrating deep neural networks using focal loss",
      "authors": [
        "J Mukhoti",
        "V Kulharia",
        "A Sanyal",
        "S Golodetz",
        "P Torr",
        "P Dokania"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Neural Inf"
    },
    {
      "citation_id": "25",
      "title": "Don't just blame overparametrization for over-confidence: Theoretical analysis of calibration in binary classification",
      "authors": [
        "Y Bai",
        "S Mei",
        "H Wang",
        "C Xiong"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "26",
      "title": "Be confident! towards trustworthy graph neural networks via confidence calibration",
      "authors": [
        "X Wang",
        "H Liu",
        "C Shi",
        "C Yang"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Neural Inf"
    },
    {
      "citation_id": "27",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "28",
      "title": "Mage: Masked generative encoder to unify representation learning and image synthesis",
      "authors": [
        "T Li",
        "H Chang",
        "S Mishra",
        "H Zhang",
        "D Katabi",
        "D Krishnan"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Con. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "29",
      "title": "Video swin transformer",
      "authors": [
        "Z Liu",
        "J Ning",
        "Y Cao",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "30",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "31",
      "title": "The relationship between precision-recall and roc curves",
      "authors": [
        "J Davis",
        "M Goadrich"
      ],
      "year": "2006",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "32",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "33",
      "title": "Deep learning-based late fusion of multimodal information for emotion classification of music video",
      "authors": [
        "Y Pandeya",
        "J Lee"
      ],
      "year": "2021",
      "venue": "Multimedia Tools Appl"
    },
    {
      "citation_id": "34",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "35",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. 3rd Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "36",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proc. 55th Annu. Meeting Assoc. Comput. Ling"
    },
    {
      "citation_id": "37",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proc. Conf. Empirical Methods Nat. Lang. Process"
    },
    {
      "citation_id": "38",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proc. 59th Annu. Meeting Assoc"
    },
    {
      "citation_id": "39",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proc. Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "40",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proc. Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "41",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation"
    },
    {
      "citation_id": "42",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "venue": "Proc. 18th ACM Int. Conf. Multimedia, 2010"
    },
    {
      "citation_id": "43",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "44",
      "title": "Speech emotion recognition based on attention weight correction using word-level confidence measure",
      "authors": [
        "J Santoso",
        "T Yamada",
        "S Makino",
        "K Ishizuka",
        "T Hiramura"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "45",
      "title": "Confidence measures for speech emotion recognition: A start",
      "authors": [
        "J Deng",
        "W Han",
        "B Schuller"
      ],
      "year": "2012",
      "venue": "Proc. Speech Commun. 10. ITG Symp"
    },
    {
      "citation_id": "46",
      "title": "Confidence measures in speech emotion recognition based on semi-supervised learning",
      "authors": [
        "J Deng",
        "B Schuller"
      ],
      "year": "2012",
      "venue": "Proc. Annu. Conf. Int. Speech Commun"
    },
    {
      "citation_id": "47",
      "title": "Confidence estimation for speech emotion recognition based on the relationship between emotion categories and primitives",
      "authors": [
        "Y Li",
        "C Papayiannis",
        "V Rozgic",
        "E Shriberg",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "48",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "49",
      "title": "Affective video content analysis: Decade review and new perspectives",
      "authors": [
        "J Xue",
        "J Wang",
        "X Liu",
        "Q Zhang",
        "X Wu"
      ],
      "year": "2024",
      "venue": "Big Data Mining and Analytics"
    },
    {
      "citation_id": "50",
      "title": "Multimodal graph learning with framelet-based stochastic configuration networks for emotion recognition in conversation",
      "authors": [
        "J Shi",
        "M Li",
        "Y Chen",
        "L Cui",
        "L Bai"
      ],
      "year": "2025",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "51",
      "title": "Gm2rc: Graph-based multitask modality refinement and complement for multimodal sentiment analysis",
      "authors": [
        "J Shi",
        "Y Chen",
        "S Zhou",
        "M Li"
      ],
      "year": "2024",
      "venue": "2024 7th International Symposium on Autonomous Systems (ISAS)"
    },
    {
      "citation_id": "52",
      "title": "Frameerc: Framelet transform based multimodal graph neural networks for emotion recognition in conversation",
      "authors": [
        "M Li",
        "J Shi",
        "L Bai",
        "C Huang",
        "Y Jiang",
        "K Lu",
        "S Wang",
        "E Hancock"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    }
  ]
}