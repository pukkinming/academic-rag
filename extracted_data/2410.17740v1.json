{
  "paper_id": "2410.17740v1",
  "title": "Emotion Recognition With Facial Attention And Objective Activation Functions",
  "published": "2024-10-23T10:14:37Z",
  "authors": [
    "Andrzej Miskow",
    "Abdulrahman Altahhan"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Attention",
    "Activation Functions",
    "VGGNet",
    "Resnet",
    "ResNetV2",
    "SEN-net",
    "ECA-Net",
    "CBAM"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we study the effect of introducing channel and spatial attention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN vision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial Emotion Recognition task. We show that not only attention can significantly improve the performance of these models but also that combining them with a different activation function can further help increase the performance of these models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The most recent breakthrough in emotion recognition is the idea of using attention to improve the accuracy of the deep learning model. The methodology behind visual-attention-based models was inspired by how humans inspect a scene at first glance.  [1]  has found that humans retrieve parts of the scene or objects sequentially to find the relevant information. Since neural networks attempt to mimic how the human brain works to complete the desired task, various methods were developed to imitate human attention. The discovery of these attention mechanisms helped improve the accuracy of emotion recognition models. In this work, we aim to discover the effect of introducing an attention mechanism to existing deep learning models to recognise facial expressions and how their performance can be further boosted via simple but effective changes to their architectures. Additionally, the new architectures will be further improved by modifying their activation functions from ReLU to ELU activation functions to solve the issue of bias shift. The paper proceeds as follows. In the next section, we present related work, while in section 3, we show the methodology, and in section 4, we show the results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attention",
      "text": "When processing a complex visual scene, human vision does not process the entire image at once. Instead, we tend to only focus on a subset of the image while ignoring the rest to speed up the visual analysis process. This process of selecting a subset of the input, and ignoring the rest, is referred to as attention  [2] . Attention can be divided into two independent categories  [3] : bottom-up unconscious (implicit) attention, referred to as saliency-based attention that operates on raw sensory input, and top-down conscious (explicit) attention, which refers to the deliberate allocation of attention to certain features.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Attention In Facial Expression Recognition Context",
      "text": "In computer vision, attention mechanisms can be treated as a selection process that weighs an input dimension and its features according to their importance to the task. Each dimension defines a different input domain that contributes to the task to a different extent. Furthermore, each portion of the input domain has varied importance to the task.\n\nFrom a deep learning perspective, attention can be infused into a CNN model by further distinguishing higher-level features from low-level features and assigning higher weights to crucial features, i.e., by attracting the model's attention to these features  [4] . From this perspective, attention mechanisms can be divided into three distinct categories; channel, spatial, and temporal attention. Additionally, these categories can be combined to form other hybrid attention mechanisms, namely: channel & spatial attention and spatial & temporal attention. Temporal attention will not be discussed as the work focuses on recognition from static images, not sequential data.\n\nSince the introduction of attention modules and their easy integration with CNN's models, researchers have switched their focus to using CNN classification with attention for FER applications. In  [5] , authors proposed using attention for FER tasks. They implemented spatial attention with a CNN and improved the model's performance by focusing less on irrelevant parts of the image and training the model on \"where\" to find the needed information. Additionally,  [6]  has explored using channel attention which defines \"what\" to look for in the model by placing a higher value on more informative features. The combination of spatial and channel attention for FER applications is envisaged to achieve state-of-the-art results.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We use three different CNN image processing models as our base model and add attention to them to boost their performances. The models that we use are VGGNet, Resnet, and ResnetV2. These models are considered a good fit for our problem due to their resilience to noise and ability to deal with degradation and vanishing gradient problems. Each one of these models has its strengths and weaknesses, and we want to study what happens when we add attention to them in the context of FER.\n\nIn addition, we vary the depth of these architectures to study the effect of different attention mechanisms on the depth of the architecture and whether they aggravate or alleviate some of the issues associated with the depth of the architecture. Furthermore, to make our study more comprehensive, we also study the effect of the activation function on these architectures when integrated with each attention mechanism.\n\nThis section starts by discussing the preprocessing stage that we adopted. Then we move to the activation functions and show a preliminary comparative study for a lab-based FER dataset, the CK+. We then discuss the different attention modules and conclude the section by conducting preliminary experiments on the reduction rate of the attention modules, again using the CK+ dataset. This section is followed by full-fledged experimental results that compare all the different architecture performances on the more challenging FER2013 dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Face Detection And Pre-Processing",
      "text": "We start by detecting the face in the image and removing the insignificant background pixels. Without this step, unwanted features in the image may be extracted and classified along with important information resulting in errors. Facial detection can be achieved using standard object detection methods. This paper uses a state-of-the-art facial detector built on top of the YOLO framework  [7] . YOLO was chosen due to its efficient one-stage object detection capability comparable to the performances of two-stage detectors while offering significantly better computational performance  [8] .\n\nThe default yolov5s weights were chosen due to their high performance and accuracy after experimenting with different weights on a subset of the dataset. More importantly, the original YOLO architecture was modified to ensure the output images had a fixed image size of 80 × 80 pixels. Since faces bounding boxes can have different proportions, cropped faces must be re-sized, so they all have the same size. This stage can be considered an external attention layer for our model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Activation Functions",
      "text": "ReLU activation function has helped to solve the vanishing gradient problem, and hence it was utilised by the architectures discussed earlier. This is because the gradients of the ReLU activation follow the identity function for positive arguments and zero otherwise, meaning that large gradient values are still used, and negative values are discarded. On the other hand, since ReLU is non-negative, it has a mean activation larger than zero. As a result, neurons with a non-zero mean activation act as a bias for the next layer causing a bias shift for the next layer. The shift in bias causes weight variance, leading to the activation function being locked to negative values, and the affected neuron can no longer contribute to the network learning. Consequently, two activation functions have been proposed that tackle the problem of bias shift differently while also solving the vanishing gradient problem.\n\nELU function was proposed that allows negative gradient values, resulting in the mean of the unit activations being closer to zero than ReLU. Like ReLU, ELU applies the identity function for positive values, whereas it utilises the exponential function if the input is negative. For this reason, ELU achieves faster learning, and significantly better generalization performance than ReLU on networks with more than five layers  [9] .\n\nSELU function  [10]  was proposed to solve the issue of bias-shift through selfnormalization. Through this property, activations automatically converge to a zero mean and unit variance. This convergence property makes SELU ideal for networks with many layers and further improves the ReLU activation function. From the results table, we can observe that ELU achieved the best accuracy on the ResNet-50 model on the CK+ dataset. This stems from the fact that SELU performs much better on models with many layers. In both cases, the change of the activation functions largely outperformed ReLU, which is utilised in most of the modern CNN architectures.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Attention Modules",
      "text": "Attention modules are designed to be integrated with CNN models to improve them further. First, we discuss how attention is implemented in each module, the benefits of each implementation, and possible improvements. Subsequently, we show how the attention modules integrate within the implemented CNN architectures.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sen-Net",
      "text": "The SEN-Net architecture was the first implementation of channel attention in computer vision tasks  [4] . The block improved the representational ability of the network by modelling the interdependencies between the channels of a convolutional layer. This is done through a feature re-calibration operation split into two sequential operations: squeeze and excitation.\n\nA set of experiments was conducted on the CK+ dataset using ResNet-50 as the backbone to find the optimal value of r in table 2. ECA-Net ECA-Net  [11]  was developed to improve channel attention used in SEN-Net. In SEN-net, the excitation module uses dimensionality reduction via two fully connected layers to extract channel-wise relationships. The channel features are mapped into a low-dimensional space and then mapped back, making the channel connection and weight indirect. Consequently, this negatively affects the direct connections between the channel and its weight, reducing the model's performance. Furthermore, empirical studies show that the operation of dimensional reduction is inefficient and unnecessary for capturing dependencies across all channels  [11] . The ECA-Net attempts to solve the issue of dimensionality reduction while improving the efficiency of the excitation operation by introducing an adaptive kernel size within its excitation operation.\n\nA 1D convolutional layer performs the excitation operation with kernel size k. The value of k is adaptively changed based on the number of channels. With this operation, ECA captures channel-wise relationships by considering every channel and its k neighbours. Therefore, instead of considering all relationships that may be direct or indirect, an ECA block only considers direct interaction between each channel and its k-nearest neighbours to control the model's complexity. Table  3  shows the effect of utilising a static value of k over the adaptive, confirming that the adaptive kernel size is the best option for FER applications. CBAM The last attention module implemented in this paper is the Convolutions Block Attention Module (CBAM)  [12] . CBAM proposed utilising both spatial and channel attention to improve the model's performance, unlike the previous attention modules, which only utilised channel attention. The motivation behind the CBAM stemmed from the fact that convolution operations extract informative features by cross-channel and spatial information together. Therefore, emphasising meaningful features along both dimensions should achieve better results.\n\nCBAM channel attention consists of squeeze and excitation operations inspired by the implementation of channel attention from SEN-Net  [4] . However, CBAM modifies the original squeeze operation from SEN-net to include average and max pooling to capture channel-wise dependencies. The idea behind utilising both pooling operations stems from the fact that all spatial regions contribute to the average pooling output, whereas max-pooling only considers the maximum values. Consequently, combining both should improve the representation power of relationships between channels. The two pooling operations are used simultaneously and are passed to a shared network consisting of two fully connected layers (W 1 and W 2 ), which perform the excitation operation (following the exact implementation from SEN-Net). After the output of each pooling operation is passed through the shared MLP, the resultant feature vectors are merged using element-wise summation.\n\nThe design of the CBAM spatial attention module follows the same idea as the CBAM channel attention module. To generate a 2D spatial attention map, we compute a 2D spatial descriptor that encodes channel information at each pixel over all spatial locations. This is done via applying average-pooling and max-pooling along the channel axis, after which their outputs are concatenated. This is because pooling along the channel axis effectively detects informative regions as per  [13] . The spatial descriptor is then passed to a convolution layer with a kernel size of 7, which outputs the spatial attention map. The choice of the large kernel size is necessary since a large receptive field is usually helpful in deciding spatially important regions. The output is passed through a sigmoid function to normalize the output.\n\nLike SEN-Net, the reduction ratio r allows us to vary the capacity and computational cost of the channel attention block, as shown in a set of experiments that we conducted on the CK+ and summarised in table  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Integration Of Different Attention Mechanisms With Different Deep Vision-Based Models",
      "text": "As mentioned, we integrate the three attention mechanisms discussed earlier with three types of vision-based deep learning architectures. The chosen attention modules are versatile and are designed to be easily integrated within CNN models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Integration With Vggnet",
      "text": "The creators of SEN-Net stated that the SE block could be integrated into standard architectures such as VGGNet by the insertion after the activation layer following each convolution. Through research in the classification of medical images, it was shown that authors had used three different ways to integrate attention in VGGNet: (1) placing attention as described by SEN-Net  [14] , (2) placing the attention module before the last fully connected layers  [15]  and (3) placing the attention modules at layers 11 and 14  [16] . Method 2 achieved the best performance for the emotion recognition task as shown in table  5 . Integration with ResNet Even though ResNet is a more complicated architecture, the creators of SEN-Net provided the most optimal way to integrate their block within the residual block, where the attention module is added before summation with the identity branch. Through research and experimentation, we did not find more optimal ways to integrate attention within ResNet; therefore, ECA-Net and CBAM followed the same integration method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fer Datasets",
      "text": "It is necessary to have datasets with emotions that are correctly labeled and contain enough data to train the model optimally. The human performance on this dataset is estimated to be 65.5%  [20] . Hence, it is widely used as a benchmark for emotion recognition models.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Of Cnn-Based Models With An Elu Activation Function",
      "text": "This section shows the results of applying the previously discussed CNN-based models with a different activation function, ELU. This is necessary to establish ground truth and isolate the effect of changing the activation function from adding attention (discussed in the next section). Table  6  displays the final evaluation accuracies of the CNN models on the three datasets. The evaluations for CK+ and JAFFE were executed three times to ensure the results' correctness; with smaller datasets, evaluation accuracies fluctuate between the runs. Out of the three executions, the highest value was chosen. Analysing the results, we see that VGG-19 achieved the best accuracy on CK+ and FER2013, while ResNetV2-50 achieved the best accuracy on the JAFFE dataset. This was an unexpected result as the initial assumption was that the deeper ResNet models should outperform VGGNet, which was not the case. We conclude that this is due to the modification of the activation function from ReLU to ELU in the CNN models. This change improved the VGG-19 accuracy from 87.91% to 90.66% on the CK+ dataset, significantly better than the deeper ResNet models for the same modification. This finding indicates that residual learning is not required to achieve good performance. Even simple architectures such as the VGGNet can achieve higher accuracy than a more complex architecture such as ResNet across different datasets by utilising the ELU activation functions. Furthermore, the deeper ResNet models consist of more parameters than VGG-19. Because deep CNNs are designed to be trained on large amounts of data, the layers at the deeper stages cannot learn informative features. Consequently, overfitting occurs, suggesting that shallower architectures are better for the given dataset. It is yet to be discovered whether a larger dataset would enhance the performance of the deeper architecture of ResNet.\n\nFrom the previous table, it can be seen that ResNet performed better than VGG on the smaller JAFFE dataset. To gain further insight into the baseline performances of the two ResNet architectures, we drill down more by comparing the relative training graphs of ResNetV1 and ResNetV2 on the JAFFE dataset in Figure  1 . Interestingly, the figures show that ResNetV2 performed significantly better than ResNet on the smallest JAFFE dataset. Original ResNet showed degradation in accuracy past depth 101 and could not increase training accuracy past depth 152 on the JAFFE dataset. On the other hand, ResNetV2 can still train on the deeper models, and the model of 50 layers performed better than the original ResNet. Relative graphs were chosen to separate the ResNet models as the larger models will have a longer computational time. Figure  1  shows that ResNetV2 converges to optimal values faster, and the performance degradation in the deeper layers is not as sudden as the original ResNet. From these results, we can conclude that the ELU activation function further enhanced the new residual blocks due to its ability to facilitate a better flow of information. This, however, should not be attributed only to the small size of the JAFFE dataset since the new improved residual blocks also performed consistently better on CK+ and FER2013 datasets.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Cnns With Different Attention Mechanisms",
      "text": "This section shows the results of augmenting the previously discussed CNNbased architectures with different attention mechanisms. Table  7  summarizes the experimental results. The networks with attention outperformed all the baselines significantly, demonstrating that attention can generalise well on various models. Moreover, the addition of attention showed performance improvement across the three studied datasets, displaying that attention could be applied to any problem size.\n\nFigure  2  shows the accuracy curves of the best-performing networks. In each case, attention achieves higher accuracies and shows a smaller gap between training and validation curves than baseline networks. As expected, CBAM had the best improvement in accuracy over the other attention modules due to the application of spatial attention. However, that comes at the cost of a significant overhead in parameters. On the other hand, ECA-Net achieved similar levels of performance increase compared to CBAM while not significantly impacting the memory requirement of each network. VGG19 still achieved the best performance on the CK+ and FER2013 datasets, while ResNetV2-50 achieved the best performance on the JAFFE dataset. However, the increase in performance was significantly higher than expected in the FER2013 dataset. Due to the size of the dataset, the expected improvement should have been 1-2% which is the improvement authors of CBAM received on the ImageNet dataset. However, CBAM achieved a performance increase of 3.15% on FER2013, displaying that attention modules can significantly impact the network's performance. Furthermore, the addition of CBAM enabled an increase of 6.55% on the JAFFE dataset, demonstrating the ability of attention modules to improve the network's generalisation ability. Additionally, the introduction of attention did not change the ranking order of the best-performing networks from the baseline CNN comparisons, emphasising the consistency of the expected boost in performance when the attention mechanism is added.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we studied the effect of infusing three different attention mechanisms, SEN-Net, ECA-Net, and CBAM, into three CNN-based deep learning architectures, namely the VGGNet, ResNet, and ResNetV2, with different depths to classify the seven basic human emotions on three datasets, namely CK+, JAFFE, and FER2013. In addition, we have replaced their internal activation function from RELU to ELU. As a result, there was a significant improvement in their performances. We studied the effect of changing the activation function first, then infused the resultant architectures with attention. We also showed that the new residual blocks presented in ResNetV2 perform significantly better than the original ResNet on smaller datasets and slightly improve on mid-sized and larger-sized datasets. Our results show that these amendments refined the extracted features and improved the generalisation capabilities of these models. The attention module hyperparameters were modified through experimentation to maximize the models' performance on emotion recognition tasks.\n\nOur work verified the attention mechanism's effect on the performance of CNNs. We have shown that each attention module outperformed the baseline models on each dataset. Consequently, attention modules could successfully improve the generalisation ability and refine the extracted features regardless of the problem size. Furthermore, our work confirmed that utilising ResNet V2 with attention modules yields better results than the original ResNet when attention modules and ELU are applied. In the future, we intend to conduct a comprehensive study on the effect of simplifying the transformation operations used in attention to speed up training time without losing competency.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Interestingly, the figures show that ResNetV2 performed significantly",
      "page": 9
    },
    {
      "caption": "Figure 1: shows that ResNetV2 con-",
      "page": 9
    },
    {
      "caption": "Figure 2: shows the accuracy curves of the best-performing networks. In each",
      "page": 11
    },
    {
      "caption": "Figure 2: Accuracy curves for the best performing models on the CK+(left),",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The Effect of Reduction Ratio Changes on the SEN-Net Attention",
      "data": [
        {
          "Reduction Ratio(r) #Parameters Accuracy": "33.56M\n28.53M\n26.02M\n24.76M"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: The Effect of Reduction Ratio Changes on the SEN-Net Attention",
      "data": [
        {
          "Kernel Size(k) #Parameters Accuracy": "33.56M\n23.50M\n23.53M\n23.59M\n23.62M\n23.65M"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Table 4: The Effect of the Change of Reduction Ratio for CBAM Attention",
      "data": [
        {
          "Reduction Ratio(r) #Parameters Accuracy": "33.57M\n28.54M\n26.02M\n24.77M"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: displays the final eval-",
      "data": [
        {
          "Architecture": "VGG-16\nVGG-19",
          "#Parameters CK+ Accuracy JAFFE Accuracy FER2013 Accuracy": "87.91%\n90.66%"
        },
        {
          "Architecture": "ResNet-50\nResNet-101\nResNet-152",
          "#Parameters CK+ Accuracy JAFFE Accuracy FER2013 Accuracy": "87.91%\n88.46%\n85.71%"
        },
        {
          "Architecture": "ResNetV2-50\nResNetV2-101 42.44M\nResNetV2-152 58.05M",
          "#Parameters CK+ Accuracy JAFFE Accuracy FER2013 Accuracy": "88.46%\n88.62%\n89.01%"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: summarizes the experimental results. The networks with attention",
      "data": [
        {
          "Architecture": "VGG-16\nVGG-16 + SEN-Net\nVGG-16 + ECA-Net\nVGG-16 + CBAM",
          "Param": "39.92 M 87.91%\n39.95M 88.46%\n39.92M 89.01%\n39.95M 89.56%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "64.44%\n68.89%\n73.33%\n75.56%",
          "FER2013 Accuracy": "60.66%\n63.05%\n62.72%\n63.46%"
        },
        {
          "Architecture": "VGG-19\nVGG-19 + SEN-Net\nVGG-19 + ECA-Net\nVGG-19 + CBAM",
          "Param": "42.87M 90.66%\n45.26M 91.21%\n45.23M 91.76%",
          "CK+ Accuracy": "45.26M 92.31% (↑ 1.65%) 77.78%",
          "JAFFE Accuracy": "68.89%\n73.33%\n75.56%",
          "FER2013 Accuracy": "60.92%\n63.23%\n63.49%\n64.07% (↑ 3.15%)"
        },
        {
          "Architecture": "ResNet-50\nResNet-50 + SEN-Net\nResNet-50 + ECA-Net\nResNet-50 + CBAM",
          "Param": "23.49M 87.91%\n26.02M 89.01%\n23.65M 90.11%\n26.02M 91.21%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "73.33%\n75.56%\n77.78%\n82.22%",
          "FER2013 Accuracy": "58.61%\n58.84%\n59.73%\n59.90%"
        },
        {
          "Architecture": "ResNet-101\nResNet-101 + SEN-Net\nResNet-101 + ECA-Net\nResNet-101 + CBAM",
          "Param": "42.46M 88.46%\n47.24M 89.01%\n42.81M 89.56%\n47.24M 90.11%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "60.00%\n68.89%\n73.33%\n75.56%",
          "FER2013 Accuracy": "58.67%\n58.92%\n60.15%\n60.92%"
        },
        {
          "Architecture": "ResNet-152\nResNet-152 + SEN-Net\nResNet-152 + ECA-Net\nResNet-152 + CBAM",
          "Param": "58.08M 85.71%\n64,71M 88.46%\n58.60M 89.56%\n64.71M 90.11%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "15.66%\n15.66%\n15.66%\n15.66%",
          "FER2013 Accuracy": "59.36%\n59.73%\n60.92%\n61.54%"
        },
        {
          "Architecture": "ResNetV2-50\nResNetV2-50 + SEN-Net\nResNetV2-50 + ECA-Net\nResNetV2-50 + CBAM",
          "Param": "23.48M 88.46%\n26.01M 88.66%\n23.64M 88.91%\n26.01M 89.01%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "77.78%\n82.22%\n82.22%\n84.44%(↑ 6.55%) 60.15%",
          "FER2013 Accuracy": "58.72%\n59.36%\n59.73%"
        },
        {
          "Architecture": "ResNetV2-101\nResNetV2-101 + CBAM",
          "Param": "42.44M 88.62%\nResNetV2-101 + SEN-Net 47,22M 89.01%\nResNetV2-101 + ECA-Net 42.79M 89.56%\n47.22M 90.66%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "62.22%\n68.89%\n70.83%\n73.33%",
          "FER2013 Accuracy": "59.07%\n59.73%\n60.15%\n60.92%"
        },
        {
          "Architecture": "ResNetV2-152\nResNetV2-152 + CBAM",
          "Param": "58.05M 89.01%\nResNetV2-152 + SEN-Net 64.68M 89.56%\nResNetV2-152 + ECA-Net 58.57M 89.82%\n64.69M 90.11%",
          "CK+ Accuracy": "",
          "JAFFE Accuracy": "66.67%\n68.89%\n73.33%\n77.78%",
          "FER2013 Accuracy": "59.40%\n60.72%\n61.54%\n62.05%"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural mechanisms of selective visual attention",
      "authors": [
        "R Desimone",
        "J Duncan"
      ],
      "year": "1995",
      "venue": "Annual review of neuroscience",
      "doi": "10.1146/ANNUREV.NE.18.030195.001205"
    },
    {
      "citation_id": "2",
      "title": "Visual Attention: Bottom-Up Versus Top-Down",
      "authors": [
        "C Connor",
        "H Egeth",
        "S Yantis"
      ],
      "year": "2004",
      "venue": "Current Biology",
      "doi": "10.1016/J.CUB.2004.09.041"
    },
    {
      "citation_id": "3",
      "title": "Bottomup and top-down attention are independent",
      "authors": [
        "Y Pinto",
        "A Van Der Leij",
        "I Sligte",
        "V Lamme",
        "H Scholte"
      ],
      "year": "2013",
      "venue": "Journal of Vision",
      "doi": "10.1167/13.3.16"
    },
    {
      "citation_id": "4",
      "title": "Squeeze-and-Excitation Networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Squeeze-and-Excitation Networks",
      "doi": "10.1109/CVPR.2018.00745"
    },
    {
      "citation_id": "5",
      "title": "A computerized analysis of facial expression: Feasibility of automated discrimination",
      "authors": [
        "J Cohn",
        "A Zlochower"
      ],
      "year": "1995",
      "venue": "American Psychological Society"
    },
    {
      "citation_id": "6",
      "title": "SCEP -A New Image Dimensional Emotion Recognition Model Based on Spatial and Channel-Wise Attention Mechanisms",
      "authors": [
        "B Li",
        "H Ren",
        "X Jiang",
        "F Miao",
        "F Feng",
        "L Jin"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3057373"
    },
    {
      "citation_id": "7",
      "title": "YOLO5Face: Why Reinventing a Face Detector",
      "authors": [
        "D Qi",
        "W Tan",
        "Q Yao",
        "J Liu"
      ],
      "year": "2021",
      "venue": "YOLO5Face: Why Reinventing a Face Detector",
      "doi": "10.48550/arxiv.2105.12931"
    },
    {
      "citation_id": "8",
      "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
      "authors": [
        "A Bochkovskiy",
        "C Wang",
        "H Liao"
      ],
      "year": "2020",
      "venue": "YOLOv4: Optimal Speed and Accuracy of Object Detection"
    },
    {
      "citation_id": "9",
      "title": "Fast and accurate deep network learning by exponential linear units (ELUs)",
      "authors": [
        "D Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations, ICLR 2016 -Conference Track Proceedings"
    },
    {
      "citation_id": "10",
      "title": "Self-normalizing neural networks",
      "authors": [
        "G Klambauer",
        "T Unterthiner",
        "A Mayr",
        "S Hochreiter"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "ECA-Net: Efficient channel attention for deep convolutional neural networks",
      "authors": [
        "Q Wang",
        "B Wu",
        "P Zhu",
        "P Li",
        "W Zuo",
        "Q Hu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR42600.2020.01155"
    },
    {
      "citation_id": "12",
      "title": "CBAM: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "LNCS",
      "doi": "$https://link.springer.com/chapter/10.1007/978-3-030-01234-2_1$"
    },
    {
      "citation_id": "13",
      "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer",
      "authors": [
        "S Zagoruyko",
        "N Komodakis"
      ],
      "year": "2016",
      "venue": "th International Conference on Learning Representations, ICLR 2017 -Conference Track Proceedings",
      "doi": "10.48550/arxiv.1612.03928"
    },
    {
      "citation_id": "14",
      "title": "Attention gated networks: Learning to leverage salient regions in medical images",
      "authors": [
        "J Schlemper",
        "O Oktay",
        "M Schaap",
        "M Heinrich",
        "B Kainz",
        "B Glocker",
        "D Rueckert"
      ],
      "year": "2019",
      "venue": "Medical Image Analysis",
      "doi": "10.1016/J.MEDIA.2019.01.012"
    },
    {
      "citation_id": "15",
      "title": "Attention-based vgg-16 model for covid-19 chest x-ray image classification",
      "authors": [
        "C Sitaula",
        "M Hossain"
      ],
      "year": "2021",
      "venue": "Applied Intelligence (Dordrecht, Netherlands)",
      "doi": "10.1007/S10489-020-02055-X"
    },
    {
      "citation_id": "17",
      "title": "Advian: Alzheimer's disease vgg-inspired attention network based on convolutional block attention module and multiple way data augmentation",
      "authors": [
        "S Wang",
        "Q Zhou",
        "M Yang",
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "Frontiers in Aging Neuroscience",
      "doi": "10.3389/FNAGI.2021.687456"
    },
    {
      "citation_id": "18",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "19",
      "title": "The Japanese Female Facial Expression (JAFFE) Dataset",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "The Japanese Female Facial Expression (JAFFE) Dataset",
      "doi": "10.5281/ZENODO.3451524"
    },
    {
      "citation_id": "20",
      "title": "FER-2013 face database",
      "authors": [
        "P Carrier",
        "A Courville",
        "I Goodfellow",
        "M Mirza",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "FER-2013 face database"
    },
    {
      "citation_id": "21",
      "title": "Facial Emotion Recognition: State of the Art Performance on FER2013 (2021)",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "venue": "Facial Emotion Recognition: State of the Art Performance on FER2013 (2021)"
    }
  ]
}