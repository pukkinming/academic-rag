{
  "paper_id": "2405.08992v2",
  "title": "Contextual Emotion Recognition Using Large Vision Language Models",
  "published": "2024-05-14T23:24:12Z",
  "authors": [
    "Yasaman Etesam",
    "Özge Nilay Yalçın",
    "Chuxuan Zhang",
    "Angelica Lim"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "How does the person in the bounding box feel?\" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Our ability to recognize emotions allows us to understand one another, build successful long-term social relationships, and interact in socially appropriate ways. Equipping virtual agents and robots with emotion recognition capabilities can help us improve and facilitate human-machine interactions  [1] . However, emotion recognition systems today still suffer from poor performance  [2]  due to the complexity of the task. This innate and seemingly effortless capability requires understanding of the causal relations, contextual information, social relationships as well as theory of mind, which are unresolved problems in affective computing research. Many image-based emotion recognition systems focus solely on using facial or body features  [3] ,  [4] , which can lead to a low accuracy in the absence of contextual information  [5] ,  [6] .\n\nIn the past few years, the affective computing research community has been moving towards creating datasets and building models that include or make use of contextual information. The EMOTIC dataset, for instance, incorporates contextual and environmental factors for apparent emotion recognition in still images  [7] . The inclusion of contextual information beyond facial features is found to significantly improve the accuracy of the emotion recognition models  [8] ,  [9] . However, using this information to infer the emotions of others requires commonsense knowledge and high-level cognitive capabilities such as reasoning and theory of mind which are missing from traditional emotion recognition models  [10] .\n\nAnother limitation of traditional emotion recognition models is that many of them are trained and tested on the same dataset  [11] . This stands in contrast to the challenge of generalization, where robots may perform poorly in novel Fig.  1 . In this paper, we evaluated two distinct zero-shot approaches: a) using CLIP to generate captions from images, followed by providing these captions alongside a prompt to large language models (LLMs) to obtain emotion labels. b) directly providing the image and a prompt to vision language models (VLMs) and requesting the emotion labels.\n\nsituations  [12] . In this study, we employ zero-shot models and observe their performance in unseen scenarios. Additionally, we demonstrate how results can be enhanced through finetuning. Both LLMs and LVMs are evaluated for this purpose.\n\nLarge language models (LLMs) that are based on the transformer architecture  [13]  have been shown to excel at natural language processing (NLP) tasks  [14] ,  [15] , offering a way to achieve emotional theory of mind through linguistic descriptors. LLMs gained success in increasing accuracy and efficiency in NLP problems including multimodal tasks such as visual question answering  [16]  and caption generation  [17] . Recently, they have been also used in commonsense reasoning  [18] ,  [19] ,  [20] , emotional inference  [21]  and theory of mind  [22]  tasks, however their capabilities on emotional theory of mind in visual emotion recognition tasks have not been explored.\n\nVision language models (VLMs) integrate natural language processing with visual comprehension to generate text from visual inputs and are capable of performing a variety of visual recognition tasks. VLMs learn intricate vision-language correlations from large-scale image-text pair datasets, enabling zero-shot predictions across a range of visual recognition tasks  [23] . Despite their success in tasks like image classification  [24]  and object detection  [25] , their capability in contextual emotion recognition has not yet been explored.\n\nIn this paper, we focus on a multi-label, contextual emotional theory of mind task by utilizing the embedded knowledge in large language models (LLMs) and vision language models (VLMs). To the best of our knowledge, this is the first evaluation of VLMs in the contextual emotion recognition task.\n\nThe contributions of this paper are as follows:\n\n• Presenting a new state-of-the-art open source method for affect estimation in the wild, using VLM fine-tuning • Proposing zero-shot approaches for contextual emotion recognition to explore generalizability for robotics • Evaluating the effectiveness of a) captioning + LLM, versus b) VLM approaches for emotion recognition",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotional theory of mind in context. Work in emotional theory of mind (in this paper, also referred to as emotion recognition) has been focusing on the inclusion of contextual information in addition to the facial or posture information in recent years. Early datasets such as HAPPEI  [26]  proposed, for instance, emotion annotation for groups of people. More recently, the EMOTIC dataset  [7]  was developed as multilabel dataset containing 26 categories of emotions, 18,316 images and 23,788 annotated people. The related emotion recognition task is to provide a list of emotion labels that matches those chosen by annotators, responding to the question of, \"How does the person in the bounding box feel?\". In approximately 25% of the person targets, the face was not visible, underscoring the role of context in estimating the emotion of a person in the image. The phrase \"emotional theory of mind\" is used here to clarify that we are not estimating the sentiment or emotional content of an image, but estimating the emotion of a particular person contained in the image. Note that we do not claim to perform felt emotion recognition, but apparent emotion recognition as perceived by labelers.\n\nVision-based approaches for contextual emotion estimation. A number of computer vision approaches have been developed in response to the release of the EMOTIC dataset. The EMOTIC baseline  [7]  uses a CNN to extracts features from the target human, as well as the entire image. Subsequent fusion methods incorporated body and context visual information  [27]  at global or local scales  [8] , investigated contextual videos  [28] , or worked to improve subsets of the EMOTIC dataset, such as  [29]  which considered the photos only including two people. In PERI  [30] , attention is modulated at various levels within their feature extraction network. In  [31] , relational region-level analysis was employed, while  [32]  utilized visual relationship between the main target and adjacent objects. To the extent of our knowledge, the current best approach was Emoticon proposed by  [9] , which explored the use of graph convolutional networks and image depth map. This approach can further improved by adding CCIM  [33]  plug-in. Overall, the latest results leave room for improvement, and no pretrained models are available for roboticists, with the exception of EMOTIC.\n\nLarge language models and theory of mind. Recent investigations into large language models (LLMs) have uncovered some latent capabilities for social intelligence, including some sub-tasks on emotion inference  [21] . Emotional theory of mind tasks using language tend to focus on appraisal based reasoning on emotions, inferring based on a sequence of events. For instance, among other social intelligence tasks,  [22]  explored how a language model could respond to an emotional event, e.g. \"Although Taylor was older and stronger, they lost to Alex in the wrestling match. How does Alex feel?\" Their findings suggested some level of social and emotional intelligence in LLMs.\n\nNatural language and emotional theory of mind. Language is a fundamental element in emotion and it plays a crucial role in emotion perception  [34] ,  [35] ,  [36] . In this work, in order to apply LLMs, we use the body of work in English literature that discusses how writers use empathy to describe characters, their actions and external cues, and \"seek to evoke feelings in readers employing the powers of narrativity.\"  [37] . Writing textbooks such as The Emotion Thesaurus  [38]  provide sets of visual cues or actions that support emotional evocation, as a guide for readers to imagine the most relevant features of the person in the scene. For example, to evoke curiosity, a writer may narrate, \"she tilted the head to the side, leaning forward, eyebrows furrowing\" or \"she raised her eyebrows and her body posture perked up.\"\n\nGeneralization and zero-shot learning. A major challenge in the field of robotics is generalization, enabling robots to adapt to new and unforeseen scenarios  [12] . Effective training on a labeled dataset often results in satisfactory performance only when evaluations are conducted on similar datasets. This is because the emphasis on minimizing training error tends to make machines capture all the correlations present in the training data, rather than understanding the actual causation  [11] . In the real world, the distribution of objects across different categories can exhibit a long-tailed pattern, where some categories are represented by a large number of training images, while others have few or no images at all  [39] ,  [40] ,  [41] . This disparity makes it challenging to identify all possible correlations. A crucial goal in visual emotion recognition is to improve the ability to recognize instances of previously unseen visual classes, an approach known as zero-shot learning  [42] ,  [43] . In our work, we leverage the embedded knowledge within large language and vision language models to address the visual emotional theory of mind task, aiming to perform this task without training on a specific dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this study, we compare two general approaches: a) 2phased image captioning and large language models, with b) end-to-end vision language models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Image Captioning And Large Language Model",
      "text": "In this first method, we use a two-phased approach to first generate a caption of the image, then use an LLM for linguistic reasoning to perform emotion inference (see Fig.  1 ). Our captioning method is called Narrative Captioning (NarraCap), and we compare it to a state-of-the-art ExpansionNet  [44]  captioning.\n\n1) Narrative Captioning: Our zero-shot Narrative Captioning (NarraCap) makes use of templates and the vision language model CLIP  [45] . First, given an image with the bounding box of a person, we extract the cropped bounding box and pass it along with a gender/age category (baby girl, baby boy, girl, boy, man, woman, elderly man, elderly woman) to CLIP to understand who is in the picture. Next, we pass the entire image to understand what is happening in the image by selecting the action with the maximum probability. The action list comprises 848 different actions extracted from the Kinetics-700  [46] , UCF-101  [47] , and HMDB datasets  [48] .\n\nWe then add the how aspect of the image by passing the cropped bounding box through CLIP, along with 889 signals (available on our website 1 ) filtered from over 1000 social signals derived from a guide on writing about emotion  [38] . Using trial and error, we found that the best approach comes from selecting signals that, when paired with an image in CLIP, returns a probability higher than mean + 9 * std of the class label scores. To provide additional context, we use 224 environmental descriptors from a writer's guide to urban  [49]  and rural  [50]  settings to describe where the person in the scene is located. The prompts we selected for CLIP are as follows: 'A photo of a(n) [gender/age/location]', and 'A photo of a person who is(has) [action/physical signals]'. Examples of narrative captions (NarraCap) can be found in Fig.  2 .\n\n2) ExpansionNet Captioning: We also evaluate a baseline captioning method, ExpansionNet  [44] , a fast end-to-end trained model for image captioning. The model achieves state of the art performance over the MS-COCO 2014 captioning challenge, and was used as a backbone for a recent approach trained on EMOTIC  [51] , and serves as a baseline to our NarraCap approach.\n\n3) Caption to Emotion using LLM (Zero Shot): Following captioning, we provide the caption, along with a prompt, to GPT-4. The prompt asks for the top emotion labels understood from the caption: \"<caption> From suffering, pain, [...], and sympathy, pick the top labels that this person is feeling at the same time.\" We also utilized an open-source LLM, Mistral 7B  [52] , which incorporates grouped-query attention  [53]  and sliding window attention  [54] ,  [55]  techniques to address common LLM limitations such as computational power and memory requirements. Mistral, with its 7 billion parameters, outperforms the best released 34-billion-parameter model, Llama 1  [56] , in reasoning tasks. The combination of a relatively low parameter count and high performance makes Mistral a potential option for applications in robotics. The prompt for Mistral is as follows: \"<caption> From suffering, pain, [...], and sympathy, the top labels that this person is feeling at the same time are:\" 4) Mistral (Fine-Tuned): Fine-tuning LLMs  [57] ,  [58] ,  [59]  has been demonstrated as an effective strategy to improve their performance. In this paper, using quantization, LoRA  [60] ,  [61]  and NarraCap, we finetune Mistral on the emotion recognition task. For Mistral, we experiment by fine-tuning on the Emotic validation set and augmentation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. End-To-End Vision Language Models",
      "text": "We next explore 3 vision language models (VLMs): CLIP, a closed-source (GPT-4) and an open source (LLaVA) VLM.\n\nwhere I e is image feature embeddings and T e is the text feature embeddings. CLIP can be used to perform zero shot classification by comparing distances between an image and various texts in a multimodal embedding space. We used the images from EMOTIC and compared the distances with each of the emotion labels and selected the six (average number of ground truth labels in validation set) labels with highest probabilities as our labels.\n\n2) GPT-4 Vision and LLaVA (Zero Shot): GPT-4 Vision is a proprietary model from OpenAI that can provide text-based responses given an image and text input. Large Language and Vision Assistant (LLaVA)  [62]  is an open source multipurpose multimodal model designed by combining CLIP's visual encoder  [45]  and LLAMA's language decoder  [56] . The model is fine-tuned end-to-end on the language-image instruction-following data generated using GPT-4  [63] .\n\n3) LLaVA (Fine-Tuned): We use the EMOTIC data to finetune LLaVA with LoRA  [60]   2  on the emotion recognition task. We experiment by fine-tuning LLaVA on the EMOTIC training set (17077 images, and 23706 individuals), EMOTIC validation set (2087 images, and 3330 individuals), and on a small dataset, created by selecting 100 images at random from the validation set. Furthermore, we perform data augmentation by shuffling the ground truth labels for each image in the validation set and using each image 3 times with different shuffled labels.\n\n4) Prompt Engineering: We used the images from EMOTIC and a text prompt: \"From suffering, pain, [...], and sympathy, pick the top labels that the person in the red bounding box is feeling at the same time.\" It has been shown that prompt engineering (e.g. chain of thought  [64] ) is an effective way to improve results. We also tested a prompt which included definitions of the emotions and specified a number of labels to output.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "Our experiment is focused on the EMOTIC dataset, which covers 26 different labels. The related emotion recognition task is to provide a list of emotion labels that matches those chosen by annotators. The training set (70%) was annotated by 1 annotator, where validation (10%) and test (20%) sets were annotated by 5 and 3 annotators respectively. While previous work on emotion recognition tasks  [7] ,  [9] ,  [30] , utilize the mean Average Precision (mAP) as a metric, in this work, outputs are textual descriptions indicating the labels the person is feeling, rather than probabilities. Therefore, we could not employ mAP, instead, we used precision, recall, F1 score, hamming loss, which demonstrates the average rate at which incorrect labels are predicted for a sample, and subset accuracy, which requires the predicted set of labels for a sample to exactly match the actual set of labels. These metrics are implemented using the scikit-learn library  [65] . We compare the following methods: EMOTIC Along with the dataset,  [7]  introduced a twobranched CNN-based network baseline. The first branch extracts body related features and the second branch extracts scene-context features. Then a fusion network combines these features and estimates the output. For the EMOTIC dataset, using the provided code 3 and thresholds calculated from the validation set, we obtained the output labels on the test set and then calculated the target metrics. This approach is the only traditional method with reproducible code. Emoticon Motivated by Frege's principle  [66] ,  [9]  proposed an approach by combining three different interpretations of context. They used pose and face features (context1), background information (context2), and interactions/social dynamics (context3). They used a depth map to model the social interactions in the images. Later, they concatenate these different features and pass it to fusion model to generate outputs  [9] . Unfortunately, the code for this project was not made available by the authors, and we could not reproduce the reported results. Consequently, we cannot provide a reliable comparison with this approach. Random We consider selecting either 6 (average number of labels per person in validation set) emotions randomly from all possible labels (Rand) or selecting 6 labels randomly where the weights are determined by the number of times each emotion is repeated in the validation set (Rand(W)). Majority This Majority baseline selects the top 6 most common emotions in the validation set (engagement, anticipation, happiness, pleasure, excitement, confidence) as the predicted labels for all test images (Maj). CLIP For the CLIP model, we employed the clip-vit-base-patch32. While utilizing the clip-vit-large-patch14-336 model did enhance the F1 score for the clip-only method to 19.60, its use significantly increased processing time, particularly for generating NarraCap captions. Therefore, to maintain consistency in our reporting and efficiency in our processing, we present results using the clip-vit-base-patch32 model. The prompt used here is: \"The person in the red bounding box is feeling {emotion label}\". Additionally, we employed Grad-CAM  [67] , to generate saliency maps, allowing us to visually highlight the areas within images that significantly influenced the model's decisions. Captions+GPT-4 After generating captions, we pass the captions to gpt-4 (gpt-4-0613). We utilized GPT-4 with the temperature parameter set to 0 and the maximum token count set to 256. Additionally, the frequency penalty, presence penalty, and top_p were configured to 0, 0, and 1, respectively. While adjusting these parameters could potentially enhance the model's performance, we refrained from hyperparameter 3 https://github.com/Tandon-A/emotic tuning for this task due to associated costs. GPT-4 Vision Using gpt-4-vision-preview, we input EMOTIC test images, with parameters set similarly to GPT-4. In this experiment, we tested both the prompt mentioned in Section 3.2.4, and also the inclusion of label definitions provided by EMOTIC to GPT, and requesting the six most likely labels. LLaVA As GPT4-Vision, we tested both the prompt mentioned in Section 3.2.4, and also the inclusion of label definitions provided by EMOTIC to GPT, and requesting six most likely labels. LLAVA fine-tuning was performed on four A40 48GB GPUs. Mistral We used huggingface 4 to run and finetune Mistral on a RTX 3090 Ti GPU. We used maximum new tokens of 256 and repetition penalty equal to 1.15.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "The results for zero shot methods are shown in Table  I , and example images with captions in Fig.  2 .\n\nWe observe that fine-tuning LLaVA with an augmented validation set provides the best overall F1 score. In Fig.  2 , we can see that LLaVA fine-tuned on the validation set predicts more labels than when trained on the training set. One explanation is that the number of annotators for the training and test sets is 1 and 3, respectively. Since we utilize the combined labels predicted by all annotators, the average number of ground truth labels per person is higher in the test set  (4.42)  than in the training set  (1.96) . This discrepancy leads the model trained on the training set to predict fewer labels than what is present in the test ground truth, causing the model to predict cautiously with high precision but miss many labels, resulting in low recall. To address this issue, we attempted fine-tuning on the validation set, which has 5 annotators and an average of 6.157 ground truth labels per person. We also experimented with fine-tuning on a small dataset, selecting 100 images at random from the validation set. This was to demonstrate that using a minimal amount of data can still yield reasonable results with vision language models (VLMs). Future work could try to balance the average number of labels in the training and test set. We also see that simple augmentation of the validation set, by shuffling the labels, improves performance. This may be due to the model learning that label ordering is not an important factor in the text output.\n\nIn Fig.  2 , we observe that the EMOTIC baseline tends to predict many more labels than the other methods, which reduces its precision and overall F1 score. For an application where choosing a precise emotion label is more important than predicting all possible labels, fine-tuned LLaVA on the training set may be the most useful model.\n\nIt is evident that CLIP, which underperforms as indicated in Table  I , misinterprets certain images, such as mistakenly attributing the emotion of embarrassment to a woman at the beach (Fig.  2 ). A deeper analysis using Grad-Cam-generated saliency maps (as seen in Fig.  3 .1) offers a plausible explanation: CLIP may inaccurately associate images displaying Fig.  3 . CLIP saliency maps using Grad-Cam  [67] . It identifies the regions in an input image that most significantly influence the classification score by leveraging the gradients of the score relative to the last convolutional layer's feature map. face (Fig.  3 .3).\n\nIn the captioning combined with GPT-4 analysis, NarraCap proves to be more effective than ExpNet in aiding GPT-4's understanding of emotions. However, it ranks as the second-best zero-shot approach. GPT-4 Vision with prompt engineering emerges as the top performer among zero-shot methods, surpassing EMOTIC, which was trained on the EMOTIC training set. How does captioning + LLM compare to the end-to-end VLM approach? In addition to our experiments in Table  I , we performed an additional study on a smaller test set. Yang et al.  [68]  recruited an annotator fluent in North American English to manually generate captions for 387 images, encompassing 14 negative emotion categories: suffering, annoyance, pain, sadness, anger, aversion, disquietment, doubt/confusion, embarrassment, fear, disapproval, fatigue, disconnection, and sensitivity. This focus on negative emotions stemmed from their comparatively poor recognition across all methods tested, relative to positive emotions. The outcomes for all methodologies applied to this dataset are detailed in Table  II     [68]  in an F1 score of 26.19. The GPT4-Vision zero-shot VLM approach attained an F1 score of 35.79. This disparity appears large; however, leveraging human-generated captions with GPT-4 (LLM) achieved an F1 score of 34.17. This indicates that while the automatic captioning + LLM method does not reach the VLM performance, human-level captioning when coupled with LLMs provides nearly comparable performance and outperforms the traditional EMOTIC baseline.\n\nHow do different prompts affect the results? Selecting an appropriate prompt for LLMs and VLMs is crucial for optimizing their performance. However, the same prompt can affect different models in varied ways. As shown in Table  I , incorporating label definitions and requesting the top 6 labels significantly enhances the results for GPT-Vision, yet it adversely impacts LLaVA's performance. Thus, tailoring prompts to the specific characteristics and capabilities of each model may be necessary to achieve the best outcomes. Furthermore, following  [45]   methods based on the number of people in the image: one, two, or multiple people. As shown in Table  III , the precision, recall, and F1 score tend to decrease as the number of people increases. This reduction in performance can be attributed to the more complex situations that arise when there are more people in a scene  [69] . Furthermore, NarraCap does not account for human interactions, and vision language models (VLMs) struggle with identifying the specific individual referred to in a prompt. This challenge is partly due to the models' limitations in interpreting visual markers (bounding boxes), which are crucial for distinguishing among multiple subjects in an image  [70] . For the EMOTIC model, which was trained on the training set, it was observed that while it surpassed the performance of NarraCap, a zero-shot approach, for images featuring a single person, it was less effective in handling images with two or more people. This discrepancy suggests that NarraCap demonstrates superior performance in more complex scenarios involving multiple individuals.\n\nHow do age, gender, activity, environment, and physical signals affect the results? One of the advantages of the NarraCap approach is that it provides a way to explicitly select image details to include for inference and perform ablations using the text representation. To assess the effect of gender, instead of using specific gender labels such as \"a baby boy,\" \"a baby girl,\", etc. we only utilized the labels \"a female\" and \"a male.\" Furthermore, to examine gender, we modified the label list to \"a baby,\" \"a kid,\" \"an adult,\" and \"an elderly person.\" To investigate the impact of activity, environment, and physical signals, we excluded those components from the captions. The findings from each study, conducted on a set of 1000 images randomly selected from the validation set, are summarized in Table  IV . This table reveals that the action depicted in an image had the most significant impact on the outcome, followed by the environment. These insights suggests that future research on image caption generation may focus on understanding image actions and contexts, as they are keys to create accurate and relevant captions.",
      "page_start": 4,
      "page_end": 7
    },
    {
      "section_name": "Vi. Limitations",
      "text": "The current study evaluating emotional theory of mind within the EMOTIC image dataset has its limitations. It primarily focused on various OpenAI models, including the zero-shot classifier CLIP, the large language model GPT-4, and the vision-language model GPT-4 Vision. Additionally,  two open-source methods were examined: LLaVA (VLM) and Mistral (LLM). Although state-of-the-art zero-shot techniques were employed, fine-tuning some models (GPT-4, GPT-4 Vision) was not feasible due to their proprietary, closedsource nature. In addition, it is not possible to know the extent that the vision language models (i.e. GPT-4 Vision and LLaVA) were exposed to the EMOTIC test set. Another limitation is the absence of certain traditional emotion recognition models (e.g., emoticon) from our study, as their code was not made publicly available by the authors and our attempts to re-implement these models failed to replicate the reported results.\n\nIn addition, we noticed that the EMOTIC dataset, while being one of the most challenging image emotion datasets including context, also has small imperfections, including some bounding boxes that contain 2 people instead of only one (Fig.  2 .2). Although the case described is rare, a future study could evaluate on other datasets, e.g. one person emotion expression datasets without context, which is considered to be a simpler task. For NarraCap, captions did not describe the social interactions or interactions with objects, which if added may increase performance. Moreover, for the activity and environment detection using CLIP, we performed a standard evaluation with a limited set of classes without a \"don't know\" or null class, resulting in some mis-captioning.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this study, we delve into the potential of vision language models (VLMs) and large language models (LLMs) for assessing visual emotional theory of mind. Our findings reveal that the zero-shot approach of GPT-4 Vision with prompt engineering could outperform the trained EMOTIC model. Furthermore, our success in enhancing the performance beyond conventional methods by fine-tuning LLaVA, an open-source VLM, on a modest dataset underscores the profound potential of these models to comprehend human emotions. Further research could explore improving the narrative captions by adding other contextual factors to the caption, such as human-object interactions and relationships with other people in the image. Further studies on the characteristics of emotionally comprehensive captioning, coupled with GPT or with human evaluators could be done. Additionally, enhancing the ability of VLMs to accurately recognize visual markers, such as bounding boxes, could significantly boost their performance.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In this paper, we evaluated two distinct zero-shot approaches: a)",
      "page": 1
    },
    {
      "caption": "Figure 2: 2) ExpansionNet Captioning: We also evaluate a baseline",
      "page": 3
    },
    {
      "caption": "Figure 2: We observe that fine-tuning LLaVA with an augmented",
      "page": 4
    },
    {
      "caption": "Figure 2: , we observe that the EMOTIC baseline tends",
      "page": 4
    },
    {
      "caption": "Figure 2: ). A deeper analysis using Grad-Cam-generated",
      "page": 4
    },
    {
      "caption": "Figure 3: 1) offers a plausible expla-",
      "page": 4
    },
    {
      "caption": "Figure 2: Qualitative analysis of EMOTIC images showcasing ground truth (GT) labels, NarraCap-generated captions, and inferred labels using (a) zero-shot",
      "page": 5
    },
    {
      "caption": "Figure 3: CLIP saliency maps using Grad-Cam [67]. It identifies the regions",
      "page": 6
    },
    {
      "caption": "Figure 2: 2). Although the case described is rare, a future study",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "NarraCap",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "b)"
        },
        {
          "Column_1": "This person is a boy who is skiing\nin a ski resort. He has hands raised\ninto the air. He has hands up,\nbacking away. He is waving\nenthusiastically. He is waving the\narms, using grand gestures.",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": ""
        },
        {
          "Column_1": "Linguistic Inference\n(GPT-4)",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Visual+Linguistic Inference\n(GPT-4 Vision)"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Engagement,\nConfidence,\nHappiness, Peace,\nExcitement, Pleasure"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a)": "Who?\nWhat?\nWhere?",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "How?"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "1. GT: Anticipation, Excitement, Happiness, Pleasure, Sympathy\nNarraCap: This person is an elderly woman who is petting an\nanimal (not a cat) in a nursing home. She is caretaking.\nCLIP: Doubt/Confusion, Esteem, Sensitivity, Engagement,\nSympathy, Affection\nGPT4: Affection, Engagement, Happiness, Peace, Pleasure,\nConfidence, Sensitivity, Sympathy\nGPT4-Vision*: Affection, Engagement, Happiness, Peace,\nPleasure, Esteem\nLLaVA: Happiness, Engagement, Affection, Sensitivity\nLLaVA†: Happiness, Pleasure, Affection, Sympathy, Anticipation,\nEngagement,\nEMOTIC: Affection, Anticipation, Engagement, Fatigue, Happiness,\nPeace, Sympathy, Yearning"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "2. GT: Engagement, Happiness, Peace\nNarraCap: This person is a woman who is packing in a airport.\nShe is chest bumping with others. She is clutching at another\nperson for support. She is pulling someone into a side hug. She is\nputting an arm around someone’s shoulders.\nCLIP: Sensitivity, Disconnection, Happiness, Suffering, Surprise,\nAffection\nGPT4: Affection, Engagement, Anticipation, Excitement,\nHappiness, Confidence, Surprise\nGPT4-Vision*: Happiness, Affection, Engagement, Confidence,\nPeace, Excitement\nLLaVA:Happiness, Engagement, Excitement, Anticipation\nLLaVA†: Excitement, Happiness, Pleasure, Affection, Anticipation\nEMOTIC: Anticipation, Engagement, Happiness"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "3. GT: Fatigue, Peace\nNarraCap: This person is a boy who is sleeping in a bookstore. He\nis curling up to take up less space. He is falling asleep in odd\nplaces at odd times. He is flipping through brochures or other\nreading material at hand. He is sleeping.\nCLIP: Sympathy, Engagement, Suffering, Disquietment,\nDisconnection, Fatigue\nGPT4: Fatigue, Disquietment, Doubt/Confusion, Disconnection,\nYearning, Sadness\nGPT4-Vision*: Fatigue, Disquietment, Disconnection, Peace,\nEngagement, Doubt/Confusion\nLLaVA: Fatigue, Disconnection\nLLaVA†: Fatigue, Disconnection, Peace, Suffering, Engagement\nEMOTIC: Annoyance, Aversion, Disapproval, Disquietment,\nDoubt/Confusion, Engagement, Fatigue, Happiness, Peace,\nSensitivity, Yearning"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "4. GT: Confidence, Esteem, Excitement, Happiness, Pleasure\nNarraCap: This person is a boy who is skiing in a ski resort. He has\nhands raised into the air. He has hands up, backing away. He is\nwaving enthusiastically. He is waving the arms, using grand\ngestures.\nCLIP: Esteem, Doubt/Confusion, Surprise, Happiness, Fear,\nConfidence\nGPT4: Excitement, Happiness, Anticipation, Engagement, Pleasure\nGPT4-Vision*: Engagement, Confidence, Happiness, Peace,\nExcitement, Pleasure\nLLaVA: Excitement, Anticipation, Engagement\nLLaVA†: Excitement, Happiness, Confidence, Anticipation,\nEngagement\nEMOTIC: Anticipation, Confidence, Engagement, Excitement,\nHappiness"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "5. GT: Happiness, Peace, Pleasure\nNarraCap: This person is a woman who is windsurfing in a beach.\nShe has chin dipping to the chest. She is participating in relaxing\nactivities.\nCLIP: Happiness, Sympathy, Embarrassment, Peace, Confidence,\nEsteem\nGPT4: Confidence, Engagement, Happiness, Peace, Pleasure,\nExcitement, Anticipation\nGPT4-Vision*: Peace, Pleasure, Confidence, Engagement,\nHappiness, Anticipation\nLLaVA: Excitement, Anticipation, Happiness\nLLaVA†: Engagement, Happiness, Anticipation, Excitement,\nConfidence, Pleasure\nEMOTIC: Affection, Anticipation, Confidence, Embarrassment,\nEngagement, Excitement, Fear, Happiness, Pleasure, Surprise"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "6. GT: Anticipation, Confidence, Doubt/Confusion, Engagement,\nExcitement, Happiness\nNarraCap: This person is a boy who is skijetting in a ski resort. He\nhas shoulders curling forward.\nCLIP: Pain, Surprise, Doubt/Confusion, Confidence, Fear, Suffering\nGPT4: Excitement, Anticipation, Engagement, Pleasure, Confidence\nGPT4-Vision*: Engagement, Anticipation, Excitement,\nDisquietment, Fatigue, Confidence, Doubt/Confusion\nLLaVA: Excitement, Anticipation, Engagement\nLLaVA†: Confidence, Anticipation, Engagement, Excitement,\nHappiness\nEMOTIC: Anticipation, Confidence, Engagement, Excitement"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "8. GT: Happiness, Pleasure\nNarraCap: is a man who is shaking hands in a board room. He is\nengaging excitedly with others. He is putting one's hands in one's\npockets. He is using a strong, businesslike handshake.\nCLIP: Suffering, Disconnection, Sensitivity, Esteem, Confidence,\nEngagement\nGPT4: Confidence, Engagement, Happiness, Excitement,\nAnticipation, Esteem\nGPT4-Vision*: Confidence, Engagement, Happiness, Peace,\nPleasure, Esteem\nLLaVA: Happiness, Engagement, Confidence\nLLaVA†: Confidence, Happiness, Esteem, Pleasure, Excitement,\nEngagement, Affection\nEMOTIC: Affection, Annoyance, Anticipation, Confidence,\nDisapproval, Doubt/Confusion, Engagement, Esteem, Happiness,\nPleasure, Sympathy"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "9. GT: Affection, Engagement, Happiness, Pleasure\nNarraCap: This person is a girl who is grooming a dog in a pet store.\nShe is displaying affection with friends or loved ones. She is gripping\none's own throat. She is pulling loved ones close to protect them.\nCLIP: Sadness, Engagement, Peace, Happiness, Pleasure,\nAffection\nGPT4: Affection, Confidence, Engagement, Happiness, Peace,\nPleasure, Anticipation, Surprise.\nGPT4-Vision*: Affection, Happiness, Peace, Pleasure, Engagement\nExcitement\nLLaVA: Affection, Happiness, Engagement\nLLaVA†: Happiness, Affection, Pleasure, Sympathy, Engagement,\nExcitement\nEMOTIC: Affection, Annoyance, Disquietment, Doubt/Confusion,\nEngagement, Fatigue, Happiness, Peace, Pleasure, Sensitivity,\nSuffering, Surprise, Sympathy"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "7. GT: Engagement, Sadness, Suffering, Sympathy\nNarraCap: This person is a man who is crying in a refugee camp.\nHe is crying with the person. He is crying, wailing, begging for help.\nHe is crying.\nCLIP: Anger, Fatigue, Affection, Sensitivity, Sadness, Sympathy\nGPT4: Suffering, Pain, Fear, Disquietment, Doubt/Confusion,\nSadness, Yearning, Disapproval, Annoyance, Fatigue\nGPT4-Vision*: Suffering, Sadness, Disquietment, Pain, Sympathy,\nFatigue\nLLaVA: Sadness, Pain, Embarrassment\nLLaVA†: Sadness, Suffering, Pain, Disapproval, Disquietment,\nDisconnection, Sensitivity, Fear\nEMOTIC: Affection, Anticipation, Aversion, Disquietment,\nDoubt/Confusion, Embarrassment, Engagement, Fear, Yearning,\nPleasure, Sadness, Sensitivity, Surprise, Sympathy, Happiness"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "1. Embarrassment",
          "Column_2": "",
          "Column_3": "2. Surprise",
          "Column_4": "",
          "Column_5": "3. Sadness"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CLIP NarraCap GPT4-V LLaVA": "1 22.07 26.44 37.53 32.06\n2 21.52 26.06 36.64 36.10\n>2 20.97 23.18 34.31 31.30"
        },
        {
          "CLIP NarraCap GPT4-V LLaVA": "1 28.76 32.94 39.07 22.79\n2 28.11 35.02 38.67 22.15\n>2 27.45 31.74 36.54 18.39"
        },
        {
          "CLIP NarraCap GPT4-V LLaVA": "1 16.20 26.53 34.75 23.53\n2 16.67 27.61 34.28 23.38\n>2 16.81 25.01 32.57 19.77"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "2",
      "title": "S-ACC\" REPRESENTS SUBSET ACCURACY, \"-F\" INDICATES THAT THIS IS A FINE-TUNED VERSION, AND \"*\" INDICATES PROMPT ENGINEERING (DEFINITIONS + REQUEST 6 LABELS, AVE. IN VALIDATION SET)",
      "authors": [
        "Table I Performance Metrics Of Various Models Using Macro Average"
      ],
      "venue": "S-ACC\" REPRESENTS SUBSET ACCURACY, \"-F\" INDICATES THAT THIS IS A FINE-TUNED VERSION, AND \"*\" INDICATES PROMPT ENGINEERING (DEFINITIONS + REQUEST 6 LABELS, AVE. IN VALIDATION SET)"
    },
    {
      "citation_id": "3",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "4",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "5",
      "title": "Expert system for automatic analysis of facial expressions",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "6",
      "title": "Recognizing emotions expressed by body pose: A biologically inspired neural model",
      "authors": [
        "K Schindler"
      ],
      "year": "2008",
      "venue": "Neural networks"
    },
    {
      "citation_id": "7",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "8",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "9",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti"
      ],
      "year": "2019",
      "venue": "PAMI"
    },
    {
      "citation_id": "10",
      "title": "Global-local attention for emotion recognition",
      "authors": [
        "N Le"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "11",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "Computational models of emotion inference in theory of mind: A review and roadmap",
      "authors": [
        "D Ong"
      ],
      "year": "2019",
      "venue": "Topics in cognitive science"
    },
    {
      "citation_id": "13",
      "title": "From dependence to causation",
      "authors": [
        "D Lopez-Paz"
      ],
      "year": "2016",
      "venue": "From dependence to causation"
    },
    {
      "citation_id": "14",
      "title": "Bc-z: Zero-shot task generalization with robotic imitation learning",
      "authors": [
        "E Jang"
      ],
      "year": "2022",
      "venue": "Bc-z: Zero-shot task generalization with robotic imitation learning"
    },
    {
      "citation_id": "15",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery"
      ],
      "year": "2022",
      "venue": "arXiv"
    },
    {
      "citation_id": "18",
      "title": "Vqa: Visual question answering",
      "authors": [
        "S Antol"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "19",
      "title": "Show and tell: A neural image caption generator",
      "authors": [
        "O Vinyals"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Socialiqa: Commonsense reasoning about social interactions",
      "authors": [
        "M Sap"
      ],
      "year": "2019",
      "venue": "Socialiqa: Commonsense reasoning about social interactions"
    },
    {
      "citation_id": "21",
      "title": "Piqa: Reasoning about physical commonsense in natural language",
      "authors": [
        "Y Bisk"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "22",
      "title": "A systematic investigation of commonsense knowledge in large language models",
      "authors": [
        "X Li"
      ],
      "year": "2022",
      "venue": "EMNLP"
    },
    {
      "citation_id": "23",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
      "authors": [
        "M Sap"
      ],
      "year": "2022",
      "venue": "arXiv"
    },
    {
      "citation_id": "25",
      "title": "Vision-language models for vision tasks: A survey",
      "authors": [
        "J Zhang"
      ],
      "year": "2024",
      "venue": "PAMI"
    },
    {
      "citation_id": "26",
      "title": "What does a platypus look like? generating customized prompts for zero-shot image classification",
      "authors": [
        "S Pratt"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "27",
      "title": "Fine-grained visual-text prompt-driven self-training for open-vocabulary object detection",
      "authors": [
        "Y Long"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "28",
      "title": "Finding happiest moments in a social context",
      "authors": [
        "A Dhall"
      ],
      "year": "2013",
      "venue": "Finding happiest moments in a social context"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition based on body and context fusion in the wild",
      "authors": [
        "Y Huang"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "30",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "31",
      "title": "Boosting emotion recognition in context using non-target subject information",
      "authors": [
        "S Thuseethan"
      ],
      "year": "2021",
      "venue": "IJCNN"
    },
    {
      "citation_id": "32",
      "title": "Peri: Part aware emotion recognition in the wild",
      "authors": [
        "A Mittel",
        "S Tripathi"
      ],
      "year": "2023",
      "venue": "ECCV 2022 Workshops"
    },
    {
      "citation_id": "33",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "W Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Context-aware emotion recognition based on visual relationship detection",
      "authors": [
        "M.-H Hoang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "D Yang"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "36",
      "title": "The role of language in emotion: Predictions from psychological constructionism",
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "37",
      "title": "Putting feelings into words",
      "authors": [
        "M Lieberman"
      ],
      "year": "2007",
      "venue": "Psychological science"
    },
    {
      "citation_id": "38",
      "title": "What's in a word? language constructs emotion perception",
      "authors": [
        "K Lindquist",
        "M Gendron"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "39",
      "title": "Narrative emotions",
      "authors": [
        "S Keen",
        "S Keen"
      ],
      "year": "2015",
      "venue": "Narrative Form: Revised and Expanded Second Edition"
    },
    {
      "citation_id": "40",
      "title": "The emotion thesaurus: A writer's guide to character expression",
      "authors": [
        "B Puglisi",
        "A Ackerman"
      ],
      "year": "2019",
      "venue": "The emotion thesaurus: A writer's guide to character expression"
    },
    {
      "citation_id": "41",
      "title": "A survey of zero-shot learning: Settings, methods, and applications",
      "authors": [
        "W Wang"
      ],
      "year": "2019",
      "venue": "TIST"
    },
    {
      "citation_id": "42",
      "title": "Generalized zero-shot learning with deep calibration network",
      "authors": [
        "S Liu"
      ],
      "year": "2018",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "43",
      "title": "A review of generalized zero-shot learning methods",
      "authors": [
        "F Pourpanah"
      ],
      "year": "2022",
      "venue": "PAMI"
    },
    {
      "citation_id": "44",
      "title": "Haptic zero-shot learning: Recognition of objects never touched before",
      "authors": [
        "Z Abderrahmane"
      ],
      "year": "2018",
      "venue": "Rob. Auton. Syst"
    },
    {
      "citation_id": "45",
      "title": "Zero-shot learning through cross-modal transfer",
      "authors": [
        "R Socher"
      ],
      "year": "2013",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "46",
      "title": "Expansionnet v2: Block static expansion in fast end to end training for image captioning",
      "authors": [
        "J Hu"
      ],
      "year": "2022",
      "venue": "arXiv"
    },
    {
      "citation_id": "47",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "48",
      "title": "A short note on the kinetics-700-2020 human action dataset",
      "authors": [
        "L Smaira"
      ],
      "year": "2020",
      "venue": "arXiv"
    },
    {
      "citation_id": "49",
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "authors": [
        "K Soomro"
      ],
      "year": "2012",
      "venue": "arXiv"
    },
    {
      "citation_id": "50",
      "title": "Hmdb: a large video database for human motion recognition",
      "authors": [
        "H Kuehne"
      ],
      "year": "2011",
      "venue": "ICCV"
    },
    {
      "citation_id": "51",
      "title": "The Urban Setting Thesaurus: A Writer's Guide to City Spaces",
      "authors": [
        "B Puglisi",
        "A Ackerman"
      ],
      "year": "2016",
      "venue": "The Urban Setting Thesaurus: A Writer's Guide to City Spaces"
    },
    {
      "citation_id": "52",
      "title": "The Rural Setting Thesaurus: A Writer's Guide to Personal and Natural Places",
      "year": "2016",
      "venue": "The Rural Setting Thesaurus: A Writer's Guide to Personal and Natural Places"
    },
    {
      "citation_id": "53",
      "title": "High-level context representation for emotion recognition in images",
      "authors": [
        "W De Lima"
      ],
      "year": "2023",
      "venue": "CVPR) Workshops"
    },
    {
      "citation_id": "54",
      "title": "Mistral 7b,\" arXiv",
      "authors": [
        "A Jiang"
      ],
      "year": "2023",
      "venue": "Mistral 7b,\" arXiv"
    },
    {
      "citation_id": "55",
      "title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints",
      "authors": [
        "J Ainslie"
      ],
      "year": "2023",
      "venue": "arXiv"
    },
    {
      "citation_id": "56",
      "title": "Generating long sequences with sparse transformers",
      "authors": [
        "R Child"
      ],
      "year": "2019",
      "venue": "arXiv"
    },
    {
      "citation_id": "57",
      "title": "Longformer: The long-document transformer",
      "authors": [
        "I Beltagy"
      ],
      "year": "2020",
      "venue": "Longformer: The long-document transformer"
    },
    {
      "citation_id": "58",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "arXiv"
    },
    {
      "citation_id": "59",
      "title": "Metaicl: Learning to learn in context",
      "authors": [
        "S Min"
      ],
      "year": "2021",
      "venue": "arXiv"
    },
    {
      "citation_id": "60",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang"
      ],
      "year": "2022",
      "venue": "arXiv"
    },
    {
      "citation_id": "61",
      "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "authors": [
        "H Liu"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "62",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu"
      ],
      "year": "2021",
      "venue": "arXiv"
    },
    {
      "citation_id": "63",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "T Dettmers"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "64",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning"
    },
    {
      "citation_id": "65",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "66",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "67",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "F Pedregosa"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "68",
      "title": "The context principle in frege's philosophy",
      "authors": [
        "M Resnik"
      ],
      "year": "1967",
      "venue": "Philosophy and Phenomenological Research"
    },
    {
      "citation_id": "69",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "70",
      "title": "Contextual emotion estimation from image captions",
      "authors": [
        "V Yang"
      ],
      "year": "2023",
      "venue": "arXiv"
    },
    {
      "citation_id": "71",
      "title": "Automatic emotion recognition for groups: a review",
      "authors": [
        "E Veltmeijer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "72",
      "title": "Contrastive region guidance: Improving grounding in vision-language models without training",
      "authors": [
        "D Wan"
      ],
      "year": "2024",
      "venue": "arXiv"
    }
  ]
}