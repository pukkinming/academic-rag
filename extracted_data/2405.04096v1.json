{
  "paper_id": "2405.04096v1",
  "title": "Speaker Characterization By Means Of Attention Pooling",
  "published": "2024-05-07T07:56:30Z",
  "authors": [
    "Federico Costa",
    "Miquel India",
    "Javier Hernando"
  ],
  "keywords": [
    "multi-head self-attention",
    "double attention",
    "speech recognition",
    "speaker verification",
    "speaker characterization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors. These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variablelength utterances into fixed-length speaker vectors. The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker recognition, placed between a CNN-based front-end and a set of fully connected layers. This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the speech signal. In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion recognition, sex classification and COVID-19 detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speaker Verification (SV) aims to determine whether a pair of audios corresponds to the same speaker. Given speech signals, speaker verification systems are able to extract speaker identity patterns from the characteristics of the voice. State-of-theart SV systems are commonly Deep Learning (DL) approaches that encode speaker characteristics into discriminative speaker vectors (also known as speaker embeddings). These architectures are usually trained as speaker classifiers in order to be used as speaker embedding extractors. One of the most known speaker representations is the x-vector  [1] , which has become state-of-the-art for speaker recognition. Recent network architectures used for speaker embedding generation are composed of a front-end feature extractor, a pooling layer and a set of Fully Connected (FC) layers. During the last years, several studies addressed different types of pooling strategies  [2, 3, 4] . X-vector originally uses statistical pooling  [5] . Self-attention mechanisms have been used to improve statistical pooling, such as  [6, 7] .\n\nSome of these speaker embedding generation systems can be adapted and used to solve Speaker Characterization (SC) tasks  [8, 9] . SC tasks are those where one or more speaker characteristics are extracted from the speech. In the last years, DL approaches were used in order to extract speaker characteristics from speech such as emotions  [10] , sex  [11] , age  [12] , language  [13] , dialect  [14] , accent  [15] , health conditions  [16] , among others.\n\nThe authors have recently proposed a DL system based on a Double Multi-Head Self-Attention (DMHSA) pooling  [17] . Its architecture consists of a Convolutional Neural Network (CNN)-based front-end, followed by an attention-based pooling layer and a set of fully connected layers. It has been proven to achieve excellent results in SV tasks and it is an improvement of the Multi-Head Self-Attention (MHSA) pooling method proposed in  [7] . Since this system is trained as a speaker classifier, it can be adapted to solve SC tasks. In this paper we will present several applications using an adapted version of this architecture on different SC tasks, namely Speech Emotion Recognition (SER), Speaker Sex Classification and Speaker COVID-19 Detection, achieving excellent results. We would like to mention that all Speaker COVID-19 Detection results obtained in this work are meant for exploratory purposes and should not be taken to draw any medical conclusions.\n\nThe rest of this paper is structured as follows. Section 2 explains DMHSA pooling. Section 3 gives the details of the DMHSA system applied to SV, with the experimental setup and results included. Section 4 gives the details of the DMHSA system applied to SC, with several applications and their experimental setup and results included. The concluding remarks are given in Section 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Double Multi-Head Self-Attention Pooling",
      "text": "In  [17] , DMHSA Pooling was proposed. It is a DL system with an attention-based pooling layer that was developed for SV. This model is trained as a speaker classifier, with the capability of learning effective discriminative speaker embeddings. The system architecture is illustrated in Figure  1 .\n\nThe proposed network was trained to classify variablelength speaker utterances. The system uses as input a spectrogram of the input audio, using the log Mel Spectrogram, with 25 milliseconds length Hamming windows and 10 milliseconds window shift. It accepts variable length input of size N (in frames) and has a fixed number of Mel bands M = 80. Then, the input resulting data is of size N x80. The audio features have been normalized with Cepstral Mean Normalization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cnn Front-End Feature Extractor",
      "text": "The feature extractor network is an adapted version of the VGG proposed in  [18] . It is fed with N x80 spectrograms to obtain a sequence of encoded hidden representations This CNN comprises 4 convolutional blocks, each of which contains two contatenated convolutional layers followed by a max pooling layer. Each convolutional layer has a set of 3x3 filters with a stride of 1 (a same convolution). Each max pooling layer consists of a 2x2 max pooling with a stride of 2 (and no padding). The first convolutional block applies a set of 128 filters, the second applies 256 filters, the third applies 512 filters and the fourth applies 1024 filters. Hence, given a spectrogram of N frames and M features, the VGG performs a down-sampling reducing its output into a tensor of N  16 x M 16 xD ′ dimension, where D ′ is the final quantity of channels. This tensor is reshaped into a sequence of hidden states hi ∈ R D , with i = 1, ..., N  16 and D = M 16 xD ′ .\n\nFigure  1 : DMHA architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Double Multi-Head Self Attention Pooling",
      "text": "From the feature extractor, we get a sequence of hidden states h1, ..., h N/16 ∈ R D . If we consider a number of K heads for the MHSA pooling, we split each hidden state ht into K new hidden states so that ht = [ht1, ..., htK ], where htj ∈ R D/K . Now, for each head j, a self-attention operation is applied over the encoded sequences [h1j, ..., h N 16 j ]. The weights of each head alignment are defined as:\n\nwhere wtj corresponds to the attention weight of the head j on the step t of the sequence, uj ∈ R D/K is a trainable parameter and d h corresponds to the hidden state dimension D/K. We then compute a new pooled representation for each head:\n\nwhere cj ∈ R D/K corresponds to the utterance level representation from head j. Each self-attention operation for the head j can be understood as a dot-product attention where the keys and the values correspond to the same representation [h1j, ..., h N 16 j ] and the query is only a trainable parameter uj.\n\nSelf-attention is now used to pool the set of head context vectors ci in order to obtain an overall context vector c:\n\nwhere w ′ i corresponds to the aligned weight of each head and u ′ ∈ R D/K is a trainable parameter. With this method, each utterance context vector c is computed as a weighted average of the context vectors among heads. This allows to capture different kinds of speaker patterns in different regions of the input and, at the same time, to weigh the relevance of each of these patterns for each utterance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fully-Connected Layers",
      "text": "The utterance-level speaker vector obtained from the pooling layer is fed into a set of four FC layers. Each of the first two FC layers is followed by a batch normalization and Rectified Linear Unit (ReLU) activations. A dense layer is adopted for the third FC layer and the last FC is a SoftMax layer that corresponds to the speaker classification output layer.\n\nOnce the network is trained, we can extract a speaker embedding from one of the intermediate FC layers. According to  [19] , we consider the second layer as the speaker embedding instead of the third one. The output of this layer then corresponds to the speaker representation that will be used for the speaker verification task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speaker Verification",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "The DMHSA system has been assessed in  [17]  by VoxCeleb dataset  [20, 21] . VoxCeleb is a large multimedia database that contains more than one million 16kHz audio utterances for more than 6K celebrities and has two different versions with several evaluation protocols. For  [17]  experiments, Vox-Celeb2 development partition with no augmentation has been used to train all models. The performance of these systems has been evaluated with Vox1-Test, Vox1-E, and Vox1-H conditions. Vox1 test only uses the test set, Vox1-E uses the whole development + test corpus and Vox1-H is restricted to audio pairs from same nationality and gender speakers.\n\nDMHSA pooling has been evaluated against two selfattentive-based pooling methods: MHSA  [7]  and vanilla Self-Attention (which is indeed a single-head MHSA). In order to evaluate them, only the pooling layer of the system (Figure  1 ) has been replaced without modifying any other block or parameter from the network. The speaker embeddings used for the verification tests have been extracted from the same FC layer for each of the pooling methods. Cosine distance has been used to compute the scores between pairs of speaker embeddings. The number of heads for both MHSA and DMHSA pooling layer were tuned: 8, 16, and 32 heads were considered.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "The results of  [17]  experiments are shown in Table  1 . Performance was evaluated using Equal Error Rate (EER). Both Self-Attention and MHSA approaches were used as baselines, since they have been proved to outperform three previous baselines: statistical and temporal pooling based methods and an i-vector + PLDA system  [7] . Self-Attention pooling has shown very similar results compared to MHSA approaches. DMHSA have shown better results for all head values compared with both Self Attention and MHSA approaches. Best performance in DMHSA based models has been achieved with 16 and 32 heads. Within Vox1-Test protocol, the best DMHSA model (16 heads) has shown a 6.73% and 5.06% relative improvement in terms of EER compared to Self Attention pooling and the best MHSA pooling (8 heads), respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker Characterization Applications",
      "text": "In this section we present several works that adapted and applied the DMHSA system to SC tasks, which are:  [22]  for SER,  [23]  for Speaker Sex Classification and  [24]  for Speaker COVID-19 Detection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker Emotion Recognition",
      "text": "SER is an SC task where the goal is to recognize the emotion from speech. Human emotion is classified using the six archetypal emotions approach, from which all emotional states can be derived  [25] . These archetypal emotions are Anger, Fear, Disgust, Surprise, Joy, Sadness and an added Neutral state. Several attempts at SER using statistical models have provided acceptable results, such as Gaussian Mixture Models (GMM)  [26] , Hidden Markov Models (HMM)  [27]  and i-vectors  [28] . Over the last years, SER has really taken off with the explosion of DL techniques  [29, 10, 30] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In  [22] , the DMHSA system was adapted to solve SER. It was assessed by the INTERFACE dataset  [31] . This database was designed for the general study of emotional speech. It contains recordings in four different languages: Slovenian, English, Spanish and French, distributed uniformly. For each language, there are 170-190 sentences spoken by two different actors, one male and one female, except for Spanish where two male actors and one female are used. Each sentence is spoken in seven different styles, for a total number of 24,197 recordings. The styles (emotion labels) are: Anger, Sadness, Joy, Fear, Disgust, Surprise and a Neutral style with different variations depending on the language. For the purpose of classification, all these styles were considered as one general \"Neutral\" style. Therefore, 28% of the samples were labelled as Neutral style. The rest of the samples are uniformly distributed between the other labels.\n\nThe DMHSA model was adapted to this problem by changing the SoftMax layer to a seven units output (one for each emotion label) and by using a 3-blocks front-end feature extractor instead of the 4-blocks one proposed in  [17] . Several models with different attention mechanisms (vanilla Self-Attention, MHSA and DMHSA) were trained: 1 for Self-Attention, 5 for MHSA and 5 for DMHSA, both with different head numbers (4, 8, 16, 32 and 64). These models were tested against a baseline model, which architecture had a statistical pooling component instead of an attention-based one.\n\nTo train the model, the entirety of the dataset was split into three parts: 70% train, 10% validation and 20% test. The training was done with early stopping parameter equal to 5, batch size was 64 and the number of seconds of temporal window taken from the audio files in training was fixed to 1 second. For validation and test the whole utterances have been encoded. Adam optimizer was used, the learning rate was set to 0.0001 and the weight decay parameter was set to 0.01. In the case of DMHSA, a head drop probability of 0.3 was fixed for 16, 32 and 64 heads, meaning that any given head had a probability of 30% to have its weight set to 0. The drop probability for 4 and 8 heads was set to 0.01 because the number of heads was too low for a 30% drop and caused instabilities in training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The results of  [22]  experiments are shown in Table  2 . Performance was evaluated on the test set using Accuracy. Selfattention performed close to statistical pooling, but slightly worse. For MHSA, the best results were obtained with 32 heads, with a relative accuracy improvement of 1.23% with respect to statistical pooling. For DMHSA, the best result was obtained again with 32 heads, but slightly worse than statistical pooling accuracy. MHSA has effectively obtained a better pooled representation than statistical pooling by being more selective about the information contained in the embedding. The reason for the underwhelming performance of Self-Attention could be the short 1 second temporal window used, since if the input contains more frames, more attention weights can be assigned and this could lead to a better pooled representation. While DMHSA provides an increase in performance for SV, the compression factor that comes with the DMHSA seems to affect the model's ability to predict emotion in this experiment (compared to MHSA which does not compress the final context vector).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speaker Sex Classification",
      "text": "Speaker Sex Classification is an SC task where the speaker's sex is extracted from a voice utterance. Over the last years, multiple investigations have tried to fully optimize Speaker Sex Classification systems' performance using DL in order to accomplish almost perfect results  [32, 33, 12, 11] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In  [23] , the DMHSA system was adapted to solve a speaker sex classification task. It was assessed by the Catalan Common Voice dataset  [34] . The Catalan Common Voice dataset was created in 2018 and has seen a huge expanding thanks to the promotion it has received in the last years and the voice donations of the Catalan-speaking population. The creation of this database and the need to adapt artificial intelligence technologies to the Catalan language, lead to the development of the AINA project  [35] . This project -promoted by the Departament de la Vicepresidència i de Polítiques Digitals i Territori of the Catalan government and the Barcelona Supercomputing Center-aims to shape speech recognition techniques to Catalan. For sex classification, three labels are given: Male (with 385,061 samples), Female (with 117,666 samples) and Other (with 418 samples). A significant imbalance towards the Male class can be observed. The class under the label Other was discarded. For this task's experiment, as the number of samples from each class in the dataset was large enough, almost completely balanced classes were used for the training set (around 70,000 samples from each class). Only utterances longer than 2.5 seconds were considered.\n\nThe DMHSA model was adapted to this problem by changing the SoftMax layer to a two units output (one for each sex label) and by using a 3-blocks front-end feature extractor. The DMHSA model was trained and tested with 32 heads and two different loss functions: Cross-Entropy (CE) and Weighted Cross-Entropy (WCE). To train the model, the entirety of the dataset was split into train, validation and test. The training was done with early stopping parameter equal to 5, batch size was 64 and the number of seconds of temporal window taken from the audio files in training was fixed to 2.5 seconds. Adam optimizer was used, the learning rate was set to 0.0001 and the weight decay parameter was set to 0.001. The results of  [23]  experiments are shown in Table  3 . Performance was evaluated on the test set using Accuracy and F-score.\n\nBoth DMHSA with CE and WCE performed with excellent results, above 95% accuracy and with a 0.96 f-score. As the classes were already almost perfectly balanced, using the WCE loss did not represent any significant improvements. The DMHSA model was adapted to this problem by chang-ing the SoftMax layer to a two units output (one for each diagnostic label) and by using a 3-blocks front-end feature extractor. Two 32-heads DMHSA models were trained using two different loss functions: CE and WCE. The training was done with early stopping parameter equal to 50 and a batch size of 64. Adam optimizer was used, the learning rate was set to 0.0001 and the weight decay parameter was set to 0.001. A head drop probability of 0.3 was fixed for the heads, meaning that any given head had a probability of 30% to have its weight set to 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "The results of  [24]  experiments are shown in Table  4 . Performance was evaluated on the test set using Area Under the receiver operating characteristic Curve (AUC). DMHSA with 32 heads and WCE loss performed better than DMHSA with 32 heads and CE loss, with a relative AUC improvement of 8.43%.\n\nIn this case, DMHSA with 32 heads and WCE achieved very good results, being able to detect effectively COVID-19 samples. Because of the imbalanced classes, using the WCE loss function improved the results drastically.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper we have described a Double Multi-Head Self-Attention pooling mechanism for speaker recognition. Its architecture consists of a CNN-based front-end, followed by an attention-based pooling layer and a set of fully connected layers. The network is trained as a speaker classifier and a bottleneck layer from these fully connected layers is used as speaker embedding. The presented approach has been evaluated in a text-independent speaker verification task using the speaker embeddings and applying the cosine distance. It has outperformed both vanilla Self Attention and Multi-Head Self-Attention pooling baseline methods. It also has been adapted to solve several speaker characterization tasks, namely speaker emotion recognition, sex classification and COVID-19 detection, obtaining excellent results.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed network was trained to classify variable-",
      "page": 1
    },
    {
      "caption": "Figure 1: DMHA architecture.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "federico.costa@upc.edu, miquel.angel.india@upc.edu,\njavier.hernando@upc.edu"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "to achieve excellent results in SV tasks and it is an improvement\nAbstract"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "of the Multi-Head Self-Attention (MHSA) pooling method pro-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "State-of-the-art Deep Learning systems for speaker verifi-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "posed in [7]. Since this system is trained as a speaker classifier,"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "cation are commonly based on speaker embedding extractors."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "it can be adapted to solve SC tasks. In this paper we will present"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "These architectures are usually composed of a feature extrac-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "several applications using an adapted version of\nthis architec-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tor front-end together with a pooling layer to encode variable-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "ture on different SC tasks, namely Speech Emotion Recogni-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "length utterances into fixed-length speaker vectors. The authors"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tion (SER), Speaker Sex Classification and Speaker COVID-19"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "have recently proposed the use of a Double Multi-Head Self-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Detection, achieving excellent results. We would like to men-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Attention pooling for\nspeaker\nrecognition, placed between a"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tion that all Speaker COVID-19 Detection results obtained in"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "CNN-based front-end and a set of fully connected layers. This"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "this work are meant for exploratory purposes and should not be"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "has shown to be an excellent approach to efficiently select\nthe"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "taken to draw any medical conclusions."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "most relevant features captured by the front-end from the speech"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "The rest of\nthis paper\nis structured as follows.\nSection 2"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "signal.\nIn this paper we show excellent experimental results by"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "explains DMHSA pooling.\nSection 3 gives the details of\nthe"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "adapting this architecture to other different speaker characteri-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "DMHSA system applied to SV, with the experimental setup and"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "zation tasks, such as emotion recognition, sex classification and"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "results\nincluded.\nSection 4 gives\nthe details of\nthe DMHSA"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "COVID-19 detection."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "system applied to SC, with several applications and their exper-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Index Terms:\nmulti-head\nself-attention,\ndouble\nattention,"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "imental setup and results included. The concluding remarks are"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "speech recognition, speaker verification, speaker characteriza-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "given in Section 5."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tion"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "2. Double Multi-Head Self-Attention"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "1.\nIntroduction"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Pooling"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Speaker Verification (SV) aims to determine whether a pair of"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "In [17], DMHSA Pooling was proposed. It is a DL system with"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "audios corresponds to the same speaker. Given speech signals,"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "an attention-based pooling layer that was developed for SV. This"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "speaker verification systems are able to extract speaker\niden-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "model\nis trained as a speaker classifier, with the capability of"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tity patterns from the characteristics of the voice. State-of-the-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "learning effective discriminative speaker embeddings. The sys-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "art SV systems are commonly Deep Learning (DL) approaches"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tem architecture is illustrated in Figure 1."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "that encode speaker characteristics into discriminative speaker"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "The proposed network was\ntrained to classify variable-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "vectors (also known as speaker embeddings). These architec-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "length speaker utterances. The system uses as input a spectro-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tures are usually trained as\nspeaker classifiers\nin order\nto be"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "gram of the input audio, using the log Mel Spectrogram, with"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "used as speaker embedding extractors. One of the most known"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "25 milliseconds length Hamming windows and 10 milliseconds"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "speaker\nrepresentations is the x-vector\n[1], which has become"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "window shift.\nIt accepts variable length input of\nsize N (in"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "state-of-the-art for speaker recognition. Recent network archi-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "frames) and has a fixed number of Mel bands M = 80. Then,"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tectures used for speaker embedding generation are composed"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "the input resulting data is of size N x80. The audio features have"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "of a front-end feature extractor, a pooling layer and a set of"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "been normalized with Cepstral Mean Normalization."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Fully Connected (FC)\nlayers.\nDuring the last years,\nseveral"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "studies addressed different types of pooling strategies [2, 3, 4]."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "2.1. CNN front-end feature extractor"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "X-vector originally uses statistical pooling [5].\nSelf-attention"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "mechanisms have been used to improve statistical pooling, such\nThe feature extractor network is an adapted version of the VGG"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "as [6, 7].\nproposed in [18].\nIt\nis fed with N x80 spectrograms to obtain"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "Some of these speaker embedding generation systems can\na sequence of encoded hidden representations This CNN com-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "be adapted and used to solve Speaker Characterization (SC)\nprises 4 convolutional blocks, each of which contains two con-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "tasks [8, 9]. SC tasks are those where one or more speaker char-\ntatenated convolutional layers followed by a max pooling layer."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "acteristics are extracted from the speech.\nIn the last years, DL\nEach convolutional layer has a set of 3x3 filters with a stride of 1"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "approaches were used in order to extract speaker characteristics\n(a same convolution). Each max pooling layer consists of a 2x2"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "from speech such as emotions [10], sex [11], age [12], language\nmax pooling with a stride of 2 (and no padding). The first con-"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "[13], dialect\n[14], accent\n[15], health conditions [16], among\nvolutional block applies a set of 128 filters,\nthe second applies"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "others.\n256 filters,\nthe third applies 512 filters and the fourth applies"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "1024 filters. Hence, given a spectrogram of N frames and M\nThe authors have recently proposed a DL system based on"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "features,\nthe VGG performs a down-sampling reducing its out-\na Double Multi-Head Self-Attention (DMHSA) pooling [17]."
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "put into a tensor of N\nIts\narchitecture\nconsists of\na Convolutional Neural Network\nis the final\n16 x M\n16 xD′ dimension, where D′"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "(CNN)-based front-end,\nfollowed by an attention-based pool-\nquantity of channels. This tensor is reshaped into a sequence of"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "hidden states hi ∈ RD, with i = 1, ..., N"
        },
        {
          "Universitat Politecnica de Catalunya, Barcelona, Spain": "ing layer and a set of fully connected layers. It has been proven\n16 and D = M\n16 xD′."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "′"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "u\n∈ RD/K is a trainable parameter."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "With this method, each utterance context vector c is com-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "puted as a weighted average of the context vectors among heads."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "This allows to capture different kinds of speaker patterns in dif-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "ferent regions of the input and, at\nthe same time,\nto weigh the"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "relevance of each of these patterns for each utterance."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "2.3. Fully-connected layers"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "The utterance-level speaker vector obtained from the pooling"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "layer is fed into a set of four FC layers. Each of the first two FC"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "layers is followed by a batch normalization and Rectified Linear"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Unit (ReLU) activations. A dense layer is adopted for the third"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "FC layer and the last FC is a SoftMax layer that corresponds to"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "the speaker classification output layer."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Once the network is trained, we can extract a speaker em-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "bedding from one of the intermediate FC layers. According to"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "[19], we consider the second layer as the speaker embedding in-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "stead of the third one. The output of this layer then corresponds"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "to the speaker representation that will be used for the speaker"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "verification task."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "3.\nSpeaker Verification"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "3.1. Experimental setup"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "The DMHSA system has been assessed in [17] by VoxCeleb"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "dataset\n[20, 21].\nVoxCeleb is\na\nlarge multimedia database"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "that contains more than one million 16kHz audio utterances"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "for more\nthan 6K celebrities\nand has\ntwo different versions"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "with several evaluation protocols. For [17] experiments, Vox-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Celeb2 development partition with no augmentation has been"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "used to train all models.\nThe performance of\nthese systems"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "has been evaluated with Vox1-Test, Vox1-E, and Vox1-H con-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "ditions. Vox1 test only uses the test set, Vox1-E uses the whole"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "development + test corpus and Vox1-H is\nrestricted to audio"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "pairs from same nationality and gender speakers."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "DMHSA pooling\nhas\nbeen\nevaluated\nagainst\ntwo\nself-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "attentive-based pooling methods: MHSA [7] and vanilla Self-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Attention (which is indeed a single-head MHSA).\nIn order\nto"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "evaluate them, only the pooling layer of the system (Figure 1)"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "has been replaced without modifying any other block or param-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "eter\nfrom the network.\nThe speaker embeddings used for\nthe"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "verification tests have been extracted from the same FC layer for"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "each of the pooling methods. Cosine distance has been used to"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "compute the scores between pairs of speaker embeddings. The"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "number of heads for both MHSA and DMHSA pooling layer"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "were tuned: 8, 16, and 32 heads were considered."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "3.2. Results"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "The results of [17] experiments are shown in Table 1. Perfor-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "mance was evaluated using Equal Error Rate (EER). Both Self-"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Attention and MHSA approaches were used as baselines, since"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "they have been proved to outperform three previous baselines:"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "statistical and temporal pooling based methods and an i-vector"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "+ PLDA system [7].\nSelf-Attention pooling has\nshown very"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "similar results compared to MHSA approaches. DMHSA have"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "shown better\nresults\nfor all head values compared with both"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Self Attention and MHSA approaches.\nBest performance in"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "DMHSA based models has been achieved with 16 and 32 heads."
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "Within Vox1-Test protocol, the best DMHSA model (16 heads)"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "has shown a 6.73% and 5.06% relative improvement\nin terms"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": "of EER compared to Self Attention pooling and the best MHSA"
        },
        {
          "′ i\ncorresponds to the aligned weight of each head and\nwhere w": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "The results of\n[22] experiments are shown in Table 2.\nPer-"
        },
        {
          "pooling (8 heads), respectively.": "4.\nSpeaker characterization applications",
          "4.1.2. Results": "formance was evaluated on the test set using Accuracy.\nSelf-"
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "attention performed close\nto statistical pooling,\nbut\nslightly"
        },
        {
          "pooling (8 heads), respectively.": "In this section we present several works that adapted and applied",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "worse. For MHSA, the best results were obtained with 32 heads,"
        },
        {
          "pooling (8 heads), respectively.": "the DMHSA system to SC tasks, which are: [22] for SER, [23]",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "with a relative accuracy improvement of 1.23% with respect to"
        },
        {
          "pooling (8 heads), respectively.": "for Speaker Sex Classification and [24] for Speaker COVID-19",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "statistical pooling.\nFor DMHSA,\nthe best\nresult was obtained"
        },
        {
          "pooling (8 heads), respectively.": "Detection.",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "again with 32 heads, but\nslightly worse than statistical pool-"
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "ing accuracy. MHSA has effectively obtained a better pooled"
        },
        {
          "pooling (8 heads), respectively.": "4.1.\nSpeaker Emotion Recognition",
          "4.1.2. Results": "representation than statistical pooling by being more selective"
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "about\nthe information contained in the embedding. The reason"
        },
        {
          "pooling (8 heads), respectively.": "SER is an SC task where the goal\nis to recognize the emotion",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "for the underwhelming performance of Self-Attention could be"
        },
        {
          "pooling (8 heads), respectively.": "from speech. Human emotion is classified using the six archety-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "the short 1 second temporal window used,\nsince if\nthe input"
        },
        {
          "pooling (8 heads), respectively.": "pal emotions approach, from which all emotional states can be",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "contains more frames, more attention weights can be assigned"
        },
        {
          "pooling (8 heads), respectively.": "derived [25]. These archetypal emotions are Anger, Fear, Dis-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "and this could lead to a better pooled representation. While"
        },
        {
          "pooling (8 heads), respectively.": "gust, Surprise, Joy, Sadness and an added Neutral state. Several",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "DMHSA provides an increase in performance for SV, the com-"
        },
        {
          "pooling (8 heads), respectively.": "attempts at SER using statistical models have provided accept-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "pression factor that comes with the DMHSA seems to affect the"
        },
        {
          "pooling (8 heads), respectively.": "able results,\nsuch as Gaussian Mixture Models (GMM)\n[26],",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "model’s ability to predict emotion in this experiment (compared"
        },
        {
          "pooling (8 heads), respectively.": "Hidden Markov Models (HMM) [27] and i-vectors [28]. Over",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "to MHSA which does not compress the final context vector)."
        },
        {
          "pooling (8 heads), respectively.": "the last years, SER has really taken off with the explosion of DL",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "techniques [29, 10, 30].",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "4.2.\nSpeaker Sex Classification"
        },
        {
          "pooling (8 heads), respectively.": "4.1.1. Experimental setup",
          "4.1.2. Results": "Speaker Sex Classification is an SC task where the speaker’s sex"
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "is extracted from a voice utterance. Over the last years, multiple"
        },
        {
          "pooling (8 heads), respectively.": "In [22],\nthe DMHSA system was adapted to solve SER. It was",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "investigations have tried to fully optimize Speaker Sex Classi-"
        },
        {
          "pooling (8 heads), respectively.": "assessed by the INTERFACE dataset [31]. This database was",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "fication systems’ performance using DL in order to accomplish"
        },
        {
          "pooling (8 heads), respectively.": "designed for\nthe general\nstudy of emotional\nspeech.\nIt con-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "almost perfect results [32, 33, 12, 11]."
        },
        {
          "pooling (8 heads), respectively.": "tains recordings in four different languages: Slovenian, English,",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "Spanish and French, distributed uniformly. For each language,",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "4.2.1. Experimental setup"
        },
        {
          "pooling (8 heads), respectively.": "there are 170-190 sentences spoken by two different actors, one",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "male and one female, except\nfor Spanish where two male ac-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "In [23],\nthe DMHSA system was adapted to solve a speaker"
        },
        {
          "pooling (8 heads), respectively.": "tors and one female are used. Each sentence is spoken in seven",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "sex classification task.\nIt was assessed by the Catalan Com-"
        },
        {
          "pooling (8 heads), respectively.": "different styles,\nfor a total number of 24,197 recordings. The",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "mon Voice dataset\n[34].\nThe Catalan Common Voice dataset"
        },
        {
          "pooling (8 heads), respectively.": "styles (emotion labels) are: Anger, Sadness, Joy, Fear, Disgust,",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "was created in 2018 and has seen a huge expanding thanks to"
        },
        {
          "pooling (8 heads), respectively.": "Surprise and a Neutral style with different variations depend-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "the promotion it has\nreceived in the last years and the voice"
        },
        {
          "pooling (8 heads), respectively.": "ing on the language. For the purpose of classification, all these",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "donations of the Catalan-speaking population. The creation of"
        },
        {
          "pooling (8 heads), respectively.": "styles were considered as one general “Neutral” style. There-",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "this database and the need to adapt artificial\nintelligence tech-"
        },
        {
          "pooling (8 heads), respectively.": "fore, 28% of\nthe samples were labelled as Neutral style. The",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "nologies to the Catalan language,\nlead to the development of"
        },
        {
          "pooling (8 heads), respectively.": "rest of the samples are uniformly distributed between the other",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "the AINA project [35]. This project -promoted by the Depar-"
        },
        {
          "pooling (8 heads), respectively.": "labels.",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "tament de la Vicepresid`encia i de Pol´ıtiques Digitals i Territori"
        },
        {
          "pooling (8 heads), respectively.": "The DMHSA model was adapted to this problem by chang-",
          "4.1.2. Results": "of the Catalan government and the Barcelona Supercomputing"
        },
        {
          "pooling (8 heads), respectively.": "ing the SoftMax layer to a seven units output (one for each emo-",
          "4.1.2. Results": "Center- aims to shape speech recognition techniques to Cata-"
        },
        {
          "pooling (8 heads), respectively.": "tion label) and by using a 3-blocks front-end feature extractor",
          "4.1.2. Results": "lan.\nFor sex classification,\nthree labels are given: Male (with"
        },
        {
          "pooling (8 heads), respectively.": "instead of\nthe 4-blocks one proposed in [17].\nSeveral mod-",
          "4.1.2. Results": "385,061 samples), Female (with 117,666 samples) and Other"
        },
        {
          "pooling (8 heads), respectively.": "els with different attention mechanisms (vanilla Self-Attention,",
          "4.1.2. Results": "(with 418 samples). A significant\nimbalance towards the Male"
        },
        {
          "pooling (8 heads), respectively.": "MHSA and DMHSA) were trained: 1 for Self-Attention, 5 for",
          "4.1.2. Results": "class can be observed. The class under the label Other was dis-"
        },
        {
          "pooling (8 heads), respectively.": "MHSA and 5 for DMHSA, both with different head numbers (4,",
          "4.1.2. Results": "carded.\nFor\nthis task’s experiment, as the number of samples"
        },
        {
          "pooling (8 heads), respectively.": "8, 16, 32 and 64). These models were tested against a baseline",
          "4.1.2. Results": "from each class in the dataset was large enough, almost com-"
        },
        {
          "pooling (8 heads), respectively.": "model, which architecture had a statistical pooling component",
          "4.1.2. Results": "pletely balanced classes were used for the training set (around"
        },
        {
          "pooling (8 heads), respectively.": "instead of an attention-based one.",
          "4.1.2. Results": "70,000 samples from each class). Only utterances longer than"
        },
        {
          "pooling (8 heads), respectively.": "",
          "4.1.2. Results": "2.5 seconds were considered."
        },
        {
          "pooling (8 heads), respectively.": "To train the model, the entirety of the dataset was split into",
          "4.1.2. Results": ""
        },
        {
          "pooling (8 heads), respectively.": "three parts: 70% train, 10% validation and 20% test. The train-",
          "4.1.2. Results": "The DMHSA model was adapted to this problem by chang-"
        },
        {
          "pooling (8 heads), respectively.": "ing was done with early stopping parameter equal\nto 5, batch",
          "4.1.2. Results": "ing the SoftMax layer\nto a\ntwo units output\n(one\nfor\neach"
        },
        {
          "pooling (8 heads), respectively.": "size was 64 and the number of seconds of\ntemporal window",
          "4.1.2. Results": "sex label) and by using a 3-blocks front-end feature extractor."
        },
        {
          "pooling (8 heads), respectively.": "taken from the audio files\nin training was fixed to 1 second.",
          "4.1.2. Results": "The DMHSA model was trained and tested with 32 heads and"
        },
        {
          "pooling (8 heads), respectively.": "For validation and test the whole utterances have been encoded.",
          "4.1.2. Results": "two different loss functions: Cross-Entropy (CE) and Weighted"
        },
        {
          "pooling (8 heads), respectively.": "Adam optimizer was used,\nthe learning rate was set\nto 0.0001",
          "4.1.2. Results": "Cross-Entropy (WCE). To train the model,\nthe entirety of\nthe"
        },
        {
          "pooling (8 heads), respectively.": "and the weight decay parameter was set\nto 0.01.\nIn the case of",
          "4.1.2. Results": "dataset was split into train, validation and test. The training was"
        },
        {
          "pooling (8 heads), respectively.": "DMHSA, a head drop probability of 0.3 was fixed for 16, 32",
          "4.1.2. Results": "done with early stopping parameter equal\nto 5, batch size was"
        },
        {
          "pooling (8 heads), respectively.": "and 64 heads, meaning that any given head had a probability of",
          "4.1.2. Results": "64 and the number of seconds of temporal window taken from"
        },
        {
          "pooling (8 heads), respectively.": "30% to have its weight set to 0. The drop probability for 4 and",
          "4.1.2. Results": "the audio files in training was fixed to 2.5 seconds. Adam op-"
        },
        {
          "pooling (8 heads), respectively.": "8 heads was set\nto 0.01 because the number of heads was too",
          "4.1.2. Results": "timizer was used,\nthe learning rate was set\nto 0.0001 and the"
        },
        {
          "pooling (8 heads), respectively.": "low for a 30% drop and caused instabilities in training.",
          "4.1.2. Results": "weight decay parameter was set to 0.001."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "Heads"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "1"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "8"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "16"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "32"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "8"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "16"
        },
        {
          "Table 1: Speaker verification evaluation results on VoxCeleb 1 protocols.": "32"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.19\nDMHSA\n16": "3.23\nDMHSA\n32",
          "3.22\n4.67": "3.18\n4.61"
        },
        {
          "3.19\nDMHSA\n16": "Table 2: SER test results from [22]",
          "3.22\n4.67": ""
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "ing the SoftMax layer to a two units output (one for each diag-"
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "nostic label) and by using a 3-blocks front-end feature extractor."
        },
        {
          "3.19\nDMHSA\n16": "Model\nAccuracy",
          "3.22\n4.67": "Two 32-heads DMHSA models were trained using two different"
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "loss functions: CE and WCE. The training was done with early"
        },
        {
          "3.19\nDMHSA\n16": "91.09%\nMHSA 32 Heads",
          "3.22\n4.67": ""
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "stopping parameter equal\nto 50 and a batch size of 64. Adam"
        },
        {
          "3.19\nDMHSA\n16": "89.98%\nStatistical Pooling",
          "3.22\n4.67": ""
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "optimizer was used, the learning rate was set to 0.0001 and the"
        },
        {
          "3.19\nDMHSA\n16": "89.87%\nDMHSA 32 Heads",
          "3.22\n4.67": ""
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "weight decay parameter was set\nto 0.001. A head drop prob-"
        },
        {
          "3.19\nDMHSA\n16": "89.32%\nSelf-Attention",
          "3.22\n4.67": ""
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "ability of 0.3 was fixed for the heads, meaning that any given"
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "head had a probability of 30% to have its weight set to 0."
        },
        {
          "3.19\nDMHSA\n16": "4.2.2. Results",
          "3.22\n4.67": "4.3.2. Results"
        },
        {
          "3.19\nDMHSA\n16": "The results of [23] experiments are shown in Table 3. Perfor-",
          "3.22\n4.67": "The results of [24] experiments are shown in Table 4. Perfor-"
        },
        {
          "3.19\nDMHSA\n16": "mance was evaluated on the test set using Accuracy and F-score.",
          "3.22\n4.67": "mance was evaluated on the test set using Area Under\nthe re-"
        },
        {
          "3.19\nDMHSA\n16": "Both DMHSA with CE and WCE performed with excellent re-",
          "3.22\n4.67": "ceiver operating characteristic Curve (AUC). DMHSA with 32"
        },
        {
          "3.19\nDMHSA\n16": "sults,\nabove 95% accuracy and with a 0.96 f-score.\nAs\nthe",
          "3.22\n4.67": "heads and WCE loss performed better\nthan DMHSA with 32"
        },
        {
          "3.19\nDMHSA\n16": "classes were already almost perfectly balanced, using the WCE",
          "3.22\n4.67": "heads and CE loss, with a relative AUC improvement of 8.43%."
        },
        {
          "3.19\nDMHSA\n16": "loss did not represent any significant improvements.",
          "3.22\n4.67": "In this case, DMHSA with 32 heads and WCE achieved very"
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "good results, being able to detect effectively COVID-19 sam-"
        },
        {
          "3.19\nDMHSA\n16": "Table 3: Speaker sex classification test results from [23]",
          "3.22\n4.67": "ples. Because of the imbalanced classes, using the WCE loss"
        },
        {
          "3.19\nDMHSA\n16": "",
          "3.22\n4.67": "function improved the results drastically."
        },
        {
          "3.19\nDMHSA\n16": "Model\nAccuracy\nF-Score",
          "3.22\n4.67": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "verification,” arXiv preprint, p. arXiv:1904.03479, 2019."
        },
        {
          "7. References": "[1] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "pur, “X-vectors: Robust dnn embeddings for speaker recognition,”",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[20]\nJ. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep"
        },
        {
          "7. References": "in 2018 IEEE International Conference on Acoustics, Speech and",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "speaker recognition,” in INTERSPEECH, 2018."
        },
        {
          "7. References": "Signal Processing (ICASSP), 2018, pp. 5329–5333.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[21] A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb: A Large-"
        },
        {
          "7. References": "[2] W. Cai, J. Chen, and M. Li, “Exploring the Encoding Layer and",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Scale Speaker Identification Dataset,” in Proc. Interspeech 2017,"
        },
        {
          "7. References": "Loss Function in End-to-End Speaker and Language Recognition",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "2017, pp. 2616–2620."
        },
        {
          "7. References": "System,” in Proc. The Speaker and Language Recognition Work-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[22] D. Arom´ı,\n“Predicting\nemotion\nin\nspeech:\na\ndeep\nlearning"
        },
        {
          "7. References": "shop (Odyssey 2018), 2018, pp. 74–81.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "approach\nusing\nattention mechanisms,” Univ.\nPolit`ecnica\nde"
        },
        {
          "7. References": "[3] W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, “Utterance-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Catalunya, B.S. Degree Thesis, 2021."
        },
        {
          "7. References": "level aggregation for\nspeaker\nrecognition in the wild,” ICASSP",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[23] D. Garriga, “Deep learning for speaker characterization,” Univ."
        },
        {
          "7. References": "2019-2019 IEEE International Conference on Acoustics, Speech",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Polit`ecnica de Catalunya, B.S. Degree Thesis, 2022."
        },
        {
          "7. References": "and Signal Processing (ICASSP), pp. 5791–5795, 2019.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[24] D. Marchan, “Dise˜no e implementaci´on de un sistema de deep"
        },
        {
          "7. References": "[4] Y. Jung, Y. Kim, H. Lim, Y. Choi, and H. Kim, “Spatial pyramid",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "learning para la detecci´on de covid por\nla tos con aumento de"
        },
        {
          "7. References": "encoding with convex length normalization for text-independent",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "datos,” Univ. Polit`ecnica de Catalunya, B.S. Degree Thesis, 2022."
        },
        {
          "7. References": "speaker verification,” arXiv preprint, p. arXiv:1906.08333, 2019.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[25] R. Cowie and R. R. Cornelius, “Describing the emotional states"
        },
        {
          "7. References": "[5] D.\nSnyder,\nD. Garcia-Romero,\nD.\nPovey,\nand\nS. Khudan-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "that are expressed in speech,” Speech Communication, vol. 40,"
        },
        {
          "7. References": "pur,\n“Deep Neural Network Embeddings\nfor Text-Independent",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "no. 1, pp. 5–32, Apr. 2003."
        },
        {
          "7. References": "Speaker Verification,” in Proc. Interspeech 2017, 2017, pp. 999–",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "1003.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[26] M. M. H. E. Ayadi, M. S. Kamel, and F. Karray, “Speech emotion"
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "recognition using gaussian mixture vector autoregressive models,”"
        },
        {
          "7. References": "[6] Y. Zhu, T. Ko, D. Snyder, B. Mak, and D. Povey, “Self-Attentive",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "in 2007 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "7. References": "Speaker Embeddings for Text-Independent Speaker Verification,”",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Signal Processing -\nICASSP ’07, vol. 4, 2007, pp.\nIV–957–IV–"
        },
        {
          "7. References": "in Proc. Interspeech 2018, 2018, pp. 3573–3577.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "960."
        },
        {
          "7. References": "[7] M. India, P. Safari, and J. Hernando, “Self Multi-Head Attention",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[27] A. Nogueiras,\nJ. B. Mari˜no, A. Bonafonte,\nand A. Moreno,"
        },
        {
          "7. References": "for Speaker Recognition,” in Proc.\nInterspeech 2019, 2019, pp.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "“Speech emotion recognition using hidden markov models,” in"
        },
        {
          "7. References": "4305–4309.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "EUROSPEECH 2001 - SCANDINAVIA - 7th European Confer-"
        },
        {
          "7. References": "[8] D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, D. Povey,",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "ence on Speech Communication and Technology, 2001, pp. 2679–"
        },
        {
          "7. References": "and S. Khudanpur,\n“Spoken Language Recognition\nusing X-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "2682."
        },
        {
          "7. References": "vectors,” in Proc. The Speaker and Language Recognition Work-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[28] R. Xia and Y. Liu, “Using i-vector space model for emotion recog-"
        },
        {
          "7. References": "shop (Odyssey 2018), 2018, pp. 105–111.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "nition,” in Proc. Interspeech 2012, 2012, pp. 2230–2233."
        },
        {
          "7. References": "[9] R. Pappagari, T. Wang, J. Villalba, N. Chen, and N. Dehak, “X-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "vectors meet emotions: A study on dependencies between emo-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[29]\nJ. Kim, G. Englebienne, K. P. Truong, and V. Evers, “Towards"
        },
        {
          "7. References": "tion and speaker recognition,” in ICASSP 2020 - 2020 IEEE Inter-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Speech Emotion Recognition “in the Wild” Using Aggregated"
        },
        {
          "7. References": "national Conference on Acoustics, Speech and Signal Processing",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Interspeech\nCorpora and Deep Multi-Task Learning,”\nin Proc."
        },
        {
          "7. References": "(ICASSP), 2020, pp. 7169–7173.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "2017, 2017, pp. 1113–1117."
        },
        {
          "7. References": "[10] H. Meng, T. Yan, F. Yuan, and H. Wei, “Speech emotion recogni-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[30] Y. Li, T. Zhao, and T. Kawahara, “Improved End-to-End Speech"
        },
        {
          "7. References": "tion from 3d log-mel spectrograms with deep learning network,”",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Emotion Recognition Using Self Attention Mechanism and Mul-"
        },
        {
          "7. References": "IEEE Access, vol. 7, pp. 125 868–125 881, 2019.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "titask Learning,” in Proc. Interspeech 2019, 2019, pp. 2803–2807."
        },
        {
          "7. References": "[11] A. A. Alnuaim, M. Zakariah, C. Shashidhar, W. A. Hatamleh,",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[31] V. Hozjan, Z. Kacic, A. Moreno, A. Bonafonte, and A. Nogueiras,"
        },
        {
          "7. References": "H. Tarazi, P. K. Shukla, and R. Ratna, “Speaker gender recogni-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "“Interface databases: Design and collection of a multilingual emo-"
        },
        {
          "7. References": "tion based on deep neural networks and resnet50,” Wireless Com-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "tional speech database,” in Proceedings of the Third International"
        },
        {
          "7. References": "munications and Mobile Computing, vol. 2022, pp. 1–13, Mar.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Conference on Language Resources and Evaluation (LREC’02),"
        },
        {
          "7. References": "2022.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "May 2002."
        },
        {
          "7. References": "[12] A. Tursunov, M. Mustaqeem,\nJ. Y. Choeh, and S. Kwon, “Age",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[32] M. Buyukyilmaz and A. O. Cibikdiken, “Voice gender recognition"
        },
        {
          "7. References": "and gender recognition using a convolutional neural network with",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "using deep learning,” in Proceedings of 2016 International Con-"
        },
        {
          "7. References": "a specially designed multi-attention module through speech spec-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "ference on Modeling, Simulation and Optimization Technologies"
        },
        {
          "7. References": "trograms,” Sensors, vol. 21, p. 5892, Sep. 2021.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "and Applications (MSOTA2016), 2016, pp. 409–411."
        },
        {
          "7. References": "[13] C. Bartz, T. Herold, H. Yang, and C. Meinel, “Language identi-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[33]\nF. Ertam, “An effective gender recognition approach using voice"
        },
        {
          "7. References": "fication using deep convolutional\nrecurrent neural networks,” in",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "data via deeper lstm networks,” Applied Acoustics, vol. 156, pp."
        },
        {
          "7. References": "Neural Information Processing, 2017, pp. 880–889.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "351–358, Aug. 2019."
        },
        {
          "7. References": "[14] D. Wang, S. Ye, X. Hu, S. Li, and X. Xu, “An End-to-End Di-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[34]\n“Mozilla\ncatalan common voice dataset,”\nJul. 2022.\n[Online]."
        },
        {
          "7. References": "alect\nIdentification System with Transfer Learning from a Mul-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Available: https://commonvoice.mozilla.org/ca"
        },
        {
          "7. References": "Inter-\ntilingual Automatic Speech Recognition Model,” in Proc.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[35]\n“Aina:\nLa nostra llengua\nes\nla teva veu,”\nJul. 2022.\n[Online]."
        },
        {
          "7. References": "speech 2021, 2021, pp. 3266–3270.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "Available: https://www.projecteaina.cat"
        },
        {
          "7. References": "[15]\nF. Weninger, Y. Sun,\nJ. Park, D. Willett,\nand P. Zhan,\n“Deep",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[36] M. A. Nessiem, M. M. Mohamed, H. Coppock, A. Gaskell, and"
        },
        {
          "7. References": "Learning Based Mandarin Accent\nIdentification for Accent Ro-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "B. W. Schuller, “Detecting covid-19 from breathing and coughing"
        },
        {
          "7. References": "bust ASR,” in Proc. Interspeech 2019, 2019, pp. 510–514.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "sounds using deep neural networks,” in 2021 IEEE 34th Interna-"
        },
        {
          "7. References": "[16] N. Cummins, A. Baird, and B. W. Schuller, “Speech analysis for",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "tional Symposium on Computer-Based Medical Systems (CBMS),"
        },
        {
          "7. References": "health: Current state-of-the-art and the increasing impact of deep",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "06 2021, pp. 183–188."
        },
        {
          "7. References": "learning,” Methods, vol. 151, pp. 41–54, Aug. 2018.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[37] A. B. Nassif,\nI. Shahin, M. Bader, A. Hassan, and N. Werghi,"
        },
        {
          "7. References": "[17] M.\nIndia, P. Safari, and J. Hernando, “Double multi-head atten-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "“Covid-19\ndetection\nsystems\nusing\ndeep-learning\nalgorithms"
        },
        {
          "7. References": "tion for speaker verification,” in ICASSP 2021 - 2021 IEEE Inter-",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "based on speech and image data,” Mathematics, vol. 10, no. 4,"
        },
        {
          "7. References": "national Conference on Acoustics, Speech and Signal Processing",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "p. 564, 2022."
        },
        {
          "7. References": "(ICASSP), 2021, pp. 6144–6148.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "[38] B. Schuller, A. Batliner, C. Bergler, and C. Mascolo, “The inter-"
        },
        {
          "7. References": "[18]\nT. Hori, S. Watanabe, Y. Zhang,\nand W. Chan,\n“Advances\nin",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "speech 2021 computational paralinguistics challenge: Covid-19"
        },
        {
          "7. References": "Joint CTC-Attention Based End-to-End Speech Recognition with",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "cough, covid-19 speech, escalation & primates,” INTERSPEECH"
        },
        {
          "7. References": "a Deep CNN Encoder and RNN-LM,” in Proc. Interspeech 2017,",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        },
        {
          "7. References": "",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": "2021, 2021."
        },
        {
          "7. References": "2017, pp. 949–953.",
          "[19] Y. Liu, L. He, and J. Liu, “Large margin softmax loss for speaker": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Exploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System",
      "authors": [
        "W Cai",
        "J Chen",
        "M Li"
      ],
      "year": "2018",
      "venue": "Proc. The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "3",
      "title": "Utterancelevel aggregation for speaker recognition in the wild",
      "authors": [
        "W Xie",
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Spatial pyramid encoding with convex length normalization for text-independent speaker verification",
      "authors": [
        "Y Jung",
        "Y Kim",
        "H Lim",
        "Y Choi",
        "H Kim"
      ],
      "year": "2019",
      "venue": "Spatial pyramid encoding with convex length normalization for text-independent speaker verification",
      "arxiv": "arXiv:1906.08333"
    },
    {
      "citation_id": "5",
      "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification",
      "authors": [
        "Y Zhu",
        "T Ko",
        "D Snyder",
        "B Mak",
        "D Povey"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Self Multi-Head Attention for Speaker Recognition",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "Spoken Language Recognition using Xvectors",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "A Mccree",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "9",
      "title": "Xvectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "Speaker gender recognition based on deep neural networks and resnet50",
      "authors": [
        "A Alnuaim",
        "M Zakariah",
        "C Shashidhar",
        "W Hatamleh",
        "H Tarazi",
        "P Shukla",
        "R Ratna"
      ],
      "year": "2022",
      "venue": "Wireless Communications and Mobile Computing"
    },
    {
      "citation_id": "12",
      "title": "Age and gender recognition using a convolutional neural network with a specially designed multi-attention module through speech spectrograms",
      "authors": [
        "A Tursunov",
        "M Mustaqeem",
        "J Choeh",
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "Language identification using deep convolutional recurrent neural networks",
      "authors": [
        "C Bartz",
        "T Herold",
        "H Yang",
        "C Meinel"
      ],
      "year": "2017",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "14",
      "title": "An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model",
      "authors": [
        "D Wang",
        "S Ye",
        "X Hu",
        "S Li",
        "X Xu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "15",
      "title": "Deep Learning Based Mandarin Accent Identification for Accent Robust ASR",
      "authors": [
        "F Weninger",
        "Y Sun",
        "J Park",
        "D Willett",
        "P Zhan"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Speech analysis for health: Current state-of-the-art and the increasing impact of deep learning",
      "authors": [
        "N Cummins",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Methods"
    },
    {
      "citation_id": "17",
      "title": "Double multi-head attention for speaker verification",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
      "authors": [
        "T Hori",
        "S Watanabe",
        "Y Zhang",
        "W Chan"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Large margin softmax loss for speaker verification",
      "authors": [
        "Y Liu",
        "L He",
        "J Liu"
      ],
      "year": "2019",
      "venue": "Large margin softmax loss for speaker verification",
      "arxiv": "arXiv:1904.03479"
    },
    {
      "citation_id": "20",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "21",
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Predicting emotion in speech: a deep learning approach using attention mechanisms",
      "authors": [
        "D Aromí"
      ],
      "year": "2021",
      "venue": "Predicting emotion in speech: a deep learning approach using attention mechanisms"
    },
    {
      "citation_id": "23",
      "title": "Deep learning for speaker characterization",
      "authors": [
        "D Garriga"
      ],
      "year": "2022",
      "venue": "Deep learning for speaker characterization"
    },
    {
      "citation_id": "24",
      "title": "Diseño e implementación de un sistema de deep learning para la detección de covid por la tos con aumento de datos",
      "authors": [
        "D Marchan"
      ],
      "year": "2022",
      "venue": "Diseño e implementación de un sistema de deep learning para la detección de covid por la tos con aumento de datos"
    },
    {
      "citation_id": "25",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "R Cowie",
        "R Cornelius"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP '07"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "A Nogueiras",
        "J Mariño",
        "A Bonafonte",
        "A Moreno"
      ],
      "year": "2001",
      "venue": "EUROSPEECH 2001 -SCANDINAVIA -7th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "28",
      "title": "Using i-vector space model for emotion recognition",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2012",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Towards Speech Emotion Recognition \"in the Wild\" Using Aggregated Corpora and Deep Multi-Task Learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Interface databases: Design and collection of a multilingual emotional speech database",
      "authors": [
        "V Hozjan",
        "Z Kacic",
        "A Moreno",
        "A Bonafonte",
        "A Nogueiras"
      ],
      "year": "2002",
      "venue": "Proceedings of the Third International Conference on Language Resources and Evaluation (LREC'02)"
    },
    {
      "citation_id": "32",
      "title": "Voice gender recognition using deep learning",
      "authors": [
        "M Buyukyilmaz",
        "A Cibikdiken"
      ],
      "year": "2016",
      "venue": "Proceedings of 2016 International Conference on Modeling, Simulation and Optimization Technologies and Applications"
    },
    {
      "citation_id": "33",
      "title": "An effective gender recognition approach using voice data via deeper lstm networks",
      "authors": [
        "F Ertam"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "34",
      "title": "Mozilla catalan common voice dataset",
      "year": "2022",
      "venue": "Mozilla catalan common voice dataset"
    },
    {
      "citation_id": "35",
      "title": "Aina: La nostra llengua és la teva veu",
      "year": "2022",
      "venue": "Aina: La nostra llengua és la teva veu"
    },
    {
      "citation_id": "36",
      "title": "Detecting covid-19 from breathing and coughing sounds using deep neural networks",
      "authors": [
        "M Nessiem",
        "M Mohamed",
        "H Coppock",
        "A Gaskell",
        "B Schuller"
      ],
      "venue": "2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)"
    },
    {
      "citation_id": "37",
      "title": "Covid-19 detection systems using deep-learning algorithms based on speech and image data",
      "authors": [
        "A Nassif",
        "I Shahin",
        "M Bader",
        "A Hassan",
        "N Werghi"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "38",
      "title": "The interspeech 2021 computational paralinguistics challenge: Covid-19 cough, covid-19 speech, escalation & primates",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "C Mascolo"
      ],
      "year": "2021",
      "venue": "The interspeech 2021 computational paralinguistics challenge: Covid-19 cough, covid-19 speech, escalation & primates"
    }
  ]
}