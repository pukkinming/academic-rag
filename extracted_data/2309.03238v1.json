{
  "paper_id": "2309.03238v1",
  "title": "Implicit Design Choices And Their Impact On Emotion Recognition Model Development And Evaluation",
  "published": "2023-09-06T02:45:42Z",
  "authors": [
    "Mimansa Jaiswal"
  ],
  "keywords": [
    "Emotion Production and Emotion Perception . . . . . . 3 1.1.2 Emotion Theories",
    "Research Challenges",
    "and Implications for Emotion Recognition . . . . . . . . . . . . . . 4 1.1.3 Addressing Challenges Through Thesis Contributions . . 5 1.2 Emotion Recognition . . 2.1.5 Demographics in Dataset Collection Recruitment . . . . 2.2 Crowdsourcing and Context in Emotion Recognition . . . . . . . . 2.3 Handling Confounding Factors . . . . . . . . . . . . . . . . . . . 2.3.1 Singularly Labeled or Unlabeled Factors . . . . . . . . . 2.3.2 Explicitly Labeled Factors . . . . . . . . . . . . . . . . 2.4 Noise and Approaches to Dealing with it in Machine Learning Models 2.5 Unintentional Sensisitve Variable Encoding",
    "and Ethical Considerations in Data Collection and Neural Networks . . . . . . . . . . . 2.6 The Role of Interpretability in Model Trustworthiness . . . ."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "way or format. Conversations with him have always left me rejuvenated, happy, and feeling peppier-a testament to how amazing a best friend he is. Sagarika (Sagarika Srishti), for all her support, both in India and when she came to the US. Her move to the US during my PhD was a major personal highlight. Ariba (Ariba Javed), thanks for all the discussions, talks, and emotional conversations, and for always being up for anything interesting, including a pottery class. Shobhit (Shobhit Narain) has been an amazing companion, helping me with job applications and always being the sarcastic, serious, yet most helpful guy I have had the pleasure of calling a friend. And finally, Sai (Sairam Tabibu) helped me fill out the PhD application for UMich on the exact deadline, without which, I would not be here at all. This is probably an unconventional paragraph in acknowledgments, but these were unconventional times during COVID. For the two years of lockdown, I turned to Among Us when I felt lonely or lost in my research. I am really thankful for the streamers whose broadcasts provided some semblance of social interaction. For almost three years, I watched them stream at least 8 hours a day while I worked, to simulate a social environment. And when my research progress stalled, I turned to anonymous Discord communities, playing Among Us and golf for hours, which helped alleviate feelings of depression and sadness, providing a much-needed uplift. My PhD journey wasn't easy, and a lot happened over the six years, but I made it through.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Acknowledgements",
      "text": "I would like to start by expressing my profound gratitude towards my PhD advisor, Emily (Emily Mower Provost). She has been my go-to person for any research brainstorming and has shown me tremendous patience, support, and guidance throughout my PhD journey. Without her persistence and suggestions, completing this PhD would not have been possible. I am also incredibly grateful for my thesis committee members: Vinod (VG Vinod Vydiswaran), Nikola (Nikola Banovic), Benjamin (Benjamin Fish), and Douwe (Douwe Kiela). Their valuable insights during my thesis proposal helped shape the final version of the thesis.\n\nFor putting up with me during my PhD journey, my immense gratitude goes towards my family, especially my parents, grandparents, and Bert. My parents have been a rock for me over the past six years. Though I could not visit them often or talk to them much, they were always there when I needed someone. They seem to have aged fifteen years in the six years of my PhD, stressed about me, but their support never wavered. My mom (Archana Kumari) received her own PhD in 2021, and my dad (Abhay Kumar) became the Vice-Chancellor of a new IIIT-two accomplishments that were their lifelong dreams and inspired me immensely. I unfortunately lost two of my grandparents during the PhD program, and I will never forget their blessings and excitement for me embarking on my higher education journey. During early 2020, in the midst of COVID, I adopted a cat named Bert-yes, named after the language model. Without him, I would not have maintained my sanity during the dark, lonely nights and tiring, long work days. His purring loudly into my ear calmed me down on the worst of nights. I was lucky enough to secure three internships and have amazing research mentors for iii all of them. Ahmad (Ahmad Beirami) taught me how to approach Conversational AI, how to create effective presentations, and how to write research proposals. Adina (Adina Williams) taught me how to work with linguistics mixed in with NLP, and how subjectivity can infiltrate seemingly objective parts (like NLI) of NLP. Ana (Ana Marasoviƒá) was the first person I worked with on really large language models (foundational models), and she taught me how to approach evaluation and benchmarking for generative models-a major part of my current research path.\n\nI want to thank my lab members, starting with Zak (Zakaria  Aldeneh) . Zak exemplifies what all senior PhD mentors should be, helping me with code, brainstorming, and working with me on papers. He has been an amazing research collaborator. I also want to thank Minxue (Minxue Sandy Niu) for being the junior research collaborator anyone would be proud of. She has not only been an amazing collaborator but was also always willing to discuss interesting research problems. I want to thank Matt (Matthew Perez) for being the batchmate who has always been there to help, to vent, to advise, and to collaborate, serving as my go-to person for any speech-based research questions. Finally, I want to thank Amrit (Amrit Romana) for being an amazing lab member; her observant questions helped me immensely during lab presentations.\n\nI also want to thank my friends, without whom this journey would not have been possible. I will start with Abhinav (Abhinav Jangda), who has been my support system throughout my PhD journey, starting from the application process. Diksha (Diksha Dhawan) was the best PhD roommate one could ask for during the first four years of my PhD. She shared laughter and tears with me, cooked with me, and supported me through all the highs and lows.\n\nWithout her, I could not have survived my PhD. She taught me the value of being proud of my interests in both my personal and professional life, and how friends can sometimes be family, which is the best gift anyone could have given me. Eesh (Sudheesh Srivastava), for all the conversations at the intersection of machine learning, physics, and philosophy, has taught me about areas and theories that I would have otherwise not encountered in any  Close-up view of the thermal and video recording equipment. . . . . . .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "4.3",
      "text": "Distribution of the activation and valence ratings in random labeling scheme (on left) and contextual labeling scheme (on right). . . . . . . . . 4.  4  An overview of the instructions provided to the annotators for annotating an utterance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 13,
      "page_end": 17
    },
    {
      "section_name": "4.5",
      "text": "Annotation scale used by MTurk workers to annotate the emotional content of the corpus. They annotate valence and activation for each utterance. . .  6.1  Mean difference between the self-reported activation and valence ratings and the random and contextual presentations. . . . . . . . . . . . . . . .",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "7.1",
      "text": "Adversarial multi-task network architecture. . . . . . . . . . . . . . . . .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "8.1",
      "text": "Privacy preserving network architecture. . . . . . . . . . . . . . . . . . . 9.  1  Diagram of the approach. In step 1, the wordlists are created (e.g., for gender, ùë§ ùëî , and for emotion, ùë§ ùëí . The length is the number of words. The brightness corresponds to the strength of the relationship between the word and the category (gender or emotion). In step 2, the model learns a shared representation with the goal of learning emotion, √äùëñ , and unlearning gender, ƒúùëñ . The shared representation is used to create a word-level saliency vector. In step 3, the HCM metrics are calculated by combining the salience with either ùë§ ùëî or ùë§ ùëí . In step 4, the model is trained with a loss that includes the original target (maximizing emotion accuracy and minimizing gender accuracy) in addition to the two HCM metrics. . . . . . . . . . . . . . . xii 9.2\n\nThe crowdsourcing interface. Users are asked to indicate their model preference given visualizations of the relative importance of specific words in the prediction of emotion. The evaluators were not presented with the names of the models and the order of model outputs was randomized for each viewing. Note that the sentence was chosen to clearly convey gender, and potentially, gendered information. In the figure, green indicates that the model is using the word in the prediction, while red indicates that it is not. The outputs correspond to: 1) GenControl -note the heavy focus on the word \"dress\", 2) SIRBias, 3) ArtNoise, 4) SIRAug, 5) SIRAdv (for model details, see Section 9.5.2). Observe that the three sensitive information reduction methods are no longer focusing on the word \"dress\". 152\n\nxiii Results using general (left) and models trained to not encode sensitive information (right) for activation and valence prediction. U-UAR, U(M/F)-UAR for male/female, L-leakage, SIR-sensitive information reduction metric, MI-membership identification. Bold-Italic shows significant improvement in metrics as compared to general classification model and Italic shows significant difference in metrics as compared to the model trained to not encode sensitive information. Significance is established using paired t-test at adjusted p-value< 0.05. . . . . . . . . . . . . . . . .",
      "page_start": 13,
      "page_end": 17
    },
    {
      "section_name": "List Of Tables",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "8.2",
      "text": "Results for activation (Act) and valence (Val) prediction using multimodal input, when adversarially unlearning gender in each input (SIR-E) [left] stream separately. U-UAR, U(M/F)-UAR for male/female, P-sensitive information reduction metric, MI-membership identification. Bold-Italic shows significant improvement in the sensitive information reduction metric as compared to model trained to not encode sensitive information by maximizing loss on the concatenated representation (SIR-C)  [right] . Significance is established using paired t-test, adjusted p-value< 0.05. . . 8.  3  Results for activation and valence prediction, for general classification (General), and, when adversarially unlearning subject identity (SIR-SubjectID) and both subject identity and gender (SIR-Multiple). U-UAR, U(M/F)-UAR for male/female, SIR-sensitive information reduction metric, MImembership identification. Bold-Italic shows significant improvement in metrics as compared to general classification model and Italic shows significant difference in metrics as compared to the models trained to not encode sensitive information. Significance is established using paired Within and cross dataset performance of models across various datasets, correlation with these metrics and using metrics for training. Note that for space, the column headings refer to H, rather than HCM. We use the notation \"Dem\" to refer to the combined demographic categories of Gender, Age, and Race. SIR refers to sensitive information reduction. . .  160  xvii",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production. To address these complexities, the thesis makes several key contributions.\n\nTo The findings from this research provide valuable insights into the nuances of emotion labeling, modeling techniques, and interpretation frameworks for robust emotion recognition.\n\nThe novel datasets collected help encapsulate the environmental and personal variability prevalent in real-world emotion expression. The data augmentation and annotation studies improve label consistency by accounting for subjectivity in emotion perception. The stressorxviii controlled models enhance adaptability and generalizability across diverse contexts and datasets. The bimodal adversarial networks aid in generating representations that avoid leakage of sensitive user information. Finally, the optimized sociological evaluation metrics reduce reliance on extensive expensive human annotations for model assessment.\n\nThis research advances robust, practical emotion recognition through multifaceted studies of challenges in datasets, labels, modeling, demographic and membership variable encoding in representations, and evaluation. The groundwork has been laid for cost-effective, generalizable emotion recognition models that are less likely to encode sensitive demographic information.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Chapter I Introduction",
      "text": "In human communication, perceiving and responding to others' emotions in interpersonal conversations play a crucial role  [76] . To create systems that can aid in human-centered interpersonal situations, it is necessary for these systems to possess the capability to recognize emotions effectively  [220] . Robust Emotion Recognition (ER) models can be beneficial in various situations, such as crisis text lines or passive mental health monitoring  [160] .\n\nHowever, these ML models often lack robustness when faced with unseen data situations, making deploying them in high-risk situations or healthcare a challenging task  [241] .\n\nRecongizing emotion is a challenging task because it is subjective in both perception and production  [178] . The labels used to train emotion recognition models are perceptually subjective  [28] . The same emotion can be perceived differently by different people, depending on their cultural background, personal experiences, and other factors  [147] . Additionally, there is production subjectivity. The same emotion can be expressed differently by different people, depending on their individual personality, cultural background, physiological and other factors  [13] . The subjectivity of emotion recognition makes it difficult to develop accurate and robust models that account for these numerous variations  [221] .\n\nIn addition to the challenges posed by subjectivity, there are challenges that relate to the information that is learned in addition to and beyond the expression of emotion itself. The manner in which emotions are expressed are correlated with a person's demographic and identifying features. Hence, systems trained to recognize emotion can often learn implicit associations between an individual's demographic factors and emotion  [208] . When used as a component in larger systems, these implicit associations can lead to either the leakage of demographic information, or can bias the larger system's output based on demographic information, even when not explicitly trained to do so.\n\nTraining any robust machine learning model necessitates having access to large amounts of diverse and labelled data. Training models for emotion recognition faces the challenge of not having access to large quantities of diverse data. Scraping data over the internet, as is done for other areas, leads to a dataset that is often demographically biased, and, often exaggerated for entertainment purposes. On the other hand, data collected in laboratory environments is intentionally cleaner and often exaggerated in case of scripted sessions.\n\nTherefore, both of these data collection methods do not encapsulate possible environmental and personal factors, which leads to models often being trained on either highly skewed or non-representative data. The resulting models are either fragile or biased, and ultimately unable to handle real-world variability.\n\nIn this dissertation, critical facets of emotion recognition are thoroughly explored, beginning with the collection of datasets, which take into account psychological factors in producing emotions. This is followed closely by examining the influence that alterations in data augmentation processes have on emotion labels, while also challenging and interrogating the validity of previously established labels. Alterations in labeling techniques and the resulting effects on annotator-assigned labels are also scrutinized. Simultaneously, the research develops robust models specifically trained to disregard certain physiological emotion production factors. Integral to the research is the creation of bimodal models that generate representations aiming to tackle the reduction of leakage of sensitive demographic variables. The concluding portion of the study involves an in-depth evaluation of the robustness and impartiality of these models, carried out in a human-centric manner, ensuring an emphasis on minimal costs for data annotation. From this extensive research, valuable insights are gained into the complexities of emotion recognition, which pave the way for more nuanced and robust labeling, modeling, and interpretation techniques. It also lays the groundwork for future efforts in the development of robust and cost-effective emotion recognition models.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Emotion Theories And The Impact On Emotion Recognition Model",
      "text": "",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Development",
      "text": "To better understand the subjectivity inherent in emotion recognition and its correlation with the research gaps and challenges, we must first explore the contrasts between emotion production and emotion perception theories. These theories elucidate the distinct factors related to the subjectivity of emotions in both production and recognition processes and offer valuable insights for developing robust and unbiased emotion recognition models.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Emotion Production And Emotion Perception",
      "text": "Emotion production refers to experiencing and generating emotional responses, encompassing several factors, including cognitive appraisal, physiological response, behavior and expression, and subjective experience. These components work together to create the unique process of producing emotions within each person.\n\nEmotion perception, conversely, focuses on recognizing and interpreting others' emotional signals, influenced by factors such as emotional cues, context and environment, past experiences and learning, and individual differences. This process involves making sense of others' emotions based on various internal and external factors.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Emotion Theories, Research Challenges, And Implications For Emotion Recognition",
      "text": "Various theories of emotion provide insights into the challenges faced in developing computational models for emotion recognition in speech or text. Below, we discuss the relevance and implications of some prominent theories in the context of speech or text-based (bimodal) emotion recognition.\n\n‚Ä¢ James-Lange Theory and Cannon-Bard Theory  [204] : Both theories emphasize physiological responses' importance in emotion. In speech or text-based recognition, it is vital to consider correlations between observable features (e.g., vocal tonality, speech patterns) and underlying physiological responses. Accounting for these correlations can help capture emotions, even though the relationship might be subjective due to personal and cultural differences.\n\n‚Ä¢ Schachter-Singer Two-Factor Theory  [204] : This theory stresses the importance of both physiological arousal and cognitive appraisal for experiencing emotions.\n\nIn speech or text-based emotion recognition, cognitive appraisal aspects such as semantic content, contextual factors, and discourse patterns can be extracted. However, the subjectivity of cognitive appraisal processes presents challenges given personal experiences' impact on interpretation.\n\n‚Ä¢ Lazarus Cognitive-Mediational Theory  [204] : Centered around the role of cognitive appraisal, this theory highlights the need for emotion recognition systems to account for individuals' interpretations of situations through cues that may suggest appraisal (e.g., word choice, phrase structure, conversational context). Advanced models might need to factor in users' personal and demographic features to better understand cognitive appraisal processes. This approach introduces more subjectivity and potential privacy concerns, as individual perspectives and experiences can vary significantly.\n\nIntegrating insights from these theories can aid unraveling the complexities and subjective nature of emotions expressed through language, as speech or text-based emotion recognition relies primarily on linguistic patterns, tone, and content analysis.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Addressing Challenges Through Thesis Contributions",
      "text": "The thesis contributions align with and address the subjectivity challenges in emotion production and perception, thus tackling the complexities involved in developing robust and unbiased emotion recognition models.\n\n‚Ä¢ Collecting datasets that account for psychological factors in emotion production: By considering psychological factors influencing unique emotional experiences, more diverse datasets are created, allowing models to account for subjectivity in emotion production and generalize across emotions.\n\n‚Ä¢ Examining the influence of data augmentation processes on emotion perception labels:\n\nThis contribution seeks to understand data augmentation's impact on ground truth labels, creating better representations of emotions in the datasets, accounting for subjectivity in emotion perception.\n\n‚Ä¢ Analyzing labeling setups' impact on annotators' emotion perception labels: This investigates how labeling setups influence emotion perception, aiming to improve label consistency and reduce inter-annotator disagreement, thus better representing subjectivity in emotion perception.\n\n‚Ä¢ Training robust models by explicitly disregarding emotion production factors: This minimizes the impact of subjective elements associated with emotion production, enabling models to focus on core emotional cues.\n\n‚Ä¢ Developing bimodal models for generation of emotion representations that are debiased and reduce encoding of demographic and membership information: This creates models that consider multiple emotional cues while disregarding sensitive features, addressing subjectivity challenges in both emotion production and perception.\n\n‚Ä¢ Evaluating models in a human-centric manner: Designing evaluation methods aligned with real-world expectations and without incurring significant annotation costs ensures the models effectively tackle subjectivity challenges in a practical way.\n\nBy focusing on these contributions, the thesis emphasizes the connection between emotion production and perception's subjectivity and its influence on model development, advancing the creation of more robust and unbiased emotion recognition models.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion recognition models are customarily trained using laboratory-collected data encompassing video, audio, and corresponding text. These algorithms strive to capture the speaker's underlying emotional state either autonomously or as part of a larger pipeline, such as response generation. Supervised learning techniques predominantly train these models.\n\nObtaining ground truth labels for the dataset samples is crucial for successfully training a supervised learning model. The emotion theories presented earlier are intrinsically linked with the complexity of emotion recognition. Understanding the interplay between these theories and model development is essential.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Emotion Labels",
      "text": "Emotion labels typically fall into two categories: categorical and dimensional. Categorical variables aim to discretely categorize emotion attributes, such as excitement, happiness, anger, or sadness. These labels' limitations align with the James-Lange and Cannon-Bard theories-emotions are subjective, making it difficult to define universal emotions across cultures. This subjectivity is intensified by both personal physiological responses to stimuli and cultural context.\n\nDimensional emotional labels describe emotions across two dimensions, valence (sad to happy) and arousal (calm to excited). The dimensional approach is more consistent with the James-Lange and Cannon-Bard theories, addressing the physiological components of emotions, as well as the cognitive components emphasized by the Schachter-Singer Two-Factor Theory and Lazarus Cognitive-Mediational Theory. However, these dimensional labels also face the challenge of cultural and personal influences on the perception and expression of emotions.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Emotion Features",
      "text": "Three primary modalities are used in combination to train emotion recognition models:\n\ntext, audio, and video. This thesis focuses predominantly on audio and its corresponding text as the feature set for these models.\n\nMel-filterbanks (MFBs) are often used as inputs to neural network models in speech.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Mfbs",
      "text": "",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Emotion Recognition Models",
      "text": "Audio-based emotion recognition models initially relied on Hidden Markov Models (HMMs) or Gaussian Mixture Models (GMMs) and later shifted focus to LSTMs and RNNs.\n\nThese models aim to capture the dynamic and time-varying nature of speech, reflecting the James-Lange Theory and Cannon-Bard Theory's emphasis on physiological responses.\n\nHowever, these models must also account for the inherent cultural and linguistic differences in the way emotions are expressed through speech.\n\nLanguage-based models, like recent advances in transformer architectures, address long and indirect contextual information challenges, in line with the Schachter-Singer Two-Factor Theory's cognitive appraisal aspects. These models strive to understand the nuances of language, cultural expressions, and individual semantic and contextual differences in recognizing emotions.\n\nMulti-modal models exploit relevant information from text, audio, or video to form powerful emotion recognition models. Informed by the emotion theories, these models take into account the subjectivity of emotions by leveraging different modalities to discern the nuances of emotion expression. By combining these modes, models can better account for the emotional complexity that arises from intercultural and personal differences in perception, expression, and context.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Challenges In Emotion Recognition",
      "text": "The variable and subjective nature of emotions make it challenging to train models that can accurately identify emotion in any given scenario. Addressing three major challenges",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Non-Representative Data",
      "text": "Emotion production in real-world settings is influenced by various factors, including data collection settings, demographics, and personal factors. Addressing these confounding factors aligns with the implications of the earlier-discussed emotion theories. Researchers can tackle this challenge by developing more robust models, incorporating real-world variability through dataset augmentation or mitigating confounding factors.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Label Subjectivity",
      "text": "As highlighted in the emotion theories, emotions are inherently subjective and deeply influenced by personal experiences, culture, and context. This subjectivity leads to difficulty in pinpointing an objective and universal ground truth for training emotion recognition models. Researchers should account for label subjectivity by using diverse and representative datasets, annotations from multiple sources, and considering multiple emotion theories during the model design process.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Unintentional Encoding And Leakage Of Sensitive Information",
      "text": "Variability can lead unintentional encoding and leakage of sensitive information concerns, specifically in human centered tasks, such as emotion recognition models, as the associative nature of the task and sensitive demographic variables may inadvertently lead to encoding personal information.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Proposed Methods",
      "text": "A robust and effective emotion recognition system must successfully navigate a range of challenges, including addressing subjectivity in emotion production and perception, handling natural variations and confounding variables, reducing encoded sensitive information, and providing relevant evaluation metrics. Here, we present a series of proposed methods aligned with the outlined contributions to address these challenges.",
      "page_start": 29,
      "page_end": 30
    },
    {
      "section_name": "Dataset Collection For Emotion Recognition",
      "text": "Tackling the challenge of subjectivity in emotion production, it's essential that we consider the issues in widely used emotion recognition datasets that arise due to design choices, methodology of data collection, and inherent subjectivity. Emotion datasets traditionally aim for minimal variation to ensure generalizability. However, this can result in non-robust models that struggle with unexpected variability. We propose the construction and validation of a new dataset called Multimodal Stressed Emotion (MuSE), which introduces a controlled situational confounder (stress) to better account for subjectivity. In addition, we discuss the use of domain adversarial networks to achieve more stable and reliable cross-corpus generalization while avoiding undesired characteristics in encodings.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Data Augmentation With Noise In Emotion Datasets",
      "text": "Addressing the challenge of subjectivity in emotion perception, we examine data augmentation with noise in emotion datasets, focusing on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset, which features dyadic interactions with text, video, and audio modalities. Introducing realistic noisy samples through environmental and synthetic noise, we evaluate how ground truth and predicted labels change due to noise sources. We discuss the effects of commonly used noisy augmentation techniques on human emotion perception, potential inaccuracies in model robustness testing, and provide recommendations for noise-based augmentation and model deployment.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Annotations Of Emotion Datasets",
      "text": "To further address subjectivity in emotion perception, we investigate how design choices in the annotation collection process impact the performance of trained models. Focusing on contextual biasing, we examine how annotators perceive emotions differently in the presence or absence of context. Commonly-used emotion datasets often involve annotators who have knowledge of previous sentences, but models are frequently evaluated on individual utterances. We explore the implications of this discrepancy on model evaluation, and its potential for generating errors.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Methods For Handling Natural Variations And Confounding Variables",
      "text": "As mentioned earlier, we collect a dataset of differences in similar emotion production under varying levels of stress. Emotion recognition models may spuriously correlate these stress-based factors to perceived emotion labels, which could limit generalization to other datasets. Consequently, we hypothesize that controlling for stress variations can improve the models' generalizability. To achieve this, we employ adversarial networks to decorrelate stress modulations from emotion representations, examining the impact of stress on both acoustic and lexical emotion predictions. By isolating stress-related factors from emotion representations, we aim to enhance the model's ability to generalize across different stress conditions. Furthermore, we analyze the transferability of these refined emotion recognition models across various domains, assessing their adaptability to evolving contexts and scenarios. Ultimately, our approach aims to improve emotion recognition model robustness by addressing the inherent variability of emotional expression due to stress and ensuring greater applicability across multiple domains.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Approaches For Tackling Sensitive Information Leakage In Trained Emotion",
      "text": "",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Recognition Models",
      "text": "Emotions are inherently related to demographic factors such as gender, age, and race.\n\nConsequently, emotion recognition models often learn these latent variables even if they are not explicitly trained to do so. This learning behavior poses a risk to user privacy, as the models inadvertently capture sensitive demographic information. Storing representations instead of raw data does not fully mitigate this issue, as latent variables can still compromise user privacy. To address this challenge, we present approaches for mitigating the learning of certain demographic factors in emotion recognition embeddings. Furthermore, we tackle the issue of user-level membership identification by employing an adversarial network that strips this information from the final encoding, reduced leakage of sensitive information from generated representations.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Methods For Model Evaluation And Perception",
      "text": "Large language models face limitations in subjective tasks like emotion recognition due to inadequate annotation diversity and data coverage. Acquiring comprehensive annotations and evaluations is often costly and time-consuming. To address these challenges, we propose cost-effective sociological metrics for emotion generalization and reduced demographic vairable leakage. These metrics reduce reliance on expensive human-based feedback while still capturing the nuances of human emotions. By evaluating model performance and demographic variables encoded in generated representations, the proposed metrics improve cross-corpus results and allow for the development of accurate, relevant emotion recognition models in a more economic manner.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Contributions",
      "text": "This dissertation proposes several investigations and novel solutions to address various concerns related to real-world emotion recognition model deployment.\n\nThe contributions of the works in this dissertation can be summarized as follows:\n\n‚Ä¢ Chapter IV:\n\n-Introduction of Multimodal Stressed Emotion (MuSE) dataset.\n\n-Detailed data collection protocol.\n\n-Potential uses and emotion content annotations.\n\n-Performance measuring baselines for emotion and stress classification.\n\n‚Ä¢ Chapter V:\n\n-Speech emotion recognition's impact under influence of various factors such as noise.\n\n-Investigation of noise-altered annotation labels and their aftermath.\n\n-Consequences on evaluation of ML models considering noise.\n\n-Specific recommendations for noise augmentations in emotion recognition datasets.\n\n‚Ä¢ Chapter VI:\n\n-Crowdsourced experiments to study the subjectivity in emotion expression and perception.\n\n-Contextual and randomized annotation schemes of the MuSE dataset.\n\n-Comparative analysis revealing contextual scheme's closeness to speaker's self-reported labels.\n\n‚Ä¢ Chapter VII:\n\n-Examination of emotion expressions under stress variations.\n\n-Utilization of adversarial networks to separate stress modulations from emotion representations.\n\n-Exploration of stress's impact on acoustic and lexical emotional predictions.\n\n-Evidence of improved generalizability with stress control during model training.\n\n‚Ä¢ Chapter VIII:\n\n-Highlighting the unintentional leak of sensitive demographic information in multimodal representations.\n\n-Use of adversarial learning paradigm to improve sensitive information reduction metric.\n\n-Maintenance of primary task performance, despite improvements to privacy.\n\n‚Ä¢ Chapter IX:\n\n-New template formulation to derive human-centered, optimizable and costeffective metrics.\n\n-Correlation establishment between emotion recognition performance, biased representations and derived metrics.\n\n-Employment of metrics for training an emotion recognition model with increased generalizability and decreased bias.\n\n-Finding of positive correlation between proposed metrics and user preference.",
      "page_start": 32,
      "page_end": 34
    },
    {
      "section_name": "Outline Of The Dissertation",
      "text": "Initiating with Chapter III, it delves into a comprehensive review of pertinent literature spanning from emotion recognition and privacy preservation to adversarial networks, model interpretability, and crowdsourcing designs. Moving forward, Chapter II provides an introduction to the common datasets, and features employed throughout this research.\n\nSubsequent chapters, from Chapter IV to IX, engage in a thorough exploration and discussion of the research work undertaken, characterized in the Contributions section. Lastly, Chapter X serves as a conclusive summary encapsulating the primary contributions made, elaborating on the proposed future works.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Chapter Ii",
      "text": "",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Related Work: Modeling Emotions",
      "text": "Emotion recognition is a complex, multifaceted field drawing on various research areas.\n\nThis chapter explores the various methods and considerations in this field, from the use of crowdsourcing to the importance of context, and from handling confounding factors to the impact of noise on machine learning models. We explore the ethical considerations of unintentional encoding of sensisitive variables in data collection and neural networks, the role of interpretability in model trustworthiness, and the importance of automating human in the loop feedback. We also delve into the challenge of generalizability in emotion recognition.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Concerns With Emotion Recognition Datasets",
      "text": "Some aspects of the above mentioned datasets limit their applicability, including: a lack of naturalness, unbalanced emotion content, unmeasured confounding variables, small size, small number of speakers, and presence of background noise. These datasets are also limited in the number of modalities they use, usually relying on visual and acoustic/lexical information.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Recorded Modalities",
      "text": "As shown in Table  2 .1, the most common modalities are video, acoustics, and text.\n\nIn addition to these modalities, we chose to record two more modalities: thermal and physiological. Previous research has shown that thermal recordings perform well as noninvasive measurement of physiological markers like, cardiac pulse and skin temperature  [173, 172, 80] . They have been shown to be correlated to stress symptoms, among other physiological measures. We used the physiological modality to measure stress responses  [234, 210]  to psychological stressors. This modality has been previously noted in literature for measuring stress  [96] , usually measured in polygraph tests. We perform baseline experiments to show that the modalities collected in the dataset are indeed informative for identifying stress and emotion.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Lack Of Naturalness",
      "text": "A common data collection paradigm for emotion is to ask actors to portray particular emotions. These are usually either short snippets of information  [36] , a single sentence in a situation  [38] , or obtained from sitcoms and rehearsed broadcasts  [47] . A common problem with this approach is that the resulting emotion display is not natural  [113] . These are more exaggerated versions of singular emotion expression rather than the general, and messier, emotion expressions that are common in the real world  [12, 21, 72] . Further, expressions in the real world are influenced by both conversation setting and psychological setting.\n\nWhile some datasets have also collected spontaneous data  [36, 38] , these utterances, though emotionally situated, are often neutral in content when annotated. The usual way to get natural emotional data is to either collect data using specific triggers that have been known\n\nto elicit a certain kind of response or to completely rely on in-the wild data, which however often leads to unbalanced emotional content in the dataset  [183] .",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Unbalanced Emotion Content",
      "text": "In-the-wild datasets are becoming more popular  [47, 118, 138] . The usual limitation to this methodology is that, firstly, for most people, many conversations are neutral in emotion expression. This leads to a considerable class imbalance  [183] . To counter this issue, MSP-Podcast  [143]  deals with unbalanced content by pre-selecting segments that are more likely to have emotional content. Secondly, data collected in particular settings, e.g., therapy  [162] , or patients with clinical issues  [130]  comprise mostly of negative emotions because of the recruitment method used in the collection protocol.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Presence Of Interactional Variables",
      "text": "The common way of inducing emotions involves either improvisation prompts or scripted scenarios. Emotion has been shown to vary with a lot of factors that are different from the intended induction  [198, 240, 156] . These factors in general can be classified into: (a)\n\nrecording environment confounders and (b) collection confounders. Recording environmentbased variables hamper the models' ability to to learn the emotion accurately. These can be environment noise  [16] , placement of sensors or just ambient temperature  [31] .",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Demographics In Dataset Collection Recruitment",
      "text": "The data collection variations influence both the data generation and data annotation stages. The most common confounders are gender, i.e., ensuring an adequate mix of male vs female, and culture, i.e., having a representative sample to train a more general classifier.\n\nAnother confounding factor includes personality traits  [242] , which influence how a person both produces  [242]  and perceives  [158]  emotion. Another confounder that can occur at the collection stage is the familiarity between the participants, like RECOLA  [183] , which led to most of the samples being mainly positive due to the colloquial interaction between the participants. They also do not account for the psychological state of the participant.\n\nPsychological factors such as stress  [132] , anxiety  [229]  and fatigue  [26]  have been shown\n\npreviously to have significant impact on the display of emotion. But the relation between these psychological factors and the performance of models trained to classify emotions in these situations has not been studied.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Crowdsourcing And Context In Emotion Recognition",
      "text": "Crowdsourcing has emerged as a highly efficient approach for gathering dependable emotion labels, as extensively investigated by Burmania et al.  [33] . In addition to this, previous studies have concentrated on enhancing the dependability of annotations by employing quality-control methods. For instance, Soleymani et al.  [201]  have proposed the utilization of qualification tests to weed out spammers from the crowd, thereby ensuring the quality of collected data. Furthermore, Burmania et al.  [35]  have explored the use of gold-standard samples to continuously monitor the reliability and fatigue levels of annotators.\n\nThe interpretation of emotions is heavily influenced by the context in which they are expressed. Various factors such as tone, choice of words, and facial expressions can significantly impact how individuals perceive and understand emotions  [129] . It is noteworthy that this contextual information is implicitly incorporated in the labeling schemes of commonly used emotion datasets like IEMOCAP  [36]  and MSP-Improv  [38] . However, a notable disparity often exists between the information available to human annotators and that accessible to emotion classification systems. This discrepancy arises because emotion recognition systems are typically trained on individual utterances  [10, 3, 157, 190] .",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Handling Confounding Factors",
      "text": "",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Singularly Labeled Or Unlabeled Factors",
      "text": "To address confounding factors that are either labeled singularly or cannot be labeled, researchers have devised specific methods. For instance, Ben-David et al.  [23]  conducted a study wherein they showed that a sentiment classifier, trained to predict the sentiment expressed in reviews, could also implicitly learn to predict the category of the products being reviewed. This finding highlights the potential of classifiers to capture additional information beyond their primary task. In a similar vein, Shinohara  [196]  employed an adversarial approach to train noise-robust networks for automatic speech recognition. By leveraging this technique, Shinohara aimed to enhance the network's ability to handle noisy and distorted speech signals.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Explicitly Labeled Factors",
      "text": "In addition to addressing confounding factors that are singularly or unlabeled, researchers have also developed methods to handle confounding factors that are explicitly labeled during the data collection process. One such approach involves the use of adversarial multi-task learning, which aims to mitigate variances caused by speaker identity  [153] . By incorporating this technique, researchers can reduce the influence of speaker-specific characteristics on the emotion recognition system, thereby enhancing its generalizability. Furthermore, a similar approach has been employed to prevent networks from learning publication source characteristics, which could introduce biases in the classification process  [149]",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Noise And Approaches To Dealing With It In Machine Learning",
      "text": "",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Models",
      "text": "The impact of noise on machine learning models has been the subject of extensive research, which can be broadly classified into three main directions: robustness in automatic speech While evaluating model robustness to noise or adversarial attacks, researchers commonly introduce noise into the dataset and assess the model's performance  [5] . However, when it comes to emotion recognition, introducing noise while ensuring that the perception of emotions remains intact can be highly challenging. It is crucial to strike a balance between adding noise for robustness evaluation purposes and preserving the original emotional content. This ensures that the introduced noise does not distort or alter the true emotional expression, enabling accurate and reliable emotion recognition systems.",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Unintentional Sensisitve Variable Encoding, And Ethical Considerations In Data Collection And Neural Networks",
      "text": "The preservation of privacy in data collection has been a key area of focus in early research. Various methods such as rule-based systems and the introduction of background noise have been explored in order to achieve this goal  [88, 69] . However, more recent studies have shifted their attention towards privacy preservation in the context of neural networks. In particular, researchers have primarily concentrated on ensuring that the input data used in these networks are not memorized and cannot be retrieved even when the model is deployed  [41, 2] .\n\nAnother crucial consideration in the field of privacy preservation is fair algorithmic representation. The objective here is to develop networks that are invariant to specific attributes, often related to demographic information, in order to ensure fairness  [29, 59, 61] .\n\nAlthough certain methods have demonstrated promise in achieving fairness, they may still inadvertently lead to privacy violations  [108] .",
      "page_start": 42,
      "page_end": 42
    },
    {
      "section_name": "The Role Of Interpretability In Model Trustworthiness",
      "text": "The aspect of interpretability plays a crucial role in establishing trustworthiness of models. Studies have indicated that individuals are more inclined to trust the decisions made by a model if its explanations align with their own decision-making processes  [203, 73, 195] .\n\nIn addition, interpretability methods can be employed by model designers to evaluate and debug a trained model  [68] . These methods provide insights into the inner workings of the model and facilitate a better understanding of its decision-making process.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Automating Human In The Loop Feedback",
      "text": "In order to automate human in the loop feedback, several approaches have been proposed.\n\nOne such approach involves the utilization of a teacher-student feedback model, where feedback from human teachers is used to improve the performance of the model  [179] .\n\nAnother avenue of research focuses on enhancing active learning techniques, which aim to select the most informative data points for annotation by human experts, thereby reducing the overall labeling effort required  [115] .\n\nThese methods often incorporate a combination of fine-tuning and prompt-based learning techniques, which further enhance the model's ability to learn from human feedback and adapt its performance accordingly  [217] . By fine-tuning the model based on the feedback received and utilizing prompts as guiding cues, these approaches enable the model to continually improve its performance, making it more effective in addressing the specific task or problem at hand.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Generalizability In Emotion Recognition",
      "text": "Achieving generalizability in emotion recognition poses a significant challenge for researchers. To address this challenge, various methods have been explored in order to obtain models that can generalize well across different datasets and scenarios. One approach is the use of combined and cross-dataset training, where multiple datasets are combined during the training process to create a more comprehensive and diverse training set. This helps the model learn a wider range of emotion patterns and improves its ability to generalize to unseen data  [137] .\n\nAnother technique that has been investigated is transfer learning, which involves leveraging knowledge acquired from pre-trained models on a related task and applying it to the emotion recognition task. By transferring the learned representations and weights from a pre-trained model, the model can benefit from the general knowledge and feature extraction capabilities it has acquired, leading to improved generalizability in emotion recognition  [137] .\n\nFurthermore, researchers have also explored the concept of generalizability from the perspective of noisy signals. Emotion recognition often deals with noisy data, such as speech with background noise or facial expressions with occlusions. By developing models that are robust to such noise and can effectively extract emotion-related information from imperfect signals, the generalizability of the models can be enhanced  [93] .",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Conclusion",
      "text": "The field of emotion recognition is complex, with many factors and considerations influencing the development and deployment of effective models. This chapter has explored some of the key areas in this field, highlighting the importance of crowdsourcing, context, handling confounding factors, dealing with noise, and ensuring that the representations don't inadverdently encode sensitive demographic or membership information. The role of interpretability in model trustworthiness and the challenge of automating human in the loop feedback were also discussed. Although progress has been made in many of these areas, the challenge of generalizability in emotion recognition remains, and future research will need to continue to address this issue.",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Chapter Iii",
      "text": "",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Datasets And Pre-Processing",
      "text": "This thesis focusses on emotion recognition as a task. For this purpose, we use a standard set of datasets and features as described in this chapter. This allows us to perform experiments with a set of known and commonly used datasets, keeping them uniform across experimental variables.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Datasets Used In Thesis",
      "text": "In the past years, there have been multiple emotional databases collected and curated to develop better emotion recognition systems. Table  2 .1 shows the major corpora that are used for emotion recognition.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Iemocap",
      "text": "The IEMOCAP dataset was created to explore the relationship between emotion, gestures, and speech. Pairs of actors, one male and one female (five males and five females in total), were recorded over five sessions. Each session consisted of a pair performing either a series of given scripts or improvisational scenarios. The data were segmented by speaker turn, resulting in a total of 10,039 utterances (5,255 scripted turns and 4,784 improvised turns).",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Msp-Improv",
      "text": "The MSP-Improv dataset was collected to capture naturalistic emotions from improvised scenarios. It partially controlled for lexical content by including target sentences with fixed lexical content that are embedded in different emotional scenarios. The data were divided into 652 target sentences, 4,381 improvised turns (the remainder of the improvised scenario, excluding the target sentence), 2,785 natural interactions (interactions between the actors in between recordings of the scenarios), and 620 read sentences for a total of 8,438 utterances.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Msp-Podcast",
      "text": "The MSP-Podcast dataset was collected to build a naturlisitic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings. This was done using machine learning algorithms, which along with a cost-effective annotation process using crowdsourcing, led to a vast and balanced dataset. We use a pre-split part of the dataset which has been identified for gender of the speakers which comprises of 13,555 utterances. The dataset as a whole contains audio recordings.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Muse",
      "text": "The MuSE dataset consists of recordings of 28 University of Michigan college students, 9 female and 19 male, in two sessions: one in which they were exposed to an external stressor (final exams period at University of Michigan) and one during which the stressor was removed (after finals have concluded). Each recording is roughly 45-minutes. We expose each subject to a series of emotional stimuli, short-videos and emotionally evocative monologue questions. These stimuli are different across each session to avoid the effect of repetition, but capture the same emotion dimensions. At the start of each session, we record a short segment of the user in their natural stance without any stimuli, to establish a baseline. We record their behavior using four main recording modalities: 1) video camera, both close-up on the face and wide-angle to capture the upper body, 2) thermal camera, close-up on the face, 3) lapel microphone, 4) physiological measurements, in which we choose to measure heart rate, breathing rate, skin conductance and skin temperature (Figure  4 .1). The data include self-report annotations for emotion and stress (Perceived Stress Scale, PSS)  [56, 57] , as well as emotion annotations obtained from Amazon Mechanical Turk (AMT). To understand the influence of personality on the interaction of stress and emotion, we obtain Big-5 personality scores  [87] , which was filled by 18 of the participants, due to the participation being voluntary.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "We use these features consistently across the thesis to have a standardized set of inputs, aiming to avoid variability that comes from different labelling or pre-processing schemas.\n\nOur preprocessing corresponds to converting Likert scale emotion annotations to classes based on quartiles. The feature processing has 2 components, acoustic and lexical, for training, testing or fine-tuning speech-only, text-only or bimodal models.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Emotion Labels",
      "text": "Each utterance in the MuSE dataset was labeled for activation and valence on a nine-point Likert scale by eight crowd-sourced annotators  [105] , who observed the data in random order across subjects. We average the annotations to obtain a mean score for each utterance, and then bin the mean score into one of three classes, defined as, {\"low\": [min, 4.5], \"mid\":\n\n(4.5, 5.5], \"high\": (5.5, max]}. The resulting distribution for activation is: {\"high\": 24.58%, \"mid\": 40.97% and \"low\": 34.45%} and for valence is {\"high\": 29.16%, \"mid\": 40.44%\n\nand \"low\": 30.40%}. Utterances in IEMOCAP and MSP-Improv were annotated for valence and activation on a five-point Likert scale. The annotated activation and valence values were averaged for an utterance and binned as: {\"low\": [1, 2.75], \"mid\": (2.75, 3.25], \"high\":\n\n(3.25, max]}",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Stress Labels",
      "text": "Utterances in the the MuSE dataset include stress annotations, in addition to the activation and valence annotations. The stress annotations for each session were self-reported by the participants using the Perceived Stress Scale (PSS)  [58] . We perform a paired t-test for subject wise PSS scores, and find that the scores are significantly different for both sets  (16.11 vs 18.53)  at ùëù < 0.05. This especially true for question three (3.15 vs 3.72), and hence, we double the weightage of the score for this question while obtaining the final sum. We bin the original nine-point adjusted stress scores into three classes, {\"low\": (min, mean-2], \"mid\": (mean-2, mean+2], \"high\": (mean+2, max]}. We assign the same stress label to all utterances from the same session. The distribution of our data for stress is \"high\":\n\n40.33%, \"mid\": 25.78% and \"low\": 38.89%\n\nImprovisation Labels. Utterances in the IEMOCAP dataset were recorded in either a scripted scenario or an improvised one. We label each utterance with a binary value {\"scripted\", \"improvised\"} to reflect this information.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Lexical And Acoustic Feature Extraction",
      "text": "",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Acoustic",
      "text": "We use Mel Filterbank (MFB) features, which are frequently used in speech processing applications, including speech recognition, and emotion recognition  [116, 126] . We extract the 40-dimensional MFB features using a 25-millisecond Hamming window with a step-size of 10-milliseconds. As a result, each utterance is represented as a sequence of 40-dimensional feature vectors. We ùëß-normalize the acoustic features by session for each speaker.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Lexical",
      "text": "We have human transcribed data available for MuSE and IEMOCAP. We use the word2vec representation based on these transcriptions, which has shown success in sentiment and emotion analysis tasks  [121] . We represent each word in the text input as a 300-dimensional vector using a pre-trained word2vec model  [155] , replacing out-of-vocab words with the ‚ü®ùë¢ùëõùëò‚ü© token. Besides, we also incorporate BERT embeddings for enhanced contextual understanding. These embeddings, generated from the pre-trained BERT model, provide deep, bidirectional representations by understanding the text context from both directions.\n\nEach utterance is eventually represented as a sequence of 768-dimensional feature vectors.\n\nWe use just acoustic inputs for MSP-Improv because human transcriptions are not available.",
      "page_start": 48,
      "page_end": 49
    },
    {
      "section_name": "Chapter Iv",
      "text": "Emotion Recognition Dataset: MuSE",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "Endowing automated agents with the ability to provide support, entertainment and interaction with human beings requires sensing of the users' affective state. These affective states are impacted by a combination of emotion inducers, current psychological state, and various contextual factors. Although emotion classification in both singular and dyadic settings is an established area, the effects of these additional factors on the production and perception of emotion is understudied. This chapter presents a dataset, Multimodal Stressed Emotion (MuSE), to study the multimodal interplay between the presence of stress and expressions of affect. We describe the data collection protocol, the possible areas of use, and the annotations for the emotional content of the recordings. The chapter also presents several baselines to measure the performance of multimodal features for emotion and stress classification.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Introduction",
      "text": "Virtual agents have become more integrated into our daily lives than ever before  [144] .\n\nFor example, Woebot is a chatbot developed to provide cognitive behavioral therapy to a user  [74] . For this chatbot agent to be effective, it needs to respond differently when the user is stressed and upset versus when the user is calm and upset, which is a common strategy in counselor training  [213] . While virtual agents have made successful strides in understanding the task-based intent of the user, social human-computer interaction can still benefit from further research  [54] . Successful integration of virtual agents into real-life social interaction requires machines to be emotionally intelligent  [27, 238] .\n\nBut humans are complex in nature, and emotion is not expressed in isolation  [90] . Instead, it is affected by various external factors. These external factors lead to interleaved user states, which are a culmination of situational behavior, experienced emotions, psychological or physiological state, and personality traits. One of the external factors that affects psychological state is stress. Stress can affect everyday behavior and emotion, and in severe states, is associated with delusions, depression and anxiety due to impact on emotion regulation mechanisms  [122, 193, 216, 225] . Virtual agents can respond in accordance to users' emotions only if the machine learning systems can recognize these complex user states and correctly perceive users' emotional intent. We introduce a dataset designed to elicit spontaneous emotional responses in the presence or absence of stress to observe and sample complex user states.\n\nThere has been a rich history of visual  [235, 111] , speech  [143] , linguistic  [207] , and multimodal emotion datasets  [38, 36, 183] . Vision datasets have focused both on facial movements  [111]  and body movement  [131] . Speech datasets have been recorded to capture both stress and emotion separately but do not account for their inter-dependence  [185, 97, 127, 243] . Stress datasets often include physiological data  [234, 210] .\n\nExisting datasets are limited because they are designed to elicit emotional behavior, while neither monitoring external psychological state factors nor minimizing their impact by relying on randomization. However, emotions produced by humans in the real world are complex. Further, our natural expressions are often influenced by multiple factors (e.g., happiness and stress) and do not occur in isolation, as typically assumed under laboratory conditions. The primary goal of this work is to collect a multimodal stress+emotion dataset  For MuSE, The extracted features for each modality, and the anonymized dataset (other than video) will be released publicly along with all the corresponding data and labels. We present baseline results for recognizing both emotion and stress in the chapter, in order to validate that the presence of these variables can be computationally extracted from the dataset, hence enabling further research.",
      "page_start": 50,
      "page_end": 51
    },
    {
      "section_name": "Muse Dataset",
      "text": "",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Experimental Protocol",
      "text": "We collect a dataset that we refer to as Multimodal Stressed Emotion (MuSE) to facilitate the learning of the interplay between stress and emotion. The protocol for data collection is shown in Figure  4 .1. There were two sections in each recording: monologues and watching emotionally evocative videos. We measure the stress level at the beginning and end of each recording. The monologue questions and videos were specifically chosen to cover all categories of emotions. At the start of each recording, we also recorded a short one-minute clip without any additional stimuli to register the baseline state of the subject.\n\nPrevious research has elicited situational stress such as public speaking  [123, 85, 8] , mental arithmetic tasks  [139]  or use Stroop Word Test  [215] . However, these types of stress are often momentary and fade rapidly in two minutes  [139] . We alleviate this concern by recording both during and after final exams (we anticipate that these periods of time are associated with high stress and low stress, respectively) in April 2018. We measure stress using Perceived Stress Scale  [57]  for each participant. We measure their self-perception of the emotion using Self-Assessment Manikins (SAM)  [30] . The recordings and the survey measures were coordinated using Qualtrics1 enabling us to ensure minimal intervention and limit the effect of the presence of another person on the emotion production.\n\nEach monologue section comprised of five questions broken into sections meant to elicit a particular emotion (Table  4 .1). These questions were shown to elicit thoughtful and emotional responses in their data pool to generate interpersonal closeness  [11] . We include an icebreaker and ending question to ensure cool off periods between change in recording section, i.e., from neutral to monologues, and from monologues to videos, hence decreasing the amount of carry-over emotion from the previous monologue to the next. Each subject was presented with a different set of questions over the two recordings to avoid repetition effect. We also shuffle the order of the other three questions to account for order effects  [133] .\n\nEach subject was asked to speak for a minimum of two minutes. After their response to each question, the subjects marked themselves on two emotion dimensions: activation and valence on a Likert Scale of one to nine using self-assessment manikins  [30] .\n\nFor the second part of the recording, the subjects were asked to watch videos in each of the four quadrants i.e., the combination of {low, high} √ó {activation, valence} of emotion.\n\nThese clips were selected from the corpus  [140, 20] , which tested for the emotion elicited from the people when watching these clips (Table  4 .2). The subjects were monitored for 1umich.qualtrics.com their reaction to the clips. After viewing a clip, subjects are asked to speak for thirty seconds about how the video made them feel. After their response, they marked a emotion category, e.g., angry, sad, etc. for the same clip. When switching videos, the subjects were asked to view a one-minute neutral clip to set their physiological and thermal measures back to the baseline  [189] .\n\nThe 28 participants were also asked to fill out an online survey used for personality measures on the big-five scale  [87] , participation being voluntary. This scale has been validated to measure five different dimensions named OCEAN (openness, conscientiousness, extraversion, agreeableness, and neuroticism) using fifty questions and has been found to correlate with passion  [60] , ambition  [19] , and emotion mechanisms  [181] . We received responses for this survey from 18 participants. These labels can be used in further work to evaluate how these personality measures interact with the affects of stress in emotion production, as previously studied in  [242] .",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "Equipment Setup",
      "text": "The modalities considered in our setup are: thermal recordings of the subject's face, audio recordings of the subject, color video recording of the subject's face, a wide-angle color video recording the subject from the waist up and physiological sensors measuring skin conductance, breathing rate, heart rate and skin temperature. For these modalities we have set up the following equipment:\n\n1. FLIR Thermovision A40 thermal camera for recording the close-up thermal recording of the subject's face. This camera provides a 640x512 image in the thermal infrared spectrum.\n\n2. Raspberry Pi with camera module V2 with wide-angle lens used for the waist up shot of the subject. We have chosen Raspberry Pi's due to its low price and support for Linux OS, which integrates easily into a generic setup.\n\n3. Raspberry Pi with camera module V2 used to record the subject from the waist up. 4. TASCAM DR-100 mk II used to record audio. We chose this product for its high fidelity. It can record 24-bit audio at 48kHz.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "5.",
      "text": "ProComp ‚àû -8 channel biofeedback and neurofeedback system v6.0 used to measure blood volume pulse (BVP sensor), skin conductance (SC sensor), skin temperature (T sensor), and abdominal respiration (BR sensor)\n\nThe equipment operator started and marked the synchronization point between video and audio recordings using a clapper. Subsequent time stamps are recorded by the qualtrics",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Benny And Joone",
      "text": "Actor plays the fool in a coffee shop Something About Mary Ben Stiller fights with a dog Neutral A display of zig-zag lines across the screen Screen-saver pattern of changing colors survey using subject click timings.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Post-Processing",
      "text": "Splitting of the Recordings. Each modality is split into neutral recordings of one-minute, five questions and four video recordings with associated monologues, resulting in fourteen recordings for emotional content, thus 28 recordings per subject. In total we have 784 distinct recordings over five modalities, 28 subjects and two stress states, for a total of 3920 recording events. Temperatures are clamped to between 0 ùëú C and 50 ùëú C. This helps reduce the size of the thermal recording files after being zipped.\n\nUtterance Construction. The five monologues extracted above were divided into utterances. However, since the monologues are a form of spontaneous speech, there are no clear sentence boundaries marking end of utterance. We manually created utterances by identifying prosodic or linguistic boundaries in spontaneous speech as defined by  [125] . or (c) a very long pause in thought. This method has been previously shown to be effective in creating utterances that mostly maintain a single level of emotion  [118] .\n\nThe dataset contains 2,648 utterances with a mean duration of 12.44 ¬± 6.72 seconds (Table  4 .3). The mean length of stressed utterances (11.73 ¬± 5.77 seconds) is significantly different (using two-sample t-test) from that of the non-stressed utterances (13.30 ¬± 6.73 seconds). We remove utterances that are shorter than 3-seconds and longer than 35-seconds and end up retaining 97.2% of our dataset. This allows us to to avoid short segments that may not have enough information to capture emotion, and longer segments that can have variable emotion, as mentioned in  [118] . Because our dataset is comprised of spontaneous utterances, the mean length of utterance is larger than those in a scripted dataset  [38]  due to more corrections and speech overflow.",
      "page_start": 56,
      "page_end": 57
    },
    {
      "section_name": "Stress State Verification.",
      "text": "We perform a paired t-test for subject wise PSS scores, and find that the mean scores are significantly different for both sets (16.11 vs 18.53) at ùëù < 0.05. This implied that our hypothesis of exams eliciting persistently more stress than normal is often true. In our dataset, we also provide levels of stress which are binned into three categories based on weighted average (using questions for which the t-test score was significant).",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Emotional Annotation",
      "text": "",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Crowdsourcing",
      "text": "Crowdsourcing has previously been shown to be an effective and inexpensive method for obtaining multiple annotations per segment  [99, 34] . We posted our experiments as Human\n\nIntelligence Tasks (HITs) on Amazon Mechanical Turk and used selection and training mechanisms to ensure quality  [106] . HITs were defined as sets of utterances in a monologue.\n\nThe workers were presented with a single utterance and were asked to annotate the activation and valence values of that utterance using Self-Assessment Manikins  [30] . Unlike the strategy adopted in  [47] , the workers could not go back and revise the previous estimate of the emotion. We did this to ensure similarity to how a human listening into the conversation might shift their perception of emotion in real time. These HITs were presented in either the contextual or the random presentation condition defined below.\n\nIn the contextual experiment, we posted each HIT as a collection of ordered utterances from each section of a subject's recording. Because each section's question was designed to elicit an emotion, to randomize the carry-over effect in perception, we posted the HITs in a random order over the sections from all the subjects in our recording. For example, a worker might see the first HIT as Utterance 1...N from Section 3 of Subject 4's stressed recording and see the second HIT as Utterance 1...M from Section 5 of Subject 10's non-stressed recording\n\nwhere N, M are the number of utterances in those sections respectively. This ensures that the annotator adapts to the topic and fluctuations in speaking patterns over the monologue being annotated.\n\nIn the randomized presentation, each HIT is an utterance from any section, by any speaker, to ensure lack of adaptation to both speaker specific style and the contextual information.\n\nThe per-utterance and the contextual labels can be used to train different machine learning models that are apt for either singular one-off instances or for holding multiple turn natural conversation, respectively.",
      "page_start": 58,
      "page_end": 59
    },
    {
      "section_name": "Emotion Content Analysis",
      "text": "We show the distribution of the annotations received in both the random and contextual setting in Table  4 .3 and Figure  4 .3. The labels obtained for our dataset form a distribution that mostly covers negative and neutral levels of activation, and all but extremities for valence.\n\nThis can also be seen in the data summary in Table  4 .3. We performed a paired t-test between the labels obtained from random vs contextual presentation and found that these labels are significantly different (using paired t-test at ùëù < 0.05 for both activation and valence for utterances in the non-stressed situation). Although the obtained labels are significantly different for valence in the stressed category using the same method as above, the same does not hold true for the activation annotations in this category.",
      "page_start": 59,
      "page_end": 60
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe our baseline experiments for predicting emotion and stress in the recorded modalities. We have a more granular marked annotation of emotion, i.e., over each utterance, as compared to stress over the complete monologue. Hence, we extract features for each modality over continuous one second frame intervals for predicting stress, and over the complete utterance for emotion. Audio and lexical features are still extracted over a complete utterance for stress due to higher interval of variation over time.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Evaluation Of Emotion Recognition",
      "text": "We use the following set of features for our baseline models:\n\n1. Acoustic Features. We extract acoustic features using OpenSmile  [71]  with the eGeMAPS configuration  [70] . The eGeMAPS feature set consists of 88 utterance- level statistics over the low-level descriptors of frequency, energy, spectral, and cepstral parameters. We perform speaker-level ùëß-normalization on all features.\n\n2. Lexical Features. We extract lexical features using Linguistic Inquiry and Word Count (LIWC)  [174] . These features have been shown to be indicative of stress, emotion, veracity and satisfaction  [86, 161, 164] . We normalize all the frequency counts by the total number of words in the sentence accounting for the variations due to utterance length.\n\n3. Thermal Features. For each subject a set of four regions were selected in the thermal image: the forehead area, the eyes, the nose and the upper lip as previously used in  [172, 80, 6] . These regions were tracked for the whole recording and a 150-bin histogram of temperatures was extracted from the four regions per frame, i.e., 30 frames a second for thermal recordings. We further reduced the histograms to the first four measures of central tendency, e.g. Mean, Standard Deviation, Skewness and Kurtosis. We combined these features over the utterance using first delta measures (min, max, mean, SD) of all the sixteen extracted measures per frame, resulting in 48 measures in total. 4. Close-up Video Features. We use OpenFace  [15]  to extract the subject's facial action units. The AUs used in OpenFace for this purpose are AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU12, AU14, AU15, AU17, AU20, AU23, AU25, AU26, AU28 and AU25 comprising of eyebrows, eyes and mouth. These features have been previously\n\nshown to be indicative of emotion  [227, 64]  and have been shown to be useful for predicting deception  [110] . We summarize all frames into a feature using summary statistics (maximum, minimum, mean, variance, quantiles) across the frames and across delta between the frames resulting in a total of 144 dimensions.\n\nNetwork Setup. We train and evaluate multiple unimodal Deep Neural Networks (DNN) models for predicting valence and activation using Keras  [91] .  [106]  have shown that a match between the context provided to the classifier and the annotator leads to better classification performance. Because we are performing single utterance classification, for all further experiments, we use the annotations obtained in a random manner as mentioned above. In all cases, we predict the continuous annotation using regression.\n\nWe also use an ensemble of these four networks (audio, lexical, visual and thermal) to measure multimodal performance. For each network setup, we follow a five-fold subject We train our networks for a maximum of 50 epochs and monitor the validation loss after each epoch. We perform early stopping if the loss doesn't decrease for 15 consecutive epochs. We save the weights that achieved the lowest validation performance during training.\n\nWe train each network five times with different seeds and average the predictions to account for variations due to random initialization.\n\nResults. We show our results in Table  4 .4. We find that between acoustic and lexical modalities, the acoustic modality carries more information about activation and the lexical for valence. This is in line with previous research  [232, 39] . We also note that the visual modality significantly outperforms both the speech and lexical modalities for valence prediction.\n\nWhen we merge these networks using late voting on each modality (decision fusion), we find that the combination of all modalities performs the best for predicting activation. But for predicting valence, the best performance is shown by the combination of acoustic, lexical, visual and thermal modalities. We believe this is true because previous work has shown that thermal features are mostly indicative of intensity and discomfort  [94]  and hence improves performance on activation prediction, while the visual expressions are most informative about valence  [186] .",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Evaluation Of Presence Of Stress",
      "text": "We use the following set of features for our baseline models. Given that stress vs non-stressed state is classified for the complete section (monologue or neutral recording),\n\nwe extract visual features differently to use the the sequential information over the whole segment, i.e., a monologue. We also use physiological features for our network, since we found that even though they are highly variable over shorter segments (utterances), they are informative for recognizing physiological state on a whole section.\n\n1. Acoustic, Lexical, and Thermal Features. We use the same features as extracted for predicting emotion.\n\n2. Wide-angle Video Features. We extract the subject's pose using OpenPose  [40, 199, 228]  at 25 frames per second. For each frame, we extract 14 three-dimensional points representing anchor points for the upper body. For classification of each 3D point is interpolated over one second using a 5 ùë°‚Ñé order spline  [167, 102] . The parameters of the splines are then used as features for classification.\n\n3. Close-up Video Features. We use OpenFace to extract the subject's action units  [15] .\n\nThe features are extracted for every frame. In each frame, features include the gaze direction vectors, gaze angles, 2D eye region landmarks, head locations, rotation angles of the head, landmark locations, and facial action units. Landmarks locations offset by the nose location. We window the data into segments of one-second windows with 0.5 second overlap and calculate summary statistics (maximum, minimum, mean, Network. We train a DNN to perform binary classification, i.e., to recognize stressed vs. non-stressed situation using ReLU as activation, with softmax as the classification method.The final layer uses a soft-max activation. We train six different networks for thermal, wide-angle video, close-up video, physiological, audio, and lexical modalities.\n\nEach network is trained in a subject-independent manner. We train network to recognize stress vs non-stress situation in both neutral recording,i.e., when the subject isn't speaking at the beginning of the recording, and during emotional monologue questions. To do so, we decide the final prediction by a majority vote over one-second predictions for the complete section of the recording. For the lexical and acoustic modality, we train the network for the question monologues, and decide the final prediction based on a majority vote over prediction for each utterance.\n\nResults. We report our results for prediction of stress vs non-stress situation using various modalities in Table  4 .5. We see that the captured modalities are indeed informative for recognizing stress vs non-stressed situations. We find that for recognizing this distinction when the subjects are speaking, audio and physiological features perform the best. This is in agreement with previous related work  [131, 234, 96] . Interestingly, we also find that the thermal and physiological modality is apt at recognizing differences in stress, even in the neutral recording, i.e., when the subject is not speaking. This advantage of thermal modality has been previously documented by researchers  [7, 173, 172, 80] . We find that answering emotional monologue questions interferes with the recorded thermal modality, leading to a poorer performance at stress recognition.",
      "page_start": 60,
      "page_end": 61
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this chapter, we introduced a dataset that aims to capture the interplay between psychological factors such as stress and emotion. While various other datasets have explored the relationship between gender or personality measures and emotion production and perception, the relationship between psychological factors and emotion is understudied from a data collection point of view, and hence an automated modeling perspective.\n\nWe verified that the presence of emotion and stress can be detected in our dataset. Our baseline results for emotion classification using DNNs with acoustic, linguistic and visual features on our dataset are similar to reported results on other datasets such as IEMOCAP  [36]  and MSP-Improv  [38] .",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Chapter V",
      "text": "Best Practices for Noise Based Augmentation in Emotion Datasets",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "Speech emotion recognition is an important component of any human centered system.\n\nBut speech characteristics produced and perceived by a person can be influenced by a multitude of reasons, both desirable such as emotion, and undesirable such as noise. To train robust emotion recognition models, we need a large, yet realistic data distribution, but emotion datasets are often small and hence are augmented with noise. Often noise augmentation makes one important assumption, that the prediction label should remain the same in presence or absence of noise, which is true for automatic speech recognition but not necessarily true for perception based tasks. In this chapter we make three novel contributions. We validate through crowdsourcing that the presence of noise does change the annotation label and hence may alter the original ground truth label. We then show how disregarding this knowledge and assuming consistency in ground truth labels propagates to downstream evaluation of ML models, both for performance evaluation and robustness testing. We end the chapter with a set of recommendations for noise augmentations in speech emotion recognition datasets.",
      "page_start": 67,
      "page_end": 71
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is increasingly included as a component in many real-world human-centered machine learning models. Modulations in speech can be produced for a multitude of reasons, both desirable and undesirable. In our case desirable modulations encode information that we want our model to learn and be informed by, such as speaker characteristics or emotion. Undesirable modulations encode information that are extrinsic factors change with the environment, such as noise. In order to handle these modulations, we need large datasets that capture the range of possible speech variations and their relationship to emotion expression. But, such datasets are generally not available for emotion tasks. To bridge this gap, researchers have proposed various methods to generate larger datasets. One of the most common is noise augmentation. The baseline assumption of noise augmentation is that the labels of the emotion examples do not change once noise has been added  [169] .\n\nWhile this assumption can be confidently made for tasks such as automatic speech recognition (ASR), the same cannot be said for perception-based tasks, such as emotion recognition.\n\nIn this chapter, we question the assumption that the annotation label remains the same in the presence of noise. We first create a noise augmented dataset and conduct a perception study to label the emotion of these augmented samples, focused on the type of noise in samples whose perception has changed or remained the same given the agumentation. We use the results from this study to classify the complete set of augmentation noises into two categories, perception-altering (i.e., noises that may change the perception of emotion) and perception-retaining (i.e., noises that do not change the perception of emotion). We propose that the perception-altering noises should not be used in supervised learning or evaluation frameworks because we cannot confidently maintain that the original annotation holds for a given sample. We evaluate the effects of disregarding emotion perception changes by examining how the performance of emotion recognition models and analyses of their robustness change in unpredictable manners when we include samples that alter human perception in the training of these models. Lastly, we provide a set of recommendations for noise based augmentation of speech emotion recognition datasets based on our results.\n\nResearchers have considered the impact of noise on emotion perception and thereby the annotation of emotions. In this chapter, we claim that the standard assumption about perception and hence, label retention of emotion in the presence of noise may not hold true in a multiple noise categories.\n\nTo understand which noises impact emotion perception, we use a common emotion dataset, IEMOCAP and introduce various kinds of noises to it, at varying signal to noise ratio (SNR) levels as well as at different positions in the sample. We then perform a crowdsourcing experiment that asks workers to annotate their perception of emotion for both the clean and the corresponding noise-augmented sample. This enables us to divide noise augmentation options into groups characterized by their potential to either influence or not influence human perception.\n\nThe results of the crowdsourcing experiments inform a series of empircal analyses focused on model performance and model robustness. We first present an empirical evaluation of the effects of including perception-altering noises in training. It will allow us to observe how the inclusion of perception-altering noises creates an impression of performance improvement.\n\nWe will discuss how this improvement is a myth, this new model will have learned to predict labels that are not truly associated with a given sample due to the perceptual effects of these noises. We consider both a general recurrent neural network (RNN) model and an We find that the crowdsourced labels do change in the presence of some kinds of noise.\n\nWe then verify that the models perform worse on noisy samples when trained only on clean datasets. But, we show that this decrease in performance is different when using the complete set of noises for augmenting the test set vs. when only using the perception-retaining noises for augmentation. We show similar patterns for noise-robust models, specifically\n\nshowing how there is an increased drop in performance for the end-to-end noise-robust model when excluding performance-altering noises during augmentation. We then discuss how our conventional metrics, those that look only at model performance, may be incorrectly asserting improvements as the model is learning to predict an emotion measure that is not in line with human perception. Troublingly, we find that the attack model is generally more effective when it has access to the set of all noises as compared to when excluding perception-altering noises for allowed augmentations. We also specifically find that given just a pool of carefully crafted reverberation modulations, the attack model can be successful in almost 65% of the cases with minimal degradation in SNR and in less than ten queries to the trained model. We end the chapter with a general set of recommendations for noise augmentations in speech emotion recognition datasets.",
      "page_start": 68,
      "page_end": 71
    },
    {
      "section_name": "Research Questions",
      "text": "In this chapter, we investigate five research questions:\n\nPurpose 1: Premise Validation through Crowdsourcing RQ1: Does the presence of noise affect emotion perception as evaluated by human raters?\n\nIs this effect dependent on the utterance length, loudness, the type of the added noise, and the original emotion?\n\nReason: Noise has been known to have masking effect on humans in specific situations.\n\nHence, humans can often understand verbalized content even in presence of noise. Our goal is to understand whether the same masking effect extends to paralinguistic cues such as emotion, and to what extent. Our continuing claim from hereon remains that only noises that do not change human perception should be used for the training and evaluation of machine learning models. Not doing so, can lead to gains or drops in performance measurement that may not actually extend to real world settings. We call these changes \"unverified\" because we cannot, with certainity, be sure that the model should have predicted the original label (i.e., the label of the sample before noise was added) because the human did not neccessarily label the noisy instance with that same label.\n\nPurpose 2: Noise Impact Quantification RQ2: Can we verify previous findings that the presence of noise affects the performance of emotion recognition models? Does this effect vary based on the type of the added noise?\n\nReason: We have known that presence of noise in data shifts the data distribution  [50] .\n\nThis shift often leads to poor performance by machine learning models. We aim to quantify the amount of performance drop based on the type of noise in these systems, both, for any kind of noise, and then, specifically for noises that do not change human perception (perception-retaining).",
      "page_start": 71,
      "page_end": 72
    },
    {
      "section_name": "Purpose 3: Denoising And Augmentation Benefits Evaluation",
      "text": "RQ3: Does dataset augmentation (Q3a) and/or sample denoising (Q3b) help improve the robustness of emotion recognition models to unseen noise?\n\nReason: We test whether the commonly-used methods for improving the performance of these models under distribution shifts is helpful. We focus on two main methods, augmentation and denoising. We specifically look at how performance changes when we augment with noises that include those that are perception-altering vs. when we exclude such noises.",
      "page_start": 72,
      "page_end": 72
    },
    {
      "section_name": "Purpose 4: Model Robustness Testing Conditions",
      "text": "RQ4: How does the robustness of a model to attacks compare when we are using test samples that with are augmented with perception-retaining noise vs. samples that are augmented with all types of noise, regardless of their effect on perception?\n\nReason: Another major metric for any deployable machine learning algorithm is its performance on \"unseen situations\" or handling incoming data shifts (i.e., robustness testing).\n\nWe test robustness using a noise augmentation algorithm that aims to forcefully and efficiently change a model's output by augmenting test samples with noise. We look at how often this algorithm is unsuccessful in being able to \"fool\" a model with its augmented samples. We look at the changes in frequency with which a model is successfully able to defend itself when the attack algorithm uses a set that includes all types of noises vs. when it only uses perception-retaining noises. Reason: We then provide a set of recommendations based on our empirical studies for deploying emotion recognition models in real world situations.",
      "page_start": 98,
      "page_end": 98
    },
    {
      "section_name": "Noise",
      "text": "We investigate the effects of two types of noise, environmental and signal distortion.\n\nEnvironmental noises are additive, while signal distortion noise involves other types of signal manipulation.",
      "page_start": 73,
      "page_end": 73
    },
    {
      "section_name": "Environmental Noise",
      "text": "We define environmental noises (ENV) as additive background noise, obtained from the ESC-50 dataset  [176] 1. ESC-50 is generally used for noise contamination and environmental sound classification  [231] . These environmental sounds are representative of many types of noise seen in real world deployments, especially in the context of virtual and smart home conversational agents. We use the following categories:\n\n‚Ä¢ Natural soundscapes (Nat), e.g., rain, wind.\n\n‚Ä¢ Human, non-speech sounds (Hum), e.g., sneezing, coughing, laughing or crying in the background etc.\n\n‚Ä¢ Interior/domestic sounds (Int), e.g., door creaks, clock ticks etc.\n\nWe manipulate three factors when adding the noise sources:\n\n‚Ä¢ Position: The position of the introduction of sound that: (i) starts and then fades out in loudness or (ii) occurs during the entirety of the duration of the utterance. In the second case, this complete additive background would represent a consistent noise source in real world (e.g., fan rotation).\n\n‚Ä¢ Quality Degradation: The decrease in the signal to noise ratio (SNR) caused by the addition of the additive background noise at levels of 20dB, 10dB and 0dB. This is used only when noise is added to the entirety of the utterance.",
      "page_start": 73,
      "page_end": 73
    },
    {
      "section_name": "Signal Distortion",
      "text": "We define signal distortion noise as modulations that aren't additive in the background.\n\nThese kinds of noise in the audio signal can occur from linguistic/paralinguistic factors, room environment, internet lags, or the physical locomotion of the speaker.\n\nWe use the nine following categories:\n\n‚Ä¢ SpeedUtt: The utterance is sped up by either 1.25√ó or 0.75√ó.\n\n‚Ä¢ SpeedSeg: A random segment within an utterance is sped up by 1.25√ó. The package pyAudio 2 that we used to speed up a segment did not permit slowing a segment down.\n\nThus, the 0.75√ó was not used here.\n\n‚Ä¢ Fade: The loudness of the utterance is faded by 2% every second, which emulates the scenario of a user moving away from the speaker. The loudness is increased for fade in, and decreased for fade out.\n\n‚Ä¢ Filler: Non-verbal short fillers such as 'uh', 'umm' (from the same speaker) are inserted in the middle of a sentence. The insertion is either just the filler or succeeded and preceded by a long pause 3.\n\n‚Ä¢ DropWord: A randomly selected set of non-essential words belonging to the set: {a, the, an, so, like, and} are dropped from an utterance using word-aligned boundaries and stiching the audio segments together.\n\n‚Ä¢ DropLetters: Following the same approach as drop word, letters are dropped in accordance with various linguistic styles chosen from the set: {/h/+vowel, vowel+/nd/+consonant(next word), consonant+/t/+consonant(next word), vowel+/r/+consonant, /ihng/}. This is supported by research that has studied phonological deletion or dropping of letters in the native US-English dialect  [1, 237] .\n\n2https://people.csail.mit.edu/hubert/pyaudio/ 3Fillers are obtained by parsing audio files for a given speaker and finding occurrences of any of the options from the above mentioned set. We will release the extracted fillers per speaker for IEMOCAP ‚Ä¢ Laugh/Cry: \"Sob\" and \"short-laughter\" sounds are added to the utterance. They are obtained from AudioSet  [82] .\n\n‚Ä¢ Pitch: The pitch is changed by ¬± 3 half octaves using the pyAudio library.\n\n‚Ä¢ Rev: Room reverberation is added to the utterance using py-audio-effects (pysndfx4).\n\nWe vary metrics such as reverberation ratio or room size to vary the type and intensity of reverberation added.",
      "page_start": 74,
      "page_end": 75
    },
    {
      "section_name": "Sampling And Noise-Perturbations",
      "text": "We randomly select 900 samples from the IEMOCAP dataset, which is far larger than the ones used for previous perception studies  [170, 191] . We select 100 samples from each activation and valence pair bin, i.e., 100 samples from the bin with activation: low, valence: low; 100 samples from the bin with activation: low, and valence: mid, and so on. This ensures that the chosen 900 samples cover the range of emotions expressed. We impose another constraint on these 100 samples from each bin, 30 of them are shorter than the first quartile or greater than fourth quartile of utterance length in seconds to cover both extremities of the spectrum, and the remaining 70 belong in the middle. We also ensure that the selected samples had a 50-50 even split amongst gender. We introduce noise to the 900 samples (Section 3.1). Each sample is modulated in ten ways: four randomly chosen types of environmental noise and six randomly chosen signal distortion noise modulations, giving us a total of 9,000 noisy samples5.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "User Study",
      "text": "We first analyze the effects of noise on human perception by relabeling the noiseenhanced data using the Amazon Mechanical Turk (AMT) platform. We use insights from this experiment to guide the machine learning analyses that follow.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Crowdsourcing Setup",
      "text": "We recruited 147 workers using Amazon Mechanical Turk who self-identify as being from the United States and as native English speakers, to reduce the impact of cultural variability. We ensured that each worker had > 98% approval rating and more than 500 approved Human Intelligence Tasks (HITs). We ensured that all workers understood the meaning of activation and valence using a qualification task that asked workers to rank emotion content similar to  [105] . The qualification task has two parts: (i) we explain the difference between valence and activation and how to identify those, and, (ii) we ask them to identify which of the two samples has a higher/lower valence and a higher/lower activation, to ensure that they have understood the concept of activation and valence annotations. All HIT workers were paid a minimum wage ($9.45/hr), pro-rated to the minute. Each HIT was annotated by three workers.\n\nFor our main task, we created pairs that contained one original and one modulated sample. We then asked each worker to annotate whether or not they perceived the pair to have the same emotion. If they said yes for both activation and valence, the noisy sample was labeled same and they could directly move to the next HIT. If they said no, the noisy sample was labeled different. In this case, they were asked to assess the activation and valence of the noisy sample using Self Assessment Manikins  [30]  on a scale of  [1, 5]  (similar to the original IEMOCAP annotation).\n\nWe also include three kinds of attention checks:\n\n1. We show two samples that have not been modified and ask them to decide if the emotion represented was different. If the person says yes, then the experiment ends.\n\n2. We observe the time spent on the task. If the time spent on the task is less than the combined length of both samples, then the user's qualification to annotate the HITs is rescinded and their responses are discarded.\n\n3. We show two samples, one which has a gold standard label, and another, which has been contaminated with significant noise (performance degradation >30dB), such that the resulting sample is incomprehensible. If people do not mark this set of samples as being different, the experiment ends.\n\nThe failure rate based on the above criteria was 8%. We ensured the quality of the annotations by paying bonuses based on time spent, not just number of HITs, and by disqualifying annotators if they annotated any sample (including those outside of the attention checks) more quickly than the combined length of the audio samples.\n\nWe then created two sets of labels for each noise-augmented clip. The first type of label compared a noise-augmented clip to its original. The noise-augmented clip was labeled the same if the modified and original clip were perceived to have the same valence or activation, otherwise it was labeled different. We created this label by taking the majority vote over all evaluations. The second type of label included valence and activation. A noise-augmented clip was given the average valence and activation over all evaluations.\n\nThe inter-annotator agreement was measured using Cohen's kappa. Conventionally, when estimating Cohen's kappa, annotators are not considered as individuals, instead reducing annotators to the generic 1, 2, and 3. The challenge is that this often leads to artificially inflated inter-annotator agreement because individual characteristics and behavior of a particular worker are not taken under consideration  [95] . We take a different approach, creating a table for the calculation of the statistic that considers annotators as individuals with separate entries for each clip, following the approach of  [95] . If an annotator didn't evaluate a given clip, the cell has a null (missing data) value. We found that the Cohen's kappa was 79% for activation and 76% for valence 6.",
      "page_start": 76,
      "page_end": 77
    },
    {
      "section_name": "Mismatch Match Mismatch Match",
      "text": "",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "Mismatch Match Mismatch Match",
      "text": "",
      "page_start": 77,
      "page_end": 77
    },
    {
      "section_name": "Methods",
      "text": "We now describe the emotion recognition approaches, presenting two separate pipelines, one that relies upon direct feature extraction (Section 5.6.2) and the other that is end-to-end (Section 5.6.3). This allows us to investigate whether noise has a consistent effect. We discuss approaches to improve noise robustness by training models with noise-augmented data or denoised data (Section 5.6.4). Finally, we describe the setup and evaluation of the model robustness using an untargeted model misclassification test, which measures a model's fragility in terms of how likely it is that the model's decisions will change when specific types of noise are observed at test time (Section 5.6.5).",
      "page_start": 83,
      "page_end": 83
    },
    {
      "section_name": "Creation Of Data Partitions",
      "text": "We use a subject-independent five-fold cross validation scheme to select our train, test and validation sets. In the first iteration, sessions 1-3 are used for training, session 4 is used as validation, and session 5 is used for testing. This is repeated in a round-robin fashion, resulting in each session serving as a validation and a test fold. We also divide possible 6The sample name, code to create the paired noisy examples, and the resulting annotations will be made available for further research noises in two different categories based on results of crowdsourcing study (see Section 5.7.1).\n\nThe first category is perception-altering, those that changed perception of humans and hence cannot be used for model training or evaluation with the old annotations. The second category is perception-retraining, those that did not change human perception, and hence, the model should produce no change in predictions when using those noise categories for sample augmentation.\n\nWe use the noise categories (seeSection 5.4) in two varying circumstances. The first category is matched, where both the training and testing sets are augmented with same kinds of noise (e.g., both have nature-based sounds in them). The second category is mismatched, where the testing set is augmented with a noise category not used for augmenting the training set (e.g., only the test set is augmented with nature-based noise while the train set is augmented with human or interior noises).",
      "page_start": 83,
      "page_end": 83
    },
    {
      "section_name": "Traditional Deep Learning Network",
      "text": "We first explore a common \"traditional\" deep learning network that is used in speech emotion recognition. In this method we extract Mel Filterbank (MFB) features as input to a model composed of convolutional and gated recurrent unit (GRU) layers.",
      "page_start": 84,
      "page_end": 84
    },
    {
      "section_name": "Features",
      "text": "We extract 40-dimensional Mel Filterbank (MFB) features using a 25-millisecond Hamming window with a step-size of 10-milliseconds using python-speech-features 7. Each utterance is represented as a sequence of 40-dimensional feature vectors. We ùëß-normalize the acoustic features using parameters extracted from the training dataset. During each cross-validation fold, the parameters are chosen from the training data and are applied to both the validation and testing data.\n\n7https://github.com/jameslyons/python speech features",
      "page_start": 84,
      "page_end": 84
    },
    {
      "section_name": "Network",
      "text": "Our baseline network is a state-of-art single utterance emotion classification model which has been used in previous research  [9, 116, 126] . The extracted MFBs are processed using a set of convolution layers and GRUs (see Table  5 .5 for the hyperparameters used for these layers). The output of these layers is then fed through a mean pooling layer to produce an acoustic representation which is then fed into a set of dense layers to classify activation or valence.",
      "page_start": 85,
      "page_end": 85
    },
    {
      "section_name": "Training.",
      "text": "We implement the models using the Keras library  [51] . We use a cross-entropy loss function for each task (e.g., valence or activation). We learn the model parameters using the RMSProp optimizer. We train our networks for a maximum of 50 epochs and use early stopping if the validation loss does not improve after five consecutive epochs. Once the training process ends, we revert the network's weights to those that achieved the lowest validation loss. We repeat the experiment five times. We report the results in terms of Unweighted Average Recall (UAR, chance is 0.33), averaged over all test samples and five repetitions. We compare the performance of different models or the same model in different noisy conditions/partitions using a paired t-test using the Bonferroni correction, asserting significance when ùëù ‚â§ 0.05.",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "End-To-End Deep Learning Networks",
      "text": "Next, we explore a transformer-based model. In this method the raw audio signal is used as input to a pre-trained and fine-tuned network and the emotion prediction is directly obtained as an output. These models do not require us to perform manual or domain knowledge-based extraction of features. They instead have a feature encoder component inside the model, which is dynamic in nature, and hence, can change its output for the same signal based on the dataset and nature of the task.",
      "page_start": 85,
      "page_end": 85
    },
    {
      "section_name": "Features",
      "text": "For the end-to-end deep learning models, we do not need to extract audio features.\n\nInstead we rely on the network itself to both normalize and extract features, that are later passed onto the deeper layers of the network. The feature set here is the original wav files that are not modified in any capacity. The eventual representations are of size 512, reproducing the setup in the state-of-the-art implementation  [175] .",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "Network",
      "text": "Our baseline network is the state-of-the-art wav2vec2.0 emotion recognition model  [175] .\n\nThe wav2vec model is comprised of three parts: (i) a convolutional neural network (CNN) that acts as feature encoder, (ii) a quantizier module, and (iii) a transformer module. The input to the model is raw audio data (16kHz) that is passed to a multi-block 1-d CNN to generate audio representations (25ms). The quantizer is similar to a variational autoencoder that encodes and extracts features using a contrastive loss. The transformer is used for masked sequence prediction and encodes the bi-directional temporal context of the features.\n\nWe use the base model, which has not been fine-tuned for ASR (wav2vec2.0-PT). We then fine-tune the base model to predict the binned emotion labels. We use the final representation of the output as an input to dense layers to produce the final output.",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "Training",
      "text": "We implement the model provided in the speech brain library 8. As in the other pipeline (Section 5.6.2), we use cross-entropy loss for each task and learn the dense layer parameters.\n\nReproducing the state of the art model  [175]  We run the model for a maximum of eight epochs. We revert the network's state to the one that achieved the lowest validation loss. We repeat this experiment five times. Again, we use UAR and report the results averaged over 8https://speechbrain.readthedocs.io/en/latest/API/ speechbrain.lobes.models.fairseq wav2vec.html both subjects and repetitions.",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "Noise Augmentation Overview",
      "text": "We will be assessing a model's ability to classify emotion given either environmental or signal distortion noise. We perform two kinds of analysis, one when using the set of noises that includes those that do alter human perception, and another when only using noises that are perception-retaining. We report overall model performances for both of these categories.\n\nFor a more thorough analysis, we then specifically focus on the categories of noise that do not significantly affect human perception. This allows us to evaluate a model's robustness, or its fragility, with respect to variations that wouldn't alter a human's perception of emotion. This is important because the overwhelming majority of the noise-augmented utterances in the IEMOCAP dataset were not included in the user study and, therefore, do not have perceptual labels (Section 5.5). We consider three types of environmental noise {Human (Hum), Interior (Int), Natural (Nat)} and three types of signal distortion noise {Speeding a segment (SpeedSeg), Fade, Reverberation (Reverb)}.\n\nWe use two separate testing paradigms: (i) matched testing, in which all noise types are introduced to the training, testing, and validation data and (ii) mismatched testing, in which ùëõ-1 types of noise are introduced to the training and validation sets and the heldout type of noise is introduced to the test set. In all cases, we analyze the test data in terms of specific noise categories. Therefore, the test sets are the same between the two paradigms.\n\nWe run both the matched and mismatched experiments twice, first with the noiseaugmented data and second with a noise-robust/denoising pipeline. The first iteration will allow us to quantify the effect of the noise on the traditional and end-to-end classification pipelines. We then repeat the experiment with either denoised data for the traditional classifier (Section 5.6.4.1) or using the noise-robust implementation of wav2vec2.0 for the end-to-end classifier (Section 5.6.4.2). This allows us to investigate how, or if, noise-robust implementations can offset the effects of background noise.",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Denoising",
      "text": "We implement denoising using the well-known Recurrent Neural Network Noise Suppression (RNNNoise, denoising feature space) approach, proposed in 2017 for noise suppression  [218] . RNNNoise is trained on environmental noise, and these noises overlap considerably with those in our dataset. We use the algorithm's default parameters and use it on an 'as-is' basis for our experiments. We assume that the system does not have the knowledge of which noise, from the set of available noise categories, is introduced and, therefore, we do not compare with other denoising algorithms that assume a priori knowledge of noise category. The result is a set of 'noise-suppressed' samples in the training, validation and testing sets.\n\nWe pass all the data, including both the original and noise-augmented data, through a denoising algorithm. This allows us to ensure that acoustic artifacts, if any, are introduced to both the original and noise-augmented data. We then train the traditional deep learning model as described in Section 5.6.2.",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Using A Noise-Robust Model",
      "text": "In the end-to-end model, we need to use a different denoising approach because the approach described in the previous section does not return a wav file, but instead is applied to the feature-space directly. Here, we enforce robustness to noise using a model trained to be noise-robust in an end-to-end fashion. We use the noise-robust version (Wav2Vec2-Large-Robust) of the aforementioned wav2vec2.0 model  [98] . The noise-robust large model was pretrained on 16kHz sampled speech audio. Noisy speech datasets from multiple domains were used to pretrain the model: Libri-Light, CommonVoice, Switchboard, and, Fisher  [98] .\n\nWe then train the end-to-end model as described in Section 5.6.3.",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Model Robustness Testing",
      "text": "Deployed emotion recognition models must be robust. One of the major scenarios that we robustness test any speech-based model for is the presence of noise. But the set of noises we choose to test robustness on can lead to different conclusions about the robustness of the models. In our case, we consider two different scenarios:\n\n1. Robustness evaluation when using perception-retaining samples, noise samples that do not change human perception 2. Robustness evaluation when using any kind of noise (i.e., both perception-retaining and perception-altering)\n\nWe perform robustness evaluation of a model by using the model's output predictions to create new noise-enhanced samples that change the model's output, compared to the original clean sample. We do this using an untargeted model misclassification test, in which we add noise to the samples. The intentional misclassification algorithm assumes black-box model access. For our purposes, it needs to have access to: (i) a subset of the dataset, (ii) noises to add to create perturbed samples, and (iii) model input and output.\n\nAs in any other perturbation-based robustness testing, the goal is to introduce perturbations to the samples such that the resulting samples are as close to the original sample as possible.\n\nThe minimally perturbed sample should be the one that causes a classifier to change its original classification. We measure the amount of perturbation using SNR, calculated using the logarithmic value of the ratio between the original signal and the noise-augmented signal's power. We note that the lower the decrease in SNR, the more minimally perturbed a sample is. The maximal decrease in SNR that we use in the algorithm is a difference of 10 dB. This condition ensures that the sample is not audibly judged as contaminated by humans  [119] .\n\nThe algorithm to choose this minimally perturbed sample has four major components:\n\n1. Requirements: some labelled samples, noise files, model input and output access, unlabelled samples for testing, and, optionally, correlation between noise type and performance degradation for a given model.",
      "page_start": 89,
      "page_end": 90
    },
    {
      "section_name": "Looping:",
      "text": "The algorithm then loops over each noise category to figure out whether it can successfully force the model to misclassify. The noise category order is random if we do not have access to the optional performance degradation correlations.\n\n3. Minimizing: The algorithm then aims to find the lowest decrease in dB, such that the model still misclassifies. This ensures that the resultant noisy sample is as imperceptible to humans as possible.",
      "page_start": 90,
      "page_end": 90
    },
    {
      "section_name": "End Condition:",
      "text": "The algorithm ends if a noise addition has been found, or if it runs out of number of tries allowed for model access.\n\nPlease see Algorithm 1 for more details. In the algorithm, numAttempts is the number of times the algorithm is allowed to access the model's input-output pairs. Classifier output refers to the prediction made by the model when the attack algorithm sends an input to the model to be classified. Classifier output changes is true when the model predicted the emotion label differently after noise was added to the sample, compared to the original clean sample. Success implies that the algorithm was successfully able to force the model to misclassify a particular sample in the allowed number of attempts. Failure implies that the algorithm could not force the model to misclassify in the allowed number of attempts and that the model can be considered robust for that sample. We use the above algorithm in two different settings, under two different pre-known assumptions, with four levels of allowed queries, and four models (64 categories): For all the test samples, we execute five runs of the above algorithm to account for randomization in noise choices. These five runs are then averaged to obtain the average success of misclassification or average robustness for a given sample (1-average success of misclassification). We then average the robustness value over all the test samples. We report our obtained results for the above mentioned scenarios. We find that the presence of environmental noise, even when loud, rarely affects annotator perception, suggesting that annotators are able to psycho-acoustically mask the background noise in various cases, as also shown in prior work (e.g.,  [206] ).",
      "page_start": 90,
      "page_end": 91
    },
    {
      "section_name": "Analysis",
      "text": "We find that the addition of signal distortion noise alters human perception. The reported change in valence and activation values is on a scale of -1 to 1 (normalized). The addition of laughter changes the activation perception of 16% of the utterances, with an average change of +22% (+.26). The valence perception is altered in 17% of the utterances, with an average change of +14% (+.11). Similarly for crying, valence is altered in 20% of the cases, with an average change of -21% (-.20). Crying changes activation perception in 22% of the cases, with an average change of -32% (-.43). Raises in pitch also alter the perception of emotion. In 22% of utterances, the perception of activation is changed. This contrasts with the perception of valence, which was altered only in 7% of utterances. In this scenario, activation increases by an average of 26% (+.  19) , and valence decreases by 12% (-.11). On the other hand, decreases in pitch change the perception of activation in 10% of the cases and of valence in 29% of the cases. In this scenario, activation decreases by an average of 16% (-.15), and valence decreases by 7% (-.07). This ties into previous work  [37] , which looked into how changes and fluctuations in pitch levels influenced the perception of emotions.\n\nChanges in the speed of an utterance affect human perception of valence in 13% (average of -.13) of the cases when speed is increased, and 28% (average of -.23) when speed is decreased. On the other hand, changes in the speed of an utterance do not affect activation as often, specifically, 3% in case of increase and 6% in case of decrease.\n\nWe ensured that our crowsdourcing samples had an even distribution over gender of the speaker and the length of the sample (see Section 5.4.3). We performed paired t-test to evaluate whether these variables influenced the outcome of emotion perception change in presence of noise. We found that the changes in perception were not tied to characteristics of the speakers. For example, there was no correlation between changes in perception and variables such as, the original emotion of the utterance, the gender of the speaker, and the length of the utterance.\n\nThe human perception study provides insight into how emotion perception changes given noise. This also provides information about the potential effects of noise addition on model behavior. In the sections that follow, we will evaluate how machine perception changes given these sources of noise.",
      "page_start": 91,
      "page_end": 92
    },
    {
      "section_name": "Rq2: Can We Verify Previous Findings That The Presence Of Noise Affects The Performance Of Emotion Recognition Models? Does This Effect Vary Based On The Type Of The Added Noise?",
      "text": "We first assess the performance of the model on the original IEMOCAP data and find that the traditional model obtains a performance of 0.67 UAR on the activation and 0.59 UAR on the valence task. On the other hand, the end-to-end model obtains a performance of 0.73 on activation and 0.64 on the valence task. We hypothesize that the wav2vec2 model when including all kinds of noises (see Table  5 .2).\n\nWe specifically want to point out how the inclusion of all noises in the test conditions changes the observed model performance. Primarily, the models on an average seem to do 20% worse than they would if we only consider noises that do not alter human perception. We note the discrepancy between the results of the two noise addition scenarios and that results should be described with respect to the perceptual effects of noise, if noise augmentation is used.",
      "page_start": 92,
      "page_end": 92
    },
    {
      "section_name": "Rq3A: Does Dataset Augmentation Help Improve The Robustness Of Emotion Recognition Models To Unseen Noise?",
      "text": "We first report results for only perception-retaining noises. When the training datasets are augmented with noise, we observe an average performance drop of 26% and 10% for matched noise conditions when using the traditional and the end-to-end deep learning model, respectively. For the mismatched noise conditions, we observe an average performance drop of 31% and 16% for the traditional and end-to-end deep learning models, respectively.\n\nBoth models see improved performance when the training dataset is augmented with continuous background noise in the matched noise setting. We find that data augmentation improves performance on mismatched noisy test data over a baseline system trained only on the clean IEMOCAP data. For example, the end-to-end model tested on environmental noise-augmented dataset (as compared to traditional deep learning model), reduces the performance drop to nearly zero. This improvement is particularly pronounced (an increase of 22% as compared to when trained on the clean partition) when the environmental noise is introduced at the start of the utterance (e.g., when the test set is introduced with nature-based noises at the start of the utterance and the train set is introduced with human and interior noises at the start of the utterance). We speculate that the network learns to assign different weights to the start and end of the utterance to account for the initial noise.\n\nHowever, we find that in both matched and mismatched conditions, it is hard to handle utterances contaminated with reverberation, a common use case, even when the training set is augmented with other types of noise. We find that this improvement in performance is even more reduced when using the wav2vec model, alluding to the model's fragility towards data integrity/structural changes. This can be because reverberation adds a continuous human speech signal in the background delayed by a small period of time. None of the other kind of noise types have speech in them, and hence augmentation doesn't aid the model to learn robustness to this kind of noise.\n\nFinally, we investigate the differences in model performance when we use all types of noise vs. those that are perception-retaining. Specifically, we focus on the perception-altering noises because samples augmented with noises in this category no longer have a known ground truth. We inquire as to whether the use of samples that alter perception may lead to the appearance of model performance improvement (note: appearance because the samples now have uncertain ground truth). To maintain equivalence, we ensure that the training and validation dataset sizes are equal even when they are augmented with more noise conditions. We observe that many cases of performance improvement occur when the noises include those that are perception-altering (see \"Al noises\" in Table  5 .2). We observe a difference of 12% to 25% between the numbers that we obtain for the perception-retaining noises vs.\n\nwhen not distinguishing between the two noise categories. This supports our claim that the choice between types of noises used for data augmentation during model training and performance evaluation affects the empirical observations and should be carefully considered.\n\nWe hypothesize that this improvement in performance may be due to the inherent nature of noises that change emotion perception, if they are perceptible enough to change emotion perception, then they may stand out enough that the model can adequately learn to separate them out and improve its prediction towards the original ground truth annotation. However, if the noise alteration truly does change perception, then the model is learning to ignore this natural human perceptual phenomenon. This may have negative consequences during model deployment.",
      "page_start": 94,
      "page_end": 95
    },
    {
      "section_name": "Rq3B: Does Sample Denoising Help Improve The Robustness Of Emotion Recognition Models To Unseen Noise?",
      "text": "In the matched training testing condition, we find that the traditional deep learning model has an average performance of 0.57 across all the datasets and testing setups, while the end-to-end models do substantially better at 0.61 UAR. See Table  5 .3 for details.\n\nIn the mismatched training testing condition, we find that for the traditional deep learning model, adding a denoising component to the models leads to a significant improvement when the original SNR is high (e.g., after continuous noise introduction the SNR decreases only by 10dB). In this case, we see an average improvement of 23% ¬± 3% across all environmental noise categories, compared to when there is no denoising or augmentation performed.\n\nHowever, when the SNR decreases by 20dB, we observe a decline in performance when using the noise suppression algorithms. We believe that this decline in performance is reflective of the mismatch in goals: the goal of noise suppression is to maintain, or improve, the comprehensibility of the speech itself, not necessarily highlight emotion. As a result, it may end up masking emotional information, as discussed in  [145] .\n\nWe further show that the addition of a denoising component does not significantly improve performance in the presence of signal distortion noise (other than reverberation) as compared to the presence of environmental noise (noise addition). For example, when samples were faded in or out or segments were sped up, the performance is significantly lower (-28%) than when tested on a clean test set. However, we did see an improvement in the performance for unseen reverberation contaminated samples as compared to data augmentation (an average of +36%). Finally, we observe a general trend of increase in emotion recognition performance for the combined dataset (noisy and non-noisy samples), as compared to when the model is trained on the clean training set, which supports the findings from previous dataset augmentation research  [10] .\n\nFor the end-to-end deep learning model, we use the noise-robust version. We find that the model is effective at countering environmental noise when trained on a dataset augmented with environmental noise, even in the mismatched condition. The performance is equivalent to the model evaluated on the clean data. We further delve into the amount of noise augmentation needed to achieve this equivalency. We consider all of the original training data. We augment a percentage of the training data, starting by augmenting a random sample of 10% with perception-retaining noise and increasing by 10% each time.\n\nWe find that we obtain equivalency after augmenting with only 30% of the training data.\n\nWe compare this compares to the traditional model, in which all of the training data are noise-augmented and we still do not see equivalency.\n\nWe separately consider the signal distortion noise samples. These were not part of the training of the wav2vec2-Large-robust model. However, this model only sees a 6% loss in performance, where the traditional robust model saw a 20% loss in performance.\n\nHowever, as discussed in the original traditional model, the end-to-end noise-robust model also fails on reverberation-based contamination even when trained on a similarly augmented dataset (note that the denoised traditional model could effectively handle reverberation). We believe that this may be because the wav2vec model is trained on continuous speech and relies on the underlying linguistic structure of speech. However, in reverberation, there is an implicit doubling of the underlying information, which is at odds with how this model was trained. This may explain why it is not able to compensate for this type of signal manipulation.\n\nNext, we analyze whether the perception category of noise used for data augmentation of the samples, in both, the train and test dataset influences the reported results for noise-robust model improvement. We find that there is a significant difference in performance when the testing dataset is augmented with any kind of noise vs. when augmented with perceptionretaining noise. Specifically, we observe that the maximal gains in performance when testing on matched noisy conditions are for samples for which we do not know whether or not the ground truth holds (i.e., both noise categories). For example, when using the noise robust traditional deep learning model, where the test and train dataset is augmented with any type of noises, we observe a performance improvement, as compared to that using a clean train dataset, of 12%. Similarly for noise robust end to end models, the performance improvement difference when using all noises vs. only perception retaining ones is 15% for activation and 13% for valence. Again, this is a problem when we think about deploying models in the real world because although the perception of these emotion expressions may change, we are assuming that the system should think of these perception labels as rigid and unchanging.",
      "page_start": 96,
      "page_end": 98
    },
    {
      "section_name": "Rq4:",
      "text": "How does the robustness of a model to attacks compare when we are using test samples that with are augmented with perception-retaining noise vs. samples that are augmented with all types of noise, regardless of their effect on perception?\n\nIn this section, we aim to show the effects of noise augmentation in general and specifically highlight noise categories that do not alter human perception. We will show that if we are not careful with the selection of our noise types, moving from noise that we know not to alter perception to noise that may, the resulting noise sources can not only impact the brittleness of models, but also lead to inaccurate evaluation metrics. We also specifically report robustness performance when using reverberation-based contamination, as we observed that it is the most likely noise category to degrade the robustness of the model. We allow the decision boundary attack algorithm a maximum of five queries to create a noise augmented sample that will change the model output. We find that if the attacker is also given perception-altering noises, compared to perception-retaining, it can more effectively corrupt the sample. It achieves an increase in success rate from 35% (only perception-retaining) to 48.5% (all noise categories). This increase in the success rate when perception-altering noises are included implies that the model does not remain robust when the effects of noise on human perception are not considered.\n\nWe next consider the type of noise (environmental vs. signal manipulation). We find that the success rate of flipping a model's output is 18% for noises belonging to the environmental category, which is generally a category of perception-retaining noise. The success rate of flipping a model's output is 37% for all noises belonging to the signal manipulation category.\n\nWhen we constrain our possible noise choices to perception-retaining signal manipulations, we see that the success rate of the intentional misclassification algorithm drops to 24%. On the other hand, we observe that when we also consider the signal manipulations that are perception-altering, the success rate of flipping a model output is 39%. See Table  5 .4 for more details.\n\nWe previously discussed the fragility of end-to-end models towards reverberation-based noise contamination, noise that is perception-retaining for human evaluators. Here, we specifically run an experiment to use only that particular noise category for the model fragility testing. If the attacker knows that the model is susceptible to reverberation-based prediction changes, the intentional misclassification algorithm can land on an optimal set of room and reverberation parameters in a maximum of five queries to be able to produce a flipped output for that particular sample. It achieves a success rate of 24%, compared to 12% for other perception-retaining noises. The traditional noise-robust deep learning model is even more challenged, compared to the end-to-end model. The number of queries required to flip the output is three, vs. five for the end-to-end model, suggesting that it is less robust. This empirical evaluation is performed primarily to demonstrate how such noise inclusions can not only invalidate the ground truth but also lead to inaccurate and fragile benchmarking and evaluation of adversarial efficiency and robustness.",
      "page_start": 98,
      "page_end": 99
    },
    {
      "section_name": "Rq5: What Are The Recommended Practices For Speech Emotion Dataset Augmentation And Model Deployment?",
      "text": "We propose a set of recommendations, for both augmentation and deployment of emotion recognition models in the wild, that are grounded in human perception. For augmentation, we suggest that:\n\n1. Environmental noise should be added to datasets to improve generalizability to varied noise conditions, whether using denoising, augmentation, or a combination of both.\n\n2. It is good to augment datasets by fading the loudness of the segments, dropping letters or words, and speeding up small (no more than 25% of the total sample length) segments of the complete sound samples in the dataset. But it is important to note that these augmented samples should not be passed through the denoising component as the denoised version loses emotion information.\n\n3. One should not change the speed of the entire utterance more than 5% and should not add intentional pauses or any background noises that elicit emotion behavior, e.g., sobs or laughter.\n\nRegarding deployment, we suggest that:\n\n1. Noisy starts and ends of utterances can be handled by augmentation, hence, if the training set included these augmentations, there should be no issue for deployed emotion recognition systems.\n\n2. Reverberation is hard to handle for even augmented emotion recognition models.\n\nHence, the samples must either be cleaned to remove the reverberation effect, or must be identified as low confidence for classification.\n\n3. Deploy complementary models that identify the presence of noise that would change a human's perception.",
      "page_start": 99,
      "page_end": 100
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we study how the presence of real world noise, environmental or signal distortion, affects human emotion perception. We identify noise sources that do not affect human perception, such that they can be confidently used for data augmentation. We look at the change in performance of the models that are trained on the original IEMOCAP dataset, but tested on noisy samples and if augmentation of the training set leads to an improvement in performance. We conclude that, unlike humans, machine learning models are extremely brittle to the introduction of many kinds of noise. While the performance of the machine learning model on noisy samples is aided from augmentation, the performance is still significantly lower when the noise in the train and test environments does not match. In this chapter, we demonstrate fragility of the emotion recognition systems and valid methods to augment the datasets, which is a critical concern in real world deployment.",
      "page_start": 100,
      "page_end": 101
    },
    {
      "section_name": "Chapter Vi",
      "text": "",
      "page_start": 101,
      "page_end": 101
    },
    {
      "section_name": "Context In Crowdsourcing Emotion Annotations",
      "text": "",
      "page_start": 101,
      "page_end": 101
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "Emotion recognition algorithms rely on data annotated with high quality labels. However, emotion expression and perception are inherently subjective. There is generally not a single annotation that can be unambiguously declared \"correct\". As a result, annotations are colored by the manner in which they were collected. In this chapter, we conduct crowdsourcing experiments to investigate this impact on both the annotations themselves and on the performance of these algorithms. We focus on one critical question: the effect of context. We present a new emotion dataset, Multimodal Stressed Emotion (MuSE), and annotate the dataset using two conditions: randomized, in which annotators are presented with clips in random order, and contextualized, in which annotators are presented with clips in order. We find that contextual labeling schemes result in annotations that are more similar to a speaker's own self-reported labels and that labels generated from randomized schemes are most easily predictable by automated systems.",
      "page_start": 102,
      "page_end": 102
    },
    {
      "section_name": "Introduction",
      "text": "Emotion technologies, both recognition and synthesis, are heavily dependent on having reliably annotated emotional data, annotations that describe the observed emotional display.\n\nThe hope is often that these annotations capture the speaker's true underlying state. Yet, in practice, this true felt sense emotion is unknown, and researchers must resort to manual labeling of data. The hope is that these manual labels are sufficiently \"correct\" to enable the training and evaluation of emotion technologies. One method of ensuring quality labels has been to require the participation of expert raters. However, it can be both expensive and time consuming to hire expert raters. More recently, researchers have embraced crowdsourcing services (e.g., Amazon Mechanical Turk) to efficiently collect annotations from non-expert workers in a cost-effective and timely manner  [201] . Once collected, annotations from non-expert workers are aggregated to form ground-truth labels that are used for training and evaluating automated systems. However, the method through which these annotations are collected can profoundly impact the behavior of the annotators. In this chapter, we study how the setup of a crowdsourcing task can influence both the collected emotion labels as well as the performance of classifiers trained using these labels.\n\nThe effective use of crowdsourcing for collecting reliable emotion labels has been an active research topic. Burmania et al. investigated the trade-off between the number of annotators and underlying reliability of the annotations  [33] . Other work has looked at quality-control techniques to improve the reliability of annotations. For example, Soleymani et al. used qualification tests to filter out spammers and retain high-quality annotators  [201] .\n\nBurmania et al. investigated the use of gold-standard samples to monitor annotators' reliability and fatigue  [35] .\n\nHowever, variability also results from context, relevant past information that provides cues as to how to interpret an emotional display. Context, such as tone, words, expressions can affect how individuals perceive emotion  [129] . Context is also implicitly included in the labeling schemes of many of the most common emotion datasets (e.g., IEMOCAP  [36]  and MSP-Improv  [38] ) because annotators rate each utterance (or time period) in order.\n\nThat means that annotators are influenced by information that they recently observed  [233] .\n\nHowever, emotion recognition systems are often trained over single utterances  [10, 3, 157, 190] , leading to a mismatch in the information available to annotators and to classification systems.\n\nIn this work, we study the difference between annotations obtained for audio clips when emotional displays are presented to annotators with context and when presented randomly. In both cases, annotators are affected by the emotion displays that they have recently observed  [180, 188] . However, only in the contextual presentation there is also a cohesive story. We investigate the following research questions:\n\n‚Ä¢ Q1: Is there a significant difference between annotations obtained from random and contextual presentations?\n\n‚Ä¢ Q2: Are annotations obtained from contextual presentations more similar to a speaker's own self-reported labels than those from random presentations?\n\n‚Ä¢ Q3: Is there a significant difference between the inter-rater agreements obtained from random and contextual presentations?\n\n‚Ä¢ Q4: How does the performance of an emotion recognition system, operating on single utterances, vary given annotations obtained from random and contextual presentations?\n\n‚Ä¢ Q5: How does the performance gain of an emotion recognition system operating across multiple utterances vary given different amounts of context (defined as number of prior utterances) and labels obtained from random and contextual presentations?\n\nThe findings from this work will provide insight into performance implications of emotion recognition system given mismatches between the amount of context provided to the annotators generating the labels and the ultimate classification system.\n\nThe main aim of the work was to annotate the dataset described in the previous chapter.\n\nFor all further experiments, we use the utterances created from the MuSE dataset.",
      "page_start": 102,
      "page_end": 104
    },
    {
      "section_name": "Crowdsourcing",
      "text": "We posted our experiments as Human Intelligence Tasks (HITs) on Amazon Mechanical Turk. HITs were defined as sets of utterances in either the contextual or random presentation condition. In each condition, workers were presented with a single utterances and were asked to annotate the activation and valence values of that utterance using Self Assessment Manikins  [30] . Once completed, the worker was presented with a new HIT and could not go back to revise a previous estimate of emotion. This annotation strategy is different than the one deployed in  [48] ,where the workers could go back and re-evaluate utterances.\n\nIn the randomized experiment, each HIT is an utterance from any section, by any speaker, from any session and all HITs appear in random order. So, a worker might see the first HIT as Utterance 10 from Section 3 of Subject 4's stressed recording and see the second HIT as Utterance 1 from Section 5 of Subject 10's non-stressed recording. This setup ensured that the workers couldn't condition to any speaker's specific style or contextual information.\n\nIn the contextual experiment, we posted each HIT as a collection of ordered utterances from a section of a particular subject's recording. Because each section's question was designed to elicit a particular emotion, we still posted the HITs in a random order over sections from all subjects. This prevented workers from conditioning to the speaking style of an individual participant. For example, a worker might see the first HIT as Utterance 1...N from Section 3 of Subject 4's stressed recording and see the second HIT as Utterance 1...M from Section 5 of Subject 10's non-stressed recording where N, M are the number of utterances in those sections respectively. We recruited from a population of workers in the United States who are native English speakers, to reduce the impact of cultural variability. We ensured that each worker had > 98% approval rating and number of HITs approved as > 500. We ensured that all workers understood the meaning of activation and valence using a qualification task that asked workers to rank emotion content. The workers were asked to select, given two clips, which clip had the higher valence and which had the higher activation. The options were chosen",
      "page_start": 105,
      "page_end": 105
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 105,
      "page_end": 105
    },
    {
      "section_name": "Rq1: Differences In Obtained Annotations",
      "text": "Hypothesis: Human annotations collected through randomized labeling are significantly different from those collected through contextualized labeling. Prior work has shown context effects emotion perception  [233] , even when observers are explicitly asked not to take it under consideration  [165, 43] . Hence, we believe that context provided by previous utterances would lead to a change in perception of a particular utterance. Tables 6.1 and 6.2 (sets of significantly different means are bolded (ùë°-test, ùëù < 0.01)) show the mean activation and valence, for the random and contextualized labeling schemes, grouped by condition and question, respectively. Table  6 .1 shows that, for non-stress conditions, the mean of the activation ratings obtained through contextual labeling is significantly higher than that obtained through random labeling. The table also shows that, for both stress and non-stress conditions, the valence means obtained through contextual labeling are significantly higher than those obtained through random labeling. Table  6 .2 shows that, although the mean valence and activation values were consistently different for the labelling schemes across all emotion elicitation techniques, the differences were significant in some elicitation techniques and not in others. Hypothesis: Annotations of outside observers are more similar to self-annotations in the contextual case, compared to the randomized case. Path models  [17]  suggest that subjective voice variation, from the established mental baseline accounts for much of the variance in emotion inference. Hence, emotion inference is aided with more cues about the speech patterns that are more readily provided through context. Figure  6 .1 shows the absolute differences between the mean crowdsourced labels (valence and activation, each for random and contextual schemes) and self-reported scores as a function of utterance position. The figure shows that contextual labels have consistently lower absolute differences, compared to self-reported labels, than the random labels. A paired ùë°-test shows that these differences between the contextual and random labels are significant (ùëù < 0.01) for both valence and activation.\n\nOur results suggest that crowdsourced emotion labels collected with access to contextual information are closer to self-reported emotion labels. Our results further suggest that these differences are consistent across recording conditions (Table  6 .3) and emotion elicitation questions ( Table  6 .4, sets of significantly different means are bolded, ùë°-test, ùëù < 0.01).",
      "page_start": 107,
      "page_end": 108
    },
    {
      "section_name": "Rq3: Inter-Annotator Agreement",
      "text": "Hypothesis: Individual annotators differ in annotation similarity in the contextual significantly different due to context-based conditioning. However, the conditioning may not impact the labels consistently across all workers, which may lead to lower inter-annotator agreement values. This suggests that it may be beneficial to consider the distribution of annotations as ground-truth, rather than averaging labels, which presumes that the impact of conditioning is consistent across all workers  [239] .",
      "page_start": 108,
      "page_end": 108
    },
    {
      "section_name": "Rq4: Non-Contextual Annotations And Static Classifiers",
      "text": "Hypothesis: A static classifier will perform better when trained and evaluated using labels annotated with a randomized presentation, compared to a contextualized presentation.\n\nPrior studies have shown that it is easier to classify data with less target variation  [141]  and matched classifier input, which in our case is labels obtained from the random labelling presentation (the classifier processes single utterances at a time, no context).\n\nWe test this hypothesis by training and evaluating classifiers for the four possible setups:\n\n{ùëüùëéùëõùëëùëúùëö, ùëêùëúùëõùë°ùëíùë•ùë°ùë¢ùëéùëô} x {ùë£ùëéùëôùëíùëõùëêùëí, ùëéùëêùë°ùëñùë£ùëéùë°ùëñùëúùëõ}. The classifier is described in Section 6.3.\n\nWe find that the RMSEs are lower for the contextual labels in the case of activation (0.91 vs. 1.00) while the errors are lower for the random labels in the case of valence (1.13 vs.\n\n1.20). Using a paired ùë°-test, we find that the differences in errors are significant in the case of valence but not activation. These findings suggest that classification performance is impacted by the labelling methodology, but that this effect may depend on emotion dimension.\n\nPrior work has demonstrated the importance of considering long-term context when predicting valence (the same effect has not been shown in activation)  [116] . The contextual annotations provided the annotators with this information, but the classifier could not take advantage of this effect. This mismatch may have contributed to the relatively lowered performance of the valence classifier, compared to the activation classifier.",
      "page_start": 110,
      "page_end": 110
    },
    {
      "section_name": "Rq5: Contextualized Annotations And Long-Context Classifiers",
      "text": "Hypothesis: We anticipate that systems trained on contextualized labels will see greater increases in performance as the amount of provided context increases. This finding would support results in the literature regarding the ordinal nature of emotion perception  [233]  and previous works in emotion recognition that have demonstrated that context can influence the performance of emotion classifiers  [116] . The classifier is described in Section 6.3. We test this hypothesis by using the contextual annotations in one classifier and the non-contextual (random) annotations for the other classifier. We select a subset of utterances in each section that have at least five consecutive utterances before them (59% of the original data). The initial classifier is trained without temporal context (but with the contextualized labels). We incrementally increase the number of past utterances (from zero to five). We run this for every task combination and report the results in Table  6 .5. Table  6 .5 shows the performance gains after incrementally adding the past utterance, relative to the baseline performance. The addition of past utterances improves the performance over baseline for all setups. Where using contextual labels, however, the performance gains are generally higher than the gains obtained after using random labels. Our results suggest that it is necessary to consider the mismatch the amount of context provided to the annotators generating the labels and the ultimate classification system.",
      "page_start": 110,
      "page_end": 111
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we showed that the amount of context provided to annotators when assigning emotion labels affects both the annotations themselves and the performance of classifiers using these annotations. We also studied the implications of a mismatch between annotation context and classifier context on classifier performance.",
      "page_start": 112,
      "page_end": 112
    },
    {
      "section_name": "Chapter Vii",
      "text": "",
      "page_start": 112,
      "page_end": 112
    },
    {
      "section_name": "Controlling For Confounders In Emotion Recognition",
      "text": "",
      "page_start": 112,
      "page_end": 112
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "Various psychological factors affect how individuals express emotions. Yet, when we collect data intended for use in building emotion recognition systems, we often try to do so by creating paradigms that are designed just with a focus on eliciting emotional behavior.\n\nAlgorithms trained with these types of data are unlikely to function outside of controlled environments because our emotions naturally change as a function of these other factors. In this work, we study how the multimodal expressions of emotion change when an individual is under varying levels of stress. We hypothesize that stress produces modulations that can hide the true underlying emotions of individuals and that we can make emotion recognition algorithms more generalizable by controlling for variations in stress. To this end, we use adversarial networks to decorrelate stress modulations from emotion representations. We study how stress alters acoustic and lexical emotional predictions, paying special attention to how modulations due to stress affect the transferability of learned emotion recognition models across domains. Our results show that stress is indeed encoded in trained emotion classifiers and that this encoding varies across levels of emotions and across the lexical and acoustic modalities. Our results also show that emotion recognition models that control for stress during training have better generalizability when applied to new domains, compared to models that do not control for stress during training. We conclude that is is necessary to consider the effect of extraneous psychological factors when building and testing emotion recognition models.",
      "page_start": 113,
      "page_end": 114
    },
    {
      "section_name": "Introduction",
      "text": "Many extraneous psychological factors influence how individuals express and perceive emotions  [171] . However, most emotion recognition algorithms, rely on data collected in controlled laboratory environments (e.g.,  [36, 38] ) where influences from such factors are either not present, or kept constant. The performance of emotion recognition algorithms is likely to vary when applied to data where these external psychological factors are present.\n\nIn this work, we study how an extraneous psychological factor, stress, affects multimodal (acoustic+lexical) emotion classifiers. Stress can affect how individuals produce and perceive emotion  [171] . Yet, the effect of stress levels on the performance of state-of-the-art emotion recognition systems has not been explored.\n\nExtraneous psychological factors can act as confounding factors, variables that influence both the output (e.g., emotion) and the input (e.g., acoustic and lexical features). Not controlling for confounding variables when training emotion classifiers can cause the classifiers to learn unintentional associations between the variables, associations that might not replicate in real world scenarios. For instance, consider a dataset where all the \"sad\" samples were unintentionally recorded from individuals who were experiencing stress at the time of recording. Not taking special care when building the models could cause a trained classifier to erroneously associate experiencing stress with being sad. In this work, we study how stress alters the performance of trained emotional classifiers in the context of neural networks. We then see how performance is affected when tested on samples out of domain, when we explicitly impede the network from learning such associations.\n\nPrevious research showed that controlling for confounding variables when training emotion recognition classifiers results in more robust models when compared to models trained without controlling for the same confounding variables. For instance, Abdelwahab et al.  [4]  and Gideon et al.  [83]  showed that controlling for domain (i.e., data source), as a confounding factor, when training emotion recognition models results in improved cross-corpus generalization performance when compared to performance of models that were trained without controlling for domain as a confounding factor. Most of the above mentioned methods rely on samples obtained from the target domain to extract representations that retain information only about emotion and not domains. Our goal is to go beyond studying the effects of variations due to domain and background noise on the robustness of trained emotion recognition models, and instead focus on how stress affects the learned acoustic and lexical emotional representations. Unlike the commonly used methods for learning domain invariance, we aim to accomplish generalizing person specific behavior by proactively \"unlearning\" the modulations due to the presence of stress while still retaining emotion information in representations.\n\nIn particular, we seek to answer the following questions:\n\n1. Can we recognize stress given representations trained solely for recognizing emotion?\n\nIs the stress recognition performance similar across the lexical and acoustic modalities?\n\n2. Can we completely decorrelate emotion representations from stress representations?\n\nIf so, how does this decorrelation impact the performance of emotion classifiers?  To the best of our knowledge, this is the first work that studies the interplay between emotion and stress in the context of automatic emotion recognition and representation learning.",
      "page_start": 114,
      "page_end": 116
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use three datasets to study the effect of stress on emotion recognition: (1) Multimodal Stressed Emotion (MuSE) dataset  [105] ; (2) Interactive Emotional Dyadic MOtion Capture (IEMOCAP) dataset  [36] ; and (3) MSP-Improv dataset  [38] . We use the acoustic and lexical features, MFBs and word2vec respectively, as defined in Section 3.3. We then describe the network architecture and the training recipe of the two emotion recognition models, one that controls for stress as a confound and one that does not.",
      "page_start": 116,
      "page_end": 116
    },
    {
      "section_name": "Architecture",
      "text": "The network consists of three components (Figure  7",
      "page_start": 117,
      "page_end": 117
    },
    {
      "section_name": "Stress-Invariance.",
      "text": "The network is trained to unlearn stress. We achieve this goal using a Gradient Reversal Layer (GRL)  [77] . The use of GRLs is a common approach that can be used to train networks that are invariant to specific properties  [153, 196, 4, 79, 150, 67] .\n\nDuring the backward pass of the training phase, the GRL multiplies the backpropagated gradients by -ùúÜ. During the forward pass, the GRL acts as an identity function. To make the network invariant to stress, we place the GRL between the embedding sub-network and the stress classifier as shown in Figure  7 .1.",
      "page_start": 117,
      "page_end": 117
    },
    {
      "section_name": "Model Variations. We Use 12 Variants Of The Network Shown In",
      "text": "",
      "page_start": 117,
      "page_end": 117
    },
    {
      "section_name": "Training",
      "text": "We implement models using the Keras library  [52] . We use a weighted cross-entropy loss function for each task and learn the model parameters using the RMSProp optimizer  [214] .\n\nWe train our networks for a maximum of 50 epochs and monitor the validation loss from the emotion classifier after each epoch, stopping the training if the validation loss does not improve after five consecutive epochs. Once the training process ends, we revert the network's weights to those that achieved the lowest validation loss on the emotion classification task. For the adversarial classification model, we ensure that the chosen model yields a validation unweighted average recall (UAR) that is random (0.33) for the stress classification task. Finally, we train each setup three times with different random seeds and average the predictions over these runs to reduce variations due to random initialization.\n\nWe use validation samples for hyper-parameter selection and early stopping. The hyperparameters that we use in our search include: number of convolutional layers {3, 4}, number of convolutional kernels {2, 3}, conv. layers width {32, 64, 128}, 1D max-pooling kernel width {2}, number of GRU layers {2, 3}, GRU layers width {32}, number of dense layers {1, 2}, dense layers width {32, 64}, GRL ùúÜ {0.3, 0.6, 0.8}. For the adversarial emotion classification setups, we use the hyper-parameters that maximize the validation emotion classification performance while minimizing the validation stress classification performance.\n\nWe assess performance using UAR, given the imbalanced nature of our data  [184] .  Stress has been shown to have varying effects on both the linguistic  [32]  and paralinguistic  [171, 205]  components of communication. Previous work has also demonstrated that the lexical part of speech carries more information about valence while the para-linguistic part carries more information about activation  [116] . As a result, we expect the performance of stress classification to vary based on modality, and emotion dimension being modeled.",
      "page_start": 118,
      "page_end": 119
    },
    {
      "section_name": "Analysis",
      "text": "To test our hypothesis, we train the 12 model variants described in section 7.3.1 with five-fold speaker-independent cross-validation. We report the average across the five folds for the normal classification and the adversarial classification setups in Tables 7.1a and 7.1b for predicting activation and valence, respectively. Our results show that a network trained to only recognize emotion is generally discriminative for stress. For instance, we obtain a maximum UAR of 0.425 when using a multimodal network that was trained to only detect activation; and a maximum UAR of 0.397 when using multimodal network that was trained to only detect valence.\n\nOur results in Table  7 .1a suggest that the acoustic modality encodes information that is relevant for recognizing stress and activation. In contrast, the results show that the",
      "page_start": 119,
      "page_end": 119
    },
    {
      "section_name": "Rq2: Decorrelate Emotion And Stress Representations",
      "text": "Question: Can we decorrelate emotion representations from stress representations? How does it impact performance of emotion classifiers?\n\nHypothesis: Decorrelating the stress and emotion representations will cause a decrease in emotion classification performance on the source domain.\n\nPrevious research demonstrated that controlling for confounders during the training process can cause the performance of the main task on the same dataset to decrease  [149, 209] . For instance, Zhang et al.  [149]  showed that the performance of detecting sarcasm decreases when controlling for publication as a confounding variable in the training process, but the prediction accuracy increases on an unseen publication set. Similarly, Ganin et al.  [79]  showed that controlling for domain while training a network for detecting sentiment can result in a performance reduction on the main task. The reduction in performance on the main task, after controlling for an extraneous confounding variable, can be attributed to the removal of information that the model can use as a \"shortcut\" for achieving the main task.\n\nOur results show that (Tables 7.1a and 7.1b):\n\n‚Ä¢ Activation classification performance decreases given adversarial training. This decrease is statistically significant for the acoustic (6.382% drop in UAR) and multimodal (6.980% drop in UAR) setups.\n\n‚Ä¢ Valence classification performance decreases given adversarial training. This decrease is statistically significant for the multimodal setup (3.754% drop).\n\nThe reduction in performance in the main task after controlling for a confounding variable can also be caused by the removal of information that is equally beneficial for both detecting stress and detecting emotion. Our results in further sections , however, show that models that control for stress are better able to recognize emotion in new domains, compared to models that do not control for stress. This suggests that the process of \"unlearning\" stress does not come at the expense of the primary task of emotion detection.",
      "page_start": 120,
      "page_end": 121
    },
    {
      "section_name": "Rq3: Impact Of Stress Levels On Emotion Recognition",
      "text": "",
      "page_start": 121,
      "page_end": 121
    },
    {
      "section_name": "Question: Does The Impact On The Performance Of Emotion Classifiers Vary Given Different Levels Of Stress",
      "text": "Hypothesis: The valence and activation emotion classes (low, medium, and high) are impacted differently by stress.\n\nPrior research demonstrated that emotions produced by stressed individuals are not recognized in the same way as those by non-stressed individuals  [171] . In particular, researchers found that speech patterns of negative emotions produced by stressed individuals are more difficult to recognize than negative emotions produced by non-stressed individuals  [171] . We expect similar patterns to hold in automatic emotion recognition systems. That is, we expect the presence of stress to have a varying effect on the performance of the classifier depending on the emotion class (for valence and activation), and the amount of stress induced.\n\nTo test our hypothesis, we study how the performance of the classifier varies for each emotion class when we control for stress. We report the changes in performance, after controlling for stress, for each emotion class, grouped by stress level (low, high), in Table  7 .2.\n\nThe results in the table show:\n\n‚Ä¢ High levels of stress impact classification more strongly (3.89% and 2.41% drop in UAR for activation and valence, respectively) than low levels of stress do (1.44% and 0.31% drop in UAR for activation and valence, respectively). This is generally true for all emotion classes (valence and activation).\n\n‚Ä¢ High levels of stress have the biggest impact on mid level of activation predictions (22.03% drop in accuracy for detecting neutral activation).\n\n‚Ä¢ High levels of stress have the biggest impact on low valence predictions (8.22% drop for low valence).\n\nThe results show that stress level effects emotion recognition, for both activation and valence.\n\nThis drop in performance can be attributed to changes in the perceived emotions by the annotators. Researchers have demonstrated that stressed sentences are usually perceived by annotators to be more neutral than they were originally intended to be  [171] .",
      "page_start": 121,
      "page_end": 122
    },
    {
      "section_name": "Rq4: Decorrelation And Model Generalizability",
      "text": "Question: Does the process of decorrelating these representations (i.e., emotion and stress) aid in model generalizability?\n\nHypothesis: Removing the confounding factor stress would aid in creating models that are more generalizable across datasets.\n\nPrevious research has shown that laboratory collected datasets are too small and often fail to capture the complete distribution of the domain  [100, 142]  present in the real world.\n\nThese datasets are often plagued with unintentional correlational factors  [142, 128] . Hence we believe that removing modulations due to stress should aid the generalizability of the model to datasets, where this psychological factor is either unmeasured or the distribution is non-uniform between training and testing.\n\nTo answer if the models trained on MuSE dataset generalize better, we perform two sets of experiments: (a) self generalizability in artificially partitioned datasets with different stress distributions for evidence of concept and (b) cross-dataset generalizability.\n\nArtificially Segmented Within-Dataset Performance. We run the first set of experiments by creating partitions of data by stress level. We do this to create artificially mismatched environments between training and testing. We reserve one set to be test set (target), while keeping the other two for training and validation combined (source). This is in similar vein to partitioning created across confounding factor for UCI Bike Rentals Dataset in  [209] . To ensure speaker independent sets, we divide the training set using an 80:20 split (train and validation), ensuring no speakers overlap. We run these models ùëõ times where ùëõ is the number of speakers in test data, that are also present in train/validation data. For each run, we remove one speaker from the train/validation data and test on that speaker. We calculate the average test performance of all these runs as the performance of the model for that setup.\n\nWe report our results in Table  7 .3. When we consider low levels of stress as our target, we see that adversarial classification significantly improves performance over normal classification for multimodal setup for activation and for both, lexical and multimodal setup for valence. Considering mid levels of stress as our target, adversarial classification significantly improves performance over normal classification for all setups for activation and for both, acoustic and multimodal setup for valence. Subsequently considering high levels of stress as our target, adversarial classification significantly improves performance over normal classification for acoustic setups for activation and for all setups for valence.\n\nCross-dataset Performance. Now that we have evidence for concept for artificially ‚Ä¢ Valence: There is a significant increase in performance using acoustic setup (0.376 vs 0.401) and multimodal setup (0.431 vs 0.472) of adversarial model when tested on IEMOCAP. We see no significant difference in performance when testing on MSP-Improv using adversarial classification model .\n\nBased on these results, we understand that removal of a psychological confounding factor, stress, generally aids in the generalizability of the model on completely unseen data, where the distribution of this confounding factor is unknown.",
      "page_start": 122,
      "page_end": 125
    },
    {
      "section_name": "Rq5: Spontaneity As Confounding Factors",
      "text": "Question: Can we proactively remove other types of confounders to improve cross-dataset performance?\n\nHypothesis: Removing the confounding factor of spontaneity in IEMOCAP will improve cross-dataset performance.\n\nWe observed in the last question that \"unlearning\" the confounder stress can aid generalizability. Now, we want to see if the same method can be used to make models trained using other datasets more reliable to change in target data distribution. We hypothesize that decorrelating the effect of spontaneity on emotion representation will lead to models that generalize better. This is because, as shown in  [146] , the emotional content expression is different in scripted vs spontaneous speech, and hence should modulate the emotion representation in trained model. To this end, we use the IEMOCAP dataset which has utterances that come from sessions that are both scripted and improvised. We do not use MSP-Improv for similar analysis here, because the scripted sessions, by corpus design, have limited lexical content and hence wouldn't cover enough input representation space for generalizability. We train the same 12 model variants described in section 7.3.1 replacing the adversarial stress classifier sub-component with adversarial spontaneity classifier for this analysis.\n\nWe report our results in Table  7 .5. We compare the performance of the adversarial and normal models:\n\n‚Ä¢ Activation: There is a significant increase in performance in lexical (0.401 vs 0.425) and multimodal setup (0.433 vs 0.467) when the adversarial model is tested on MuSE dataset. We see no significant difference in performance when the adversarially trained model is tested on MSP-Improv.\n\n‚Ä¢ Valence: There is a significant increase in the performance using all setups of the adversarial classification model when tested on MuSE. We observe a significant increase in the performance in the acoustic setup of the adversarial model (0.410 vs 0.438) when tested on MSP-Improv.\n\nWe see that the removal of modulations due to the data collection methodology improves generalizability for many cross-dataset cases. This suggests that this method can be extended to train stabler models by explicitly accounting for confounding variables in limited amounts of training data.",
      "page_start": 125,
      "page_end": 126
    },
    {
      "section_name": "Rq6: Lexical Patterns In Samples That Benefit From Adversarial Training",
      "text": "Question: Are there identifiable lexical patterns in samples that are especially successfully classified by the adversarially trained model for emotion classification? Our results in questions 4 and 5 of section 7.4.4 demonstrate that decorrelating the representations from modulations due to confounding factors can positively affect the classification performance of our trained emotion recognition models when applied to datasets whose properties differ from the data on which the models were trained.\n\nIn this section, we aim to understand what properties of input features in a given sample correlate with the probability of successful classification in trained emotion recognition models due to decorrelation of such modulations. Understanding the relationship between the properties of the input features and the likelihood of success can help us identify data points that are more likely to be correctly classified using adversarially trained models. This can help us identify samples in an unseen dataset for which we can trust the classification label obtained from the adversarial model as compared to the normal classification model. We analyze this relationship using word tokens, which are low-dimensional and human-interpretable.",
      "page_start": 126,
      "page_end": 127
    },
    {
      "section_name": "Adjusted Probability Of Success",
      "text": "We study the correlation between the lexical patterns of data samples and the probability that those samples are correctly classified. We focus on improvements in classifaction, moving from the normal model to the adversarial model. This allows us to focus on improvement and mitigates the challenge that certain samples may just be particularly easy or hard to classify. We define probability of success for a sample as the ùëÉ ùê¥,ùë† (ùëÜùë¢ùëêùëêùëíùë†ùë†)\n\nwhere A can either be a normal (normal) classification model or an adversarial classification model (adv) and ùë† refers to the index of a particular sample. We calculate ùëÉ ùê¥ (ùëÜùë¢ùëêùëêùëíùë†ùë†)\n\nas the ratio of the number of times a model correctly classifies a given sample to the total number of fifteen runs. If a sample is correctly classified across all runs by adversarial model, the ùëÉ ùëéùëëùë£ (ùëÜùë¢ùëêùëêùëíùë†ùë†) for that sample would be 1. But we want to concentrate on the gain in performance of using adversarial over normal classification. It might be the case that this sample is correctly classified across all runs by normal classification model as well, the ùëÉ ùëõùëúùëüùëöùëéùëô (ùëÜùë¢ùëêùëêùëíùë†ùë†) for that sample would be 1. In this case, we do not see any betterment as a result of using adversarial training paradigm. To mitigate the above limitation, we define adjusted probability of success in the following manner: We define adjusted probability of success (APS) for sample ùë† as: ùëÉ ùëéùëëùë£,ùë† (ùëÜùë¢ùëêùëêùëíùë†ùë†) -ùëÉ ùëõùëúùëüùëöùëé,ùë†ùëô (ùëÜùë¢ùëêùëêùëíùë†ùë†). When the APS is greater than 0, the sample is more accurately classified using the adversarial model. When the APS is less than zero, it is more accurately classifed using the normal model.",
      "page_start": 127,
      "page_end": 128
    },
    {
      "section_name": "Features",
      "text": "Our goal is to correlate APS with interpretable lexical features. We use the Linguistic Inquiry and Word Count (LIWC)  [174]  tool. LIWC assigns a predefined category to a word based on its association with social, affective and cognitive process. These categories have been shown to be highly predictive of both emotion  [114] , spontatenity  [53]  and stress  [226] .\n\nWe form a twelve length feature vector for each utterance by counting the number of words that fall into each of the nine LIWC categories (adverb, pronoun, social process, negation, positive and negative emotion, insight, tentative and certainty). We normalize the feature vector by how many words in the utterance. We augment this feature vector to include: (1) fillers (e.g., \"uhh\"), hesitation (e.g., \"like\"), and discourse markers (e.g., \"so\")   2 ) content rate, defined as the number of words per unit length of time. The final feature vector comprises of all the above mentioned categories.",
      "page_start": 128,
      "page_end": 129
    },
    {
      "section_name": "Discussion",
      "text": "Decorrelating Stress. We report the Pearson correlation coefficient and the resulting Benjamini-Hochberg adjusted  [25]  ùëù-values that we obtain between each feature in the vector, and the APS for each sample. We perform this assessment for both the activation and valence normal and adversarial lexical models. We focus on the cross-dataset case in which the models were trained on MuSE and tested on IEMOCAP (in Table  7 .6). A large positive correlation between a category and APS implies that samples with larger numbers of words in a given category are likely to be classified correctly more often given the adversarial model versus the normal model.\n\n‚Ä¢ Activation recognition: APS is significantly positively correlated with the presence of words that relate to: Adverb (0.217), pronoun (0.165), positive emotion (0.154), certainity (0.138), fillers (0.154), discourse markers (0.141), and content rate (0.196).\n\n‚Ä¢ Valence recognition: APS is significantly positively correlated with the presence of words that relate to: Adverb (0.177), negative emotion (0.143), fillers (0.182), content rate (0.178).\n\nThis finding is consistent with previous research  [151] , where the authors have shown that there is often an increase in usage of function words and intensifiers in stressed conditions.\n\nSo, for example a sentence \"I am really really sad about losing my pen\" would have more likelihood of being correctly classified by the adversarial model compared to the normal emotion classification model. Hence, we can hypothesise that an increase in the likelihood of correct classification of samples containing these intensifiers occurs due to reduced weightage of adverbs in adversarial training paradigm.\n\nThere are fewer significant categories for valence than for activation. This is consistent with the results in Table  7 .4 for the lexical modality. Although we see a significant correlation between filler words and APS for activation classification  [65] , we do not observe the same for discourse markers and presence of social process words. The absence of significance in these cases implies that though these values are markers of stress, the normal classifier is still able to learn reliable representations invariant of stress for predicting the correct target label, resulting in negligible impact on classification performance when decorrelating the representations.\n\nDecorrelating Spontatenity. We do a similar analysis for analyzing lexical properties of samples that were aided by decorrelating spontaneity. We report the Pearson correlation coefficient and the resulting Benjamini-Hochberg adjusted  [25]  ùëù-values that we obtain from the LIWC features and APS for both emotion axes lexical-based classification models (trained on IEMOCAP; tested on MuSE) in Table  7 .7.\n\n‚Ä¢ Activation recognition: APS is significantly positively correlated with the presence of ‚Ä¢ Valence recognition: APS is significantly positively correlated with the presence of words that relate to: Social Process (0.166), positive (0.161) and negative (0.148) emotion, certainty (0.172), and content rate (0.144).\n\nThe results suggest that there are identifiable linguistic properties of samples whose likelihood of correct classification benefits from the model trained adversarially to decorrelate spontaneity and emotion representation as compared to normal classification model. This is especially true for the use of words in the certainty category for both emotion dimensions and all hesitation categories for activation. Spontaneous speech has been shown to have more of these words in  [53] . Scripted content has been shown to have more exaggerated displays of emotion through words and facial expressions  [113] . Controlling for the weights assigned to words in positive and negative emotion categories using the adversarial model, leads to better classification of samples that are comprised of these word tokens.",
      "page_start": 129,
      "page_end": 132
    },
    {
      "section_name": "Conclusions",
      "text": "This work focused on the interplay between stress and emotion in the context of automatic emotion recognition. We first showed that the presence of stress affects the performance of emotion recognition models. We then observed that these effects vary depending on modality (acoustic or lexical) and task (activation or valence classification). We then showed how decorrelating stress modulations from emotion representations aids the generalizability of the model. Next, we showed how a similar method could be used to control for variations due to spontaneity; facilitating the generalizability of the model. Finally, we identified human interpretable lexical markers that correlate with successful generalization of the model; especially concentrating on samples that are aided by decorrelation of stress and emotion representation.\n\nOur results suggest that an extraneous psychological factor, such as stress, can significantly impact the performance of emotion recognition systems both within and across datasets.\n\nAs a result, extraneous psychological factors should be accounted for when collecting data for training emotion recognition systems, especially when being used to predict labels of data that may or may not be modulated by those same factors. We then show how proactive decorrelation of this confounder can improve generalization of the model to other dataset at time of deployment.",
      "page_start": 132,
      "page_end": 132
    },
    {
      "section_name": "Chapter Viii",
      "text": "",
      "page_start": 132,
      "page_end": 132
    },
    {
      "section_name": "Reducing Leakage Of Demographic And Membership",
      "text": "Information Through Adversarial Networks",
      "page_start": 133,
      "page_end": 133
    },
    {
      "section_name": "Motivation And Contributions",
      "text": "The rise of mobile applications and conversational agents harnessing emotion recognition capabilities has brought a crucial challenge to the forefront: the safeguarding of sensitive user information. This data, encapsulating detailed demographic aspects, is often extracted from user devices and stored on centralized servers. In worst-case scenarios, these details could be exploited without user consent or manipulated by harmful entities. Storing abstracted representations rather than raw data, while a potential solution, doesn't fully counter the problem. Indeed, these representations, intended for tasks like emotion recognition, are susceptible to inadvertent demographic leakage undermining user-defined settings (for example, to not receive gender targetted ads) and exposing sensitive variables from unimodal (textual or acoustic) or multimodal data.\n\nThe consequences of unintentional demographic leakage are significant. Continual exposure of sensitive attributes threatens the integrity of user data. Moreover, the inherent tie-in of demographics such as age, gender, and race with emotion recognition models makes these models potential reservoirs of sensitive user information-even when storing only representational data. As a result, measures to counter demographic leakage have become a and to gain access to sensitive information. A way to counter this issue in data collected by mobile or smart home applications is to generate a data representation on the device and then to transfer that representation to the server for additional processing. The benefit is that these representations can decrease leakage of content and sensitive information by partially obfuscating the actual content of the conversation  [24] . However, they still contain sensitive demographic information.\n\nThe implications of sensitive information leakage is profound: research has shown that discrimination occurs across variables of age, race, and gender in hiring, policing and credit ratings  [92] .  [2]  showed how adding random noise to aggregated dataset or individual samples can ensure defense against attacks that aim to classify sensitive information from the representation. But, previous research has shown that using additional noise can often be exploited if the adversary has access to the network used to generate anonymity  [120] .\n\nTherefore to ensure robustness, we consider a scenario of the attacker having access to the same embedding sub-network to generate the representations that will be used to train its attack network.\n\nIn this work, we focus on sensitive information encoding in the context of emotion recognition. Emotion recognition provides an important test case because emotion production varies significantly across gender and race. As a result, the outputs of emotion recognition models are often highly correlated with these secondary demographic signals  [45, 202] . We design approaches to first measure leakage and then to counteract this leakage. We measure sensitive information encoding in two ways: 1) using a sensitive information reduction metric, which we define as the incapability of an attacker to recover demographic information from representations, and 2) by determining an adversary's ability to perform membership identification  [136] , defined as the ability to determine if a given user was in a dataset from which the emotion recognition models were trained (this can be harmful if the training data are collected in a sensitive context, such as counselling or therapy). We ask the following seven questions:\n\n1. Does demographic leakage differ in umimodal and multimodal emotion recognition models?\n\n2. How does the sensitive information reduction metric change when a network is trained to not encode sensitive information?\n\n3. How does emotion recognition performance change when networks are trained to not encode sensitive information?\n\n4. How does the adversarial component's strength impact emotion recognition performance and the sensitive information reduction metric?\n\n5. Focusing on gender, how does the performance of emotion recognition change when a network is trained to not encode sensitive information?\n\n6. Does the location of the adversarial component within a network affect the sensitive information reduction metric and emotion recognition performance?\n\n7. Does the sensitive information reduction paradigm that we used to reduce encoding of gender information also help defend against other attacks such as membership identification?\n\nOur results show that representations obtained for emotion recognition can be exploited by an adversary to predict sensitive variables given unimodal information (either audio or lexical). We further show that multimodal models contain even more sensitive information, as lexical and audio each encode different aspects of demographic information. We show that we can increase the defense against this attack by adversarially training representations to be invariant to gender. The novelty of this work is two fold:  (1)  we analyze how the demographic variable encoded in a representation differs across modalities and how it can be increased using adversarial paradigm; and, (2) we obtain enhanced representations that defend against multiple sensitive information leakage or prediction attacks while still maintaining performance on emotion recognition.\n\nGiven most of the previous work on representations that aim to reduce encoded sensitive information concentrates on just lexical information, we tackle the questions that arise from desiring such preservation in multimodal representations for emotion recognition. While the primary goal of most previous works has been to avoid unintentional inference by the application itself, we concentrate on minimizing the potency of an attacker to deliberately recover sensitive attributes from an invariant representation.",
      "page_start": 133,
      "page_end": 137
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use four common emotion recognition datasets: MSP-Improv  [38] , MSP-Podcast  [143] ,\n\nInteractive Emotional Dyadic MOtion Capture (IEMOCAP) dataset  [36] , and Multimodal Stressed Emotion (MuSE) dataset  [105] . We use the acoustic and lexical features, MFBs and word2vec respectively, as defined in Section 3.3. Next, we describe the network architecture, the training recipe, and the metrics in consideration.",
      "page_start": 137,
      "page_end": 137
    },
    {
      "section_name": "Architecture",
      "text": "The objective of this system is to maximize the performance of the emotion classifier while minimizing the performance of the gender classifier (see Figure  8 .1). The main network consists of three components: (1) embedding sub-network, ùëÄùëúùëëùëíùëô(ùúÉ); (2) emotion classifier, ùúÉ ùëê and output ùë¶ ùëñ ; and (3) gender classifier, ùê∑ ùëñ (ùúÉ ùëë ùëñ ), with output, ùëè ùëñ . We then disucss how an attacker network could maliciously use this information to obtain sensitive demographic information.\n\nMain Network. The embedding sub-network uses a state-of-the-art multimodal approach in emotion recognition  [9]  in which the acoustic and lexical information are processed separately and then joined after the application of modality-specific global mean pooling.\n\nThe acoustic input stream (ùë• ùëñ (ùëé)), where ùëñ represents an input frame (40-dimensional) and ùëé represents acoustic, is processed using a set of convolution layers and Gated Recurrent Units (GRU), which are fed through a mean pooling layer to produce an acoustic representation (‚Ñé ùëé ). The lexical input (ùë• ùëñ (ùëô)), where ùëñ represents an input word (300-dimensional) and ùëô represents lexical, is passed through GRUs and pooled to obtain a lexical representation (‚Ñé ùëô ). For the multimodal setup, these two representations, (‚Ñé ùëé ) and (‚Ñé ùëô ), are concatenated (‚Ñé). The representations (‚Ñé, ‚Ñé ùëé , ‚Ñé ùëô ) are of fixed-length given acoustic and lexical inputs. The emotion classifier takes in the representation (‚Ñé, ‚Ñé ùëé , or ‚Ñé ùëô ) as input and estimates valence or activation using a set of dense layers. The gender classifier estimates the gender label (i.e., male or female) using a set of dense layers.\n\nGender-Leakage. The main network is trained to unlearn gender. To achieve this goal, we use a Gradient Reversal Layer (GRL)  [77] . GRLs are a common multi-task approach to train networks such that they are invariant to specific properties  [153, 107] . During the backward pass of the training phase, the GRL multiplies the backpropogated gradients by -ùúÜ (i.e., the strength of the adversarial component). During the forward pass, the GRL acts as an identity function. To make the network invairant to gender, we place the GRL function between the embedding sub-network and the gender classifier. We obtain gender-invariant representations using the following loss function: where ùëÅ is the number of targeted sensitive variables (here ùëÅ=1). The loss function ensures that while the output components are trained to be good predictors, the representation is trained to be maximally good for the primary task (emotion) and maximally poor for the secondary task (gender).\n\nAttacker Network. We assume that the attacker has access to a held-out dataset (either a different dataset or a section of the original dataset) with known gender labels. The attacker generates representations for this dataset using the previously described embedding subnetwork. The network then learns to predict gender labels from the generated representations using a set of dense layers. Since the parameters used to construct the representation are fixed, the attacker only acts upon its own parameters to optimize the gender prediction linear loss.\n\nThe purpose of the attacker's network is to recover gender information from representations whose labels are unknown. Though testing using a singular network isn't a guarantee of robustness of the representation to attacks that aim to predict or extract sensitive information from representations, for the scope of this chapter, we use a feed forward network, one of the powerful learning methods on a fixed size static representation. Training. We implement models using the Keras library.  [52] . We use a weighted cross-entropy loss function for each task and learn the model parameters using the RMSProp optimizer.  [214] . We train our networks for a maximum of dense layers width {32, 64}. We report the UAR performance of our models, given the imbalanced nature of our data.  [184] .",
      "page_start": 137,
      "page_end": 137
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 137,
      "page_end": 137
    },
    {
      "section_name": "Metrics",
      "text": "Performance. We define performance for emotion recognition as the ability of the model to correctly classify either activation or valence into 3 categories: low, medium, and high. We measure performance using UAR (chance is 0.33).\n\nDemographic Leakage. Leakage is defined as the ability of a trained gender classifier to predict gender from the representations which are obtained when the network is simultaneously trained to perform the primary task.\n\nDemographic Sensitive Information Reduction Metric. We define the sensitive information reduction metric as the inability of an attacker to be able to recover gender from the representations trained on a primary task. To test this, we use four phases of training.\n\n1. We train the main network on a dataset (D1), represented by the pair (ùë• ùê∑1 , ùë¶ ùê∑1 ), where ùë• is the data input while ùë¶ is the gender label. We obtain representations for this dataset (‚Ñé(ùë• ùê∑1 )).\n\n2. We consider that the attacker has access to another dataset or unused subset of the same dataset (D2) represented by the pair (ùë• ùê∑2 , ùë¶ ùê∑2 ). We generate representations ‚Ñé(ùë• ùê∑2 )\n\nfor the pairs in this dataset using the embedding sub-network of the main network.\n\n3. We train a model (ùëÄ ùëéùë°ùë° ) to predict gender labels using the representations obtained in step 2, represented as ùëÄ ùëéùë°ùë° ((‚Ñé(ùë• ùê∑2 ), ùë¶ ùê∑2 )).\n\n4. Using the model obtained previously (ùëÄ ùëéùë°ùë° ), we choose ‚Ñé(ùë• ùê∑1 ) as inputs, and measure the gender prediction capability of the attacker ùëà ùê¥ùëÖ(ùëÄ ùëéùë°ùë° ((‚Ñé(ùë• ùê∑1 ), ùë¶ ùê∑1 )). The Demographic Sensitive Information Reduction metric of an attacker is then quantitatively defined as 1 -ùëà ùê¥ùëÖ(ùëÄ ùëéùë°ùë° ).\n\nThe range of the sensitive information reduction metric goes from 0 (the attacker is always correct) to 0.5 (the attacker has a chance UAR).\n\nMembership Identification. Membership identification is the possibility of an attacker being able to recognize if a speaker belongs to the training set. We assume that the adversary can obtain samples from a speaker from the same distribution as that for the training set.\n\nConsider that the adversary knows some speakers for whom representations definitely exist in the training set and some for whom they definitely don't. We test the possibility of membership identification using four steps:\n\n1. We simulate the above using cross-validation. Given five speaker independent folds, we use three for the training set. From the remaining two folds, we add some samples of the selected speakers to the training set.\n\n2. We consider that the attacker knows both, the speakers selected and not selected for training from set four (ùë†4), but has no information about this split for set five (ùë†5). The objective of the attacker is to predict whether speakers were selected for inclusion in ‚Ä¢ A network trained to only recognize emotion is generally discriminative for gender as well. For instance we obtain a leakage of 0.73 when training a multi-modal network for activation and of 0.64 when trained for valence on MuSE.\n\n‚Ä¢ In unimodal systems, leakage is higher when systems are trained using only audio streams compared to lexical.\n\n‚Ä¢ Leakage of gender in learned representation is higher for multimodal systems than that for the unimodal systems for both, MuSE and IEMOCAP (the two datasets with both audio and lexical information).\n\nOur results suggest that models that aren't explicitly trained for gender recognition, or, that don't use gender as an input feature, still learn representations that are discriminative to identify gender. This leakage is more prominent when the input stream is audio as compared to lexical, but the leakage compounds in multimodal systems.",
      "page_start": 140,
      "page_end": 145
    },
    {
      "section_name": "Rq2: Privacy Preservation Performance",
      "text": "Q: How does the sensitive information reduction metric change when a network is trained to not encode sensitive information?\n\nHypothesis: Representations that are gender-invariant are less prone to leakage when attacked by an adversary, leading to reduced leakage of sensistive information.\n\nPrevious research has shown that obtaining a representation from a model trained to be invariant to gender, age, or location leads to better protection from an attacker who tries to recover this information  [55] . Previous research  [67]  has also shown that while the representations might be trained such that leakage of sensitive variable is reduced to chance, the attacker might still be able to recover this information. Hence, we concentrate on using this incapability as our primary metric. To test our hypothesis, we train the adversarial variants of the six models as mentioned above, while making sure that the leakage in the models is reduced to chance performance and compare our results to those in Table  8 .1a and ‚Ä¢ The sensitive information reduction metric is always higher when the representations are trained adversarially, compared to generally.\n\n‚Ä¢ Even when leakage is adversarially reduced to chance, the attacker is still able to recover information about gender.\n\n‚Ä¢ The sensitive information reduction metric is in general always lower for audio than for lexical based unimodal systems.\n\n‚Ä¢ Multimodal systems often have the lowest sensitive information reduction metric.\n\nOur results suggest that, though the sensitive information encoded in the learned representation is reduced using the proposed method, the attacker can still recover that information. This effect is especially compounded for multimodal systems. While previous work has concentrated on text (Section II), our work shows how audio is the major culprit and that models involving audio as input are easier to exploit, even when trained adversarially for not encoding sensitive information.",
      "page_start": 145,
      "page_end": 146
    },
    {
      "section_name": "Rq3: Privacy Preserving Emotion Recognition Performance",
      "text": "",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Q: How Does Emotion Recognition Performance Change When Networks Are Trained To Not Encode Sensitive Information?",
      "text": "Hypothesis: There is a minor drop in emotion recognition performance when models are trained to not encode sensitive information.\n\nPrevious research has shown that training a model invariant to a dataset variable might lead to drop in performance on the primary task, especially when there exists known correlations or biases in the datasets between the target label for the primary task and the secondary task  [153]  We compare the performance for predicting activation and valence of the models trained just to predict emotion (Act:   8 .1b). Our results suggest that, in general there is no significant effect on the performance on the primary task when we train networks to not encode sensitive information. We find that the performance is either maintained, e.g., Act: multimodal-MuSE;\n\nVal: multimodal-IEMOCAP, or there is a slight decrease in performance for some setups, e.g., Act: unimodal-acoustic-MuSE; Val: multimodal-MuSE. In multiple cases, such as Act/Val:unimodal-lexical-MuSE/IEMOCAP, contrary to some previous work, we also see a significant increase in performance, implying that making the model invariant to gender increases its robustness by not learning replicable associations between gender and emotion label.",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Rq4: Privacy Preservation Strength",
      "text": "",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Q: How Does The Adversarial Component'S Strength Impact Emotion Recognition Performance And The Sensitive Information Reduction Metric?",
      "text": "Hypothesis: As the strength of the adversarial component increase, the sensitive information reduction metric increases and the performance on the pimary task is unchanged.\n\nOur results in Section 8.4.2 suggest that while the leakage of the model was reduced to chance performance, the attacker is still capable of recovering this information. We analyze the effect of the strength of the adversarial component on the performance of the primary task and the sensitive information reduction metric.\n\nWe find that the emotion recognition performance is generally unaffected with a change in ùúÜ, as expected from the results in Section 8.4.3. We observe that the the attacker is usually less capable of inferring gender from learned representations when ùúÜ = 0.50 as compared to when ùúÜ = 0.75. For example, the sensitive information reduction metric for the unimodal audio system trained on MuSE increases from 0.39 to 0.45. But contrary to our expectation, we often see a decrease in the sensitive information reduction metric when we move from ùúÜ = 0.75 to ùúÜ = 1.00 for both activation and valence. For example, the sensitive Table  8 .2: Results for activation (Act) and valence (Val) prediction using multimodal input, when adversarially unlearning gender in each input (SIR-E) [left] stream separately. U-UAR, U(M/F)-UAR for male/female, P-sensitive information reduction metric, MI-membership identification. Bold-Italic shows significant improvement in the sensitive information reduction metric as compared to model trained to not encode sensitive information by maximizing loss on the concatenated representation (SIR-C)  [  Membership identification is defined as an attack that tries to identify if samples from a speaker 'x' were present in the training set  [136] .  [168]  showed that removing identifying factors from learned representations reduces the probability of membership leakage. For this analysis, we ask two questions: (a) can we defend against membership identification using a proxy task and, (b) can we defend against both, gender and membership identification?\n\nWe train an attack model for membership identification as specified in Section 8.3.2. We find that while adversarial removal of gender in the learned representation (Act: Table  8 .1a\n\nand Val: Table  8 .1b) does lead to reduced membership identification, as compared to a model trained solely for emotion recognition (Act: Table  8 .1a and Val: Table  8 .1b), the membership identification is still far higher than chance.\n\nOur goal is to be unable to identify whether samples from speaker 'x' exist in the training set. This is different from the usual membership defense that prevents prediction of presence of a data-point pair (ùëñùëõùëùùë¢ùë° ùë• , ùëúùë¢ùë° ùëùùë¢ùë° ùë• ) is in the training set. As a result, we require a proxy task, because our model cannot use samples from the speakers not in the training set even to induce invariance. We hypothesize that given randomly chosen speakers from the population, speaker-invariant training leads to representations that are less likely to encode speaker-specific information. This will make it harder for the attacker to identify membership of a particular speaker in the training set. We train the emotion recognition models specified Table  8 .3: Results for activation and valence prediction, for general classification (General), and, when adversarially unlearning subject identity (SIR-SubjectID) and both subject identity and gender (SIR-Multiple). U-UAR, U(M/F)-UAR for male/female, SIR-sensitive information reduction metric, MI-membership identification. Bold-Italic shows significant improvement in metrics as compared to general classification model and Italic shows significant difference in metrics as compared to the models trained to not encode sensitive information. Significance is established using paired t-test at adjusted p-value< 0.05. We show our results in Table  8 .3. We find that models trained to be invariant to speaker identity have significantly lower UAR for membership identification than those trained solely to recognize emotion, or trained invariant to gender, which matches our hypothesis.",
      "page_start": 147,
      "page_end": 151
    },
    {
      "section_name": "General",
      "text": "Extension towards multi-attribute invariance. We train our emotion recognition model using both the adversarial components (speaker id and gender) and the primary classification task i.e., emotion recognition. This ensures that the model can defend against both, gender and membership identification attacks. We report our results in Table  8 .  3 . We find that we can successfully train models that are safer against both, gender and membership identification attacks, while still maintaining similar performance on the primary task, as an evidence towards multi-attribute invariance.",
      "page_start": 151,
      "page_end": 151
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we show how sensitive information preserving networks trained for emotion recognition can be used to protect against gender and membership identification. This provides a compelling case for separating the process of data processing on user devices and of task-specific training on central servers. While in this chapter we concentrate on a single primary task i.e., emotion recognition, this method can be extended to maximize utility on multiple primary tasks that are loosely related to each other and are benefited from a multi-task setup as shown for dialogue act and turn detection, and sentiment and topic classification  [187] .\n\nWe propose model-agnostic evaluation templates can be instantiated with sociological word lists, and these templates can be generalized to any task with existing word lists. We introduce two metrics: one for emotion generalization and another for intentional reduction of learnt sensitive information. These metrics aim to comprehensively assess model performance, while reducing dependency on expensive human-based feedback. By effectively evaluating the robustness and sensitive variable leakage of models, the proposed metrics provide valuable insights into model performance, resulting in more efficient utilization of resources.\n\nEvaluating their effectiveness in enhancing model performance and reducing leakage of sensitive demographic, we find that these metrics are significantly correlated with model performance and can improve cross-corpus results. The proposed method empowers us to leverage the benefits of diverse and comprehensive emotion recognition models, while mitigating the costs associated with the traditional, labor-intensive feedback processes.\n\nUltimately, this approach leads to the development of more accurate and relevant emotion recognition models in a cost-effective manner.",
      "page_start": 152,
      "page_end": 153
    },
    {
      "section_name": "Introduction",
      "text": "Recent advances in natural language processing have led to the development of large and very large language models that can be trained on massive amounts of labeled data.\n\nHowever, despite their impressive performance on various natural language processing tasks, these models do not necessarily learn behavior that is similar to that of humans. Data diversity is particularly important in subjective, paralinguistic tasks, such as those seen in emotion recognition and other behavior modeling areas. But paralinguistic datasets are often relatively small, compared to the size of datasets seen in fields such as speech recognition.\n\nFurther, it can be difficult to identify appropriate metrics for these subjective tasks and, as a result, those common in emotion recognition tend to be metrics such as cross-entropy loss between the predicted and original labels.\n\nThe challenge with the common metrics is that they do not capture the full complexity of the problem, leading to models that have limitations at their core. Models trained to optimize for cross-entropy are often suffused with biases. Reducing learnt sensitive information or generalizing these models can be extremely difficult because sensitive information reduction processes generally rely on these same data and labels. Therefore, limitations in the original dataset are still present when the model attempts to unlearn these problematic associations  [192, 81] .\n\nResearchers have looked at countering these issues in several different ways: from data generation, to data augmentation, and, from annotation diversification to comprehensive data collection  [46] . Each of these methods comes with its own pros and cons. For example, generating data samples that are not perceived differently by humans is not only challenging, but also, requires extensive post-hoc evaluations  [109] . Researchers have also looked at diverse data collection methods, focusing on active learning optimizing for data diversity  [182] . While this method generally improves performance by a significant measure, we can still end up with datasets that reinforce unwanted stereotypes and prejudices  [49] .\n\nThis has led to a growing need for feedback that is based on human evaluation to ensure that the models produce more accurate and relevant results. One of the main challenges of using human-based feedback though is that it can be expensive to obtain, particularly for tasks that involve emotions. This is because emotions are subjective and can vary widely depending on demographic and situational factors. As a result, there is a need for alternative methods that can provide metrics for evaluating the performance of language models that are centered around human behavior.\n\nTo address this issue, we propose leveraging metrics that are based on sociological literature, which can capture the nuances of human behavior and emotions. By drawing on insights from sociology, we can develop more nuanced and context-sensitive metrics that can better capture the complexities of human behavior. This approach has the potential to provide a more comprehensive optimization of language models, while also reducing the dependence on expensive and time-consuming human-based feedback.",
      "page_start": 154,
      "page_end": 155
    },
    {
      "section_name": "Intuition Behind Human-Centered Metric Design",
      "text": "In this chapter, we propose a novel approach to developing task-invariant metric templates that are centered around human perception. To begin with, we define metric blueprints, called as metric templates. These templated metrics that have generic components that can be adapted for specific tasks. Our aim is to leverage these templates to evaluate a model, and to add a training objective that relies on sociological literature in addition to human-obtained labels. This is a significant departure from the traditional approach of using labeled data, which can be costly and time-consuming to obtain.\n\nIdeally, important words identified by models should align well with human's interpretations. To achieve this, we use model interpretability; any method that provides us some intuition as to the words that the model pays attention to for its prediction to define the metric templates. This allows us to have a training objective that optimizes for importance assigned to certain words based on sociolinguistic literature. This approach is different from using labels because it enables us to incorporate sociolinguistic knowledge into the metric templates.\n\nThe metric templates we propose consist of three components: directionality (sign), input interpretation saliency weights, and word lists. The combination of the three components results in a task-invariant metric template.\n\nThe sign component specifies whether the metric is meant to be maximized or minimized.\n\nBy setting the sign, we can control the direction of the correlational relationship between the input and output variables, and thus, improve the accuracy of our predictions.\n\nThe input interpretation saliency weights component specifies how the input data should be interpreted. They are saliency values obtained from the model for each word token in the input. These values are highly correlated with the predicted label and can be positive, negative, or zero. A positive value indicates that the presence of a word correlates with the prediction of that label. A negative value indicates that the presence of that word correlates with \"not predicting the given label.\" A zero value means that the word has no influence on the prediction.\n\nThe wordlists component provides a list of words that should be given special consideration when computing the metric. These lists can be grouped by class labels and may have associated weights with each word, but both of these features are optional. Wordlists are an essential tool for training machine learning models, as they allow us to focus on specific words or concepts that are relevant to our task. By using wordlists, we can improve the accuracy and efficiency of our models, as well as gain valuable insights into the relationships between words and concepts in our data.\n\nIn summary, our metric template allows us to focus on three separate aspects, each aligned with one of the components. Sign allows us to encourage the model to learn the correlation between a particular word and the output (sign of loss function). The input interpretation saliency weights allow us to measure how well the model captures the correlation between all words in a sentence and the output. The wordlists allow us to impact the learned correlations to align with our expectations about the relationshp between a given category and word choice.\n\nWe now ground these aspects, focusing on a given task-related wordlist focusing on the task of emotion recognition. For our purposes, we use the NRC-VAD  [159]  and a self curated, Emotion-Gender wordlists. We first calculate the overlap between the words in the input sentence and the task-related wordlist(s). This overlap allows us to identify words whose correlations (learned by a model) may need to change. For class-based wordlists, we use the signed correlation that corresponds to the predicted class label, which is controlled by the sign component of our metric template. By setting the sign, we can encourage the model to learn the correlation between a particular word and the output. In cases where we want to unlearn a task completely, we aim to move the correlation of those words to zero to remove any influence on the model output. The input interpretation saliency weights component of our metric template specifies how well the model captures the correlation between all words in a sentence and the output. By leveraging the overlap between task-related words and the input sentence, we can effectively optimize the correlations and improve the model's performance. Our proposed metric is designed to help models learn the correlations that are relevant to the task at hand while minimizing the impact of irrelevant words. By using the metric template, we can focus on three separate aspects, each aligned with one of the components: sign, iiw, and wordlists, allowing us to impact the learned correlations to align with our expectations about the relationship between a given category and word choice.\n\nThe focus of this chapter is on the extensive testing of a metric template designed for the task of emotion recognition. We had the option to choose between task coverage or usability. Task coverage involves testing multiple tasks on a single state-of-the-art (SoTA) dataset and model, while usability involves extensively evaluating the proposed metric for one subjective task of emotion, but testing it on multiple datasets and different model combinations. We chose the latter approach to validate the usability of the metric rather than the task coverage. Our approach involves selecting one task for learning, which is emotion, and one subtask for unlearning, which is demographics. Through this chapter, we iterate over various combinations of this setup, thoroughly validating the utility of the metric for both cases, including emotion classification improvement and reducing encoded sensitive information in emotion classification models, while varying the datasets and other factors.\n\nOur results demonstrate the effectiveness of the proposed metric in improving emotion classification and producing models with reduced sensitive information leakage.\n\nTo ensure that our proposed metric aligns with annotator choices, we conduct a crowdsourcing survey. The survey results indicate that our metric aligns well with annotator choices, thereby validating the effectiveness of our approach. The metric can be similarly applied to any other human-centered subjective tasks, such as, deception detection, opinion analysis etc.",
      "page_start": 156,
      "page_end": 156
    },
    {
      "section_name": "Requirements",
      "text": "The proposed templates has three inputs: input interpretation saliency weights, direction of learning (sign) and relevant wordlists.",
      "page_start": 159,
      "page_end": 159
    },
    {
      "section_name": "Input Interpretation Saliency Weights",
      "text": "The goal of input interpretation saliency weights is to understand what a trained black-box neural network is doing. This is achieved by examining the learned correlations from the model, which are obtained through a post-hoc method. By analyzing these correlations, we can gain insights into how the model is making its predictions and identify any potential leakage of sensitive information or limitations.\n\nWe calculate the reliance of a model on any given word in the input using the Captum interpretability library for PyTorch  [124] . Captum provides state-of-the-art algorithms to identify how input features contribute to a model's output.\n\nWe use the attribution algorithms implemented via integrated gradients  [211] . Integrated gradients represent the integral of gradients with respect to inputs along the path from a given baseline (absence of the cause) to input sample. The output is a set of words for each model instance that contribute towards the prediction along with their attribution weights.\n\nThe sign of the attribution weight shows the direction of the correlation, and the value is the strength of the correlation. Words that do not influence prediction should have zero correlation.",
      "page_start": 159,
      "page_end": 159
    },
    {
      "section_name": "Wordlists",
      "text": "A wordlist here refers to linguistic indicators extracted from the literature for a given task. 2. HCM sir Dem is a metric calculated over a dataset that is focused on sensitive information reduction (ùë†ùëñùëü) with respect to a demographic variable (ùê∑ùëíùëö, e.g., gender). It can also be evaluated in terms of either standalone performance or relative improvement.\n\n1We do not specifically call out words with negligible saliency.",
      "page_start": 160,
      "page_end": 161
    },
    {
      "section_name": "Metric Types And Calculation",
      "text": "The template enables two different types of model evaluations: (a) standalone (stn), and, (b) relative improvement (ri).",
      "page_start": 162,
      "page_end": 162
    },
    {
      "section_name": "Standalone (Hcm Stn )",
      "text": "We refer to standalone evaluation as benchmarking a model in isolation. We will first consider the case where we have combined wordlists (e.g., ùúÇ ùêø ùê¥+ùêµ ). This condition would be used when we do not have the class labels for a specific task in our setup, as in the case for the sensitive variable of age. It allows us to identify words that are broadly associated with age in general, rather than a specific age grouping (e.g., older adults).\n\nWe define {ùúÜ ùë• } ùë•‚ààùë† as the set of saliency values for all words in a given sample ùë†. We then define an intersection set, which identifies words that occur both in the sample (ùë• ‚àà ùë†)\n\nand in a wordlist (ùúÇ ùêø ùê¥+ùêµ ). The intersection set is defined as:\n\nWe can then derive the standalone metric (HCM stn ) from this intersection set for a given sample. We consider each word (ùë•) in the intersection set defined by Equation  9 .1. We weigh the saliency attribution for that word (ùúÜ ùë• ) by the wordlist weight (ùúî ùë• ) and sum this value over all words in intersection set for the sample. We normalize by the number of words Higher values of ùúÜ ùë• for words with higher wordlist weights ùúî ùë• suggest that the model is relying on words that are known to be related to a given task (e.g., the word \"thrilled\" and positive valence).\n\nWe can use the same metric to anticipate the degree to which a model reduces encoded sensitive information by replacing |ùúÜ ùë• | with 1 -|ùúÜ ùë• | (Equation  9 .4) for sample ùë†. Lower values of ùúÜ ùë• (and therefore high weights of 1 -|ùúÜ ùë• |) for words with higher wordlist weights ùúî ùë• suggest that the model is not relying upon words that are known to be related to a demographic variable (e.g., the word \"she\" and the task of gender classification).",
      "page_start": 162,
      "page_end": 163
    },
    {
      "section_name": "Hcm Sir Stn [T] : S",
      "text": "We again calculate the sensitive information reduction metric for a dataset by averaging over the number of samples in the dataset (Equation 9.5).",
      "page_start": 163,
      "page_end": 163
    },
    {
      "section_name": "Hcm Sir Stn",
      "text": "Next, we consider the case where we do have labels for a given sensitive variable (e.g., gender). In this case, we calculate the value of the metric with respect to the pairwise association between the wordlist for a given label (e.g., ùúÇ ùêø ùê¥ ) and the model's output saliency attribution value for words within the sample ({ùúÜ ùë• } ùë•‚ààùë† ).\n\nWe must consider four separate intersections, each of which evaluates the alignment between the directionality of the saliency attribution for a given word (i.e., positive vs.\n\nnegative, ùúÜ ùë•,ùë•‚ààùë† ) and whether or not the word is in a given class ùê¥. For simplicity, we will discuss the metric applied to a single class2, class ùê¥.\n\n2The metric can be extended to multiclass problems.\n\nWe first define four intersections, using the notation: (ùëñùëõùë° ùëñ , ùëñ ‚àà {1, 2, 3, 4}). We will discuss sets of words with certain properties:\n\n1. Words with positive saliency and in wordlist, ùúÇ ùêø ùê¥ : We average over the samples within the dataset to obtain the dataset-level metric for generalizability metric as in Equation  9 .3 and for the sensitive information reduction metric as in Equation  9 .5.",
      "page_start": 163,
      "page_end": 163
    },
    {
      "section_name": "Relative Improvement (Hcm Ri )",
      "text": "We refer to relative improvement evaluation as measuring how a model improves in a given task after adding an additional component (e.g., to reduce sensitive information encoded in a model). In aggregated level metrics such as accuracy, the relative improvement is measured by difference in performance aggregated over all sample's predictions. In our proposed method, we instead of focus on relative improvement per sample, as measured by the weights assigned to words/phrases. This is to measure the shift in the learnt associations of the model as compared to just the output predictions. As in the standalone case, we first calculate the relative improvement at the sample-level (e.g., HCM",
      "page_start": 165,
      "page_end": 165
    },
    {
      "section_name": "Methods",
      "text": "In this section, we provide an overview for the all the baseline models that we will evaluate with respect to HCM.",
      "page_start": 168,
      "page_end": 168
    },
    {
      "section_name": "Emotion Recognition",
      "text": "",
      "page_start": 168,
      "page_end": 168
    },
    {
      "section_name": "Baseline Emotion Recognition Model: Base",
      "text": "We use the base version of Bidirectional Encoder Representations from Transformers (BERT) model due to the prevalence of this approach  [63] . We use a pre-trained BeRT tokenizer for the model. We implement and fine-tune the model using the HuggingFace library  [230] .",
      "page_start": 168,
      "page_end": 168
    },
    {
      "section_name": "Multi-Dataset Training: Multi-D",
      "text": "Our goal is to create an emotion recognition model that is not biased by dataset. To do this, we start with the same model described above (Base). We introduce an additional adversarial task, recognizing the dataset (e.g., IEMOCAP, Section 3.1), following the domain adversarial networks method suggested in  [78] . We train our model on combinations of two datasets and test on the third.",
      "page_start": 168,
      "page_end": 168
    },
    {
      "section_name": "Sensitive Information Reduction",
      "text": "Our goal is to reduce a given model's ability to detect a sensitive attribute (e.g., gender) from embeddings learned for the task of emotion recognition. In all cases, the models are initialized with the Base model.\n\nWe measure sensitive information reduction success using Sensitive Information Leakage Measurement  [152] . It assumes that an adversary has black-box access to the representations learned from the emotion recognition algorithm and access to a gender labeled subset of the dataset. The adversary uses these representation to train an auxiliary model to predict gender for any representation from that model, as described in our prior work  [108] .",
      "page_start": 169,
      "page_end": 169
    },
    {
      "section_name": "Sensitive Information Reduction By Adversarial Training: Siradv",
      "text": "SIRAdv uses an adversarial training paradigm to reduce the sensitive information encoded in the generated embeddings with respect to gender. The main network is trained to unlearn gender using a Gradient Reversal Layer (GRL)  [77] , a multi-task approach to train models invariant to specific properties  [153] . We place the GRL between the embedding sub-network and gender classifier to obtain gender-invariant representations.",
      "page_start": 169,
      "page_end": 169
    },
    {
      "section_name": "Sensitive Information Reduction By Data Augmentation: Siraug",
      "text": "SIRAug is is trained using an augmented data set using gender-swapping to compare our proposed method and resultant metric to other successful approaches  [104] . We use a pronoun-based word list and create a gender-swapped equivalent for each sentence, e.g., replacing \"he\" with \"she\", \"his\" with \"hers\", and so on. Data augmentation has its own issues: it doubles the training data size with no added label information, has expensive list creation of gender-based words in the dataset to be replaced, and nonsensical sentence creation  [22] .",
      "page_start": 169,
      "page_end": 169
    },
    {
      "section_name": "Sensitive Information Reduction By Bias Fine-Tuning: Sirbias",
      "text": "SIRBias leverages an additional outside dataset that is has reduced correlations with respect to the sensitive variable, which is used to train an initial model. We use the improvised turns subset of MSP-Improv (see Section 3.1). We chose this subset because both actors (the male and female actor) perform improvisations using the same initial set of scenarios, controlling the word choice due to topic variations. Therefore, the manner and speaking and word choice is more likely to be related to gender differences, rather than topic differences.\n\nWe then fine tune a BERT model to be unable to distinguish between genders for a particular improvisational target. This clusters samples together by prompt, accounting for topic-based word variations. The result of this fine-tuning should ideally be a model with reduced leakage of gender information. This resulting model is then finetuned for emotion recognition on IEMOCAP (see Section 3.1).",
      "page_start": 169,
      "page_end": 169
    },
    {
      "section_name": "Helper Model",
      "text": "",
      "page_start": 169,
      "page_end": 169
    },
    {
      "section_name": "Gender Control: Gencontrol",
      "text": "Gender Control is a sanity check for verifying the reliability of our template. We hypothesize that if our proposed method indeed captures relevant information, then a model trained specifically to recognize gender should have the lowest HCM sir ri Gen value. We train a multi-task model for predicting both gender and emotion.",
      "page_start": 170,
      "page_end": 170
    },
    {
      "section_name": "Artificially Noisy Model: Artnoise",
      "text": "We create a parallel corpus of IEMOCAP to use as an attention check for the crowdsourced task. We add six artificial \"noisy\" features, {'zq0', 'zq1', 'zq2', 'zx0', 'zx1', 'zx2'}, such that they correlate specifically with both emotion and gender and train a classification model to predict emotion on this dataset. The added signals are 100% correlated to the sub-classes, ensuring that the model always learns these added nonsensical tokens as salient features.",
      "page_start": 170,
      "page_end": 170
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 170,
      "page_end": 170
    },
    {
      "section_name": "Models For Metric Correlation To Performance",
      "text": "We perform all training experiments three times, using one dataset for training at a time, and testing on the same dataset (within-corpus testing) as well as the other two (cross-corpus testing). We train six models as described in Section 9.5.1 and Section 9. We also perform experiments with each demographic metric on its own and other permutations of the setup.",
      "page_start": 170,
      "page_end": 170
    },
    {
      "section_name": "Crowdsourcing",
      "text": "Finally, we explore whether user preference aligns with our proposed metrics. We select a representative sample population by considering three levels of: 1) valence (low, medium, and high, see Section 3.2.1) and 2) the metrics of ùêªùê∂ ùëÄ ùë†ùëñùëü ùë†ùë°ùëõ ùê∫ùëõùëë : ùë† (low, medium, and high, measured via quantiles). This results in nine bins. The evaluators were not presented with the names of the models and the order of model outputs was randomized for each viewing. Note that the sentence was chosen to clearly convey gender, and potentially, gendered information. In the figure, green indicates that the model is using the word in the prediction, while red indicates that it is not. The outputs correspond to: 1) GenControl -note the heavy focus on the word \"dress\", 2) SIRBias, 3) ArtNoise, 4) SIRAug, 5) SIRAdv (for model details, see Section 9.5.2). Observe that the three sensitive information reduction methods are no longer focusing on the word \"dress\".\n\nWe select 60 random samples from each bin for a total 540 samples. We classify each sample using our five models (SIRAdv, SIRAug, SIRBias, GenControl, ArtNoise, described above). We extract sample-level predictions. In addition, we extract sample-level explanations using Captum  [211] , Section 9.3.1.1).\n\nArtNoise is used as a attention check baseline for human evaluation, ensuring that the evaluators are paying attention to the task at hand. We consider this option as an attention check and discard any response where the crowdsourced worker preferred the result and/or the explanation from this model (7.31% samples were discarded in total).\n\nWe presented the predictions from the models and heat map-based explanations. We asked the workers to choose between the five models, focusing on the following questions:\n\n(1) which model are you more likely to trust for emotion prediction and (2) which model are you more likely to trust for not being able to predict sensitive information.\n\nWe recruited annotators using Prolific from a population of workers with the following characteristics: 1) in the United States, and 2) native English speakers. Each task was annotated by three workers. We used qualification tests to check that all workers understood valence. Each task took an average of one-minute. The compensation was $9.45/hr. The baseline emotion classification model (Base) for within-corpus valence classification has 0.65 UAR for IEMOCAP, 0.68 for MuSE, and, 0.54 for MSP-Podcast (chance is 0.33 UAR), which is comparable to existing approaches on this domain  [9] .\n\nIn the cross-corpus setting, we see an average drop of at least 15% on all the models and datasets permutations. For example, a model trained on IEMOCAP and tested on MuSE has a UAR of 0.58 for Base, and, 0.53 for the SIRAdv model, which is significantly lower (paired t-test adjusted for multiple comparisons, p=0.0017) than the performance of these models when trained and tested on MuSE (0.68 and 0.71 respectively).\n\nThe multi-dataset emotion classification model has an overall higher performance in the cross-corpus setting. For example, the Multi-D has 0.64 UAR for IEMOCAP as compared to 0.61 and 0.57 obtained using Base.\n\nWe find that while reducing sensitive information leakage reduces within-corpus emotion recognition performance in many cases, these sensitive information leakage reduced models, especially SIRAdv usually have equivalent or better performance as compared to Base in cross-corpus setting, e.g., Base and SIRAdv trained on MuSE and tested on IEMOCAP has a UAR of 0.61 and 0.63, respectively. We initially validate the assumption that the HCM sir ri Gen metric is very low for the GenControl model (-0.21). This aligns with our expectation that the relative sensitive information reduction should be low. The goal of the GenControl model is to learn, rather than ignore, gender identity.\n\nFor within corpus settings, our expectation is that the SIRAug and SIRAug models will have the highest HCM sir ri Gen values. We find that across all the datasets, and models, gender leakage is significantly negatively correlated (paired t-test adjusted for multiple comparisons)\n\nwith HCM sir ri Gen for both within corpus (-0.95 * ) and cross-corpus (-0.89 * ) evaluation. ( * refers to significant value when using paired t-test adjusted for multiple comparisons)\n\nWe expect that models that generalize well will have high HCM in cross-corpus settings (Table  9 .1). In within corpus settings, we find that the addition of metric still benefits sensitive information reduction but can also lead to a non-negligible drop in emotion recognition performance. We also find that while age (HCM sir stn Age) and race (HCM sir stn Race) metrics on their own do not signify any positive change in cross-corpus dataset performance. We find that the metrics can be jointly considered in a simple multi-task setup between the primary task (i.e., emotion) and an adversarial component (HCM sir stn Gen). The multi-task setup results in the best performing gender information reduction (we do not have labels to measure age/race) of emotion recognition model (Table  9 .2).",
      "page_start": 171,
      "page_end": 173
    },
    {
      "section_name": "Rq3: How Does Additionally Optimizing For Hcm Gz Stn Emo Affect Emotion Recognition?",
      "text": "We find that optimizing for alignment with the human-centered metric leads to better cross-corpus performance across every setup (Table  9 .1). For example, when we augment the Base model trained on IEMOCAP with HCM gz stn Emo. The UAR increases to 0.6 and 0.45 for Muse and MSP-Podcast respectively, as compared to 0.58 and 0.43. Additionally, we find that, training the model with these metrics give us better or equivalent performance to when using the combined dataset method, which was the best solely emotion recognition model. This shows the effectiveness of the metric, and lowers the required computation time and power.\n\nWe also see that while we see a performance increase for cross-dataset testing, some within dataset performances are lower than the generally trained model (e.g., Base has UAR 0.54 for MSP-Podcast, but Base+HCM gz stn Emo has a UAR 0.52). We hypothesize that this occurs because the models were relying on spurious correlations (sprurious, with respect to emotion recognition), which, when removed, decreased the model's performance.  We find that the two components interact to give better performance on both cross-corpus emotion recognition and sensitive information reduction performance. For example, we see an improved UAR of 0.67 on IEMOCAP (cross-corpus), compared to 0.62 from the Base method). Further, we also see that the gender leakage universally reduces when training the models on this combination (cross-corpus testing on MSP-Podcast when using composite metric training on Muse improves performance by 6%). We hypothesize that encouraging certain generalizable correlations while discouraging spurious correlations leads to models that are more robust and have reduced sensitive information leakage across the spectrum, but the enforcement of specific 'unlearning' often leads to negligible drops in within-corpus sensitive information reduction performance.\n\nFinally, we found that inclusion of age and race, to gender, as sensitive information reduction components improves emotion recognition performance in cross corpus evaluation (Please see the full results table in the appendix). We show that user preference depends on what the user was asked to consider the model's behavior for, with respect to emotion or to sensitive information leakage  [103] . The user preference for emotion recognition models is significantly correlated with within-corpus valence recognition performance (0.78) and with the HCM gz stn Emo metric (0.71).\n\nThe correlation between user preference for emotion recognition and the HCM sir ri Gen metric is negligible (0.09). This suggests that when asked to focus on emotion recognition performance with models that have the same prediction but different highlighted words, both performance and the potential for generalizability (when grasped using the highlighted words/phrases for attributed by the model for the prediction) dominate user preference.\n\nWhen choosing sensitive information leakage reducing model, we find that user's preference is moderately correlated with emotion recognition performance (0.46) and strongly negatively correlated with the ability to predict gender (-0.86). This is also supported by the correlation between user preference and HCM sir ri Gen (0.64) and HCM gz stn Emo (0.81).\n\nThis support the hypothesis that, human perception can be encoded into models through the proposed metric template (HCM[T]).",
      "page_start": 176,
      "page_end": 177
    },
    {
      "section_name": "Conclusion",
      "text": "In this chapter, we present a novel set of emotion generalizability metrics focused on reducing sensitive information encoding, and reliance on emotionally-unrelated words.\n\nWe evaluate these metrics across the tasks of emotion recognition and classification of demographic variables (here, gender). We demonstrate that these metrics are correlated with the performance of algorithms, both within and across corpus, and that these metrics can be used during model training to improve classification performance and leakage of gender information. In future work, we aim to look at finer grained controls over reducing learnt sensitive information when evaluated by the end user, and methods to incorporate user-specific personalization of the baseline metric.\n\nPlease see the table on the following page that provides a detailed accounting for the results discussed in the rest of the chapter.\n\n-We unveiled a unique dataset, Multimodal Stressed Emotion (MuSE), purposed for studying the interaction between stress and emotion in speech.\n\n-We elucidated the procedure for data collection, the potential applications, and the methodology for annotating emotional content.\n\n-We showcased the dataset's potential in the development and evaluation of models focused on emotion recognition and stress detection.\n\n‚Ä¢ Chapter V brought forth:\n\n-Augmenting IEMOCAP with realistic noisy samples through the use of various types of environmental and synthetic noise.\n\n-An evaluation of how noise influences both the ground truth and predicted labels with associated guidance for noise-based augmentation within speech emotion datasets.\n\n-A strong emphasis on the need to consider the influence of noise on human emotion perception and the necessity of robustly testing models.\n\n‚Ä¢ Chapter VI provided insights on:\n\n-How context influences the annotation of emotional content within the MuSE dataset.\n\n-The comparison between two labeling methods, randomized and contextualized, and the discovery that contextualized labeling produces annotations closer to self-reported labels.\n\n-The finding that labels generated using the randomized method can be more efficiently predicted by automated systems.\n\n‚Ä¢ Chapter VII offered:\n\n-A new method for separating stress modulations from emotion representations utilizing adversarial networks.\n\n-Evidence showing that controlling for stress during training boosts the generalizability of emotion recognition models across new domains.\n\n-An effective demonstration of our unique approach's usability using the MuSE dataset and its potent applicability to other confounding variables present in emotion datasets.\n\n‚Ä¢ Chapter VIII achieved:\n\n-A comprehensive analysis of demographic leakage in representations obtained from textual, acoustic, and multimodal data when trained for emotion recognition.\n\n-The introduction of an adversarial learning paradigm to reduce demographic leakage from generated representations.\n\n-A valid demonstration of our novel approach's efficacy on numerous datasets and its potential use to defend against set-based membership identification.\n\n‚Ä¢ Chapter IX proposed:\n\n-Automatic and quantifiable metrics for measuring generalizability and sensitive information encoding in the representation of machine learning models within the context of speech emotion recognition.\n\n-A verification method of the utility of the proposed metrics using crowdsourcing,\n\nshowcasing their adaptability for evaluating cross-corpus generalization.\n\n-A cost-efficient and dependable way of appraising the effectiveness of machine learning models within speech emotion recognition.\n\nThese points, in sum, illustrate the specific contributions this dissertation makes to improve understanding, methodologies, and tools in the area of emotion recognition research.",
      "page_start": 178,
      "page_end": 183
    },
    {
      "section_name": "Future Work",
      "text": "Building upon the contributions of this dissertation, there are two primary areas where future research could focus.\n\nFirstly, when using interpretation and explanation, whether for emotion recognition or Schemas. These schemas can be thought of as templates, but with a key distinction. They not only focus on varying the input sample, but also on modifying the prompt specifications.",
      "page_start": 184,
      "page_end": 184
    }
  ],
  "figures": [
    {
      "caption": "Figure 4: 1). The data include self-report annotations for emotion and stress (Perceived Stress Scale,",
      "page": 47
    },
    {
      "caption": "Figure 4: 1: Experimental Protocol For Recording",
      "page": 52
    },
    {
      "caption": "Figure 4: 1. There were two sections in each recording: monologues and watching",
      "page": 52
    },
    {
      "caption": "Figure 4: 2: Close-up view of the thermal and video recording equipment.",
      "page": 57
    },
    {
      "caption": "Figure 4: 3. The labels obtained for our dataset form a distribution",
      "page": 59
    },
    {
      "caption": "Figure 4: 3: Distribution of the activation and valence ratings in random labeling scheme (on",
      "page": 60
    },
    {
      "caption": "Figure 4: 4: An overview of the instructions provided to the annotators for annotating an",
      "page": 61
    },
    {
      "caption": "Figure 4: 5: Annotation scale used by MTurk workers to annotate the emotional content of",
      "page": 62
    },
    {
      "caption": "Figure 6: 1 shows the absolute",
      "page": 108
    },
    {
      "caption": "Figure 6: 1: Mean difference between the self-reported activation and valence ratings and the",
      "page": 111
    },
    {
      "caption": "Figure 7: 1: Adversarial multi-task network architecture.",
      "page": 116
    },
    {
      "caption": "Figure 7: 1): (1) embedding sub-network; (2)",
      "page": 117
    },
    {
      "caption": "Figure 7: 1, the concatenation",
      "page": 117
    },
    {
      "caption": "Figure 7: 1 with the",
      "page": 117
    },
    {
      "caption": "Figure 8: 1: Privacy preserving network architecture.",
      "page": 137
    },
    {
      "caption": "Figure 8: 1). The main",
      "page": 137
    },
    {
      "caption": "Figure 8: 1) only allows for the same strength and parameters of the adversarial component to",
      "page": 149
    },
    {
      "caption": "Figure 9: 1: Diagram of the approach. In step 1, the wordlists are created (e.g., for gender,",
      "page": 156
    },
    {
      "caption": "Figure 9: 1 for a pictorial",
      "page": 171
    },
    {
      "caption": "Figure 9: 2: The crowdsourcing interface. Users are asked to indicate their model preference",
      "page": 172
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ha": "hl"
        }
      ],
      "page": 137
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "General Classification": "L\nU(M)\nU(F)\nU\nSIR\nMI",
          "Privacy Preserving Classification": "U(M)\nU(F)\nU\nSIR\nMI"
        },
        {
          "General Classification": "0.69\n0.65\n0.62\n0.63\n0.35\n0.71\n0.71\n0.69\n0.70\n0.70\n0.32\n0.73\n0.73\n0.66\n0.69\n0.67\n0.30\n0.72\n0.72\n0.61\n0.64\n0.63\n0.33\n0.75",
          "Privacy Preserving Classification": "0.64\n0.57\n0.60\n0.68\n0.44\n0.68\n0.69\n0.69\n0.44\n0.68\n0.70\n0.68\n0.69\n0.43\n0.67\n0.58\n0.61\n0.60\n0.45\n0.69"
        },
        {
          "General Classification": "0.62\n0.51\n0.52\n0.52\n0.39\n0.59\n0.64\n0.54\n0.56\n0.55\n0.38\n0.60",
          "Privacy Preserving Classification": "0.55\n0.56\n0.56\n0.48\n0.55\n0.57\n0.58\n0.58\n0.58\n0.47"
        },
        {
          "General Classification": "0.74\n0.66\n0.70\n0.68\n0.30\n0.74\n0.73\n0.65\n0.66\n0.66\n0.31\n0.76",
          "Privacy Preserving Classification": "0.66\n0.69\n0.68\n0.41\n0.67\n0.65\n0.64\n0.65\n0.43\n0.69"
        }
      ],
      "page": 143
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "General Classification": "L\nU(M)\nU(F)\nU\nSIR\nMI",
          "Privacy Preserving Classification": "U(M)\nU(F)\nU\nSIR\nMI"
        },
        {
          "General Classification": "0.56\n0.53\n0.49\n0.51\n0.44\n0.70\n0.60\n0.56\n0.57\n0.56\n0.42\n0.71\n0.62\n0.60\n0.61\n0.60\n0.39\n0.70\n0.58\n0.50\n0.47\n0.48\n0.42\n0.72",
          "Privacy Preserving Classification": "0.51\n0.49\n0.48\n0.68\n0.48\n0.55\n0.56\n0.56\n0.70\n0.47\n0.60\n0.62\n0.61\n0.68\n0.45\n0.48\n0.47\n0.46\n0.71\n0.47"
        },
        {
          "General Classification": "0.61\n0.64\n0.65\n0.65\n0.41\n0.62\n0.57\n0.68\n0.69\n0.68\n0.45\n0.63",
          "Privacy Preserving Classification": "0.62\n0.67\n0.68\n0.67\n0.46\n0.70\n0.71\n0.70\n0.47\n0.62"
        },
        {
          "General Classification": "0.68\n0.67\n0.71\n0.69\n0.32\n0.70\n0.64\n0.67\n0.66\n0.67\n0.38\n0.71",
          "Privacy Preserving Classification": "0.68\n0.70\n0.69\n0.45\n0.68\n0.64\n0.65\n0.65\n0.71\n0.46"
        }
      ],
      "page": 143
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Phonological history of english consonant clusters=",
      "venue": "Phonological history of english consonant clusters="
    },
    {
      "citation_id": "2",
      "title": "Deep learning with differential privacy",
      "authors": [
        "Martin Abadi",
        "Andy Chu",
        "Ian Goodfellow",
        "H Brendan Mcmahan",
        "Ilya Mironov",
        "Kunal Talwar",
        "Li Zhang"
      ],
      "venue": "Proc. 2016 ACM Conference CCS"
    },
    {
      "citation_id": "3",
      "title": "Incremental adaptation using active learning for acoustic emotion recognition",
      "authors": [
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Practical hidden voice attacks against speech and speaker recognition systems",
      "authors": [
        "Hadi Abdullah",
        "Washington Garcia",
        "Christian Peeters",
        "Patrick Traynor",
        "Kevin Rb Butler",
        "Joseph Wilson"
      ],
      "year": "2019",
      "venue": "Practical hidden voice attacks against speech and speaker recognition systems",
      "arxiv": "arXiv:1904.05734"
    },
    {
      "citation_id": "6",
      "title": "Analyzing thermal and visual clues of deception for a non-contact deception detection approach",
      "authors": [
        "Mohamed Abouelenien",
        "Rada Mihalcea",
        "Mihai Burzo"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments"
    },
    {
      "citation_id": "7",
      "title": "Deception detection using a multimodal approach",
      "authors": [
        "Mohamed Abouelenien",
        "Veronica P√©rez-Rosas",
        "Rada Mihalcea",
        "Mihai Burzo"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, Icmi '14"
    },
    {
      "citation_id": "8",
      "title": "Voce corpus: Ecologically collected speech annotated with physiological and psychological stress assessments",
      "authors": [
        "Ana Aguiar",
        "Mariana Kaiseler",
        "Mariana Cunha",
        "Hugo Meinedo",
        "Pedro Almeida",
        "Jorge Silva"
      ],
      "year": "2014",
      "venue": "LREC 2014, Ninth International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Dimitrios Dimitriadis, and Emily Mower Provost. Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Zakaria Aldeneh",
        "Soheil Khorram"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "The experimental generation of interpersonal closeness: A procedure and some preliminary findings",
      "authors": [
        "Arthur Aron",
        "Edward Melinat",
        "Elaine Aron",
        "Darrin Vallone",
        "Renee Bator"
      ],
      "year": "1997",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "12",
      "title": "Prosodic correlates of acted vs. spontaneous discrimination of expressive speech: a pilot study",
      "authors": [
        "Nicolas Audibert",
        "V√©ronique Auberg√©",
        "Albert Rilliard"
      ],
      "year": "2010",
      "venue": "Speech Prosody 2010-Fifth International Conference"
    },
    {
      "citation_id": "13",
      "title": "A constructivist view of emotion",
      "authors": [
        "James R Averill"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "14",
      "title": "Differential privacy has disparate impact on model accuracy",
      "authors": [
        "Eugene Bagdasaryan",
        "Vitaly Shmatikov"
      ],
      "year": "2019",
      "venue": "Differential privacy has disparate impact on model accuracy",
      "arxiv": "arXiv:1905.12101"
    },
    {
      "citation_id": "15",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "16",
      "title": "Noise analysis in audio-visual emotion recognition",
      "authors": [
        "Ntombikayise Banda",
        "Peter Robinson"
      ],
      "year": "2011",
      "venue": "Proceedings of the 11th International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "17",
      "title": "Path models of vocal emotion communication",
      "authors": [
        "Tanja B√§nziger",
        "Georg Hosoya",
        "Klaus Scherer"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "18",
      "title": "A face is exposed for aol searcher no. 4417749",
      "authors": [
        "Michael Barbaro",
        "Tom Zeller",
        "Saul Hansell"
      ],
      "year": "2006",
      "venue": "A face is exposed for aol searcher no. 4417749"
    },
    {
      "citation_id": "19",
      "title": "The big five personality dimensions and job performance: a meta-analysis",
      "authors": [
        "R Murray",
        "Michael Barrick",
        "Mount"
      ],
      "year": "1991",
      "venue": "Personnel psychology"
    },
    {
      "citation_id": "20",
      "title": "Eliciting emotion with film: Development of a stimulus set",
      "authors": [
        "Ellen Elizabeth"
      ],
      "year": "2011",
      "venue": "Eliciting emotion with film: Development of a stimulus set"
    },
    {
      "citation_id": "21",
      "title": "Can you tell apart spontaneous and read speech if you just look at prosody?",
      "authors": [
        "Anton Batliner",
        "Ralf Kompe",
        "E Kie√üling",
        "H N√∂th",
        "Niemann"
      ],
      "year": "1995",
      "venue": "Speech Recognition and Coding"
    },
    {
      "citation_id": "22",
      "title": "Analysis methods in neural language processing: A survey",
      "authors": [
        "Yonatan Belinkov",
        "James Glass"
      ],
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "A theory of learning from different domains",
      "authors": [
        "Shai Ben-David",
        "John Blitzer",
        "Koby Crammer",
        "Alex Kulesza",
        "Fernando Pereira",
        "Jennifer Vaughan"
      ],
      "year": "2010",
      "venue": "Machine learning"
    },
    {
      "citation_id": "24",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
      "authors": [
        "Yoav Benjamini",
        "Yosef Hochberg"
      ],
      "year": "1995",
      "venue": "Journal of the Royal statistical society: series B (Methodological)"
    },
    {
      "citation_id": "26",
      "title": "Acute sleep restriction effects on emotion responses in 30-to 36-monthold children",
      "authors": [
        "Rebecca Berger",
        "Alison Miller",
        "Ronald Seifer",
        "Stephanie Cares",
        "Monique Lebourgeois"
      ],
      "year": "2012",
      "venue": "Journal of sleep research"
    },
    {
      "citation_id": "27",
      "title": "Real-time speech emotion and sentiment recognition for interactive dialogue systems",
      "authors": [
        "Dario Bertero",
        "Farhad Bin Siddique",
        "Chien-Sheng Wu",
        "Yan Wan",
        "Ricky Ho Yin Chan",
        "Pascale Fung"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Modeling multimodal expression of user's affective subjective experience",
      "authors": [
        "Nadia Bianchi-Berthouze",
        "Christine Lisetti"
      ],
      "year": "2002",
      "venue": "User modeling and user-adapted interaction"
    },
    {
      "citation_id": "29",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "31",
      "title": "Temperature and emotions: Effects of physical temperature on responses to emotional advertising",
      "authors": [
        "Pascal Bruno",
        "Valentyna Melnyk",
        "Franziska V√∂lckner"
      ],
      "year": "2017",
      "venue": "International Journal of Research in Marketing"
    },
    {
      "citation_id": "32",
      "title": "Acute stress reduces speech fluency",
      "authors": [
        "Tony Buchanan",
        "Jacqueline Laures-Gore",
        "Melissa Duff"
      ],
      "year": "2014",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "33",
      "title": "Tradeoff between quality and quantity of emotional annotations to characterize expressive behaviors",
      "authors": [
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "A stepwise analysis of aggregated crowdsourced labels describing multimodal emotional behaviors",
      "authors": [
        "Alec Burmania",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "Alec Burmania",
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "Shrikanth narayanan fundamental frequency analysis for speech emotion processing. The role of prosody in affective speech",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Sungbok Lee"
      ],
      "year": "2009",
      "venue": "Shrikanth narayanan fundamental frequency analysis for speech emotion processing. The role of prosody in affective speech"
    },
    {
      "citation_id": "38",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Benchmarking multimodal sentiment analysis",
      "authors": [
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Hussain",
        "Subramanyam"
      ],
      "year": "2017",
      "venue": "International Conference on Computational Linguistics and Intelligent Text Processing"
    },
    {
      "citation_id": "40",
      "title": "Realtime multi-person 2d pose estimation using part affinity fields",
      "authors": [
        "Zhe Cao",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh"
      ],
      "year": "2017",
      "venue": "Cvpr"
    },
    {
      "citation_id": "41",
      "title": "The secret sharer: Evaluating and testing unintended memorization in neural networks",
      "authors": [
        "Nicholas Carlini",
        "Chang Liu",
        "√ölfar Erlingsson",
        "Jernej Kos",
        "Dawn Song"
      ],
      "year": "2019",
      "venue": "The secret sharer: Evaluating and testing unintended memorization in neural networks"
    },
    {
      "citation_id": "42",
      "title": "Audio adversarial examples: Targeted attacks on speech-to-text",
      "authors": [
        "Nicholas Carlini",
        "David Wagner"
      ],
      "year": "2018",
      "venue": "IEEE Security and Privacy Workshops"
    },
    {
      "citation_id": "43",
      "title": "Where did the anger go? the role of context in interpreting emotion in speech",
      "authors": [
        "Richard T Cauldwell"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "44",
      "title": "Privacy-preserving classification on deep neural network",
      "authors": [
        "Herv√© Chabanne",
        "Amaury De Wargny",
        "Jonathan Milgram"
      ],
      "year": "2017",
      "venue": "Constance Morel, and Emmanuel Prouff"
    },
    {
      "citation_id": "45",
      "title": "Gender and emotion expression: A developmental contextual perspective",
      "authors": [
        "Tara Chaplin"
      ],
      "year": "2015",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "46",
      "title": "Athanasios Katsamanis, Alexandros Potamianos, and Shrikanth Narayanan. Data augmentation using gans for speech emotion recognition",
      "authors": [
        "Aggelina Chatziagapi",
        "Georgios Paraskevopoulos",
        "Dimitris Sgouropoulos",
        "Georgios Pantazopoulos",
        "Malvina Nikandrou",
        "Theodoros Giannakopoulos"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "47",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sheng-Yeh Chen",
        "Chao-Chun Hsu",
        "Chuan-Chun Kuo",
        "K Ting-Hao",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations"
    },
    {
      "citation_id": "48",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sheng-Yeh Chen",
        "Chao-Chun Hsu",
        "Chuan-Chun Kuo",
        "Lun-Wei Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "49",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Yunliang Chen",
        "Jungseock Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition in noisy environment",
      "authors": [
        "Farah Chenchah",
        "Zied Lachiri"
      ],
      "year": "2016",
      "venue": "2016 2nd International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)"
    },
    {
      "citation_id": "51",
      "title": "Franc ¬∏ois Chollet. keras",
      "year": "2015",
      "venue": "Franc ¬∏ois Chollet. keras"
    },
    {
      "citation_id": "52",
      "title": "Franc ¬∏ois Chollet. keras",
      "year": "2015",
      "venue": "Franc ¬∏ois Chollet. keras"
    },
    {
      "citation_id": "53",
      "title": "Using uh and um in spontaneous speaking",
      "authors": [
        "H Herbert",
        "Jean E Fox Clark",
        "Tree"
      ],
      "year": "2002",
      "venue": "Cognition"
    },
    {
      "citation_id": "54",
      "title": "What makes a good conversation?: Challenges in designing truly conversational agents",
      "authors": [
        "Leigh Clark",
        "Nadia Pantidi",
        "Orla Cooney",
        "Philip Doyle",
        "Diego Garaialde",
        "Justin Edwards",
        "Brendan Spillane",
        "Emer Gilmartin",
        "Christine Murad",
        "Cosmin Munteanu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "55",
      "title": "Privacy-preserving neural representations of text",
      "authors": [
        "Maximin Coavoux",
        "Shashi Narayan",
        "Shay Cohen"
      ],
      "year": "2018",
      "venue": "Privacy-preserving neural representations of text",
      "arxiv": "arXiv:1808.09408"
    },
    {
      "citation_id": "56",
      "title": "Perceived stress in a probability sample of the united states",
      "authors": [
        "Sheldon Cohen"
      ],
      "year": "1988",
      "venue": "Perceived stress in a probability sample of the united states"
    },
    {
      "citation_id": "57",
      "title": "Perceived stress scale. Measuring stress: A guide for health and social scientists",
      "authors": [
        "Sheldon Cohen",
        "T Kamarck",
        "Mermelstein"
      ],
      "year": "1994",
      "venue": "Perceived stress scale. Measuring stress: A guide for health and social scientists"
    },
    {
      "citation_id": "58",
      "title": "A global measure of perceived stress",
      "authors": [
        "Sheldon Cohen",
        "Tom Kamarck",
        "Robin Mermelstein"
      ],
      "year": "1983",
      "venue": "Journal of health and social behavior"
    },
    {
      "citation_id": "59",
      "title": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "authors": [
        "Sam Corbett-Davies",
        "Sharad Goel"
      ],
      "year": "2018",
      "venue": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "arxiv": "arXiv:1808.00023"
    },
    {
      "citation_id": "60",
      "title": "From personality to passion: The role of the big five factors",
      "authors": [
        "Julien Dalp√©",
        "Martin Demers",
        "J√©r√©mie Verner-Filion",
        "Robert Vallerand"
      ],
      "year": "2019",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "61",
      "title": "Racial bias in hate speech and abusive language detection datasets",
      "authors": [
        "Thomas Davidson",
        "Debasmita Bhattacharya",
        "Ingmar Weber"
      ],
      "year": "2019",
      "venue": "Racial bias in hate speech and abusive language detection datasets",
      "arxiv": "arXiv:1905.12516"
    },
    {
      "citation_id": "62",
      "title": "Acute stress eliminates female advantage in detection of ambiguous negative affect",
      "authors": [
        "Joshua Daniel J Dedora",
        "Lilianne R Mujica-Parodi Carlson"
      ],
      "year": "2011",
      "venue": "Evolutionary Psychology"
    },
    {
      "citation_id": "63",
      "title": "Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "64",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "65",
      "title": "Exploring filler words and their impact",
      "authors": [
        "Emily Duvall",
        "Alan Robbins",
        "Thomas Graham",
        "Scott Divett"
      ],
      "year": "2014",
      "venue": "Exploring filler words and their impact"
    },
    {
      "citation_id": "66",
      "title": "Age as a sociolinguistic variable",
      "authors": [
        "Penelope Eckert"
      ],
      "year": "2017",
      "venue": "Age as a sociolinguistic variable"
    },
    {
      "citation_id": "67",
      "title": "Adversarial removal of demographic attributes from text data",
      "authors": [
        "Yanai Elazar",
        "Yoav Goldberg"
      ],
      "year": "2018",
      "venue": "Adversarial removal of demographic attributes from text data",
      "arxiv": "arXiv:1808.06640"
    },
    {
      "citation_id": "68",
      "title": "Auditing employment algorithms for discrimination",
      "authors": [
        "Alex Engler"
      ],
      "year": "2021",
      "venue": "Auditing employment algorithms for discrimination"
    },
    {
      "citation_id": "69",
      "title": "Randomization in privacy preserving data mining",
      "authors": [
        "Alexandre Evfimievski"
      ],
      "year": "2002",
      "venue": "ACM Sigkdd Explorations Newsletter"
    },
    {
      "citation_id": "70",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "72",
      "title": "Emotion and expression: Naturalistic studies",
      "authors": [
        "Carlos Crivelli"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "73",
      "title": "In ai we trust incrementally: a multi-layer model of trust to analyze human-artificial intelligence interactions",
      "authors": [
        "Andrea Ferrario",
        "Michele Loi",
        "Eleonora Vigan√≤"
      ],
      "year": "2019",
      "venue": "Philosophy & Technology"
    },
    {
      "citation_id": "74",
      "title": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): a randomized controlled trial",
      "authors": [
        "Kathleen Fitzpatrick",
        "Alison Darcy",
        "Molly Vierhile"
      ],
      "year": "2017",
      "venue": "JMIR mental health"
    },
    {
      "citation_id": "75",
      "title": "Comprehension of affective prosody in veterans with chronic posttraumatic stress disorder",
      "authors": [
        "John Thomas W Freeman",
        "Tim Hart",
        "Elliott Kimbrell",
        "Ross"
      ],
      "year": "2009",
      "venue": "The Journal of neuropsychiatry and clinical neurosciences"
    },
    {
      "citation_id": "76",
      "title": "The verbal communication of emotion: Introduction and overview",
      "authors": [
        "Susan Fussell"
      ],
      "year": "2002",
      "venue": "The verbal communication of emotions"
    },
    {
      "citation_id": "77",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2014",
      "venue": "Unsupervised domain adaptation by backpropagation",
      "arxiv": "arXiv:1409.7495"
    },
    {
      "citation_id": "78",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "E Yaroslav Ganin",
        "Hana Ustinova",
        "Pascal Ajakan",
        "H Germain",
        "Larochelle",
        "Mario Franc ¬∏ois Laviolette",
        "Victor Marchand",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "In J. Mach. Learn. Res"
    },
    {
      "citation_id": "79",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¬∏ois Laviolette",
        "Victor Marchand",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "80",
      "title": "Contact-free measurement of cardiac pulse based on the analysis of thermal imagery",
      "authors": [
        "Marc Garbey",
        "Nanfei Sun",
        "Arcangelo Merla",
        "Ioannis Pavlidis"
      ],
      "year": "2007",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "81",
      "title": "",
      "authors": [
        "Ismael Garrido-Mu√±oz",
        "A Montejo-R√°ez",
        "Fernando Mart√≠nez-Santiago",
        "L Alfonso Ure√±a-L√≥pez"
      ],
      "year": "2021",
      "venue": ""
    },
    {
      "citation_id": "82",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "F Jort",
        "Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Jansen"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "83",
      "title": "Barking up the right tree: Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "authors": [
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "Barking up the right tree: Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (addog)",
      "arxiv": "arXiv:1903.12094"
    },
    {
      "citation_id": "84",
      "title": "Emotion recognition from natural phone conversations in individuals with and without recent suicidal ideation",
      "authors": [
        "John Gideon",
        "Melvin Heather T Schatten",
        "Emily Mcinnis",
        "Provost"
      ],
      "year": "2019",
      "venue": "The 20th Annual Conference of the International Speech Communication Association INTERSPEECH 2019"
    },
    {
      "citation_id": "85",
      "title": "Multimodal expressions of stress during a public speaking task: Collection, annotation and global analyses",
      "authors": [
        "Tom Giraud",
        "Mariette Soury",
        "Jiewen Hua",
        "Agnes Delaborde",
        "Marie Tahon",
        "David Antonio Gomez",
        "Victoria Jauregui",
        "Edith Eyharabide",
        "Christine Filaire",
        "Laurence Scanff",
        "Devillers"
      ],
      "year": "2013",
      "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "86",
      "title": "Predicting personality from twitter",
      "authors": [
        "Jennifer Golbeck",
        "Cristina Robles",
        "Michon Edmondson",
        "Karen Turner"
      ],
      "year": "2011",
      "venue": "2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing"
    },
    {
      "citation_id": "87",
      "title": "The development of markers for the big-five factor structure",
      "authors": [
        "Lewis R Goldberg"
      ],
      "year": "1992",
      "venue": "Psychological assessment"
    },
    {
      "citation_id": "88",
      "title": "Data leak prevention through named entity recognition",
      "authors": [
        "Maria Gomez-Hidalgo",
        "Jose Miguel Martin-Abreu",
        "Javier Nieves",
        "Igor Santos",
        "Felix Brezo",
        "Pablo Bringas"
      ],
      "year": "2010",
      "venue": "IEEE Second International Conference on Social Computing"
    },
    {
      "citation_id": "89",
      "title": "Crafting adversarial examples for speech paralinguistics applications",
      "authors": [
        "Yuan Gong",
        "Christian Poellabauer"
      ],
      "year": "2017",
      "venue": "Crafting adversarial examples for speech paralinguistics applications",
      "arxiv": "arXiv:1711.03280"
    },
    {
      "citation_id": "90",
      "title": "Iii. basic emotions, complex emotions, machiavellian emotions 1",
      "authors": [
        "E Paul",
        "Griffiths"
      ],
      "year": "2003",
      "venue": "Royal Institute of Philosophy Supplements"
    },
    {
      "citation_id": "91",
      "title": "Deep Learning with Keras",
      "authors": [
        "Antonio Gulli",
        "Sujit Pal"
      ],
      "year": "2017",
      "venue": "Deep Learning with Keras"
    },
    {
      "citation_id": "92",
      "title": "A study on the impact of data anonymization on anti-discrimination",
      "authors": [
        "Sara Hajian",
        "Josep Domingo-Ferrer"
      ],
      "year": "2012",
      "venue": "IEEE"
    },
    {
      "citation_id": "93",
      "title": "A generalizable speech emotion recognition model reveals depression and remission",
      "authors": [
        "Lasse Hansen",
        "Yan-Ping Zhang",
        "Detlef Wolf",
        "Konstantinos Sechidis",
        "Nicolai Ladegaard",
        "Riccardo Fusaroli"
      ],
      "year": "2021",
      "venue": "A generalizable speech emotion recognition model reveals depression and remission"
    },
    {
      "citation_id": "94",
      "title": "Skin temperature reveals the intensity of acute stress",
      "authors": [
        "Katherine Herborn",
        "James Graves",
        "Paul Jerem",
        "Neil Evans",
        "Ruedi Nager",
        "Dominic Mccafferty",
        "Dorothy Mckeegan"
      ],
      "year": "2015",
      "venue": "Physiology & behavior"
    },
    {
      "citation_id": "95",
      "title": "Evaluating discourse annotation: Some recent insights and new approaches",
      "authors": [
        "Jet Hoek",
        "Merel Scholman"
      ],
      "year": "2017",
      "venue": "Proceedings of the 13th Joint ISO-ACL Workshop on Interoperable Semantic Annotation"
    },
    {
      "citation_id": "96",
      "title": "An experimental comparison of the psychological stress evaluator and the galvanic skin response in detection of deception",
      "authors": [
        "Frank Horvath"
      ],
      "year": "1978",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "97",
      "title": "Detecting deception: the promise and the reality of voice stress analysis",
      "authors": [
        "Frank Horvath"
      ],
      "year": "1982",
      "venue": "Journal of Forensic Science"
    },
    {
      "citation_id": "98",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "Wei-Ning Hsu",
        "Anuroop Sriram",
        "Alexei Baevski",
        "Tatiana Likhomanenko",
        "Qiantong Xu",
        "Vineel Pratap",
        "Jacob Kahn",
        "Ann Lee",
        "Ronan Collobert",
        "Gabriel Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "99",
      "title": "Data quality from crowdsourcing: a study of annotation selection criteria",
      "authors": [
        "Pei-Yun Hsueh",
        "Prem Melville",
        "Vikas Sindhwani"
      ],
      "year": "2009",
      "venue": "Proceedings of the NAACL HLT 2009 workshop on active learning for natural language processing"
    },
    {
      "citation_id": "100",
      "title": "Frankenstein: Learning deep face representations using small data",
      "authors": [
        "Guosheng Hu",
        "Xiaojiang Peng",
        "Yongxin Yang",
        "Timothy Hospedales",
        "Jakob Verbeek"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "101",
      "title": "Automatic dialogue generation with expressed emotions",
      "authors": [
        "Chenyang Huang",
        "Osmar Zaiane",
        "Amine Trabelsi",
        "Nouha Dziri"
      ],
      "year": "2018",
      "venue": "Proceedings of NAACL"
    },
    {
      "citation_id": "102",
      "title": "3-d motion estimation and object tracking using b-spline curve modeling",
      "authors": [
        "Z Huang",
        "F Cohen"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "103",
      "title": "End-user privacy in human-computer interaction",
      "authors": [
        "Giovanni Iachello",
        "Jason Hong"
      ],
      "year": "2007",
      "venue": "End-user privacy in human-computer interaction"
    },
    {
      "citation_id": "104",
      "title": "Dealing with bias via data augmentation in supervised learning scenarios",
      "authors": [
        "Vasileios Iosifidis",
        "Eirini Ntoutsi"
      ],
      "year": "2018",
      "venue": "Dealing with bias via data augmentation in supervised learning scenarios"
    },
    {
      "citation_id": "105",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "106",
      "title": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Cristian-Paul Bara",
        "Yuanhang Luo",
        "Mihai Burzo",
        "Rada Mihalcea",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "Muse-ing on the impact of utterance ordering on crowdsourced emotion annotations"
    },
    {
      "citation_id": "107",
      "title": "Controlling for confounders in multimodal emotion classification via adversarial learning",
      "authors": [
        "Mimansa Jaiswal",
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "Controlling for confounders in multimodal emotion classification via adversarial learning",
      "arxiv": "arXiv:1908.08979"
    },
    {
      "citation_id": "108",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "Mimansa Jaiswal",
        "Emily Provost"
      ],
      "year": "2020",
      "venue": "Aaai"
    },
    {
      "citation_id": "109",
      "title": "Best practices for noise-based augmentation to improve the performance of emotion recognition",
      "authors": [
        "Mimansa Jaiswal",
        "Emily Provost"
      ],
      "venue": "Best practices for noise-based augmentation to improve the performance of emotion recognition"
    },
    {
      "citation_id": "110",
      "title": "The truth and nothing but the truth: Multimodal analysis for deception detection",
      "authors": [
        "Mimansa Jaiswal",
        "Sairam Tabibu",
        "Rajiv Bajpai"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)"
    },
    {
      "citation_id": "111",
      "title": "Predicting emotions in usergenerated videos",
      "authors": [
        "Yu-Gang Jiang",
        "Baohan Xu",
        "Xiangyang Xue"
      ],
      "year": "2014",
      "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "112",
      "title": "Constance: Modeling annotation contexts to improve stance classification",
      "authors": [
        "Kenneth Joseph",
        "Lisa Friedland",
        "William Hobbs",
        "Oren Tsur",
        "David Lazer"
      ],
      "year": "2017",
      "venue": "Constance: Modeling annotation contexts to improve stance classification",
      "arxiv": "arXiv:1708.06309"
    },
    {
      "citation_id": "113",
      "title": "Effect of acting experience on emotion expression and recognition in voice: Non-actors provide better stimuli than expected",
      "authors": [
        "Rebecca J√ºrgens",
        "Annika Grass",
        "Matthis Drolet",
        "Julia Fischer"
      ],
      "year": "2015",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "114",
      "title": "Measuring emotional expression with the linguistic inquiry and word count",
      "authors": [
        "Renee Jeffrey H Kahn",
        "Audra Tobin",
        "Jennifer Massey",
        "Anderson"
      ],
      "year": "2007",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "115",
      "title": "On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study",
      "authors": [
        "Divyansh Kaushik",
        "Douwe Kiela",
        "Zachary Chase Lipton",
        "Wen Tau"
      ],
      "year": "2021",
      "venue": "On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study"
    },
    {
      "citation_id": "116",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "117",
      "title": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "authors": [
        "Soheil Khorram",
        "Mimansa Jaiswal",
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2018",
      "venue": "The priori emotion dataset: Linking mood to emotion detected in-the-wild"
    },
    {
      "citation_id": "118",
      "title": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "authors": [
        "Soheil Khorram",
        "Mimansa Jaiswal",
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2018",
      "venue": "The priori emotion dataset: Linking mood to emotion detected in-the-wild",
      "arxiv": "arXiv:1806.10658"
    },
    {
      "citation_id": "119",
      "title": "Determining the energetic and informational components of speech-on-speech masking",
      "authors": [
        "Gerald Kidd",
        "Christine Mason",
        "Jayaganesh Swaminathan",
        "Elin Roverud",
        "Virginia Kameron K Clayton",
        "Best"
      ],
      "year": "2016",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "120",
      "title": "No free lunch in data privacy",
      "authors": [
        "Daniel Kifer",
        "Ashwin Machanavajjhala"
      ],
      "year": "2011",
      "venue": "Proceedings of the ACM SIGMOD International Conference on Management of data"
    },
    {
      "citation_id": "121",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Convolutional neural networks for sentence classification",
      "arxiv": "arXiv:1408.5882"
    },
    {
      "citation_id": "122",
      "title": "Life hassles and delusional ideation: Scoping the potential role of cognitive and affective mediators",
      "authors": [
        "Cara Kingston",
        "James Schuurmans-Stekhoven"
      ],
      "year": "2016",
      "venue": "Psychology and Psychotherapy: Theory, Research and Practice"
    },
    {
      "citation_id": "123",
      "title": "The 'trier social stress test'-a tool for investigating psychobiological stress responses in a laboratory setting",
      "authors": [
        "Clemens Kirschbaum",
        "Karl-Martin Pirke",
        "Dirk Hellhammer"
      ],
      "year": "1993",
      "venue": "Neuropsychobiology"
    },
    {
      "citation_id": "124",
      "title": "A unified and generic model interpretability library for pytorch",
      "authors": [
        "Narine Kokhlikyan",
        "Vivek Miglani",
        "Miguel Martin",
        "Edward Wang",
        "Bilal Alsallakh",
        "Jonathan Reynolds",
        "Alexander Melnikov",
        "Natalia Kliushkina",
        "Carlos Araya",
        "Siqi Yan"
      ],
      "year": "2020",
      "venue": "A unified and generic model interpretability library for pytorch",
      "arxiv": "arXiv:2009.07896"
    },
    {
      "citation_id": "125",
      "title": "Automatic Segmentation of Speech into Sentence-like Units",
      "authors": [
        "J√°chym Kol√°≈ô"
      ],
      "year": "2008",
      "venue": "Automatic Segmentation of Speech into Sentence-like Units"
    },
    {
      "citation_id": "126",
      "title": "A study of allconvolutional encoders for connectionist temporal classification",
      "authors": [
        "Kalpesh Krishna",
        "Liang Lu",
        "Kevin Gimpel",
        "Karen Livescu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "127",
      "title": "Stress detection from speech and galvanic skin response signals",
      "authors": [
        "Hindra Kurniawan",
        "Alexandr Maslov",
        "Mykola Pechenizkiy"
      ],
      "year": "2013",
      "venue": "Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems"
    },
    {
      "citation_id": "128",
      "title": "Robust text classification in the presence of confounding bias",
      "authors": [
        "Virgile Landeiro",
        "Aron Culotta"
      ],
      "year": "2016",
      "venue": "Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "129",
      "title": "On how things are said: Voice tone, voice intensity, verbal content, and perceptions of politeness",
      "authors": [
        "Debi Laplante",
        "Nalini Ambady"
      ],
      "year": "2003",
      "venue": "Journal of Language and Social Psychology"
    },
    {
      "citation_id": "130",
      "title": "The eu-emotion voice database",
      "authors": [
        "Amandine Lassalle",
        "Delia Pigat",
        "O' Helen",
        "Steve Reilly",
        "Shimrit Berggen",
        "Shahar Fridenson-Hayo",
        "Sigrid Tal",
        "Anna Elfstr√∂m",
        "Ofer R√•de",
        "Sven Golan",
        "Simon B√∂lte",
        "Daniel Baron-Cohen",
        "Lundqvist"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "131",
      "title": "Environmental stress",
      "authors": [
        "S Richard",
        "Judith Lazarus",
        "Cohen"
      ],
      "year": "1977",
      "venue": "Human behavior and environment"
    },
    {
      "citation_id": "132",
      "title": "Stress and emotion recognition using acoustic speech analysis",
      "authors": [
        "Margaret Lech",
        "Ling He"
      ],
      "year": "2014",
      "venue": "Mental Health Informatics"
    },
    {
      "citation_id": "133",
      "title": "Apparatus and method for inducing emotions",
      "authors": [
        "Mihee Lee",
        "Seokwon Bang",
        "Gyunghye Yang"
      ],
      "year": "2011",
      "venue": "US Patent"
    },
    {
      "citation_id": "134",
      "title": "Automatic identification of gender from speech",
      "authors": [
        "Sarah Ita Levitan",
        "Taniya Mishra",
        "Srinivas Bangalore"
      ],
      "year": "2016",
      "venue": "Proceeding of Speech Prosody"
    },
    {
      "citation_id": "135",
      "title": "An overview of noiserobust automatic speech recognition",
      "authors": [
        "Jinyu Li",
        "Li Deng",
        "Yifan Gong",
        "Reinhold Haeb-Umbach"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "136",
      "title": "Membership privacy: a unifying framework for privacy definitions",
      "authors": [
        "Ninghui Li",
        "Wahbeh Qardaji",
        "Dong Su",
        "Yi Wu",
        "Weining Yang"
      ],
      "venue": "Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security"
    },
    {
      "citation_id": "137",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "Tao Li",
        "Shan Yang",
        "Liumeng Xue",
        "Lei Xie"
      ],
      "year": "2021",
      "venue": "Controllable emotion transfer for end-to-end speech synthesis"
    },
    {
      "citation_id": "138",
      "title": "Towards an \"in-thewild\" emotion dataset using a game-based framework",
      "authors": [
        "Wei Li",
        "Farnaz Abtahi",
        "Christina Tsangouri",
        "Zhigang Zhu"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "139",
      "title": "Laboratory-induced mental stress, cardiovascular response, and psychological characteristics",
      "authors": [
        "Li-Mei Liao",
        "Mary Carey"
      ],
      "year": "2015",
      "venue": "Reviews in cardiovascular medicine"
    },
    {
      "citation_id": "140",
      "title": "Mahnob-hci-tagging database",
      "authors": [
        "Jeroen Lichtenauer",
        "Mohammad Soleymani"
      ],
      "year": "2011",
      "venue": "Mahnob-hci-tagging database"
    },
    {
      "citation_id": "141",
      "title": "Classification with noisy labels by importance reweighting",
      "authors": [
        "Tongliang Liu",
        "Dacheng Tao"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "142",
      "title": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "authors": [
        "Francesco Locatello",
        "Stefan Bauer",
        "Mario Lucic",
        "Sylvain Gelly",
        "Bernhard Sch√∂lkopf",
        "Olivier Bachem"
      ],
      "year": "2018",
      "venue": "Challenging common assumptions in the unsupervised learning of disentangled representations",
      "arxiv": "arXiv:1811.12359"
    },
    {
      "citation_id": "143",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "144",
      "title": "It's only a computer: Virtual humans increase willingness to disclose",
      "authors": [
        "Jonathan Gale M Lucas",
        "Aisha Gratch",
        "Louis-Philippe King",
        "Morency"
      ],
      "year": "2014",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "145",
      "title": "Human emotions track changes in the acoustic environment",
      "authors": [
        "Weiyi Ma",
        "William Thompson"
      ],
      "year": "2015",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "146",
      "title": "Learning spontaneity to improve emotion recognition in speech",
      "authors": [
        "Karttikeya Mangalam",
        "Tanaya Guha"
      ],
      "year": "2017",
      "venue": "Learning spontaneity to improve emotion recognition in speech",
      "arxiv": "arXiv:1712.04753"
    },
    {
      "citation_id": "147",
      "title": "Cultural influences on the perception of emotion",
      "authors": [
        "David Matsumoto"
      ],
      "year": "1989",
      "venue": "Journal of Cross-Cultural Psychology"
    },
    {
      "citation_id": "148",
      "title": "Into the wild: Transitioning from recognizing mood in clinical interactions to personal conversations for individuals with bipolar disorder",
      "authors": [
        "Katie Matton",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "149",
      "title": "Adversarial training for satire detection: Controlling for confounding variables",
      "authors": [
        "Robert Mchardy",
        "Heike Adel",
        "Roman Klinger"
      ],
      "year": "2019",
      "venue": "Adversarial training for satire detection: Controlling for confounding variables"
    },
    {
      "citation_id": "150",
      "title": "Adversarial training for satire detection: Controlling for confounding variables",
      "authors": [
        "Robert Mchardy",
        "Heike Adel",
        "Roman Klinger"
      ],
      "year": "2019",
      "venue": "Adversarial training for satire detection: Controlling for confounding variables",
      "arxiv": "arXiv:1902.11145"
    },
    {
      "citation_id": "151",
      "title": "Natural language indicators of differential gene regulation in the human immune system",
      "authors": [
        "Matthias R Mehl",
        "Thaddeus Charles L Raison",
        "Jesusa Pace",
        "Steve Arevalo",
        "Cole"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "152",
      "title": "Debiasing methods in natural language understanding make bias more accessible",
      "authors": [
        "Michael Mendelson",
        "Yonatan Belinkov"
      ],
      "year": "2021",
      "venue": "Debiasing methods in natural language understanding make bias more accessible",
      "arxiv": "arXiv:2109.04095"
    },
    {
      "citation_id": "153",
      "title": "Yifan Gang, and Biing-Hwang Juang. Speaker-invariant training via adversarial learning",
      "authors": [
        "Zhong Meng",
        "Jinyu Li",
        "Zhuo Chen",
        "Yang Zhao",
        "Vadim Mazalov"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "154",
      "title": "Mirroring to build trust in digital assistants",
      "authors": [
        "Katherine Metcalf",
        "Barry-John Theobald",
        "Garrett Weinberg",
        "Robert Lee",
        "Ing-Marie Jonsson",
        "Russ Webb",
        "Nicholas Apostoloff"
      ],
      "year": "2019",
      "venue": "Mirroring to build trust in digital assistants",
      "arxiv": "arXiv:1904.01664"
    },
    {
      "citation_id": "155",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "Tomas Mikolov",
        "Ilya Sutskever",
        "Kai Chen",
        "Greg Corrado",
        "Jeff Dean"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "156",
      "title": "On the validity of the autobiographical emotional memory task for emotion induction",
      "authors": [
        "Caitlin Mills",
        "Sidney D' Mello"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "157",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "158",
      "title": "The relationship between emotional recognition and personality traits",
      "authors": [
        "Lyndon Mitchell"
      ],
      "year": "2006",
      "venue": "The relationship between emotional recognition and personality traits"
    },
    {
      "citation_id": "159",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "160",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "161",
      "title": "Linguistic markers of emotion regulation and cardiovascular reactivity among older caregiving spouses",
      "authors": [
        "Richard Joan K Monin",
        "Edward Schulz",
        "Thomas Lemay",
        "Cook"
      ],
      "year": "2012",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "162",
      "title": "Predicting couple therapy outcomes based on speech acoustic features",
      "authors": [
        "Md Nasir",
        "Brian Baucom",
        "Panayiotis Georgiou",
        "Shrikanth Narayanan"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "163",
      "title": "Gender differences in language use: An analysis of 14,000 text samples",
      "authors": [
        "Carla Matthew L Newman",
        "Lori Groom",
        "James Handelman",
        "Pennebaker"
      ],
      "year": "2008",
      "venue": "Discourse Processes"
    },
    {
      "citation_id": "164",
      "title": "Lying words: Predicting deception from linguistic styles",
      "authors": [
        "James Matthew L Newman",
        "Diane Pennebaker",
        "Jane Berry",
        "Richards"
      ],
      "year": "2003",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "165",
      "title": "Use of context in emotion perception: The role of top-down control, cue type, and perceiver's age",
      "authors": [
        "Nhi Ngo",
        "Derek Isaacowitz"
      ],
      "year": "2015",
      "venue": "Emotion"
    },
    {
      "citation_id": "166",
      "title": "Understanding and using the implicit association test: Ii. method variables and construct validity",
      "authors": [
        "Brian Nosek",
        "Anthony Greenwald",
        "Mahzarin Banaji"
      ],
      "year": "2005",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "167",
      "title": "Human gesture recognition using sparse b-spline polynomial representations",
      "authors": [
        "M Oikonomopoulos",
        "Pantic",
        "Patras"
      ],
      "year": "2008",
      "venue": "Proceedings of Belgium-Netherlands Conf. Artificial Intelligence (BNAIC 2008)"
    },
    {
      "citation_id": "168",
      "title": "A marauder's map of security and privacy in machine learning",
      "authors": [
        "Nicolas Papernot"
      ],
      "year": "2018",
      "venue": "A marauder's map of security and privacy in machine learning",
      "arxiv": "arXiv:1811.01134"
    },
    {
      "citation_id": "169",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Jes√∫s Villalba",
        "Piotr ≈ªelasko",
        "Laureano Moro-Velazquez",
        "Najim Dehak"
      ],
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "170",
      "title": "The perception of emotions in noisified nonsense speech",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Alice Baird",
        "Anton Batliner",
        "Nicholas Cummins"
      ],
      "year": "2017",
      "venue": "18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "171",
      "title": "How psychological stress affects emotional prosody",
      "authors": [
        "Silke Paulmann",
        "Desire Furnes",
        "Ming B√∏kenes",
        "Philip Cozzolino"
      ],
      "year": "2016",
      "venue": "Plos one"
    },
    {
      "citation_id": "172",
      "title": "Thermal image analysis for polygraph testing",
      "authors": [
        "Ioannis Pavlidis",
        "James Levine"
      ],
      "year": "2002",
      "venue": "IEEE Engineering in Medicine and Biology Magazine"
    },
    {
      "citation_id": "173",
      "title": "Thermal imaging for anxiety detection",
      "authors": [
        "Ioannis Pavlidis",
        "James Levine",
        "Paulette Baukol"
      ],
      "year": "2000",
      "venue": "Proceedings IEEE Workshop on Computer Vision Beyond the Visible Spectrum: Methods and Applications (Cat. No. PR00640)"
    },
    {
      "citation_id": "174",
      "title": "Linguistic inquiry and word count: Liwc",
      "authors": [
        "Martha James W Pennebaker",
        "Roger Francis",
        "Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc"
    },
    {
      "citation_id": "175",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "176",
      "title": "Esc: Dataset for environmental sound classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia"
    },
    {
      "citation_id": "177",
      "title": "Pre-wakeword speech processing",
      "authors": [
        "Kurt Piersol",
        "Gabriel Beddingfield"
      ],
      "year": "2019",
      "venue": "Pre-wakeword speech processing"
    },
    {
      "citation_id": "178",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "179",
      "title": "Evaluating explanations: How much do explanations from the teacher aid students?",
      "authors": [
        "Danish Pruthi",
        "Bhuwan Dhingra",
        "Baldini Livio",
        "Michael Soares",
        "Zachary Collins",
        "Graham Chase Lipton",
        "William Neubig",
        "Cohen"
      ],
      "year": "2022",
      "venue": "Evaluating explanations: How much do explanations from the teacher aid students?"
    },
    {
      "citation_id": "180",
      "title": "Transient emotional events and individual affective traits affect emotion recognition in a perceptual decision-making task",
      "authors": [
        "Emilie Qiao-Tasserit",
        "Maria Garcia Quesada",
        "Lia Antico",
        "Daphne Bavelier",
        "Patrik Vuilleumier",
        "Swann Pichon"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "181",
      "title": "Sad but true?-how induced emotional states differentially bias self-rated big five personality traits",
      "authors": [
        "Jan Quereng√§sser",
        "Sebastian Schindler"
      ],
      "year": "2014",
      "venue": "BMC Psychology"
    },
    {
      "citation_id": "182",
      "title": "A survey of deep active learning",
      "authors": [
        "Pengzhen Ren",
        "Yun Xiao",
        "Xiaojun Chang",
        "Po-Yao Huang",
        "Zhihui Li",
        "B Brij",
        "Xiaojiang Gupta",
        "Xin Chen",
        "Wang"
      ],
      "year": "2021",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "183",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on"
    },
    {
      "citation_id": "184",
      "title": "Classifying skewed data: Importance weighting to optimize average recall",
      "authors": [
        "Andrew Rosenberg"
      ],
      "year": "2012",
      "venue": "Thirteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "185",
      "title": "Voice stress analysis",
      "authors": [
        "J Leon",
        "Pascal Rothkrantz",
        "Jan-Willem A Wiggers",
        "Robert J Van Van Wees",
        "Vark"
      ],
      "year": "2004",
      "venue": "International conference on text, speech and dialogue"
    },
    {
      "citation_id": "186",
      "title": "Social content and emotional valence modulate gaze fixations in dynamic scenes",
      "authors": [
        "Marius Rubo",
        "Matthias Gamer"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "187",
      "title": "An overview of multi-task learning in deep neural networks",
      "authors": [
        "Sebastian Ruder"
      ],
      "year": "2017",
      "venue": "An overview of multi-task learning in deep neural networks",
      "arxiv": "arXiv:1706.05098"
    },
    {
      "citation_id": "188",
      "title": "Emotion recognition: Is it universal?",
      "authors": [
        "Russell James"
      ],
      "year": "2017",
      "venue": "Emotion recognition: Is it universal?"
    },
    {
      "citation_id": "189",
      "title": "Eliciting positive, negative and mixed emotional states: A film library for affective scientists",
      "authors": [
        "Andrea C Samson",
        "Sylvia Kreibig",
        "Blake Soderstrom",
        "Ayanna Wade",
        "James Gross"
      ],
      "year": "2016",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "190",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "Mousmita Sarma",
        "Pegah Ghahremani",
        "Daniel Povey",
        "Kumar Nagendra",
        "Kandarpa Goel",
        "Najim Sarma",
        "Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "191",
      "title": "The effect of noise on emotion perception in an unknown language",
      "authors": [
        "Odette Scharenborg",
        "Sofoklis Kakouros",
        "Jiska Koemans"
      ],
      "year": "2018",
      "venue": "Speech Prosody 2018. Isca"
    },
    {
      "citation_id": "192",
      "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
      "authors": [
        "Timo Schick",
        "Sahana Udupa",
        "Hinrich Sch√ºtze"
      ],
      "year": "2021",
      "venue": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp"
    },
    {
      "citation_id": "193",
      "title": "The perceived stress reactivity scale: Measurement invariance, stability, and validity in three countries",
      "authors": [
        "Wolff Schlotz",
        "Ilona Yim",
        "Peggy Zoccola",
        "Lars Jansen",
        "Peter Schulz"
      ],
      "year": "2011",
      "venue": "Psychological assessment"
    },
    {
      "citation_id": "194",
      "title": "Adversarially robust generalization requires more data",
      "authors": [
        "Ludwig Schmidt",
        "Shibani Santurkar",
        "Dimitris Tsipras",
        "Kunal Talwar",
        "Aleksander Madry"
      ],
      "year": "2018",
      "venue": "Advances in NIPS"
    },
    {
      "citation_id": "195",
      "title": "Quantifying interpretability and trust in machine learning systems",
      "authors": [
        "Philipp Schmidt",
        "Felix Biessmann"
      ],
      "year": "2019",
      "venue": "Quantifying interpretability and trust in machine learning systems",
      "arxiv": "arXiv:1901.08558"
    },
    {
      "citation_id": "196",
      "title": "Adversarial multi-task learning of deep neural networks for robust speech recognition",
      "authors": [
        "Yusuke Shinohara"
      ],
      "year": "2016",
      "venue": "Interspeech"
    },
    {
      "citation_id": "197",
      "title": "Google books ngram viewer",
      "authors": [
        "Anna Shparberg"
      ],
      "year": "2021",
      "venue": "Google books ngram viewer"
    },
    {
      "citation_id": "198",
      "title": "Experimental methods for inducing basic emotions: A qualitative review",
      "authors": [
        "Ewa Siedlecka",
        "Thomas Denson"
      ],
      "year": "2019",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "199",
      "title": "Hand keypoint detection in single images using multiview bootstrapping",
      "authors": [
        "Tomas Simon",
        "Hanbyul Joo",
        "Iain Matthews",
        "Yaser Sheikh"
      ],
      "year": "2017",
      "venue": "Cvpr"
    },
    {
      "citation_id": "200",
      "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
      "authors": [
        "Kihyuk Sohn",
        "David Berthelot",
        "Chun-Liang Li",
        "Zizhao Zhang",
        "Nicholas Carlini",
        "D Ekin",
        "Alex Cubuk",
        "Han Kurakin",
        "Colin Zhang",
        "Raffel"
      ],
      "year": "2020",
      "venue": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
      "arxiv": "arXiv:2001.07685"
    },
    {
      "citation_id": "201",
      "title": "Crowdsourcing for affective annotation of video: Development of a viewer-reported boredom corpus",
      "authors": [
        "Mohammad Soleymani",
        "Martha Larson"
      ],
      "year": "2010",
      "venue": "SIGIR-Workshops"
    },
    {
      "citation_id": "202",
      "title": "Emotion recognition across cultures: The influence of ethnicity on empathic accuracy and physiological linkage",
      "authors": [
        "Jose Angel",
        "Robert Levenson"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "203",
      "title": "Human trust modeling for bias mitigation in artificial intelligence",
      "authors": [
        "Fabian Sperrle",
        "Udo Schlegel",
        "Mennatallah El-Assady",
        "Daniel Keim"
      ],
      "year": "2019",
      "venue": "ACM CHI 2019 Workshop: Where is the Human? Bridging the Gap Between AI and HCI"
    },
    {
      "citation_id": "204",
      "title": "The paradigmatic behaviorism theory of emotions: Basis for unification",
      "authors": [
        "W Arthur",
        "Georg Staats",
        "Eifert"
      ],
      "year": "1990",
      "venue": "Clinical Psychology Review"
    },
    {
      "citation_id": "205",
      "title": "Speech under stress conditions: overview of the effect on speech production and on system performance",
      "authors": [
        "J Herman",
        "John Hl Steeneken",
        "Hansen"
      ],
      "year": "1999",
      "venue": "1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No. 99CH36258)"
    },
    {
      "citation_id": "206",
      "title": "Speech masking speech in everyday communication: The role of inhibitory control and working memory capacity",
      "authors": [
        "Victoria Stenb√§ck"
      ],
      "year": "2016",
      "venue": "Speech masking speech in everyday communication: The role of inhibitory control and working memory capacity"
    },
    {
      "citation_id": "207",
      "title": "Semeval-2007 task 14: Affective text",
      "authors": [
        "Carlo Strapparava",
        "Rada Mihalcea"
      ],
      "year": "2007",
      "venue": "Proceedings of the 4th International Workshop on the Semantic Evaluations"
    },
    {
      "citation_id": "208",
      "title": "Emotion-augmented machine learning: Overview of an emerging domain",
      "authors": [
        "Harald Str√∂mfelt",
        "Yue Zhang",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "209",
      "title": "Learning predictive models that transport",
      "authors": [
        "Adarsh Subbaswamy",
        "Peter Schulam",
        "Suchi Saria"
      ],
      "year": "2018",
      "venue": "Learning predictive models that transport",
      "arxiv": "arXiv:1812.04597"
    },
    {
      "citation_id": "210",
      "title": "Activity-aware mental stress detection using physiological sensors",
      "authors": [
        "Feng-Tso Sun",
        "Cynthia Kuo",
        "Heng-Tze Cheng",
        "Senaka Buthpitiya",
        "Patricia Collins",
        "Martin Griss"
      ],
      "year": "2012",
      "venue": "Mobile Computing, Applications, and Services"
    },
    {
      "citation_id": "211",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "year": "2017",
      "venue": "Axiomatic attribution for deep networks",
      "arxiv": "arXiv:1703.01365"
    },
    {
      "citation_id": "212",
      "title": "The psychological meaning of words: Liwc and computerized text analysis methods",
      "authors": [
        "R Yla",
        "James Tausczik",
        "Pennebaker"
      ],
      "year": "2010",
      "venue": "The psychological meaning of words: Liwc and computerized text analysis methods"
    },
    {
      "citation_id": "213",
      "title": "Students with anxiety: Implications for professional school counselors",
      "authors": [
        "Phyllis Heather Thompson",
        "Russ Robertson",
        "Melodie Curtis",
        "Frick"
      ],
      "year": "2013",
      "venue": "Professional School Counseling"
    },
    {
      "citation_id": "214",
      "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning",
      "authors": [
        "T Tieleman",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning"
    },
    {
      "citation_id": "215",
      "title": "Characterization of stress reactions to the stroop color word test",
      "authors": [
        "Jhm Tulen",
        "Moleman",
        "Van Steenis",
        "Boomsma"
      ],
      "year": "1989",
      "venue": "Pharmacology Biochemistry and Behavior"
    },
    {
      "citation_id": "216",
      "title": "A preliminary investigation of the relationship between emotion regulation difficulties and posttraumatic stress symptoms",
      "authors": [
        "Heidi Matthew T Tull",
        "Elaine Barrett",
        "Lizabeth Mcmillan",
        "Roemer"
      ],
      "year": "2007",
      "venue": "Behavior Therapy"
    },
    {
      "citation_id": "217",
      "title": "Improving the compromise between accuracy, interpretability and personalization of rule-based machine learning in medical problems",
      "authors": [
        "Francisco Valente",
        "Sim√£o Paredes",
        "Jorge Henriques"
      ],
      "year": "2021",
      "venue": "Improving the compromise between accuracy, interpretability and personalization of rule-based machine learning in medical problems"
    },
    {
      "citation_id": "218",
      "title": "A hybrid dsp/deep learning approach to real-time full-band speech enhancement",
      "authors": [
        "Jean-Marc Valin"
      ],
      "year": "2018",
      "venue": "2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "219",
      "title": "Immediacy bias in emotion perception: Current emotions seem more intense than previous emotions",
      "authors": [
        "Leaf Van Boven",
        "Katherine White",
        "Michaela Huber"
      ],
      "year": "2009",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "220",
      "title": "Overview on emotion recognition system",
      "authors": [
        "Ann Ashwini",
        "Jacob Varghese",
        "Jubilant Cherian",
        "Kizhakkethottam"
      ],
      "year": "2015",
      "venue": "2015 international conference on soft-computing and networks security (ICSNS)"
    },
    {
      "citation_id": "221",
      "title": "The subjective sense of presence, emotion recognition, and experienced emotions in auditory virtual environments",
      "authors": [
        "Daniel V√§stfj√§ll"
      ],
      "year": "2003",
      "venue": "CyberPsychology & Behavior"
    },
    {
      "citation_id": "222",
      "title": "Updating the oed on the historical lgbtq lexicon",
      "authors": [
        "Nicholas Lo"
      ],
      "year": "2021",
      "venue": "Updating the oed on the historical lgbtq lexicon"
    },
    {
      "citation_id": "223",
      "title": "The fading affect bias: But what the hell is it for?",
      "authors": [
        "Richard Walker",
        "John Skowronski"
      ],
      "year": "2009",
      "venue": "Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition"
    },
    {
      "citation_id": "224",
      "title": "Allennlp interpret: A framework for explaining predictions of nlp models",
      "authors": [
        "Eric Wallace",
        "Jens Tuyls",
        "Junlin Wang",
        "Sanjay Subramanian",
        "Matt Gardner",
        "Sameer Singh"
      ],
      "year": "2019",
      "venue": "Allennlp interpret: A framework for explaining predictions of nlp models",
      "arxiv": "arXiv:1909.09251"
    },
    {
      "citation_id": "225",
      "title": "Emotion regulation and stress",
      "authors": [
        "Manjie Wang",
        "Kimberly Saudino"
      ],
      "year": "2011",
      "venue": "Journal of Adult Development"
    },
    {
      "citation_id": "226",
      "title": "Twitter analysis: Studying us weekly trends in work stress and emotion",
      "authors": [
        "Wei Wang",
        "Ivan Hernandez",
        "Daniel Newman",
        "Jibo He",
        "Jiang Bian"
      ],
      "year": "2016",
      "venue": "Applied Psychology"
    },
    {
      "citation_id": "227",
      "title": "Mapping the emotional face. how individual face parts contribute to successful emotion recognition",
      "authors": [
        "Martin Wegrzyn",
        "Maria Vogt",
        "Berna Kireclioglu",
        "Julia Schneider",
        "Johanna Kissler"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "228",
      "title": "Convolutional pose machines",
      "authors": [
        "Shih-En Wei",
        "Varun Ramakrishna",
        "Takeo Kanade",
        "Yaser Sheikh"
      ],
      "year": "2016",
      "venue": "Cvpr"
    },
    {
      "citation_id": "229",
      "title": "Assessing emotion regulation in social anxiety disorder: The emotion regulation interview",
      "authors": [
        "Philippe Kelly H Werner",
        "Goldin",
        "M Tali",
        "Richard Ball",
        "James Heimberg",
        "Gross"
      ],
      "year": "2011",
      "venue": "Journal of Psychopathology and Behavioral Assessment"
    },
    {
      "citation_id": "230",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "R√©mi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "231",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "232",
      "title": "Prediction of the distribution of perceived music emotions using discrete samples",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "233",
      "title": "The ordinal nature of emotions",
      "authors": [
        "Roddy Georgios N Yannakakis",
        "Carlos Cowie",
        "Busso"
      ],
      "year": "2017",
      "venue": "Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "234",
      "title": "The impact of stress on body function: A review",
      "authors": [
        "Yunes Habib Yaribeygi",
        "Hedayat Panahi",
        "Thomas Sahraei",
        "Amirhossein Johnston",
        "Sahebkar"
      ],
      "year": "2017",
      "venue": "EXCLI journal"
    },
    {
      "citation_id": "235",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "236",
      "title": "Grade-level invariance of a theoretical causal structure predicting reading comprehension with vocabulary and oral reading fluency",
      "authors": [
        "Paul Yovanoff",
        "Luke Duesbery",
        "Julie Alonzo",
        "Gerald Tindal"
      ],
      "year": "2005",
      "venue": "Grade-level invariance of a theoretical causal structure predicting reading comprehension with vocabulary and oral reading fluency"
    },
    {
      "citation_id": "237",
      "title": "Automatic detection of \"g-dropping\" in american english using forced alignment",
      "authors": [
        "Jiahong Yuan",
        "Mark Liberman"
      ],
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "238",
      "title": "An approach to integrating emotion in dialogue management",
      "authors": [
        "Xiaobu Yuan"
      ],
      "year": "2015",
      "venue": "International Conference in Swarm Intelligence"
    },
    {
      "citation_id": "239",
      "title": "Predicting the distribution of emotion perception: capturing inter-rater variability",
      "authors": [
        "Biqiao Zhang",
        "Georg Essl",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "240",
      "title": "How does this make you feel? a comparison of four affect induction procedures",
      "authors": [
        "Xuan Zhang",
        "Hui Yu",
        "Lisa Barrett"
      ],
      "year": "2014",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "241",
      "title": "Differential privacy preservation in deep learning: Challenges, opportunities and solutions",
      "authors": [
        "Jingwen Zhao",
        "Yunfang Chen",
        "Wei Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "242",
      "title": "Personality-aware personalized emotion recognition from physiological signals",
      "authors": [
        "Sicheng Zhao",
        "Guiguang Ding",
        "Jungong Han",
        "Yue Gao"
      ],
      "year": "2018",
      "venue": "Ijcai"
    },
    {
      "citation_id": "243",
      "title": "A cross gender and cross lingual study on acoustic features for stress recognition in speech",
      "authors": [
        "Xin Zuo",
        "Pascale Fung"
      ],
      "year": "2011",
      "venue": "ICPhS"
    }
  ]
}