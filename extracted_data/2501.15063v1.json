{
  "paper_id": "2501.15063v1",
  "title": "Cross-Modal Context Fusion And Adaptive Graph Convolutional Network For Multimodal Conversational Emotion Recognition",
  "published": "2025-01-25T03:53:53Z",
  "authors": [
    "Junwei Feng",
    "Xueyan Fan"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "co-attention transformer",
    "graph convolutional network",
    "multi-task learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition has a wide range of applications in human-computer interaction, marketing, healthcare, and other fields. In recent years, the development of deep learning technology has provided new methods for emotion recognition. Prior to this, many emotion recognition methods have been proposed, including multimodal emotion recognition methods, but these methods ignore the mutual interference between different input modalities and pay little attention to the directional dialogue between speakers. Therefore, this article proposes a new multimodal emotion recognition method, including a cross modal context fusion module, an adaptive graph convolutional encoding module, and an emotion classification module. The cross modal context module includes a cross modal alignment module and a context fusion module, which are used to reduce the noise introduced by mutual interference between different input modalities. The adaptive graph convolution module constructs a dialogue relationship graph for extracting dependencies and self dependencies between speakers. Our model has surpassed some state-of-the-art methods on publicly available benchmark datasets and achieved high recognition accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversation (ERC)  [1] -  [4] , as a significant research area in artificial intelligence, holds immense application potential in fields such as human-computer interaction  [5] , marketing  [6] , and healthcare  [7] . With the rapid advancement of deep learning technologies, ERC methods have witnessed remarkable innovation and progress  [8] . Among these, multimodal emotion recognition approaches  [9] , which integrate information from multiple modalities, have gained significant attention due to their ability to comprehensively and accurately capture emotional expressions. In everyday interactions, emotional expressions are often conveyed through a combination of modalities, including language, facial expressions, and vocal tone. These modalities are inherently complementary and interdependent, offering rich emotional context when combined.\n\nDespite their promise, existing multimodal emotion recognition methods  [10] -  [12]  face notable challenges. First, integrating information from multiple modalities often introduces noise due to mutual interference, which can negatively impact recognition accuracy. Second, in conversational scenarios, these methods frequently overlook the bidirectional dependencies and intricate relationships between speakers, which are crucial for understanding emotional dynamics. Consequently, the inability to fully explore speaker relationships and dialogue context limits the depth and effectiveness of current models in capturing emotional interactions. Recently, diffusion models  [13] -  [15]  have shown promise in mitigating such challenges by leveraging progressive noise reduction to refine features across multiple modalities. Their ability to model complex dependencies and generate context-aware representations offers potential advantages for capturing intricate emotional dynamics in multimodal and conversational settings.\n\nTo address these challenges, this paper proposes a novel multimodal emotion recognition framework that leverages cross-modal context fusion and adaptive graph convolutional networks to enhance performance. The proposed method consists of three key components: a cross-modal context fusion module, an adaptive graph convolutional encoding module, and an emotion classification module. The cross-modal context fusion module reduces noise by aligning and integrating contextual information across modalities, while the adaptive graph convolutional encoding module constructs a dialogue relationship graph to capture speaker dependencies and conversational directionality. Finally, the emotion classification module decodes these enriched features to classify emotions. Experimental results on publicly available ERC datasets demonstrate that the proposed model outperforms state-of-the-art methods, offering a new perspective for advancing multimodal emotion recognition research. Our main contributions are summarized as follows:\n\n• We propose a novel multimodal emotion recognition framework that achieves state-of-the-art performance on two widely used ERC benchmark datasets. • We introduce a multi-task learning-based loss function that enables the model to simultaneously handle coarsegrained and fine-grained emotion recognition tasks, enhancing its overall performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Emotion Recognition In Conversation",
      "text": "With the widespread use of social media and smart devices, a vast amount of data is generated in daily life, including text, images, and audio. These data contain rich emotional information, such as emotional states, reactions, and expressions. Consequently, Emotion Recognition in Conversations (ERC) has become an important research area. ERC can be applied not only in natural language processing, computer vision, and speech recognition but also provides effective solutions for human-computer interaction, sentiment analysis, and public opinion monitoring. With the advancement of deep learning technologies, numerous ERC methods based on deep learning have emerged. A model based on LSTM  [16]  was proposed to capture contextual information from the surrounding environment within the same video, aiding the classification process. The CMN  [17]  conversational memory network was introduced, leveraging contextual information from conversational history. This framework employs a multimodal approach, including audio, visual, and textual features, with gated recurrent units to model each speaker's past utterances as memories. These memories are then merged through attentionbased jumps to capture dependencies between speakers. A DialogueRNN  [18]  model based on recurrent neural networks was developed to track the states of various parties throughout the conversation and use this information for emotion classification. The DialogueGCN  [19] , a graph convolutional neural network-based ERC method, was first proposed, focusing solely on textual features. A new model, MMGCN  [20] , based on multimodal fusion graph convolutional networks, was introduced, which can effectively utilize multimodal dependencies and model dependencies between and within speakers. However, this direct fusion approach may lead to redundant information and loss of heterogeneous information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Graph Neural Network",
      "text": "Convolutional neural networks (CNNs) have been widely used for extracting image features  [12] . However, CNNs exhibit inherent limitations when handling graph-structured data, as they are primarily designed for Euclidean space data. To address these challenges, graph neural networks (GNNs)  [21] ,  [22]  have emerged as a powerful alternative, enabling effective learning and inference in non-Euclidean domains. Unlike traditional deep learning models, which focus on processing vectors and matrices, GNNs leverage the topological structure of graphs and the relationships between nodes to capture complex dependencies.\n\nSeveral GNN architectures have been proposed, including GCN  [23] , GraphSAGE  [24] , and GAT  [25] , each offering unique approaches to graph-based learning. The core idea of GCN is to generalize convolution operations from Euclidean space to graph structures. In traditional CNNs, convolutional operations extract local features via sliding windows. In contrast, GCN performs feature aggregation by combining information from neighboring nodes in the graph. Specifically, GCN updates each node's representation by applying a linear combination of its features and those of its neighbors, weighted by a learnable matrix. GCN's strengths include parameter sharing, adaptive aggregation, node embedding representation, and enhanced predictive capabilities. GAT introduces an attention mechanism to GNNs, assigning different weights to neighbor nodes during feature aggregation. This mechanism dynamically adjusts the importance of neighbors based on their connections, enabling GAT to better adapt to diverse graph structures and capture a broader range of information. This flexibility makes GAT particularly effective in scenarios where certain nodes contribute more significantly to the task at hand. GraphSAGE, on the other hand, adopts a sampling-based approach to efficiently learn node representations. Instead of aggregating all neighbor nodes like GCN, GraphSAGE samples a fixed number of neighbors and aggregates their features to approximate the global structure of the graph. This approach significantly reduces computational complexity, making it suitable for large-scale graph data. By allowing different sampling strategies and aggregation methods, GraphSAGE can adapt to various graph structures and capture richer contextual information.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "The proposed model,named MERC-GCN,is designed for multimodal emotion recognition in conversations. The model consists of three steps: cross-modal context fusion, adaptive graph convolutional encoding, and emotion classification. The overall framework is illustrated in Fig.  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definition",
      "text": "Assume there are M speakers in a conversation, with the sequence of utterances represented as u 1 , u 2 , • • • , u N , where each utterance u i is spoken by speaker p s (u i ). Each utterance contains three emotional modalities u V i , u A i , u T i , where V , A, and T represent information from visual, audio, and textual sources, respectively. Our task is to predict the emotional category y i of the speaker corresponding to each utterance u i .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Preprocessing: Unimodal Feature Extraction",
      "text": "Text Modality: RoBERTa  [26]  is a variant of BERT  [27]  that employs more efficient pre-training methods, making it a more robust pre-trained language model than BERT. In this paper, RoBERTa is used to encode text information into a 200-dimensional feature vector. All text features are denoted as U T .\n\nAudio Modality: openSMILE  used in affective computing for automatic emotion recognition. openSMILE performs the following four types of feature extraction operations: signal processing, data processing, audio features (low-level), and functionals. In this paper, openSMILE is used to encode audio information into a 100-dimensional feature vector. All audio features are denoted as U A .\n\nVisual Modality: DenseNet  [29]  is a type of CNN network whose basic concept is similar to ResNet  [30]  but establishes dense connections between all preceding layers and subsequent layers, enabling feature reuse through connections across channels. CNN networks are better suited for capturing image features, and in this paper, DenseNet is used to encode video information into a 100-dimensional feature vector. All video features are denoted as U V .\n\nC. Method 1) Cross-modal Context Fusion: Different modalities at the same time have correlations. If these are directly concatenated as input features to the network, the network might confuse the correlations between different modal features. Therefore, this paper uses a co-attention transformer (CT)  [31] for crossmodal enhancement to learn distinct cross-modal correlated features.\n\nAs shown in the Fig.  2 , each CT learns cross-modal representations between two modalities; thus, three co-attention transformers are required to learn cross-modal representations for each pair of the three modalities in the ERC task. Each CT block consists of two identical parts, left and right, with symmetrical input. In the left part, one input modality is used as the query, while the other modality is used as the key and value, with the latter weighted and summed under the guidance of the former. The right part of the CT block undergoes a symmetrical process simultaneously. This entire process repeats T times, outputting the mutual cross-modal representations of the two input modalities.\n\nCo-attention transformer reduces the semantic gap between modalities and enhances shared features between them, achieving modality alignment and reducing noise in the input modalities. The entire process is mathematically represented as:\n\nHere, ⊕ denotes the concatenation operation. Q, K, V ∈ R L×d model represent two of the input modalities U A , U V , U T , as previously described. L is the length of the input feature vector of the corresponding modality. The feedforward neural network consists of two linear layers, mathematically represented as:\n\nWhere X ∈ R L×d model is the output after the first residual connection and layer normalization in the CT block, and σ represents the activation function, with ReLU being used in this paper. The CT block is stacked T times, with the output of the previous CT block serving as the input to the next, achieving enhanced representation. This entire step can be described mathematically as follows:\n\nCT represents the co-attention transformer, constructed by T stacked co-attention transformer blocks. E T -A denotes the cross-modal representation of the text modality relative to the visual modality, and so on.\n\nWe concatenate the learned cross-modal correlated features with the original features to prepare for the next step of context feature fusion. This is mathematically represented as:\n\nFor the i-th utterance, the features it carries are denoted as f i , so:\n\nConversations occur sequentially, with contextual information flowing along this sequence. Based on this characteristic, we constructed a bidirectional gated recurrent unit (BiGRU)  [32]  to capture contextual information. The input modality features include both the original modality features and the cross-modal correlated features, achieving fusion and interaction within the flow of contextual information. The specific mathematical formula is as follows:\n\nHere, g i represents the feature after sequential context fusion. This step integrates sequential contextual modality features but does not yet account for speaker identity and interspeaker dependencies. These aspects will be considered in the next step.\n\n2) Adaptive Graph Convolution Encoding: We constructed a graph convolutional neural network to encode the relationships between speakers, thereby capturing both inter-speaker dependencies and self-dependencies.\n\nFirst, we define the following symbols: based on a scenario with N utterances, we construct a directed graph G = (V, E, R, W ), where nodes v i ∈ V and r ij ∈ R represent a directed edge from node v i to node v j , and α ij ∈ W represents",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Add & Norm",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Add & Norm Feed Forward",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Add & Norm",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Add & Norm Feed Forward",
      "text": "Multi-Head Attention",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dialogue Graph Construction",
      "text": "Nodes: Each utterance u i in the conversation represents a node v i ∈ V in the graph. For each node\n\nwe initialize it with the encoded sequential context feature vector g i . This vector serves as the feature of the node. After speakerlevel encoding within the model, the sequential context feature vector is transformed into the corresponding speaker-level feature vector.\n\nEdges: The construction of edges models the conversational relationships between speakers. Assuming each utterance is a vertex, it affects and is affected by all other vertices (including itself) to varying degrees. This relationship is represented by directed edges in a directed graph, where the influence of u i on itself is represented by a directed edge from u i to u i . This reflects, in practical terms, the inertia of the speaker themselves. However, using all N utterances to construct this directed graph results in a computational complexity of O(N 2 ), which can be very costly when there are many utterances. In practice, instead of using all utterances, we can consider only those within a certain time frame, representing a past context window of p utterances observed in the past and a future context window of f utterances to be observed in the future, constructing a directed graph with p + f vertices. Each vertex u i has edges directed to the past p vertices and the future f vertices, representing its influence on past and future utterances. In this paper's experiments, we set both the past and future context window sizes to 10, meaning the directed graph is constructed using 10 past and 10 future utterances.\n\nEdge Weights: The weights of the edges are calculated using a similarity-based attention mechanism. The calculation method of the attention function ensures that for each vertex, the total weight of incoming edges sums to 1. Considering the past context window size p and the future context window size f , the weight calculation is as follows:\n\nThis ensures that the total weight contribution of the incoming edges for vertex v i from vertices v i-p , • • • , v i+f sums to 1. Different weight values represent the varying influence of the corresponding vertices.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Representation Learning",
      "text": "Before this step, the feature g is a multimodal fusion feature independent of speaker relationships, including text semantics and real-time representations of audio and video. Next, we use a graph convolutional network to perform a two-step feature transformation to extract representations of connections between speakers.\n\nIn the first step, we use one layer of GCN to aggregate neighborhood information of vertices, thereby initially encoding the directional nature of conversations between speakers. In this step, we use the DropMessage method to enhance the aggregation capability of GCN. We generate a mask matrix of the same size as the message matrix based on a Bernoulli distribution, where each element in the message matrix is dropped to a certain extent as determined by the corresponding value in the mask matrix. After applying dropmessage, the mathematical formula for the node features is:\n\nwhere V M denotes the masked nodes, g[M ] represents the feature vector of the masked nodes, and g[M ] represents the updated node features.\n\nThe mathematical formula for masked edges is:\n\nwhere ϕ M denotes the masked edges, e ij [M ] represents the weight of the masked edges, and êij [M ] denotes the updated edge weight. The overall learning formula for this step is:\n\nwhere α ii and α ij are the edge weights, and N r i is the neighborhood index of vertex i under relationship r ∈ R c . c i,r is a normalization constant specific to the task and automatically learned in a gradient-based learning setup. σ is an activation function like ReLU, and W r and W 0 are learnable transformation parameters.\n\nIn the second step, we apply GCN again to extract relationship features between vertices, reinforcing the extraction of features that capture the conversational relationships between speakers:\n\nwhere W c and W 0 are learnable parameters, and σ is an activation function.\n\nThis step constructs a graph model of conversational relationships between speakers, building upon the previous step's cross-modal context feature fusion to capture the conversational relationship features between speakers.\n\n3) Emotion Classification: We fuse the context encoding vector with the speaker encoding vector and use an attention mechanism to learn the importance of different features:\n\nFinally, we feed the resulting features into an MLP for decoding. The softmax function outputs the final predicted probability distribution for each class, and we select the label corresponding to the highest probability as the prediction result:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Optimization Objective",
      "text": "We use the categorical cross-entropy loss function as the objective function for training. We adopt a multi-task learning strategy, with the loss function consisting of two parts that reflect the model's learned emotional biases at both fine-grained and coarse-grained levels. Emotions are divided into coarsegrained and fine-grained categories. Taking the IEMOCAP  [33]  dataset as an example, the fine-grained emotional labels are happy, excited, neutral, sad, angry, and frustrated. Among them, happy and excited ones are considered positive, neutral ones are still neutral, and others are negative, resulting in coarse-grained emotional labels.\n\nThe coarse-grained emotion loss function is: The fine-grained emotion loss function is:\n\nOur final training objective is:\n\nwhere N is the number of conversations, c(s) is the number of utterances in conversation s, P i,j is the probability distribution of the predicted emotion label for utterance j in conversation i,α is the coarse-grained loss weight.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluate our model on two benchmark datasets: IEMO-CAP  [33]  and MELD  [35] . These two datasets are designed for emotion recognition and contain three modalities: text, video, and audio.\n\nIEMOCAP consists of 10 hours of multimodal conversations performed by 10 actors. Each emotional conversation is carried out between two actors to simulate emotional communication in real-life situations. The dataset includes five emotion labels: Happy, Anger, Sadness, Neutral, and Excitement.\n\nMELD contains 1,430 dialogue segments from the TV show \"Friends,\" with each segment consisting of multiple dialogue turns. The dataset includes seven emotion labels: Anger, Disgust, Fear, Joy, Sadness, Surprise, and Neutral.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Hyperparameters",
      "text": "The experiments were conducted on an RTX 4090 GPU, with a batch size set to 32 and a total of 60 training epochs. The Adam optimizer was used with a learning rate of 0.005.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Comparison",
      "text": "We compared the performance of our proposed MERC-GCN framework with state-of-the-art MMGCN and other baseline methods as shown in the table I. On the IEMOCAP dataset, MERC-GCN achieved a new state-of-the-art accuracy of 68.98%, which is about 3% better than MMGCN and DialogueGCN, and at least 10% better than all other models, outperforming SOTA methods in three emotional dimensions. Similarly, on the MELD dataset, MERC-GCN achieved a weighted accuracy of 62.54% across four emotional dimensions, outperforming other baseline models. The reason for this gap lies in the inherent differences of the models. MERC-GCN, DialogueGCN, and DialogueRNN all attempt to extract speaker-level features, while other models usually focus solely on context information. Extensive research has shown that speaker-level features are crucial for emotion recognition tasks, which is why algorithms that focus on speaker-level information tend to outperform those that neglect it.\n\nRegarding the performance differences between MERC-GCN, DialogueGCN, and DialogueRNN, DialogueRNN uses Gated Recurrent Units (GRU) to extract speaker-level information, while DialogueGCN uses graph convolutional networks to overcome the issue of long sequence information propagation caused by the limitations of the recurrent encoder in DialogueRNN. We speculate that speaker-level information is often hidden in the interactions of the text, speech, and video modalities. Other algorithms only extract speaker-level information through text, which may result in insufficient use of all three modalities. This happens in real-world scenarios where there are inconsistencies between text and video at the speaker level, such as when the meaning conveyed by the text contrasts with the body language reflected in the video. In contrast, MERC-GCN extracts sufficient speakerlevel information across multiple modalities and conversation relationships through cross-modal attention, thus overcoming the issue of single-modality speaker-level extraction.\n\nMoreover, the standard deviations for DialogueGCN and DialogueRNN across different categories are 12.65 and 10.04, respectively, while our MERC-GCN has a standard deviation of only 7.83. This is due to the multi-task learning strategy, which merges categories or uses coarse-grained classification,  making the model's performance on each category more balanced during training. Fig.  3  presents the confusion matrix of our model on two datasets. It can be seen that our model has a high recognition accuracy and is not easily confused on the same coarse-grained task, thanks to the training strategy we adopted for multi-task learning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Ablation Study And Analysis",
      "text": "As shown in the table II,we conducted ablation experiments on different stages (i.e., cross-modal context fusion and adaptive graph convolutional encoder), as shown in the table. We found that the speaker-level encoder is slightly more important for overall performance. We speculate that relying solely on either cross-modal context fusion or the adaptive graph convolutional encoder may not fully capture the complexity of emotional expressions. The synergy of both components better models the emotions of different speakers, highlighting the importance of cross-modal context fusion and the adaptive graph convolutional encoder in dialogue emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Hyperparameter Optimization 1) Context Fusion Encoding Model:",
      "text": "We conducted ablation experiments on different context fusion models.As shown in Fig.  4 , when the context fusion model used our GRU module, both the F-score and accuracy were better than those using DialogueRNN and LSTM, with the F-score being approximately 15% higher than DialogueRNN and accuracy about 12% higher. Compared to LSTM and DialogueRNN, the gated units used in GRU can more effectively capture contextual information. The update and reset gates in GRU better control the flow of information. Furthermore, GRU's tolerance to noise  and precise control of information flow make it perform more effectively in dialogue emotion recognition tasks.\n\n2) Multi-task Learning Hyperparameter Optimization: We conducted a comparison experiment on different coarsegrained weights with respect to the learning rate, as shown in the Fig.  5 . On the IEMOCAP dataset, when the coarse-grained weight was set to 0.7, both the F-score and accuracy were optimal, while on the MELD dataset, the optimal parameter was 0.5. This difference may be due to the class imbalance in the datasets. In IEMOCAP, the samples for the Anger, Happy, and Sadness labels are relatively abundant, while in MELD, there are more samples for Anger and Happy. When coarse-grained classification is not used at all, the model tends to predict the larger classes in the training set, thereby lowering overall accuracy. Merging classes or applying coarsegrained classification helps to reduce the imbalance between categories, making the model's performance on each category more balanced during training. The model performs best when the dataset distribution is imbalanced, as it helps the model fit the true labels more accurately when updating weights.\n\n3) Modality Ablation Experiment: We conducted ablation experiments on different modalities of information, including individual modalities and pairs of combined modalities, as shown in the table III. The contribution of each modality to performance improvement varies, with the video modality making the greatest contribution, followed by audio, while the text modality has the least impact. For pairs of modalities, although theoretically they can achieve information complementarity, due to issues like information loss and modality alignment, the combination did not significantly improve performance and may have even caused interference. The model achieved the best performance when all three modalities were used together.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we proposed cross-Modal context fusion and adaptive graph convolutional neural networks for multimodal emotion recognition. The model learns cross-modal representations between pairs of three input modalities to achieve modality alignment and complementarity, enriching the input feature representation, and integrating them in the flow of contextual information. The dialogue relationship dependency graph is constructed based on the mutual and self-dependence between speakers, learning the dialogue relationship features between speakers. High detection performance was achieved on two benchmark ERC datasets.Future work. We will focus on designing more advanced feature fusion methods and integrating the semantic understanding capabilities of large language models to enhance the model's inference ability.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall framework of our proposed method.The model consists of three key steps: A. Cross-Modal Context",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) Cross-Modal Alignment Module (CAM). The in-",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrix.In a confusion matrix, each row",
      "page": 7
    },
    {
      "caption": "Figure 4: Accuracy and F-score comparisons with different",
      "page": 7
    },
    {
      "caption": "Figure 5: Effect of parameter α on F-score.On the IEMOCAP",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fjw@mail.nwpu.edu.cn": "Abstract—Emotion recognition has a wide\nrange of applica-",
          "fanxueyan@mail.nwpu.edu.cn": "noise due to mutual\ninterference, which can negatively impact"
        },
        {
          "fjw@mail.nwpu.edu.cn": "tions in human-computer interaction, marketing, healthcare, and",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "recognition\naccuracy.\nSecond,\nin\nconversational\nscenarios,"
        },
        {
          "fjw@mail.nwpu.edu.cn": "other fields.\nIn recent years,\nthe development of deep learning",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "these methods frequently overlook the bidirectional dependen-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "technology has provided new methods\nfor emotion recognition.",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "cies\nand intricate\nrelationships between speakers, which are"
        },
        {
          "fjw@mail.nwpu.edu.cn": "Prior\nto\nthis, many\nemotion\nrecognition methods\nhave\nbeen",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "proposed,\nincluding multimodal\nemotion recognition methods,",
          "fanxueyan@mail.nwpu.edu.cn": "crucial\nfor understanding emotional dynamics. Consequently,"
        },
        {
          "fjw@mail.nwpu.edu.cn": "but\nthese methods\nignore\nthe mutual\ninterference between dif-",
          "fanxueyan@mail.nwpu.edu.cn": "the inability to fully explore speaker relationships and dialogue"
        },
        {
          "fjw@mail.nwpu.edu.cn": "ferent\ninput modalities and pay little attention to the directional",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "context\nlimits the depth and effectiveness of current models in"
        },
        {
          "fjw@mail.nwpu.edu.cn": "dialogue\nbetween\nspeakers. Therefore,\nthis\narticle\nproposes\na",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "capturing emotional\ninteractions. Recently, diffusion models"
        },
        {
          "fjw@mail.nwpu.edu.cn": "new multimodal emotion recognition method,\nincluding a cross",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "[13]–[15] have shown promise in mitigating such challenges"
        },
        {
          "fjw@mail.nwpu.edu.cn": "modal\ncontext\nfusion module, an adaptive graph convolutional",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "encoding module, and an emotion classification module. The cross",
          "fanxueyan@mail.nwpu.edu.cn": "by leveraging progressive noise\nreduction to refine\nfeatures"
        },
        {
          "fjw@mail.nwpu.edu.cn": "modal context module includes a cross modal alignment module",
          "fanxueyan@mail.nwpu.edu.cn": "across multiple modalities. Their\nability\nto model\ncomplex"
        },
        {
          "fjw@mail.nwpu.edu.cn": "and\na\ncontext\nfusion module, which\nare\nused\nto\nreduce\nthe",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "dependencies and generate context-aware representations of-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "noise introduced by mutual\ninterference between different\ninput",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "fers\npotential\nadvantages\nfor\ncapturing\nintricate\nemotional"
        },
        {
          "fjw@mail.nwpu.edu.cn": "modalities. The adaptive graph convolution module\nconstructs",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "dynamics in multimodal and conversational settings."
        },
        {
          "fjw@mail.nwpu.edu.cn": "a dialogue\nrelationship graph for\nextracting dependencies and",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "self dependencies between speakers. Our model has\nsurpassed",
          "fanxueyan@mail.nwpu.edu.cn": "To address\nthese\nchallenges,\nthis paper proposes\na novel"
        },
        {
          "fjw@mail.nwpu.edu.cn": "some state-of-the-art methods on publicly available benchmark",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "multimodal\nemotion\nrecognition\nframework\nthat\nleverages"
        },
        {
          "fjw@mail.nwpu.edu.cn": "datasets and achieved high recognition accuracy.",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "cross-modal context\nfusion and adaptive graph convolutional"
        },
        {
          "fjw@mail.nwpu.edu.cn": "Index\nTerms—multimodal\nemotion\nrecognition,co-attention",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "networks to enhance performance. The proposed method con-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "transformer,graph convolutional network,multi-task learning",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "sists of\nthree key components: a cross-modal context\nfusion"
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "module,\nan\nadaptive\ngraph\nconvolutional\nencoding module,"
        },
        {
          "fjw@mail.nwpu.edu.cn": "I.\nINTRODUCTION",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "and an emotion classification module. The cross-modal context"
        },
        {
          "fjw@mail.nwpu.edu.cn": "Emotion Recognition in Conversation (ERC)\n[1]–[4], as a",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "fusion module reduces noise by aligning and integrating con-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "significant\nresearch area\nin artificial\nintelligence, holds\nim-",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "textual information across modalities, while the adaptive graph"
        },
        {
          "fjw@mail.nwpu.edu.cn": "mense application potential\nin fields such as human-computer",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "convolutional encoding module constructs a dialogue relation-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "interaction [5], marketing [6],\nand healthcare\n[7]. With the",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "ship graph to capture speaker dependencies and conversational"
        },
        {
          "fjw@mail.nwpu.edu.cn": "rapid advancement of deep learning technologies, ERC meth-",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "directionality. Finally,\nthe\nemotion classification module de-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "ods have witnessed remarkable innovation and progress\n[8].",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "codes\nthese\nenriched\nfeatures\nto\nclassify\nemotions. Experi-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "Among these, multimodal emotion recognition approaches [9],",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "mental results on publicly available ERC datasets demonstrate"
        },
        {
          "fjw@mail.nwpu.edu.cn": "which\nintegrate\ninformation\nfrom multiple modalities,\nhave",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "that\nthe proposed model outperforms state-of-the-art methods,"
        },
        {
          "fjw@mail.nwpu.edu.cn": "gained\nsignificant\nattention\ndue\nto\ntheir\nability\nto\ncompre-",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "offering a new perspective for advancing multimodal emotion"
        },
        {
          "fjw@mail.nwpu.edu.cn": "hensively\nand\naccurately\ncapture\nemotional\nexpressions.\nIn",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "recognition research. Our main contributions are summarized"
        },
        {
          "fjw@mail.nwpu.edu.cn": "everyday interactions,\nemotional\nexpressions\nare often con-",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "",
          "fanxueyan@mail.nwpu.edu.cn": "as follows:"
        },
        {
          "fjw@mail.nwpu.edu.cn": "veyed\nthrough\na\ncombination\nof modalities,\nincluding\nlan-",
          "fanxueyan@mail.nwpu.edu.cn": ""
        },
        {
          "fjw@mail.nwpu.edu.cn": "guage, facial expressions, and vocal tone. These modalities are",
          "fanxueyan@mail.nwpu.edu.cn": "• We\npropose\na\nnovel multimodal\nemotion\nrecognition"
        },
        {
          "fjw@mail.nwpu.edu.cn": "inherently\ncomplementary\nand\ninterdependent,\noffering\nrich",
          "fanxueyan@mail.nwpu.edu.cn": "framework that achieves state-of-the-art performance on"
        },
        {
          "fjw@mail.nwpu.edu.cn": "emotional context when combined.",
          "fanxueyan@mail.nwpu.edu.cn": "two widely used ERC benchmark datasets."
        },
        {
          "fjw@mail.nwpu.edu.cn": "Despite their promise, existing multimodal emotion recog-",
          "fanxueyan@mail.nwpu.edu.cn": "• We design\na\ncross-modal\nalignment module\nto reduce"
        },
        {
          "fjw@mail.nwpu.edu.cn": "nition methods [10]–[12]\nface notable challenges. First,\ninte-",
          "fanxueyan@mail.nwpu.edu.cn": "noise caused by mutual\ninterference between different in-"
        },
        {
          "fjw@mail.nwpu.edu.cn": "grating information from multiple modalities often introduces",
          "fanxueyan@mail.nwpu.edu.cn": "put modalities, improving the effectiveness of multimodal"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "fusion.": "• We\nintroduce\na multi-task learning-based loss\nfunction",
          "GCN is to generalize convolution operations from Euclidean": "space to graph structures.\nIn traditional CNNs, convolutional"
        },
        {
          "fusion.": "that enables the model\nto simultaneously handle coarse-",
          "GCN is to generalize convolution operations from Euclidean": "operations\nextract\nlocal\nfeatures\nvia\nsliding windows.\nIn"
        },
        {
          "fusion.": "grained and fine-grained emotion recognition tasks, en-",
          "GCN is to generalize convolution operations from Euclidean": "contrast, GCN performs\nfeature\naggregation\nby\ncombining"
        },
        {
          "fusion.": "hancing its overall performance.",
          "GCN is to generalize convolution operations from Euclidean": "information from neighboring nodes in the graph. Specifically,"
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "GCN updates\neach node’s\nrepresentation by applying a\nlin-"
        },
        {
          "fusion.": "II. RELATED WORK",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "ear\ncombination\nof\nits\nfeatures\nand\nthose\nof\nits\nneighbors,"
        },
        {
          "fusion.": "A. Emotion Recognition in Conversation",
          "GCN is to generalize convolution operations from Euclidean": "weighted\nby\na\nlearnable matrix. GCN’s\nstrengths\ninclude"
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "parameter sharing, adaptive aggregation, node embedding rep-"
        },
        {
          "fusion.": "With the widespread use of social media and smart devices,",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "resentation, and enhanced predictive capabilities. GAT intro-"
        },
        {
          "fusion.": "a vast\namount of data\nis generated in daily life,\nincluding",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "duces\nan attention mechanism to GNNs,\nassigning different"
        },
        {
          "fusion.": "text,\nimages,\nand audio. These data\ncontain rich emotional",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "weights\nto neighbor nodes during feature\naggregation. This"
        },
        {
          "fusion.": "information,\nsuch as emotional\nstates,\nreactions, and expres-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "mechanism dynamically adjusts\nthe importance of neighbors"
        },
        {
          "fusion.": "sions. Consequently, Emotion Recognition in Conversations",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "based\non\ntheir\nconnections,\nenabling GAT to\nbetter\nadapt"
        },
        {
          "fusion.": "(ERC) has become an important\nresearch area. ERC can be",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "to diverse graph structures\nand capture\na broader\nrange of"
        },
        {
          "fusion.": "applied\nnot\nonly\nin\nnatural\nlanguage\nprocessing,\ncomputer",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "information. This flexibility makes GAT particularly effective"
        },
        {
          "fusion.": "vision,\nand\nspeech\nrecognition\nbut\nalso\nprovides\neffective",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "in scenarios where certain nodes contribute more significantly"
        },
        {
          "fusion.": "solutions for human-computer\ninteraction, sentiment analysis,",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "to the task at hand. GraphSAGE, on the other hand, adopts"
        },
        {
          "fusion.": "and public opinion monitoring. With the advancement of deep",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "a\nsampling-based\napproach\nto\nefficiently\nlearn\nnode\nrepre-"
        },
        {
          "fusion.": "learning technologies, numerous ERC methods based on deep",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "sentations.\nInstead\nof\naggregating\nall\nneighbor\nnodes\nlike"
        },
        {
          "fusion.": "learning have\nemerged. A model based on LSTM [16] was",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "GCN, GraphSAGE samples a fixed number of neighbors and"
        },
        {
          "fusion.": "proposed to capture contextual information from the surround-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "aggregates their features to approximate the global structure of"
        },
        {
          "fusion.": "ing environment within the\nsame video,\naiding the\nclassifi-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "the graph. This approach significantly reduces computational"
        },
        {
          "fusion.": "cation process. The CMN [17]\nconversational memory net-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "complexity, making\nit\nsuitable\nfor\nlarge-scale\ngraph\ndata."
        },
        {
          "fusion.": "work was introduced,\nleveraging contextual\ninformation from",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "By\nallowing\ndifferent\nsampling\nstrategies\nand\naggregation"
        },
        {
          "fusion.": "conversational history. This framework employs a multimodal",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "methods, GraphSAGE can adapt\nto various graph structures"
        },
        {
          "fusion.": "approach,\nincluding audio, visual, and textual\nfeatures, with",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "and capture richer contextual\ninformation."
        },
        {
          "fusion.": "gated recurrent units to model each speaker’s past utterances as",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "memories. These memories are then merged through attention-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "III. METHOD"
        },
        {
          "fusion.": "based\njumps\nto\ncapture\ndependencies\nbetween\nspeakers. A",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "The\nproposed model,named MERC-GCN,is\ndesigned\nfor"
        },
        {
          "fusion.": "DialogueRNN [18] model based on recurrent neural networks",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "multimodal emotion recognition in conversations. The model"
        },
        {
          "fusion.": "was developed to track the states of various parties throughout",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "consists of\nthree steps: cross-modal context\nfusion, adaptive"
        },
        {
          "fusion.": "the conversation and use this information for emotion classifi-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "graph convolutional encoding, and emotion classification. The"
        },
        {
          "fusion.": "cation. The DialogueGCN [19], a graph convolutional neural",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "overall\nframework is illustrated in Fig.1."
        },
        {
          "fusion.": "network-based ERC method, was\nfirst\nproposed,\nfocusing",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "solely\non\ntextual\nfeatures. A new model, MMGCN [20],",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "A. Problem Definition"
        },
        {
          "fusion.": "based on multimodal fusion graph convolutional networks, was",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "Assume there are M speakers\nin a conversation, with the"
        },
        {
          "fusion.": "introduced, which can effectively utilize multimodal depen-",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "dencies and model dependencies between and within speakers.",
          "GCN is to generalize convolution operations from Euclidean": "sequence of utterances represented as u1, u2, · · ·\n, uN , where"
        },
        {
          "fusion.": "However,\nthis direct\nfusion approach may lead to redundant",
          "GCN is to generalize convolution operations from Euclidean": "each utterance ui\nis spoken by speaker ps(ui). Each utterance"
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": ", uA\n, uT\ncontains three emotional modalities uV\n, where V , A,"
        },
        {
          "fusion.": "information and loss of heterogeneous information.",
          "GCN is to generalize convolution operations from Euclidean": "i\ni\ni"
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "and T represent\ninformation from visual, audio, and textual"
        },
        {
          "fusion.": "B. Graph Neural Network",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "sources,\nrespectively. Our\ntask\nis\nto\npredict\nthe\nemotional"
        },
        {
          "fusion.": "Convolutional neural networks\n(CNNs) have been widely",
          "GCN is to generalize convolution operations from Euclidean": "of\nthe\nspeaker\ncorresponding to each utterance\ncategory yi"
        },
        {
          "fusion.": "used\nfor\nextracting\nimage\nfeatures\n[12]. However, CNNs",
          "GCN is to generalize convolution operations from Euclidean": "ui."
        },
        {
          "fusion.": "exhibit\ninherent\nlimitations when\nhandling\ngraph-structured",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "",
          "GCN is to generalize convolution operations from Euclidean": "B. Preprocessing: Unimodal Feature Extraction"
        },
        {
          "fusion.": "data, as they are primarily designed for Euclidean space data.",
          "GCN is to generalize convolution operations from Euclidean": ""
        },
        {
          "fusion.": "To address\nthese challenges, graph neural networks\n(GNNs)",
          "GCN is to generalize convolution operations from Euclidean": "Text Modality: RoBERTa [26]\nis a variant of BERT [27]"
        },
        {
          "fusion.": "[21],\n[22] have\nemerged as\na powerful\nalternative,\nenabling",
          "GCN is to generalize convolution operations from Euclidean": "that employs more efficient pre-training methods, making it a"
        },
        {
          "fusion.": "effective\nlearning\nand\ninference\nin\nnon-Euclidean\ndomains.",
          "GCN is to generalize convolution operations from Euclidean": "more robust pre-trained language model\nthan BERT.\nIn this"
        },
        {
          "fusion.": "Unlike traditional deep learning models, which focus on pro-",
          "GCN is to generalize convolution operations from Euclidean": "paper, RoBERTa\nis used to encode\ntext\ninformation into a"
        },
        {
          "fusion.": "cessing vectors and matrices, GNNs leverage the topological",
          "GCN is to generalize convolution operations from Euclidean": "200-dimensional\nfeature vector. All\ntext\nfeatures are denoted"
        },
        {
          "fusion.": "structure of graphs\nand the\nrelationships between nodes\nto",
          "GCN is to generalize convolution operations from Euclidean": "as U T ."
        },
        {
          "fusion.": "capture complex dependencies.",
          "GCN is to generalize convolution operations from Euclidean": "Audio Modality:\nopenSMILE [28]\n(open-source Speech"
        },
        {
          "fusion.": "Several GNN architectures have been proposed,\nincluding",
          "GCN is to generalize convolution operations from Euclidean": "and Music\nInterpretation\nby Large-space Extraction)\nis\nan"
        },
        {
          "fusion.": "GCN [23], GraphSAGE [24],\nand GAT [25],\neach offering",
          "GCN is to generalize convolution operations from Euclidean": "open-source toolkit\nfor audio feature extraction and classifi-"
        },
        {
          "fusion.": "unique approaches to graph-based learning. The core idea of",
          "GCN is to generalize convolution operations from Euclidean": "cation\nof\nspeech\nand music\nsignals.\nopenSMILE is widely"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "1h"
        },
        {
          "A.Cross-modal Context Fusion": "+\n1g\nGRU\n1f\nCAM",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "GCN"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "2h\n4g"
        },
        {
          "A.Cross-modal Context Fusion": "+\n2g\nGRU\n2f\nCAM",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "Drop-message"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "+\n3g\n3f\nCAM\nGRU",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "+\n4g\nGRU\n4f\nCAM",
          "B.Adaptive Graph Convolution Encoding": "Coarse-grained"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "Recognition"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "FC\n+"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": ""
        },
        {
          "A.Cross-modal Context Fusion": "5g\n+\nGRU\n5f\nCAM",
          "B.Adaptive Graph Convolution Encoding": "p1 "
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "Fine-grained"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "p \n2"
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "Recognition\np1 \n["
        },
        {
          "A.Cross-modal Context Fusion": "",
          "B.Adaptive Graph Convolution Encoding": "Emotions"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attention\n+\nFC\n+": "ih"
        },
        {
          "Attention\n+\nFC\n+": "5g\np1 \n+\nGRU\n5f\nCAM"
        },
        {
          "Attention\n+\nFC\n+": "Fine-grained"
        },
        {
          "Attention\n+\nFC\n+": "Towards past\np "
        },
        {
          "Attention\n+\nFC\n+": "2"
        },
        {
          "Attention\n+\nFC\n+": "Towards future\nRecognition\n]\n[\np1 \n]\n["
        },
        {
          "Attention\n+\nFC\n+": "Emotions"
        },
        {
          "Attention\n+\nFC\n+": "Concatenation\n+"
        },
        {
          "Attention\n+\nFC\n+": "p \nFeatures"
        },
        {
          "Attention\n+\nFC\n+": "2\nMulti-task Learning"
        },
        {
          "Attention\n+\nFC\n+": "Fig.\n1: Overall\nframework\nof\nour\nproposed method.The model\nconsists\nof\nthree\nkey\nsteps: A. Cross-Modal Context"
        },
        {
          "Attention\n+\nFC\n+": "Fusion. Initially,\nthe extracted features are processed through the Cross-Modal Alignment Module (CAM) to obtain enhanced"
        },
        {
          "Attention\n+\nFC\n+": "information between modalities. After\nfusion,\nthe features are further\nintegrated with a bidirectional GRU to achieve deeper"
        },
        {
          "Attention\n+\nFC\n+": "contextual feature fusion. B. Adaptive Graph Convolutional Encoding. In this step, speakers are modeled as a graph structure"
        },
        {
          "Attention\n+\nFC\n+": "based on their conversational\nrelationships. By processing through drop-message and graph convolutional network,\nthe model"
        },
        {
          "Attention\n+\nFC\n+": "effectively extracts dependencies among speakers and the directionality of\nthe conversation. C. Emotion Classification. The"
        },
        {
          "Attention\n+\nFC\n+": "encoded features are decoded and mapped to the dimensions of classification labels in this step. The model employs a multi-"
        },
        {
          "Attention\n+\nFC\n+": "task learning training paradigm, with the loss\nfunction being the sum of\nlosses\nfor coarse-grained and fine-grained emotion"
        },
        {
          "Attention\n+\nFC\n+": "classification."
        },
        {
          "Attention\n+\nFC\n+": "CT block consists of\ntwo identical parts,\nleft and right, with\nused in affective computing for automatic emotion recognition."
        },
        {
          "Attention\n+\nFC\n+": "symmetrical\ninput.\nIn\nthe\nleft\npart,\none\ninput modality\nis\nopenSMILE performs\nthe\nfollowing\nfour\ntypes\nof\nfeature"
        },
        {
          "Attention\n+\nFC\n+": "used as\nthe query, while\nthe other modality is used as\nthe\nextraction operations: signal processing, data processing, audio"
        },
        {
          "Attention\n+\nFC\n+": "key and value, with the\nlatter weighted and summed under\nfeatures (low-level), and functionals. In this paper, openSMILE"
        },
        {
          "Attention\n+\nFC\n+": "is used to encode audio information into a 100-dimensional\nthe guidance of\nthe former. The right part of\nthe CT block"
        },
        {
          "Attention\n+\nFC\n+": "undergoes a symmetrical process\nsimultaneously. This entire\nfeature vector. All audio features are denoted as U A."
        },
        {
          "Attention\n+\nFC\n+": "process\nrepeats T\ntimes, outputting the mutual\ncross-modal\nVisual Modality: DenseNet [29] is a type of CNN network"
        },
        {
          "Attention\n+\nFC\n+": "representations of\nthe two input modalities.\nwhose basic concept\nis similar\nto ResNet\n[30] but establishes"
        },
        {
          "Attention\n+\nFC\n+": "dense\nconnections between all preceding layers\nand subse-\nCo-attention transformer reduces the semantic gap between"
        },
        {
          "Attention\n+\nFC\n+": "quent layers, enabling feature reuse through connections across\nmodalities and enhances shared features between them, achiev-"
        },
        {
          "Attention\n+\nFC\n+": "channels. CNN networks are better suited for capturing image\ning modality alignment and reducing noise in the input modal-"
        },
        {
          "Attention\n+\nFC\n+": "features, and in this paper, DenseNet\nis used to encode video\nities. The entire process is mathematically represented as:"
        },
        {
          "Attention\n+\nFC\n+": "information into a 100-dimensional\nfeature vector. All video"
        },
        {
          "Attention\n+\nFC\n+": "features are denoted as U V ."
        },
        {
          "Attention\n+\nFC\n+": "(1)\nMultiHead(Q, K, V ) = (head1 ⊕ · · · ⊕ headh)W O,"
        },
        {
          "Attention\n+\nFC\n+": "C. Method"
        },
        {
          "Attention\n+\nFC\n+": ",\n,\n,\n(2)\nVi = V W V\nKi = KW K\nQi = QW Q"
        },
        {
          "Attention\n+\nFC\n+": "1) Cross-modal Context Fusion: Different modalities at the"
        },
        {
          "Attention\n+\nFC\n+": "same time have correlations. If these are directly concatenated\n(cid:19)"
        },
        {
          "Attention\n+\nFC\n+": "(cid:18) QiK T"
        },
        {
          "Attention\n+\nFC\n+": "i√\n(3)\nheadi = Att(Qi, Ki, Vi) = softmax\nVi.\nas\ninput\nfeatures\nto the network,\nthe network might confuse"
        },
        {
          "Attention\n+\nFC\n+": "dh"
        },
        {
          "Attention\n+\nFC\n+": "the correlations between different modal\nfeatures. Therefore,"
        },
        {
          "Attention\n+\nFC\n+": "this paper uses a co-attention transformer\n(CT)\n[31]for cross-\n∈\nHere, ⊕ denotes\nthe\nconcatenation operation. Q, K, V"
        },
        {
          "Attention\n+\nFC\n+": "RL×dmodel"
        },
        {
          "Attention\n+\nFC\n+": "modal\nenhancement\nto learn distinct\ncross-modal\ncorrelated\nrepresent\ntwo of the input modalities U A, U V , U T ,"
        },
        {
          "Attention\n+\nFC\n+": "features.\nL\nas\npreviously\ndescribed.\nis\nthe\nlength\nof\nthe\ninput"
        },
        {
          "Attention\n+\nFC\n+": "∈\nAs\nshown in the Fig.2,\neach CT learns\ncross-modal\nrep-\nfeature\nvector\nof\nthe\ncorresponding\nmodality. W O"
        },
        {
          "Attention\n+\nFC\n+": "Rhdh×dmodel , W Q\n, W K\n, W V\n∈ Rdmodel×dh\nare learnable hy-\nresentations between two modalities;\nthus,\nthree co-attention"
        },
        {
          "Attention\n+\nFC\n+": "i\ni\ni"
        },
        {
          "Attention\n+\nFC\n+": "transformers are required to learn cross-modal representations\nperparameters. dmodel and h are inherent hyperparameters of"
        },
        {
          "Attention\n+\nFC\n+": "for each pair of\nthe three modalities\nin the ERC task. Each\nthe model, and in this paper, h = 8 and dh = dmodel/h = 64."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "achieving\nenhanced\nrepresentation. This\nentire\nstep\ncan\nbe"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "described mathematically as follows:"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "ET −A, EA−T = CT (U T , U A),\n(5)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "EV −A, EA−V = CT (U A, U V ),\n(6)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "ET −V , EV −T = CT (U T , U V ).\n(7)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "CT represents\nthe co-attention transformer, constructed by"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "T stacked co-attention transformer blocks. ET −A denotes the"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "cross-modal representation of the text modality relative to the"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "visual modality, and so on."
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "We concatenate the learned cross-modal correlated features"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "with the original features to prepare for the next step of context"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "feature fusion. This is mathematically represented as:"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "F = [ET −A, EA−T , EV −A, EA−V ,"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "(8)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "ET −V , EV −T , U T , U A, U V ]."
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "For\nthe i-th utterance,\nthe features it carries are denoted as"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "fi, so:"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "(9)\nF = [f1, f2, . . . , fN ]."
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "Conversations occur sequentially, with contextual\ninforma-"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "tion flowing along this sequence. Based on this characteristic,"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "we constructed a bidirectional gated recurrent unit\n(BiGRU)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "[32]\nto\ncapture\ncontextual\ninformation. The\ninput modality"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "features\ninclude both the original modality features\nand the"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "cross-modal correlated features, achieving fusion and interac-"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "tion within the flow of contextual\ninformation. The specific"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "mathematical\nformula is as follows:"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "(cid:105)\n(cid:104)−−→\n←−−"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ".\n(10)\ngi =\nGRU(fi, gi−1),\nGRU(fi, gi+1)"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": ""
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "Here,\nrepresents\nthe\nfeature\nafter\nsequential\ncontext\ngi"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "fusion. This\nstep\nintegrates\nsequential\ncontextual modality"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "features but does not yet account for speaker identity and inter-"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "speaker dependencies. These aspects will be considered in the"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "next step."
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "2) Adaptive Graph Convolution Encoding: We constructed"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "a graph convolutional neural network to encode the relation-"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "ships between speakers,\nthereby capturing both inter-speaker"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "dependencies and self-dependencies."
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "First, we define\nthe\nfollowing symbols: based on a\nsce-"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "nario with N utterances, we construct a directed graph G ="
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "(V, E, R, W ), where nodes vi ∈ V\nand rij ∈ R represent a"
        },
        {
          "of\nthe previous CT block serving as\nthe\ninput\nto the next,": "directed edge from node vi\nto node vj, and αij ∈ W represents"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "past context window size p and the future context window size": "f ,\nthe weight calculation is as follows:",
          "In the second step, we apply GCN again to extract relation-": "ship features between vertices,\nreinforcing the\nextraction of"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "features that capture the conversational\nrelationships between"
        },
        {
          "past context window size p and the future context window size": "αij = softmax (cid:0)gT\ni We[gi−p, . . . , gi+f ](cid:1) ,",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "speakers:"
        },
        {
          "past context window size p and the future context window size": "(11)",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "for j = i − p, . . . , i + f.",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": " \n \n(cid:88)"
        },
        {
          "past context window size p and the future context window size": "This ensures that the total weight contribution of the incom-",
          "In the second step, we apply GCN again to extract relation-": "W (2)h(1)\n+ W (2)\nh(2)\n=σ\nh(1)\n,"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "0\nj\ni\ni"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "(15)"
        },
        {
          "past context window size p and the future context window size": "sums to\ning edges for vertex vi\nfrom vertices vi−p, · · ·\n, vi+f",
          "In the second step, we apply GCN again to extract relation-": "j∈N r"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "i"
        },
        {
          "past context window size p and the future context window size": "1. Different weight values\nrepresent\nthe varying influence of",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "i = 1, 2, . . . , N.\nfor"
        },
        {
          "past context window size p and the future context window size": "the corresponding vertices.",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "are learnable parameters, and σ is an\nwhere Wc\nand W0"
        },
        {
          "past context window size p and the future context window size": "2. Graph Representation Learning",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "activation function."
        },
        {
          "past context window size p and the future context window size": "Before this step,\nthe feature g is a multimodal\nfusion feature",
          "In the second step, we apply GCN again to extract relation-": "This step constructs a graph model of conversational\nrela-"
        },
        {
          "past context window size p and the future context window size": "independent of speaker relationships,\nincluding text semantics",
          "In the second step, we apply GCN again to extract relation-": "tionships between speakers, building upon the previous step’s"
        },
        {
          "past context window size p and the future context window size": "and real-time\nrepresentations of\naudio and video. Next, we",
          "In the second step, we apply GCN again to extract relation-": "cross-modal\ncontext\nfeature\nfusion to capture\nthe\nconversa-"
        },
        {
          "past context window size p and the future context window size": "use\na\ngraph\nconvolutional\nnetwork\nto\nperform a\ntwo-step",
          "In the second step, we apply GCN again to extract relation-": "tional\nrelationship features between speakers."
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "3) Emotion Classification: We\nfuse\nthe\ncontext\nencoding"
        },
        {
          "past context window size p and the future context window size": "feature transformation to extract representations of connections",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "vector with the speaker encoding vector and use an attention"
        },
        {
          "past context window size p and the future context window size": "between speakers.",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "mechanism to learn the importance of different\nfeatures:"
        },
        {
          "past context window size p and the future context window size": "In the first\nstep, we use one\nlayer of GCN to aggregate",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "neighborhood information of vertices,\nthereby initially encod-",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "],\n(16)\nhi = [gi, h(2)"
        },
        {
          "past context window size p and the future context window size": "ing the directional nature of conversations between speakers.",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "In this step, we use the DropMessage method to enhance the",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "aggregation capability of GCN. We generate a mask matrix of",
          "In the second step, we apply GCN again to extract relation-": "(17)\nβi = softmax (cid:0)hT\ni Wβ[h1, h2, . . . , hN ](cid:1) ,"
        },
        {
          "past context window size p and the future context window size": "the\nsame\nsize\nas\nthe message matrix based on a Bernoulli",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "distribution, where\neach\nelement\nin\nthe message matrix\nis",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "dropped to a certain extent as determined by the corresponding",
          "In the second step, we apply GCN again to extract relation-": "(18)\nhi = βi[h1, h2, . . . , hN ]T ."
        },
        {
          "past context window size p and the future context window size": "value\nin the mask matrix. After\napplying dropmessage,\nthe",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "Finally, we\nfeed\nthe\nresulting\nfeatures\ninto\nan MLP for"
        },
        {
          "past context window size p and the future context window size": "mathematical\nformula for\nthe node features is:",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "decoding. The\nsoftmax\nfunction\noutputs\nthe final\npredicted"
        },
        {
          "past context window size p and the future context window size": "(cid:40)",
          "In the second step, we apply GCN again to extract relation-": "probability distribution for each class, and we select\nthe label"
        },
        {
          "past context window size p and the future context window size": "vi ∈ VM ,\ng[M ],",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "(12)\ngi =",
          "In the second step, we apply GCN again to extract relation-": "corresponding\nto\nthe\nhighest\nprobability\nas\nthe\nprediction"
        },
        {
          "past context window size p and the future context window size": "gi,\nvi /∈ VM .",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "result:"
        },
        {
          "past context window size p and the future context window size": "represents the\nwhere VM denotes the masked nodes, g[M ]",
          "In the second step, we apply GCN again to extract relation-": "(cid:16)\n(cid:17)"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": "˜"
        },
        {
          "past context window size p and the future context window size": "",
          "In the second step, we apply GCN again to extract relation-": ",\n(19)\nli = ReLU\nWl\nhi + bl"
        },
        {
          "past context window size p and the future context window size": "feature vector of\nthe masked nodes, and ˜g[M ]\nrepresents the",
          "In the second step, we apply GCN again to extract relation-": ""
        },
        {
          "past context window size p and the future context window size": "updated node features.",
          "In the second step, we apply GCN again to extract relation-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": ""
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": ""
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "Happy"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "32.63"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "30.38"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "29.91"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "33.18"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "47.10"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "45.45"
        },
        {
          "TABLE I: Performance comparison on IEMOCAP and MELD datasets for different emotion recognition models.": "68.90"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The fine-grained emotion loss function is:": "c(i)",
          "V. EXPERIMENTAL RESULTS": "A. Comparison"
        },
        {
          "The fine-grained emotion loss function is:": "1",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "N(cid:88) i\n(cid:88) j\n(23)\nLF = −\nlog Pi,j[yF\nj,i].",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "We\ncompared\nthe\nperformance\nof\nour\nproposed MERC-"
        },
        {
          "The fine-grained emotion loss function is:": "N(cid:80) s\n=1\n=1\nc(s)",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "GCN framework with\nstate-of-the-art MMGCN and\nother"
        },
        {
          "The fine-grained emotion loss function is:": "=1",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "baseline methods as shown in the table I. On the IEMOCAP"
        },
        {
          "The fine-grained emotion loss function is:": "Our final\ntraining objective is:",
          "V. EXPERIMENTAL RESULTS": "dataset, MERC-GCN achieved a new state-of-the-art accuracy"
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "of\n68.98%, which\nis\nabout\n3% better\nthan MMGCN and"
        },
        {
          "The fine-grained emotion loss function is:": "L = αLC + (1 − α)LF + λ∥θ∥",
          "V. EXPERIMENTAL RESULTS": "DialogueGCN, and at\nleast 10% better\nthan all other models,"
        },
        {
          "The fine-grained emotion loss function is:": "c(i)",
          "V. EXPERIMENTAL RESULTS": "outperforming SOTA methods in three emotional dimensions."
        },
        {
          "The fine-grained emotion loss function is:": "1",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "N(cid:88) i\n(cid:88) j\n= −\n(cid:8)α log Pi,j[yC\ni,j]",
          "V. EXPERIMENTAL RESULTS": "Similarly,\non\nthe MELD dataset, MERC-GCN achieved\na"
        },
        {
          "The fine-grained emotion loss function is:": "(24)",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "weighted accuracy of 62.54% across\nfour emotional dimen-"
        },
        {
          "The fine-grained emotion loss function is:": "N(cid:80) s\n=1\n=1\nc(s)",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "=1",
          "V. EXPERIMENTAL RESULTS": "sions, outperforming other baseline models. The\nreason for"
        },
        {
          "The fine-grained emotion loss function is:": "+ (1 − α) log Pi,j[yF\ni,j](cid:9) + λ∥θ∥.",
          "V. EXPERIMENTAL RESULTS": "this gap lies in the inherent differences of the models. MERC-"
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "GCN, DialogueGCN, and DialogueRNN all attempt\nto extract"
        },
        {
          "The fine-grained emotion loss function is:": "where N is the number of conversations, c(s) is the num-",
          "V. EXPERIMENTAL RESULTS": "speaker-level features, while other models usually focus solely"
        },
        {
          "The fine-grained emotion loss function is:": "ber\nof\nutterances\nin\nconversation\nis\nthe\nprobability\ns, Pi,j",
          "V. EXPERIMENTAL RESULTS": "on\ncontext\ninformation. Extensive\nresearch\nhas\nshown\nthat"
        },
        {
          "The fine-grained emotion loss function is:": "distribution of\nthe predicted emotion label\nfor utterance j\nin",
          "V. EXPERIMENTAL RESULTS": "speaker-level\nfeatures\nare\ncrucial\nfor\nemotion\nrecognition"
        },
        {
          "The fine-grained emotion loss function is:": "conversation i,α is the coarse-grained loss weight.",
          "V. EXPERIMENTAL RESULTS": "tasks, which is why algorithms\nthat\nfocus on speaker-level"
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "information tend to outperform those that neglect\nit."
        },
        {
          "The fine-grained emotion loss function is:": "IV. EXPERIMENTAL SETUP",
          "V. EXPERIMENTAL RESULTS": "Regarding\nthe\nperformance\ndifferences\nbetween MERC-"
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "GCN, DialogueGCN, and DialogueRNN, DialogueRNN uses"
        },
        {
          "The fine-grained emotion loss function is:": "A. Datasets",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "Gated Recurrent Units (GRU) to extract speaker-level informa-"
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "tion, while DialogueGCN uses graph convolutional networks"
        },
        {
          "The fine-grained emotion loss function is:": "We evaluate our model on two benchmark datasets: IEMO-",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "to overcome\nthe\nissue of\nlong sequence\ninformation propa-"
        },
        {
          "The fine-grained emotion loss function is:": "CAP [33] and MELD [35]. These two datasets are designed for",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "gation caused by the limitations of\nthe recurrent encoder\nin"
        },
        {
          "The fine-grained emotion loss function is:": "emotion recognition and contain three modalities:\ntext, video,",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "DialogueRNN. We\nspeculate\nthat\nspeaker-level\ninformation"
        },
        {
          "The fine-grained emotion loss function is:": "and audio.",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "is often hidden in the\ninteractions of\nthe\ntext,\nspeech,\nand"
        },
        {
          "The fine-grained emotion loss function is:": "IEMOCAP consists of 10 hours of multimodal conversa-",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "video modalities. Other algorithms only extract speaker-level"
        },
        {
          "The fine-grained emotion loss function is:": "tions performed by 10 actors. Each emotional\nconversation",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "information through text, which may result\nin insufficient use"
        },
        {
          "The fine-grained emotion loss function is:": "is\ncarried\nout\nbetween\ntwo\nactors\nto\nsimulate\nemotional",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "of all\nthree modalities. This happens\nin real-world scenarios"
        },
        {
          "The fine-grained emotion loss function is:": "communication\nin\nreal-life\nsituations. The\ndataset\nincludes",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "where\nthere\nare\ninconsistencies\nbetween\ntext\nand\nvideo\nat"
        },
        {
          "The fine-grained emotion loss function is:": "five\nemotion\nlabels: Happy, Anger,\nSadness, Neutral,\nand",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "the\nspeaker\nlevel,\nsuch\nas when\nthe meaning\nconveyed\nby"
        },
        {
          "The fine-grained emotion loss function is:": "Excitement.",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "the\ntext\ncontrasts with\nthe\nbody\nlanguage\nreflected\nin\nthe"
        },
        {
          "The fine-grained emotion loss function is:": "MELD contains\n1,430\ndialogue\nsegments\nfrom the TV",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "video.\nIn\ncontrast, MERC-GCN extracts\nsufficient\nspeaker-"
        },
        {
          "The fine-grained emotion loss function is:": "show ”Friends,” with\neach\nsegment\nconsisting\nof multiple",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "level\ninformation across multiple modalities and conversation"
        },
        {
          "The fine-grained emotion loss function is:": "dialogue\nturns. The\ndataset\nincludes\nseven\nemotion\nlabels:",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "relationships through cross-modal attention,\nthus overcoming"
        },
        {
          "The fine-grained emotion loss function is:": "Anger, Disgust, Fear, Joy, Sadness, Surprise, and Neutral.",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "the issue of single-modality speaker-level extraction."
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "Moreover,\nthe\nstandard deviations\nfor DialogueGCN and"
        },
        {
          "The fine-grained emotion loss function is:": "B. Hyperparameters",
          "V. EXPERIMENTAL RESULTS": ""
        },
        {
          "The fine-grained emotion loss function is:": "",
          "V. EXPERIMENTAL RESULTS": "DialogueRNN across different categories are 12.65 and 10.04,"
        },
        {
          "The fine-grained emotion loss function is:": "The\nexperiments were\nconducted on an RTX 4090 GPU,",
          "V. EXPERIMENTAL RESULTS": "respectively, while our MERC-GCN has a standard deviation"
        },
        {
          "The fine-grained emotion loss function is:": "with a batch size set\nto 32 and a total of 60 training epochs.",
          "V. EXPERIMENTAL RESULTS": "of only 7.83. This\nis due to the multi-task learning strategy,"
        },
        {
          "The fine-grained emotion loss function is:": "The Adam optimizer was used with a learning rate of 0.005.",
          "V. EXPERIMENTAL RESULTS": "which merges categories or uses coarse-grained classification,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the": "predicted class.",
          "actual": "",
          "class,": "",
          "and each column represents": ""
        },
        {
          "the": "",
          "actual": "",
          "class,": "TABLE II: Ablation Study.",
          "and each column represents": ""
        },
        {
          "the": "Module A",
          "actual": "",
          "class,": "Module B",
          "and each column represents": "F-score"
        },
        {
          "the": "✗",
          "actual": "",
          "class,": "✗",
          "and each column represents": ""
        },
        {
          "the": "",
          "actual": "",
          "class,": "",
          "and each column represents": "38.52"
        },
        {
          "the": "✗",
          "actual": "",
          "class,": "✓",
          "and each column represents": ""
        },
        {
          "the": "",
          "actual": "",
          "class,": "",
          "and each column represents": "66.25"
        },
        {
          "the": "✓",
          "actual": "",
          "class,": "✗",
          "and each column represents": ""
        },
        {
          "the": "",
          "actual": "",
          "class,": "",
          "and each column represents": "65.69"
        },
        {
          "the": "✓",
          "actual": "",
          "class,": "✓",
          "and each column represents": ""
        },
        {
          "the": "",
          "actual": "",
          "class,": "",
          "and each column represents": "68.98"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "binations"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "Modality\nF-score\nAcc"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "T\n65.31\n65.41"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "V\n67.31\n67.33"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "A\n66.30\n66.50"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "T-V\n65.32\n65.35"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "T-A\n65.87\n66.24"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "A-V\n65.66\n65.92"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "68.98\n69.18\nT-A-V"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "although theoretically they can achieve information comple-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "mentarity, due\nto issues\nlike\ninformation loss\nand modality"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "alignment,\nthe combination did not significantly improve per-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "formance and may have even caused interference. The model"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "achieved the best performance when all\nthree modalities were"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "used together."
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "VI. CONCLUSION"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "In this paper, we proposed cross-Modal context\nfusion and"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "adaptive graph convolutional neural networks for multimodal"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "emotion recognition. The model\nlearns cross-modal\nrepresen-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "tations\nbetween\npairs\nof\nthree\ninput modalities\nto\nachieve"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "modality alignment and complementarity, enriching the input"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "feature\nrepresentation,\nand\nintegrating\nthem in\nthe flow of"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "contextual\ninformation. The dialogue relationship dependency"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "graph is constructed based on the mutual and self-dependence"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "between speakers,\nlearning the dialogue relationship features"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "between speakers. High detection performance was achieved"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "on two benchmark ERC datasets.Future work. We will focus"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "on\ndesigning more\nadvanced\nfeature\nfusion methods\nand"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "integrating\nthe\nsemantic\nunderstanding\ncapabilities\nof\nlarge"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "language models to enhance the model’s inference ability."
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "REFERENCES"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[1] X. Li, Z. Yang, Z. Li, and Y. Li, “Erc dmsp: Emotion recognition in"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "conversation based on dynamic modeling of\nspeaker personalities,” in"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "2024 International Joint Conference on Neural Networks (IJCNN), 2024,"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "pp. 1–10."
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[2]\nL. Ge,\nF. Huang, Q. Li,\nand Y. Ye,\n“Modeling\nsentiment-speaker-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "dependency for\nemotion recognition in conversation,”\nin 2024 Inter-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "national Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8."
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[3]\nF. Xu, G. Li, Z. Zhong, Y. Zhou, and W. Zhou, “D-man: a distance-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "based multi-channel attention network for erc,” in 2024 International"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8."
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[4]\nF. Xu, T. Sun, W. Zhou, Z. Yu, and J. Lu, “Ctf-erc: Coarse-to-fine rea-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": ""
        },
        {
          "TABLE III: Performance metrics for different modality com-": "soning for emotion recognition in conversations,” in 2024 International"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8."
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[5] M.\nJagadeesh,\nS. Viswanathan,\nand\nS. Varadarajan,\n“Deep\nlearning"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "approaches for effective human computer interaction: A comprehensive"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "survey on single and multimodal emotion detection,” in 2024 IEEE 9th"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "International Conference for Convergence in Technology (I2CT).\nIEEE,"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "2024, pp. 1–8."
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[6] B. Ribeiro, G. Oliveira, A. Laranjeira, and J. P. Arrais, “Deep learning"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "in digital marketing: brand detection and emotion recognition,” Interna-"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "tional Journal of Machine Intelligence and Sensory Signal Processing,"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "vol. 2, no. 1, pp. 32–50, 2017."
        },
        {
          "TABLE III: Performance metrics for different modality com-": "[7] R. K. Kanna, B. S. Panigrahi, S. K. Sahoo, A. R. Reddy, Y. Manchala,"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "and N. K. Swain,\n“Cnn\nbased\nface\nemotion\nrecognition\nsystem for"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "healthcare application,” EAI Endorsed Transactions on Pervasive Health"
        },
        {
          "TABLE III: Performance metrics for different modality com-": "and Technology, vol. 10, 2024."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "J. Hu, Y. Liu, J. Zhao, and Q. Jin, “Mmgcn: Multimodal fusion via deep"
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "graph convolution network for\nemotion recognition in conversation,”"
        },
        {
          "language models to enhance the model’s inference ability.": "REFERENCES",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "of\nthe\n59th Annual Meeting\nof\nthe Association\nfor\nin Proceedings"
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "Computational Linguistics and the 11th International Joint Conference"
        },
        {
          "language models to enhance the model’s inference ability.": "[1] X. Li, Z. Yang, Z. Li, and Y. Li, “Erc dmsp: Emotion recognition in",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "on Natural Language Processing (Volume 1: Long Papers), Jan 2021."
        },
        {
          "language models to enhance the model’s inference ability.": "conversation based on dynamic modeling of\nspeaker personalities,” in",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "F. Shen, Y. Xie, J. Zhu, X. Zhu, and H. Zeng, “Git: Graph interactive"
        },
        {
          "language models to enhance the model’s inference ability.": "2024 International Joint Conference on Neural Networks (IJCNN), 2024,",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "transformer\nfor vehicle re-identification,” IEEE Transactions on Image"
        },
        {
          "language models to enhance the model’s inference ability.": "pp. 1–10.",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "Processing, 2023."
        },
        {
          "language models to enhance the model’s inference ability.": "[2]\nL. Ge,\nF. Huang, Q. Li,\nand Y. Ye,\n“Modeling\nsentiment-speaker-",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "F. Shen, X. Shu, X. Du, and J. Tang, “Pedestrian-specific bipartite-aware"
        },
        {
          "language models to enhance the model’s inference ability.": "dependency for\nemotion recognition in conversation,”\nin 2024 Inter-",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "the\nsimilarity learning for text-based person retrieval,” in Proceedings of"
        },
        {
          "language models to enhance the model’s inference ability.": "national Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8.",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "31th ACM International Conference on Multimedia, 2023."
        },
        {
          "language models to enhance the model’s inference ability.": "[3]\nF. Xu, G. Li, Z. Zhong, Y. Zhou, and W. Zhou, “D-man: a distance-",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "T. Kipf\nand M. Welling,\n“Semi-supervised\nclassification with\ngraph"
        },
        {
          "language models to enhance the model’s inference ability.": "based multi-channel attention network for erc,” in 2024 International",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "convolutional networks,” arXiv: Learning,arXiv: Learning, Sep 2016."
        },
        {
          "language models to enhance the model’s inference ability.": "Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8.",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "[4]\nF. Xu, T. Sun, W. Zhou, Z. Yu, and J. Lu, “Ctf-erc: Coarse-to-fine rea-",
          "cessing, Jan 2019.": "[24] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learn-"
        },
        {
          "language models to enhance the model’s inference ability.": "",
          "cessing, Jan 2019.": "Information Processing\nSystems,Neural\ning\non\nlarge\ngraphs,” Neural"
        },
        {
          "language models to enhance the model’s inference ability.": "soning for emotion recognition in conversations,” in 2024 International",
          "cessing, Jan 2019.": ""
        },
        {
          "language models to enhance the model’s inference ability.": "Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8.",
          "cessing, Jan 2019.": "Information Processing Systems, Jun 2017."
        },
        {
          "language models to enhance the model’s inference ability.": "[5] M.\nJagadeesh,\nS. Viswanathan,\nand\nS. Varadarajan,\n“Deep\nlearning",
          "cessing, Jan 2019.": "Z. Liu and J. Zhou, Graph Attention Networks, Jan 2020, p. 39–41."
        },
        {
          "language models to enhance the model’s inference ability.": "approaches for effective human computer interaction: A comprehensive",
          "cessing, Jan 2019.": "Z. Liu, W. Lin, Y. Shi, and J. Zhao, A Robustly Optimized BERT Pre-"
        },
        {
          "language models to enhance the model’s inference ability.": "survey on single and multimodal emotion detection,” in 2024 IEEE 9th",
          "cessing, Jan 2019.": "training Approach with Post-training, Jan 2021, p. 471–484."
        },
        {
          "language models to enhance the model’s inference ability.": "International Conference for Convergence in Technology (I2CT).\nIEEE,",
          "cessing, Jan 2019.": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training"
        },
        {
          "language models to enhance the model’s inference ability.": "2024, pp. 1–8.",
          "cessing, Jan 2019.": "of deep bidirectional\ntransformers for\nlanguage understanding,” in Pro-"
        },
        {
          "language models to enhance the model’s inference ability.": "[6] B. Ribeiro, G. Oliveira, A. Laranjeira, and J. P. Arrais, “Deep learning",
          "cessing, Jan 2019.": "ceedings of\nthe 2019 Conference of\nthe North, Jan 2019."
        },
        {
          "language models to enhance the model’s inference ability.": "in digital marketing: brand detection and emotion recognition,” Interna-",
          "cessing, Jan 2019.": "F.\nEyben, M. W¨ollmer,\nand B.\nSchuller,\n“Opensmile:\nthe munich"
        },
        {
          "language models to enhance the model’s inference ability.": "tional Journal of Machine Intelligence and Sensory Signal Processing,",
          "cessing, Jan 2019.": "versatile and fast open-source audio feature extractor,” in Proceedings"
        },
        {
          "language models to enhance the model’s inference ability.": "vol. 2, no. 1, pp. 32–50, 2017.",
          "cessing, Jan 2019.": "of\nthe 18th ACM international\nconference on Multimedia, 2010, pp."
        },
        {
          "language models to enhance the model’s inference ability.": "[7] R. K. Kanna, B. S. Panigrahi, S. K. Sahoo, A. R. Reddy, Y. Manchala,",
          "cessing, Jan 2019.": "1459–1462."
        },
        {
          "language models to enhance the model’s inference ability.": "and N. K. Swain,\n“Cnn\nbased\nface\nemotion\nrecognition\nsystem for",
          "cessing, Jan 2019.": "[29] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely"
        },
        {
          "language models to enhance the model’s inference ability.": "healthcare application,” EAI Endorsed Transactions on Pervasive Health",
          "cessing, Jan 2019.": "connected convolutional networks,” in 2017 IEEE Conference on Com-"
        },
        {
          "language models to enhance the model’s inference ability.": "and Technology, vol. 10, 2024.",
          "cessing, Jan 2019.": "puter Vision and Pattern Recognition (CVPR), Jul 2017."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "recognition,” in 2016 IEEE Conference on Computer Vision and Pattern"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Recognition (CVPR), Jun 2016."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "[31]\nJ. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image co-"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "attention for visual question answering,” Neural Information Processing"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Systems,Neural\nInformation Processing Systems, Jan 2016."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "[32]\nJ.-Y. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "arXiv:\nof\ngated\nrecurrent\nneural\nnetworks\non\nsequence modeling,”"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Neural\nand Evolutionary Computing,arXiv: Neural\nand Evolutionary"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Computing, Dec 2014."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "[33] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\ninteractive emotional"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "dyadic motion capture database,” Language Resources and Evaluation,"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "p. 335–359, Dec 2008."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "[34] D. Hazarika, S. Poria, R. Mihalcea, E. Cambria, and R. Zimmermann,"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "“Icon: Interactive conversational memory network for multimodal emo-"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "the 2018 Conference on Empirical\ntion detection,”\nin Proceedings of"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "Methods in Natural Language Processing, Jan 2018."
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "[35]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihal-"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "cea, “Meld: A multimodal multi-party dataset\nfor emotion recognition"
        },
        {
          "[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning for image": "in conversations,” arXiv preprint arXiv:1810.02508, 2018."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Erc dmsp: Emotion recognition in conversation based on dynamic modeling of speaker personalities",
      "authors": [
        "X Li",
        "Z Yang",
        "Z Li",
        "Y Li"
      ],
      "year": "2024",
      "venue": "2024 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "2",
      "title": "Modeling sentiment-speakerdependency for emotion recognition in conversation",
      "authors": [
        "L Ge",
        "F Huang",
        "Q Li",
        "Y Ye"
      ],
      "year": "2024",
      "venue": "2024 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "3",
      "title": "D-man: a distancebased multi-channel attention network for erc",
      "authors": [
        "F Xu",
        "G Li",
        "Z Zhong",
        "Y Zhou",
        "W Zhou"
      ],
      "year": "2024",
      "venue": "2024 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "4",
      "title": "Ctf-erc: Coarse-to-fine reasoning for emotion recognition in conversations",
      "authors": [
        "F Xu",
        "T Sun",
        "W Zhou",
        "Z Yu",
        "J Lu"
      ],
      "year": "2024",
      "venue": "2024 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "5",
      "title": "Deep learning approaches for effective human computer interaction: A comprehensive survey on single and multimodal emotion detection",
      "authors": [
        "M Jagadeesh",
        "S Viswanathan",
        "S Varadarajan"
      ],
      "year": "2024",
      "venue": "2024 IEEE 9th International Conference for Convergence in Technology"
    },
    {
      "citation_id": "6",
      "title": "Deep learning in digital marketing: brand detection and emotion recognition",
      "authors": [
        "B Ribeiro",
        "G Oliveira",
        "A Laranjeira",
        "J Arrais"
      ],
      "year": "2017",
      "venue": "International Journal of Machine Intelligence and Sensory Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Cnn based face emotion recognition system for healthcare application",
      "authors": [
        "R Kanna",
        "B Panigrahi",
        "S Sahoo",
        "A Reddy",
        "Y Manchala",
        "N Swain"
      ],
      "year": "2024",
      "venue": "EAI Endorsed Transactions on Pervasive Health and Technology"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information fusion"
    },
    {
      "citation_id": "9",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "Enhancing aerial object detection with selective frequency interaction network",
      "authors": [
        "W Weng",
        "M Wei",
        "J Ren",
        "F Shen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Lr-fpn: Enhancing remote sensing object detection with location refined feature pyramid network",
      "authors": [
        "H Li",
        "R Zhang",
        "Y Pan",
        "J Ren",
        "F Shen"
      ],
      "year": "2024",
      "venue": "Lr-fpn: Enhancing remote sensing object detection with location refined feature pyramid network",
      "arxiv": "arXiv:2404.01614"
    },
    {
      "citation_id": "12",
      "title": "Triplet contrastive learning for unsupervised vehicle re-identification",
      "authors": [
        "F Shen",
        "X Du",
        "L Zhang",
        "J Tang"
      ],
      "year": "2023",
      "venue": "Triplet contrastive learning for unsupervised vehicle re-identification",
      "arxiv": "arXiv:2301.09498"
    },
    {
      "citation_id": "13",
      "title": "Imagdressing-v1: Customizable virtual dressing",
      "authors": [
        "F Shen",
        "X Jiang",
        "X He",
        "H Ye",
        "C Wang",
        "X Du",
        "Z Li",
        "J Tang"
      ],
      "year": "2024",
      "venue": "Imagdressing-v1: Customizable virtual dressing",
      "arxiv": "arXiv:2407.12705"
    },
    {
      "citation_id": "14",
      "title": "Imagpose: A unified conditional framework for pose-guided person generation",
      "authors": [
        "F Shen",
        "J Tang"
      ],
      "year": "2024",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Boosting consistency in story visualization with rich-contextual conditional diffusion models",
      "authors": [
        "F Shen",
        "H Ye",
        "S Liu",
        "J Zhang",
        "C Wang",
        "X Han",
        "W Yang"
      ],
      "year": "2024",
      "venue": "Boosting consistency in story visualization with rich-contextual conditional diffusion models",
      "arxiv": "arXiv:2407.02482"
    },
    {
      "citation_id": "16",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "18",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Natural Language Processing,International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Git: Graph interactive transformer for vehicle re-identification",
      "authors": [
        "F Shen",
        "Y Xie",
        "J Zhu",
        "X Zhu",
        "H Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "22",
      "title": "Pedestrian-specific bipartite-aware similarity learning for text-based person retrieval",
      "authors": [
        "F Shen",
        "X Shu",
        "X Du",
        "J Tang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "arXiv: Learning,arXiv: Learning"
    },
    {
      "citation_id": "24",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems,Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Graph Attention Networks",
      "authors": [
        "Z Liu",
        "J Zhou"
      ],
      "year": "2020",
      "venue": "Graph Attention Networks"
    },
    {
      "citation_id": "26",
      "title": "A Robustly Optimized BERT Pretraining Approach with Post-training",
      "authors": [
        "Z Liu",
        "W Lin",
        "Y Shi",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "A Robustly Optimized BERT Pretraining Approach with Post-training"
    },
    {
      "citation_id": "27",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "28",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Hierarchical question-image coattention for visual question answering",
      "authors": [
        "J Lu",
        "J Yang",
        "D Batra",
        "D Parikh"
      ],
      "year": "2016",
      "venue": "Neural Information Processing Systems,Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J.-Y Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "arXiv: Neural and Evolutionary Computing,arXiv: Neural and Evolutionary Computing"
    },
    {
      "citation_id": "33",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "34",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    }
  ]
}