{
  "paper_id": "2401.15164v1",
  "title": "Amuse: Adaptive Multimodal Analysis For Speaker Emotion Recognition In Group Conversations",
  "published": "2024-01-26T19:17:05Z",
  "authors": [
    "Naresh Kumar Devulapally",
    "Sidharth Anand",
    "Sreyasee Das Bhattacharjee",
    "Junsong Yuan",
    "Yu-Ping Chang"
  ],
  "keywords": [
    "Artificial Intelligence",
    "Supervised Learning",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network (MAN) that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN \"injects\" cross-modal attention via its Peripheral keyvalue pairs within each layer of a mode-specific Central query network. The resulting cross-attended mode-specific descriptors are then combined using an Adaptive Fusion (AF) technique that enables the model to integrate the discriminative and complementary mode-specific data patterns within an instancespecific multimodal descriptor. Given a dialogue represented by a sequence of utterances, the proposed AMuSE (Adaptive Multimodal Analysis for Speaker Emotion) model condenses both spatial (within-mode and within-utterance) and temporal (across-mode and across-utterances in the sequence) features into two dense descriptors: speaker-level and utterance-level. This helps not only in delivering better classification performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy) in large-scale public datasets (MELD and IEMOCAP) but also helps the users in understanding the reasoning behind each emotion prediction made by the model via its Multimodal Explainability Visualization module.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Understanding the emotional nuances within conversations has become a pivotal task, with applications ranging from sentiment analysis in social media to affect-aware human-robot interactions.The complexities inherent in multi-party conversations, involving interactions between multiple speakers via various modalities like text, video, and audio, pose significant challenges for accurate emotion recognition. Extracting the subtle interplay of emotions from these diverse modalities necessitates a comprehensive approach that can effectively capture the spatio-temporal evolution of heterogeneous cooccurring mode-specific patterns and their mutual interactions across various modes, while also taking into consideration the unique and dynamic nature of the speaker's expressions.\n\nIt is particularly to note that the affective state of each individual evolves continuously during conversation. Such transitions depend on various intra (e.g. personal background, unique behavioral traits, and habits) and inter (e.g. audience behavior, their interpreting conducts) personal contexts as well as other environmental circumstances. While each speaker's visual and auditory cues offer valuable insights into their emotional states, the relationships between these modalities can be subtle and context-dependent. Additionally, the importance of different modes is not constant but varies in an instancespecific manner, depending on situations and contexts. For example, there may be a scenario in which a speaker's facial expression may be less explicitly representative of their emotion and its temporal evolution compared to their acoustic signal. In another scenario, a speaker may feel more restrained in expressing their emotions due to the conventional setting, such as at public gathering. In such cases, the surrounding contexts may provide important cues to facilitate accurate inferences regarding the speaker's emotion. Thus the challenges related to the presence of strong heterogeneity among such crossmodal representations and the influence of an individual's intra-personal and other situational contexts toward learning a variety of the cross-modal interaction patterns are crucial yet under-studied.\n\nTo address these challenges, we propose a two-level information integration technique. First, a Multimodal Attention Network (MAN) is trained to bridge the heterogeneity gaps among the mode-specific representations by capturing the cross-modal interactions at various levels of spatial abstraction. MAN incorporates an interactive bunch of mode-specific Peripheral and Central networks, where the utterance-level mode-specific emotion patterns at every layer of a Central network are attended by injected feedback from a Peripheral network. This enables each mode-specific Central network to prioritize the mode-invariant spatial (within utterance) details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned model. Intuitively, this is necessary because while representing a specific emotional state, various modalities like text, audio, and video exhibit correlations that may be apparent at different levels of abstraction. While these correlating patterns are important, certain mode-exclusive cues (e.g., the non-verbal response of the audience) may also convey useful insights about the speaker's evolving emotional state at the next time-stamp. Second, an Adaptive Fusion (AF) technique is employed, recognizing that not all modalities contribute equally to the process of emotion recognition for Fig.  1 . Proposed AMuSE Architecture that captures cross-modal interactions and their spatio-temporal evolution to predict the speaker's emotion in a conversation. every query instance. The resulting cross-attended uni-modal feature descriptors derived from the mode-specific Central networks are then interpolated via AF in an instance-specific manner. These interpolated descriptors from the AF are analyzed on both the conversational level and on a speaker level, allowing us to track emotional changes for the entire group and for each individual speaker.Thus, the key contributions of the proposed model for the Adaptive Multimodal Analysis for Speaker Emotion (aka AMuSE), are: 1) Cross-Attended Feature Representation via Multimodal Attention Network that models cross-modal interaction by \"injecting\" features from multiple Peripheral networks into the layers of a Central network. This helps the model to prioritize the mode-invariant spatial (within utterance) details of the emotion patterns, while also retaining its modeexclusive aspects at various levels of abstraction. 2) Adaptive Fusion (AF) that interpolates the cross-attended mode-specific descriptors to combine the novel instancespecific and category-specific utterance-level spatial patterns within the learned multimodal descriptor. 3) Extensive Evaluations with Explainable Visualization using publicly available (MELD  [29] , IEMOCAP  [3] ) datasets not only demonstrate an impressive classification performance (3 -5% improvement in Weighted-F1 and 5 -7% improvement in Accuracy) of AMuSE, its userfriendly interface also facilitate the multimodal reasonings behind a specific prediction made by the model to deliver improved reliability on its decision.\n\nII. RELATED WORK Traditionally, works on Emotion Recognition in Conversations (ERC) have focused heavily on unimodal techniques, primarily due to the strength of natural language transcriptions or descriptors as a strong emotional indicator  [7] ,  [10] ,  [23] . However, while text serves the purpose in simple scenarios, it often struggles to evaluate more complex human responses involving sarcasm or confusion, where more information can be gleaned about the speaker's emotional state by studying the tone, face, posture, and gestures  [39] . Recent works  [5] ,  [19]  demonstrate the superiority of multimodal techniques. Most current ERC research has focused on modeling cross-modal interactions by either concatenating the processed unimodal feature vectors  [13] ,  [23] ,  [28]  or by a predefined fixed combination (e.g., a weighted average of feature vectors)  [25] ,  [34] . While better than unimodal approaches, these techniques ignore various levels of information that may be critical for the comprehensive modeling of spatial and temporal multimodal relationships.\n\nThough promising, many existing methods  [16] ,  [31]  suffer from the weakness of an insufficient fusion of cross-modal interactions, unlike popular beliefs, which may not be uniform across instances or categories  [8]  and vary given an individual's unique socio-behavioral responses. Furthermore, the precision vs. explainability trade-off continues to pose challenges for the systems. Toward addressing these, we leverage multimodal information from each sample to track the evolution of emotion over the conversation both for individual speakers and the group as a whole, while simultaneously providing meaningful insights that attempt to explain the model's decision.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "Problem Definition: Given a multi-party conversation represented as a sequence of utterances {u j }j ∈ D, the objective is to evaluate the dominant emotional state of the speaker for each utterance u j . For brevity, we will henceforth omit the suffix j, and an arbitrary utterance u j will be represented as u unless the suffix is specifically required. Each u ∈ D contains (T, V, A), where T is the text transcription, V is the video and A is the audio.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Unimodal Self-Attended Feature Representation",
      "text": "To capture the spatial evolution of information within each utterance, we propose a mode-specific unimodal feature representation scheme as described below 1) Text Representation: To derive a compact descriptor for the text component T represented as a sequence of p words, i.e. T = {ω 1 , ω 2 , ..., ω p }, we employ the pretrained model  [32]  to obtain the fixed language embedding f T ∈ R p×dt for the text component T . The masked and permuted language modeling (MPNet) inherits the advantages of BERT  [6]  and XLNet  [37]  by leveraging the dependency within the predicted tokens through permuted language modeling and utilizes the auxiliary position information to mitigate the position discrepancy. In fact, to explicitly capture the contextual meaning of each word in an utterance, the initial MPNet-based word embeddings are used as input to a Bidirectional-Long Short-Term Memory (Bi-LSTM) followed by the embedding layer to produce a derived word representation vector h i for each ω i , which in turn develops a derived text representation vector w 0 = [h 1 , h 2 , ...h p ] for the text component T . Toward attaining an attention-aware text descriptor, w 0 is further processed through a M-layered attention\n\nw m and the resulting attention-enhanced average pooled text descriptor is defined as f T = w M .\n\n2) Video Representation: For the visual component V of each utterance u ∈ D, FFmpeg is used to identify n keyframes and MTCNN  [41]  is applied to extract the aligned faces from each keyframe. To represent the facial expression information within the context of the individuals' environment, each keyframe is then decomposed into two components: \"face frames\", which is a derived frame containing only the face regions of the keyframes; \"back frames\", which captures the background environment by removing all identified faces. JAA-Net  [30] , which jointly performs Action Units (AU) detection and facial landmark detection, is employed to extract AUs from each of these \"face frames\". Thus, the visual content V of u is represented in terms of two equal-sized derived frame sequences: v f ace = {au 1 , au 2 , ..., au n } and v back = {b 1 , b 2 , ..., b n }, where each au j and b j represent a learned descriptor describing the j th element in v f ace and v back respectively. Two identical Bi-LSTM-based sequence representation modules, which take v f ace or v back as inputs, are employed to obtain the initial regional descriptors\n\n, where d hid v is the number of the final embedding layer units in the Bi-LSTM model. Similar to the approach followed in the text feature representation process, a stacked self-attention layer network (F SA v ), which retrieves the multi-view attention between v f ace and v back to derive a self-attended visual descriptor f v ∈ R 2n×dv for V .\n\n3) Audio Representation: Patchout fast (2-D) spectrogram transformer (PASST)  [17]  model, which is initialized from an ImageNet vision transformer model, and further pre-trained on 10s audio from AudioSet  [9] , is used to represent the audio component A of the utterance u. Each segment is then represented in terms of their PASST descriptor, so A = {a 1 , a 2 , ..., a e }, where a i ∈ R dpasst is the PASST feature of the i th audio frame. Similar to visual and text representations discussed above, a Bi-LSTM network followed by an M-layered self-attention module F SA a is leveraged to obtain attention-enhanced descriptor f a ∈ R e×da .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Multimodal Attention Network (Man) For Cross Attended Feature Representation",
      "text": "While the intra-modal feature discriminability can be addressed by the proposed technique above, toward integrating the contents across modalities, heterogeneity of the data patterns across multiple modes is often a bottleneck. For example, an utterance by a speaker may reflect an impression on the speaker's face as well as on that of other participants in the conversation. Similarly, the transcript of the utterance should also be relevant to the visual background context or may have been a response to another utterance by a previous speaker. With this intuition, we propose a Multimodal cross-attended feature representation learning using a Multimodal Attention Network (MAN) that takes f m (m ∈ {v, t, a}) as inputs, and models cross-modal interactions at various levels of detail. As observed in Figure  1 , each layer of our MAN models cross-modal interaction by \"injecting\" features from multiple Peripheral networks into a Central network.\n\nMore specifically, for each mode m in consideration, its mode-specific Central query network is designed using h dense layers followed by a Softmax layer  [18] , which takes f m as input and its intermediate l th dense layer output g l m , is cross-attended by one or more pairs of Peripheral key (K l mi ) and value (V l mi ), such that m i ̸ = m. Each key-value pair is generated via linear mappings. Thus, we have\n\nIn a multi-head attention learning framework, a particular head for the cross-attended Central network output from its l th layer is computed as:\n\nWhere M represents the set of all modes in consideration. Such responses from multiple heads are average pooled to derive the final output of the Central network.\n\nWhile MAN architecture is generic and can be extended for any number of modalities, as described in Section III-A, in our experiments, we use information from three different modes (i.e. M ⊆ {t, v, a}). As shown in Figure  1 , each modespecific Central query network for each m ∈ {a, v, t}) of MANs thus produces an average pooled cross-attended modespecific descriptor f m CA ∈ R d for the uni-mode components T , V , and A for u. We will discuss the learning algorithm later in Section III-E.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Adaptive Fusion (Af)",
      "text": "Most of the existing methods  [2] ,  [14] ,  [24]  leverage a static approach for multimodal feature fusion. Here we make an important observation: The relative importance of each modality is not uniform and varies across samples exhibiting different emotions. For example, a speaker's emotion may reflect an influence of several contexts like audience reaction or surrounding environment. However, the transition of an individual's expression of emotion is continuous. Therefore, slight changes in the utterance representations should not have caused drastic changes in the utterance's emotion labels. Keeping this in mind, we propose an Adaptive Fusion (AF) function A that learns a linear combination of the modespecific representations (f m CA ), to derive a comprehensive spatial multimodal descriptor A(u) for an utterance u as follows:\n\nwhere 0 ≤ α m mi ∀m, m i ∈ M ≤ 1 are learnable parameters and represents the concatenation operator. Thus, the proposed fusion function A provides a flexible multimodal representation mechanism, by which the resulting multimodal descriptor A(u) for an utterance u is able to retain categoryspecific discriminative data patterns, however not completely disregarding the unique instance-specific data patterns observed in the utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Emotion Classification",
      "text": "Given a conversational dialogue represented using a sequence of n utterances {u j } n j ∈ D, our task is to evaluate the emotion of a user-identified speaker s by utilizing the spatio-temporal contexts observed in the dialogue. To attain this objective, we design a speaker-specific representation of the dialogue by using two parallel utterance sequences: Dialogue Context, which describes the entire sequence {A(u j )} j ; Speaker Context, a derived utterance sub-sequence {A(u sj )} j , where the sub-sequence {u sj } sj ∈  [1,n]  is generated from the dialogue and includes only those utterances, in which s vocally contributes to the conversation. Given the voice of a speaker identified by the user in the first keyframe, we use the Librosa library function to match it across utterances to identify this derived subsequence. Two parallel Bi-LSTMs are trained to capture the spatio-temporal contexts independently from these contexts' perspectives. In our experiments, Bi-LSTMs were found to be more useful than LSTm due to their ability to capture the bidirectional temporal contexts. For example using the Speaker Context, the pertinent spatio-temporal representation s l ∈ R s of an utterance u s l is reasoned from the representation s (l-1) of u s (l-1) , while also considering the current state of utterance u s l denoted as z s l (which are initially null) -as s l = ← ---→ LSTM s (s (l-1) , z s l ). Similarly, using the Dialogue Context, the pertinent spatio-temporal representation d l ∈ R s of an utterance u l is reasoned from the representation d (l-1) of u (l-1) , while also considering the current state of utterance u l denoted as w d l (which are initially null) -as\n\nThe final representation of each utterance in the Speaker Context is then derived as, e l = s l ⊕ d l , which is used as an input to a simple neural network comprising of a linear layer followed a softmax activation to estimate the occurrence of emotion in the utterance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Learning",
      "text": "The learning algorithm of AMuSE includes two independent learning objectives. A loss objective (L) for MAN learning and an AF objective to optimize the values of α m mi . 1) MAN Learning: Given an utterance u j ∈ D, the proposed Multimodal Attention Network (MAN) jointly learns the cross-attended representations f t CA,j , f v CA,j , and f a CA,j with twofold objectives: 1) preserving Instance-level discriminability  [11] ,  [26]  within them. This is equivalent to obtaining the learned representations such that they must be more discriminative of u compared to the other samples in D. To incorporate this intuition, we use Noise Contrastive Estimation (NCE)  [11]  loss (L N CE ); 2) preserving the 'category-level' information within their learned representations, such that the predictions obtained from each of these learned mode-specific representations may also align with the ground-truth labels. We leverage Focal Loss  [22]  for this purpose.\n\nWe leverage an aggregated noise contrastive estimation (L ACE ) and an averaged focal loss (L f l ), as defined below:\n\nwith\n\nthat computes the probability of both features f m CA,j and f mi CA,j representing the same instance u j compared to other elements in a uniformly sampled negative set N j .\n\nThe averaged Focal Loss  [22] , specifically effective for an imbalanced dataset like ours, is defined below:\n\nwhere p m c (u j ) is the predicted class-membership probability score for the sample u j by the m th mode-specific Central query network and γ is a tunable parameter. We use a combined loss function L = L ACE + L f l to jointly learn its mode-specific Central query networks.\n\n2) AF Learning: As observed in Eqn. 2, the multimodal descriptor A(u) interpolates all cross-attended mode-specific descriptors (F mi CA ) to reveal all the discriminative feature information by leveraging the changes in the model behavior in response to varying inputs. As such, it is intuitive to note that a slight change in the feature representation should not cause any observable change in the model's decision. Nevertheless, manual selection for any of the interpolation coefficients (however small it is) α u mi may not be equally effective across all samples. Thus, the threefold approximation task specific to our scenario is solved in a pairwise manner. In other words, we perform the learning of these interpolation parameters by first approximating α\n\n) and equating the coefficients of like terms, we obtain\n\nFor optimizing the interpolation parameter α ′ 1 (and similarly α ′ 2 ), we adopt the optimization approach of  [27] , which is as follows:\n\nwhere y * is the ground truth label for the sample u, ⊘ is the element-wise division, ϵ is the hyper-parameter that controls the amount of interpolation in the result, and Q mi represents the i th mode-specific Central query network. To facilitate the learning process, we randomly identify a set of informative samples from the validation pool for which the loss due to a small interpolation is indeed affected (i.e. system prediction indeed changes by a slight change in the interpolation parameters) to use in the following training epochs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "Derived from the TV series F.R.I.E.N.D.S, MELD  [29]  is a multi-party multimodal conversation dataset comprising 7 emotions -'Anger', 'Disgust', 'Sadness', 'Joy', 'Surprise', 'Fear', and 'Neutral'. IEMOCAP  [3]  is a dyadic conversational dataset, with recordings of professional actors performing scripted and improvised scenarios comprising 6 emotions -'Happy', 'Sad', 'Neutral', 'Angry', 'Excited', 'Frustrated'.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Results & Comparative Study",
      "text": "Figure  2  provides some qualitative results. Table  I  and Table  II  present the results on MELD and IEMOCAP test sets, respectively by using F1-score  [24]  as the evaluation metric. The results are compared against several state-ofthe-art algorithms  [2] ,  [5] ,  [12] ,  [14] ,  [20] ,  [21] ,  [24] ,  [36] ,  [38] ,  [40] ,  [42] . For each emotion category, we also evaluate the classification performance using their weighted averages To this end, as we compare the last sub-row of row-12 and row-13, the proposed MAN based cross-attention appears to be extremely beneficial in improving the weighted F1score (w-avg F1) by around 6%. Finally, using a flexible and efficient fusion approach, the proposed AMuSE facilitates further improvement in the performance by reporting ∼ 74% w-avg F1-yet another ∼ 2% improvement compared to the results reported in the baseline row-13 scenario. Row-13 reports the experiment results, wherein cross-attended modespecific feature descriptors (Section III-A) are simply fused using equal values of the interpolation parameters in Eqn 2 (i.e. α s1 mi = α s2 mj ∀m i , m j ∈ M, ∀u s1 , u s1 ∈ D). As we compare this performance (in row-14) with row-8 and row-9 of Table ??, we observe that AMuSE reports around 4 -7% improved performance compared to the best performing existing methods  [5] ,  [20] . A similar performance pattern is also observed in Table  II , wherein AMuSE is compared against several baseline methods using the IEMOCAP dataset. By comparing the last sub-row of row-13 and row-14, we find that the proposed MAN-based cross-attention enables the mode to attain an impressive 10% improvement over its baseline test scenario, in which only the mode-specific feature descriptors (Section III-A) are simply concatenated to define a multimodal descriptor. Finally, by employing AF for feature fusion, the model attains ∼ 74% w-avg F1, which overshoots some of the best-performing baselines  [5] ,  [20]  by around 2 -4%. While most of the works use F1-score as the evaluation metric, compared to a handful few recent works  [14] ,  [42] , which have also reported classification accuracy of their method, proposed AMuSE reports an impressive performance. Compared to one of the best-performing baselines M2FNet  [5]  that reports 66.71 accuracy in the MELD dataset and 69.69% accuracy in the IEMOCAP dataset, AMuSE reports around 7% (i.e. 73.28% accuracy score in MELD dataset) and 5% (i.e. 74.49% accuracy score in IEMOCAP dataset) improvement respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Ablation Study",
      "text": "As observed in Table  III , compared to the other testing scenarios (where either the {α u mi ∀m i ∈ M, ∀u ∈ D} parameters were chosen at random or were fixed to the same value for all samples in D and modes M) the proposed AMuSE shows an improved performance in Test-3 experiment setting, wherein it leverages the learning algorithm for the AF interpolation parameters, introduced in Section ??, to optimize the choices of these parameters in mode-specific and inputspecific manner. This makes the model more adaptable to the newer data patterns, observed in analyzing speakers' emotions from diverse socio-racial backgrounds, compared to those available in the training collection. In the other set of ablation study experiments, we choose different values for the tunable parameter γ in the focal loss function defined in Eqn. 3. Again as observed in Table  IV , in both datasets, the chosen value of γ = 1 produces a slightly better W-Avg F1 score, compared to the other values of γ. In fact, the performance of AMuSE remains mostly stable over a range of values in [0.75, 1.25], which highlights the system stability in the performance over the choice of γ values. Finally, in Table V we also report ablation study results for the number of MAN layers in the model. The performance remains fairly consistent when using 3, 4, or 5 layers and peaks at 4, which is the number of layers we have chosen in the model.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed AMuSE Architecture that captures cross-modal interactions and their spatio-temporal evolution to predict the speaker’s",
      "page": 2
    },
    {
      "caption": "Figure 1: , each layer of our MAN models",
      "page": 3
    },
    {
      "caption": "Figure 1: , each mode-",
      "page": 4
    },
    {
      "caption": "Figure 2: provides some qualitative results. Table I and",
      "page": 5
    },
    {
      "caption": "Figure 2: Some example results with 3 mode-specific explainability analysis, wherein explanation columns regions/texts contributing to the",
      "page": 7
    },
    {
      "caption": "Figure 2: , we present the explainability analysis in various",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MFN [38]",
          "Mode": "T + A",
          "Neutral": "0.762",
          "Surprise": "0.407",
          "Fear": "0.0",
          "Sadness": "0.137",
          "Joy": "0.467",
          "Disgust": "0.0",
          "Anger": "0.408",
          "w-Avg F1": "0.547",
          "w-avg F1\n5-CLS": "0.5732"
        },
        {
          "Method": "ICON [12]",
          "Mode": "T",
          "Neutral": "0.762",
          "Surprise": "0.462",
          "Fear": "0.0",
          "Sadness": "0.189",
          "Joy": "0.485",
          "Disgust": "0.0",
          "Anger": "0.301",
          "w-Avg F1": "0.546",
          "w-avg F1\n5-CLS": "0.5718"
        },
        {
          "Method": "",
          "Mode": "A",
          "Neutral": "0.669",
          "Surprise": "0.0",
          "Fear": "0.0",
          "Sadness": "0.0",
          "Joy": "0.086",
          "Disgust": "0.0",
          "Anger": "0.315",
          "w-Avg F1": "0.377",
          "w-avg F1\n5-CLS": "0.3947"
        },
        {
          "Method": "",
          "Mode": "T + A",
          "Neutral": "0.736",
          "Surprise": "0.500",
          "Fear": "0.0",
          "Sadness": "0.232",
          "Joy": "0.502",
          "Disgust": "0.0",
          "Anger": "0.448",
          "w-Avg F1": "0.563",
          "w-avg F1\n5-CLS": "0.5897"
        },
        {
          "Method": "DialogueRNN [24]",
          "Mode": "T",
          "Neutral": "0.737",
          "Surprise": "0.449",
          "Fear": "0.054",
          "Sadness": "0.234",
          "Joy": "0.476",
          "Disgust": "0.0",
          "Anger": "0.415",
          "w-Avg F1": "0.551",
          "w-avg F1\n5-CLS": "0.5759"
        },
        {
          "Method": "",
          "Mode": "A",
          "Neutral": "0.53",
          "Surprise": "0.156",
          "Fear": "0.0",
          "Sadness": "0.083",
          "Joy": "0.112",
          "Disgust": "0.051",
          "Anger": "0.321",
          "w-Avg F1": "0.34",
          "w-avg F1\n5-CLS": "0.3542"
        },
        {
          "Method": "",
          "Mode": "T + A",
          "Neutral": "0.732",
          "Surprise": "0.519",
          "Fear": "0.0",
          "Sadness": "0.248",
          "Joy": "0.532",
          "Disgust": "0.0",
          "Anger": "0.456",
          "w-Avg F1": "0.57",
          "w-avg F1\n5-CLS": "0.5971"
        },
        {
          "Method": "ConGCN [40]",
          "Mode": "T",
          "Neutral": "0.749",
          "Surprise": "0.498",
          "Fear": "0.065",
          "Sadness": "0.226",
          "Joy": "0.524",
          "Disgust": "0.088",
          "Anger": "0.432",
          "w-Avg F1": "0.574",
          "w-avg F1\n5-CLS": "0.5969"
        },
        {
          "Method": "",
          "Mode": "A",
          "Neutral": "0.641",
          "Surprise": "0.254",
          "Fear": "0.047",
          "Sadness": "0.193",
          "Joy": "0.155",
          "Disgust": "0.030",
          "Anger": "0.341",
          "w-Avg F1": "0.422",
          "w-avg F1\n5-CLS": "0.44"
        },
        {
          "Method": "",
          "Mode": "T + A",
          "Neutral": "0.767",
          "Surprise": "0.503",
          "Fear": "0.087",
          "Sadness": "0.285",
          "Joy": "0.531",
          "Disgust": "0.106",
          "Anger": "0.468",
          "w-Avg F1": "0.594",
          "w-avg F1\n5-CLS": "0.6175"
        },
        {
          "Method": "DialogueCRN [14]",
          "Mode": "T + A",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.6073",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "EmoCaps [20]",
          "Mode": "T + A + V",
          "Neutral": "0.7712",
          "Surprise": "0.6319",
          "Fear": "0.0303",
          "Sadness": "0.4254",
          "Joy": "0.5750",
          "Disgust": "0.0769",
          "Anger": "0.5754",
          "w-Avg F1": "0.6400",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "M2FNet\n[5]",
          "Mode": "T + A + V",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.6785",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Cross-Modal Distribution Matching [21]",
          "Mode": "T + A",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.571",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Transformer Based Cross-modality Fusion [35]",
          "Mode": "T + A +V",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.64",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Hierarchical Uncertainty\nfor Multimodal Emotion Recognition [4]",
          "Mode": "T + A +V",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.59",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Shape of Emotion [1]",
          "Mode": "T + A +V",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.63",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "UniMSE [15]",
          "Mode": "T + A +V",
          "Neutral": "-",
          "Surprise": "-",
          "Fear": "-",
          "Sadness": "-",
          "Joy": "-",
          "Disgust": "-",
          "Anger": "-",
          "w-Avg F1": "0.66",
          "w-avg F1\n5-CLS": "-"
        },
        {
          "Method": "Proposed Uni-mode\nFeature Rep.\n(Section III-A)\n+Classifier\n(Section III-D)",
          "Mode": "T",
          "Neutral": "0.7439",
          "Surprise": "0.6191",
          "Fear": "0.0209",
          "Sadness": "0.3914",
          "Joy": "0.5178",
          "Disgust": "0.0613",
          "Anger": "0.5036",
          "w-Avg F1": "0.6041",
          "w-avg F1\n5-CLS": "0.6306"
        },
        {
          "Method": "",
          "Mode": "A",
          "Neutral": "0.3838",
          "Surprise": "0.3581",
          "Fear": "0.0209",
          "Sadness": "0.3286",
          "Joy": "0.3617",
          "Disgust": "0.0613",
          "Anger": "0.3529",
          "w-Avg F1": "0.3537",
          "w-avg F1\n5-CLS": "0.3684"
        },
        {
          "Method": "",
          "Mode": "V",
          "Neutral": "0.5562",
          "Surprise": "0.4905",
          "Fear": "0.0209",
          "Sadness": "0.3374",
          "Joy": "0.4098",
          "Disgust": "0.0613",
          "Anger": "0.3713",
          "w-Avg F1": "0.4615",
          "w-avg F1\n5-CLS": "0.4813"
        },
        {
          "Method": "Proposed Uni-mode\nFeature Rep.(Section III-A) + Feature Concat.\n+ Classifier\n(Section III-D)",
          "Mode": "T + A",
          "Neutral": "0.7627",
          "Surprise": "0.6318",
          "Fear": "0.0241",
          "Sadness": "0.4214",
          "Joy": "0.5316",
          "Disgust": "0.0613",
          "Anger": "0.5597",
          "w-Avg F1": "0.6265",
          "w-avg F1\n5-CLS": "0.6540"
        },
        {
          "Method": "",
          "Mode": "T + V",
          "Neutral": "0.7427",
          "Surprise": "0.6218",
          "Fear": "0.0241",
          "Sadness": "0.4214",
          "Joy": "0.5316",
          "Disgust": "0.0613",
          "Anger": "0.5597",
          "w-Avg F1": "0.6158",
          "w-avg F1\n5-CLS": "0.6428"
        },
        {
          "Method": "",
          "Mode": "A + V",
          "Neutral": "0.5562",
          "Surprise": "0.5796",
          "Fear": "0.0209",
          "Sadness": "0.3610",
          "Joy": "0.4098",
          "Disgust": "0.0613",
          "Anger": "0.4318",
          "w-Avg F1": "0.4810",
          "w-avg F1\n5-CLS": "0.5017"
        },
        {
          "Method": "",
          "Mode": "T + A + V",
          "Neutral": "0.7671",
          "Surprise": "0.6518",
          "Fear": "0.0319",
          "Sadness": "0.4629",
          "Joy": "0.5291",
          "Disgust": "0.0691",
          "Anger": "0.5713",
          "w-Avg F1": "0.6356",
          "w-avg F1\n5-CLS": "0.6632"
        },
        {
          "Method": "Proposed MAN-based\nFeature Rep.(Section III-B) + Feature Concat\n+Classifier\n(Section III-D)",
          "Mode": "T + A + V",
          "Neutral": "0.8359",
          "Surprise": "0.7094",
          "Fear": "0.0674",
          "Sadness": "0.4468",
          "Joy": "0.6297",
          "Disgust": "0.0891",
          "Anger": "0.6389",
          "w-Avg F1": "0.6992",
          "w-avg F1\n5-CLS": "0.7286"
        },
        {
          "Method": "AMuSE",
          "Mode": "T + A + V",
          "Neutral": "0.8469",
          "Surprise": "0.7283",
          "Fear": "0.0674",
          "Sadness": "0.4632",
          "Joy": "0.6481",
          "Disgust": "0.0891",
          "Anger": "0.6574",
          "w-Avg F1": "0.7132",
          "w-avg F1\n5-CLS": "0.7431"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MFN [38]",
          "Mode": "T + A",
          "Happy": "-",
          "Sad": "-",
          "Neutral": "-",
          "Angry": "-",
          "Excited": "-",
          "Frustrated": "-",
          "w-Avg F1": "0.3490"
        },
        {
          "Method": "ICON [12]",
          "Mode": "T + A + V",
          "Happy": "0.3280",
          "Sad": "0.7440",
          "Neutral": "0.6060",
          "Angry": "0.6820",
          "Excited": "0.6840",
          "Frustrated": "0.6620",
          "w-Avg F1": "0.6350"
        },
        {
          "Method": "DialogueRNN [24]",
          "Mode": "T + A + V",
          "Happy": "0.3318",
          "Sad": "0.7880",
          "Neutral": "0.5921",
          "Angry": "0.5128",
          "Excited": "0.7186",
          "Frustrated": "0.5891",
          "w-Avg F1": "0.6275"
        },
        {
          "Method": "MMGCN [33]",
          "Mode": "T + A + V",
          "Happy": "0.4235",
          "Sad": "0.7867",
          "Neutral": "0.6173",
          "Angry": "0.6900",
          "Excited": "0.7433",
          "Frustrated": "0.6232",
          "w-Avg F1": "0.6622"
        },
        {
          "Method": "DialogueCRN [14]",
          "Mode": "T + A",
          "Happy": "0.6261",
          "Sad": "0.8186",
          "Neutral": "0.6005",
          "Angry": "0.5849",
          "Excited": "0.7517",
          "Frustrated": "0.6008",
          "w-Avg F1": "0.6620"
        },
        {
          "Method": "ERLDK [42]",
          "Mode": "T + A",
          "Happy": "0.4730",
          "Sad": "0.7919",
          "Neutral": "0.5642",
          "Angry": "0.6054",
          "Excited": "0.7444",
          "Frustrated": "0.6385",
          "w-Avg F1": "0.6390"
        },
        {
          "Method": "Hierarchical Uncertainty\nfor Multimodal Emotion Recognition [4]",
          "Mode": "T + A + V",
          "Happy": "-",
          "Sad": "-",
          "Neutral": "-",
          "Angry": "-",
          "Excited": "-",
          "Frustrated": "-",
          "w-Avg F1": "0.6598"
        },
        {
          "Method": "DAG-ERC+HCL [36]",
          "Mode": "T",
          "Happy": "-",
          "Sad": "-",
          "Neutral": "-",
          "Angry": "-",
          "Excited": "-",
          "Frustrated": "-",
          "w-Avg F1": "0.6803"
        },
        {
          "Method": "M2FNet\n[5]",
          "Mode": "T + A + V",
          "Happy": "-",
          "Sad": "-",
          "Neutral": "-",
          "Angry": "-",
          "Excited": "-",
          "Frustrated": "-",
          "w-Avg F1": "0.6986"
        },
        {
          "Method": "Multimodal Attentive Learning [2]",
          "Mode": "T + A + V",
          "Happy": "-",
          "Sad": "-",
          "Neutral": "-",
          "Angry": "-",
          "Excited": "-",
          "Frustrated": "-",
          "w-Avg F1": "0.6540"
        },
        {
          "Method": "Proposed Uni-mode\nFeature Rep.\n(Section III-A)",
          "Mode": "T",
          "Happy": "0.2991",
          "Sad": "0.6141",
          "Neutral": "0.5251",
          "Angry": "0.5728",
          "Excited": "0.5918",
          "Frustrated": "0.5969",
          "w-Avg F1": "0.5526"
        },
        {
          "Method": "",
          "Mode": "A",
          "Happy": "0.2991",
          "Sad": "0.3894",
          "Neutral": "0.3951",
          "Angry": "0.2749",
          "Excited": "0.326",
          "Frustrated": "0.3316",
          "w-Avg F1": "0.3417"
        },
        {
          "Method": "",
          "Mode": "V",
          "Happy": "0.3038",
          "Sad": "0.5329",
          "Neutral": "0.5619",
          "Angry": "0.2749",
          "Excited": "0.326",
          "Frustrated": "0.431",
          "w-Avg F1": "0.4260"
        },
        {
          "Method": "Proposed Uni-mode\nFeature Rep.\n(Section III-A)\n+ Feature Concat.",
          "Mode": "T + A",
          "Happy": "0.3038",
          "Sad": "0.6368",
          "Neutral": "0.5619",
          "Angry": "0.598",
          "Excited": "0.6027",
          "Frustrated": "0.6069",
          "w-Avg F1": "0.5727"
        },
        {
          "Method": "",
          "Mode": "T + V",
          "Happy": "0.3359",
          "Sad": "0.6368",
          "Neutral": "0.5885",
          "Angry": "0.598",
          "Excited": "0.6027",
          "Frustrated": "0.6069",
          "w-Avg F1": "0.5815"
        },
        {
          "Method": "",
          "Mode": "A + V",
          "Happy": "0.3038",
          "Sad": "0.5592",
          "Neutral": "0.6328",
          "Angry": "0.321",
          "Excited": "0.326",
          "Frustrated": "0.5293",
          "w-Avg F1": "0.4782"
        },
        {
          "Method": "",
          "Mode": "T + A + V",
          "Happy": "0.3917",
          "Sad": "0.6368",
          "Neutral": "0.6354",
          "Angry": "0.6374",
          "Excited": "0.6027",
          "Frustrated": "0.6399",
          "w-Avg F1": "0.6117"
        },
        {
          "Method": "Proposed MAN-based\nFeature Rep.(Section III-B)+ Feature Concat",
          "Mode": "T + A + V",
          "Happy": "0.6591",
          "Sad": "0.8106",
          "Neutral": "0.7248",
          "Angry": "0.6599",
          "Excited": "0.7769",
          "Frustrated": "0.6734",
          "w-Avg F1": "0.7147"
        },
        {
          "Method": "AMuSE",
          "Mode": "T + A + V",
          "Happy": "0.7025",
          "Sad": "0.8418",
          "Neutral": "0.7548",
          "Angry": "0.6748",
          "Excited": "0.7935",
          "Frustrated": "0.6923",
          "w-Avg F1": "0.7391"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "authors": [
        "Harsh Agarwal",
        "Keshav Bansal",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "year": "2021",
      "venue": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "arxiv": "arXiv:2112.01938"
    },
    {
      "citation_id": "2",
      "title": "Multimodal attentive learning for real-time explainable emotion recognition in conversations",
      "authors": [
        "Balaji Arumugam",
        "Sreyasee Das Bhattacharjee",
        "Junsong Yuan"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Modeling hierarchical uncertainty for multimodal emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Anjie Zhu",
        "Deqiang Ouyang",
        "Xueliang Liu",
        "Heng Tao Shen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "5",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019"
    },
    {
      "citation_id": "8",
      "title": "Cross-modal stimulus conflict: the behavioral effects of stimulus input timing in a visual-auditory stroop task",
      "authors": [
        "Sarah Donohue",
        "Lawrence Appelbaum",
        "Christina Park",
        "Kenneth Roberts",
        "Marty Woldorff"
      ],
      "year": "2013",
      "venue": "PloS one"
    },
    {
      "citation_id": "9",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "F Jort",
        "Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations"
    },
    {
      "citation_id": "11",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "authors": [
        "Michael Gutmann",
        "Aapo Hyvärinen"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "12",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations"
    },
    {
      "citation_id": "14",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "15",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "16",
      "title": "Efficient large-scale multi-modal classification",
      "authors": [
        "Douwe Kiela",
        "Edouard Grave",
        "Armand Joulin",
        "Tomas Mikolov"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout",
      "authors": [
        "Khaled Koutini",
        "Jan Schlüter"
      ],
      "year": "2021",
      "venue": "Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout"
    },
    {
      "citation_id": "18",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2017",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "19",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "20",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "21",
      "title": "Semi-supervised multimodal emotion recognition with cross-modal distribution matching",
      "authors": [
        "Jingjun Liang",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Focal loss for dense object detection"
    },
    {
      "citation_id": "23",
      "title": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations"
    },
    {
      "citation_id": "24",
      "title": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations"
    },
    {
      "citation_id": "25",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltrušaitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI '16"
    },
    {
      "citation_id": "26",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "27",
      "title": "Anton van den Hengel, and Javen Qinfeng Shi. Active learning by feature mixing",
      "authors": [
        "Amin Parvaneh",
        "Ehsan Abbasnejad",
        "Damien Teney",
        "Reza Haffari"
      ],
      "year": "2022",
      "venue": "Anton van den Hengel, and Javen Qinfeng Shi. Active learning by feature mixing"
    },
    {
      "citation_id": "28",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Iti Chaturvedi",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "year": "2016",
      "venue": "2016 IEEE 16th International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "29",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multiparty dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "30",
      "title": "Jaa-net: Joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2020",
      "venue": "Jaa-net: Joint facial action unit detection and face alignment via adaptive attention"
    },
    {
      "citation_id": "31",
      "title": "Learning modality-fused representation based on transformer for emotion analysis",
      "authors": [
        "Piao Shi",
        "Min Hu",
        "Fuji Ren",
        "Xuefeng Shi",
        "Liangfeng Xu"
      ],
      "year": "2022",
      "venue": "Journal of Electronic Imaging"
    },
    {
      "citation_id": "32",
      "title": "Mpnet: Masked and permuted pre-training for language understanding",
      "authors": [
        "Kaitao Song",
        "Xu Tan",
        "Tao Qin",
        "Jianfeng Lu",
        "Tie-Yan Liu"
      ],
      "year": "2020",
      "venue": "Mpnet: Masked and permuted pre-training for language understanding"
    },
    {
      "citation_id": "33",
      "title": "Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua",
        "Mmgcn"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia, MM '19"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels",
      "authors": [
        "Chung-Hsien Wu",
        "Wei-Bin Liang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "Baijun Xie",
        "Mariia Sidulova",
        "Chung Hyuk"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "36",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2021",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19"
    },
    {
      "citation_id": "40",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19"
    },
    {
      "citation_id": "41",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "42",
      "title": "Real-time video emotion recognition based on reinforcement learning and domain knowledge",
      "authors": [
        "Ke Zhang",
        "Yuanqing Li",
        "Jingyu Wang",
        "Erik Cambria",
        "Xuelong Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    }
  ]
}