{
  "paper_id": "2409.10056v1",
  "title": "Tbdm-Net: Bidirectional Dense Networks With Gender Information For Speech Emotion Recognition",
  "published": "2024-09-16T07:36:14Z",
  "authors": [
    "Vlad Striletchi",
    "Cosmin Striletchi",
    "Adriana Stan"
  ],
  "keywords": [
    "speech emotion recognition",
    "dense nets",
    "convolutions",
    "bidirectional layers",
    "SER"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a novel deep neural network-based architecture tailored for Speech Emotion Recognition (SER). The architecture capitalises on dense interconnections among multiple layers of bidirectional dilated convolutions. A linear kernel dynamically fuses the outputs of these layers to yield the final emotion class prediction. This innovative architecture is denoted as TBDM-Net: Temporally-Aware Bi-directional Dense Multi-Scale Network. We conduct a comprehensive performance evaluation of TBDM-Net, including an ablation study, across six widely-acknowledged SER datasets for unimodal speech emotion recognition. Additionally, we explore the influence of gender-informed emotion prediction by appending either golden or predicted gender labels to the architecture's inputs or predictions. The implementation of TBDM-Net is accessible at: https: //github.com/adrianastan/tbdm-net.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) plays an important role in adding a new dimension to speech-enabled applications. As opposed to the linguistic content which can be to a large extent controlled by the speaker, emotions pose a harder task when the speakers intends to limiting or control them in conversations. Understanding emotions expressed through speech can significantly improve human-computer interaction, enabling machines to respond more appropriately to user needs and preferences. However, a major problem in SER-related tasks is still the lack of accurate training data, as many of the available datasets contain only acted speech, i.e. various emotions are elicited by actors in a controlled scenario. Thus, benchmarking 1 SER can still pose challenges for real-life data deployments and evaluation.\n\nAs is the case for many other research fields, SER systems are to a large extent based on deep neural networks with increasingly complex architectures. An important research di-rection is also the use multimodal (e.g. text, image, audio or video) characteristics. However, for this study we rely solely on the information available within the speech signal.\n\nSome of the most notable results in this area are those reported by  [7] . Light-SERNet focuses on reducing the computational complexity of SER and uses a network based only on convolutional layers and local feature learning blocks. Convolutional networks were also used by  [8, 9] . Wen et al.  [10]  introduce an architecture based on capsule nets and transfer learning. It also deals with cross-corpus evaluation, for which an adversarial module is used. Gradient-based adversary learning framework that jointly estimates the SER labels while also normalising speaker characteristics is proposed in  [11] . The method is based on large pretrained models and also analyses the use of small labelled datasets. Croitoru et al.  [12]  describe a novel general method for learning rate adjustments during the training process where the learning rate of the layers closer to the network's output are adjusted more finely. The proposed method is evaluated over several distinct prediction tasks, including speech emotion recognition. The architecture used in SER is based on a transformer and a dense net structure.\n\nSchuller et al.  [13]  attempt to preserve the emotional saturation of speech frames by employing frame-level speech features and attention-based LSTM recurrent neural networks. Recurrent layers were also adopted by  [14] , and combined with Wavelet transforms and 1D CNNs to extract multiresolution representations of the input speech. The Dual-TBNet model of  [15]  proposed the combination of 1D convolutional layers, transformer, and BiLSTM modules, to maximise the robustness of the independent speech abstractions which are then fused to provide the final emotion decision. Notably, Ye et al.  [16]  introduce TIM-Net, which outperforms previous methods on six standard SER datasets. Inspired by this, we draw from their work and implement several modifications to enhance overall emotion prediction accuracy.\n\nBuilding upon prior research, this paper introduces a novel architecture for Speech Emotion Recognition (SER) classification. The architecture employs temporally-aware bidirectional dense networks, referred to as Temporally-Aware Bi-directional Dense Multi-Scale Network (TBDM-Net). The primary contributions of the paper can be summarised  as follows: (i) the introduction of a new deep architecture for SER; (ii) an assessment of the proposed architecture across six multilingual SER datasets; (iii) an ablation study to analyse the impact of each architectural module on final performance; and (iv) an examination of the influence of speaker gender information on emotion classification accuracy.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Tbdm-Net Architecture",
      "text": "TBDM-Net uses a series of temporally-aware convolution blocks (TABs) with incremental dilation coefficients over the direct and inverse time representation of the input speech signal. All forward and reverse temporal blocks are densely connected. These connections enable the network to exploit previously computed features and reduce the number of required parameters  [17] . The intermediate representations obtained from each TAB are summed with their reverse time correspondents. The concatenation of the forward and reverse time temporal blocks' output is passed through a dimension reduction convolutional layer, such that all intermediate representations have the same dimension. All TAB representations are then concatenated and dynamically fused. The final emotion prediction probabilities are obtained through a simple feed forward layer. The activation function employed within the Temporal Attention Blocks (TABs) is the Gaussian Error Linear Unit (GELU)  [18] . GELU is characterised by its smoothness, maintaining a continuous first derivative across its range, which fosters stable and efficient optimisation processes. Its attribute of granting non-zero gradients for both positive and negative inputs facilitates unrestricted information flow during both forward and backward propagation. Additionally, GELU has demonstrated efficacy in addressing the vanishing gradient issue in deeper neural networks.\n\nThe complete architecture is presented in Figure  1 . Its implementation is available at: https://github.com/ adrianastan/tbdm-net.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Datasets And Features",
      "text": "For the evaluation we used six standard speech emotion recognition datasets An overview of these datasets is shown in Table  1 . We note that for the IEMOCAP dataset, due to the high class imbalance in the original dataset, we select only 4 classes, and relabel the excited class into happy.  2  The final subset contains 5531 speech samples.\n\nFrom Table  1  it can be noticed that there is no complete overlap between all sets of emotions annotated or rendered within the speech corpora. There is also a high variation among the amount of speech data, as well as the number and gender of the speakers. One other important characteristic of this dataset selection is its multilingual aspect: IEMOCAP, RAVDESS and SAVEE are English datasets, CASIA is Chinese, EMOVO is Italian, and EMODB is German. All these features of the data selection enable us to perform a thorough analysis of the proposed architecture across several dimensions of speech and emotion variation.\n\nSimilar to  [16] , we use 39 Mel frequency cepstral coefficient (MFCC) representations extracted with the default settings in the Librosa module.  3  A fixed number of frames, different across the datasets, are used to represent the utterances. Shorter utterances are zero-padded left and right, while longer utterances are cropped to a central segment.  4  The maximum temporal dimension of the MFCC representations is between 172 frames for the CASIA dataset, and 606 frames for the IEMOCAP dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Objective Measures And Training Procedure",
      "text": "Because of the varying sample sizes and emotions within each dataset, we opted to assess the performance of our network independently on each speech dataset through a 10-fold crossvalidation process. The folds are chosen randomly, without any dependency on speaker or emotion.\n\nAs objective measures, we used the unweighted average recall (U AR), weighted average recall (W AR) and F1-score (F 1). In standard implementations of multi-class classification, the U AR is equal to the global prediction accuracy.  5  The W AR is a weighted measure which takes into account the number of samples from each class within the test set. U AR, W AR and F 1 -score enable us to directly compare our results against the previously published methods.\n\nThe TBDM-Net's architecture uses 6 temporal blocks for all speech datasets. The dilation rates are: 1, 2, 4, 8, 16, and 32, respectively. The number of filters for each convolution is equal to the number of MFCC coefficients (i.e. 39), and use a kernel size of 2. The models were trained for 300 epochs, using an ADAM optimiser with a learning rate of 1e -3, and betas = (0.93, 0.98). The batch size was set to 64. No early stopping or learning rate scheduler were used. The best model across the 300 epochs in terms of training set W AR was saved, and used to provide the intermediate evaluation results for the respective fold. A separate result is extracted from the model obtained after 300 epochs of training.  6",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Results",
      "text": "The results of TBDM-Net's evaluation are shown in Table  2 . The table includes the performance of TBDM-Net as evaluated in two scenarios: (i) the best model in terms of training subset WAR (TBDM-Net::BT); and (ii) the model obtained after 300 epochs of training (TBDM-Net::300). The table also introduces the recomputed results for the TIM-Net architecture using the authors' official implementation.  7  The same set of input features and number of training epochs as for TBDM-Net were used. We also present the top performance measures for each dataset as reported by previously published peer-reviewed methods.\n\nObservations reveal that TBDM-Net demonstrates heightened performance across nearly all assessed datasets. We posit that facilitating connectivity between the Temporal Attention Blocks (TABs) at varying resolutions and incorporating bidirectional paths enhances the capture of emotional abstraction. However, exceptions arise with the CASIA and SAVEE datasets, where the Dual-TBNet architecture distinctly outperforms. Further analysis is necessary to ascertain the reasons behind the diminished accuracy of TBDM-Net on these datasets. A notable increase in performance is obtained for the EMOVO, RAVDESS and IEMOCAP datasets in terms of U AR-with over 3% absolute recall increase. It is important to notice that IEMOCAP is the largest dataset, and the one which poses most challenges across all published studies in SER. These results are encouraging, as we are planning to use the proposed architecture in real-life scenarios where large volumes of data and less well-defined emotion elicitation would be found.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "We also introduce the results of an ablation study over the RAVDESS dataset. Similar results were found for the other datasets, as well. The study includes the following modifications made to TBDM-Net, with all the other hyperparameters of the architecture and training step having been frozen:\n\n• activation function (ReLU) -we use the ReLU activation function in the TABs instead of GELU; • directionality (w/o BD) -we compare the bidirectional(BD) network with a forward time-only network variation; • multi-scale (w/o MS) -our proposed model uses multiscale (MS) fusion of the intermediary states between TABs.\n\nWe compare this with a variation which only uses the last state, disregarding the others; • number of TABs (5 TABs) -the proposed model uses 6\n\nTABs, and we compare it with a variation containing only 5 TABs.\n\nThe results are shown in Table  3 . It can be noticed that in terms of both U AR and W AR, all modifications to the network yield performance increments, with the number of TABs having the least impact over the final results. This is encouraging especially if the network needs to be optimised for real-time applications. Most improvement is obtained from the use of bidirectional modules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gender-Informed Results",
      "text": "Given that previous literature strongly motivates the use of gender-differentiated systems in SER-related tasks  [21, 22] , we perform a similar evaluation for the TBDM-Net architecture. We also examine how real-life applications would perform the task when the speaker gender is also estimated from the input speech. Therefore, we first build a gender classifier based on large pretrained models' derived speech embeddings. Previous studies  [23]  over such embeddings showed that the TitaNet architecture  [24]  exhibits a high correlation between its embeddings and the speaker's gender. The TitaNet-L 8  pretrained model was selected and the corresponding embeddings extracted. To ensure a good classification performance, several classifiers were trained on the development partition of the VOXCELEB dataset  [25] , and tested on the VOXCELEB test partition, as well as the RAVDESS dataset. The results are shown in Table  4 . The SVC-based gender classifier's prediction in binary or probabilistic formats was used in the follow-up evaluation. The speaker's gender information was added to the TBDM-Net architecture either as post-hoc boosting information or concatenated to the input MFCC representations over the coefficients' dimensions. The results are reported",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The TBDM-Net architecture. The forward and reverse time speech representations are passed through a series of",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "CASIA [1]\nangry, fearful, happy, neutral, sad, surprised\nChinese\n1200\n43’\n8\n4/4"
        },
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "EMOVO [2]\nangry, disgusted, fearful, happy, neutral, sad, surprised\nItalian\n588\n30’\n6\n3/3"
        },
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "EMODB [3]\nangry, bored, disgusted, fearful, happy, neutral, sad\nGerman\n535\n24’\n10\n5/5"
        },
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "IEMOCAP [4]\nangry, happy, neutral, sad\nEnglish\n5531\n11h 37’\n10\n5/5"
        },
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "RAVDESS [5]\nangry, calm, disgusted, happy, fearful, sad, surprised\nEnglish\n1440\n1h 28’\n24\n12/12"
        },
        {
          "Dataset\nEmotions\nLanguage\nNo. samples\nDuration\nSpeakers\nM/F": "SAVEE [6]\nangry, disgusted, fearful, happy, neutral, sad, surprised\nEnglish\n480\n30’\n4\n4/0"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: Ablation study on the RAVDESS dataset. The re-",
      "data": [
        {
          "Classifier": "Logistic Regression\nMLP Classifier\nRandom Forest\nXGBoost\nSupport Vector Classifier",
          "Accuracy [%] ↑\nVOXCELEB\nRAVDESS": "93.45\n81.56\n99.83\n82.70\n98.85\n87.32\n98.15\n90.90\n99.97\n98.26"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Design of Speech Corpus for Mandarin Text to Speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "Blizzard Challenge Workshop"
    },
    {
      "citation_id": "3",
      "title": "EMOVO Corpus: an Italian Emotional Speech Database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "7",
      "title": "Machine Audition: Principles, Algorithms and Systems, chapter Multimodal Emotion Recognition",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems, chapter Multimodal Emotion Recognition"
    },
    {
      "citation_id": "8",
      "title": "LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami",
        "Benoît Champagne"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using convolutional and Recurrent Neural Networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "11",
      "title": "CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for Single-Corpus and Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Jiaxin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Speaker Normalization for Self-Supervised Speech Emotion Recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "LeRaC: Learning Rate Curriculum",
      "authors": [
        "Florinel-Alin",
        "Nicolae-Catalin Croitoru",
        "Radu Ristea",
        "Tudor Ionescu",
        "N Sebe"
      ],
      "year": "2022",
      "venue": "ArXiv"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Yue Xie",
        "Ruiyu Liang",
        "Zhenlin Liang",
        "Chengwei Huang",
        "Cairong Zou",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Wavelet multiresolution analysis based speech emotion recognition system using 1d cnn lstm networks",
      "authors": [
        "Aditya Dutt",
        "Paul Gader"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Dual-TBNet: Improving the Robustness of Speech Features via Dual-Transformer-BiLSTM for Speech Emotion Recognition",
      "authors": [
        "Zheng Liu",
        "Xin Kang",
        "Fuji Ren"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "year": "2023",
      "venue": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition"
    },
    {
      "citation_id": "18",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "19",
      "title": "Gaussian Error Linear Units (GELUs)",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2020",
      "venue": "Gaussian Error Linear Units (GELUs)"
    },
    {
      "citation_id": "20",
      "title": "A Speech Emotion Recognition Model Based on Multi-Level Local Binary and Local Ternary Patterns",
      "authors": [
        "Yesim Ulgen",
        "Asaf Varol"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Speakeraware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
      "authors": [
        "Huan Zhao",
        "Bo Li",
        "Zixing Zhang"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "22",
      "title": "Modeling gender information for emotion recognition using Denoising autoencoder",
      "authors": [
        "Rui Xia",
        "Jun Deng",
        "Björn Schuller",
        "Yang Liu"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "23",
      "title": "Gender Differentiated Convolutional Neural Networks for Speech Emotion Recognition",
      "authors": [
        "Puneet Mishra",
        "Ruchir Sharma"
      ],
      "year": "2020",
      "venue": "2020 12th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)"
    },
    {
      "citation_id": "24",
      "title": "Residual Information in Deep Speaker Embedding Architectures",
      "authors": [
        "Adriana Stan"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "25",
      "title": "TitaNet: Neural Model for speaker representation with 1D Depth-wise separable convolutions and global context",
      "authors": [
        "Nithin Rao Koluguri",
        "Taejin Park",
        "Boris Ginsburg"
      ],
      "year": "2021",
      "venue": "TitaNet: Neural Model for speaker representation with 1D Depth-wise separable convolutions and global context",
      "arxiv": "arXiv:2110.04410"
    },
    {
      "citation_id": "26",
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    }
  ]
}