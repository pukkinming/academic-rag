{
  "paper_id": "2312.13567v1",
  "title": "Fine-Grained Disentangled Representation Learning For Multimodal Emotion Recognition",
  "published": "2023-12-21T04:31:18Z",
  "authors": [
    "Haoqin Sun",
    "Shiwan Zhao",
    "Xuechen Wang",
    "Wenjia Zeng",
    "Yong Chen",
    "Yong Qin"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "disentangled representation learning",
    "fine-grained alignment",
    "fine-grained disparity",
    "fine-grained predictor"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MMER) is an active research field that aims to accurately recognize human emotions by fusing multiple perceptual modalities. However, inherent heterogeneity across modalities introduces distribution gaps and information redundancy, posing significant challenges for MMER. In this paper, we propose a novel fine-grained disentangled representation learning (FDRL) framework to address these challenges. Specifically, we design modality-shared and modality-private encoders to project each modality into modality-shared and modality-private subspaces, respectively. In the shared subspace, we introduce a fine-grained alignment component to learn modality-shared representations, thus capturing modal consistency. Subsequently, we tailor a fine-grained disparity component to constrain the private subspaces, thereby learning modality-private representations and enhancing their diversity. Lastly, we introduce a fine-grained predictor component to ensure that the labels of the output representations from the encoders remain unchanged. Experimental results on the IEMOCAP dataset show that FDRL outperforms the state-of-the-art methods, achieving 78.34% and 79.44% on WAR and UAR, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MMER) is an important subfield of affective computing that aims to utilize information from speech and other perceptual modalities, such as textual expressions and body language, to recognize and comprehend human emotional states. In recent years, MMER has found notable applications in various domains, including human customer service  [1] , robotics  [2] , and car driving  [3] . However, the inherent heterogeneity among modalities results in inconsistency and imbalance of information, thereby increasing the difficulty of multimodal representation learning and fusion.\n\nThe current mainstream research primarily focuses on two directions. First, a considerable amount of effort has been dedicated to designing intricate fusion strategies for obtaining effective multimodal representations, which in turn facilitate a more comprehensive understanding of emotions and enhance the performance of multimodal technologies  [4, 5] . For instance, Liu et al.  [6]  proposed a multi-scale fusion framework that combines feature level and decision level fusion for achieving multimodal interaction between speech and text. However, their approach directly concatenates the representations of the two modalities, thereby ignoring the heterogeneity across them. Second, researchers have been committed to exploring the correlations between different modal information to obtain more robust modal semantics and, consequently, improve the accuracy of emotion recognition tasks  [7, 8, 9] . For example, Hazarika et al.  [10]  utilized difference loss to promote the orthogonality of features, thereby increasing the multimodal complementary information. Nonetheless, this might not be an ideal solution as the aforementioned difference loss alone is insufficient for obtaining reliable representations and may lead to trivial solutions without appropriate constraints. More recently, Yang et al.  [11]  proposed a method for disentangled representation learning, which extracts both common and private feature representations for each modality. While this approach holds promise, it primarily focuses on global modality alignment and disparity, neglecting fine-grained feature disentanglement. This oversight limits the model's ability to capture the nuances of each modality, resulting in only suboptimal performance.\n\nTo address the limitations of existing approaches and facilitate fine-grained feature disentanglement, we introduce a novel finegrained disentangled representation learning (FDRL) framework, designed to address the heterogeneity of modalities. This process consists of two main steps. First, we learn modality-shared representations where a shared encoder projects each modality into a modality-shared subspace. Here, a fine-grained alignment component is employed to balance global and local distribution alignment, capturing modality consistency. Second, we learn modality-private representations where private encoders map each modality, separately, into a modality-private subspace. To ensure diversity, a finegrained disparity component constrains the private latent spaces. Moreover, we leverage a fine-grained predictor component to ensure that the label associated with the output representations remains unchanged. Finally, the cross-modal fusion (CMF) module is utilized to efficiently fuse the different representations for MMER. Experimental results on the IEMOCAP dataset demonstrate that our method surpasses state-of-the-art methods, achieving 78.34% and 79.44% on WAR and UAR, respectively.\n\nOur main contributions are summarized as follows:\n\nWe propose the FDRL framework, a fine-grained disentangled representation learning method for MMER. FDRL focuses on achieving fine-grained feature disentanglement to alleviate the modal heterogeneity gap.\n\nWe introduce fine-grained alignment, disparity, and predictor components to learn refined disentangled features. These components are employed to adaptively align feature distributions, reduce information redundancy, increase feature diversity, and guide the learning of representations.\n\nExperimental results on the benchmark dataset, IEMOCAP, demonstrate the effectiveness of our proposed method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Method",
      "text": "As shown in Fig.  1 , our proposed method consists of three modules: the feature extraction module, the disentangled representation learning module, and the cross-modal fusion (CMF) module. Notably, the disentangled representation learning module contains three key components: the fine-grained alignment component, the fine-grained disparity component, and the fine-grained predictor component.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction Module",
      "text": "The sequence representations of the speech and text modalities are denoted as Xa ∈ R La×da and Xt ∈ R L t ×d t , respectively, where L represents the sequence length and d represents the embedding dimension. To obtain the high-level representation of each modality Hi, i ∈ {a, t}, we utilize Wav2vec2  [12]  and Bert  [13]  encoders to enrich the modal information:\n\nwhere θ wav a and θ bert t represent the trainable parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Disentangled Representation Learning Module",
      "text": "Shared and Private Encoders: The heterogeneity of modalities leads to information redundancy and distribution gaps in the highlevel representations. To address these issues, we propose using shared and private encoders to capture the commonality and complementarity of heterogeneous modalities. Specifically, the encoders project the features into shared and private feature subspaces. Both the shared encoder Es(•; θs) and the private encoder Ei(•; θi), i ∈ {a, t} are implemented using two fully connected layers with the ReLU activation function. With the encoders, the shared and private representations can be computed as follows:\n\nwhere Si and Pi ∈ R d . θs and θi are trainable parameters of the encoders. The parameters θs are shared among all modalities, whereas θi is learned separately for each modality. Fine-grained Alignment: Inspired by the dynamic adversarial adaptation network (DAAN)  [14] , this component is designed to align the shared representations Si. It comprises three elements: the global domain discriminator, the local subdomain discriminator, and the dynamic factor, as depicted in Fig.  2 .\n\n(i) Global domain discriminator: The blue part in Fig.  2  represents the global domain discriminator G d , which is utilized to align the global distribution of the speech and text modalities. The structure of the global domain discriminator is inspired by the domain adversarial neural network (DANN)  [15] , comprising two layers of perceptrons activated by the ReLU function. The loss of the global domain discriminator is calculated as follows:\n\nwhere lce denotes the cross-entropy loss (domain discriminator loss), and ym represents the ground-truth modality label.\n\nwhere l c ce denotes the cross-entropy loss for category c, and y c i represents the predicted probability distribution of category c for Si.\n\n(iii) Dynamic factor: The core of the fine-grained alignment component is the dynamic factor µ, which dynamically estimates the relative importance of global and local distributions. The global A-distance of the global domain discriminator is calculated as:\n\nwhere Ma and Mt denote the speech and text modalities.\n\nThe local A-distance of the local domain discriminator is:\n\nwhere M c a and M c t are samples from category c of the speech and text modalities, and L c l is the local subdomain discriminator loss for category c. Finally,the dynamic factor µ is computed as:\n\nFine-grained Disparity: This component consists of two parts: global modality discriminator and local orthogonal constraint.\n\n(i) Global modality discriminator: To generate refined private representations and enhance feature diversity, we encourage the global modality discriminator ℓGD(•; θGD) to distinguish the source of modality:\n\nwhere θGD are trainable parameters. The global modality discriminator comprises two-layer perceptrons with a ReLU activation function.\n\n(ii) Local orthogonal constraint: We apply a disparity loss to encourage the orthogonalization of shared and private features, which reduces information redundancy:\n\nwhere ∥∥ 2 F denotes the squared Frobenius norm. The local orthogonal constraint prevents the generation of trivial solutions during the process of encouraging feature orthogonalization.\n\nFine-grained Predictor: Our shared and private encoders only need to learn how to move the feature vectors in the feature space. It is worth noting that the mentioned restriction strategy alone is not sufficient to obtain reliable encoders, since it may randomly manipulate the representation without proper constraints. To this end, we require the fine-grained predictor component ℓF P (•; θF P ) to restrict that the output representation of the encoder should not change the associated label.\n\nwhere S represents the sum of Sa and St. ye represents the groundtruth emotion label. θF P is trainable parameters. The fine-grained predictor component is a one-layer perceptron.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Fusion Module",
      "text": "After obtaining these modal representations, we first stack them into a matrix F = [Sa, St, Pa, Pt] ∈ R 4×d . Then, we use the multihead self-attention mechanism  [16]  to obtain the final cross-modal representation.\n\nwhere\n\nW is trainable parameters. Finally, we concatenate each head to obtain the representation F task for the downstream task. The task loss and final loss are calculated as follows:\n\nwhere α, β, and γ represent the trade-off parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Dataset",
      "text": "The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [17]  is a widely used multimodal database for research in emotion recognition and analysis. The dataset is designed to facilitate research in automatic emotion recognition, sentiment analysis, and other related fields using multimodal data. The dataset is collected from ten actors, who participated in improvised sessions to simulate natural interactions. Our experiment considers four emotions: angry, happy, sad and neutral, where excitement class is merged into the happy class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We employ 5-fold cross-validation to evaluate our proposed framework, with four sessions for training and one session for testing. The batch size is set to 8 and the maximum training epoch is set to 100. We choose the AdamW optimizer with an initial learning rate of 10 -5 . The trade-off parameters α = γ = 1 and β = 0.5. We use the weighted average recall (WAR) and unweighted average recall (UAR) as metrics to evaluate the performance of our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "To evaluate the effectiveness of our proposed method, we compare it with various state-of-the-art (SOTA) methods, and the experimental results are listed in Table  1 . We observe that our method achieves the best performance in both WAR and UAR metrics, significantly outperforming the previous SOTA methods. This superior performance is mainly attributed to our method's exceptional ability to focus on learning the consistency and complementarity of heterogeneous modalities. These results provide strong evidence that our proposed fine-grained disentangled representation learning method can deliver promising performance for MMER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conduct ablation studies on the IEMOCAP dataset to assess the importance of different components in FDRL. Table  2  shows the results with the following observations. Importance of Regularization: Without the fine-grained alignment and disparity components (S1), our framework only learns the Table  1 . Classification performance of various state-of-the-art approaches on the IEMOCAP dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "WAR(%) UAR(%) BERT + FBK  [18]  70.56 71.46 SMCN  [19]  73.50 73.00 MCNN+BLSTM  [6]  74.98 75.05 BERT + Wav2vec2  [20]  -76.31 Multi-granularity model  [21]  74.80 75.50 SDTF-NET  [22]  77.20 76.70 MISA  [10]  76.18 75.73 Co-att + BAM+knowledge  [23]  75.50 77.00 Ours 78.34 79.44\n\nrepresentations of different modalities without feature disentanglement. This results in poor performance as it fails to overcome the heterogeneity of the modalities and the diversity of modal information. Moreover, we observe that constraining the encoders with the fine-grained predictor component (S6) also enhances the framework's performance. Importance of Global/Local discriminator: We evaluate the impact of global and local discriminators in the fine-grained alignment component by comparing the performance of S0 with S2 and S0 with S3. The FDRL framework improves by 0.95% on WAR and 1.18% on UAR compared to S2, and by 0.83% on WAR and 1.00% on UAR compared to S3. This indicates that merely aligning the global distributions of different modalities, although beneficial, is insufficient for learning better representations. Furthermore, aligning the local distributions of different modalities is crucial for learning more refined representations.\n\nImportance of Different Representations: By keeping the entire framework but using only partial representations in the CMF module (S4/S5), we observe that our proposed method outperform S4 and S5. This underscores the importance and necessity of learning both shared and private representations. Notably, private representations are found to be more expressive than shared representations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visualization",
      "text": "In Fig.  3  (a), we use t-SNE  [24]  to visualize the distributions of shared and private representations with and without the fine-grained alignment and disparity components. When α = 0 and β = 0, the distributions of Sa and St do not overlap, while the distributions of Si and Pi sometimes overlap, indicating that disentangled representations are not learned. In contrast, when α ̸ = 0 and β ̸ = 0, the distributions of Si begin to mix and overlap, while the distribution of Pi becomes progressively more distinct. This suggests that our method captures the commonality and complementarity of heterogeneous modalities, with the fine-grained alignment component minimizing the distribution between modalities and the fine-grained disparity component maximizing the feature diversity of modalities.\n\nIn Fig.  3  (b), we visualize the centroids of the shared representations Si for each category of text and speech modalities to further demonstrate the effectiveness of aligning local subdomain distributions (associated with categories) in the fine-grained alignment component. When µ = 0, the feature centroids of the same emotion from different modalities are distant from each other, indicating that shared features of local subdomains are not learned. When µ ̸ = 0, the feature centroids of the same emotion gradually come  closer together. This suggests that the distribution of local subdomains is brought closer by the fine-grained alignment component, which helps improve the performance of the framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a fine-grained disentangled representation learning framework to deal with the heterogeneity of modalities by disentangling the features of each modality. Our framework learns shared representations of heterogeneous modalities by capturing the consistency of global and local distributions of modalities through a fine-grained alignment component. Subsequently, we utilize the fine-grained disparity component to enhance the quality of private representations, thus reducing information redundancy. The finegrained predictor is employed to ensure the encoder does not alter the associated label. Finally, the CMF module is used to generate refined cross-modal representations. The effectiveness of our proposed method is demonstrated through a series of comparative experiments and ablation studies on the IEMOCAP dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall structure of the proposed framework.",
      "page": 2
    },
    {
      "caption": "Figure 1: , our proposed method consists of three modules:",
      "page": 2
    },
    {
      "caption": "Figure 2: (i) Global domain discriminator: The blue part in Fig. 2 repre-",
      "page": 2
    },
    {
      "caption": "Figure 2: represents the local subdomain discriminator Gc",
      "page": 2
    },
    {
      "caption": "Figure 2: Detail of the fine-grained alignment component. GRL repre-",
      "page": 3
    },
    {
      "caption": "Figure 3: (a), we use t-SNE [24] to visualize the distributions of",
      "page": 4
    },
    {
      "caption": "Figure 3: (b), we visualize the centroids of the shared repre-",
      "page": 4
    },
    {
      "caption": "Figure 3: (a) Visualization of the shared and private representations",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "speech and text. However,\ntheir approach directly concatenates the"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "representations of\nthe two modalities,\nthereby ignoring the hetero-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "geneity across them.\nSecond,\nresearchers have been committed to"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "exploring the correlations between different modal\ninformation to"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "obtain more\nrobust modal\nsemantics\nand,\nconsequently,\nimprove"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "the accuracy of emotion recognition tasks [7, 8, 9].\nFor example,"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "Hazarika et al.\n[10] utilized difference loss to promote the orthogo-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "nality of features, thereby increasing the multimodal complementary"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "information. Nonetheless,\nthis might not be an ideal solution as the"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "aforementioned difference loss alone is\ninsufficient\nfor obtaining"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "reliable representations and may lead to trivial\nsolutions without"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "appropriate constraints. More recently, Yang et al.\n[11] proposed"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "a method for disentangled representation learning, which extracts"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "both common and private feature representations for each modality."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "While this approach holds promise,\nit primarily focuses on global"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "modality alignment\nand disparity, neglecting fine-grained feature"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "disentanglement.\nThis oversight\nlimits the model’s ability to cap-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "ture\nthe nuances of\neach modality,\nresulting in only suboptimal"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "performance."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "To address the limitations of existing approaches and facilitate"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "fine-grained feature disentanglement, we\nintroduce\na novel fine-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "grained disentangled representation learning (FDRL)\nframework,"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "designed to address the heterogeneity of modalities.\nThis process"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "consists of\ntwo main steps.\nFirst, we learn modality-shared rep-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "resentations where a shared encoder projects each modality into a"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "modality-shared subspace. Here, a fine-grained alignment compo-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "nent is employed to balance global and local distribution alignment,"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "capturing modality consistency. Second, we learn modality-private"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "representations where private encoders map each modality,\nsepa-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "rately,\ninto a modality-private subspace. To ensure diversity, a fine-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "grained disparity component\nconstrains\nthe private\nlatent\nspaces."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "Moreover, we leverage a fine-grained predictor component to ensure"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "that\nthe\nlabel\nassociated with the output\nrepresentations\nremains"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "unchanged.\nFinally,\nthe cross-modal\nfusion (CMF) module is uti-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "lized to efficiently fuse\nthe different\nrepresentations\nfor MMER."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "Experimental results on the IEMOCAP dataset demonstrate that our"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "method surpasses\nstate-of-the-art methods, achieving 78.34% and"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "79.44% on WAR and UAR, respectively."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "Our main contributions are summarized as follows:"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "We\npropose\nthe FDRL framework,\na fine-grained\ndisentan-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "gled\nrepresentation learning method for MMER. FDRL focuses"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "on achieving fine-grained feature disentanglement\nto alleviate the"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "modal heterogeneity gap."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "We introduce fine-grained alignment, disparity,\nand predictor"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "components to learn refined disentangled features.\nThese compo-"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "nents are employed to adaptively align feature distributions, reduce"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "information redundancy,\nincrease feature diversity,\nand guide the"
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": ""
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "learning of representations."
        },
        {
          "2 Lingxi (Beijing) Technology Co., Ltd.": "Experimental\nresults\non\nthe\nbenchmark\ndataset,\nIEMOCAP,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. The overall structure of the proposed framework.": "the\nalign the shared representations Si. It comprises three elements:"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "global domain discriminator, the local subdomain discriminator, and"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "the dynamic factor, as depicted in Fig. 2."
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "(i) Global domain discriminator: The blue part\nin Fig. 2 repre-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "sents the global domain discriminator Gd, which is utilized to align"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "the global distribution of the speech and text modalities. The struc-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "ture of\nthe global domain discriminator\nis inspired by the domain"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "adversarial neural network (DANN) [15], comprising two layers of"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "perceptrons activated by the ReLU function. The loss of the global"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "domain discriminator is calculated as follows:"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "(cid:88)"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "(4)\nLg =\nlce (Gd (Si) , ym) ,"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "i∈a∪t"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "where lce denotes the cross-entropy loss (domain discriminator loss),"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "and ym represents the ground-truth modality label."
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "(ii) Local subdomain discriminator: The orange part\nin Fig.\n2"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "represents the local subdomain discriminator Gc\nd, which is used to"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "align the local distribution of the speech and text modalities. The lo-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "cal subdomain discriminator consists of two-layer perceptrons with"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "the activation function of ReLU. Specifically,\nlocal subdomain dis-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "criminators are associated with categories.\nEach category corre-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "The output of softmax in-\nsponds to a domain discriminator Gc\nd."
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "dicates the degree of attention given to Si by the domain discrimi-"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "nator. The loss of the local subdomain discriminator is computed as"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": "follows:"
        },
        {
          "Fig. 1. The overall structure of the proposed framework.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "representation.": "(cid:19)\n(cid:18) QKT"
        },
        {
          "representation.": "√\nV,\nAttention(Q, K, V ) = softmax\n(12)"
        },
        {
          "representation.": "dh"
        },
        {
          "representation.": "(cid:17)\n(cid:16)"
        },
        {
          "representation.": ", KW k\n, VW v\n,\nQW q\n(13)\nheadi = Attention\ni"
        },
        {
          "representation.": "where Q = K = V = F. W is trainable parameters.\nFinally,"
        },
        {
          "representation.": "we concatenate each head to obtain the representation Ftask for the"
        },
        {
          "representation.": "downstream task. The task loss and final\nloss are calculated as fol-"
        },
        {
          "representation.": "lows:"
        },
        {
          "representation.": ""
        },
        {
          "representation.": "(14)\nLtask = lce (Ftask, ye) ,"
        },
        {
          "representation.": ""
        },
        {
          "representation.": "Ltotal = Ltask + α[(1 − µ)Lg + µLl]"
        },
        {
          "representation.": "(15)\n+β(Lp + Ld) + γLf ,"
        },
        {
          "representation.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Classification performance of various state-of-the-art ap- Table 2. Results of ablation studies on IEMOCAP dataset (WAR",
      "data": [
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "proaches on the IEMOCAP dataset.",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "",
          "(WAR": ""
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "Methods",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "Model",
          "(WAR": "UAR"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "BERT + FBK [18]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "FDRL",
          "(WAR": "79.44"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "SMCN[19]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o (Lg + Ll) + (Lp+ Ld)",
          "(WAR": "77.36"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "MCNN+BLSTM [6]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o Global discriminator",
          "(WAR": "78.26"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "BERT + Wav2vec2 [20]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o Local discriminator",
          "(WAR": "78.44"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "Multi-granularity model [21]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o fine-grained alignment component",
          "(WAR": "77.43"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "SDTF-NET [22]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o fine-grained disparity component",
          "(WAR": "78.01"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "MISA [10]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "w/o fine-grained predictor component",
          "(WAR": "78.67"
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "Co-att + BAM+knowledge[23]",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "",
          "(WAR": ""
        },
        {
          "Table 1. Classification performance of various state-of-the-art ap-": "Ours",
          "Table 2. Results of ablation studies on IEMOCAP dataset": "",
          "(WAR": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Toutanova,\n“Bert:\nPre-training\nof\ndeep\nbidirectional"
        },
        {
          "5. REFERENCES": "[1] Chul Min Lee and Shrikanth S Narayanan, “Toward detecting",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "arXiv\npreprint\ntransformers\nfor\nlanguage\nunderstanding,”"
        },
        {
          "5. REFERENCES": "emotions in spoken dialogs,” IEEE transactions on speech and",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "arXiv:1810.04805, 2018."
        },
        {
          "5. REFERENCES": "audio processing, vol. 13, no. 2, pp. 293–303, 2005.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[14] Chaohui Yu, Jindong Wang, Yiqiang Chen, and Meiyu Huang,"
        },
        {
          "5. REFERENCES": "[2]\nFranc¸ois Michaud, Paolo Pirjanian, Jonathan Audet, and Do-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "“Transfer\nlearning with dynamic adversarial adaptation net-"
        },
        {
          "5. REFERENCES": "minic L´etourneau,\n“Artificial emotion and social\nrobotics,”",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "work,” in 2019 IEEE international conference on data mining"
        },
        {
          "5. REFERENCES": "Distributed autonomous robotic systems 4, pp. 121–130, 2000.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "(ICDM). IEEE, 2019, pp. 778–786."
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Ger-"
        },
        {
          "5. REFERENCES": "[3] Bj¨orn Schuller, Gerhard Rigoll, and Manfred Lang,\n“Speech",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "main, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand,"
        },
        {
          "5. REFERENCES": "emotion recognition combining acoustic features and linguis-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "and Victor Lempitsky, “Domain-adversarial training of neural"
        },
        {
          "5. REFERENCES": "tic information in a hybrid support vector machine-belief net-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "networks,” The journal of machine learning research, vol. 17,"
        },
        {
          "5. REFERENCES": "work architecture,” in 2004 IEEE international conference on",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "no. 1, pp. 2096–2030, 2016."
        },
        {
          "5. REFERENCES": "acoustics, speech, and signal processing. IEEE, 2004, vol. 1,",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "pp. I–577.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[16] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszko-"
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "5. REFERENCES": "[4] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Advances in neural\nPolosukhin,\n“Attention is all you need,”"
        },
        {
          "5. REFERENCES": "Zadeh, and Louis-Philippe Morency,\n“Words can shift: Dy-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "information processing systems, vol. 30, 2017."
        },
        {
          "5. REFERENCES": "namically adjusting word representations using nonverbal be-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "haviors,”\nin Proceedings of the AAAI Conference on Artificial",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[17] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "5. REFERENCES": "Intelligence, 2019, vol. 33, pp. 7216–7223.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Chang, Sungbok Lee,\nand Shrikanth S Narayanan,\n“Iemo-"
        },
        {
          "5. REFERENCES": "[5] Yi Zhang, Mingyuan Chen,\nJundong Shen,\nand Chongjun",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "cap:\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "5. REFERENCES": "Wang,\n“Tailor versatile multi-modal\nlearning for multi-label",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Language\nresources and evaluation,\nvol. 42,\npp. 335–359,"
        },
        {
          "5. REFERENCES": "emotion recognition,” in Proceedings of the AAAI Conference",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "2008."
        },
        {
          "5. REFERENCES": "on Artificial Intelligence, 2022, vol. 36, pp. 9100–9108.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[18] Edmilson Morais,\nRon Hoory, Weizhong\nZhu,\nItai Gat,"
        },
        {
          "5. REFERENCES": "[6] Yang Liu, Haoqin Sun, Wenbo Guan, Yuqi Xia,\nand Zhen",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Matheus Damasceno, and Hagai Aronowitz, “Speech emotion"
        },
        {
          "5. REFERENCES": "Zhao,\n“Multi-modal\nspeech\nemotion\nrecognition\nusing",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "recognition using self-supervised features,”\nin ICASSP 2022-"
        },
        {
          "5. REFERENCES": "self-attention mechanism and multi-scale fusion framework,”",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "5. REFERENCES": "Speech Communication, vol. 139, pp. 1–9, 2022.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "Signal Processing (ICASSP). IEEE, 2022, pp. 6922–6926."
        },
        {
          "5. REFERENCES": "[7]\nFengmao Lv, Xiang Chen, Yanyong Huang, Lixin Duan, and",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[19] Mixiao Hou, Zheng Zhang, and Guangming Lu, “Multi-modal"
        },
        {
          "5. REFERENCES": "Guosheng Lin,\n“Progressive modality reinforcement\nfor hu-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "emotion recognition with self-guided modality calibration,” in"
        },
        {
          "5. REFERENCES": "man multimodal emotion recognition from unaligned multi-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "ICASSP 2022-2022 IEEE International Conference on Acous-"
        },
        {
          "5. REFERENCES": "the IEEE/CVF Confer-\nmodal sequences,”\nin Proceedings of",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "tics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp."
        },
        {
          "5. REFERENCES": "ence on Computer Vision and Pattern Recognition, 2021, pp.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "4688–4692."
        },
        {
          "5. REFERENCES": "2554–2562.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[20] Zihan Zhao, Yanfeng Wang, and Yu Wang, “Multi-level fusion"
        },
        {
          "5. REFERENCES": "[8] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "of wav2vec 2.0 and bert for multimodal emotion recognition,”"
        },
        {
          "5. REFERENCES": "Kolter, Louis-Philippe Morency,\nand Ruslan Salakhutdinov,",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "arXiv preprint arXiv:2207.04697, 2022."
        },
        {
          "5. REFERENCES": "“Multimodal\ntransformer\nfor unaligned multimodal\nlanguage",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[21] Weiquan Fan, Xiaofen Xing, Bolun Cai, and Xiangmin Xu,"
        },
        {
          "5. REFERENCES": "the\nconference. Association\nsequences,”\nin Proceedings of",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "“Mgat:\nMulti-granularity\nattention\nbased\ntransformers\nfor"
        },
        {
          "5. REFERENCES": "for Computational Linguistics. Meeting. NIH Public Access,",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "ICASSP 2023-2023\nmulti-modal\nemotion\nrecognition,”\nin"
        },
        {
          "5. REFERENCES": "2019, vol. 2019, p. 6558.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "IEEE International Conference on Acoustics, Speech and Sig-"
        },
        {
          "5. REFERENCES": "[9] Georgios Ioannides, Michael Owen, Andrew Fletcher, Viktor",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "nal Processing (ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "5. REFERENCES": "Rozgic, and Chao Wang, “Towards Paralinguistic-Only Speech",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[22] Lu-Yao Liu, Wen-Zhe Liu, and Lin Feng, “Sdtf-net: Static and"
        },
        {
          "5. REFERENCES": "Representations\nfor End-to-End Speech Emotion Recogni-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "dynamic time–frequency network for speech emotion recogni-"
        },
        {
          "5. REFERENCES": "tion,” in Proc. INTERSPEECH 2023, 2023, pp. 1853–1857.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "tion,” Speech Communication, vol. 148, pp. 1–8, 2023."
        },
        {
          "5. REFERENCES": "[10] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Po-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[23] Zihan Zhao, Yu Wang, and Yanfeng Wang, “Knowledge-aware"
        },
        {
          "5. REFERENCES": "ria, “Misa: Modality-invariant and-specific representations for",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "bayesian co-attention for multimodal emotion recognition,” in"
        },
        {
          "5. REFERENCES": "the 28th\nmultimodal sentiment analysis,”\nin Proceedings of",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "ICASSP 2023-2023 IEEE International Conference on Acous-"
        },
        {
          "5. REFERENCES": "ACM international conference on multimedia, 2020, pp. 1122–",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "tics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp."
        },
        {
          "5. REFERENCES": "1131.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "1–5."
        },
        {
          "5. REFERENCES": "[11] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du,",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "[24] Laurens Van der Maaten and Geoffrey Hinton,\n“Visualizing"
        },
        {
          "5. REFERENCES": "and Lihua Zhang,\n“Disentangled representation learning for",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "data using t-sne.,”\nJournal of machine learning research, vol."
        },
        {
          "5. REFERENCES": "of\nthe\nmultimodal\nemotion\nrecognition,”\nin Proceedings",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": "9, no. 11, 2008."
        },
        {
          "5. REFERENCES": "30th ACM International Conference on Multimedia, 2022, pp.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "1642–1651.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "[12] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "Michael Auli, “wav2vec 2.0: A framework for self-supervised",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "learning of speech representations,” Advances in neural infor-",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        },
        {
          "5. REFERENCES": "mation processing systems, vol. 33, pp. 12449–12460, 2020.",
          "[13]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "Min Chul",
        "Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "3",
      "title": "Artificial emotion and social robotics",
      "authors": [
        "Paolo Franc ¸ois Michaud",
        "Jonathan Pirjanian",
        "Dominic Audet",
        "Létourneau"
      ],
      "year": "2000",
      "venue": "Distributed autonomous robotic systems"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE international conference on acoustics, speech, and signal processing"
    },
    {
      "citation_id": "5",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Yi Zhang",
        "Mingyuan Chen",
        "Jundong Shen",
        "Chongjun Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Yang Liu",
        "Haoqin Sun",
        "Wenbo Guan",
        "Yuqi Xia",
        "Zhen Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "8",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "10",
      "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
      "authors": [
        "Georgios Ioannides",
        "Michael Owen",
        "Andrew Fletcher",
        "Viktor Rozgic",
        "Chao Wang"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "11",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "12",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "15",
      "title": "Transfer learning with dynamic adversarial adaptation network",
      "authors": [
        "Chaohui Yu",
        "Jindong Wang",
        "Yiqiang Chen",
        "Meiyu Huang"
      ],
      "year": "2019",
      "venue": "2019 IEEE international conference on data mining (ICDM)"
    },
    {
      "citation_id": "16",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¸ois Laviolette",
        "Victor Marchand",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal emotion recognition with self-guided modality calibration",
      "authors": [
        "Mixiao Hou",
        "Zheng Zhang",
        "Guangming Lu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Multi-level fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "Zihan Zhao",
        "Yanfeng Wang",
        "Yu Wang"
      ],
      "year": "2022",
      "venue": "Multi-level fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "arxiv": "arXiv:2207.04697"
    },
    {
      "citation_id": "22",
      "title": "Mgat: Multi-granularity attention based transformers for multi-modal emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiaofen Xing",
        "Bolun Cai",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Sdtf-net: Static and dynamic time-frequency network for speech emotion recognition",
      "authors": [
        "Lu-Yao Liu",
        "Wen-Zhe Liu",
        "Lin Feng"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "24",
      "title": "Knowledge-aware bayesian co-attention for multimodal emotion recognition",
      "authors": [
        "Zihan Zhao",
        "Yu Wang",
        "Yanfeng Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}