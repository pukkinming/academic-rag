{
  "paper_id": "2407.06957v1",
  "title": "Listen And Speak Fairly: A Study On Semantic Gender Bias In Speech Integrated Large Language Models",
  "published": "2024-07-09T15:35:43Z",
  "authors": [
    "Yi-Cheng Lin",
    "Tzu-Quan Lin",
    "Chih-Kai Yang",
    "Ke-Han Lu",
    "Wei-Chih Chen",
    "Chun-Yi Kuan",
    "Hung-yi Lee"
  ],
  "keywords": [
    "stereotype",
    "large language model",
    "bias"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Integrated Large Language Models (SILLMs) combine large language models with speech perception to perform diverse tasks, such as emotion recognition to speaker verification, demonstrating universal audio understanding capability. However, these models may amplify biases present in training data, potentially leading to biased access to information for marginalized groups. This work introduces a curated spoken bias evaluation toolkit and corresponding dataset. We evaluate gender bias in SILLMs across four semanticrelated tasks: speech-to-text translation (STT), spoken coreference resolution (SCR), spoken sentence continuation (SSC), and spoken question answering (SQA). Our analysis reveals that bias levels are language-dependent and vary with different evaluation methods. Our findings emphasize the necessity of employing multiple approaches to comprehensively assess biases in SILLMs, providing insights for developing fairer SILLM systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Large Language Models (LLMs) have shown promising capability in recent years. However, LLMs cannot perceive non-textual modalities such as audio or image like humans. Speech is an important modality in human communications, providing more information such as emotion and speaker than text cannot. Many previous works try to incorporate LLMs with speech perception capabilities, resulting in Speech-Integrated Large Language Models (SILLMs). These models enable tasks such as speech translation and speech question answering (SQA)  [1] [2] [3] [4] .\n\nSILLMs include two main types: cascaded systems, which combine automatic speech recognition (ASR) with LLMs, and end-toend systems known as speech large language models (SLLMs). Cascaded systems use an ASR model to transcribe speech into text, which is then processed by an LLM. In contrast, SLLMs are designed to handle speech inputs directly and perform end-to-end processing.\n\nSILLMs use robust LLMs pre-trained on large-scale internet data like Wikipedia and Reddit and fine-tuned on extensive speechtext paired data. However, imbalances in the domain, type, and speaker representation in the training data can introduce and amplify biases, leading to stereotypical outputs  [5] [6] [7] . For instance, 87% of the editors on Wikipedia are male  [8] , and less than 19% of English biographies are about women  [9] , which might lead to a skewed representation of females in such a widely-used dataset  [10] , causing unintended consequences in deployed SILLM.\n\nBiased SILLM may affect the society in several aspects. In education, students relying on SILLM for speech-to-text translation to * Equal second contribution learn languages and access educational content may receive biased or incorrect information, impacting their learning outcomes  [11, 12] . In business, biased SILLM creates barriers for non-native speakers and marginalized groups in accessing information  [13] , services  [14] , and opportunities  [15] , impacting economic mobility and inclusion  [16] . However, despite there are some works on analyzing the stereotype and bias in LLM  [17] [18] [19]  and Multi-modal LLM  [20] [21] [22] , none of them discuss speech-integrated LLMs.\n\nThis study introduces a toolkit and the corresponding dataset for evaluating gender bias in SILLMs across four semantic-related tasks: speech-to-text translation (STT), spoken coreference resolution (SCR), spoken sentence continuation (SSC), and spoken question and answering (SQA).\n\nIn STT, translating from English to grammatically gendered languages is challenging because these languages have complex gender systems that require nouns, articles, verbs, and adjectives to match the noun's gender. This can cause incorrect translations from English to languages like Spanish, due to English's lack of explicit gender indicators  [23] . In SCR, a model may exhibit coreference bias when it links gendered pronouns to occupations stereotypically associated with that gender more accurately than to occupations not associated with that gender  [24] . In SSC and SQA, a model might choose the continuation or answer based on stereotypes. This might harm particular social groups by reinforcing existing biases and assigning biased traits to individuals based on perceived identities. To our knowledge, this is the first work to evaluate social bias in SLLMs.\n\nOur work yields the following contributions:\n\n• We curate an evaluation tool on the gender bias of current SILLMs on 4 different tasks.\n\n• We curate and release the spoken bias evaluation dataset based on datasets in natural language processing.\n\n• We find out speech-text fine-tuning LLM to SLLM can reduce stereotypical association in STT and SCR. However, this finetuning has little effect on reducing biases SSC and SQA.\n\n• We find out the bias in SILLM is language dependent. Prompting LLM with different languages to translate to or using different language QA yields different results.\n\n• Our findings show that model bias levels vary with different evaluation methods, highlighting the necessity of using multiple approaches to assess biases in SILLMs and ASR-LLMs comprehensively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Recently, there has been an increasing focus on fairness issues within the speech domain. Earlier research has thoroughly examined the influence of social biases on various speech tasks, such as automatic speech recognition  [25] [26] [27] , speech translation  [28, 29] , and emotion recognition  [30] [31] [32] . These studies primarily analyze biases in models designed for specific tasks. Additionally, Meng et al.  [33]  explored the impact of data bias on self-supervised speech models and their downstream tasks, while Lin et al.  [34]  investigated how model architecture affects bias in self-supervised speech model representations. However, most studies focus on speech models, leaving speech-text models largely unexplored.\n\nThe development of LLMs also raises concerns about potential bias issues. Numerous works have proposed methods to measure biases in LLM generations like coreference resolution  [24, 35, 36] . At the same time, several datasets have been proposed for social bias evaluation. Crows-Pairs  [18]  and StereoSet  [37]  use crowdsourced sentences to reveal a wide range of social biases in language models, and concurrently BBQ  [17]  was further proposed and designed as a QA task to measure biases in language models.\n\nFollowing the trend of LLM, Multi-modal LLMs are designed to process and analyze different types of data, such as images, videos, and audio. This has raised concerns about biases in these pre-trained models. In the vision-language domain, some studies have investigated social biases in pre-trained vision-language models, such as image captioning  [20]  and image classification  [21, 22] . However, research on speech-language models remains limited. To our knowledge, our work is the first to investigate bias and fairness in speechlanguage models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experiment Setup",
      "text": "Our experimental framework is built to be easily reproducible, allowing researchers to integrate and evaluate additional SILLMs and various assessment tasks seamlessly.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Specialized Model",
      "text": "Among the 4 evaluation tasks, only STT has specialized models. We employ the state-of-the-art STT model, SeamlessM4T v2 Large  [38]  as a baseline. We leverage both the speech-to-text translation and speech-to-speech translation capabilities of SeamlessM4T. Spoken English sentences are input into this model, and the resulting translated text is directly used for bias evaluation in the speech-to-text mode. For the speech-to-speech mode, the translated speech output is first transcribed using Whisper v3 Large  [39]  and then used for bias evaluation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Automatic Speech Recognition With Llm",
      "text": "For our cascaded system experiments, we use Whisper Large v3, a strong encoder-decoder transformer model that reaches SOTA in multilingual ASR. We choose LLMs based on two categories:\n\n1. SOTA LLMs: This includes open-source models like LLaMA 3 (8B and 70B Instruct) and commercial products like Chat-GPT API (GPT-3.5 and GPT-4o). These models are chosen for their exceptional language modeling capabilities.\n\n2. Backbone LLM of SLLMs: This includes models that serve as the backbone for speech large language models before instruction finetuning, such as Qwen  [40] , Vicuna (7B and 13B)  [41] , and LLaMA 2  [42]  (7B Chat).\n\nThese selections allow us to comprehensively evaluate the performance and biases of cascaded systems combining ASR and LLMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Llm",
      "text": "We evaluated all tasks using three leading Speech LLMs: Qwen-Audio-Chat  [1] , SALMONN  [4] , and WavLLM  [43] . Each model uses pre-trained encoders that convert audio inputs into continuous features and a text-based LLM to process both speech features and text instructions to generate responses. Table  1  details the pretrained models used. The SLLMs are fine-tuned end-to-end with instruction-tuning datasets for speech-processing tasks, enabling them to understand speech inputs and execute instructions from the underlying LLMs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset And Evaluation",
      "text": "All the prompts used in our evaluation are described in Table  2 . The examples of these datasets are shown in Fig.  1 . We will release this evaluation toolkit and associated dataset in the future.  1 For the evaluations with multiple options (SCR, SSC, and SQA), we first check if the model output contains only a single option. If so, we select it. If not, we calculate the Character Error Rate (CER) for each option with the output. We choose the option with the lowest CER if it is below 10%; otherwise, we consider the output as not following the instructions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech-To-Text Translation",
      "text": "In our study, we employ a subset of WinoST  [29]  to assess biases. WinoST, the spoken counterpart to the WinoMT  [23]  dataset, evaluates bias by examining the inflection of gendered terms in sentences translated from English to other languages. The example in Fig.  1  shows that the female CEO is translated as male-gendered (El CEO) in Spanish. For our analysis, we select German (de), Spanish (es), French (fr), and Italian (it) as the target languages.\n\nTo evaluate bias in STT, we adopt three metrics: accuracy, ∆G, and ∆S, following  [23] . The accuracy is the percentage of correctly identified gender entities. ∆G is the difference between F1 score of male entities and female entities. ∆S is the difference between F1 score of pro-stereotypical entities and anti-stereotypical entities. When △S is lower, it indicates that the model is less likely to exhibit gender bias in translations due to gender stereotypes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Coreference Resolution",
      "text": "To evaluate bias in coreference resolution, we utilize the development set of the WinoBias  [24]  dataset and the audio clips from WinoST. WinoBias is a dataset specifically designed to examine biases related to entities identified by their occupations, utilizing 40 distinct professions. The associated occupation statistics from the population survey  [47]  are used to determine stereotypical relations. The example in Fig.  1  shows that \"she\" logically and grammatically refers to the chief rather than the accountant, but SLLM might stereotypically choose \"accountant\" due to gender roles.\n\nWinoBias contains two types of data: Type 1 and Type 2. Type 1 is more challenging because it requires the model to identify coreference entities based on real-world knowledge of the given circumstances. In contrast, Type 2 is easier, as coreference can be resolved using the syntactic relationship of the entities.\n\nWe report the F1 score of coreference resolution for prostereotypical and anti-stereotypical entities, and we also report the F1 score of coreference resolution for male and female entities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Continuation",
      "text": "We use the inter-sentence task and domain gender of Stereoset  [37]  for continuation stereotype evaluation, consisting of 242 instances. The original dataset can be formulated as D = {(C, Cs, Ca, Ci)}, where C is a context describing the target group, Cs, Ca, Ci denotes a stereotypical, anti-stereotypical, or irrelevant continuation to the context sentence, respectively. We construct spoken context by synthesizing C using 3 Text-To-Speech (TTS) providers: Azure 2  , Google 3  , and Amazon 4  , selecting one male and one female speaker from each to ensure diversity, totaling six speakers.\n\nWe utilize four metrics to quantify stereotypes in continuations: language modeling score (lms), stereotypical score (ss), idealized Context Association Tests score (icat), and instruction following rate (ifr). lms is the percent of continuations that are related to the context (Cs or Ca). ss is the percentage that the model prefers stereotypical continuations over anti-stereotypical continuations. An ideal model would consistently select meaningful responses of both Cs and Ca with equal likelihood, resulting in an lms of 100 and an ss of 50. Conversely, a biased model would predominantly choose stereotypical responses, leading to an ss of 100. icat is a metric designed for real-world model deployment, showing the balance between a model's language modeling capability and bias. icat is defined as:\n\nso that an ideal language model has an icat score of 100 and a stereotypical model has an icat score of 0. Finally, since some SLLMs may fail to follow the format specified in the instruction, we calculate ifr to measure how well an SLLM can follow the format and its response can be parsed. ifr is defined as the percentage of model responses that can be parsed as one and only one of the three options.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Question And Answer",
      "text": "We use the \"gender identity\" domain from the Bias Benchmark for QA (BBQ)  [17]  and the \"gender\" domain from the Chinese Bias Benchmark for QA (CBBQ)  [48] . Both are similarly structured but differ in language and social context. For BBQ, we separate instances into transgender and binary gender subsets to better identify bias targets, using all 864 transgender-targeted instances and a similar number for binary-gendered targets. For CBBQ, we randomly select a subset for binary-gendered targets. We exclude transgender instances from the CBBQ dataset because they are generated using only one template and are not sufficient for sound conclusions.\n\nEach BBQ and CBBQ instance includes a context sentence, a question, and three options. Instances are grouped into four types: negative or non-negative questions with ambiguous or disambiguous contexts. The negative question reflects a social bias toward certain social groups, breaking a social value in the US. The non-negative question does not. The ambiguous context only provides a general scenario, leading to an \"UNKNOWN\" answer. The disambiguous context offers detailed clues, ensuring the correct answer is one of the groups mentioned in the question.\n\nWe synthesize spoken BBQ context using the same speakers and TTS providers as in Stereoset, and spoken CBBQ contexts are synthesized using Google, Azure, and TTSMaker TTS API 5  with balanced gender  6  .\n\nWe use accuracy and bias scores to quantify the bias. Accuracy is the percentage of correctly answered questions among all questions. Bias score is defined as  [17]  so that 0% indicates no bias is measured, 100% indicates all answers are aligned with social bias, and -100% indicates all answers are in contrast to social bias. The bias score in disambiguous context is:\n\nand the bias score for ambiguous context is: The bias score for ambiguous context is adjusted based on accuracy, emphasizing that bias is more significant when it occurs more frequently.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Result",
      "text": "To ensure the synthesized speeches are representative, we report the confidence intervals for our measurements in SSC and SQA. This is done by calculating the standard deviation of the mean for each measurement across six synthesized speeches.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech-To-Text Translation",
      "text": "Table  3  presents the gender bias across different language pairs. Translating to various languages results in different levels of bias and accuracy patterns, highlighting the importance of evaluating multiple languages. First, we can observe that compared to other models, GPT-4o achieves a significantly higher accuracy in translating gender entities. Additionally, GPT-4o's △G and △S are notably low, indicating that gender stereotypes are unlikely to influence its translation outcomes. Compared to GPT-3.5, GPT-4o not only shows significant improvements in the accuracy of predicting gender entities but also exhibits a considerable reduction in gender bias. This improvement may be due to GPT-4o undergoing more extensive safety testing and refinement.\n\nSecond, the translation accuracy of SLLMs is generally lower compared to other models, suggesting they are not yet capable of high-quality speech-to-text translation, with ASR-LLM cascaded models usually performing better. Additionally, SLLMs exhibit a higher △G, indicating that they translate male entities with a much greater F1 score than female entities. This might be because male entities are more likely to exist in speech-text fine-tuning data  [49] . Despite the significant performance gap between genders in SLLMs (△G), surprisingly, they are less likely to be influenced by gender stereotypes in their translation outcomes (△S).\n\nThird, Seamless, despite being specialized in speech translation, does not exhibit high accuracy for translating gender entities, suggesting that speech-to-text/speech translation is more challenging than text-to-text translation. While its accuracy is lower than GPT-4o, Seamless is less influenced by gender stereotypes, showing a similarly low △S as GPT-4o.\n\nFourth, comparing SLLMs to their backbone models, SLLMs show lower accuracy, higher △G, and lower △S across all trans-lated languages. This indicates that SLLMs rely less on stereotypes but at the cost of overall accuracy, especially for translating feminine words. This increased △G (decreased female F1 score) may be due to the higher frequency of masculine words in the speech-text fine-tuning dataset.\n\nIn summary, GPT-4o not only exhibits the best translation performance but also minimally suffers from the influence of gender stereotypes in its translations. Additionally, SLLMs generally do not perform well in translations and are more accurate in translating male entities than female entities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Coreference Resolution",
      "text": "Table  4  demonstrates the F1 score of coreference on the Winobias dataset. First, all models exhibit some gender stereotypes, creating biases (pro/anti diff) when determining co-reference, but they are less biased by the gender of the entity itself (male/female diff).\n\nSecond, SLLMs perform poorly in coreference tasks, with significantly lower F1 scores than LLMs, particularly SALMONN 7B, indicating that coreference in speech-to-text is more challenging than in text-to-text. Among LLMs, LLaMA 3 70B excels at coreference tasks and is less affected by gender stereotypes compared to other LLMs. Within SLLMs, WavLLM shows slightly more bias than others.\n\nThird, comparing SLLMs and their backbone models, SLLMs show lower pro-stereotypical bias than ASR-LLMs. This could be attributed to the fine-tuning process of SLLMs using audioinstruction pairs. These datasets are designed to answer specific attributes of audio and speech without including complex stereotypical relationships, which may lead SLLMs to lose some of the intricate bias associations learned from the more complex, webcrawled pretraining data, resulting in less biased SLLMs.\n\nUnlike machine translation, GPT-4o does not have a significant advantage over other LLMs in the coreference task. Besides performing worse than LLaMA 3 70B, it also exhibits relatively greater gender bias compared to other LLMs in some cases.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Continuation",
      "text": "Table  5  shows the Stereoset evaluation result. First, ASR-LLMs have higher language modeling scores than SLLMs. Most ASR-LLM models also achieve over 80% in ifr, while only Qwen-Audio and SALMONN in the SLLMs exceed this threshold, indicating that some SLLMs have lower language generation and instructionfollowing capabilities. ASR-LLMs, especially SOTA LLMs, show higher bias scores, suggesting stronger social biases compared to SLLMs. This can be attributed to the same reason in 4.2. Second, among the SLLMs, Qwen-Audio achieves the highest ICAT score, indicating that it offers the best balance between usability and fairness, making it the most suitable for deployment as a speech language modeling service.\n\nThird, comparing SLLM and their backbone LLMs, Qwen-Audio and Vicuna 13B has lower ss and higher ifr, while Vicuna 7B and WavLLM has higher ss and lower ifr. These results indicate that finetuning with speech instruction datasets does not consistently reduce bias across all models and tasks.   6  presents the results of the BBQ evaluation. First, ASR-LLM systems exhibit high accuracy, demonstrating robust language understanding capabilities, except Vicuna 7B and LLaMA2 7B, which struggle in ambiguous contexts. Conversely, SLLMs show low accuracy in ambiguous contexts, often failing to respond with \"unknown\" when no clear answer exists, thereby exhibiting greater bias in such scenarios. This issue may stem from the fine-tuning data, as most examples are paired with specific answers, causing SLLMs to overlook responding with \"I don't know\" or its equivalents in English. Second, comparing SLLM and its backbone LLM, SLLMs are more biased than cascaded systems, especially SALMONN 7B and WavLLM, which are the most biased models in BBQ regardless of targets and contexts.\n\nMoreover, we find that the cascaded systems with GPT-4o and LLaMA 3 70B have the smallest bias score and the highest accuracy across most evaluations, highlighting a good balance between performance and bias.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Chinese Qa: Cbbq",
      "text": "Refer to Table  7 , the results in Chinese QA differ from those in English. We excluded LLaMA-2 from the evaluation on CBBQ because it struggles with following Chinese instructions and outputs unstable responses. For ASR-LLM systems, performance is better in ambiguous contexts compared to disambiguous ones. These systems exhibit a medium bias score of around -5 to 15 in ambiguous contexts, and large bias scores ranging from -86 to -3 in disambiguous contexts, indicating a greater bias than observed in English QA.\n\nSLLMs perform better in ambiguous contexts in Chinese QA, showing higher accuracy and better handling of \"unknown\" responses compared to English QA. However, their accuracy in dis-ambiguous contexts is generally lower than that of English QA, with larger bias scores also observed in disambiguous scenarios.\n\nComparing SLLMs and their backbone LLMs, all models have less bias scores in ambiguous contexts, but only one model Qwen has less bias scores in the disambiguous context. Notably, most of the speech-text instruction fine-tuning datasets are in English. Therefore, the complex relationship between bias and instruction finetuning requires further investigation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitation And Future Work",
      "text": "The evaluation in this study primarily addresses bias in the semantics of speech, providing a detailed and rigorous analysis of a subset of gendered languages and tasks. However, future research should expand this focus to include biases related to speaker identity, such as gender and race. By incorporating these additional dimensions of bias, subsequent evaluations can offer a more comprehensive understanding of the impact of speech large language models on diverse user groups. Moreover, broadening the scope to investigate biases in a wider range of languages and contexts will help uncover additional biases, thereby contributing to the development of more inclusive and equitable speech technologies.\n\nThe datasets Stereset and BBQ focus on biases within the social context of the US, while the CBBQ dataset addresses biases in China. These datasets are not comprehensive and may not encompass biases present in other cultural contexts. Future research can develop more diverse datasets that reflect a wider range of social contexts. This will improve bias evaluation and facilitate the development of more effective debiasing techniques across different cultures and regions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces a gender bias evaluation toolkit and corresponding datasets for SILLMs. Our evaluation reveals that SILLMs exhibit gender bias across the four tasks assessed. We also found that prompting SILLMs for different languages results in varying bias patterns. In both STT and SCR, SLLMs show less pro-stereotypical association than their backbone LLMs, suggesting instruction finetuning can reduce these biases. However, this reduction is not consistent in SSC and SQA. This is likely because biases in STT and SCR are more explicit, while biases in SSC and SQA are more complex and implicit.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of our evaluation for bias on 4 tasks. SLLM and ASR-LLM systems process speech inputs and textual instructions to",
      "page": 2
    },
    {
      "caption": "Figure 1: We will release this",
      "page": 3
    },
    {
      "caption": "Figure 1: shows that the female CEO is translated as male-gendered (El CEO)",
      "page": 3
    },
    {
      "caption": "Figure 1: shows that “she” logically and grammati-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "National Taiwan University, Taiwan": "learn languages and access educational content may receive biased"
        },
        {
          "National Taiwan University, Taiwan": "or incorrect information, impacting their learning outcomes [11,12]."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "In business, biased SILLM creates barriers\nfor non-native speak-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "ers and marginalized groups in accessing information [13], services"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "[14], and opportunities [15], impacting economic mobility and inclu-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "sion [16]. However, despite there are some works on analyzing the"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "stereotype and bias in LLM [17–19] and Multi-modal LLM [20–22],"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "none of them discuss speech-integrated LLMs."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "This\nstudy introduces a toolkit and the corresponding dataset"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "for evaluating gender bias in SILLMs across four semantic-related"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "tasks:\nspeech-to-text\ntranslation (STT), spoken coreference resolu-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "tion (SCR), spoken sentence continuation (SSC), and spoken ques-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "tion and answering (SQA)."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "In STT, translating from English to grammatically gendered lan-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "guages is challenging because these languages have complex gender"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "systems that require nouns, articles, verbs, and adjectives to match"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "the noun’s gender. This can cause incorrect translations from English"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "to languages like Spanish, due to English’s lack of explicit gender in-"
        },
        {
          "National Taiwan University, Taiwan": "dicators [23]. In SCR, a model may exhibit coreference bias when it"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "links gendered pronouns to occupations stereotypically associated"
        },
        {
          "National Taiwan University, Taiwan": "with that gender more accurately than to occupations not associ-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "ated with that gender [24].\nIn SSC and SQA, a model might choose"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "the continuation or answer based on stereotypes. This might harm"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "particular social groups by reinforcing existing biases and assign-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "ing biased traits to individuals based on perceived identities. To our"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "knowledge, this is the first work to evaluate social bias in SLLMs."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "Our work yields the following contributions:"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "• We curate an evaluation tool on the gender bias of current"
        },
        {
          "National Taiwan University, Taiwan": "SILLMs on 4 different tasks."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "• We\ncurate\nand release\nthe\nspoken bias\nevaluation dataset"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "based on datasets in natural language processing."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "• We find out speech-text fine-tuning LLM to SLLM can reduce"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "stereotypical association in STT and SCR. However, this fine-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "tuning has little effect on reducing biases SSC and SQA."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "• We\nfind\nout\nthe\nbias\nin\nSILLM is\nlanguage\ndependent."
        },
        {
          "National Taiwan University, Taiwan": "Prompting LLM with\ndifferent\nlanguages\nto\ntranslate\nto"
        },
        {
          "National Taiwan University, Taiwan": "or using different language QA yields different results."
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "• Our findings show that model bias levels vary with different"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "evaluation methods, highlighting the necessity of using mul-"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "tiple approaches to assess biases in SILLMs and ASR-LLMs"
        },
        {
          "National Taiwan University, Taiwan": ""
        },
        {
          "National Taiwan University, Taiwan": "comprehensively."
        },
        {
          "National Taiwan University, Taiwan": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: details the pre-",
      "data": [
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "transformer model"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "multilingual ASR. We choose LLMs based on two categories:"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "1. SOTA LLMs: This includes open-source models like LLaMA"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "3 (8B and 70B Instruct) and commercial products like Chat-"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "GPT API (GPT-3.5 and GPT-4o). These models are chosen"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "for their exceptional language modeling capabilities."
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "2. Backbone LLM of SLLMs: This includes models that serve"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "as the backbone for speech large language models before in-"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "struction finetuning, such as Qwen [40], Vicuna (7B and 13B)"
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": "[41], and LLaMA 2 [42] (7B Chat)."
        },
        {
          "For our cascaded system experiments, we use Whisper Large v3,": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: details the pre-",
      "data": [
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "generate the textual responses.",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "speech recognition [25–27],\nspeech translation [28, 29], and emo-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "is first\ntranscribed using Whisper v3 Large [39] and then used for"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "tion recognition [30–32]. These studies primarily analyze biases in",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "bias evaluation."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "models designed for specific tasks. Additionally, Meng et al.\n[33]",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "explored the impact of data bias on self-supervised speech models",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "and their downstream tasks, while Lin et al.\n[34]\ninvestigated how",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "3.1.2. Automatic Speech Recognition with LLM"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "model architecture affects bias in self-supervised speech model rep-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "For our cascaded system experiments, we use Whisper Large v3,"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "resentations. However, most studies focus on speech models, leaving",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "a strong encoder-decoder\ntransformer model\nthat\nreaches SOTA in"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "speech-text models largely unexplored.",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "multilingual ASR. We choose LLMs based on two categories:"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "The development of LLMs also raises concerns about potential",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "bias issues. Numerous works have proposed methods to measure",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "1. SOTA LLMs: This includes open-source models like LLaMA"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "biases in LLM generations like coreference resolution [24, 35, 36].",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "3 (8B and 70B Instruct) and commercial products like Chat-"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "At the same time, several datasets have been proposed for social bias",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "GPT API (GPT-3.5 and GPT-4o). These models are chosen"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "evaluation. Crows-Pairs [18] and StereoSet [37] use crowdsourced",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "for their exceptional language modeling capabilities."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "sentences to reveal a wide range of social biases in language models,",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "and concurrently BBQ [17] was further proposed and designed as a",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "2. Backbone LLM of SLLMs: This includes models that serve"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "QA task to measure biases in language models.",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "as the backbone for speech large language models before in-"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "Following the trend of LLM, Multi-modal LLMs are designed to",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "struction finetuning, such as Qwen [40], Vicuna (7B and 13B)"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "process and analyze different\ntypes of data, such as images, videos,",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "[41], and LLaMA 2 [42] (7B Chat)."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "and audio. This has raised concerns about biases in these pre-trained",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "models.\nIn the vision-language domain, some studies have investi-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "These selections allow us to comprehensively evaluate the perfor-"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "gated social biases in pre-trained vision-language models, such as",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "mance and biases of cascaded systems combining ASR and LLMs."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "image captioning [20] and image classification [21, 22]. However,",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "research on speech-language models remains limited. To our knowl-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "edge, our work is the first to investigate bias and fairness in speech-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "3.1.3.\nSpeech LLM"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "language models.",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "We evaluated all\ntasks using three leading Speech LLMs: Qwen-"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "3. EXPERIMENT SETUP",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "Audio-Chat\n[1], SALMONN [4], and WavLLM [43]. Each model"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "uses pre-trained encoders that convert audio inputs into continuous"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "Our experimental\nframework is built\nto be easily reproducible, al-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "features and a text-based LLM to process both speech features and"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "lowing researchers to integrate and evaluate additional SILLMs and",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "text\ninstructions\nto generate\nresponses.\nTable 1 details\nthe pre-"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "various assessment tasks seamlessly.",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "trained models used.\nThe SLLMs are fine-tuned end-to-end with"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "instruction-tuning\ndatasets\nfor\nspeech-processing\ntasks,\nenabling"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "3.1. Model",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "them to understand speech inputs and execute instructions from the"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "underlying LLMs."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "3.1.1.\nSpecialized model",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "Among the 4 evaluation tasks, only STT has specialized models. We",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "Table 1. Overview of architectures of SLLMs."
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "employ the state-of-the-art STT model, SeamlessM4T v2 Large [38]",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "Model\nEncoder\nLLM"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "as a baseline. We leverage both the speech-to-text\ntranslation and",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "speech-to-speech translation capabilities of SeamlessM4T. Spoken",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "SALMONN\nBeats [44] + Whisper\nVicuna 7B, 13B [45]"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "WavLLM\nWavLM [46] + Whisper\nLLaMA2-7B-chat"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "English sentences are input\ninto this model, and the resulting trans-",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": "Qwen-Audio-Chat\nWhisper\nQwen-7B [40]"
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "lated text\nis directly used for bias evaluation in the speech-to-text",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        },
        {
          "Fig. 1. Examples of our evaluation for bias on 4 tasks.": "mode. For the speech-to-speech mode,\nthe translated speech output",
          "SLLM and ASR-LLM systems process speech inputs and textual\ninstructions to": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "language modeling score (lms),\nstereotypical\nscore (ss),\nidealized"
        },
        {
          "3.2. Dataset and evaluation": "All the prompts used in our evaluation are described in Table 2. The",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "Context Association Tests score (icat), and instruction following rate"
        },
        {
          "3.2. Dataset and evaluation": "examples of these datasets are shown in Fig. 1. We will release this",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "(ifr).\nlms is the percent of continuations that are related to the context"
        },
        {
          "3.2. Dataset and evaluation": "evaluation toolkit and associated dataset in the future.1",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "(Cs or Ca). ss is the percentage that the model prefers stereotypical"
        },
        {
          "3.2. Dataset and evaluation": "For the evaluations with multiple options (SCR, SSC, and SQA),",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "continuations over anti-stereotypical continuations. An ideal model"
        },
        {
          "3.2. Dataset and evaluation": "we first check if the model output contains only a single option. If so,",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "would consistently select meaningful responses of both Cs and Ca"
        },
        {
          "3.2. Dataset and evaluation": "we select it.\nIf not, we calculate the Character Error Rate (CER) for",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "with equal\nlikelihood,\nresulting in an lms of 100 and an ss of 50."
        },
        {
          "3.2. Dataset and evaluation": "each option with the output. We choose the option with the lowest",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "Conversely, a biased model would predominantly choose stereotyp-"
        },
        {
          "3.2. Dataset and evaluation": "CER if\nit\nis below 10%; otherwise, we consider\nthe output as not",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "icat\nical\nresponses,\nleading to an ss of 100.\nis a metric designed"
        },
        {
          "3.2. Dataset and evaluation": "following the instructions.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "for\nreal-world model deployment,\nshowing the balance between a"
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "model’s language modeling capability and bias.\nicat is defined as:"
        },
        {
          "3.2. Dataset and evaluation": "3.2.1.\nSpeech-to-text Translation",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "min(ss, 100 − ss)"
        },
        {
          "3.2. Dataset and evaluation": "In our study, we employ a subset of WinoST [29] to assess biases.",
          "We utilize four metrics to quantify stereotypes in continuations:": "icat = lms ×\n,\n(1)"
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "50"
        },
        {
          "3.2. Dataset and evaluation": "WinoST, the spoken counterpart to the WinoMT [23] dataset, evalu-",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "ates bias by examining the inflection of gendered terms in sentences",
          "We utilize four metrics to quantify stereotypes in continuations:": "so that an ideal language model has an icat score of 100 and a stereo-"
        },
        {
          "3.2. Dataset and evaluation": "translated from English to other languages. The example in Fig. 1",
          "We utilize four metrics to quantify stereotypes in continuations:": "typical model has an icat score of 0.\nFinally, since some SLLMs"
        },
        {
          "3.2. Dataset and evaluation": "shows that the female CEO is translated as male-gendered (El CEO)",
          "We utilize four metrics to quantify stereotypes in continuations:": "may fail\nto follow the format specified in the instruction, we calcu-"
        },
        {
          "3.2. Dataset and evaluation": "in Spanish. For our analysis, we select German (de), Spanish (es),",
          "We utilize four metrics to quantify stereotypes in continuations:": "late ifr to measure how well an SLLM can follow the format and"
        },
        {
          "3.2. Dataset and evaluation": "French (fr), and Italian (it) as the target languages.",
          "We utilize four metrics to quantify stereotypes in continuations:": "its response can be parsed.\nifr is defined as the percentage of model"
        },
        {
          "3.2. Dataset and evaluation": "To evaluate bias in STT, we adopt three metrics: accuracy, ∆G,",
          "We utilize four metrics to quantify stereotypes in continuations:": "responses that can be parsed as one and only one of the three options."
        },
        {
          "3.2. Dataset and evaluation": "and ∆S, following [23]. The accuracy is the percentage of correctly",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "identified gender entities. ∆G is the difference between F1 score",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "3.2.4. Question and Answer"
        },
        {
          "3.2. Dataset and evaluation": "of male entities and female entities. ∆S is the difference between",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "F1 score of pro-stereotypical entities and anti-stereotypical entities.",
          "We utilize four metrics to quantify stereotypes in continuations:": "We use the “gender identity” domain from the Bias Benchmark for"
        },
        {
          "3.2. Dataset and evaluation": "When △S is lower, it indicates that the model is less likely to exhibit",
          "We utilize four metrics to quantify stereotypes in continuations:": "QA (BBQ)\n[17] and the “gender” domain from the Chinese Bias"
        },
        {
          "3.2. Dataset and evaluation": "gender bias in translations due to gender stereotypes.",
          "We utilize four metrics to quantify stereotypes in continuations:": "Benchmark for QA (CBBQ) [48]. Both are similarly structured but"
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "differ\nin language and social context.\nFor BBQ, we separate in-"
        },
        {
          "3.2. Dataset and evaluation": "3.2.2. Coreference resolution",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "stances into transgender and binary gender subsets to better identify"
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "bias targets, using all 864 transgender-targeted instances and a sim-"
        },
        {
          "3.2. Dataset and evaluation": "To evaluate bias in coreference resolution, we utilize the develop-",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "ilar number for binary-gendered targets. For CBBQ, we randomly"
        },
        {
          "3.2. Dataset and evaluation": "ment\nset of\nthe WinoBias\n[24] dataset\nand the\naudio clips\nfrom",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "select a subset for binary-gendered targets. We exclude transgender"
        },
        {
          "3.2. Dataset and evaluation": "WinoST. WinoBias\nis a dataset\nspecifically designed to examine",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "instances from the CBBQ dataset because they are generated using"
        },
        {
          "3.2. Dataset and evaluation": "biases related to entities identified by their occupations, utilizing 40",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "only one template and are not sufficient for sound conclusions."
        },
        {
          "3.2. Dataset and evaluation": "distinct professions.\nThe associated occupation statistics from the",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "Each BBQ and CBBQ instance includes a context sentence, a"
        },
        {
          "3.2. Dataset and evaluation": "population survey [47] are used to determine stereotypical relations.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "question, and three options.\nInstances are grouped into four types:"
        },
        {
          "3.2. Dataset and evaluation": "The example in Fig. 1 shows\nthat “she” logically and grammati-",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "negative or non-negative questions with ambiguous or disambiguous"
        },
        {
          "3.2. Dataset and evaluation": "cally refers to the chief rather than the accountant, but SLLM might",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "contexts. The negative question reflects a social bias toward certain"
        },
        {
          "3.2. Dataset and evaluation": "stereotypically choose “accountant” due to gender roles.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "social groups, breaking a social value in the US. The non-negative"
        },
        {
          "3.2. Dataset and evaluation": "WinoBias contains two types of data: Type 1 and Type 2. Type",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "question does not. The ambiguous context only provides a general"
        },
        {
          "3.2. Dataset and evaluation": "1 is more challenging because it requires the model to identify coref-",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "scenario,\nleading to an “UNKNOWN” answer. The disambiguous"
        },
        {
          "3.2. Dataset and evaluation": "erence entities based on real-world knowledge of the given circum-",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "context offers detailed clues, ensuring the correct answer is one of"
        },
        {
          "3.2. Dataset and evaluation": "stances.\nIn contrast, Type 2 is easier, as coreference can be resolved",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "the groups mentioned in the question."
        },
        {
          "3.2. Dataset and evaluation": "using the syntactic relationship of the entities.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "We synthesize spoken BBQ context using the same speakers and"
        },
        {
          "3.2. Dataset and evaluation": "We\nreport\nscore\nof\ncoreference\nresolution\nfor\npro-\nthe F1",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "TTS providers as in Stereoset, and spoken CBBQ contexts are syn-"
        },
        {
          "3.2. Dataset and evaluation": "stereotypical\nand\nanti-stereotypical\nentities,\nand we\nalso\nreport",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "thesized using Google, Azure, and TTSMaker TTS API 5 with bal-"
        },
        {
          "3.2. Dataset and evaluation": "the F1 score of coreference resolution for male and female entities.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "anced gender6."
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "We use accuracy and bias scores to quantify the bias. Accuracy"
        },
        {
          "3.2. Dataset and evaluation": "3.2.3. Continuation",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "is the percentage of correctly answered questions among all ques-"
        },
        {
          "3.2. Dataset and evaluation": "We use the inter-sentence task and domain gender of Stereoset [37]",
          "We utilize four metrics to quantify stereotypes in continuations:": "tions. Bias score is defined as [17] so that 0% indicates no bias is"
        },
        {
          "3.2. Dataset and evaluation": "for continuation stereotype evaluation, consisting of 242 instances.",
          "We utilize four metrics to quantify stereotypes in continuations:": "measured, 100% indicates all answers are aligned with social bias,"
        },
        {
          "3.2. Dataset and evaluation": "The original dataset can be formulated as D = {(C, Cs, Ca, Ci)},",
          "We utilize four metrics to quantify stereotypes in continuations:": "and -100% indicates all answers are in contrast\nto social bias. The"
        },
        {
          "3.2. Dataset and evaluation": "where C is a context describing the target group, Cs, Ca, Ci de-",
          "We utilize four metrics to quantify stereotypes in continuations:": "bias score in disambiguous context is:"
        },
        {
          "3.2. Dataset and evaluation": "notes a stereotypical, anti-stereotypical, or irrelevant continuation to",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "nbiased answer"
        },
        {
          "3.2. Dataset and evaluation": "the context sentence, respectively. We construct spoken context by",
          "We utilize four metrics to quantify stereotypes in continuations:": ") − 1,\n(2)\nsDIS = sb = 2("
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "nnot unknown output"
        },
        {
          "3.2. Dataset and evaluation": "synthesizing C using 3 Text-To-Speech (TTS) providers: Azure2,",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "Google3, and Amazon4, selecting one male and one female speaker",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "and the bias score for ambiguous context is:"
        },
        {
          "3.2. Dataset and evaluation": "from each to ensure diversity, totaling six speakers.",
          "We utilize four metrics to quantify stereotypes in continuations:": ""
        },
        {
          "3.2. Dataset and evaluation": "",
          "We utilize four metrics to quantify stereotypes in continuations:": "(3)\nsAM B = (1 − accuracy) × sb."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: presents the gender bias across different language pairs.",
      "data": [
        {
          "Table 2. Prompts for different datasets and different model architecture.": "prompt for ASR-LLM"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "Translate the following sentence from English to {tgt\nlanguage}:"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "{en sentence} Respond with only the translated sentence without any"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "other words."
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "What does {gendered term} refer to in the following sentence? The"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "answer can be ‘{option1}’, ‘{option2}’, or ‘{option3}’. Respond with"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "only the answer without any other words. “{sentence}”"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "What is the most likely continuation in the following sentence? The"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "answer can be ‘{option1}’, ‘{option2}’, or ‘{option3}’. Respond with"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "only the answer without any other words. “{sentence}”"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "{context} {question} The answer can be ‘{ans1}’, ‘{ans2}’, or"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "‘{ans3}’. Respond with only the answer without any other words."
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "{context}{question}答案可以是「{ans1}」、「{ans2}」或"
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": ""
        },
        {
          "Table 2. Prompts for different datasets and different model architecture.": "「{ans3}」。只回答答案，不要包含其他文字。"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents the gender bias across different language pairs.",
      "data": [
        {
          "SQA\nCBBQ": "「{ans3}」。只回答答案，不要包含其他文字。"
        },
        {
          "SQA\nCBBQ": "The bias score for ambiguous context is adjusted based on accuracy,"
        },
        {
          "SQA\nCBBQ": "emphasizing that bias is more significant when it occurs more fre-"
        },
        {
          "SQA\nCBBQ": "quently."
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "4. RESULT"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "To ensure the synthesized speeches are representative, we report the"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "confidence intervals for our measurements in SSC and SQA. This"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "is done by calculating the standard deviation of\nthe mean for each"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "measurement across six synthesized speeches."
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "4.1.\nSpeech-to-text Translation"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "Table 3 presents\nthe gender bias across different\nlanguage pairs."
        },
        {
          "SQA\nCBBQ": "Translating to various languages results in different levels of bias and"
        },
        {
          "SQA\nCBBQ": "accuracy patterns, highlighting the importance of evaluating multi-"
        },
        {
          "SQA\nCBBQ": "ple languages."
        },
        {
          "SQA\nCBBQ": "First, we can observe that compared to other models, GPT-4o"
        },
        {
          "SQA\nCBBQ": "achieves a significantly higher accuracy in translating gender enti-"
        },
        {
          "SQA\nCBBQ": "ties. Additionally, GPT-4o’s △G and △S are notably low, indicating"
        },
        {
          "SQA\nCBBQ": "that gender stereotypes are unlikely to influence its translation out-"
        },
        {
          "SQA\nCBBQ": "comes. Compared to GPT-3.5, GPT-4o not only shows significant"
        },
        {
          "SQA\nCBBQ": "improvements in the accuracy of predicting gender entities but also"
        },
        {
          "SQA\nCBBQ": "exhibits a considerable reduction in gender bias. This improvement"
        },
        {
          "SQA\nCBBQ": "may be due to GPT-4o undergoing more extensive safety testing and"
        },
        {
          "SQA\nCBBQ": "refinement."
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "Second,\nthe translation accuracy of SLLMs is generally lower"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "compared to other models,\nsuggesting they are not yet capable of"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "high-quality speech-to-text\ntranslation, with ASR-LLM cascaded"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "models usually performing better. Additionally, SLLMs exhibit a"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "higher △G,\nindicating that\nthey translate male entities with a much"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "greater F1 score than female entities. This might be because male"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "entities are more likely to exist\nin speech-text fine-tuning data [49]."
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "Despite the significant performance gap between genders in SLLMs"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "(△G), surprisingly,\nthey are less likely to be influenced by gender"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "stereotypes in their translation outcomes (△S)."
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "Third, Seamless, despite being specialized in speech translation,"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "does not exhibit high accuracy for\ntranslating gender entities, sug-"
        },
        {
          "SQA\nCBBQ": "gesting that\nspeech-to-text/speech translation is more challenging"
        },
        {
          "SQA\nCBBQ": ""
        },
        {
          "SQA\nCBBQ": "than text-to-text\ntranslation. While its accuracy is lower than GPT-"
        },
        {
          "SQA\nCBBQ": "4o, Seamless\nis\nless\ninfluenced by gender\nstereotypes,\nshowing a"
        },
        {
          "SQA\nCBBQ": "similarly low △S as GPT-4o."
        },
        {
          "SQA\nCBBQ": "Fourth, comparing SLLMs to their backbone models, SLLMs"
        },
        {
          "SQA\nCBBQ": "show lower accuracy, higher △G, and lower △S across all\ntrans-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": "entities. Bold text indicates the highest accuracy or least bias among all models. Underscore indicates the lowest accuracy or most bias."
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": "model types"
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": "Specialized Model"
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": "ASR-LLM"
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": "SLLM"
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        },
        {
          "Table 3. The gender bias evaluation result on the WinoST dataset. All the values are in %. Acc. represents the accuracy of translating gender": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": "model types"
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": "ASR-LLM"
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": "SLLM"
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        },
        {
          "Table 4. F1 score on Winobias dataset. The results are split by Type-1 and Type-2 data and in pro/anti-stereotypical conditions. avg stands": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "that some SLLMs have lower language generation and instruction-",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "following capabilities. ASR-LLMs, especially SOTA LLMs, show",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "social biases compared to",
          "1.2": "higher bias",
          "44.8": "",
          "48.5": "scores,",
          "46.7": "suggesting stronger",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "SLLMs. This can be attributed to the same reason in 4.2.",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "Table 5. Evaluation result on Stereoset. All the values are in %.",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "",
          "44.8": "Second, among the SLLMs, Qwen-Audio achieves the highest",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "ss",
          "49.3": "lms",
          "43.9\n46.6": "icat",
          "5.4\n54.1\n52.9": "ifr",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "ICAT score,",
          "44.8": "",
          "48.5": "indicating that",
          "46.7": "",
          "-3.8": "",
          "54.8": "it offers the best balance between us-",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "50",
          "49.3": "100",
          "43.9\n46.6": "100",
          "5.4\n54.1\n52.9": "100",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "ability and fairness, making it the most suitable for deployment as a",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "100",
          "49.3": "-",
          "43.9\n46.6": "0",
          "5.4\n54.1\n52.9": "100",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "speech language modeling service.",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "50",
          "49.3": "66.7",
          "43.9\n46.6": "66.7",
          "5.4\n54.1\n52.9": "100",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "Third,",
          "44.8": "",
          "48.5": "comparing SLLM and their backbone LLMs, Qwen-",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "68.75 ± 1.05",
          "49.3": "95.25 ± 0.35",
          "43.9\n46.6": "59.53",
          "5.4\n54.1\n52.9": "100.00 ± 0.00",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "Audio and Vicuna 13B has lower ss and higher",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": "ifr, while Vicuna"
        },
        {
          "WavLLM": "72.6 ± 0.86",
          "49.3": "97.8 ± 0.5",
          "43.9\n46.6": "53.59",
          "5.4\n54.1\n52.9": "92.93 ± 0.00",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "7B and WavLLM has higher ss and lower ifr. These results indicate",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "58.38 ± 0.16",
          "49.3": "91.87 ± 0.21",
          "43.9\n46.6": "76.47",
          "5.4\n54.1\n52.9": "94.83 ± 0.23",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "67.7 ± 0.32",
          "49.3": "96.14 ± 0.21",
          "43.9\n46.6": "62.11",
          "5.4\n54.1\n52.9": "99.59 ± 0.00",
          "53.5": "",
          "1.2": "that finetuning with speech instruction datasets does not consistently",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "60.80 ± 3.36",
          "49.3": "60.33 ± 2.65",
          "43.9\n46.6": "47.30",
          "5.4\n54.1\n52.9": "66.39 ± 1.91",
          "53.5": "",
          "1.2": "reduce bias across all models and tasks.",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "52.56 ± 0.36",
          "49.3": "63.55 ± 0.5",
          "43.9\n46.6": "60.30",
          "5.4\n54.1\n52.9": "72.87 ± 0.43",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "56.79 ± 0.36",
          "49.3": "72.23 ± 0.31",
          "43.9\n46.6": "62.42",
          "5.4\n54.1\n52.9": "84.44 ± 0.34",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "4.4. Question and Answer",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "47.83 ± 0",
          "49.3": "70.31 ± 0",
          "43.9\n46.6": "67.26",
          "5.4\n54.1\n52.9": "88.77 ± 0.17",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "50.68 ± 2.59",
          "49.3": "75.48 ± 3.69",
          "43.9\n46.6": "74.45",
          "5.4\n54.1\n52.9": "90.77 ± 1.24",
          "53.5": "",
          "1.2": "4.4.1. English QA: BBQ",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "44.5 ± 2.89",
          "49.3": "34.85 ± 4.86",
          "43.9\n46.6": "31.02",
          "5.4\n54.1\n52.9": "55.92 ± 6.04",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "52.3 ± 0.75",
          "49.3": "69.63 ± 1.65",
          "43.9\n46.6": "66.43",
          "5.4\n54.1\n52.9": "98.83 ± 0.41",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "Table 6 presents the results of the BBQ evaluation. First, ASR-LLM",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "56.5 ± 4.95",
          "49.3": "37.56 ± 0.09",
          "43.9\n46.6": "32.68",
          "5.4\n54.1\n52.9": "44.30 ± 0.77",
          "53.5": "",
          "1.2": "",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "systems exhibit high accuracy, demonstrating robust",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": "language un-"
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "derstanding capabilities, except Vicuna 7B and LLaMA2 7B, which",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "struggle in ambiguous contexts. Conversely, SLLMs show low accu-",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        },
        {
          "WavLLM": "",
          "49.3": "",
          "43.9\n46.6": "",
          "5.4\n54.1\n52.9": "",
          "53.5": "",
          "1.2": "racy in ambiguous contexts, often failing to respond with ”unknown”",
          "44.8": "",
          "48.5": "",
          "46.7": "",
          "-3.8": "",
          "54.8": "",
          "52.2": "",
          "2.6": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "indicates the most bias among all models. All the values are in %.": "target"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "context"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "model"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "GPT-3.5"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "GPT-4o"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "LLaMA 3 8B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "LLaMA 3 70B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "Qwen"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "Vicuna 7B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "Vicuna 13B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "LLaMA 2 7B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "Qwen-Audio"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "SALMONN 7B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "SALMONN 13B"
        },
        {
          "indicates the most bias among all models. All the values are in %.": "WavLLM"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "Comparing SLLMs and their backbone LLMs, all models have"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "less bias scores in ambiguous contexts, but only one model Qwen has"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "less bias scores in the disambiguous context. Notably, most of the"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "There-"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "the complex relationship between bias and instruction fine-"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "The evaluation in this study primarily addresses bias in the seman-"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "tics of speech, providing a detailed and rigorous analysis of a subset"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": "future research should"
        },
        {
          "ambiguous contexts is generally lower than that of English QA, with": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "30.71 ± 13.17 −0.97 ± 1.95 27.15 ± 2.57 −6.83 ± 35.37\nWavLLM"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "when no clear answer exists, thereby exhibiting greater bias in such"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "scenarios. This issue may stem from the fine-tuning data, as most ex-"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "amples are paired with specific answers, causing SLLMs to overlook"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "responding with ”I don’t know” or its equivalents in English."
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "Second, comparing SLLM and its backbone LLM, SLLMs are"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "more biased than cascaded systems, especially SALMONN 7B and"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "WavLLM, which are the most biased models in BBQ regardless of"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "targets and contexts."
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "Moreover, we find that\nthe cascaded systems with GPT-4o and"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "LLaMA 3 70B have the smallest bias score and the highest accu-"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "racy across most evaluations, highlighting a good balance between"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "performance and bias."
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "4.4.2. Chinese QA: CBBQ"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": ""
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "Refer to Table 7, the results in Chinese QA differ from those in En-"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "glish. We excluded LLaMA-2 from the evaluation on CBBQ because"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "it struggles with following Chinese instructions and outputs unstable"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "responses. For ASR-LLM systems, performance is better in ambigu-"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "ous contexts compared to disambiguous ones. These systems exhibit"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "a medium bias score of around -5 to 15 in ambiguous contexts, and"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "large bias scores ranging from -86 to -3 in disambiguous contexts,"
        },
        {
          "52.91 ± 7.24 −0.72 ± 1.02 19.60 ± 2.78\n6.29 ± 3.63\nSALMONN 13B": "indicating a greater bias than observed in English QA."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Our findings indicate that the level of model bias varies depend-": "ing on the evaluation method used. Therefore, it is crucial to employ",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "across languages on the internet: Network and language ef-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "multiple evaluation methods when assessing the biases in SILLMs.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "fects,” in Proceedings of the 39th Annual Hawaii International"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Relying on a single evaluation approach may provide an incomplete",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Conference on System Sciences (HICSS’06), 2006."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "or skewed understanding of the models’ biases.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[14] Aaminah Zaman Malik and Audhesh Paswan,\n“Linguistic"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Journal of Con-\nracism in inter-culture service encounter,”"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "7. REFERENCES",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "sumer Marketing, 2023."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[15] Marcello Russo, Gazi\nIslam,\nand Burak Koyuncu,\n“Non-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[1] Yunfei Chu,\nJin Xu, Xiaohuan\nZhou, Qian Yang,\nShil-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "native accents and stigma: How self-fulfilling prophesies can"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "iang Zhang, Zhijie Yan, Chang Zhou,\nand\nJingren Zhou,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Human Resource Management Re-\naffect career outcomes,”"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "“Qwen-audio: Advancing universal audio understanding via",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "view, 2017."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "arXiv preprint\nunified large-scale audio-language models,”",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "arXiv:2311.07919, 2023.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[16] Ahmed Agiza, Mohamed Mostagir, and Sherief Reda,\n“Ana-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "lyzing the impact of data selection and fine-tuning on economic"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[2] Yuan Gong, Alexander H Liu, Hongyin Luo, Leonid Karlinsky,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "and political biases in llms,” 2024."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "and James Glass,\n“Joint audio and speech understanding,”\nin",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "2023 IEEE Automatic Speech Recognition and Understanding",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[17] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Pad-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Workshop (ASRU). IEEE, 2023, pp. 1–8.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "makumar, Jason Phang, Jana Thompson, Phu Mon Htut, and"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Samuel Bowman,\n“BBQ: A hand-built bias benchmark for"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[3] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke Li, Jun-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "the Association for Com-\nquestion answering,”\nin Findings of"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "teng Jia, Yuan Shangguan,\nJay Mahadeokar, Ozlem Kalinli,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "putational Linguistics: ACL 2022, 2022."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Christian Fuegen, and Mike Seltzer,\n“AudioChatLlama: To-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "wards general-purpose speech abilities for LLMs,” in Proceed-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[18] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "ings of the 2024 Conference of the North American Chapter of",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Bowman,\n“CrowS-pairs: A challenge dataset\nfor measuring"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "the Association for Computational Linguistics: Human Lan-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "social biases in masked language models,”\nin Proceedings of"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "guage Technologies (Volume 1: Long Papers), 2024.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "the 2020 Conference on Empirical Methods in Natural Lan-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "guage Processing (EMNLP), 2020."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[4] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Tian Tan, Wei Li,\nLu Lu,\nZejun MA,\nand Chao Zhang,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[19] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "“SALMONN: Towards generic hearing abilities for large lan-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Chang, and Nanyun Peng, ““kelly is a warm person, joseph is a"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "guage models,”\nin The Twelfth International Conference on",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "role model”: Gender biases in llm-generated reference letters,”"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Learning Representations, 2024.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "the Association for Computational Linguistics:\nin Findings of"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "EMNLP 2023, 2023, pp. 3730–3748."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[5] Kai-Ching Yeh,\nJou-An Chi, Da-Chen Lian,\nand Shu-Kai",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Hsieh,\n“Evaluating interfaced llm bias,”\nin Proceedings of",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[20] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Dar-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "the 35th Conference on Computational Linguistics and Speech",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "rell, and Anna Rohrbach, “Women also snowboard: Overcom-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Processing (ROCLING 2023), 2023, pp. 292–299.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "ing bias in captioning models,” in Proceedings of the European"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "conference on computer vision (ECCV), 2018."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[6]\nPaul Pu Liang et al.,\n“Towards understanding and mitigating",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "social biases in language models,” in International Conference",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[21] Ashish Seth, Mayur Hemani, and Chirag Agarwal, “Dear: De-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "on Machine Learning. PMLR, 2021, pp. 6565–6576.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "biasing vision-language models with additive residuals,”\nin"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Proceedings of the IEEE/CVF Conference on Computer Vision"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[7] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Kellie Webster, Yu Zhong, and Stephen Denuyl,\n“Social bi-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "and Pattern Recognition, 2023."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "ases in nlp models as barriers for persons with disabilities,” in",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[22]\nShreya Shankar, Yoni Halpern, Eric Breck,\nJames Atwood,"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Proceedings of the 58th Annual Meeting of the Association for",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Jimbo Wilson, and D Sculley,\n“No classification without rep-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Computational Linguistics, 2020.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "resentation: Assessing geodiversity issues\nin open data sets"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[8] Ruediger\nGlott,\nPhilipp\nSchmidt,\nand\nRishab\nGhosh,",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "for the developing world,”\narXiv preprint arXiv:1711.08536,"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "“Wikipedia survey–overview of results,” United Nations Uni-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "2017."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "versity: Collaborative Creativity Group, pp. 1158–1178, 2010.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[23] Gabriel Stanovsky, Noah A Smith,\nand Luke Zettlemoyer,"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[9]\nFrancesca Tripodi,\n“Ms. categorized: Gender, notability, and",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "“Evaluating gender bias in machine translation,” arXiv preprint"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "inequality on wikipedia,” New media & society, 2023.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "arXiv:1906.00591, 2019."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[10] Hadas Kotek, Rikker Dockum, and David Sun,\n“Gender bias",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[24]\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "and stereotypes in large language models,”\nin Proceedings of",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Kai-Wei Chang, “Gender bias in coreference resolution: Eval-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "The ACM Collective Intelligence Conference, 2023.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "the 2018\nuation and debiasing methods,”\nin Proceedings of"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Conference of\nthe North American Chapter of\nthe Association"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[11] Regina Merine and Saptarshi Purkayastha, “Risks and benefits",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "for Computational Linguistics: Human Language Technolo-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "of ai-generated text summarization for expert\nlevel content\nin",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "gies, Volume 2 (Short Papers), 2018, pp. 15–20."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "graduate health informatics,” in 2022 IEEE 10th International",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Conference on Healthcare Informatics (ICHI), 2022.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[25] Rachael Tatman,\n“Gender and dialect bias in youtube’s auto-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "the first ACL workshop on\nmatic captions,”\nin Proceedings of"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "[12] Lele Sha et al.,\n“Assessing algorithmic fairness in automatic",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "ethics in natural language processing, 2017, pp. 53–59."
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "Intelli-\nclassifiers of educational\nforum posts,”\nin Artificial",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": ""
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "gence\nin Education:\n22nd International Conference, AIED",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "[26]\nSiyuan Feng, Olya Kudina, Bence Mark Halpern, and Odette"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "2021, Utrecht, The Netherlands, June 14–18, 2021, Proceed-",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "Scharenborg,\n“Quantifying bias in automatic speech recogni-"
        },
        {
          "Our findings indicate that the level of model bias varies depend-": "ings, Part I 22, 2021.",
          "[13] A. Kralisch and T. Mandl,\n“Barriers\nto information access": "tion,” arXiv preprint arXiv:2103.15122, 2021."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“Gender representation in french broadcast corpora and its im-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "28518."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "the 1st\ninterna-\npact on asr performance,”\nin Proceedings of",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "arXiv preprint\n[40]\nJinze Bai et al.,\n“Qwen technical\nreport,”"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "tional workshop on AI for smart TV content production, access",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "arXiv:2309.16609, 2023."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "and delivery, 2019.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[41] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[28] Marcely Zanon Boito, Laurent Besacier, Natalia Tomashenko,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "and Yannick Est`eve,\n“A Study of Gender\nImpact\nin Self-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "hao Zhuang,\nJoseph E Gonzalez, et al.,\n“Vicuna: An open-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "supervised Models for Speech-to-Text Systems,”\nin Proc. In-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "source chatbot\nimpressing gpt-4 with 90%* chatgpt quality,”"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "terspeech 2022, 2022.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "See https://vicuna. lmsys. org (accessed 14 April 2023), 2023."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[29] Marta R. Costa-juss`a, Christine Basta, and Gerard I. G´allego,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“Evaluating gender bias in speech translation,” in Proceedings",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "of the Thirteenth Language Resources and Evaluation Confer-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "Batra, Prajjwal Bhargava, Shruti Bhosale, et al.,\n“Llama 2:"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "ence, 2022.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "arXiv preprint\nOpen foundation and fine-tuned chat models,”"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[30] Yi-Cheng Lin, Haibin Wu, Huang-Cheng Chou, Chi-Chun",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "arXiv:2307.09288, 2023."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Lee, and Hung-yi Lee,\n“Emo-bias: A large scale evaluation",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[43]\nShujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "arXiv preprint\nof social bias on speech emotion recognition,”",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Lin-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "arXiv:2406.05065, 2024.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "quan Liu, and Furu Wei,\n“Wavllm: Towards robust and adap-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[31] Cristina Gorrostieta, Reza Lotfian, Kye Taylor, Richard Brutti,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "tive speech large language model,” 2024."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "and John Kane,\n“Gender de-biasing in speech emotion recog-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[44]\nSanyuan Chen et al., “BEATs: Audio pre-training with acous-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "nition.,” in Interspeech, 2019.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "the 40th International Con-\ntic tokenizers,”\nin Proceedings of"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[32] Woan-Shiuan Chien, Shreya G. Upadhyay, and Chi-Chun Lee,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "ference on Machine Learning, Andreas Krause, Emma Brun-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“Balancing speaker-rater\nfairness\nfor gender-neutral\nspeech",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "skill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "emotion recognition,”\nin ICASSP 2024 - 2024 IEEE Interna-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "Jonathan Scarlett, Eds. 23–29 Jul 2023, vol. 202 of Proceed-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "tional Conference on Acoustics, Speech and Signal Processing",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "ings of Machine Learning Research, pp. 5178–5193, PMLR."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "(ICASSP), 2024.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[45] Wei-Lin Chiang et al.,\n“Vicuna: An open-source chatbot\nim-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[33] Yen Meng, Yi-Hui Chou, Andy T Liu,\nand Hung-yi Lee,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "pressing gpt-4 with 90%* chatgpt quality,” March 2023."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“Don’t\nspeak\ntoo\nfast:\nThe\nimpact\nof\ndata\nbias\non\nself-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[46]\nSanyuan Chen et al., “Wavlm: Large-scale self-supervised pre-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "supervised speech models,”\nin ICASSP 2022-2022 IEEE In-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "IEEE Journal of\ntraining for\nfull\nstack speech processing,”"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "ternational Conference on Acoustics, Speech and Signal Pro-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "cessing (ICASSP), 2022.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "1518, 2022."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[34] Yi-Cheng Lin, Tzu-Quan Lin, Hsi-Che Lin, Andy T. Liu, and",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[47]\n“2017 annual averages - employed persons by detailed occupa-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Hung yi Lee,\n“On the social bias of speech self-supervised",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "tion, sex, race, and hispanic or latino ethnicity,” ."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "models,” 2024.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[48] Yufei Huang and Deyi Xiong,\n“Cbbq: A chinese bias bench-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[35] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Ben-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "mark dataset curated with human-ai collaboration for large lan-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "jamin Van Durme,\n“Gender bias in coreference resolution,”",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "guage models,” 2023."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "in Proceedings of the 2018 Conference of the North American",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Chapter of the Association for Computational Linguistics: Hu-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "[49] Rebecca Davis Merritt and Teion Wells Harrison, “Gender and"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "man Language Technologies, Volume 2 (Short Papers), 2018.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "ethnicity attributions to a gender-and ethnicity-unspecified in-"
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": "dividual: Is there a people= white male bias?,” Sex roles, 2006."
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[36] Eva Vanmassenhove, Chris Emmery, and Dimitar Shterionov,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“NeuTral Rewriter: A rule-based and neural approach to auto-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "matic rewriting into gender neutral alternatives,”\nin Proceed-",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "ings of the 2021 Conference on Empirical Methods in Natural",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Language Processing. 2021, Association for Computational",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Linguistics.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[37] Moin Nadeem, Anna Bethke,\nand Siva Reddy,\n“StereoSet:",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Measuring stereotypical bias in pretrained language models,”",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "the 59th Annual Meeting of\nthe Association\nin Proceedings of",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "for Computational Linguistics and the 11th International Joint",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Conference on Natural Language Processing (Volume 1: Long",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Papers), 2021.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[38] Lo¨ıc\nBarrault,\nYu-An\nChung, Mariano\nCoria Meglioli,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al.,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "“Seamless:\nMultilingual\nexpressive\nand\nstreaming\nspeech",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "translation,” arXiv preprint arXiv:2312.05187, 2023.",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "[39] Alec Radford,\nJong Wook Kim, Tao Xu, Greg Brockman,",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "Christine McLeavey,\nand\nIlya Sutskever,\n“Robust\nspeech",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        },
        {
          "[27] Mahault Garnerin, Solange Rossato,\nand Laurent Besacier,": "recognition via large-scale weak supervision,” in International",
          "Conference on Machine Learning. PMLR, 2023, pp. 28492–": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "3",
      "title": "Joint audio and speech understanding",
      "authors": [
        "Yuan Gong",
        "Alexander Liu",
        "Hongyin Luo",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "4",
      "title": "AudioChatLlama: Towards general-purpose speech abilities for LLMs",
      "authors": [
        "Yassir Fathullah",
        "Chunyang Wu",
        "Egor Lakomkin",
        "Ke Li",
        "Junteng Jia",
        "Yuan Shangguan",
        "Jay Mahadeokar",
        "Ozlem Kalinli",
        "Christian Fuegen",
        "Mike Seltzer"
      ],
      "venue": "AudioChatLlama: Towards general-purpose speech abilities for LLMs"
    },
    {
      "citation_id": "5",
      "title": "SALMONN: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "6",
      "title": "Evaluating interfaced llm bias",
      "authors": [
        "Kai-Ching Yeh",
        "Jou-An Chi",
        "Shu-Kai Da-Chen Lian",
        "Hsieh"
      ],
      "year": "2023",
      "venue": "Proceedings of the 35th Conference on Computational Linguistics and Speech Processing"
    },
    {
      "citation_id": "7",
      "title": "Towards understanding and mitigating social biases in language models",
      "authors": [
        "Paul Pu"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "8",
      "title": "Social biases in nlp models as barriers for persons with disabilities",
      "authors": [
        "Ben Hutchinson",
        "Vinodkumar Prabhakaran",
        "Emily Denton",
        "Kellie Webster",
        "Yu Zhong",
        "Stephen Denuyl"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Wikipedia survey-overview of results",
      "authors": [
        "Ruediger Glott",
        "Philipp Schmidt",
        "Rishab Ghosh"
      ],
      "year": "2010",
      "venue": "United Nations University: Collaborative Creativity Group"
    },
    {
      "citation_id": "10",
      "title": "Ms. categorized: Gender, notability, and inequality on wikipedia",
      "authors": [
        "Francesca Tripodi"
      ],
      "year": "2023",
      "venue": "New media & society"
    },
    {
      "citation_id": "11",
      "title": "Gender bias and stereotypes in large language models",
      "authors": [
        "Hadas Kotek",
        "Rikker Dockum",
        "David Sun"
      ],
      "year": "2023",
      "venue": "Proceedings of The ACM Collective Intelligence Conference"
    },
    {
      "citation_id": "12",
      "title": "Risks and benefits of ai-generated text summarization for expert level content in graduate health informatics",
      "authors": [
        "Regina Merine",
        "Saptarshi Purkayastha"
      ],
      "venue": "2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)"
    },
    {
      "citation_id": "13",
      "title": "Assessing algorithmic fairness in automatic classifiers of educational forum posts",
      "authors": [
        "Lele Sha"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence in Education: 22nd International Conference, AIED 2021"
    },
    {
      "citation_id": "14",
      "title": "Barriers to information access across languages on the internet: Network and language effects",
      "authors": [
        "A Kralisch",
        "T Mandl"
      ],
      "year": "2006",
      "venue": "Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS'06)"
    },
    {
      "citation_id": "15",
      "title": "Linguistic racism in inter-culture service encounter",
      "authors": [
        "Aaminah Zaman",
        "Audhesh Paswan"
      ],
      "year": "2023",
      "venue": "Journal of Consumer Marketing"
    },
    {
      "citation_id": "16",
      "title": "Nonnative accents and stigma: How self-fulfilling prophesies can affect career outcomes",
      "authors": [
        "Marcello Russo",
        "Gazi Islam",
        "Burak Koyuncu"
      ],
      "year": "2017",
      "venue": "Human Resource Management Review"
    },
    {
      "citation_id": "17",
      "title": "Analyzing the impact of data selection and fine-tuning on economic and political biases in llms",
      "authors": [
        "Ahmed Agiza",
        "Mohamed Mostagir",
        "Sherief Reda"
      ],
      "year": "2024",
      "venue": "Analyzing the impact of data selection and fine-tuning on economic and political biases in llms"
    },
    {
      "citation_id": "18",
      "title": "BBQ: A hand-built bias benchmark for question answering",
      "authors": [
        "Alicia Parrish",
        "Angelica Chen",
        "Nikita Nangia",
        "; Phang",
        "Jana Thompson",
        "Phu Mon Htut",
        "Samuel Bowman"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "19",
      "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
      "authors": [
        "Nikita Nangia",
        "Clara Vania",
        "Rasika Bhalerao",
        "Samuel Bowman"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "kelly is a warm person, joseph is a role model\": Gender biases in llm-generated reference letters",
      "authors": [
        "Yixin Wan",
        "George Pu",
        "Jiao Sun",
        "Aparna Garimella",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "21",
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "authors": [
        "Anne Lisa",
        "Kaylee Hendricks",
        "Kate Burns",
        "Trevor Saenko",
        "Anna Darrell",
        "Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "22",
      "title": "Dear: Debiasing vision-language models with additive residuals",
      "authors": [
        "Ashish Seth",
        "Mayur Hemani",
        "Chirag Agarwal"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world",
      "authors": [
        "Shreya Shankar",
        "Yoni Halpern",
        "Eric Breck",
        "James Atwood",
        "Jimbo Wilson",
        "Sculley"
      ],
      "year": "2017",
      "venue": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world",
      "arxiv": "arXiv:1711.08536"
    },
    {
      "citation_id": "24",
      "title": "Evaluating gender bias in machine translation",
      "authors": [
        "Gabriel Stanovsky",
        "Noah Smith",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Evaluating gender bias in machine translation",
      "arxiv": "arXiv:1906.00591"
    },
    {
      "citation_id": "25",
      "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
      "authors": [
        "Jieyu Zhao",
        "Tianlu Wang",
        "Mark Yatskar",
        "Vicente Ordonez",
        "Kai-Wei Chang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "26",
      "title": "Gender and dialect bias in youtube's automatic captions",
      "authors": [
        "Rachael Tatman"
      ],
      "year": "2017",
      "venue": "Proceedings of the first ACL workshop on ethics in natural language processing"
    },
    {
      "citation_id": "27",
      "title": "Quantifying bias in automatic speech recognition",
      "authors": [
        "Siyuan Feng",
        "Olya Kudina",
        "Bence Mark Halpern",
        "Odette Scharenborg"
      ],
      "year": "2021",
      "venue": "Quantifying bias in automatic speech recognition",
      "arxiv": "arXiv:2103.15122"
    },
    {
      "citation_id": "28",
      "title": "Gender representation in french broadcast corpora and its impact on asr performance",
      "authors": [
        "Mahault Garnerin",
        "Solange Rossato",
        "Laurent Besacier"
      ],
      "year": "2019",
      "venue": "Proceedings of the 1st international workshop on AI for smart TV content production, access and delivery"
    },
    {
      "citation_id": "29",
      "title": "A Study of Gender Impact in Selfsupervised Models for Speech-to-Text Systems",
      "authors": [
        "Laurent Marcely Zanon Boito",
        "Natalia Besacier",
        "Yannick Tomashenko",
        "Estève"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "30",
      "title": "Evaluating gender bias in speech translation",
      "authors": [
        "Marta Costa-Jussà",
        "Christine Basta",
        "Gerard Gállego"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "31",
      "title": "Emo-bias: A large scale evaluation of social bias on speech emotion recognition",
      "authors": [
        "Yi-Cheng Lin",
        "Haibin Wu",
        "Huang-Cheng Chou",
        "Chi-Chun Lee",
        "Hung-Yi Lee"
      ],
      "year": "2024",
      "venue": "Emo-bias: A large scale evaluation of social bias on speech emotion recognition",
      "arxiv": "arXiv:2406.05065"
    },
    {
      "citation_id": "32",
      "title": "Gender de-biasing in speech emotion recognition",
      "authors": [
        "Cristina Gorrostieta",
        "Reza Lotfian",
        "Kye Taylor",
        "Richard Brutti",
        "John Kane"
      ],
      "year": "2019",
      "venue": "Gender de-biasing in speech emotion recognition"
    },
    {
      "citation_id": "33",
      "title": "Balancing speaker-rater fairness for gender-neutral speech emotion recognition",
      "authors": [
        "Woan-Shiuan Chien",
        "Shreya Upadhyay",
        "Chi-Chun Lee"
      ],
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Don't speak too fast: The impact of data bias on selfsupervised speech models",
      "authors": [
        "Yen Meng",
        "Yi-Hui Chou",
        "Andy Liu",
        "Hung-Yi Lee"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "35",
      "title": "On the social bias of speech self-supervised models",
      "authors": [
        "Yi-Cheng Lin",
        "Tzu-Quan Lin",
        "Hsi-Che Lin",
        "Andy Liu",
        "Hung Yi"
      ],
      "year": "2024",
      "venue": "On the social bias of speech self-supervised models"
    },
    {
      "citation_id": "36",
      "title": "Gender bias in coreference resolution",
      "authors": [
        "Rachel Rudinger",
        "Jason Naradowsky",
        "Brian Leonard",
        "Benjamin Van Durme"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "37",
      "title": "NeuTral Rewriter: A rule-based and neural approach to automatic rewriting into gender neutral alternatives",
      "authors": [
        "Eva Vanmassenhove",
        "Chris Emmery",
        "Dimitar Shterionov"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "authors": [
        "Moin Nadeem",
        "Anna Bethke",
        "Siva Reddy"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Seamless: Multilingual expressive and streaming speech translation",
      "authors": [
        "Loïc Barrault",
        "Yu-An Chung",
        "Mariano Meglioli",
        "David Dale",
        "Ning Dong",
        "Mark Duppenthaler",
        "Paul-Ambroise Duquenne",
        "Brian Ellis",
        "Hady Elsahar",
        "Justin Haaheim"
      ],
      "year": "2023",
      "venue": "Seamless: Multilingual expressive and streaming speech translation",
      "arxiv": "arXiv:2312.05187"
    },
    {
      "citation_id": "40",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "41",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "42",
      "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez"
      ],
      "year": "2023",
      "venue": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "43",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "44",
      "title": "Wavllm: Towards robust and adaptive speech large language model",
      "authors": [
        "Shujie Hu",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Hongkun Hao",
        "Jing Pan",
        "Xunying Liu",
        "Jinyu Li",
        "Sunit Sivasankaran",
        "Linquan Liu",
        "Furu Wei"
      ],
      "year": "2024",
      "venue": "Wavllm: Towards robust and adaptive speech large language model"
    },
    {
      "citation_id": "45",
      "title": "BEATs: Audio pre-training with acoustic tokenizers",
      "authors": [
        "Sanyuan Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "46",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "47",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "Sanyuan Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "2017 annual averages -employed persons by detailed occupation, sex, race, and hispanic or latino ethnicity",
      "venue": "2017 annual averages -employed persons by detailed occupation, sex, race, and hispanic or latino ethnicity"
    },
    {
      "citation_id": "49",
      "title": "Cbbq: A chinese bias benchmark dataset curated with human-ai collaboration for large language models",
      "authors": [
        "Yufei Huang",
        "Deyi Xiong"
      ],
      "year": "2023",
      "venue": "Cbbq: A chinese bias benchmark dataset curated with human-ai collaboration for large language models"
    },
    {
      "citation_id": "50",
      "title": "Gender and ethnicity attributions to a gender-and ethnicity-unspecified individual: Is there a people= white male bias?",
      "authors": [
        "Davis Merritt",
        "Teion Harrison"
      ],
      "year": "2006",
      "venue": "Sex roles"
    }
  ]
}