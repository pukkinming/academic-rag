{
  "paper_id": "2111.14262v1",
  "title": "Customizing An Affective Tutoring System Based On Facial Expression And Head Pose Estimation",
  "published": "2021-11-21T13:06:56Z",
  "authors": [
    "Mahdi Pourmirzaei",
    "Gholam Ali Montazer",
    "Ebrahim Mousavi"
  ],
  "keywords": [
    "Intelligent Tutoring System",
    "Affective Tutoring System",
    "Emotion Recognition",
    "Head Pose Estimation",
    "Deep Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, the main problem in e-learning has shifted from analyzing content to personalization of learning environment by Intelligence Tutoring Systems (ITSs). Therefore, by designing personalized teaching models, learners are able to have a successful and satisfying experience in achieving their learning goals. Affective Tutoring Systems (ATSs) are some kinds of ITS that can recognize and respond to affective states of learners. In this study, we designed, implemented, and evaluated a system to personalize the learning environment based on the facial emotions recognition, head pose estimation, and cognitive style of learners. First, a unit called Intelligent Analyzer (AI) created which was responsible for recognizing facial expression and head angles of learners. Next, the ATS was built which mainly made of two units: ITS, IA. Results indicated that with the ATS, participants needed less efforts to pass the tests. In other words, we observed when the IA unit was activated, learners could pass the final tests in fewer attempts than those for whom the IA unit was deactivated. Additionally, they showed an improvement in terms of the mean passing score and academic satisfaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "An Intelligent Tutoring System (ITS) is a hybrid system that uses behavioral, cognitive, psychological, educational, and computer sciences to enhance learning in an e-learning environment. The term \"intelligent\" in ITS refers to the ability to adapt the learning environment to the learner's personal characteristics such as learning speed and domains where the learner exhibits superior or weaker learning ability. Researchers believe that the performance of an ITS can be significantly improved by considering the learner's affective state in the learning environment  [1] ,  [2] . Affect refers to the neurophysiological state of experiencing a particular feeling such as emotion. Thus, emotion is an affective state associated with our neurophysiological response to stimuli  [3] . One of the important determinants of the success of personal (private) education is the teacher's ability to detect and respond to the affective state of the learner  [4] . In a study  [5] , it has been shown that only a few positive changes in mental state can make people more creative, flexible, precise, and efficient in solving problems. Despite this evidence, the majority of existing ITSs ignore the affective state of the learner. This can be due to the complexity of the concept of affect and the difficulty of detecting affective states in educational settings  [6] . Systems that take into account the learner's affective state in their educational strategies are called Affective Tutoring Systems (ATSs). These systems have been developed to mimic the behaviors of a real teacher in adapting the learning environment to the learner's affective state  [7] . There are two main challenges in developing an ATS  [8] :  (1)  how to detect the affective state of the learner, and (2) how to adapt the learning environment to the learner's affects. The most important indicators of people's emotions and affective states are their facial expressions. One of the most popular methods of emotion recognition from facial expressions is the Ekman model  [9] . In this model, emotions are divided into eight categories: enjoyment, sadness, contempt, surprise, fear, anger, disgust, and neutral. Despite its popularity, this method does not work well in learning because it cannot identify the intensity of positive and negative emotions. Also, important emotions associated with fatigue, confusion, and anxiety cannot be detected and measured by the Ekman method. Furthermore, emotions like fear, sadness, and disgust are rarely experienced in the learning environment  [10] -  [12] . Considering the above issues, this study uses Russell's two-dimensional model for emotion recognition  [13] . This model defines emotions in two dimensions: Arousal (Activation vs. Deactivation) and Valence (Pleasant vs. Unpleasant). In the diagram of this model, the vertical and horizontal axes represent arousal and valence spectra, respectively, with the center representing the neutral state (Figure  1 ). Using the Russell model, a person's emotion at any level can be represented on the arousal and valence spectra. The reason for using this model rather than the Ekman model is its higher effectiveness and accuracy in educational settings, where the recognition of certain positive and negative emotions and their intensity has shown to be more important  [11] ,  [14] .\n\nFigure  1 . Diagram of Russell's two-dimensional emotion model  [13] .\n\nIn this study, the learning environment was adapted based on the learner's cognitive style, emotion, and head pose while watching educational content. The learner's cognitive style was identified using the method of a work  [15] , which assesses the wholistic-analytical and verbalizer-imager dimensions. Here, only the analytical and wholistic dimensions of the cognitive style were considered. Thus, people were divided into three categories of analytic, wholistic, and middle, and the curriculum was personalized for each category. Also, a combination of visual, aural, and written content was used for all learners. The learner's state was determined by fusing the prediction of three methods (face recognition, emotion recognition, and head pose detection) with the help of webcams while the learner consumed the educational content. The results were compiled into a system that could take a large number of videos of the learner watching a lesson and place the person in one of 21 learning states. This state affected two things: (1) the textual feedback that will be displayed to the learner at the end of each lesson; (2) the supplementary content that will be recommended to the learner if needed. This study was carried out in two phases:\n\n1. Design and implementation. This phase involved designing the ITS, designing the intelligent analyzer (IA) unit (the part of the system that modifies the learning process according to the emotion and head pose), and finally implementing the designed ATS. 2. Evaluation. In this phase, the designed ATS was evaluated by preparing educational content suitable for each cognitive style and presenting them to a number of learners with and without the IA. In this step, performance was measured in terms of \"academic success\" and \"academic satisfaction\".",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "As mentioned in the introduction, it could be very difficult to design an effective ATS, as this will require combining psychology with educational and computer sciences  [16] . One of the most important challenges in this area is how to detect affective states. In many ATSs, affective states are primarily detected through emotion recognition from facial expressions [8],  [9] . In a study  [17] , this approach was used to create an ATS for a Massive Open Online Courses (MOOCs). In this ATS, the learner's affective state while studying the content was determined through facial expression recognition based on the Ekman method and the adaptation was done based on predetermined rules according to the identified state. In the system proposed in a work  [18] , in addition to emotion recognition from the face, a module called \"Semantic Clues Emotion Voting\" was used to detect the learner's affective state based on a glossary of keywords that students use in the classroom, and ultimately the content was modified based on a combination of the results of the two methods. A person's affective state can be estimated not only from the face but also through the method called sentiment analysis  [19] ,  [20]  and the analysis of biometric information  [21] . In a study  [20] , comments and feedbacks of a group of learners in an exam were analyzed and classified into two categories of positive and negative sentiments and the results were used to evaluate the quality of exercises and design better exams. In the system proposed in a study  [19] , the learning process continues if the learner's emotion was assessed to be positive. Otherwise, the system started asking questions about positive emotions to encourage the learner to think positively. In another ATS  [21] , the learner's affective state was measured by biometric technologies and the obtained information was used to tailor the educational content to each user.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Design And Implementation Of Ats",
      "text": "This phase of the work comprised three steps: (1) designing the ITS; (2) designing the IA unit; (3) implementing the ATS. In the following subsections, we explain how the learning environment is designed, including how the learning is modified for each cognitive style, how aggregator and analyzer components are added to the system in the form of an IA, and finally how the designed ATS is implemented. The main components of the proposed system are illustrated in Figure  2 .\n\nFigure  2 . The parts of the proposed web-based learning system. The ATS part is on the server-side and the user interface part is on the client-side. ATS consists of three units: intelligent analyzer, teaching unit, and knowledge base.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Its",
      "text": "The design of this unit consists of two parts: (1) design of the educational environment, and (2) personalization of the environment based on the learner's cognitive style. The e-learning process in this learning environment involves watching and study of multiple sessions, each comprising a number of lessons. The goal is to personalize this environment for each learner. This personalization is done in two ways, which are explained below. One of the most common ways of personalizing educational content is to consider the learning styles. While researchers have proposed a variety of different learning style models -assuming that people are different in terms of the ability to learn through visual, aural, read/write, and kinesthetic sensory modalities (VARK) -, there are still many doubts about the effectiveness of the methods that prioritize individual learning styles. In fact, despite the popularity of this subject, several studies have shown that there is little relationship between learning styles and academic achievement  [22] -  [27] . Meanwhile, it has been shown that using visual, aural, and read/write modalities at the same time is more likely to lead to successful learning  [22] ,  [28] .\n\nIn a study  [15] , two dimensions were defined for the learner's cognitive style: (1) the wholistic-analytical dimension, which indicates whether a person tends to process information in whole or in parts; (2) the verbalizer-imager dimension, which indicates whether a person tends to process information in verbal form or in visual form and can be considered a subset of VARK. Considering the shortcomings of VARK learning styles, in this study, we only use the first dimension (i.e. wholistic-analytical) to classify learners, and use a combination of visual, aural, and written contents for all of them. In other words, for every cognitive group, content related to each lesson is provided in the form of a video containing text, video, and audio.\n\nIn the first personalization method, educational content is tailored to three cognitive styles, The learner's cognitive style is detected using the test provided in a study  [29] , which divides people into two categories of wholistic and analytical. People whose test results do not put them squarely in either of these categories (wholistic or analytical) are placed in a third category called \"middle\". Accordingly, learners will be classified into the three following groups: 1. Analytical 2. Wholistic 3. Middle Three groups of lessons, one for each cognitive style, are created. For learners in the middle group, educational content is a mixture of the contents prepared for wholistic and analytical learners. More detailed information about the content is provided in Section 4-1. Each learner can only access the content prepared specifically for his or her cognitive style category. The curriculum is designed such that different groups might not have the same number of lessons at each session. Figure  3  shows an example of how personalized content is created. The second personalization method is the analysis of the behavior of learners in videos. This is done by a unit named IA, which will be described below.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Intelligent Analyzer",
      "text": "The purpose of the IA designed in this study is to monitor the state of learners while they consume the content and then, give the system the information it needs to personalize the content and feedback for each learner (Figure  4 ). This unit consists of two components, analyzer and aggregator. The analyzer is first designed for the analysis of each frame and then, extended for analyzing videos. The final analyzer -video analyzer -is combined with an aggregator component to create the IA. In the following, first, the methods of face detection, head pose estimation, and emotion recognition are explained, and then, creating of the analyzer and aggregator components are described. Face Detection: To monitor the emotion and head pose of learners, it is necessary to detect the exact location of their faces in each frame. In ATSs, face detection must be done instantly for multiple learners at the same time. Therefore, the face detection method should be able to process frames instantaneously with acceptable precision. In this study, the FaceBoxes model  [30]  is used for this purpose. Comprised of multiple layers of neural networks, this model is designed to perform high-speed face detection processing on GPU and CPU. This model takes an image as input and gives the exact location of faces in the frame as output.\n\nHead Pose Estimation: In addition to the learner's affective state, his or her head pose relative to the webcam can also provide valuable information about the person's learning condition. For example, the head pose can help determine whether the learner is focused on the content (if the head is oriented significantly away from the webcam for a prolonged period, it might indicate a lack of focus). In this study, in addition to the affective state, the head pose is also considered in the learning personalization. To determine the head pose, three Euler angles need to be measured: yaw, pitch, and roll. Each of these angles is measured with respect to the coordinate of the origin, which here is the webcam lens. The yaw, pitch, and roll angles measured by the face detection component are indicated in Figure  5 . Following the approach used in a work  [31] , in this study, the head pose is estimated with a network method based on the HopeNet  [32] , which takes cropped images of the face as input. In the HopeNet architecture, the exact yaw, pitch, and roll values are converted to classification labels and then the regression values are obtained from the mathematical expectation of classification heads. Here, self-supervised learning with auxiliary heads is also performed to achieve a lower average error. As mentioned in the main work  [31] , the selfsupervised method is of the puzzling type besides three supervised head pose heads. All training settings and datasets have been derived from  [31]  and the deep model used for training is ResNet50  [33] . This network takes the cropped image of the head as input and gives its yaw, pitch, and roll angles as outputs.\n\nEmotion Recognition: As mentioned, emotion recognition is performed using Russell's two-dimensional model. For this purpose, the method of a work  [34]  is used to design and train a neural network for recognizing the valence and arousal values. Like the head pose estimation network, this method is based on HopeNet. It takes face images as input, and in addition to detecting the two axes of emotion as output, uses self-supervised learning with auxiliary heads to reduce the average error. In this method, Efficientnet-B0 architecture  [35]  and AffectNet images  [36]  are used for training. This network operates by taking the cropped images of the face as input and returning two values for valence and arousal as outputs.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Analyzer Design",
      "text": "The overall process of the IA is shown in Figure  5 . This unit consists of two components, analyzer and aggregator. The analyzer is created by combining face detection, emotion recognition, and head pose estimation networks. In the following, we first explain how the process and the methods are applied to one frame (Figure  6 ) and then extend the process to the processing of a video (Figure  7 ). Upon receiving a frame as input, the face detection algorithm can respond in three ways: (1) not detecting any faces, which will cause the processing procedure to stop; (2) detecting multiple faces, which will also cause the processing to stop, as personalization is supposed to be done for each person individually; (3) detecting a single face, in which case, the face will be cut out of the original image and converted to the appropriate size for use in the next two networks.\n\nIn the next step, the cropped image is given to the head pose estimation network, which analyzes the head pose and estimates its pitch and yaw angles. In the next step, the obtained angles are compared with a set of thresholds. If the angles fall within the defined ranges, it will be interpreted as the learner being focused on the content, but if they do not, it will mean that the learner is not focused and the processing will stop. The thresholds used for this focus range are provided in Table  1 . If the angles suggest that the learner is focused, the image will be given to the emotion recognition network. This network produces two outputs, arousal value and valence value, each ranging from -10 to 10. These values are used to place the person's emotional state into one of five classes of engaged, tired, confused, disengaged, and neutral based on the two-dimensional circumplex model (Figure  7 ). This classification is done based on the thresholds α1 to α6, the values of which have been obtained through experimental research on different genders, ethnicities, and appearances. The values of these parameters are provided in Table  1 . While each frame can be processed on its own, the goal is to use the IA in ATS to detect the behavior of learners in videos. Therefore, for the system to work as intended on the videos, it is necessary to slightly change the processing procedure and the way states are detected by the networks. The procedure of video processing in the analyzer is shown in Figure  7 . Each video is converted to 15 frames per second one by the way. Assuming that the video is 10 seconds long, this conversion provides 150 frames for each video. In the first step, all frames of one video are given to the face detection network, where the output for each frame can be no face, one face, or multiple faces like the previous method. If the ratio of the number of No-Face outputs to all frames for that video is greater than the threshold given in Table  1 , the video will be labeled \"No-Face\" and no further processing will be performed. Otherwise, if the ratio of the number of Multiple-Faces outputs to all frames for the video is greater than the threshold given in Table  1 , the video will be labeled \"Multiple-Faces\" and if not, the video will be labeled \"Single-Face\" and the processing proceeds to the next step. Next, the square enveloping the face is cut out of the image and is given to the head post estimation network. To be precise, the frames containing a single face are labeled according to Table  1 . If the ratio of the number of \"Unfocused\" labels to the number of all labels is greater than the threshold given in Table  1 , the state will be recognized as \"Unfocused\" and if not, the processing proceeds to the next step. The frames labeled \"Focused\" are given to the emotion recognition network, where the arousal and valance values are going to be calculated for each frame. All resulting valance and arousal values will then be averaged to obtain two mean valance and arousal values. Figure  8  indicates the circumplex diagram for determining the emotional state based on these two values. As can be seen, the person's emotional state can be recognized to be one of the following: engaged, tired, confused, disengaged, and neutral. Note that in Table  1 , the thresholds with the term aggregator are related to the aggregation stage and the rest are related to the video analyzer stage.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Aggregator Design",
      "text": "While the analyzer can now take multi-second videos and determine which of the eight states they belong to, each lesson tends to last at least several minutes. A multi-minute video cannot be given to the analyzer at once, due to the fact that it requires a lot of processing and might also contain a shift in the learner's affective state, in which case the system will miss one of the states exhibited in the video. To solve this problem, the video recording process is considered to be in the form of 10-second sampling. Thus, we will ultimately have for each lesson as many states as there are 10-second video samples from that lesson. Each 10-second video sample can be labeled as one of the eight mentioned states by the way. Next, the multiple states obtained for each lesson will be combined in the aggregator. The aggregator selects the aggregation of the states based on how many 10second videos indicate each state and whether this number exceeds the corresponding threshold in Table  1 . For example, the final state \"Tired+Confused\" will be selected if more than two 10-second videos indicate \"Tired\" and \"Confused\". The important point in the selection of the final state out of the above 21 options is to check them in the above order from top to bottom (Figure  9 ). This order is determined by trial and error, after removing triple and higher combinations as well as low probability states. More detailed information about the states is provided in the appendix. Figure  9  shows the process of state selection in the aggregator. Ultimately, the IA is designed to recognize the state of learners in every lessons. Inside the IA a bunch of 10second (or shorter) videos are processed by analyzer and then all results are combined using the aggregator. The exact operational procedure of the IA in the learning environment is shown in Figure  10 . As the figure illustrates, first, the videos are processed separately and their results are stored in the database. Once a lesson is complete, the aggregator retrieves the results of all the videos recorded by the webcam from the database and aggregates them. In the end, the final state is reported back to the learning environment.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Implementation Of Ats",
      "text": "To implement the ATS, the IA is embedded in the ITS. The resulting system consists of three main units (Figure  2 ): Intelligent Analyzer: the purpose of this part is to continuously monitor the state of each learner while they watch the lessons. This component has two types of outputs: (1) a feedback message that is displayed to the user, (2) a recommendation of supplementary content for each lesson. Since the number of lessons varies in each cognitive style group, so does the supplementary content that can be recommended for each lesson. If the output of the IA is Unfocused + Confused, Engaged + Confused, Disengaged + Confused, or Confused, this unit notifies the teaching unit that the information of the supplementary content for the lesson (if there is any) must be displayed to the learner along with a message. Knowledge Base: this is a database comprised of multiple sections. The records section contains the records of every learner including their cognitive styles, the history of lessons watched and related feedback, and the history of test results. The lesson section contains the educational content for each cognitive style. The test section contains the tests of each session, and the recommendations section contains all the feedback and recommendations that can be provided depending on the state identified by the IA. Out of the 21 states that can be detected by the IA can, three states elicit the recommendation of supplementary content or textual feedback if no supplementary content is available. The message displayed for all recommendations is in the singular first person in order to create a more person-to-person teaching experience. The text of the recommendation messages is provided in the appendix. Teaching Unit: In this unit, the presenter agent retrieves the person's cognitive style from the records section (determined before login) and prepares the lessons for the person accordingly. Once the lesson is watched and the person's state is identified by the IA, the recommendation agent prepares the appropriate recommendation for display. It also prepares the link to the supplementary content when needed. The test agent prepares the test part of the session when all the lessons are over. If the person does not earn a passing grade, this agent will display all the supplementary contents of that session on the session page. The user interface component is designed outside the learning system. It allows people to view the content of each course in their browser and interact with ATS using mouse, keyboard, and webcam. This component uses the webcam to take videos of the learner's face while they watch the content and send them to the learning system.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Evaluation Of Ats",
      "text": "To evaluate the system, we designed a course with contents adapted to three cognitive styles. This course was presented to a number of participants, who were divided into three groups based on their cognitive styles, and in the end, the performance of the system in terms of academic success and academic satisfaction was evaluated. These steps are described below in more detail.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Educational Content",
      "text": "To evaluate the learning system, a course named \"Artificial Intelligence for everyone\" was prepared. In addition to evaluating the learning system, the purpose of this course was to provide an introduction to the concepts of artificial intelligence with a completely non-specialized approach. Figure  11  displays the arrangement of sessions in this course. As explained in Section 3-1, the test  [29]  was used to put each learner in one of three categories of analytical, wholistic, or middle before they log into the system.\n\nIn wholistic group, the educational content was created such that they gained a general picture of what they were supposed to learn in the session before proceeding to specific session. Also, the content for these learners was modified to contain less detailed information. However, to compensate the lack of detail in the lessens, supplementary contents prepared for these learners were more extensive and contained more examples and details than those prepared for the analytical group. In analytical group, the educational content was created such that they went through the topics methodically step by step and received an overview of the covered topics at the end of each lesson. Furthermore, they received an overview of the lessons at the end of each session as well. In other words, contrary to wholistic learners, analytical learners got a general picture of what they have supposed to learn at the end of the sessions. Supplementary contents prepared for this group were not as extensive as those for the wholistic one. For learners placed in the middle group, the contents prepared for wholistic learners were modified to contain slightly more details. Supplementary contents prepared for this style were similar to those for wholistic learners. The supplementary content prepared for each video was the same as the main content, except that the concepts were summarized and explained slower. These supplementary contents were created by providing simpler explanations with multiple examples for each lesson. As explained earlier, it is important for the supplementary contents to be hidden by default. In other words, they should become accessible under two conditions: (1) If the system recognizes that the main content is confusing or ambiguous for the learner; (2) If the learner does not earn the minimum score in the final exam of the session. In both cases, the supplementary content will appear on the lessons page.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Learners",
      "text": "Participants were chosen from a group of volunteering undergraduate and graduate students, who had little to no knowledge about artificial intelligence and were completely unfamiliar with the topics to be discussed in the course. The characteristics of the participants and the results of their cognitive style test are shown in Table  2 . All participants were selected through social networks by random selection from 30 volunteers. The participants were Persian speakers residing inside and outside Iran.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Evaluation",
      "text": "The performance of the educational system and environment was evaluated in terms of \"academic success\" and \"academic satisfaction\" [8], which refer to the effectiveness of ATS on the participants' performance in the learning process and their satisfaction with the learning environment respectively. In order to examine these two factors, two random persons from each cognitive group were chosen as the \"control group\". These learners could only watch the videos and did not have access to the feedbacks of the IA unit. The rest of the participants, for whom the IA was activated, were named the \"test group\". Academic success was measured with the following criteria: the test score of each session, the average score in the retaken tests, and the average number of attempts to pass the test of each session. Academic satisfaction was measured by a questionnaire that was administered at the end of the course. The test results for the first and second sessions are shown in Tables  3  and 4 . At the end of the course, the participants were given a questionnaire containing three questions. The first and second questions were about the quality of educational content and how easy or hard it was to work with the system (Table  5 ). In third question, they were asked to describe their experience of working with the elearning system and the problems they encountered during the course. The results of the evaluation of the system in terms of academic satisfaction and academic achievement were as follows:\n\n• People in the control group (for whom the IA was deactivated) needed more attempts to pass the tests in both sessions. For the first session, the average number of attempts in the control group was 11% higher than in the test group. For the second session, this figure was 68%. • According to the responses given to the third question of the academic satisfaction questionnaire, in the first session, people in the test group had a sense of excitement for receiving feedback from the system and spent a lot of time testing the feedbacks instead of focusing on the educational content. This might explain the significant difference between the two groups in terms of the mean time spent watching videos and the mean test score in the first session. In the second session, however, people in the test group, who were then familiar with the feedback mechanism, paid more attention to the learning process. As a result, they showed significant improvement in terms of the mean passing score, the mean number of attempts to earn a passing score, and the mean time spent watching videos. • While test and control groups were given the same content, people in the test group had a more favorable view of the quality of the content. This shows the effect of the intelligent learning system on the participants' perception of and interest in the provided content. Although the IA was disabled for the control group, two people in this group stated that the learning environment was intelligent and was able to present the lessons in a comprehensible way. This perception could be attributed to the provision of supplementary contents upon failure to earn the passing score and the personalization of content based on the learner's cognitive style.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, considering the learner's affective state, head pose estimation, and cognitive style had an impact on the learning process, an ATS was designed, implemented, and evaluated. In other words, the proposed system personalized the learning environment based on facial emotions, head pose, and cognitive style of learners. The results of performance evaluations showed when the Intelligent Analyzer unit -a unit that responsible for recognizing learner's state based on the face detection, head pose estimation and facial emotion recognition networks -was activated, learners could pass the final tests in fewer attempts and had 10% higher academic satisfaction than those for whom the Intelligent Analyzer unit was deactivated.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "B. Intelligent Analyzer",
      "text": "Table  1  shows the processing time of different components of the intelligent analyzer. As this table shows, it takes Nvidia-GTX 1080Ti graphics card about 4 seconds to process a 10-second video record of the system. Since videos are recorded with 10-second pauses, four users can be analyzed simultaneously in real-time.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "C. Aggregator",
      "text": "The state of the learner and the text message that must be displayed for each state are determined in the aggregator unit of the system. For the states involving confusion, the system has two sets of messages, one for when the lesson has supplementary content, and another for when it has not:\n\nNo-face+Multiple-Faces: To receive accurate feedback, you need to be alone in front of the webcam and stay there the entire time the video is being played.\n\nMultiple-Faces: It appears that you are not watching the content alone. Please make sure that you alone are in front of the webcam and start watching the video again.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Numerous-No-Face:",
      "text": "We could not find you in front of the webcam. Please rewatch the video while staying in front of the webcam.\n\nTired+Unfocused: It appears that you have not chosen the right time to watch the videos and may lack sufficient concentration. Please drink coffee/tea and then rewatch the video.\n\nTired+Confused: It appears that you are a little tired and find the content confusing. Please take a rest or drink coffee/tea before continuing.\n\nUnfocused+Confused: (1) It appears that you find the content a little confusing. This could be because you have not focused enough on watching the video. You might find it helpful to find a quiet place and put away your mobile phone before rewatching the video. (2) It appears that you find the content a little confusing. This could be because you have not focused enough on watching the video. You might find it helpful to find a quiet place and put away your mobile phone before rewatching the video. You are also recommended to watch the following supplementary videos to better understand the subject.\n\nEngaged+Tired: Thank you for paying attention to the videos, but you seem to be a little tired. Please take a rest before proceeding to the next video.\n\nEngaged+Confused: (1) Thank you for paying attention to the videos, but it appears that you find some of the content confusing. Please watch the video again. (2) Thank you for paying attention to the videos, but it appears that you need further explanation about some of the content. You can find these explanations and more examples in the following supplementary videos. Tired+Disengaged: If you feel that the content is not right for you, it could be because you are tired. Please take a rest and return another time.\n\nEngaged+No-Face: Thank you for paying attention to the content, but it appears you cannot stay in front of the webcam. Watching all parts of the video will have a great impact on your learning.\n\nDisengaged+No-Face: If you do not want to watch videos now or cannot stay in front of the webcam, it is recommended that you return another time.",
      "page_start": 15,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Using the Russell model, a",
      "page": 2
    },
    {
      "caption": "Figure 1: Diagram of Russell’s two-dimensional emotion model [13].",
      "page": 2
    },
    {
      "caption": "Figure 2: Figure 2. The parts of the proposed web-based learning system. The ATS part is on the server-side and the user interface part is on the",
      "page": 3
    },
    {
      "caption": "Figure 3: shows an example of how personalized content is created.",
      "page": 4
    },
    {
      "caption": "Figure 3: Lesson arrangement for learners in wholistic, analytical, and middle cognitive groups. In addition to the arrangement of the",
      "page": 4
    },
    {
      "caption": "Figure 4: ). This unit consists of two components, analyzer and aggregator. The analyzer is first designed for the",
      "page": 4
    },
    {
      "caption": "Figure 4: Relationship of the intelligent analyzer with the learning environment. The intelligent analyzer unit is the product of the fusion of",
      "page": 5
    },
    {
      "caption": "Figure 5: Figure 5. The outputs of the Head Pose estimation network: roll, pitch, and yaw.",
      "page": 5
    },
    {
      "caption": "Figure 5: This unit consists of two components, analyzer and",
      "page": 6
    },
    {
      "caption": "Figure 6: Flowchart of frame processing in the analyzer. At the end each frame will belong to one of eight states marked by dotted lines.",
      "page": 6
    },
    {
      "caption": "Figure 7: ). This classification is done",
      "page": 6
    },
    {
      "caption": "Figure 7: Figure 7. Flowchart of video processing in the analyzer.",
      "page": 7
    },
    {
      "caption": "Figure 8: indicates the circumplex diagram for determining the",
      "page": 8
    },
    {
      "caption": "Figure 8: Circumplex diagram for determining the learners’ emotional state based on arousal and valance values. The parameters are",
      "page": 8
    },
    {
      "caption": "Figure 9: ). This order is determined by trial and error, after removing triple and higher",
      "page": 8
    },
    {
      "caption": "Figure 9: shows the process of state selection in the aggregator.",
      "page": 8
    },
    {
      "caption": "Figure 9: Flowchart of final state selection in the aggregator.",
      "page": 9
    },
    {
      "caption": "Figure 10: As the figure",
      "page": 9
    },
    {
      "caption": "Figure 10: Work flowchart of the intelligent analyzer unit in the learning environment.",
      "page": 9
    },
    {
      "caption": "Figure 11: Course information.",
      "page": 10
    },
    {
      "caption": "Figure 11: displays the arrangement of",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Threshold values in the intelligent analyzer.",
      "data": [
        {
          "Threshold": "Face Detection confidence",
          "value": "0.7"
        },
        {
          "Threshold": "No-Face / Total",
          "value": "0.25"
        },
        {
          "Threshold": "Multiple-Faces / Total",
          "value": "0.25"
        },
        {
          "Threshold": "Unfocused / Total",
          "value": "0.35"
        },
        {
          "Threshold": "Focus Yaw Range",
          "value": "-29 to 29"
        },
        {
          "Threshold": "Focus Pitch Range",
          "value": "-37 to 16"
        },
        {
          "Threshold": "α1",
          "value": "-1.5"
        },
        {
          "Threshold": "α2",
          "value": "1"
        },
        {
          "Threshold": "α3",
          "value": "1"
        },
        {
          "Threshold": "α4",
          "value": "-2"
        },
        {
          "Threshold": "α5",
          "value": "6"
        },
        {
          "Threshold": "α6",
          "value": "-5"
        },
        {
          "Threshold": "Emotion Multiplier",
          "value": "1.0"
        },
        {
          "Threshold": "Disengaged# - Aggregator",
          "value": "0"
        },
        {
          "Threshold": "Engaged# - Aggregator",
          "value": "1"
        },
        {
          "Threshold": "Tired# - Aggregator",
          "value": "2"
        },
        {
          "Threshold": "Confused# - Aggregator",
          "value": "2"
        },
        {
          "Threshold": "Multiple-Faces# - Aggregator",
          "value": "2"
        },
        {
          "Threshold": "No-Face# - Aggregator",
          "value": "2"
        },
        {
          "Threshold": "Numerous No-Faces# - Aggregator",
          "value": "4"
        },
        {
          "Threshold": "Unfocused# - Aggregator",
          "value": "2"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: All participants were selected through social networks by random selection from 30 volunteers. The participants",
      "data": [
        {
          "Total Number": "Education level",
          "14": "Bachelor’s"
        },
        {
          "Total Number": "",
          "14": "Master’s"
        },
        {
          "Total Number": "Age",
          "14": "Minimum"
        },
        {
          "Total Number": "",
          "14": "Maximum"
        },
        {
          "Total Number": "",
          "14": "Mean"
        },
        {
          "Total Number": "Gender",
          "14": "Male"
        },
        {
          "Total Number": "",
          "14": "Female"
        },
        {
          "Total Number": "Cognitive style",
          "14": "Wholistic"
        },
        {
          "Total Number": "",
          "14": "Analytical"
        },
        {
          "Total Number": "",
          "14": "Middle"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Results of the participants in the first session.",
      "data": [
        {
          "Mean time spent \nMean number of \nMean score in \nMean score in the \nMean passing \nwatching the \nattempts to earn a \nthe first \nsecond attempt \nscore (%) \ncontents (minutes) \npassing score \nattempt (%) \n(%)": "71 \n2.9 \n89.9 \n72.2 \n83"
        },
        {
          "Mean time spent \nMean number of \nMean score in \nMean score in the \nMean passing \nwatching the \nattempts to earn a \nthe first \nsecond attempt \nscore (%) \ncontents (minutes) \npassing score \nattempt (%) \n(%)": "112 \n2.4 \n83.7 \n65 \n81.3"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 3: Results of the participants in the first session.",
      "data": [
        {
          "Mean time spent \nMean number of \nMean score in \nMean score in \nMean passing \nwatching the contents \nattempts to earn a \nthe first \nthe second \nscore (%) \n(minutes) \npassing score \nattempt (%) \nattempt (%)": "68 \n3.2 \n86.6 \n67.1 \n79.8"
        },
        {
          "Mean time spent \nMean number of \nMean score in \nMean score in \nMean passing \nwatching the contents \nattempts to earn a \nthe first \nthe second \nscore (%) \n(minutes) \npassing score \nattempt (%) \nattempt (%)": "98 \n1.9 \n94.4 \n81 \n100"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 1: shows the processing time of different components of the intelligent analyzer. As this table shows, it",
      "data": [
        {
          "Time (ms)": "1.8 \n6 \n6 \n10"
        },
        {
          "Time (ms)": "23.8"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "onnecting with learning: Motivation, affect and cognition in interest processes",
      "authors": [
        "M Ainley"
      ],
      "year": "2006",
      "venue": "Educ. Psychol. Rev",
      "doi": "10.1007/s10648-006-9033-0"
    },
    {
      "citation_id": "2",
      "title": "Analyzing learner affect in a scenario-based intelligent tutoring system",
      "authors": [
        "B Nye"
      ],
      "venue": "Analyzing learner affect in a scenario-based intelligent tutoring system",
      "doi": "10.1007/978-3-319-61425-0_60"
    },
    {
      "citation_id": "3",
      "title": "Affect, Mood, and motion",
      "authors": [
        "P Kkekakis"
      ],
      "year": "2020",
      "venue": "Measurement in Sport and Exercise Psychology"
    },
    {
      "citation_id": "4",
      "title": "Agent based affective tutoring systems: A pilot study",
      "authors": [
        "X Mao",
        "Z Li"
      ],
      "year": "2010",
      "venue": "Comput. Educ",
      "doi": "10.1016/j.compedu.2010.01.005"
    },
    {
      "citation_id": "5",
      "title": "Positive affect and decision making",
      "authors": [
        "M Srite",
        "B Ayres"
      ],
      "year": "1999",
      "venue": "Am. Conf. Inf. Syst"
    },
    {
      "citation_id": "6",
      "title": "motions and Personality in Adaptive e-Learning Systems: An Affective omputing Perspective",
      "authors": [
        "O Santos"
      ],
      "venue": "motions and Personality in Adaptive e-Learning Systems: An Affective omputing Perspective"
    },
    {
      "citation_id": "7",
      "title": "The influence of using affective tutoring system in accounting remedial instruction on learning performance and usability",
      "authors": [
        "H Lin",
        "C Wu",
        "Y Hsueh"
      ],
      "year": "2014",
      "venue": "Comput. Human Behav",
      "doi": "10.1016/j.chb.2014.09.052"
    },
    {
      "citation_id": "8",
      "title": "An affect-sensitive intelligent tutoring system with an animated pedagogical agent that adapts to student emotion like a human tutor",
      "authors": [
        "T Alexander"
      ],
      "venue": "An affect-sensitive intelligent tutoring system with an animated pedagogical agent that adapts to student emotion like a human tutor"
    },
    {
      "citation_id": "9",
      "title": "Are There Basic motions?",
      "authors": [
        "P Kman"
      ],
      "year": "1992",
      "venue": "Psychol. Rev",
      "doi": "10.1037/0033-295X.99.3.550"
    },
    {
      "citation_id": "10",
      "title": "DAIS : dataset for affective states in e-learning environments",
      "authors": [
        "A Gupta",
        "R Jaiswal",
        "S Adhikari",
        "V Balasubramanian"
      ],
      "year": "2016",
      "venue": "DAIS : dataset for affective states in e-learning environments"
    },
    {
      "citation_id": "11",
      "title": "motions in medical education: xamining the validity of the Medical motion Scale (M S) across authentic medical learning environments",
      "authors": [
        "M Duffy",
        "S Lajoie",
        "R Pekrun",
        "K Lachapelle"
      ],
      "year": "2020",
      "venue": "Learn. Instr",
      "doi": "10.1016/j.learninstruc.2018.07.001"
    },
    {
      "citation_id": "12",
      "title": "Beyond cold technology: A systematic review and meta-analysis on emotions in technology-based learning environments",
      "authors": [
        "K Loderer",
        "R Pekrun",
        "J Lester"
      ],
      "year": "2020",
      "venue": "Learn. Instr",
      "doi": "10.1016/j.learninstruc.2018.08.002"
    },
    {
      "citation_id": "13",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J. Pers. Soc. Psychol",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "14",
      "title": "Searching for the role of emotions in e-learning",
      "authors": [
        "R Mayer"
      ],
      "year": "2020",
      "venue": "Learn. Instr",
      "doi": "10.1016/j.learninstruc.2019.05.010"
    },
    {
      "citation_id": "15",
      "title": "ognitive style and instructional preferences",
      "authors": [
        "E Sadler-Smith",
        "R Riding"
      ],
      "year": "1999",
      "venue": "Instr. Sci",
      "doi": "10.1007/bf00892031"
    },
    {
      "citation_id": "16",
      "title": "On Incorporating Affective Support to an Intelligent Tutoring System: An mpirical Study",
      "authors": [
        "C Cunha-Perez",
        "M Arevalillo-Herraez",
        "L Marco-Gimenez",
        "D Arnau"
      ],
      "year": "2018",
      "venue": "Rev. Iberoam. Tecnol. del Aprendiz",
      "doi": "10.1109/RITA.2018.2831760"
    },
    {
      "citation_id": "17",
      "title": "An Affective Tutoring System for Massive Open Online ourses",
      "authors": [
        "M Soltani",
        "H Zarzour",
        "M Babahenini",
        "Hemam"
      ],
      "year": "2020",
      "venue": "An Affective Tutoring System for Massive Open Online ourses",
      "doi": "10.1007/978-3-030-21005-2_20"
    },
    {
      "citation_id": "18",
      "title": "onstructing an affective tutoring system for designing course learning and evaluation",
      "authors": [
        "C.-H Wang",
        "H Lin"
      ],
      "year": "2018",
      "venue": "J. Educ. Comput. Res"
    },
    {
      "citation_id": "19",
      "title": "Usability of an Affective Emotional Learning Tutoring System for Mobile Devices",
      "authors": [
        "T.-H Wang",
        "H.-C Lin",
        "H.-R Chen",
        "Y.-M Huang",
        "W.-T Yeh",
        "C.-T Li"
      ],
      "venue": "Sustainability",
      "doi": "10.3390/su13147890"
    },
    {
      "citation_id": "20",
      "title": "Sentiment Analysis in an Affective Intelligent Tutoring System",
      "authors": [
        "M Barron-Estrada",
        "R Zatarain-Cabada",
        "R Oramas-Bustillos",
        "F Gonzalez-Hernandez"
      ],
      "venue": "Sentiment Analysis in an Affective Intelligent Tutoring System"
    },
    {
      "citation_id": "21",
      "title": "Affective tutoring system for built environment management",
      "authors": [
        "A Kaklauskas"
      ],
      "year": "2015",
      "venue": "Comput. Educ",
      "doi": "10.1016/j.compedu.2014.11.016"
    },
    {
      "citation_id": "22",
      "title": "Testing the ATI hypothesis: Should multimedia instruction accommodate verbalizer-visualizer cognitive style?",
      "authors": [
        "L Massa",
        "R Mayer"
      ],
      "year": "2006",
      "venue": "Learn. Individ. Differ",
      "doi": "10.1016/j.lindif.2006.10.001"
    },
    {
      "citation_id": "23",
      "title": "The stubborn myth of'learning styles' state teacher-license prep materials peddle a debunked theory",
      "authors": [
        "W Furey"
      ],
      "year": "2020",
      "venue": "Education Next"
    },
    {
      "citation_id": "24",
      "title": "Another Nail in the offin for Learning Styles? Disparities among Undergraduate Anatomy Students' Study Strategies, lass Performance, and Reported VARK Learning Styles",
      "authors": [
        "P Husmann",
        "V O'loughlin"
      ],
      "year": "2019",
      "venue": "Anat. Sci. Educ",
      "doi": "10.1002/ase.1777"
    },
    {
      "citation_id": "25",
      "title": "Learning styles concepts and evidence",
      "authors": [
        "H Pashler",
        "M Mcdaniel",
        "D Rohrer",
        "R Bjork"
      ],
      "year": "2008",
      "venue": "Psychol. Sci. Public Interes. Suppl",
      "doi": "10.1111/j.1539-6053.2009.01038.x"
    },
    {
      "citation_id": "26",
      "title": "The relationship between the vark learning styles and academic achievement in dental students",
      "authors": [
        "H Mozaffari",
        "M Janatolmakan",
        "R Sharifi",
        "F Ghandinejad",
        "B Andayeshgar",
        "A Khatony"
      ],
      "year": "2020",
      "venue": "Adv. Med. Educ. Pract",
      "doi": "10.2147/AMEP.S235002"
    },
    {
      "citation_id": "27",
      "title": "The Learning Styles ducational Neuromyth: Lack of Agreement Between Teachers' Judgments, Self-Assessment, and Students' Intelligence",
      "authors": [
        "M Papadatou-Pastou",
        "M Gritzali",
        "A Barrable"
      ],
      "year": "2018",
      "venue": "Front. Educ",
      "doi": "10.3389/feduc.2018.00105"
    },
    {
      "citation_id": "28",
      "title": "Opinion: Uses, misuses, and validity of learning styles",
      "authors": [
        "R Felder"
      ],
      "year": "2020",
      "venue": "Adv. Eng. Educ"
    },
    {
      "citation_id": "29",
      "title": "Riding and I. heema, \" ognitive Styles-an overview and integration",
      "year": "1991",
      "venue": "Riding and I. heema, \" ognitive Styles-an overview and integration",
      "doi": "10.1080/0144341910110301"
    },
    {
      "citation_id": "30",
      "title": "FaceBoxes: A PU real-time face detector with high accuracy",
      "authors": [
        "S Zhang",
        "X Zhu",
        "Z Lei",
        "H Shi",
        "X Wang",
        "S Li"
      ],
      "venue": "FaceBoxes: A PU real-time face detector with high accuracy",
      "doi": "10.1109/BTAS.2017.8272675"
    },
    {
      "citation_id": "31",
      "title": "How Self-Supervised Learning Can be Used for Fine-Grained Head Pose stimation?",
      "authors": [
        "M Pourmirzaei",
        "G Montazer",
        "F Smaili"
      ],
      "venue": "How Self-Supervised Learning Can be Used for Fine-Grained Head Pose stimation?"
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "Accessed"
      ],
      "venue": ""
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "Online"
      ],
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "Fine-grained head pose estimation without keypoints",
      "authors": [
        "N Ruiz",
        "J Rehg"
      ],
      "venue": "Fine-grained head pose estimation without keypoints",
      "doi": "10.1109/CVPRW.2018.00281"
    },
    {
      "citation_id": "35",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "36",
      "title": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation",
      "authors": [
        "M Pourmirzaei",
        "G Montazer",
        "F Smaili"
      ],
      "venue": "Using Self-Supervised Auxiliary Tasks to Improve Fine-Grained Facial Representation"
    },
    {
      "citation_id": "37",
      "title": "fficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "venue": "fficientNet: Rethinking model scaling for convolutional neural networks"
    },
    {
      "citation_id": "38",
      "title": "AffectNet: A Database for Facial xpression, Valence, and Arousal omputing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "39",
      "title": "Missing parts 2. Wrong labels",
      "venue": "Missing parts 2. Wrong labels"
    },
    {
      "citation_id": "40",
      "title": "Wrong data type",
      "venue": "Wrong data type"
    }
  ]
}