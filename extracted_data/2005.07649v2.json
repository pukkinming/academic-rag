{
  "paper_id": "2005.07649v2",
  "title": "Convolutional Neural Network For Emotion Recognition To Assist Psychiatrists And Psychologists During The Covid-19 Pandemic: Experts' Opinion",
  "published": "2020-05-15T17:09:10Z",
  "authors": [
    "Hugo Mitre-Hernandez",
    "Rodolfo Ferro-Perez",
    "Francisco Gonzalez-Hernandez"
  ],
  "keywords": [
    "Emotion recognition",
    "Convolutional Neural Network",
    "COVID-19",
    "psychologists and psychiatrists"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A web application with real-time emotion recognition for psychologists and psychiatrists is presented. Mental health effects during COVID-19 quarantine need to be handled because society is being emotionally impacted. The human micro-expressions can describe genuine emotions that can be captured by Convolutional Neural Networks (CNN) models. But the challenge is to implement it under the poor performance of a part of society's computers and the low speed of internet connection, i.e., improve the computational efficiency and reduce the data transfer. To validate the computational efficiency premise, we compare CNN architectures results, collecting the floating-point operations per second (FLOPS), the Number of Parameters (NP) and accuracy from the MobileNet, PeleeNet, Extended Deep Neural Network (EDNN), Inception-Based Deep Neural Network (IDNN) and our proposed Residual mobile-based Network model (ResmoNet). Also, we compare the trained models' results in terms of Main Memory Utilization (MMU) and Response Time to complete the Emotion (RTE) recognition. Besides, we design a data transfer that includes the raw data of emotions and the basic patient' information. The web application was evaluated with the System Usability Scale (SUS) and a utility questionnaire by psychologists and psychiatrists. ResmoNet model generated the most reduced NP, FLOPS, and MMU results, only EDNN overcomes ResmoNet in 0.01sec in RTE. The optimizations to our model impacted the accuracy, therefore IDNN and EDNN are 0.02 and 0.05 more accurate than our model respectively. Finally, according to the psychologists and psychiatrists, the web application has good usability (73.8 of 100) and utility (3.94 of 5).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotions \"are a process, a particular kind of automatic appraisal influenced by our evolutionary and personal past, in which we sense that something important to our welfare is occurring, and a set of psychological changes and emotional behaviors begins to deal with the situation\"  [1] . There are several negative emotional behaviors in the society evoked by certain situations during the COVID-19 pandemic.\n\nChinese children and adolescents are experiencing fear due to interrogatories about the COVID-19 pandemic  [2] . Also, the unpleasant experience of quarantine is generating anger in society  [3] , sadness, and disgust  [4] . Emotions are mainly influenced by depression, anxiety, and stress. Anger, fear, sadness, and happiness are evoked emotions during the clinical disorders of depression, anxiety, and mixed anxiety-depression. Also, sadness and disgust can be elevated in the depressed and mixed disorders, whereas increased levels of anger and fear, and decreased levels of happiness  [4] , indicating a stress situation, in such a way, post-traumatic stress disorder was reported in China, Spain, Italy, Iran, the US, Turkey, Nepal, and Denmark  [5] . Also, sadness and fear are evoked in social media, and their intensity fluctuates over time  [6] . The clinical disorders and its origins need to be explored.\n\nMuch of society's emotions are being affected by the COVID-19 pandemic. Not only infected people are affected, but also children, parents, healthcare workers, and more. Children's mental disturbance is tied to parent's stress  [7] , and health care workers have higher levels of psychiatric symptoms  [8] . We consider much of society is emotionally affected, for this reason, psychological and psychiatric intervention is urgently needed.\n\nThe evidence-based treatments for an individual with emotional disorders are basic for its empirical validation  [9] . That is why emotional records are indispensable as part of such evidence. Is well known that movements of facial muscles are associated with emotions  [10] . Spontaneous emotions are typically expressed through facial micro-expressions. Micro-expressions have been a growing interest in computer vision and artificial intelligence studies to automate emotion recognition; the researchers name it Facial Emotion Recognition (FER) and its data can be used as emotional evidence for the treatments of psychologists and psychiatrists.\n\nThere is an imminent risk of contagion between patient-psychologist or patient-psychiatrist in consulting rooms. Taking advantage of the fact that 70.1% of the population aged 6 and over in Mexico has access to the internet  [11] , web applications can mitigate this risk. But the major problem in México and other countries is the poor performance of computers in a major part of society and the low speed of internet connection in several regions. For these reasons, we propose a web application to record realtime emotions in a patient's card (personal and treatment information) developed with the following considerations: the CNN model must generate fewer processor' operations using less primary memory and reducing time to emotions recognition than other models to achieve the performance needed, data transfer size must be very small to works in low bandwidth internet connections. Also, our proposal needs to be evaluated by psychologists and psychiatrists with many years of experience and patients treated due to the COVID-19 circumstances.\n\nComparing the results of our Residual mobile-based Network (ResmoNet) model with the best model. It was obtained that ResmoNet generated 115,976 number of parameters less than MobileNet, 243,901 floating-point operations per second (FLOPS) less than MobileNet, and 5% less accuracy than EDNN (95%). Moreover, ResmoNet used less main memory utilization than any model, only EDNN overcomes ResmoNet in 0.01 seconds for the time to complete the emotion recognition. For data transfer, the patient's card and raw emotional data have 2 kb with a UTF-8 encoding approximately. Finally, according to the psychologists and psychiatrists, the web application has good usability (73.8 of 100) and utility (3.94 of 5).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "We present a background of CNN optimized models, the proposed CNN model with its the preprocessing and training stages and the web application for psychiatrists and psychologists. Also, the methods to compare the IDNN, EDNN, MobileNet and to evaluate the usability and utility the Web application.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Optimized Cnn Models",
      "text": "In deep-learning-based FER approaches, the Deep Convolutional Neural Networks (Deep CNNs) have a large performance improvement in comparison to conventional unsupervised approaches of machine learning  [12] [13] [14] . In general, the function of CNNs models receive the face image, it is convolved through a filter collection in the convolution layers to create the feature map, this is combined to fully connected networks, and then face expression is recognized into a particular class as the output of the softmax algorithm  [15] . Studies of CNN models need to be analyzed to design an optimized model.\n\nFor CNN models effectiveness, the Residual Networks (ResNet) are easier to optimize and can improve the accuracy from considerably increased depth  [16] . ResNet models are often implemented with two or three layers skips that contain non-linearities (ReLU, Refined Linear Units) and batch normalization  [17]  or concatenate --e.g, the EDNN model  [18]  in between. Another strategy to reduce the floating-point operations is the depthwise separable convolution defined in MobileNet  [19] , which deals with the depth dimension --the number of channels. A depthwise separable convolution works with kernels that cannot be \"factored\" into two smaller kernels, therefore, it splits the kernel into 2 separate kernels that do two convolutions: the depthwise convolution and the pointwise convolution. Inception module architecture  [20]  deploys multiple convolutions with multiple filters and pooling layers simultaneously in parallel within the same layer --the inception layer. Mollahosseini et al.  [21]  confirmed in its proposal of Inception-Based Deep Neural Network (IDNN), the use of this module can increase the depth and width of the architecture while preserves the computational cost constant.\n\nMobileNet model was built primarily from depthwise separable convolutions used in inception models  [22]  to reduce the computational cost in the first few layers. Depthwise separable convolution deals with the depth dimension, i.e. the number of channels. A depthwise separable convolution works with kernels that cannot be \"factored\" into two smaller kernels. Therefore, it splits the kernel into two separate kernels that do two convolutions: the depthwise convolution and the pointwise convolution (a simple 1x1 convolution). In the comparative study of  [19] , MobileNet achieved a 79.4% of accuracy with 5.60 million less Mult-Adds (computation) compared to FaceNet  [23]  who reached 83% accuracy. The MobileNet's architecture generates fewer Mult-Adds, sacrificing accuracy for resource-limited devices.\n\nPeleeNet  [17]  model is a variant of the DenseNet architecture that follows its connectivity pattern  [24] . Looking to optimize memory and computational cost, its architecture was based on (i) the two-way dense layer of GoogLeNet  [25] ; (ii) the inception-v4  [26]  and the deeply supervised object detectors  [27]  to design a cost-efficient stem block before the first dense layer; and (iii) bath normalization  [22]  for postactivation in the composite function. The major improvements of speed and accuracy in PeleeNet were the adjustments to (i) the feature map selection based on Single Shot MultiBox Detector  [28]  in 5 scale features: 19x19, 10x10, 5x5, 3x3, and 1x1 (excluding 38 x 38 feature map); (ii) the residual prediction block  [16]  for each feature map; and (iii) a 1x1 convolutional kernels for prediction, this reduced 21.5 % of computational cost and the model size (number of parameters). PeleeNet and MobileNetV2 were evaluated on NVIDIA TX2 embedded platform processing 100 pictures with 1 batch size, even though MobileNetV2 achieves high accuracy with 300 FLOPS, the speed of PeleeNet was better than MobileNet with 569 FLOPS.\n\nThe EDNN model was designed for facial emotion recognition  [18] . For face detection authors proposed a deep convolution neural network composed of six convolution layers, two blocks of deep residual learning, after each convolution layer. Also, deep residual blocks are implemented after the second and fourth convolution, two fully connected layers, each with a ReLU activation function, and dropout for the training layer. The combination of fully connected layers and residual blocks could improve the overall result. EDNN obtained the accuracy of 93.24 % trained with the Extended Cohn-Kanade dataset (CK+)  [29]  for emotion recognition of sad, happy, surprise angry, disgust, and fear, surpassing other models of various authors that presented a lower accuracy at 92.71 %  [30] , 92.35 %  [31] , 92.74 %  [32] , and 92.73 %  [33] .\n\nIDNN  [21]  model was inspired by GoogLeNet  [20]  and AlexNet  [34] , it consists of two CNN modules -a convolution layer by a max-pooling layer-using the ReLU activation function Refined Linear Units to avoid the vanishing gradient problem. After these two modules, the network-in-network architecture and two inception modules were applied -made up of 1x1, 3x3, and 5x5 convolution layers using ReLU in parallel. The inception layers are concatenated as output and, finally, two fully connected layers as the classifying layers. The trained model of IDNN with the CK+ dataset obtained the average accuracy of 97.8 % in comparison with other works at 88.5 %  [25] , 92 %  [35] , 92.4 %  [36]  and 93.6 %  [37] . The authors create their proposal based on the inception layer to improve the recognition of local features such as the eye and mouth, expressions that can describe an emotion. Also, they applied the network-innetwork theory  [38]  to enhance the local feature performance, increase the global pooling performance and reduce prone to overfitting. Our CNN model was based on several optimizations mentioned before, mainly on stem, mobile and residual block structures, small kernel size, among other optimization practices.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Residual Mobile-Based Network",
      "text": "Our proposal ResmoNet contains a stem block, which is fed with our input images, followed by the composition of two networks: a mobile network containing one mobile block, and a residual network containing a residual block and a transition block. After these blocks, a dense block of neurons is added before the output layer. Motivated by  [17] , we implemented a Stem Block before the mobile section of our proposal. The structure of the Stem Block can be seen in Figure  1 . The Mobile Network that follows the Stem Block can be composed of a sequence of m Mobile Blocks by defining the parameter m, which is a mobile-depth parameter (it specifies the number of Mobile Blocks). Our proposal contains a single Mobile Block (m=1). The Mobile Block is implemented as defined in  [19] , which is structured by the following sequence: a depthwise 2D convolution with batch normalization and ReLU as the activation function, a 2D pointwise convolution with batch normalization and ReLU as the activation function, and finally an average pooling. The Residual Block that follows the Mobile Block can be composed of a sequence of r Residual Blocks by defining the parameter r, which is a residual-depth parameter (it specifies the number of Residual Blocks) and a Transition Block. It contains a single Residual Block (r=1). The structure of the Residual Block can be seen in Figure  2 . The Transition Block is structured as follows: it contains a 2D convolution with batch normalization and ReLU as the activation function and finally an average pooling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fig. 2",
      "text": "Residual Block structure.\n\nFinally, a Dense Block of two layers with dropout is added. A complete overview of the model architecture is shown in Figure  3 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fig. 3",
      "text": "Overview of the ResMoNet model architecture.\n\nResmoNet was built using the Keras library  [39]  on top of Google's TensorFlow  [40]  framework. The versions used were 2.2.4 on Keras and 2.0.0 on TensorFlow. For image preprocessing, we used OpenCV 3.4.3 with Python.\n\nThe training of our proposed neural network was done on a machine with the following hardware specifications: GPU: 1×Tesla K80 with CUDA 10.1; CPU: 1×Single core hyperthreaded (i.e. 1 core, 2 threads) Intel(R)Xeon(R) CPU @ 2.30GHz, 45 MB Cache; RAM: ∼12 GB available; Disk: ∼310 GB available.\n\nOnce that our proposed model was trained, it was deployed, tested, and measured on a Raspberry Pi 3 Model B with the same software specifications as the previously mentioned for training, and with the following hardware specifications: CPU: 4×ARM Cortex-A53, 1.2GHz; GPU: Broadcom VideoCore IV; RAM: 1GB LPDDR2 (900 MHz); Disk: ∼32 GB available.\n\nOn this deployment, the execution time response (ETR) and the main memory utilization (MMU) were measured as computational resources for the evaluation of the modeling production.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset, Preprocessing, And Training",
      "text": "The Radboud Faces Database (RaFD) is a high-quality faces database containing pictures of 67 models (including Caucasian males and females, Caucasian children, both boys and girls, and Moroccan Dutch males) displaying 8 emotional expressions (accordingly to the Facial Action Coding System): Anger, disgust, fear, happiness, sadness, surprise, contempt, and neutral. Each emotion was shown with three different gaze directions  [41] . To train ResmoNet, the images from the RaFD with front faces were used, including the images with the view to the front and the sides.\n\nTo generate the set of ROIs that will be split into the input images to train and test our model, we preprocessed raw images from the RaFD dataset following the steps shown in Figure  4 . Each step of the whole preprocess consists of the following sub-processes:\n\n• Face detection on images (see Figure  4A ). This process consists of the detection of ROIs inside the images that contain the face. For this, the default frontal face Haar cascades from OpenCV were used and the (x, y) coordinates of the bounding box around the face were extracted. • Face cropping from images (see Figure  4B ). Once that the face is detected on the image, we use the extracted (x, y) coordinates of the bounding box to crop the detected face and generate a squared ROI. • Cropped face resizing (see Figure  4C ). This process consists of resizing the ROI (cropped image) into the defined input size for the model (224x224x3), so regarding the sizes for different faces detected, we standardize the input size. Once that all the images were cropped and resized to have only 224x224x3 sized images of faces, we proceeded to augment data. The augmented data consists of the following new images:\n\n• The sub-section of the top-left corner, removing the remaining border of the bottom-right section to create a sub-image with size 186x186x3 which was also resized to 224x224x3. This process was repeated for the top-right corner, the bottom-left corner, and the bottomright corner, removing the remaining borders to create a sub-image with size 186x186x3 which were also resized to 224x224x3.\n\n• The sub-section of the center of the image, removing the remaining border around the original ROI to create a sub-image with size 148x148x3 which was also resized to 224x224x3. • The flipped image around the Y-axis of the original image.\n\n• Each of the previous processes for data augmentation for the flipped image.\n\nAn example of images generated in the data augmentation process from a single image from the Radboud Faces Database can be seen in Figure  5 . The preprocessed RafD dataset was split into training (80 %) and testing (20 %) subsets of images. With the training subset, the data augmentation process was applied. Our proposed model, along with the other models for comparison were trained 150 epochs with a batch size of 128 in the previously specified GPU environment. To maintain consistency in the experiment replica for the different network architectures, we set a random seed, so the same images for training and testing were used on each network.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Method Of The Dnn Model",
      "text": "We performed a comparison of IDNN, EDNN, 1.0 MobileNet, 0.75 MobileNet, and ResMoNet models using computational efficiency measures. Let us define Computational Efficiency as the properties of an algorithm/software which relate to the amount of computational resources used by the algorithm/software  [42] . The comparison of architectures and trained models in terms of computational properties allow us to know its efficiency for resource-limited devices.\n\nTo measure the computational efficiency of the models' architecture, we used the number of parameters (NP) in a given layer as the count of \"learnable\" elements in that layer. This means the number of elements to be optimized. Also, the floating-point operations per second (FLOPS) is a measure of computer performance, which requires floating-point calculations multiply-accumulate (Mult-Add) operations. The Mult-Add operation is a common step that computes the product of two numbers and adds that product to an accumulator. Finally, the classification accuracy is obtained.\n\nOther measures were collected from the trained model installed in the Raspberry Pi 3 Model B. The Response Time to complete the Emotion recognition (RTE)  [43]  is calculated as the average of time records elapsed between the start and completion of a task of emotion recognition; five executions of a minute in each trained model give us the time records. The Main Memory Utilization (MMU)  [44]  is measured as the amount of main memory used during trained model execution.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Web Application",
      "text": "The web application was built using diverse libraries on the language programming Python, we use Flask, a web framework to process requests from the Internet and Dash with Plotly for generating real-time plots on the web. The web application consists of three main pages for user interaction (see Figure  6 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fig. 6",
      "text": "Web application. (a) a login page, which provides access to the psychologist or psychiatrist; (b) A medical records page, which contains medical records for each patient, it displays information on plots of emotions (anger, disgust, fear, happiness, sadness, surprise, and neutral) generated in the web application; (c) A register medical activity page to add new activities in real-time; and the main technologies used.\n\nOn the other hand, in the background, the web application continuously consumes a real-time database hosted on Firebase technology, a technology to keep data on the cloud. To saving full histories of interaction from the web application. This database can be downloaded for additional reviews, and it can be filtered in specific periods of any session.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Method Of The Application Web",
      "text": "Usability and utility of the web application were evaluated with the experts' support (see the questionnaires in Appendix 1). We consider an expert a psychologist or psychiatrist with more than 5 years of experience, moreover, experts should have cared for patients emotionally affected by the circumstances generated by the COVID-19 pandemic.\n\nWe presented a video web application used by the experts. For the usability evaluation, we used the System Usability Scale (SUS) questionnaire  [45] . To calculate the SUS score for each expert, we follow the next steps: 1. Convert the scale into numbers for each of the 10 questions\n\n• Strongly Disagree: 1 point • Disagree: 2 points • Neither agree nor disagree: 3 points • Agree: 4 points • Strongly Agree: 5 points 2. Calculate the SUS score:\n\n• X = Sum of the points for all odd-numbered questions -5. Odd -questions 1, 3, 5, 7, and 9\n\n• Y = 25 -Sum of the points for all even-numbered questions. Even -questions 2, 4, 6, 8, and 10\n\n• SUS score = (X + Y) * 2.5 3. Finally, E is calculated as the average of the SUS scores' experts.\n\nThe AVG of SUS scores' experts (E) can be evaluated under the following criteria:\n\nAlso, inspired by web application utility, we define the utility questionnaire based on this study  [46] . For all answers, we used the five-level Likert scale (1 Strongly disagree, 2 Disagree, 3 Neither agree nor disagree, 4 Agree, and 5 Strongly agree). Finally, two opened questions about the patients and web application.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussions",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cnn Models",
      "text": "The Table  I  presents the results of the computational efficiency of the models' architecture (NP, FLOPS, Mul-Add operations, and accuracy) and the trained model installed (RTE and MMU). Resmonet presented the smallest size in architecture, and therefore the minor number of NP and mult-add operations than IDNN, EDNN, 1.0 MobilNet, 0.75 MobileNet, and PeleeNet models. We consider that concatenation and convolutional blocks (dense blocks) could increase the number of mult-add operations in general. Also, ResmoNet obtained a 90% of accuracy, only the EDNN and the IDNN models surpassed our model by 5% and 2% correspondingly. In practice, shallower networks take less time being compiled in our testing portable device. Similarly, the prediction task is more efficient in shallower networks. Deeper networks tend to have a longer RTE and more MMU. For these reasons, ResmoNet presented the best result in MMU, and a difference of 0.01 seconds in RTE with the EDNN model.   The training graph contains the loss during the model training.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Fig. 8",
      "text": "The training graph contains the accuracy during the model training.\n\nThe confusion matrix of ResMoNet model on the testing set is shown in Figure  9 . It can be seen in the confusion matrix that neutral expression is the most difficult state to recognize correctly, and tends to be confused with a sadness state with 13%; also, surprise with fear can be confused with 11%.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fig. 9",
      "text": "Figure  9 . The confusion matrix of the testing data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Web Application",
      "text": "We obtained valuable information of five adult experts (M = 29.8 age, SD age =4.7 age), all women with more than 5 years of experience (M=11.6 years, SD=5.23 years), and all with a postgraduate degree (see Table  II ).\n\nWe consider that Years of Experience (YE) and the number of Patients Treated (PT) of two experts have a different impact on results, e.g., the qualitative data from an expert with 23 YE and 260 PT has a greater impact on usability and utility results than an expert with 6 YE and 21 PT. For this reason, as can be seen in Table  II , we calculated the YE pondered as the years of experience of an expert divided by the experts' total years of experience. Also, the PT pondered as the patients treated of an expert divided by the experts' total patients treated. Finally, YE and PT equally matter, therefore we calculate a weight (W) as the average of YE and PT pondered. The real impact on the expert score value is obtained by multiplying the W by the final usability or utility score, e.g., if we calculate W * AVG utility of the expert one, 0.40 * 3.5, the result is 1.39.\n\nTable  II",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limitations",
      "text": "Qualitative evaluation with experts considering its experience and treated patients is valuable for others psychologists and psychiatrists, also for mental health institutions. But the sample size of experts is no enough to test a hypothesis of interest.\n\nThe web application achieves real-time emotion recognition, even under the use of limited resources in devices. But it has a communication limitation, the voice IP module is not included in the current web application. Therefore, now, psychologists and psychiatrists must make a phone call to their patients.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusions",
      "text": "There is an imminent need for psychological and psychiatric intervention over society is emotionally affected by the circumstances of the COVID-19 pandemic. Our web application with Facial Emotion Recognition (FER) could support this need.\n\nWe presented the ResMoNet model for FER, which was mainly based on the depthwise separable convolutions presented in MobileNet and the residuality advantage proved in IDNN, EDNN, and PeleeNet, with a major adjustment in the Residual Block. The model architecture generates a smaller number of parameters and mult-add operations. Also, the trained model reduced the time to recognize emotion and the main memory utilization.\n\nA web application for psychologists and psychiatrists is proposed, which provides access to medical records, plots of patient emotions (anger, disgust, fear, happiness, sadness, surprise, and neutral), and a register of medical patient activity. Five experts evaluated the usability (73.8 of 100) and utility (3.94 of 5) of this tool as good.\n\nFor evidence-based emotional disorders strategies, this proposal for psychological support service can be integrated into the new platform as suggested by  [47] . Moreover, voice IP modules need to be considered in a platform with low data transfer.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Declarations",
      "text": "This work is supported by the FORDECYT 296737 project \"Consorcio en Inteligencia Artificial\" for the publication of this work. We declare no conflict of interests or competing interests. The psychiatrists and psychologists consented to participate in the experts' opinion questionnaires.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Appendix 1: Questionnaires",
      "text": "Usability questionnaire. 1. I think I can use this system frequently. 2. I think the system is unnecessarily complex.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "3.",
      "text": "I think the system is easy to use. 4.\n\nI think I could need help from technical staff to use the system. 5.\n\nI think many functions of the system are well integrated 6.\n\nI think there are many inconsistencies in the system 7.\n\nOther people could learn to use this system quickly 8. I think this system is uncomfortable to use 9.\n\nI feel so sure to use this system 10.\n\nI need to learn many things before I use this system Utility questionnaire: 11. I found the system helps to understand better the emotions of patients on the COVID-19 pandemic. 12.\n\nI think the system helps to record enough information from patients. 13. I found the system is useful for institutes of mental health on time of COVID-19 pandemic. 14. I think the system can be useful for other psychiatrists or psychologists. Opened questions:\n\n15.\n\nThe circumstances in times of the COVID-19 pandemic have affected the mental health of society. How many patients have you treated in the time of the COVID-19 pandemic? 16.\n\nSuggestions and comments to the web application.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Stem Block structure",
      "page": 4
    },
    {
      "caption": "Figure 2: The Transition Block is structured as follows: it contains a 2D convolution",
      "page": 4
    },
    {
      "caption": "Figure 2: Residual Block structure.",
      "page": 5
    },
    {
      "caption": "Figure 3: Overview of the ResMoNet model architecture.",
      "page": 5
    },
    {
      "caption": "Figure 4: Each step of the",
      "page": 6
    },
    {
      "caption": "Figure 4: A).  This process consists of the detection of ROIs",
      "page": 6
    },
    {
      "caption": "Figure 4: B). Once that the face is detected on the image, we",
      "page": 6
    },
    {
      "caption": "Figure 4: C). This process consists of resizing the ROI (cropped",
      "page": 6
    },
    {
      "caption": "Figure 4: (a) The face is detected from the original image. (b) The detected face is cropped into a new image. (c)",
      "page": 6
    },
    {
      "caption": "Figure 5: (a) The cropped face from the original image. (b) The sub-section of the center of the cropped image (up)",
      "page": 7
    },
    {
      "caption": "Figure 6: Web application. (a) a login page, which provides access to the psychologist or psychiatrist; (b) A",
      "page": 8
    },
    {
      "caption": "Figure 7: The training graph contains the loss during the model training.",
      "page": 10
    },
    {
      "caption": "Figure 8: The training graph contains the accuracy during the model training.",
      "page": 10
    },
    {
      "caption": "Figure 9: It can be seen in the",
      "page": 10
    },
    {
      "caption": "Figure 9: Figure 9. The confusion matrix of the testing data.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "IDNN",
          "NP": "16,158,790",
          "Mult-Add Ops.": "32,302,489",
          "Accuracy": "0.92",
          "RTE": "0.33 sec.",
          "MMU": "296.45 MB"
        },
        {
          "Model": "EDNN",
          "NP": "4,621,638",
          "Mult-Add Ops.": "9,235,929",
          "Accuracy": "0.95",
          "RTE": "0.15 sec.",
          "MMU": "245.86 MB"
        },
        {
          "Model": "1.0 MobileNet",
          "NP": "3,235,014",
          "Mult-Add Ops.": "6,481,263",
          "Accuracy": "0.55",
          "RTE": "1.17 sec.",
          "MMU": "282.32 MB"
        },
        {
          "Model": "0.75 MobileNet",
          "NP": "1,837,590",
          "Mult-Add Ops.": "3,683,679",
          "Accuracy": "0.48",
          "RTE": "0.99 sec.",
          "MMU": "274.43 MB"
        },
        {
          "Model": "PeleeNet",
          "NP": "2,123,502",
          "Mult-Add Ops.": "4,239,183",
          "Accuracy": "0.84",
          "RTE": "0.44 sec.",
          "MMU": "457.42 MB"
        },
        {
          "Model": "ResMoNet",
          "NP": "1,721,614",
          "Mult-Add Ops.": "3,439,778",
          "Accuracy": "0.90",
          "RTE": "0.16 sec.",
          "MMU": "235.62 MB"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Expert \n(E)": "E1",
          "Years of \nExperience \n(YE)": "23",
          "Age": "42",
          "Num. of \nPatients \ntreated \n(PT)": "260",
          "Education degrees": "Bsc. in Psychology, \nMs. in family \ntherapy.",
          "YE \npondered": "0.34",
          "PT \npondered": "0.46",
          "Weight \n(W)": "0.40"
        },
        {
          "Expert \n(E)": "E2",
          "Years of \nExperience \n(YE)": "21",
          "Age": "43",
          "Num. of \nPatients \ntreated \n(PT)": "86",
          "Education degrees": "Bsc. in Psychology, \nPhD. in psychology.",
          "YE \npondered": "0.31",
          "PT \npondered": "0.15",
          "Weight \n(W)": "0.23"
        },
        {
          "Expert \n(E)": "E3",
          "Years of \nExperience \n(YE)": "10",
          "Age": "45",
          "Num. of \nPatients \ntreated \n(PT)": "3",
          "Education degrees": "Bsc. in Psychology, \nMs. in learning \ndifficulties.",
          "YE \npondered": "0.15",
          "PT \npondered": "0.01",
          "Weight \n(W)": "0.08"
        },
        {
          "Expert \n(E)": "E4",
          "Years of \nExperience \n(YE)": "8",
          "Age": "37",
          "Num. of \nPatients \ntreated \n(PT)": "200",
          "Education degrees": "Bsc. in Psychiatry, \nSpecialty in \nPaidopsychiatry.",
          "YE \npondered": "0.12",
          "PT \npondered": "0.35",
          "Weight \n(W)": "0.23"
        },
        {
          "Expert \n(E)": "E5",
          "Years of \nExperience \n(YE)": "6",
          "Age": "32",
          "Num. of \nPatients \ntreated \n(PT)": "21",
          "Education degrees": "Bsc. in Psychology, \nMs. in Psychology.",
          "YE \npondered": "0.09",
          "PT \npondered": "0.04",
          "Weight \n(W)": "0.06"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "# Question": "1",
          "E1": "4",
          "E2": "4",
          "E3": "3",
          "E4": "5",
          "E5": "4"
        },
        {
          "# Question": "2",
          "E1": "3",
          "E2": "1",
          "E3": "1",
          "E4": "2",
          "E5": "3"
        },
        {
          "# Question": "3",
          "E1": "5",
          "E2": "2",
          "E3": "5",
          "E4": "4",
          "E5": "3"
        },
        {
          "# Question": "4",
          "E1": "2",
          "E2": "5",
          "E3": "2",
          "E4": "3",
          "E5": "2"
        },
        {
          "# Question": "5",
          "E1": "5",
          "E2": "5",
          "E3": "5",
          "E4": "5",
          "E5": "4"
        },
        {
          "# Question": "6",
          "E1": "3",
          "E2": "2",
          "E3": "1",
          "E4": "4",
          "E5": "3"
        },
        {
          "# Question": "7",
          "E1": "4",
          "E2": "5",
          "E3": "4",
          "E4": "5",
          "E5": "5"
        },
        {
          "# Question": "8",
          "E1": "2",
          "E2": "1",
          "E3": "1",
          "E4": "4",
          "E5": "1"
        },
        {
          "# Question": "9",
          "E1": "4",
          "E2": "4",
          "E3": "3",
          "E4": "5",
          "E5": "2"
        },
        {
          "# Question": "10",
          "E1": "2",
          "E2": "3",
          "E3": "2",
          "E4": "1",
          "E5": "3"
        },
        {
          "# Question": "11",
          "E1": "4",
          "E2": "5",
          "E3": "1",
          "E4": "4",
          "E5": "4"
        },
        {
          "# Question": "12",
          "E1": "2",
          "E2": "4",
          "E3": "4",
          "E4": "4",
          "E5": "4"
        },
        {
          "# Question": "13",
          "E1": "4",
          "E2": "5",
          "E3": "3",
          "E4": "4",
          "E5": "4"
        },
        {
          "# Question": "14",
          "E1": "4",
          "E2": "5",
          "E3": "3",
          "E4": "5",
          "E5": "4"
        },
        {
          "# Question": "",
          "E1": "17",
          "E2": "15",
          "E3": "15",
          "E4": "19",
          "E5": "13"
        },
        {
          "# Question": "",
          "E1": "13",
          "E2": "13",
          "E3": "18",
          "E4": "11",
          "E5": "13"
        },
        {
          "# Question": "",
          "E1": "75",
          "E2": "70",
          "E3": "82.5",
          "E4": "75",
          "E5": "65"
        },
        {
          "# Question": "",
          "E1": "29.79",
          "E2": "16.09",
          "E3": "6.28",
          "E4": "17.57",
          "E5": "4.07"
        },
        {
          "# Question": "",
          "E1": "3.5",
          "E2": "4.75",
          "E3": "2.75",
          "E4": "4.25",
          "E5": "4"
        },
        {
          "# Question": "",
          "E1": "1.39",
          "E2": "1.09",
          "E3": "0.21",
          "E4": "1.00",
          "E5": "0.25"
        },
        {
          "# Question": "",
          "E1": "73.8",
          "E2": "",
          "E3": "",
          "E4": "",
          "E5": ""
        },
        {
          "# Question": "",
          "E1": "3.94",
          "E2": "",
          "E3": "",
          "E4": "",
          "E5": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions revealed",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2004",
      "venue": "British Medical Journal Publishing Group"
    },
    {
      "citation_id": "2",
      "title": "Behavioral and Emotional Disorders in Children during the COVID-19 Epidemic",
      "authors": [
        "Wen Jiao",
        "Lin Yan",
        "Na Wang",
        "Juan Liu",
        "Shuan Feng Fang",
        "Yong Fu",
        "Massimo Jiao",
        "Eli Pettoello-Mantovani",
        "Somekh"
      ],
      "year": "2020",
      "venue": "The Journal of pediatrics"
    },
    {
      "citation_id": "3",
      "title": "The psychological impact of quarantine and how to reduce it: rapid review of the evidence",
      "authors": [
        "Samantha Brooks",
        "Rebecca Webster",
        "Louise Smith",
        "Lisa Woodland",
        "Simon Wessely",
        "Neil Greenberg",
        "Gideon James"
      ],
      "year": "2020",
      "venue": "The Lancet"
    },
    {
      "citation_id": "4",
      "title": "Basic and complex emotions in depression and anxiety",
      "authors": [
        "M Power",
        "M Tarsia"
      ],
      "year": "2007",
      "venue": "Clinical Psychology & Psychotherapy"
    },
    {
      "citation_id": "5",
      "title": "Impact of COVID-19 pandemic on mental health in the general population: A systematic review",
      "authors": [
        "Jiaqi Xiong",
        "Orly Lipsitz",
        "Flora Nasri",
        "M W Leanna",
        "Hartej Lui",
        "Lee Gill",
        "David Phan",
        "Chen-Li"
      ],
      "year": "2020",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "6",
      "title": "Emotions of COVID-19: Content Analysis of Self-Reported Information Using Artificial Intelligence",
      "authors": [
        "Achini Adikari",
        "Rashmika Nawaratne",
        "Daswin Silva",
        "Sajani Ranasinghe"
      ],
      "year": "2021",
      "venue": "J Med Internet Res",
      "doi": "10.2196/27341"
    },
    {
      "citation_id": "7",
      "title": "Impact of COVID-19 pandemic on the mental health of children in Bangladesh: A cross-sectional study",
      "authors": [
        "Sabina Yeasmin",
        "Rajon Banik",
        "Sorif Hossain"
      ],
      "year": "2020",
      "venue": "Children and youth services review"
    },
    {
      "citation_id": "8",
      "title": "COVID-19 pandemic and mental health consequences: Systematic review of the current evidence",
      "authors": [
        "Nina Vindegaard",
        "Michael Benros"
      ],
      "year": "2020",
      "venue": "Brain, behavior, and immunity"
    },
    {
      "citation_id": "9",
      "title": "What is an emotional disorder? A transdiagnostic mechanistic definition with implications for assessment, treatment, and prevention",
      "authors": [
        "Jacqueline Bullis",
        "Hannah Boettcher",
        "Shannon Sauer-Zavala",
        "Todd Farchione",
        "David Barlow"
      ],
      "year": "2019",
      "venue": "Clinical Psychology: Science and Practice"
    },
    {
      "citation_id": "10",
      "title": "Universal facial expressions of emotion",
      "authors": [
        "Paul Ekman",
        "Dacher Keltner"
      ],
      "year": "1997",
      "venue": "Nonverbal communication: Where nature meets culture"
    },
    {
      "citation_id": "11",
      "title": "En hogares",
      "authors": [
        "Inegi"
      ],
      "year": "2019",
      "venue": "En hogares"
    },
    {
      "citation_id": "12",
      "title": "Deep neural network concepts for background subtraction: A systematic review and comparative evaluation",
      "authors": [
        "Bouwmans",
        "Sajid Thierry",
        "Maryam Javed",
        "Soon Sultana",
        "Jung Ki"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "13",
      "title": "Generative adversarial nets",
      "authors": [
        "Ian Goodfellow",
        "Jean Pouget-Abadie",
        "Mehdi Mirza",
        "Bing Xu",
        "David Warde-Farley",
        "Sherjil Ozair",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Improved techniques for training gans",
      "authors": [
        "Tim Salimans",
        "Ian Goodfellow",
        "Wojciech Zaremba",
        "Vicki Cheung",
        "Alec Radford",
        "Xi Chen"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "A brief review of facial emotion recognition based on visual information. sensors",
      "authors": [
        "Byoung Ko"
      ],
      "year": "2018",
      "venue": "A brief review of facial emotion recognition based on visual information. sensors"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Pelee: A real-time object detection system on mobile devices",
      "authors": [
        "Robert Wang",
        "Xiang Li",
        "Charles Ling"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "Deepak Jain",
        "Pourya Kumar",
        "Paramjit Shamsolmoali",
        "Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2019.01.008"
    },
    {
      "citation_id": "19",
      "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "authors": [
        "Andrew Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
    },
    {
      "citation_id": "20",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "21",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "Ali Mollahosseini",
        "David Chan",
        "Mohammad Mahoor"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "22",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "Sergey Ioffe",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "ICML'15: Proceedings of the 32nd International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Going deeper with convolutions",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Inception v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "Christian Szegedy",
        "Sergey Ioffe",
        "Vincent Vanhoucke",
        "Alexander Alemi"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Dsod: Learning deeply supervised object detectors from scratch",
      "authors": [
        "Zhiqiang Shen",
        "Zhuang Liu",
        "Jianguo Li",
        "Yu-Gang Jiang",
        "Yurong Chen",
        "Xiangyang Xue"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Ssd: Single shot multibox detector",
      "authors": [
        "Wei Liu",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Christian Szegedy",
        "Scott Reed",
        "Cheng-Yang Fu",
        "Alexander Berg"
      ],
      "year": "2016",
      "venue": "Ssd: Single shot multibox detector"
    },
    {
      "citation_id": "29",
      "title": "Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild",
      "authors": [
        "Mengyi Liu",
        "Ruiping Wang",
        "Shaoxin Li",
        "Shiguang Shan",
        "Zhiwu Huang",
        "Xilin Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction",
      "doi": "10.1145/2663204.2666274"
    },
    {
      "citation_id": "30",
      "title": "Hybrid deep neural networks for face emotion recognition",
      "authors": [
        "Neha Jain",
        "Shishir Kumar",
        "Amit Kumar",
        "Pourya Shamsolmoali",
        "Masoumeh Zareapoor"
      ],
      "year": "2018",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "31",
      "title": "Spatial--temporal recurrent neural network for emotion recognition",
      "authors": [
        "Tong Zhang",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Yang Li"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "32",
      "title": "Facial emotion recognition using minmax similarity classifier",
      "authors": [
        "Olga Krestinskaya",
        "Alex Pappachen"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)"
    },
    {
      "citation_id": "33",
      "title": "Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order",
      "authors": [
        "André Lopes",
        "Edilson Teixeira",
        "Alberto F De De Aguiar",
        "Thiago Souza",
        "Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2016.07.026"
    },
    {
      "citation_id": "34",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Au-aware deep networks for facial expression recognition",
      "authors": [
        "Mengyi Liu",
        "Shaoxin Li",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "36",
      "title": "Deeply learning deformable facial action parts model for dynamic expression analysis",
      "authors": [
        "Mengyi Liu",
        "Shaoxin Li",
        "Shiguang Shan",
        "Ruiping Wang",
        "Xilin Chen"
      ],
      "year": "2014",
      "venue": "Asian conference on computer vision"
    },
    {
      "citation_id": "37",
      "title": "Facial expression recognition using Lp-norm MKL multiclass-SVM. Machine Vision and Applications 26",
      "authors": [
        "Xiao Zhang",
        "Mohammad Mahoor",
        "S Mohammad"
      ],
      "year": "2015",
      "venue": "Facial expression recognition using Lp-norm MKL multiclass-SVM. Machine Vision and Applications 26"
    },
    {
      "citation_id": "38",
      "title": "Network in network",
      "authors": [
        "Min Lin",
        "Qiang Chen",
        "Shuicheng Yan"
      ],
      "year": "2013",
      "venue": "Network in network",
      "arxiv": "arXiv:1312.4400"
    },
    {
      "citation_id": "39",
      "title": "",
      "authors": [
        "François Chollet",
        "Others"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "40",
      "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems",
      "authors": [
        "Martín Abadi",
        "Ashish Agarwal",
        "Paul Barham",
        "Eugene Brevdo",
        "Zhifeng Chen",
        "Craig Citro",
        "Greg Corrado"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"
    },
    {
      "citation_id": "41",
      "title": "Presentation and validation of the Radboud Faces Database",
      "authors": [
        "Oliver Langner",
        "Ron Dotsch",
        "Gijsbert Bijlstra",
        "H J Daniel",
        "Skyler Wigboldus",
        "Ad Hawk",
        "Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699930903485076"
    },
    {
      "citation_id": "42",
      "title": "Methods for Assessing Still Image Compression Efficiency",
      "authors": [
        "Dinu Dragan",
        "B Veljko",
        "Dragan Petrovic",
        "Ivetic"
      ],
      "year": "2016",
      "venue": "Handbook of Research on Computational Simulation and Modeling in Engineering",
      "doi": "10.4018/978-1-4666-8823-0.ch013"
    },
    {
      "citation_id": "43",
      "title": "Encyclopedia of Information Science and Technology",
      "authors": [
        "Khosrow-Pour",
        "Mehdi"
      ],
      "year": "2015",
      "venue": "Encyclopedia of Information Science and Technology",
      "doi": "10.4018/978-1-4666-5888-2"
    },
    {
      "citation_id": "44",
      "title": "Encyclopedia of Information Science and Technology",
      "authors": [
        "Mehdi Khosrow-Pour"
      ],
      "year": "2018",
      "venue": "Encyclopedia of Information Science and Technology",
      "doi": "10.4018/978-1-5225-2255-3"
    },
    {
      "citation_id": "45",
      "title": "SUS: a quick and dirty usability scale",
      "authors": [
        "J Brooke",
        "P Weerdmesster B Jordan",
        "B Thomas",
        "I Mcclelland"
      ],
      "year": "1996",
      "venue": "SUS: a quick and dirty usability scale"
    },
    {
      "citation_id": "46",
      "title": "A Mobile Web App to Improve Health Screening Uptake in Men (ScreenMen): Utility and Usability Evaluation Study",
      "authors": [
        "Chin Teo",
        "Hai",
        "Jenn Chirk",
        "Sin Kuang Ng",
        "Chip Lo",
        "Alan Lim",
        "White"
      ],
      "year": "2019",
      "venue": "JMIR mHealth and uHealth",
      "doi": "10.2196/10216"
    },
    {
      "citation_id": "47",
      "title": "Digital Solutions to Alleviate the Burden on Health Systems During a Public Health Care Crisis: COVID-19 as an Opportunity",
      "authors": [
        "Sofie Willems",
        "Jyotsna Rao",
        "Sailee Bhambere",
        "Dipu Patel",
        "Yvonne Biggins",
        "Jessica Guite"
      ],
      "year": "2021",
      "venue": "JMIR mHealth and uHealth",
      "doi": "10.2196/25021"
    }
  ]
}