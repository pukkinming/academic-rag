{
  "paper_id": "2505.22133v2",
  "title": "Developing A Top-Tier Framework In Naturalistic Conditions Challenge For Categorized Emotion Prediction: From Speech Foundation Models And Learning Objective To Data Augmentation And Engineering Choices",
  "published": "2025-05-28T08:58:22Z",
  "authors": [
    "Tiantian Feng",
    "Thanathai Lertpetchpun",
    "Dani Byrd",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Speech emotion recognition",
    "speech foundation model",
    "data augmentation",
    "affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER), particularly for naturally expressed emotions, remains a challenging computational task. Key challenges include the inherent subjectivity in emotion annotation and the imbalanced distribution of emotion labels in datasets. This paper introduces the SAILER system developed for participation in the INTERSPEECH 2025 Emotion Recognition Challenge (Task 1). The challenge dataset, which contains natural emotional speech from podcasts, serves as a valuable resource for studying imbalanced and subjective emotion annotations. Our system is designed to be simple, reproducible, and effective, highlighting critical choices in modeling, learning objectives, data augmentation, and engineering choices. Results show that even a single system (without ensembling) can outperform more than 95% of the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of three systems further improves performance, achieving a competitively ranked score (top-3 performing team). Our model is at: https://github.com/tiantiaf0627/vox-profile-release.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Despite notable advances in speech emotion recognition (SER) driven by speech foundation models  [1, 2] , accurately recognizing emotions from speech remains a challenging computational problem in machine learning  [3] . Key challenges include inherent ambiguity in emotion expressions  [4]  and subjectivity in annotating emotions  [5, 6] , imbalanced distributions in emotion labels, and complexity of speaker (and listener) environments and contexts. For example, the previous Odyssey-Speech Emotion Challenge reports that the top-performing team achieved only around 0.35 macro-F1 score in an 8-emotion classification problem  [7, 8] . Previous approaches have commonly employed techniques such as focal loss to mitigate class imbalance  [9, 8] . Moreover, a universally deployed approach is used to leverage recently developed speech foundation models, which have shown promising improvements in SER performance  [7] .\n\nThis paper introduces a promising solution to Task 1 of the IS25-Speech Emotion Recognition in Naturalistic Conditions Challenge (which we refer to as the IS25-SER Challenge)  [10] . Apart from building on the success of the past Odyssey-SER Challenge, we identified several elements to target for improvement in this IS25-SER challenge 1  [7] . First, the predominant approach to modeling SER relies on hard labeling (one-hot encoding), often ignoring the complete array of annotations avail-*indicates equal contribution 1 https://lab-msp.com/MSP-Podcast Competition/IS2025/ able in the dataset. This limitation leads many approaches to discard samples lacking annotation agreement during training, resulting in significant data loss. Second, data augmentation, a simple technique for diversifying input data, has rarely been explored to mitigate data imbalance in previous challenges. Finally, many top-performing teams opt to ensemble five or more models in their final solutions, making reproducibility practically unfriendly for the research community. Therefore, we focus on designing a reproducibility-friendly solution with minimum complexity and choice of hyperparameters.\n\nWe introduce SAIL-Emotion Recognition (SAILER), a SER framework that systematically explores several key design considerations for effective SER modeling. SAILER addresses various aspects of SER, including modeling choice, learning objectives, data augmentation, and engineering design choices to tackle class imbalances. Our design philosophy prioritizes simplicity, reproducibility, and efficiency, avoiding over-engineered solutions, unnecessary system complexity, and bulky ensembles. We aim for the community to replicate our system with minimal time and effort beyond the challenge. Experimental results show that a single system (without any ensemble) can already achieve the top-tier ranks in the leaderboard, a macro-F1 above 0.4 among over 150 submissions. Furthermore, an ensemble of three systems improves the macro-F1 score above 0.41 (outperforming 95% submissions).\n\nOur implementation, along with the best-performing single system, will be publicly released and could be considered as baselines for future research. The key design concepts and findings are summarized below: • A simple concatenation of output from speech and text foundation models is sufficient to achieve top-tier scores. • Modeling emotion distribution is more effective than using cross-entropy loss by utilizing more data in the training • Simple but novel data augmentation techniques, such as audio mixing and annotation dropout, further improve SER performance, especially for predicting minority emotion classes. • Engineering choices, including distribution reweighting, integrating additional validation metrics for minority emotion classes, and predicting other emotion attribute labels, are benefit SER performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Foundation Model",
      "text": "The use of speech foundation models, such as Whisper  [11]  and WavLM  [12] , have demonstrated effectiveness in SER. Many recent studies  [13, 14]  show that simply leveraging pre-trained speech representations is adequate to achieve competitive performance compared to traditional hand-crafted features. Fur- thermore, our literature review indicates that downstream architecture has a minimum impact on SER performance. Consequently, the SAILER system adopts the simple downstream model described in  [15] . Specifically, the model processes the last hidden output or a weighted average of all hidden outputs from the encoder layers. This representation is passed through a 3-layer pointwise convolutional module, followed by temporal averaging. Finally, the averaged output is fed into a two-layer MLP with ReLU activation functions in between.\n\nIn addition to speech modeling, prior studies have identified that textual information benefits SER  [16, 14] . We process the transcript using pre-trained text models to integrate text modeling into the system. Like speech modeling, we apply a weighted average to all encoder outputs, then the temporal averaging. The averaged text output is then concatenated with the averaged speech output to create the multimodal output, which is subsequently passed through a two-layer MLP for classification. Our multimodal architecture is presented in Fig.  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Learning Objective",
      "text": "Most prior studies approach SER as a hard-label classification problem. However, speech often conveys multiple emotions simultaneously  [4] , and majority-vote hardlabeling fails to capture these multifaceted characteristics  [17] . Moreover, as described in prior research  [18] , hard labeling can lead to a substantial data loss in training due to the ambiguity of emotions in some speech samples. For example, in the IS25-SER Challenge, 15,932 training speech samples lacked consensus, accounting for around 19% of the training data. To better represent the complexity of emotions expressed from the speech and use every speech sample in the dataset, SAILER frames SER as a soft labeling problem, as shown in Fig.  2 . Therefore, instead of relying on one-hot classification, we adopt distribution modeling and use KL divergence loss as the learning objective.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "The modeling choice and learning objective are relatively straightforward to identify in the literature, but determining an effective data augmentation strategy for SER remains challenging. One primary issue in the IS25-SER challenge is the imbalanced training data distribution. Implementing an effective data augmentation method could potentially increase the SER performance by a large margin. Here, we define majority classes as neutral, happy, sad, and angry, while the remaining emotions are minority classes. We introduce two novel data augmentations to effectively address the data imbalance issue: annotation dropout and audio mixing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hard Labeling",
      "text": "Annotation Dropout. Our first data augmentation technique is annotation dropout. The key observation we have in (speech) emotion labeling is that the oracle emotion distribution of a speech sample is difficult to obtain in practice. While each sample in the challenge dataset has at least five annotations, this is still insufficient for estimating its precise emotion distribution d. As a result, simple aggregation of annotations introduces inherent noise and biases. To mitigate the noise or biases in this scenario, we randomly drop 20% of the annotations for each speech sample during training to increase the robustness of the model to a slight distribution shift from d to d. Given the highly skewed label distribution of the training data, we specifically drop annotations from the majority classes. However, we want to highlight that a more systematic approach would be to drop annotations based on the empirical emotion distributions.\n\nAudio Mixing. Our second data augmentation technique involves mixing different audio samples in the training dataset to mitigate data imbalance. Specifically, we aim to mix speech samples from majority classes with those from minority classes.\n\nFor each majority-class speech sample xmaj, we apply this augmentation with a probability of pa. We then select a minorityclass speech sample, xmin, based on an inverse empirical distribution, meaning higher sampling probabilities to less frequently occurring emotion classes. To introduce variability, we randomly determine the order of xmaj and xmin when mixing. Additionally, we introduce further variability by deciding, via a coin flip, whether to add silence between the samples or create overlapping segments. Finally, we sample a time value t ∈ [0, 2] to determine the duration of the silence or overlap.\n\nGiven the emotion distribution of dmaj and dmin, the emotion distribution for the mixed audio dmix = (dmaj + dmin)/2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Engineering Design Choices To Tackle Class Imbalance",
      "text": "In addition to the aforementioned design choices, we describe several simple engineering choices that we find helpful in tackling class imbalance in SER training.\n\nDistribution Re-weighting. The first choice is to re-weight the emotion distribution. We first estimate the training data's empirical emotion distribution q by aggregating emotion distributions from each training sample. Then, the weight w i for the i-th emotion is simply 1 q i . We subsequently normalize w to  Validation Metrics. We frequently find that, although a system achieves a decent overall performance as measured by macro-F1 scores, this may be largely driven by the majority classes.\n\nRelying solely on the overall macro-F1 scores could lead to worse performance on the test set where the emotion distribution is balanced. Therefore, it is critical to incorporate additional metrics that specifically assess the performance of minority classes. To address this, we introduce the average precision of minority class prediction as an additional validation metric.\n\nPredicting Additional Ground Truth. Given the additional ground-truth annotations for each speech sample, such as secondary emotions, arousal, valence, and dominance labels, it is worthwhile to explore a simple multi-task learning approach that simultaneously learns the primary emotion distribution along with other affective elements of speech.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset And Experiment",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "The IS2025 Emotion Recognition Challenge used the MSP-Podcast dataset v1.12  [19, 10] . The dataset consists of podcast data from the Internet, including spontaneous speech with nat-ural human emotion expressions. The dataset is annotated with different emotion attributes. The dataset consists of five subsets: the training, development, and three unique test sets. The IS2025 SER challenge uses the test-3 set as the test set where the ground-truth labels have not been made public. Our experiments used the entire training set, which includes nine primary emotion classes: neutral, happy, angry, disgust, sad, surprise, contempt, fear, and others. We also included samples with no agreement on emotion labeling.\n\nGiven that we confirmed with the challenge organizer that no specified rule limited the use of the dataset development set for training, we decided to also include the 'other' and 'noagreement' samples from the development set as training data in some later challenge submissions. While it may be trivial to improve the current system (by at least 1-2%) by including more of the remaining development samples, we choose not to do so in order to ensure a consistent finding. Detailed information on the training set and development set is reported in Table  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Details",
      "text": "While we acknowledge that hyperparameter tuning could improve our current system, our primary goal is to show the effectiveness of SAILER with minimal hyperparameter search. Therefore, all of our systems are trained with a learning rate of 0.0005 for 15 epochs. The filter size in the downstream convolutional module is consistently set to 256 across all experiments. We set the maximum of the speech input to 15 seconds and used a fixed seed for all experiments. We base these parameters on our previous work in  [15]  without significant modifications.\n\nWe evaluate two speech foundation models: WavLM Large and Whisper Large-V3. Following the challenge baseline, we fine-tune the pre-trained WavLM Large along with the downstream models, whereas for Whisper Large-V3, we fine-tune only the downstream models. For WavLM Large, we apply a weighted average over all encoder outputs, while for Whisper Large-V3, we use only the representations from the last layer. We use the RoBERTa-Large  [20]  as the pre-trained text model. The limited opportunities for test set submissions make it difficult to systematically report test performance; consequently, we primarily report validation results in most of our experiments. We find that training SER based on Whisper-Large V3 is highly efficient, requiring just 12GB of GPU memory and approximately 15 GPU hours.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Do Speech Foundation Models Impact Ser?",
      "text": "As suggested by  [14] , we first investigate whether the choice of speech foundation models impacts the SER performance. Specifically, we compare the performance of WavLM Large and Whisper-Large V3 under both speech-only and multimodal conditions that are trained using only the training set. Our results in Table  2  indicate that the selection of the pre-trained model has a notable impact on SER performance, with Whisper-Large V3 consistently outperforming WavLM Large. One plausible reason for this advantage is the larger dataset used to train Whisper-Large V3, which likely yields a more generalizable speech representation for downstream tasks. We wish to highlight that even a simple unimodal WavLM Large solution could yield a test macro-F1 score above 0.37, achieving the top 30 ranks (out of 166 submissions) in the leaderboard.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Does Data Augmentation Impact Ser?",
      "text": "Next, we explore the impact of data augmentation on SER performance, comparing results with and without augmentation. Specifically, we evaluate audio mixing, annotation dropout, and their combination within the Whisper-Large multimodal setup, as presented in Table  3 . We want to highlight that, in this experiment, we included speech samples with 'other' and 'no agreement' labels from the development set as training data. While the results show no improvement in overall accuracy, both augmentation techniques improve SER performance, as measured by the macro-F1 score. Additionally, the minority class average precision indicates that both augmentation techniques are beneficial in improving predictions for minority classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Does Predicting Additional Labels Impact Ser?",
      "text": "This section investigates whether multitask learning with additional emotion annotations, such as secondary emotions or attribute scores (e.g., arousal), benefits SER performance. We conduct our experiments using the same settings as in the data augmentation experiments, where audio mixing and annotation dropout are applied. The results indicate that incorporating additional prediction targets can improve the overall macro-F1 score. However, we observe a decline in performance for minority classes. It is worth noting that a single system that predicts both primary and secondary emotions, as shown in Table 5, achieves a top-15 ranking on the leaderboard. Although we have not yet tested all models listed in Table  5 , we anticipate that the best-performing single system might be sufficient 4.4. How many system ensembles are needed?\n\nFollowing our design philosophy of prioritizing simplicity and reproducibility while avoiding complex ensembles, we limit our implementation to a maximum of three systems. Encouragingly, we found that an ensemble of three systems is sufficient to achieve a ranked performance on the leaderboard. Specifically, our ensemble includes system-1, the unimodal WavLM-Large model; system-2, a multimodal Whisper-Large model with multitask learning of secondary emotions; system-3, a multimodal Whisper-Large model with multitask learning of all emotion attributes and secondary emotions. Moreover, we present a twosystem ensemble comprising system-1 and system-2 as a reference. While integrating additional systems may further improve SER performance, we leave this exploration for future work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "On Further Improvements",
      "text": "While our current system shows competitive performance, we highlight several promising and easy-to-prototype directions that researchers can explore for developing next-generation, state-of-the-art SER systems. One is to study pre-trained speech models with emotional speech data like Emotion2Vec  [21] .\n\nLearning Objective. While we use the KL-Divergence as the loss function, a straightforward approach to model the emotion distribution, there are other promising alternatives. For example, prior work by  [18]  demonstrated that soft cross-entropy consistently outperforms KL-Divergence loss.\n\nAudio Mixing. Prior work in  [22]  has explored similar ideas of mixing audio for SER. However, unlike our method, they mix speech samples with the same emotion to increase the confidence levels of the predictions. Thus, a promising research direction is systematically exploring audio mixing.\n\nPredicting Additional Ground Truth. Although SAILER considered modeling additional ground-truth labels, such as secondary emotions from the dataset, other speaker-specific information, like gender and age information, has not been explored in this current system. However, emotional expression can differ significantly across speaker groups. A straightforward way to improve our system could include sex and/or age prediction as an additional learning objective.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we describe the SAILER framework, a simple and reproducible-friendly SER model for categorized motion prediction in task 1 of the IS25-SER challenge. SAILER considers designs from speech foundation models and learning objectives to data augmentation and engineering choices to tackle imbalanced data. Experimental results show that SAILER is highly competitive in the IS25-SER challenge, achieving top-tier performance with minimum system complexity.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed multimodal SER framework employs",
      "page": 2
    },
    {
      "caption": "Figure 1: 2.2. Learning Objective",
      "page": 2
    },
    {
      "caption": "Figure 2: Therefore, in-",
      "page": 2
    },
    {
      "caption": "Figure 2: Our proposed soft-labeling approach. This leads to",
      "page": 2
    },
    {
      "caption": "Figure 3: Our proposed data augmentation technique to address data imbalance in SER modeling.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A": "1",
          "C": "0",
          "D": "0",
          "F": "0",
          "H": "0",
          "N": "0",
          "S": "0",
          "U": "0",
          "O": "0"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A": "0.6",
          "C": "0",
          "D": "0.2",
          "F": "0",
          "H": "0",
          "N": "0.2",
          "S": "0",
          "U": "0",
          "O": "0"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "2",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "An engineering view on emotions and speech: From analysis and predictive models to responsible human-centered applications",
      "authors": [
        "C.-C Lee",
        "T Chaspari",
        "E Provost",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "4",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "E Mower",
        "A Metallinou",
        "C.-C Lee",
        "A Kazemzadeh",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII), Amsterdam"
    },
    {
      "citation_id": "5",
      "title": "Balancing speakerrater fairness for gender-neutral speech emotion recognition",
      "authors": [
        "W.-S Chien",
        "S Upadhyay",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "People make mistakes: Obtaining accurate ground truth from continuous annotations of subjective constructs",
      "authors": [
        "B Booth",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "7",
      "title": "Odyssey 2024-speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Development"
    },
    {
      "citation_id": "8",
      "title": "1st place solution to odyssey emotion recognition challenge task1: Tackling class imbalance problem",
      "authors": [
        "M Chen",
        "H Zhang",
        "Y Li",
        "J Luo",
        "W Wu",
        "Z Ma",
        "P Bell",
        "C Lai",
        "J Reiss",
        "L Wang",
        "P Woodland",
        "X Chen",
        "H Phan",
        "T Hain"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "9",
      "title": "Double multi-head attention multimodal system for odyssey 2024 speech emotion recognition challenge",
      "authors": [
        "F Costa",
        "M India",
        "J Hernando"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "10",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I Ülgen",
        "T Thebaud",
        "L Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "11",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "14",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Peft-ser: On the use of parameter efficient transfer learning approaches for speech emotion recognition using pretrained speech models",
      "authors": [
        "T Feng"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "16",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "A framework for automatic human emotion classification using emotional profiles",
      "authors": [
        "E Mower",
        "M Mataric",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an allinclusive aggregation rule",
      "authors": [
        "H.-C Chou",
        "L Goncalves",
        "S.-G Leem",
        "A Salman",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "21",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "22",
      "title": "Emix: a data augmentation method for speech emotion recognition",
      "authors": [
        "A Dang",
        "T Vu",
        "J.-C Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}