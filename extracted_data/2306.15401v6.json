{
  "paper_id": "2306.15401v6",
  "title": "Explainable Multimodal Emotion Recognition",
  "published": "2023-06-27T11:54:57Z",
  "authors": [
    "Zheng Lian",
    "Haiyang Sun",
    "Licai Sun",
    "Hao Gu",
    "Zhuofan Wen",
    "Siyuan Zhang",
    "Shun Chen",
    "Mingyu Xu",
    "Ke Xu",
    "Kang Chen",
    "Lan Chen",
    "Shan Liang",
    "Ya Li",
    "Jiangyan Yi",
    "Bin Liu",
    "Jianhua Tao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition is an important research topic in artificial intelligence, whose main goal is to integrate multimodal clues to identify human emotional states. Current works generally assume accurate labels for benchmark datasets and focus on developing more effective architectures. However, emotion annotation relies on subjective judgment. To obtain more reliable labels, existing datasets usually restrict the label space to some basic categories, then hire plenty of annotators and use majority voting to select the most likely label. However, this process may result in some correct but non-candidate or non-majority labels being ignored. To ensure reliability without ignoring subtle emotions, we propose a new task called \"Explainable Multimodal Emotion Recognition (EMER)\". Unlike traditional emotion recognition, EMER takes a step further by providing explanations for these predictions. Through this task, we can extract relatively reliable labels since each label has a certain basis. Meanwhile, we borrow large language models (LLMs) to disambiguate unimodal clues and generate more complete multimodal explanations. From them, we can extract richer emotions in an open-vocabulary manner. This paper presents our initial attempt at this task, including introducing a new dataset, establishing baselines, and defining evaluation metrics. In addition, EMER can serve as a benchmark task to evaluate the audio-video-text understanding performance of multimodal LLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition has experienced rapid development in recent years  [1, 2] . Current works predominantly revolve around two aspects: the collection of larger and more realistic datasets  [3, 4]  and the development of more effective architectures  [5, 6] . Despite promising progress, emotion recognition suffers from label ambiguity  [7] . It arises due to the inherent subjectivity in the emotion annotation process, i.e., different annotators may assign distinct labels to the same video. Label ambiguity results in potentially unreliable labels of existing datasets, bringing obstacles to the systems developed on these datasets to meet requirements in practical applications.\n\nTo enhance label reliability, current works mainly focus on restricting the label space to reduce the annotation diversity  [8, 9] , while increasing the number of annotators and using majority voting to determine the most likely label  [10, 11] . However, this approach may exclude correct but noncandidate or non-dominant labels, resulting in inaccurate annotations.\n\nOne-hot label: surprise EMER description: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment. OV labels extracted from the EMER description: surprise, nervous, dissatisfied Figure  1 : One example (\"sample_00000669\") to illustrate the differences between the one-hot label, EMER description, and EMER-based open vocabulary labels.\n\nTo obtain reliable labels but not ignore subtle ones, we introduce a new task called \"Explainable Multimodal Emotion Recognition (EMER)\". Unlike traditional emotion prediction, EMER goes a step further and provides explanations for these predictions. In this way, the identified labels are more reliable because there is a corresponding basis. Meanwhile, with the reasoning capability of large language models (LLMs), we can disambiguate unimodal clues and generate more comprehensive multimodal descriptions with rich emotion categories.\n\nAnother motivation behind EMER is that emotions are related to multi-faceted clues, such as prosody  [12] , facial expressions  [13]  (or micro-expressions  [14] ), gestures  [15]  (or micro-gestures  [16] ), etc. Current works generally identify emotions from one or several aspects. Unlike existing works, EMER provides a common format for emotion-related tasks, aiming to integrate all clues to generate more accurate labels. Meanwhile, emotions are complex. Current datasets limit the label space to a few categories, causing annotators being unable to describe emotional states accurately. Differently, EMER does not limit the label space and can generate richer labels in an open-vocabulary manner. This paper proposes a new task EMER, aiming to achieve more reliable and accurate emotion recognition technology. To facilitate further research, we establish a new dataset, baselines, and evaluation metrics. Figure  1  shows the differences between the traditional one-hot label, EMER description, and EMER-based open vocabulary (OV) labels. We observe that more accurate labels can be extracted in this way. In addition to surprise, we can also extract nervous and dissatisfied. The main contributions of this paper can be summarized as follows:\n\n• This paper introduces the EMER task for reliable and accurate emotion recognition. On the one hand, it provides the evidence and reasoning process for identified emotions. On the other hand, it can integrate all emotion-related clues to generate more accurate labels.\n\n• To facilitate further research, we construct a dataset, establish baselines, and define evaluation metrics. Meanwhile, we will open-source the code and intermediate results.\n\n• Besides emotion recognition, EMER can serve as a benchmark task to evaluate the audiotext-video understanding ability of multimodal LLMs (MLLMs).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Emotion Recognition. Multimodal emotion recognition aims to integrate multimodal clues to identify emotions. Unlike other tasks with clearly defined categories (such as object or action recognition), emotions are relatively ambiguous. Especially in multimodal scenarios, emotions are more complex  [8]  and there may be a modality repulsion problem  [4]  (i.e., different modalities may convey distinct emotions). To improve the annotation consistency, previous works often restricted the label space and used majority voting to determine the most likely label  [9, 10] . For example, Lian et al.  [9]  employed at least six annotators and used multi-stage checks to select samples with explicit emotions. Li et al.  [10]  labeled each sample about 40 times and used the EM algorithm to filter out unreliable labels. Although these works enhance the label reliability, some correct but non-majority or non-candidate labels may be ignored. This paper introduces a new task, EMER, which provides a pathway to recognizing emotions in an open-vocabulary manner. With this task, we aim to generate more accurate labels for each sample. To the best of our knowledge, this is the first attempt to address emotion recognition in this manner.\n\nOpen Vocabulary Learning. Open vocabulary learning aims to identify categories beyond the annotated label space  [17] . It has been widely used in various tasks and domains, including object detection  [18, 19] , segmentation  [20, 21] , and scene understanding  [21, 22] . For example, the object detection dataset COCO  [23]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Construction",
      "text": "We build our dataset based on MER2023  [9] , a widely used corpus in multimodal emotion recognition.\n\nDuring the annotation process, we need to annotate multi-faceted clues, which requires a lot of manual effort. To reduce costs, we select 332 samples from MER2023 for annotation. In the future, we will explore ways to reduce costs and expand the dataset size. In this section, we introduce the data annotation process and analyze the multi-faceted capabilities of the annotated results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Annotation",
      "text": "We have some basic findings during the annotation process: Video subtitle is generally short, colloquial, and has vague emotional expressions. But by combining visual and acoustic clues, we can disambiguate the subtitle and generate more accurate descriptions. Therefore, we mainly annotate visual and acoustic clues and then use LLMs for disambiguation. Figure  2  shows the pipeline of data annotation and Table  1  provides prompts involved in this process. Additionally, we provide an example to visualize the output of each step (see Appendix A).\n\nPre-labeling. Initially, we attempt to annotate visual and acoustic clues directly. However, the description obtained in this way is generally short and cannot cover all clues. Therefore, we use GPT-4V to generate initial annotations. Considering that GPT-4V does not support videos but only images, we sample the video and use the prompt (see #1 in Table  1 ) to extract visual clues. To get acoustic clues, we try converting the audio to a mel-spectrogram, but GPT-4V fails to generate proper responses on the mel-spectrogram. Considering that the subtitle in audio also contains emotion-related clues, we use the prompt (see #2 in Table  1 ), and its output is treated as the acoustic clues.\n\nTwo-round Checks. During the proofreading process, we find some errors in the pre-labeled visual and acoustic clues. For visual clues, GPT-4V may produce hallucinatory responses, i.e., it may contain some clues that do not exist. For acoustic clues, the textual content is usually brief and colloquial. Without incorporating multimodal information, the clue merely based on the textual As an expert in the field of emotions, please focus on facial expressions, body language, environmental cues, and events in the video and predict the emotional state of the character. Please ignore the character's identity. We uniformly sample 3 frames from this video. Please consider the temporal relationship between these frames and provide a complete description of this video. Avoid using descriptions like \"the first image\" and \"the second image\", and instead use terms like \"beginning\", \"middle\", and \"end\" to denote the progression of time.\n\n#2 Pre-label Acoustic Clue\n\nPlease assume the role of an expert in the field of emotions. We have a piece of text. Please analyze which parts of it can be used to infer the emotional states of the characters, and provide reasoning for your inference.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "#3 Disambiguate",
      "text": "Please assume the role of an expert in the field of emotions. We provide audio and video cues that may be related to the emotions of the characters. Additionally, we provide the original subtitle of the video. Please analyze which parts of the subtitle can be used to infer the emotional states of the characters and provide reasoning for your inference. In the process of inference, please integrate the audio and video cues for analysis.\n\ncontent may be incorrect. Additionally, there are repeated expressions and some key clues are missing.\n\nTo obtain more reliable clues, we conduct two rounds of manual checks.\n\nDisambiguation. To obtain lexical clues, we use the checked acoustic and visual clues to disambiguate the subtitle (see Figure  2 ). In this process, we rely on GPT-3.5 and use the #3 prompt in Table  1 . With its powerful reasoning ability, we can generate accurate lexical clues. Finally, we combine all clues and generate multimodal descriptions. These descriptions are noted as EMER(Multi).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Analysis",
      "text": "EMER(Multi) contains multi-modal emotion-related clues. From them, we can extract multi-faceted results, including visual clues, discrete emotions, valence scores, and open-vocabulary emotion labels.\n\nTo realize these functions, we rely on GPT-3.5 and use the prompts in Table  2 .\n\nVisual Clue Analysis. EMER(Multi) contains a variety of visual clues. In this section, we provide a statistical analysis of the number of visual clues. To extract visual clues, we use the prompt #1 in Table  2 . Experimental results demonstrate that each sample has an average of 4.95 visual clues, suggesting that EMER(Multi) contains rich clues for emotion recognition.\n\nDiscrete Emotion Recognition. Then, we attempt to reveal whether discrete emotions can be identified from EMER(Multi). Considering that our dataset is based on MER2023, which provides relatively accurate discrete labels, we treat its label as the ground truth. To identify emotions from EMER(Multi), we use the #2 prompt in Table  2  and restrict the label space to be consistent with MER2023. Experimental results show that the Top-1 and Top-2 accuracy can reach 93.48 and 96.89, respectively. Through further analysis, these errors are mainly caused by inaccurate labels in MER2023 or ranking errors of GPT-3.5. Therefore, we can conclude that EMER(Multi) contains clues for discrete emotion recognition.\n\nValence Estimation. In addition to discrete emotion recognition, we also validate the valence estimation results based on EMER(Multi). Considering that MER2023 provides relatively accurate valence scores, we treat its label as the ground truth. To estimate the valence from EMER(Multi), we use the #3 prompt in Table  2  and set the score range -5∼5, consistent with the range in MER2023. Then, we calculate the PCC value between MER2023-based and EMER-based valence scores.\n\nExperimental results show that their PCC can reach 0.88, a relatively high level, indicating that EMER(Multi) also contains clues for valence estimation.\n\nOV Emotion Recognition. One of the main purposes behind EMER is to obtain richer emotions. Therefore, we extract all emotion labels from EMER(Multi) using the #4 prompt in Table  2 . In this process, we do not restrict the label space and predict emotions in an open-vocabulary manner. We perform a statistical analysis on the number of extracted labels. There are a total of 301 candidate labels, far more than 6 candidate labels in MER2023. Meanwhile, each sample has an average of 3 labels. Therefore, EMER(Multi) contains richer emotion labels than previous datasets. Please assume the role of an expert in the field of emotions. We provide clues related to the emotions of the characters in the video. Please output the facial movements and body gestures involved in the description, separated by commas. The output format should be in list form.\n\n#2 Discrete Emotion Rec.\n\nPlease assume the role of an expert in the emotional domain. We provide clues that may be related to the emotions of the character. Based on the provided clues, identify the emotional states of the main characters. We provide a set of emotional candidates, please rank them in order of likelihood from high to low. The candidate set is {Candidate Labels}.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "#3 Valence Estimation",
      "text": "As an expert in the emotional domain, we provide clues that may be related to the emotions of characters. Based on the provided clues, please identify the overall positive or negative emotional polarity of the main characters. The output should be a floating-point number ranging from -5 to +5. Here, - 4 Experimental Setup",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "Considering that MLLMs can address various multimodal tasks, we attempt to use them to solve EMER. Since emotion recognition relies on temporal information, we choose MLLMs that support at least video or audio. Appendix B provides model cards of MLLMs involved in this paper. To build MLLMs, a mainstream idea is to align pre-trained models of other modalities to LLMs. For example, VideoChat  [24]  uses Q-Former  [25]  to map visual queries into textual embedding space. SALMONN  [26]  proposes a window-level Q-Former to align speech and audio encoders with LLMs.\n\nAfter instruction fine-tuning, MLLMs can understand instructions and multimodal inputs.\n\nTo generate EMER-like descriptions using MLLMs, we first use the prompt in Appendix B (the prompt without subtitles) and its output is denoted C. Considering that the subtitle contains important clues for emotion recognition, we follow the disambiguation process in Figure  2  and use the clue C to disambiguate the subtitle. For a fair comparison, we use similar prompts for audio, video, and audio-video LLMs. In Section 5, we further investigate the role of subtitles and discuss different ways to integrate them. Please refer to the corresponding section for more details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "One of the main purposes of EMER is to identify richer emotion labels. In this paper, we use the overlap rate between the predicted and annotated label sets as the evaluation metric. In addition, we also calculate some matching-based metrics, including BLEU 1 , BLEU 4 , METEOR, and ROUGE l .\n\nEmotion Recognition. Since we do not fix the label space, MLLMs may generate synonyms, i.e., labels with different expressions but similar meanings (such as happy and joy). These synonyms affect the overlap rate between the annotated and predicted label sets. To reduce their impact, we first use GPT-3.5 to group all labels before metric calculation:\n\nPlease assume the role of an expert in the field of emotions. We provide a set of emotions. Please group the emotions, with each group containing emotions with the same meaning. Directly output the results. The output format should be a list containing multiple lists.\n\nAfter that, we get a function G(•) that can map each label to its group ID. Suppose {y i } M i=1 and {ŷ i } N i=1 are the annotated and predicted label sets respectively, where M and N are the number of labels. To reduce the impact of synonyms, we first map each label into its group ID:\n\nThen, we define the following two metrics:\n\nThese two metrics are similar to traditional precision and recall but are defined at the set level. Accuracy s denotes how many predicted labels are correct; Recall s indicates whether the predicted results cover all annotated labels. We use the average of these two metrics for the final ranking:\n\nWord-level Matching. Besides metrics for emotion recognition, we also calculate some typical metrics for natural language generation, including BLEU 1 , BLEU 4 , METEOR, and ROUGE l . The main purpose behind them is that emotion-based metrics require OpenAI API call costs. We try to analyze whether there is also a strong correlation between matching-based and emotion-based metrics. If so, we can only calculate matching-based metrics to reduce the evaluation costs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "This paper uses the closed-source GPT for dataset construction and metric calculation. In this process, GPT-3.5  [27]  (\"gpt-3.5-turbo-16k-0613\") and GPT-4V  [28]  (\"gpt-4-vision-preview\") perform similarly in plain text analysis. To reduce API call costs, we use GPT-3.5 for text processing and GPT-4V for image processing. We run all experiments twice and report the average score and standard deviation. For baseline MLLMs, we use their original 7B weights. All models are implemented with PyTorch and all inference processes are carried out with a 32G NVIDIA Tesla V100 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "EMER aims to achieve reliable and accurate emotion recognition. This paper mainly focuses on accuracy and the reliability analysis is left to our future work. In this section, we first reveal the impact of language on the evaluation metrics. Then, we report the performance of MLLMs and conduct ablation studies around modality influence and how to integrate subtitles. Finally, we reveal the relationship between one-hot and OV labels and visualize the correlation between different metrics.  Language Influence. The initial EMER dataset is in Chinese and we use GPT-3.5 to translate it into English. Therefore, our dataset has English and Chinese versions. In this section, we attempt to reveal the language influence on the evaluation metrics. As shown in Figure  3 , we first extract emotion labels Y EE and Y CC from the English and Chinese datasets, respectively. Then, we translate these labels into another language and get Y EC and Y CE . Referring to Section 4.2, we define a metric called overlap rate to measure label similarity. Specifically, assume {p 1 i } N1 i=1 and {p 2 i } N2 i=1 are two label sets and G(•) is the synonym mapping function. We first map each label to its group ID:\n\n} and calculate the following metric:\n\nA higher overlap rate indicates a higher degree of label similarity. From Figure  3 , we observe some interesting phenomena: 1) Translation reduces the overlap rate. For example, Y EE to Y EC results in a 0.15 decrease in the overlap rate. The main reason lies in the increased difficulty of label grouping, i.e., G(•), in the cross-language setting. In some cases, we find that the grouping process may be based on language type rather than label similarity.\n\n2) There are certain differences in labels extracted from descriptions in different languages. For example, under the same language setup, Y EE to Y CE results in a 0.18 decrease in the overlap rate.\n\nThe reason may lie in some differences in the definition of emotion in distinct languages. To obtain more accurate labels, we merge labels extracted from both languages and perform manual checks. These checked labels are regarded as ground truth, noted as Y gt .\n\n3) If we extract labels from descriptions in different languages and calculate overlap rates in a cross-language setup, it will cause the largest drop. For example, Y EE to Y CC (or Y EC to Y CE ) results in a reduction of 0.22 (or 0.27). These results further confirm the two findings mentioned above.\n\nMain Results. In this section, we report the emotion recognition results of different methods.\n\nBesides MLLMs, we introduce two heuristic baselines: Empty and Random. For the former, we predict each sample as \"unable to judge the emotional state\". For the latter, we randomly select a label from the MER2023's candidate set (i.e., worried, happy, neutral, angry, surprised, sad) and generate a description like \"through the video, we can judge the emotional state is {emotion}\". These two baselines reflect performance lower bounds. According to our previous findings, there are certain differences in labels extracted from descriptions in distinct languages. Therefore, we report results in both Chinese and English. Specifically, take Figure  3   Experimental results in Table  3  demonstrate that MLLMs outperform the heuristic baselines, indicating that MLLMs can address emotion recognition to some extent. However, there is still a significant performance gap between the predictions of MLLMs and the ground truth Y gt , which highlights the limitations of existing MLLMs and the difficulty of this task. Meanwhile, models that perform well in Chinese generally perform well in English. These results suggest that language differences have a limited impact on performance rankings.\n\nImpact of Modality. EMER(Multi) uses visual and acoustic clues to disambiguate subtitles and generate lexical clues. To study the impact of modality, we further generate three descriptions: EMER(Audio), EMER(Text), and EMER(Video). The generation process is shown in Figure  4 .  Specifically, for EMER(Audio), we only use the acoustic clue to disambiguate subtitles. For EMER(Text), we infer the emotional state from subtitles and use the #2 prompt in Table  1  to generate lexical clues. Meanwhile, we directly use the visual clue as EMER(Video).\n\nIn Table  3 , we observe that EMER(Multi) can achieve the best performance in emotion recognition. The reason lies in that emotions are conveyed through various modalities. Combining all clues can realize more accurate emotion recognition. Meanwhile, EMER(Text) performs worst among the four descriptions. This also validates our basic principle in dataset construction (see Section 3.1). That is, the subtitle is relatively vague. However, by incorporating clues from other modalities, we can disambiguate the subtitle and generate more accurate lexical clues. Furthermore, we observe that EMER(Audio) performs better than EMER(Video). The reason lies in the samples in our dataset focusing more on audio to convey emotions, which is consistent with previous findings  [2] .\n\nImpact of Subtitles. To generate EMER-like descriptions using MLLMs, this paper uses the prompts without subtitles to extract clues and then exploits the extracted clues to disambiguate subtitles, noted as \"S2\". In this section, we reveal the impact of different ways to integrate subtitles and introduce two additional baselines: 1) S0 uses the prompts without subtitles to generate descriptions; 2) S1 uses the prompts with subtitles to generate descriptions. More details about these prompts can be found in Appendix B. Specifically, S2 is equivalent to first using S0 to extract clues and then using these clues to disambiguate subtitles. This disambiguation process relies on GPT-3.5. Compared with S2, S1 merges two steps in one, taking into account subtitles and other clues simultaneously.\n\nFigure  5  shows the emotion recognition results of different strategies. More results can be found in Appendix C. From these figures, we observe that S1 and S2 generally perform better than S0. These results demonstrate the importance of subtitles in emotion recognition. Meanwhile, S2 generally outperforms S1. The reason lies in that including subtitles in the prompt makes the prompt more complex. However, current open-source MLLMs may have difficulty understanding complex prompts, resulting in limited performance. By separating this process into two steps, S2 can reduce the task difficulty and achieve better performance. These results also demonstrate the rationality of this paper using S2 as the default strategy to integrate subtitles.\n\nOne-hot vs. OV Labels. This section reveals the relationship between MER2023-based one-hot labels and OV labels Y gt . In Table  4 , we observe that one-hot labels have relatively high accuracy but relatively low recall. These results show that the one-hot labels provided by MER2023 are generally correct. However, they cannot cover all emotions due to the limited label space and the limited number of labels. Meanwhile, these results demonstrate the necessity of using avg for the final ranking, which ensures the generation of more accurate and comprehensive labels.  Metric Correlation Analysis. In Table  3 , we observe that EMER(Multi) achieves the best performance in emotion recognition. Therefore, a conjecture arises: whether descriptions \"closer\" to EMER(Multi) lead to better emotion recognition performance. The most common ways to measure the \"closeness\" between two sentences are matching-based metrics, such as BLEU 1 , BLEU 4 , ME-TEOR, and ROUGE l . Therefore, in this section, we reveal the correlation between two types of metrics: emotionbased and matching-based metrics.\n\nFigure  6  shows the PCC scores between different metrics.\n\nIn this figure, we report the results in English. In Appendix D, we provide results of other languages, as well as raw scores for matching-based metrics. From these results, we observe relatively high correlations within emotionbased (or matching-based) metrics. However, the correlation between these metrics is relatively low. Therefore, there are certain differences between them.",
      "page_start": 6,
      "page_end": 9
    },
    {
      "section_name": "Limitations And Societal Impacts",
      "text": "This paper proposes a new task EMER. Due to the high annotation cost, our initial dataset contains 332 samples. In the future, we will explore ways to reduce the cost and expand the dataset size. Meanwhile, we evaluate some typical MLLMs but do not cover all models. In the future, we will expand the evaluation scope. In addition, EMER aims to achieve reliable and accurate emotion recognition. This paper mainly focuses on accuracy. In the future, we will define more metrics and evaluate different MLLMs from the reliability perspective. Moreover, this paper focuses on the problem definition, dataset construction, metric definition, and evaluation. In the future, we will design more effective frameworks to solve this challenging task.\n\nEmotion recognition technology has a positive impact on the development of human-computer interaction. However, machines that are too human-like may affect social stability and cause panic. Therefore, we need to monitor the development of this technology, although the current systems are still some distance away from fully human-like systems.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a new task, EMER. Unlike traditional emotion recognition, EMER requires further evidence to support the prediction results. By introducing this task, we aim to improve the reliability and accuracy of emotion recognition technology. To facilitate further research, we construct an initial dataset, develop baselines, and define evaluation metrics. Then, we use MLLMs as the baselines to solve this task and evaluate their performance. Experimental results demonstrate that MLLMs struggle to achieve satisfactory results, indicating the difficulty of this task. Meanwhile, we systematically analyze the impact of language, modality, and subtitle integration strategy. We also reveal the correlations between different metrics. In summary, EMER contains multi-faceted multi-modal clues and can serve as a general format for emotion-related tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Example For Data Annotation",
      "text": "Our data annotation process involves three key steps: pre-labeling, two-round checks, and disambiguation. In this section, we provide an example to visualize the output of each step. From the generated EMER description, we can extract richer emotions in an open-vocabulary manner.\n\nPre-labeled visual clues: In the opening scene, we see a man wearing a white lab coat. He looks confident, with a slight smile on his lips and eyes directly facing the camera, as if he is explaining or narrating something. The background features computers and office equipment, suggesting that this might be an office or laboratory setting. As the scene progresses to the middle part, the man's head turns slightly to one side, and his gaze shifts sideways, seemingly interacting with someone next to him. His smile broadens, and his eyebrows lift slightly, indicating that he might be having a friendly conversation or sharing a light-hearted moment. In the next scene, the man's expression becomes more exaggerated; his mouth opens wide as if expressing surprise or emphasizing a point. This kind of facial expression and body language is typically associated with sharing surprising news or reacting strongly to something. In the final scene, he returns to facing the camera directly, his expression calm once again with a slight smile, giving the impression that he might have just finished explaining or narrating something and feels satisfied or confident. Overall, based on these scenes, we can infer that the man in the video is likely conducting some sort of demonstration or explanation. He interacts with the audience, shares information, and possibly expresses surprise at some point. His emotional state throughout the video appears to be positive, exhibiting confidence and a friendly demeanor. Pre-labeled acoustic clues: From this text, it can be inferred that the character's emotional state is one of dissatisfaction or anger. The reasoning is as follows: 1. \"Why are you all looking at me like that?\" This question expresses the character's dissatisfaction with being stared at. It implies that the character feels uncomfortable or displeased with others watching him. 2. \"So, as long as it's a woman, does she have to have a relationship with me?\" This statement conveys the character's frustration with others' expectations or prejudices. The character feels annoyed or angry about the assumption that there should be a special relationship between him and a woman simply because she is female. In summary, the text suggests that the character is experiencing emotions of dissatisfaction or anger. Checked visual clues: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. Checked acoustic clues: In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. EMER description: In the video, the screen shows a male character in an indoor setting. At the beginning of the video, his eyes are wide open and his mouth is also open, indicating a surprised facial expression. In the following scenes, he looks around, seemingly explaining or narrating something to the people around him. Overall, his emotions are not positive or optimistic. In the audio, the character speaks with a stutter, which usually expresses feelings of nervousness, anxiety, or unease. Combined with the text content, the character seems to be unhappy and angry due to the prejudice of the people around him. The subtitle in the text says, \"Why are you all looking at me like that? So, as long as it's a woman, does she have to have a relationship with me?\" This sentence expresses the male character's dissatisfaction and anger towards the people around him. Based on the surprised and negative facial expression of the male character in the video clues, as well as the stuttering speech in the audio clues, we can infer that the male character is expressing a feeling of dissatisfaction and anger in this sentence. He may feel troubled by the prejudice of the people around him and is unhappy with this unfair treatment.\n\nOV labels extracted from the EMER description: surprise, nervous, dissatisfied",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: One example (“sample_00000669”) to illustrate the differences between the one-hot label,",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the differences between the traditional one-hot label, EMER",
      "page": 2
    },
    {
      "caption": "Figure 2: Pipeline of generating multimodal descriptions EMER(Multi).",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the pipeline of",
      "page": 3
    },
    {
      "caption": "Figure 2: ). In this process, we rely on GPT-3.5 and use the #3 prompt in Table",
      "page": 4
    },
    {
      "caption": "Figure 2: and use the clue",
      "page": 5
    },
    {
      "caption": "Figure 3: Language influence analysis.",
      "page": 6
    },
    {
      "caption": "Figure 3: , we first extract emotion la-",
      "page": 6
    },
    {
      "caption": "Figure 3: , we observe some",
      "page": 6
    },
    {
      "caption": "Figure 3: as an example. Under the Chinese condition,",
      "page": 7
    },
    {
      "caption": "Figure 4: Pipeline for generating unimodal and multimodal descriptions.",
      "page": 8
    },
    {
      "caption": "Figure 5: Performance of different subtitle integration strategies on varying MLLMs.",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the emotion recognition results of different strategies. More results can be found in",
      "page": 9
    },
    {
      "caption": "Figure 6: Metric correlation analysis.",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the PCC scores between different metrics.",
      "page": 9
    },
    {
      "caption": "Figure 7: One example (“sample_00000669”) to visualize the data annotation process.",
      "page": 13
    },
    {
      "caption": "Figure 8: , we combine the results in Table",
      "page": 16
    },
    {
      "caption": "Figure 8: Visualization of metric correlations. In this figure, we consider both metric and language",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Withitspowerfulreasoningability,wecangenerateaccuratelexicalclues. Finally,wecombineall",
      "data": [
        {
          "Function": "#1 Pre-label Visual Clue",
          "Prompt": "As an expert\nin the field of emotions, please focus on facial expressions, body language,\nenvironmental cues, and events in the video and predict the emotional state of the character.\nPlease ignore the character’s identity. We uniformly sample 3 frames from this video. Please\nconsider the temporal relationship between these frames and provide a complete description\nof this video. Avoid using descriptions like “the first image” and “the second image”, and\ninstead use terms like “beginning”, “middle”, and “end” to denote the progression of time."
        },
        {
          "Function": "#2 Pre-label Acoustic Clue",
          "Prompt": "Please assume the role of an expert in the field of emotions. We have a piece of text. Please\nanalyze which parts of it can be used to infer the emotional states of the characters, and\nprovide reasoning for your inference."
        },
        {
          "Function": "#3 Disambiguate",
          "Prompt": "Please assume the role of an expert in the field of emotions. We provide audio and video cues\nthat may be related to the emotions of the characters. Additionally, we provide the original\nsubtitle of the video. Please analyze which parts of the subtitle can be used to infer the\nemotional states of the characters and provide reasoning for your inference. In the process of\ninference, please integrate the audio and video cues for analysis."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Function": "#1 Visual Clue Analysis",
          "Prompt": "Please assume the role of an expert in the field of emotions. We provide clues related to the\nemotions of the characters in the video. Please output the facial movements and body gestures\ninvolved in the description, separated by commas. The output format should be in list form."
        },
        {
          "Function": "#2 Discrete Emotion Rec.",
          "Prompt": "Please assume the role of an expert in the emotional domain. We provide clues that may be\nrelated to the emotions of the character. Based on the provided clues, identify the emotional\nstates of the main characters. We provide a set of emotional candidates, please rank them in\norder of likelihood from high to low. The candidate set is {Candidate Labels}."
        },
        {
          "Function": "#3 Valence Estimation",
          "Prompt": "As an expert in the emotional domain, we provide clues that may be related to the emotions\nof characters. Based on the provided clues, please identify the overall positive or negative\nemotional polarity of the main characters. The output should be a floating-point number ranging\nfrom -5 to +5. Here, -5 indicates extremely negative emotions, 0 indicates neutral emotions,\nand +5 indicates extremely positive emotions. Larger numbers indicate more positive emotions,\nwhile smaller numbers indicate more negative emotions. Please provide your judgment as a\nfloating-point number with two decimal places, directly outputting the numerical result without\nincluding the analysis process."
        },
        {
          "Function": "#4 OV Emotion Rec.",
          "Prompt": "Please assume the role of an expert\nin the field of emotions. We provide clues that may be\nrelated to the emotions of the characters. Based on the provided clues, please identify the\nemotional states of the main characters. Please separate different emotional categories with\ncommas and output only the clearly identifiable emotional categories in a list format. If none\nare identified, please output an empty list."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nL\nV\nA": "×\n×\n×\nEmpty\n×\n×\n×\nRandom\n√\n√",
          "English\nAvg\nRecalls\nAccuracys": "0.00±0.00\n0.00±0.00\n0.00±0.00\n19.13±0.06\n24.85±0.15\n13.42±0.04",
          "Chinese\nAvg\nRecalls\nAccuracys": "0.00±0.00\n0.00±0.00\n0.00±0.00\n18.59±0.00\n24.70±0.00\n12.48±0.00"
        },
        {
          "Model\nL\nV\nA": "×\nQwen-Audio [29]\n√\n√\n×\nOneLLM [30]\n√\n√\n×\nOtter [31]\n√\n√\n×\nVideoChat [24]\n√\n√\n×\nVideo-LLaMA [32]\n√\n√\n√\nPandaGPT [33]\n√\n√\n×\nSALMONN [26]\n√\n√\n×\nVideo-LLaVA [34]\n√\n√\n×\nVideoChat2 [35]\n√\n√\n×\nOneLLM [30]\n√\n√\n×\nLLaMA-VID [36]\n√\n√\n×\nmPLUG-Owl [37]\n√\n√\n×\nVideo-ChatGPT [38]\n√\n√\n×\nChat-UniVi [39]\n√\n√\n×\nGPT-4V [28]\n√",
          "English\nAvg\nRecalls\nAccuracys": "40.23±0.09\n49.42±0.18\n31.04±0.00\n43.04±0.06\n45.92±0.05\n40.15±0.06\n44.40±0.09\n50.71±0.10\n38.09±0.09\n45.70±0.09\n42.90±0.27\n48.49±0.10\n44.74±0.14\n44.14±0.13\n45.34±0.15\n46.21±0.17\n50.03±0.01\n42.38±0.33\n48.06±0.04\n50.20±0.04\n45.92±0.04\n47.12±0.15\n48.58±0.02\n45.66±0.29\n49.60±0.28\n54.72±0.41\n44.47±0.15\n50.99±0.08\n55.93±0.09\n46.06±0.06\n51.29±0.09\n52.71±0.18\n49.87±0.00\n52.79±0.13\n54.54±0.13\n51.04±0.13\n50.73±0.06\n54.03±0.04\n47.44±0.07\n53.09±0.01\n53.68±0.00\n52.50±0.02\n56.69±0.04\n48.52±0.07\n64.86±0.00",
          "Chinese\nAvg\nRecalls\nAccuracys": "43.53±0.04\n53.71±0.00\n33.34±0.09\n46.77±0.01\n52.07±0.06\n41.47±0.08\n46.92±0.04\n52.65±0.16\n41.18±0.08\n45.63±0.04\n47.20±0.12\n44.05±0.05\n47.27±0.03\n47.98±0.07\n46.56±0.01\n47.88±0.02\n53.01±0.08\n42.75±0.11\n48.53±0.03\n52.24±0.00\n44.82±0.05\n49.59±0.05\n53.95±0.03\n45.23±0.13\n49.90±0.06\n57.12±0.08\n42.68±0.04\n51.84±0.08\n56.43±0.04\n47.26±0.11\n52.45±0.02\n57.30±0.00\n47.61±0.03\n51.43±0.03\n56.40±0.11\n46.47±0.18\n55.34±0.02\n61.15±0.10\n49.52±0.06\n54.20±0.02\n58.54±0.01\n49.86±0.03\n57.34±0.01\n54.61±0.02\n60.07±0.01"
        },
        {
          "Model\nL\nV\nA": "×\n×\nEMER(Text)\n√\n×\n×\nEMER(Video)\n√\n√\n×\nEMER(Audio)\n√\n√\n√\nEMER(Multi)",
          "English\nAvg\nRecalls\nAccuracys": "47.13±0.08\n54.41±0.15\n39.84±0.01\n60.67±0.12\n63.29±0.08\n58.05±0.16\n65.42±0.04\n67.54±0.08\n63.30±0.00\n80.05±0.24\n80.03±0.37\n80.07±0.10",
          "Chinese\nAvg\nRecalls\nAccuracys": "44.09±0.24\n50.69±0.26\n37.50±0.23\n62.05±0.10\n66.47±0.13\n57.62±0.08\n68.59±0.07\n70.10±0.06\n67.07±0.08\n85.20±0.03\n87.09±0.00\n83.31±0.05"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Otter\nVideoChat\nVideoChat2\nVideo-LLaVA\nVideo-LLaMA\nVideo-ChatGPT\nLLaMA-VID\nmPLUG-Owl\nChat-UniVi",
          "Support Modality": "Video, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text\nVideo, Text",
          "Link": "https://github.com/Luodian/Otter\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat\nhttps://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2\nhttps://github.com/PKU-YuanGroup/Video-LLaVA\nhttps://github.com/DAMO-NLP-SG/Video-LLaMA\nhttps://github.com/mbzuai-oryx/Video-ChatGPT\nhttps://github.com/dvlab-research/LLaMA-VID\nhttps://github.com/X-PLUG/mPLUG-Owl\nhttps://github.com/PKU-YuanGroup/Chat-UniVi"
        },
        {
          "Models": "SALMONN\nQwen-Audio",
          "Support Modality": "Audio, Text\nAudio, Text",
          "Link": "https://github.com/bytedance/SALMONN\nhttps://github.com/QwenLM/Qwen-Audio"
        },
        {
          "Models": "OneLLM\nPandaGPT",
          "Support Modality": "Audio, Video, Text\nAudio, Video, Text",
          "Link": "https://github.com/csuhan/OneLLM\nhttps://github.com/yxuansu/PandaGPT"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Audio LLM",
          "Subtitle": "×",
          "Prompt": "As an expert in the field of emotions, please focus on the acoustic information in the\naudio to discern clues related to the emotions of the individual. Please provide a detailed\ndescription and ultimately predict the emotional state of the individual."
        },
        {
          "Models": "",
          "Subtitle": "√",
          "Prompt": "Subtitle content of the audio: {subtitle}; As an expert in the field of emotions, please\nfocus on the acoustic information and subtitle content in the audio to discern clues related\nto the emotions of the individual. Please provide a detailed description and ultimately\npredict the emotional state of the individual in the audio."
        },
        {
          "Models": "Video LLM",
          "Subtitle": "×",
          "Prompt": "As an expert\nin the field of emotions, please focus on the facial expressions, body\nmovements, environment, etc., in the video to discern clues related to the emotions of\nthe individual. Please provide a detailed description and ultimately predict the emotional\nstate of the individual in the video."
        },
        {
          "Models": "",
          "Subtitle": "√",
          "Prompt": "Subtitle content of the video: {subtitle}; As an expert in the field of emotions, please\nfocus on the facial expressions, body movements, environment, subtitle content, etc.,\nin the video to discern clues related to the emotions of the individual. Please provide a\ndetailed description and ultimately predict the emotional state of the individual."
        },
        {
          "Models": "Audio-Video LLM",
          "Subtitle": "×",
          "Prompt": "As an expert\nin the field of emotions, please focus on the facial expressions, body\nmovements, environment, acoustic information, etc., in the video to discern clues related\nto the emotions of the individual. Please provide a detailed description and ultimately\npredict the emotional state of the individual in the video."
        },
        {
          "Models": "",
          "Subtitle": "√",
          "Prompt": "Subtitle content of the video: {subtitle}; As an expert in the field of emotions, please\nfocus on the facial expressions, body movements, environment, acoustic information,\nsubtitle content, etc., in the video to discern clues related to the emotions of the individual.\nPlease provide a detailed description and ultimately predict the emotional state of the\nindividual in the video."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nStrategy": "Otter [31]\nS0\nOtter [31]\nS1\nOtter [31]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "35.45±0.02\n40.41±0.03\n30.48±0.01\n22.95±0.06\n26.05±0.08\n19.86±0.04\n44.40±0.09\n50.71±0.10\n38.09±0.09",
          "Chinese\nAvg\nRecalls\nAccuracys": "31.61±0.11\n35.71±0.15\n27.51±0.07\n25.56±0.04\n29.14±0.03\n21.99±0.05\n46.92±0.04\n52.65±0.16\n41.18±0.08"
        },
        {
          "Model\nStrategy": "PandaGPT [33]\nS0\nPandaGPT [33]\nS1\nPandaGPT [33]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "27.14±0.02\n29.18±0.08\n25.10±0.04\n34.86±0.22\n36.77±0.30\n32.94±0.14\n46.21±0.17\n50.03±0.01\n42.38±0.33",
          "Chinese\nAvg\nRecalls\nAccuracys": "28.85±0.01\n30.95±0.00\n26.76±0.03\n34.90±0.16\n37.27±0.15\n32.53±0.18\n47.88±0.02\n53.01±0.08\n42.75±0.11"
        },
        {
          "Model\nStrategy": "Video-ChatGPT [38]\nS0\nVideo-ChatGPT [38]\nS1\nVideo-ChatGPT [38]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "34.98±0.05\n37.66±0.13\n32.30±0.03\n42.04±0.24\n45.59±0.24\n38.49±0.23\n50.73±0.06\n54.03±0.04\n47.44±0.07",
          "Chinese\nAvg\nRecalls\nAccuracys": "37.79±0.15\n40.33±0.05\n35.25±0.25\n41.17±0.03\n45.07±0.00\n37.28±0.05\n55.34±0.02\n61.15±0.10\n49.52±0.06"
        },
        {
          "Model\nStrategy": "Video-LLaMA [32]\nS0\nVideo-LLaMA [32]\nS1\nVideo-LLaMA [32]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "28.18±0.27\n28.64±0.36\n27.72±0.18\n34.48±0.16\n35.82±0.20\n33.15±0.11\n44.74±0.14\n44.14±0.13\n45.34±0.15",
          "Chinese\nAvg\nRecalls\nAccuracys": "30.72±0.11\n30.09±0.14\n31.34±0.08\n34.05±0.24\n35.16±0.22\n32.94±0.26\n47.27±0.03\n47.98±0.07\n46.56±0.01"
        },
        {
          "Model\nStrategy": "VideoChat [24]\nS0\nVideoChat [24]\nS1\nVideoChat [24]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "31.95±0.01\n31.73±0.13\n32.17±0.10\n45.13±0.07\n46.24±0.05\n44.01±0.10\n45.70±0.09\n42.90±0.27\n48.49±0.10",
          "Chinese\nAvg\nRecalls\nAccuracys": "34.56±0.02\n33.53±0.01\n35.60±0.05\n44.25±0.09\n44.76±0.02\n43.75±0.16\n45.63±0.04\n47.20±0.12\n44.05±0.05"
        },
        {
          "Model\nStrategy": "VideoChat2 [35]\nS0\nVideoChat2 [35]\nS1\nVideoChat2 [35]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "36.78±0.04\n43.08±0.00\n30.47±0.09\n38.53±0.05\n44.62±0.00\n32.43±0.10\n49.60±0.28\n54.72±0.41\n44.47±0.15",
          "Chinese\nAvg\nRecalls\nAccuracys": "36.01±0.01\n41.16±0.00\n30.86±0.01\n39.51±0.10\n45.14±0.13\n33.88±0.08\n49.90±0.06\n57.12±0.08\n42.68±0.04"
        },
        {
          "Model\nStrategy": "mPLUG-Owl [37]\nS0\nmPLUG-Owl [37]\nS1\nmPLUG-Owl [37]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "39.25±0.14\n40.56±0.15\n37.94±0.12\n45.85±0.05\n47.49±0.04\n44.22±0.07\n52.79±0.13\n54.54±0.13\n51.04±0.13",
          "Chinese\nAvg\nRecalls\nAccuracys": "40.53±0.33\n40.44±0.24\n40.62±0.43\n48.01±0.04\n49.33±0.03\n46.69±0.05\n51.43±0.03\n56.40±0.11\n46.47±0.18"
        },
        {
          "Model\nStrategy": "SALMONN [26]\nS0\nSALMONN [26]\nS1\nSALMONN [26]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "40.72±0.11\n41.38±0.25\n40.07±0.04\n39.80±0.04\n39.54±0.01\n40.05±0.06\n48.06±0.04\n50.20±0.04\n45.92±0.04",
          "Chinese\nAvg\nRecalls\nAccuracys": "43.45±0.23\n43.24±0.30\n43.66±0.16\n41.43±0.13\n41.11±0.03\n41.76±0.22\n48.53±0.03\n52.24±0.00\n44.82±0.05"
        },
        {
          "Model\nStrategy": "Qwen-Audio [29]\nS0\nQwen-Audio [29]\nS1\nQwen-Audio [29]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "33.03±0.04\n41.92±0.00\n24.14±0.08\n37.49±0.11\n46.69±0.15\n28.29±0.08\n40.23±0.09\n49.42±0.18\n31.04±0.00",
          "Chinese\nAvg\nRecalls\nAccuracys": "32.59±0.08\n40.84±0.13\n24.33±0.03\n46.81±0.00\n58.08±0.00\n35.53±0.00\n43.53±0.04\n53.71±0.00\n33.34±0.09"
        },
        {
          "Model\nStrategy": "Video-LLaVA [34]\nS0\nVideo-LLaVA [34]\nS1\nVideo-LLaVA [34]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "32.65±0.03\n33.31±0.01\n32.00±0.05\n30.59±0.01\n34.10±0.03\n27.08±0.05\n47.12±0.15\n48.58±0.02\n45.66±0.29",
          "Chinese\nAvg\nRecalls\nAccuracys": "32.76±0.03\n33.19±0.06\n32.33±0.00\n31.99±0.11\n33.40±0.19\n30.58±0.04\n49.59±0.05\n53.95±0.03\n45.23±0.13"
        },
        {
          "Model\nStrategy": "LLaMA-VID [36]\nS0\nLLaMA-VID [36]\nS1\nLLaMA-VID [36]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "35.20±0.14\n36.71±0.15\n33.69±0.14\n42.43±0.03\n43.97±0.04\n40.89±0.03\n51.29±0.09\n52.71±0.18\n49.87±0.00",
          "Chinese\nAvg\nRecalls\nAccuracys": "33.30±0.04\n33.12±0.06\n33.48±0.03\n42.57±0.08\n43.28±0.11\n41.86±0.04\n52.45±0.02\n57.30±0.00\n47.61±0.03"
        },
        {
          "Model\nStrategy": "Chat-UniVi [39]\nS0\nChat-UniVi [39]\nS1\nChat-UniVi [39]\nS2",
          "English\nAvg\nRecalls\nAccuracys": "40.02±0.18\n42.32±0.21\n37.72±0.15\n48.11±0.19\n50.96±0.20\n45.26±0.18\n53.09±0.01\n53.68±0.00\n52.50±0.02",
          "Chinese\nAvg\nRecalls\nAccuracys": "36.85±0.30\n37.74±0.27\n35.96±0.33\n47.04±0.00\n48.07±0.00\n46.01±0.00\n54.20±0.02\n58.54±0.01\n49.86±0.03"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 3: andTable8andcalculatethePCCcorrelationscoresbetweendifferentmetrics. Inthisfigure,we",
      "data": [
        {
          "Model\nL\nV\nA": "×\n×\n×\nEmpty\n×\n×\n×\nRandom\n√\n√",
          "English\nBLEU1\nBLEU4 METEOR\nROUGEl": "0.00\n0.00\n0.67\n1.75\n0.03\n0.01\n3.87\n7.75",
          "Chinese\nBLEU1\nBLEU4 METEOR\nROUGEl": "0.00\n0.00\n1.49\n2.38\n0.01\n0.00\n3.18\n5.98"
        },
        {
          "Model\nL\nV\nA": "×\nQwen-Audio [29]\n√\n√\n×\nOneLLM [30]\n√ √\n×\nOtter [31]\n√ √\n×\nVideoChat [24]\n√ √\n×\nVideo-LLaMA [32]\n√ √ √\nPandaGPT [33]\n√\n√\n×\nSALMONN [26]\n√ √\n×\nVideo-LLaVA [34]\n√ √\n×\nVideoChat2 [35]\n√ √\n×\nOneLLM [30]\n√ √\n×\nLLaMA-VID [36]\n√ √\n×\nmPLUG-Owl [37]\n√ √\n×\nVideo-ChatGPT [38]\n√ √\n×\nChat-UniVi [39]\n√ √\n×\nGPT-4V [28]\n√",
          "English\nBLEU1\nBLEU4 METEOR\nROUGEl": "21.87\n6.55\n21.65\n20.81\n33.81\n8.54\n28.00\n22.46\n27.26\n7.55\n23.42\n21.05\n26.44\n5.41\n30.58\n19.11\n28.76\n6.41\n31.22\n20.41\n33.69\n7.64\n30.29\n22.07\n31.89\n7.19\n28.42\n20.99\n33.48\n8.25\n29.68\n22.34\n31.60\n8.10\n26.61\n21.65\n32.19\n8.10\n28.44\n22.25\n33.81\n8.26\n30.31\n22.36\n33.04\n7.75\n30.24\n21.75\n32.64\n7.65\n30.25\n22.01\n32.80\n7.83\n31.12\n22.15\n39.40\n18.41\n43.67\n32.60",
          "Chinese\nBLEU1\nBLEU4 METEOR\nROUGEl": "27.64\n12.07\n26.09\n25.24\n42.75\n16.60\n34.42\n26.81\n35.35\n14.41\n29.34\n25.91\n31.36\n10.86\n37.48\n22.57\n34.88\n12.13\n37.61\n24.25\n43.02\n15.83\n37.94\n26.87\n39.00\n14.00\n35.12\n25.35\n42.72\n15.97\n36.87\n26.90\n41.18\n16.15\n33.54\n26.80\n41.31\n15.15\n35.15\n25.98\n43.01\n16.23\n37.92\n27.20\n41.69\n15.16\n37.81\n26.39\n41.96\n15.50\n38.18\n26.35\n40.76\n15.05\n38.75\n26.43\n45.45\n29.08\n53.76\n40.37"
        },
        {
          "Model\nL\nV\nA": "×\n×\nEMER(Text)\n√\n×\n×\nEMER(Video)\n√\n√\n×\nEMER(Audio)\n√ √ √\nEMER(Multi)",
          "English\nBLEU1\nBLEU4 METEOR\nROUGEl": "18.97\n5.32\n18.55\n16.93\n48.31\n30.35\n41.93\n42.69\n34.19\n17.54\n30.86\n32.87\n100.0\n100.0\n100.0\n100.0",
          "Chinese\nBLEU1\nBLEU4 METEOR\nROUGEl": "25.24\n10.30\n23.01\n20.15\n58.19\n42.65\n51.86\n49.36\n46.74\n30.80\n42.19\n40.24\n100.0\n100.0\n100.0\n100.0"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "3",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Multi-label learning, modality robustness, and semisupervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "Xianye Ben",
        "Yi Ren",
        "Junping Zhang",
        "Su-Jing Wang",
        "Kidiyo Kpalma",
        "Weixiao Meng",
        "Yong-Jin Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Adrian Ciprian",
        "Dorota Corneanu",
        "Tomasz Kamińska",
        "Sergio Sapiński",
        "Gholamreza Escalera",
        "Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Smg: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "Haoyu Chen",
        "Henglin Shi",
        "Xin Liu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Towards open vocabulary learning: A survey",
      "authors": [
        "Jianzong Wu",
        "Xiangtai Li",
        "Shilin Xu",
        "Haobo Yuan",
        "Henghui Ding",
        "Yibo Yang",
        "Xia Li",
        "Jiangning Zhang",
        "Yunhai Tong",
        "Xudong Jiang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Open-vocabulary object detection using captions",
      "authors": [
        "Alireza Zareian",
        "Kevin Rosa",
        "Derek Hu",
        "Shih-Fu Chang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Open-vocabulary object detection via vision and language knowledge distillation",
      "authors": [
        "Xiuye Gu",
        "Tsung-Yi Lin",
        "Weicheng Kuo",
        "Yin Cui"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "Scaling open-vocabulary image segmentation with image-level labels",
      "authors": [
        "Golnaz Ghiasi",
        "Xiuye Gu",
        "Yin Cui",
        "Tsung-Yi Lin"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Languagedriven semantic segmentation",
      "authors": [
        "Boyi Li",
        "Q Kilian",
        "Serge Weinberger",
        "Vladlen Belongie",
        "Rene Koltun",
        "Ranftl"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Pla: Language-driven open-vocabulary 3d scene understanding",
      "authors": [
        "Runyu Ding",
        "Jihan Yang",
        "Chuhui Xue",
        "Wenqing Zhang",
        "Song Bai",
        "Xiaojuan Qi"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "25",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "",
      "authors": [
        "Openai",
        "Chatgpt"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "28",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "29",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "30",
      "title": "One framework to align all modalities with language",
      "authors": [
        "Jiaming Han",
        "Kaixiong Gong",
        "Yiyuan Zhang",
        "Jiaqi Wang",
        "Kaipeng Zhang",
        "Dahua Lin",
        "Yu Qiao",
        "Peng Gao",
        "Xiangyu Yue",
        "Onellm"
      ],
      "year": "2023",
      "venue": "One framework to align all modalities with language",
      "arxiv": "arXiv:2312.03700"
    },
    {
      "citation_id": "31",
      "title": "Otter: A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "32",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "33",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "34",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "35",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2023",
      "venue": "Llama-vid: An image is worth 2 tokens in large language models",
      "arxiv": "arXiv:2311.17043"
    },
    {
      "citation_id": "37",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "38",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "39",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Caiwan Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "arxiv": "arXiv:2311.08046"
    }
  ]
}