{
  "paper_id": "2404.09010v1",
  "title": "Mma-Dfer: Multimodal Adaptation Of Unimodal Models For Dynamic Facial Expression Recognition In-The-Wild",
  "published": "2024-04-13T13:39:26Z",
  "authors": [
    "Kateryna Chumachenko",
    "Alexandros Iosifidis",
    "Moncef Gabbouj"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of selfsupervised learning (SSL) for pre-training of strong multimodal encoders  [40] . Another line of research has focused on adapting pre-trained static models for DFER  [8] . In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW  [19]  and MFAW  [29] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ability to perceive non-verbal communication cues is essential for development of truly intelligent interactive technologies. Automatic understanding of human emotional states, whether in the form of facial expressions, voice characteristics, or language semantics, sees a rapid development in the recent years with adoption in a vast number of applications. These include, among others, collaborative robotics  [32, 42] , healthcare  [3, 6]  and humancomputer interaction  [9, 49] .\n\nThe prevailing signal for this task has conventionally been the vision modality in the form of facial expressions. The field of facial expression recognition has undergone an evolution from solving static expression recognition problems (SFER) on images captured in controlled environments, to dynamic facial expression recognition (DFER), and further the multimodal dynamic facial expression recognition (most commonly taking the form of audiovisual emotion recognition -AVER). Recently, the field has seen an increased interest towards in-the-wild analysis, facilitated by the introduction of large-scale datasets firstly in static domain  [36] , and further in dynamic and multimodal domain  [19, 29] . Methods capable of in-the-wild analysis enable development of novel applications, and ability to arXiv:2404.09010v1 [cs.CV] 13 Apr 2024\n\nhandle dynamic scenes increases robustness towards realworld scenarios. The amount of data surrounding us is vast, and data relevant to facial expression recognition extends beyond the visual domain. Additional data inputs, such as audio or language can aid in the analysis by providing additional information about the signal. This idea has gained momentum also in the DFER field and inspired numerous multimodal facial expression recognition methods.\n\nWithin the multimodal DFER, a large portion of existing works has focused on designing elaborate fusion methods and relying on joint learning of multimodal features that utilizes each modality to its fullest. At the same time, closing the gap between DFER in constrained environments and inthe-wild DFER requires the ability to generalize to wider set of data distributions and to be robust to various challenges and variable factors associated with in-the-wild data. In the context of multimodal DFER, collection of large amounts of multimodal data representative of emotion labels poses a challenging task.\n\nThese facts naturally invite the application of selfsupervised learning (SSL) methods, that are able to learn meaningful features from data without requiring labels (followed by downstream fine-tuning for the task in hand). Indeed, previous works aiming at in-the-wild DFER have already shown benefits of such approach in both unimodal  [38, 39]  and multimodal  [40]  settings. Specifically in the multimodal space, HiCMAE  [40]  learns a joint audiovisual model by large-scale multimodal SSL pre-training, followed by full fine-tuning for DFER.\n\nAt the same time, an abundance of off-the-shelf unimodal foundational models are publicly available, and progress made in unimodal domains generally precedes their adoption in multimodal space by a significant margin, with multimodal extensions often being ad-hoc and not easily transferable. We find ourselves wishing for universal feature extractors that can be mix-and-matched for multimodal inference without the loss of performance on downstream tasks and without requiring additional multimodal pre-training. In this work, we aim to address the gap in multimodal adaptation of in-the-wild DFER from only publicly available unimodal foundational models, pre-trained independently, and on unrelated datasets. We show that with appropriate adaptation, we can obtain beyond state-of-theart results on two popular DFER benchmarks.\n\nOur contributions can be summarized as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dynamic Facial Expression Recognition In-Thewild",
      "text": "For a period of time, the field of emotion recognition or facial expression recognition has largely relied on datasets collected in controlled environments and methods designed for them, often leading to suboptimal performance in realworld applications. In the recent years, dynamic facial expression recognition in-the-wild has emerged as a separate task within the paradigm of affective behavior analysis, thanks to the appearance of a number of large-scale inthe wild datasets  [19, 22, 29] . Recently, a number of novel methods has emerged in this area. Among them, there is an ongoing trend of reliance on large-scale models performing large-scale pre-training, often in a self-supervised manner. Specifically, DFER-CLIP  [54]  relies on joint text-image space of CLIP for mapping dynamic videos to emotion labels; SVFAP  [39]  and MAE-DFER  [38]  explore Video-MAE-like masked reconstruction pre-training on dataset of face videos; HiCMAE  [40]  extends this idea to multimodal inputs. Our work differs from these approaches in a way that we explore adaptation of pre-trained unimodal models for multimodal DFER without multimodal pre-training. Another line of work, most similar to ours in spirit, is S2D  [8]  that explores temporal expansion of pre-trained static facial expression recognition model for dynamic recognition, and employs temporal adaptation modules, and landmark-guided modules to achieve competitive results. We pose a similar question in the multimodal domain, and show that we can outperform S2D with 20% less trainable parameters (both ours and S2D vision encoder follows ViT-b architecture), while accommodating another modality and not requiring static facial expression recognition pretraining, only self-supervised pre-training in each modality independently.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Adaptation Of Pre-Trained Models",
      "text": "End-to-end fine-tuning, or fine-tuning with a frozen backbone has been a dominant approach in the task of adaptation of large-scale pre-trained models for downstream tasks. Recently, dictated by the adoption of Large Language Models, a different paradigm emerged, namely prompt tuning, primarily targeted at Transformer models. In the NLP domain, prompt tuning was introduced as a way to adapt the model for a downstream task, while requiring substantially less trainable parameters by introduction of additional tokens in the input space, that are concatenated to the input, but are optimized via backpropagation  [26] [27] [28] . Visual prompt tuning, introduced in  [18]  extends this idea to the vision domain, where learnable tokens are concatenated to the input sequence of patches of a pre-trained model. A few works aim at cross-modal adaptation via prompt tuning  [20]  (targeted at VLMs), where prompts are introduced in multiple input modalities, and interact throughout the model.\n\nWhile tackling a multimodal task, our adoption of prompt tuning is aimed at each modality independently, with the goal of reducing the domain shifts in each of the unimodal foundation models. Moreover, we introduce the idea of progressive prompt adaptation, aimed at adapting the model at multiple levels of granularity independently, and therefore simplifying the adaptation task.\n\nAnother challenge differentiating DFER from static facial expression recognition from images is the need to learn temporal dependencies in the data, and identify most relevant ones. To address this challenge, prior works have either employed temporally-aware pre-training on videos  [38, 40]  or performed temporal adaptation of pre-trained static models at the fine-tuning stage  [8, 54] . A natural choice for the latter approach is often a form of temporal self-attention, introduced in different stages of feature extraction. Working with a static image encoder, we also follow a similar temporal self-attention-based approach and propose a Multimodal Temporal Transformer for temporal information extraction. We also evaluate its efficacy at different stages of the model.\n\nIn the field of multimodal facial expression recognition, prior works have to a large extent focused on joint crossmodal feature learning through complex fusion methods and/or multimodal pre-training  [1, 10, 35, 37, 40, 41] . The fusion methods are often ad-hoc and range from simple concatenation, to multimodal fusion Transformers or other elaborate frameworks. We are instead focusing on adapting already pre-trained unimodal models via lightweight bottleneck fusion adaptors while preserving their unimodal feature extraction capabilities.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Our method operates by employing two off-the-shelf SSLpre-trained unimodal encoders trained independently on audio sequences and static images, and adapts them for audiovisual dynamic facial expression recognition in-the-wild. Specifically, we choose among the models pre-trained with Masked Autoendoder (MAE) reconstruction objective, dictated by outstanding performance of MAEs on tasks requiring fine-grained features  [14] . Following this, as foundation models we employ two publicly available Vision Transformer-based encoders, namely AudioMAE  [17]  and MAE-Face  [33]  that both follow ViT-base  [11]  architec-ture and therefore have the same depth. AudioMAE is pretrained on AudioSet  [12]  and MAE-Face is pre-trained on a combination of static face image datasets  [33] .\n\nWhile large-scale pre-training of unimodal encoders ensures rich and to an extent generalizable representation within each modality, it is associated with certain challenges when adapting to multimodal DFER. These challenges can be distinguished into three categories: • Within-modality domain gap: naturally, although pretrained on large-scale data, certain level of domain gap can be expected between the data distributions of the pretrained models and the downstream domains, especially if the pre-training datasets have little relevance to DFER. • Modality alignment gap: considering a model where two encoders are pre-trained without awareness of another modality, and potentially on different data sets, there exists a gap between the distributions of feature spaces of the two modalities, both on the decision-level, and on the intermediate feature level. • Temporal adaptation gap: considering a pre-trained model that was trained on static images, adaptation to dynamic facial expression recognition could benefit from learning temporal dependencies between the frames. In the following sections, we discuss how to address each of these gaps.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Formulation",
      "text": "Let us start with formally defining a problem. Each data sample is represented by a pair of audio sequence and frame sequence and corresponds to a single emotion class label. Each frame is initially split into patches independently, following the Transformer embedding layer  [11, 46] . Frames corresponding to the same video are concatenated batchwise and we describe their temporal interaction within the batch further. Similarly, a spectrogram is extracted from audio sequence and split into patches, too, following the AudioMAE  [17]  procedure.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Progressive Prompt Learning",
      "text": "First, we propose to address the domain gap within each modality encoder independently. To achieve this, we introduce a set of learnable prompts for each modality that are concatenated to the data sequences and are updated via backpropagation. As the tokens are processed by the model, learnable prompts interact with the data tokens and are able to divert their feature representation distribution so that it is closer to the initial distribution of the data on which the model was trained.\n\nSpecifically, learnable prompts are a set of M tokens p m in R d , where d is a hidden dimension of the encoder, and the learnable prompts are concatenated to the input sequence following the embedding layer.\n\nWe note that Transformer adaptation via learnable prompts has been shown to be successful in few application areas. However, in the prior work, this idea is applied purely on the input space level. At the same time, discrepancies in feature distributions at different depths can have different nature, and it can be difficult to address all of them with the same prompt on the input level. Instead, applying dedicated prompts at different depths can aid the adaptation. Therefore, we propose progressive prompt adaptation, where we introduce a set of M l tokens at different depths of the model, that complement the initial M learnable prompt tokens and are introduced to the network progressively. Formally, given M learnable prompt tokens P ∈ R M ×d , and L intermediate layers, we introduce L sets of tokens P l ∈ R M l ×d , M l = M L and at l th layer, the prompts are updated as:\n\n(1) This way, adaptation of the model to new data distribution happens progressively thereby simplifying the task.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Modality Fusion Bottlenecks",
      "text": "To exploit cross-modal dependencies for knowledge extraction, we introduce fusion of modalities at multiple intermediate stages. Our Modality Fusion blocks are based on creating a low-dimensional bottleneck where multimodal features are obtained by fusing compressed and normalized representations of each modality, and routing them back to the corresponding unimodal branches via a gating mechanism. This approach relates to principles of learning to control the amount of transferred information, as well as information compression followed by its expansion for highlighting the most relevant features, that have been successfully applied as building blocks of different methods in the field  [2, 4, 16, 21] . Our formalization in modality fusion allows to flexibly adapt pre-trained unimodal models. A schematic representation is provided in the Figure  2 .\n\nGiven a video representation V = {v} t corresponding to t frames, and an audio sequence A, we first project each of them to a low-dimensional latent space to obtain corresponding low-dimensional representations V and Â:\n\nwhere G V and G A consist of a Linear layer projecting from high-to low-dimensional space, and a Layer Normalization layer. Further, each low-dimensional representation is followed by a corresponding aggregation function to obtain global sequence representation in each modality. For audio modality, this is represented by a Global Average Pooling over tokens of audio sequence, and for vision modality, by Global Average Pooling over tokens of all image sequences in all frames corresponding to one video: Having obtained global low-dimensional representations for each modality, we fuse them via addition to the opposite modality (unaggregated), and employ upsampling functions F A and F V to expand the joint representations back to the original dimensionality spaces. In our implementation, F A and F V are represented by a Linear layer, followed by a GELU activation:\n\nFinally, the obtained fused representations are added to the original ones via gated skip-connections, where we employ a learnable parameter α that controls the strength of multimodal representations:\n\nWe initialize α to zero such that the modality encoders receive the original (unimodal) feature representations in the first iterations of the training, and the appropriate magnitude of multimodal features is progressively learnt by the model, hence making adaptation to multimodal features easier. As a result, compression of the features of each modality to a low-dimensional latent space helps highlight the most relevant information in each modality, and further expand it back to the corresponding branch, while keeping this process adaptive to the strength of each modality via gating.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Fusion",
      "text": "The task of dynamic facial expression recognition can benefit from knowledge about between-frame temporal dependencies. However, a model that was pre-trained on static images lacks such capability. We address this challenge by employing a Multimodal Temporal Transformer.\n\nRecall that in the image encoder, given a sequence of t frames of a video, we concatenate them in a batch manner and process independently in a parallel manner. Following this stage, we extract the [CLS] token of every frame corresponding to the same video and concatenate them to form a temporal sequence. Therefore, the input tensor in the vision branch is reshaped from B * t × N × D to B × t × D, where B is the batch size, t is the temporal length (number of frames), N is the sequence length of each frame (number of patches), and D is the dimensionality. We additionally fuse the corresponding [CLS] token of audio branch to the video sequence via addition, and process the new multimodal sequence with a Joint Adaptation Module, which in our implementation is formalized as a single Linear layer (with slightly reduced dimensionality than prior encoders). Further, we add learnable temporal embeddings to the multimodal temporal sequence and concatenate a new [CLS] token. We process the new sequence with a Transformer block, which is now operating on temporal level on joint multimodal sequence, hence we refer to it as Multimodal Temporal Transformer. The [CLS] token is further used as input to the classifier.\n\nWe also note that some works have suggested adoption of temporal adaptation modules on intermediate feature level  [8] , with the aim of enriching image features with temporal information already at the feature extraction stage. We perform ablation studies on the placement of our Multimodal Temporal Transformer with intermediate temporal modules in experimental section, and find our approach to be beneficial. This can be partially attributed to fusion modules that already implicitly embed a certain level of temporal information at intermediate stages by fusing an audio sequence to image patches.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We benchmark our method on two popular multimodal dynamic facial expression recognition in-the-wild datasets, namely DFEW  [19]  and MAFW  [29] . DFEW consists of 16,000 audiovisual clips split into 5 folds, posing a 7class classification task, with classes being emotion labels of 'happy', 'sad', 'angry', 'neutral', 'surprise', 'disgust', and 'fear'. MAFW contains 10,045 clips and follows a 5-fold split as well. In this dataset set of emotions follows 11 classes: 'anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'contempt', 'anxiety', 'helplessness', 'disappointment', and 'neutral'.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We follow the traditional setup of extracting 16 frames to form a video sequence during training  [8, 40] . We notice the discrepancies in image sizes in the experimental setup of existing methods, therefore report results on 112x112 images  [29] , 160x160 images  [40] , and on 224x224 images  [8] . For MAFW, we extract faces with MTCNN  [50] . We use learning rate of 1e-4 that is annealed via cosine schedule to 0 over 25 epochs. We use batch size of 8 and weight decay of 1e-2, and AdamW optimizer with default parameters. We fix the random seed to 1. In unimodal encoders, we interpolate the positional embeddings to new sequence lengths and keep them tunable to adapt to the new resolutions, the rest of unimodal encoder parameters remain frozen. Therefore, tunable parameters include the Fusion Bottleneck blocks, Multimodal Temporal Transformer, learnable prompts, classifier and positional embeddings, totaling 7.5M parameters. For comparison, S2D that follows a similar framework of adapting pre-trained (for static emotion recognition in their case) network, includes 9M tunable parameters.\n\nMultimodal Temporal Transformer is a 1-layer Transformer with hidden dimension of 512 and 8 heads. Fusion Bottleneck latent dimensionality is set to 128, and 6 learnable prompts are introduced in each modality, and progressive updates are introduced twice in the network, after 1st and 7th layers, with 3 tokens each.\n\nWe follow the 5-fold experimental protocol in each dataset with the provided splits. We train the models on the train set and report the result of final checkpoint, i.e., at 25th epoch, on the test set. Training is done on a single Nvidia Tesla V-100-32 GPU, and single training (of single fold) takes approximately 8 hours on resolution 160. We report results both with following the 16-frame uniform sampling at inference time, as well as widely adopted 2-clip average results  [8, 40] , where two clips are sampled uniformly from a single video and predictions are averaged. We note that we do not observe a significant difference between these two approaches. As prior works, we report Unweighted Average Recall (UAR) and Weighted Average Recall (WAR).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "The comparison to state-of-the-art methods is provided in Table  1 . As can be seen, MMA-DFER outperforms the competing methods. Specifically, MMA-DFER outperforms current state-of-the-art of S2D by 1.5% UAR and WAR on DFEW dataset and 1% on MAFW. We also note that for S2D, the best UAR and WAR are obtained from different models/training strategies (with and without oversampling of underrepresented classes), while in MMA-DFER this is achieved by a single model. Compared to the best multimodal model -HiCMAE, we achieve stronger results as well, both on 224 and 160 resolution. With the same image resolution, we obtain 2-3% improvement on DFEW   [24]  55.71 69.24 --V 112 M3DFEL  [47]  56.10 69.25 --V -CLIPER  [25]  57.56 70.84 --V 224 TMEP  [51]  57.16 68.85 37.17 51.15 AV 112 DFER-CLIP  [53]  59.61 71.  25     [8, 40] . M denotes the modality, and Res denotes the image resolution. and 1.5% on MAFW.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Of Temporal Adaptation Approaches",
      "text": "In MMA, the Multimodal Temporal Transformer is placed following the unimodal branches. However, some works have suggested a different approach, where temporal adaptation happens already in the intermediate features of the model  [8] . We perform an experiment where we add such temporal adaptors in the intermediate steps of the network, following each Bottleneck Fusion block. Such temporal adaptors are similar to TMA described in  [8]   The results can be seen in Table  2 , where we report UAR, WAR, and number of parameters that correspond to temporal processing blocks. We note that the variant with ITA of dimensionality 128 has less parameters than ITA-64 due to internal dimensionality of Bottleneck Fusion already being 128, therefore no additional layers need to be introduced to this variant to project to new dimensionality space, unlike all other variants. We can see that the best result is achieved by MTM and the second-best by MTM combined with ITA. Among ITA, we observe the variant with d=128 to be outperforming the competing ones. We also note that the performance is not directly correlated with the number of parameters, but more with the placement of the temporal module.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Of Fusion Approaches",
      "text": "We additionally compare our Fusion Bottleneck blocks with other popular multimodal fusion approaches. Specifically, we compare with: • Addition in original space, i.e., without compression and expansion (ADD); • Multimodal Transformer following  [45] , where we introduce 2 multi-head self-attention blocks, audio-to-vision, and vision-to-audio, outputs of which are added to corresponding branches (MULT); • Single multimodal Transformer on concatenated audio and image features, with global average pooled representation added back to each branch (MULT-concat). • No modality fusion; For MULT, we utilize 2 heads with total dimensionality of 128, to match the Fusion Bottleneck dimensionality. We compare the results on 1st fold of DFEW on 160x160 resolution.\n\nThe results can be seen in Table  3 . As can be seen, our approach outperforms the competing methods by a significant margin, indicating the effectiveness of the proposed Fusion Bottlenecks. Poor performance of MULT and MULT-concat can be associated with difficulty of drawing As can be seen, performance of plain pre-trained models is rather poor, and each of the components progressively improves the performance. The biggest effect is brought by Fusion Bottleneck which improves WAR by 5% and UAR by 3.7% if comparing variants without MTT. We also note how performance increases by using MTT by 1.5%, but when used in conjunction with Fusion Bottlenecks, MTT improves the performance by 3.5%. This shows that fusion of features on intermediate level helps late-stage adaptation that precedes temporal modeling.\n\nWe further study the effect of each modality independently. Here, in the unimodal cases each branch is frozen and only the classifier is updated. The results are shown in Table  5 . As can be seen, performance of each individual modality is significantly lower than the combination.\n\nWe additionally study the dimensionality of the bottleneck space. Recall that dimensionality of both audio and image encoders in ViT-base are 768  [11] , and we set our bottleneck space to 128. Here, we evaluate the effect of larger and smaller dimensionalities of the latent space. The results are shown in Table  6 . As can be seen, larger values do increase the performance of the model, but require significantly more parameters. We also find that increasing dimensionality beyond d-128 brings rather diminishing returns in terms of UAR and WAR. Finally, we also study the frequency of prompt updates in the case of progressive prompting. The results are shown in Table  7 . Here, 12 corresponds to introducing new learnable prompts every layer, 6 every 2nd layer, 2 every 3rd layer, 2 every 6th layer (our final model case), and 0 not using learnable prompts at all. As can be seen, any number of updates results in better accuracy than no prompts, with the best result achieved when introducing them twice.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "We have investigated adaptation of pre-trained unimodal models for multimodal dynamic facial expression recognition in-the-wild. We identified key limitations associated with adapting pre-trained models for this task, namely intramodality adaptation, cross-modal alignment, and temporal adaptation, and proposed solutions to address them. Our proposed model, MMA-DFER sets a new state-of-the art on two popular DFER benchmarks DFEW and MAFW. Future work could include experimentation with additional unimodal backbones and exploitation of further modali-ties/sensors, such as landmarks or vision-language latent spaces.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Acknowledgement",
      "text": "This work was partially funded by the NSF CBL and Business Finland project AMALIA, and by the Horizon Europe programme PANDORA (GA 101135775).",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic description of MMA-DFER: Two pre-trained",
      "page": 1
    },
    {
      "caption": "Figure 2: Given a video representation V = {v}t corresponding",
      "page": 4
    },
    {
      "caption": "Figure 2: Fusion bottleneck. The features of each modality are",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Dynamic Facial Expression Recognition (DFER) has re-"
        },
        {
          "Abstract": "ceived significant interest in the recent years dictated by its"
        },
        {
          "Abstract": "pivotal\nrole in enabling empathic and human-compatible"
        },
        {
          "Abstract": "technologies.\nAchieving\nrobustness\ntowards\nin-the-wild"
        },
        {
          "Abstract": "data in DFER is particularly important\nfor real-world ap-"
        },
        {
          "Abstract": "plications. One of\nthe directions aimed at\nimproving such"
        },
        {
          "Abstract": "models is multimodal emotion recognition based on audio"
        },
        {
          "Abstract": "and video data. Multimodal\nlearning in DFER increases"
        },
        {
          "Abstract": "the model capabilities by leveraging richer, complementary"
        },
        {
          "Abstract": "data representations. Within the field of multimodal DFER,"
        },
        {
          "Abstract": "recent methods have focused on exploiting advances of self-"
        },
        {
          "Abstract": "supervised learning (SSL) for pre-training of strong multi-"
        },
        {
          "Abstract": "modal encoders [40]. Another line of research has focused"
        },
        {
          "Abstract": "on adapting pre-trained static models for DFER [8]. In this"
        },
        {
          "Abstract": "work, we propose a different perspective on the problem and"
        },
        {
          "Abstract": "investigate the advancement of multimodal DFER perfor-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "mance by adapting SSL-pre-trained disjoint unimodal en-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "coders. We identify main challenges associated with this"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "task, namely, intra-modality adaptation, cross-modal align-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ment, and temporal adaptation, and propose solutions to"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "each of them. As a result, we demonstrate improvement over"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "current state-of-the-art on two popular DFER benchmarks,"
        },
        {
          "Abstract": "namely DFEW [19] and MFAW [29]."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1. Introduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "The ability to perceive non-verbal communication cues is"
        },
        {
          "Abstract": "essential\nfor\ndevelopment\nof\ntruly\nintelligent\ninteractive"
        },
        {
          "Abstract": "technologies.\nAutomatic understanding of human emo-"
        },
        {
          "Abstract": "tional\nstates, whether\nin the\nform of\nfacial\nexpressions,"
        },
        {
          "Abstract": "voice characteristics, or\nlanguage semantics,\nsees a rapid"
        },
        {
          "Abstract": "development\nin the recent years with adoption in a vast"
        },
        {
          "Abstract": "number of applications. These include, among others, col-"
        },
        {
          "Abstract": "laborative robotics [32, 42], healthcare [3, 6] and human-"
        },
        {
          "Abstract": "computer interaction [9, 49]."
        },
        {
          "Abstract": "The prevailing signal\nfor\nthis\ntask has\nconventionally"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "art results on two popular DFER benchmarks.",
          "independently.": ""
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "",
          "independently.": "2.2. Adaptation of pre-trained models"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "Our contributions can be summarized as follows:",
          "independently.": ""
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "• We\nshow that orthogonally to the works proposed in",
          "independently.": "End-to-end fine-tuning, or fine-tuning with a frozen back-"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "recent\nliterature,\nstate-of-the-art performance on multi-",
          "independently.": "bone has been a dominant approach in the task of adaptation"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "modal dynamic face recognition can be achieved with-",
          "independently.": "of large-scale pre-trained models for downstream tasks. Re-"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "out a) large-scale paired multimodal pre-training; b) pre-",
          "independently.": "cently, dictated by the adoption of Large Language Models,"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "training for static facial expression recognition",
          "independently.": "a different paradigm emerged, namely prompt\ntuning, pri-"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "• We identify key challenges in adaptation of pre-trained",
          "independently.": "marily targeted at Transformer models. In the NLP domain,"
        },
        {
          "appropriate adaptation, we can obtain beyond state-of-the-": "models\nfor multimodal DFER and propose solution to",
          "independently.": "prompt\ntuning was introduced as a way to adapt\nthe model"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "handle dynamic scenes increases robustness towards real-": "world scenarios.",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "includes progressive prompt\ntuning for bridging intra-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "The amount of data surrounding us is vast, and data rel-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "modality gap, Fusion Bottleneck blocks for cross-modal"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "evant\nto facial expression recognition extends beyond the",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "alignment,\nand Multimodal Temporal Transformer\nfor"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "visual domain. Additional data inputs, such as audio or lan-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "temporal alignment"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "guage can aid in the analysis by providing additional infor-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "• We\nset new SOTA on two popular\nin-the-wild DFER"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "mation about\nthe signal. This idea has gained momentum",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "benchmarks, namely DFEW and MAFW."
        },
        {
          "handle dynamic scenes increases robustness towards real-": "also in the DFER field and inspired numerous multimodal",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "facial expression recognition methods.",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "2. Related Work"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "Within the multimodal DFER, a large portion of existing",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "2.1. Dynamic Facial Expression Recognition in-the-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "works has focused on designing elaborate fusion methods",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "wild"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "and relying on joint learning of multimodal features that uti-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "lizes each modality to its fullest. At the same time, closing",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "For a period of\ntime,\nthe field of emotion recognition or"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "the gap between DFER in constrained environments and in-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "facial expression recognition has largely relied on datasets"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "the-wild DFER requires the ability to generalize to wider set",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "collected in controlled environments and methods designed"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "of data distributions and to be robust\nto various challenges",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "for them, often leading to suboptimal performance in real-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "and variable factors associated with in-the-wild data. In the",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "world applications.\nIn the\nrecent years,\ndynamic\nfacial"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "context of multimodal DFER, collection of\nlarge amounts",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "expression recognition in-the-wild has emerged as a sepa-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "of multimodal data representative of emotion labels poses a",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "rate task within the paradigm of affective behavior analy-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "challenging task.",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "sis, thanks to the appearance of a number of large-scale in-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "These\nfacts\nnaturally\ninvite\nthe\napplication\nof\nself-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "the wild datasets [19, 22, 29]. Recently, a number of novel"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "supervised learning (SSL) methods,\nthat are able to learn",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "methods has emerged in this area. Among them, there is an"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "meaningful features from data without requiring labels (fol-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "ongoing trend of reliance on large-scale models performing"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "lowed by downstream fine-tuning for the task in hand).\nIn-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "large-scale pre-training, often in a self-supervised manner."
        },
        {
          "handle dynamic scenes increases robustness towards real-": "deed, previous works aiming at\nin-the-wild DFER have al-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "Specifically, DFER-CLIP [54]\nrelies on joint\ntext-image"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "ready shown benefits of\nsuch approach in both unimodal",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "space of CLIP for mapping dynamic videos\nto emotion"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "[38, 39] and multimodal\n[40] settings.\nSpecifically in the",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "labels; SVFAP [39] and MAE-DFER [38] explore Video-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "multimodal\nspace, HiCMAE [40]\nlearns a joint audiovi-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "MAE-like masked reconstruction pre-training on dataset of"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "sual model by large-scale multimodal SSL pre-training, fol-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "face videos; HiCMAE [40] extends this idea to multimodal"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "lowed by full fine-tuning for DFER.",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "inputs. Our work differs from these approaches in a way"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "At\nthe same time,\nan abundance of off-the-shelf uni-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "that we explore adaptation of pre-trained unimodal models"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "modal\nfoundational models\nare\npublicly\navailable,\nand",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "for multimodal DFER without multimodal pre-training."
        },
        {
          "handle dynamic scenes increases robustness towards real-": "progress made\nin unimodal domains generally\nprecedes",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "Another\nline of work, most similar\nto ours in spirit,\nis"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "their adoption in multimodal space by a significant margin,",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "S2D [8]\nthat explores\ntemporal expansion of pre-trained"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "with multimodal extensions often being ad-hoc and not eas-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "static\nfacial\nexpression\nrecognition model\nfor\ndynamic"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "ily transferable. We find ourselves wishing for universal",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "recognition, and employs temporal adaptation modules, and"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "feature extractors that can be mix-and-matched for multi-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "landmark-guided modules\nto achieve competitive results."
        },
        {
          "handle dynamic scenes increases robustness towards real-": "modal\ninference without\nthe loss of performance on down-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "We pose a similar question in the multimodal domain, and"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "stream tasks and without\nrequiring additional multimodal",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "show that we\ncan outperform S2D with 20% less\ntrain-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "pre-training. In this work, we aim to address the gap in mul-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "able parameters (both ours and S2D vision encoder follows"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "timodal adaptation of in-the-wild DFER from only publicly",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "ViT-b architecture), while accommodating another modal-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "available unimodal\nfoundational models, pre-trained inde-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "ity and not requiring static facial expression recognition pre-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "pendently, and on unrelated datasets. We show that with",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "training, only self-supervised pre-training in each modality"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "appropriate adaptation, we can obtain beyond state-of-the-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "independently."
        },
        {
          "handle dynamic scenes increases robustness towards real-": "art results on two popular DFER benchmarks.",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "2.2. Adaptation of pre-trained models"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "Our contributions can be summarized as follows:",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": ""
        },
        {
          "handle dynamic scenes increases robustness towards real-": "• We\nshow that orthogonally to the works proposed in",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "End-to-end fine-tuning, or fine-tuning with a frozen back-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "recent\nliterature,\nstate-of-the-art performance on multi-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "bone has been a dominant approach in the task of adaptation"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "modal dynamic face recognition can be achieved with-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "of large-scale pre-trained models for downstream tasks. Re-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "out a) large-scale paired multimodal pre-training; b) pre-",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "cently, dictated by the adoption of Large Language Models,"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "training for static facial expression recognition",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "a different paradigm emerged, namely prompt\ntuning, pri-"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "• We identify key challenges in adaptation of pre-trained",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "marily targeted at Transformer models. In the NLP domain,"
        },
        {
          "handle dynamic scenes increases robustness towards real-": "models\nfor multimodal DFER and propose solution to",
          "each of\nthem,\nshowing their efficacy.\nSpecifically,\nthis": "prompt\ntuning was introduced as a way to adapt\nthe model"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "main, where learnable tokens are concatenated to the input": "sequence of patches of a pre-trained model. A few works",
          "within each modality,\nit\nis\nassociated with certain chal-": "lenges when adapting to multimodal DFER. These chal-"
        },
        {
          "main, where learnable tokens are concatenated to the input": "aim at cross-modal adaptation via prompt\ntuning [20] (tar-",
          "within each modality,\nit\nis\nassociated with certain chal-": "lenges can be distinguished into three categories:"
        },
        {
          "main, where learnable tokens are concatenated to the input": "geted at VLMs), where prompts are introduced in multiple",
          "within each modality,\nit\nis\nassociated with certain chal-": "• Within-modality domain gap: naturally, although pre-"
        },
        {
          "main, where learnable tokens are concatenated to the input": "input modalities, and interact throughout the model.",
          "within each modality,\nit\nis\nassociated with certain chal-": "trained on large-scale data, certain level of domain gap"
        },
        {
          "main, where learnable tokens are concatenated to the input": "While\ntackling\na multimodal\ntask,\nour\nadoption\nof",
          "within each modality,\nit\nis\nassociated with certain chal-": "can be expected between the data distributions of the pre-"
        },
        {
          "main, where learnable tokens are concatenated to the input": "prompt\ntuning is\naimed at\neach modality independently,",
          "within each modality,\nit\nis\nassociated with certain chal-": "trained models and the downstream domains, especially"
        },
        {
          "main, where learnable tokens are concatenated to the input": "with the goal of reducing the domain shifts in each of the",
          "within each modality,\nit\nis\nassociated with certain chal-": "if the pre-training datasets have little relevance to DFER."
        },
        {
          "main, where learnable tokens are concatenated to the input": "unimodal foundation models. Moreover, we introduce the",
          "within each modality,\nit\nis\nassociated with certain chal-": "• Modality alignment gap:\nconsidering a model where"
        },
        {
          "main, where learnable tokens are concatenated to the input": "idea of progressive prompt adaptation, aimed at adapting",
          "within each modality,\nit\nis\nassociated with certain chal-": "two encoders are pre-trained without awareness of an-"
        },
        {
          "main, where learnable tokens are concatenated to the input": "the model at multiple levels of granularity independently,",
          "within each modality,\nit\nis\nassociated with certain chal-": "other modality,\nand potentially on different data\nsets,"
        },
        {
          "main, where learnable tokens are concatenated to the input": "and therefore simplifying the adaptation task.",
          "within each modality,\nit\nis\nassociated with certain chal-": "there\nexists\na gap between the distributions of\nfeature"
        },
        {
          "main, where learnable tokens are concatenated to the input": "Another challenge differentiating DFER from static fa-",
          "within each modality,\nit\nis\nassociated with certain chal-": "spaces of the two modalities, both on the decision-level,"
        },
        {
          "main, where learnable tokens are concatenated to the input": "cial expression recognition from images is the need to learn",
          "within each modality,\nit\nis\nassociated with certain chal-": "and on the intermediate feature level."
        },
        {
          "main, where learnable tokens are concatenated to the input": "temporal dependencies in the data, and identify most rele-",
          "within each modality,\nit\nis\nassociated with certain chal-": "• Temporal adaptation gap:\nconsidering a pre-trained"
        },
        {
          "main, where learnable tokens are concatenated to the input": "vant ones. To address this challenge, prior works have either",
          "within each modality,\nit\nis\nassociated with certain chal-": "model\nthat was\ntrained on static images,\nadaptation to"
        },
        {
          "main, where learnable tokens are concatenated to the input": "employed temporally-aware pre-training on videos [38, 40]",
          "within each modality,\nit\nis\nassociated with certain chal-": "dynamic facial expression recognition could benefit from"
        },
        {
          "main, where learnable tokens are concatenated to the input": "or performed temporal adaptation of pre-trained static mod-",
          "within each modality,\nit\nis\nassociated with certain chal-": "learning temporal dependencies between the frames."
        },
        {
          "main, where learnable tokens are concatenated to the input": "els at the fine-tuning stage [8, 54]. A natural choice for the",
          "within each modality,\nit\nis\nassociated with certain chal-": "In the following sections, we discuss how to address each"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for a downstream task, while requiring substantially less": "trainable parameters by introduction of additional\ntokens",
          "ture and therefore have the same depth. AudioMAE is pre-": "trained on AudioSet [12] and MAE-Face is pre-trained on a"
        },
        {
          "for a downstream task, while requiring substantially less": "in the input space,\nthat are concatenated to the input, but",
          "ture and therefore have the same depth. AudioMAE is pre-": "combination of static face image datasets [33]."
        },
        {
          "for a downstream task, while requiring substantially less": "are optimized via backpropagation [26–28]. Visual prompt",
          "ture and therefore have the same depth. AudioMAE is pre-": "While large-scale pre-training of unimodal encoders en-"
        },
        {
          "for a downstream task, while requiring substantially less": "tuning, introduced in [18] extends this idea to the vision do-",
          "ture and therefore have the same depth. AudioMAE is pre-": "sures\nrich\nand\nto\nan\nextent\ngeneralizable\nrepresentation"
        },
        {
          "for a downstream task, while requiring substantially less": "main, where learnable tokens are concatenated to the input",
          "ture and therefore have the same depth. AudioMAE is pre-": "within each modality,\nit\nis\nassociated with certain chal-"
        },
        {
          "for a downstream task, while requiring substantially less": "sequence of patches of a pre-trained model. A few works",
          "ture and therefore have the same depth. AudioMAE is pre-": "lenges when adapting to multimodal DFER. These chal-"
        },
        {
          "for a downstream task, while requiring substantially less": "aim at cross-modal adaptation via prompt\ntuning [20] (tar-",
          "ture and therefore have the same depth. AudioMAE is pre-": "lenges can be distinguished into three categories:"
        },
        {
          "for a downstream task, while requiring substantially less": "geted at VLMs), where prompts are introduced in multiple",
          "ture and therefore have the same depth. AudioMAE is pre-": "• Within-modality domain gap: naturally, although pre-"
        },
        {
          "for a downstream task, while requiring substantially less": "input modalities, and interact throughout the model.",
          "ture and therefore have the same depth. AudioMAE is pre-": "trained on large-scale data, certain level of domain gap"
        },
        {
          "for a downstream task, while requiring substantially less": "While\ntackling\na multimodal\ntask,\nour\nadoption\nof",
          "ture and therefore have the same depth. AudioMAE is pre-": "can be expected between the data distributions of the pre-"
        },
        {
          "for a downstream task, while requiring substantially less": "prompt\ntuning is\naimed at\neach modality independently,",
          "ture and therefore have the same depth. AudioMAE is pre-": "trained models and the downstream domains, especially"
        },
        {
          "for a downstream task, while requiring substantially less": "with the goal of reducing the domain shifts in each of the",
          "ture and therefore have the same depth. AudioMAE is pre-": "if the pre-training datasets have little relevance to DFER."
        },
        {
          "for a downstream task, while requiring substantially less": "unimodal foundation models. Moreover, we introduce the",
          "ture and therefore have the same depth. AudioMAE is pre-": "• Modality alignment gap:\nconsidering a model where"
        },
        {
          "for a downstream task, while requiring substantially less": "idea of progressive prompt adaptation, aimed at adapting",
          "ture and therefore have the same depth. AudioMAE is pre-": "two encoders are pre-trained without awareness of an-"
        },
        {
          "for a downstream task, while requiring substantially less": "the model at multiple levels of granularity independently,",
          "ture and therefore have the same depth. AudioMAE is pre-": "other modality,\nand potentially on different data\nsets,"
        },
        {
          "for a downstream task, while requiring substantially less": "and therefore simplifying the adaptation task.",
          "ture and therefore have the same depth. AudioMAE is pre-": "there\nexists\na gap between the distributions of\nfeature"
        },
        {
          "for a downstream task, while requiring substantially less": "Another challenge differentiating DFER from static fa-",
          "ture and therefore have the same depth. AudioMAE is pre-": "spaces of the two modalities, both on the decision-level,"
        },
        {
          "for a downstream task, while requiring substantially less": "cial expression recognition from images is the need to learn",
          "ture and therefore have the same depth. AudioMAE is pre-": "and on the intermediate feature level."
        },
        {
          "for a downstream task, while requiring substantially less": "temporal dependencies in the data, and identify most rele-",
          "ture and therefore have the same depth. AudioMAE is pre-": "• Temporal adaptation gap:\nconsidering a pre-trained"
        },
        {
          "for a downstream task, while requiring substantially less": "vant ones. To address this challenge, prior works have either",
          "ture and therefore have the same depth. AudioMAE is pre-": "model\nthat was\ntrained on static images,\nadaptation to"
        },
        {
          "for a downstream task, while requiring substantially less": "employed temporally-aware pre-training on videos [38, 40]",
          "ture and therefore have the same depth. AudioMAE is pre-": "dynamic facial expression recognition could benefit from"
        },
        {
          "for a downstream task, while requiring substantially less": "or performed temporal adaptation of pre-trained static mod-",
          "ture and therefore have the same depth. AudioMAE is pre-": "learning temporal dependencies between the frames."
        },
        {
          "for a downstream task, while requiring substantially less": "els at the fine-tuning stage [8, 54]. A natural choice for the",
          "ture and therefore have the same depth. AudioMAE is pre-": "In the following sections, we discuss how to address each"
        },
        {
          "for a downstream task, while requiring substantially less": "latter approach is often a form of temporal self-attention, in-",
          "ture and therefore have the same depth. AudioMAE is pre-": "of these gaps."
        },
        {
          "for a downstream task, while requiring substantially less": "troduced in different stages of feature extraction. Working",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "3.1. Formulation"
        },
        {
          "for a downstream task, while requiring substantially less": "with a static image encoder, we also follow a similar tempo-",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "ral self-attention-based approach and propose a Multimodal",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "Let us start with formally defining a problem.\nEach data"
        },
        {
          "for a downstream task, while requiring substantially less": "Temporal Transformer for temporal information extraction.",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "sample is represented by a pair of audio sequence and frame"
        },
        {
          "for a downstream task, while requiring substantially less": "We also evaluate its efficacy at different stages of the model.",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "sequence and corresponds to a single emotion class label."
        },
        {
          "for a downstream task, while requiring substantially less": "In the field of multimodal facial expression recognition,",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "Each frame is initially split into patches independently, fol-"
        },
        {
          "for a downstream task, while requiring substantially less": "prior works have to a large extent\nfocused on joint cross-",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "lowing the Transformer embedding layer [11, 46]. Frames"
        },
        {
          "for a downstream task, while requiring substantially less": "modal\nfeature\nlearning through complex fusion methods",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "corresponding to the same video are concatenated batch-"
        },
        {
          "for a downstream task, while requiring substantially less": "and/or multimodal pre-training [1, 10, 35, 37, 40, 41]. The",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "wise and we describe their temporal\ninteraction within the"
        },
        {
          "for a downstream task, while requiring substantially less": "fusion methods\nare often ad-hoc\nand range\nfrom simple",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "batch further.\nSimilarly, a spectrogram is extracted from"
        },
        {
          "for a downstream task, while requiring substantially less": "concatenation,\nto multimodal fusion Transformers or other",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "audio sequence and split\ninto patches,\ntoo,\nfollowing the"
        },
        {
          "for a downstream task, while requiring substantially less": "elaborate frameworks. We are instead focusing on adapting",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "AudioMAE [17] procedure."
        },
        {
          "for a downstream task, while requiring substantially less": "already pre-trained unimodal models via lightweight bottle-",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "neck fusion adaptors while preserving their unimodal\nfea-",
          "ture and therefore have the same depth. AudioMAE is pre-": "3.2. Progressive prompt learning"
        },
        {
          "for a downstream task, while requiring substantially less": "ture extraction capabilities.",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "First, we propose to address the domain gap within each"
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "modality encoder\nindependently.\nTo achieve this, we in-"
        },
        {
          "for a downstream task, while requiring substantially less": "3. Methodology",
          "ture and therefore have the same depth. AudioMAE is pre-": ""
        },
        {
          "for a downstream task, while requiring substantially less": "",
          "ture and therefore have the same depth. AudioMAE is pre-": "troduce a set of\nlearnable prompts for each modality that"
        },
        {
          "for a downstream task, while requiring substantially less": "Our method operates by employing two off-the-shelf SSL-",
          "ture and therefore have the same depth. AudioMAE is pre-": "are concatenated to the data sequences and are updated via"
        },
        {
          "for a downstream task, while requiring substantially less": "pre-trained unimodal encoders trained independently on au-",
          "ture and therefore have the same depth. AudioMAE is pre-": "backpropagation. As the tokens are processed by the model,"
        },
        {
          "for a downstream task, while requiring substantially less": "dio sequences and static images, and adapts them for audio-",
          "ture and therefore have the same depth. AudioMAE is pre-": "learnable prompts interact with the data tokens and are able"
        },
        {
          "for a downstream task, while requiring substantially less": "visual dynamic facial expression recognition in-the-wild.",
          "ture and therefore have the same depth. AudioMAE is pre-": "to divert\ntheir\nfeature representation distribution so that\nit"
        },
        {
          "for a downstream task, while requiring substantially less": "Specifically, we choose among the models pre-trained with",
          "ture and therefore have the same depth. AudioMAE is pre-": "is closer to the initial distribution of the data on which the"
        },
        {
          "for a downstream task, while requiring substantially less": "Masked Autoendoder (MAE) reconstruction objective, dic-",
          "ture and therefore have the same depth. AudioMAE is pre-": "model was trained."
        },
        {
          "for a downstream task, while requiring substantially less": "tated by outstanding performance of MAEs on tasks\nre-",
          "ture and therefore have the same depth. AudioMAE is pre-": "Specifically, learnable prompts are a set of M tokens pm"
        },
        {
          "for a downstream task, while requiring substantially less": "quiring fine-grained features [14]. Following this, as foun-",
          "ture and therefore have the same depth. AudioMAE is pre-": "in Rd, where d is a hidden dimension of the encoder, and the"
        },
        {
          "for a downstream task, while requiring substantially less": "dation models we\nemploy two publicly available Vision",
          "ture and therefore have the same depth. AudioMAE is pre-": "learnable prompts are concatenated to the input sequence"
        },
        {
          "for a downstream task, while requiring substantially less": "Transformer-based encoders, namely AudioMAE [17] and",
          "ture and therefore have the same depth. AudioMAE is pre-": "following the embedding layer."
        },
        {
          "for a downstream task, while requiring substantially less": "MAE-Face [33]\nthat both follow ViT-base [11] architec-",
          "ture and therefore have the same depth. AudioMAE is pre-": "We\nnote\nthat\nTransformer\nadaptation\nvia\nlearnable"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "prompts has been shown to be successful\nin few applica-": "tion areas. However,\nin the prior work,\nthis idea is applied",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "(cid:80)t\n(cid:80)Nv\n(cid:80)Na"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "Vj\nAj"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "purely on the input space level. At\nthe same time, discrep-",
          "in all frames corresponding to one video:": "i\nj=1\ni=1\nj=1"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": ".\nand\n(3)\nLa =\nLv ="
        },
        {
          "prompts has been shown to be successful\nin few applica-": "ancies in feature distributions at different depths can have",
          "in all frames corresponding to one video:": "t ∗ Nv\nNa"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "different nature, and it can be difficult to address all of them",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "with the same prompt on the input\nlevel.\nInstead, apply-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "ing dedicated prompts at different depths can aid the adap-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "tation. Therefore, we propose progressive prompt adapta-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "tion, where we introduce a set of M l\ntokens at different",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "depths of the model,\nthat complement\nthe initial M learn-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "able prompt\ntokens and are introduced to the network pro-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "gressively.\nFormally,\ngiven M learnable prompt\ntokens",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "P ∈ RM ×d, and L intermediate layers, we introduce L",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "sets of tokens Pl ∈ RM l×d, M l = M",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "L and at lth layer, the",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "prompts are updated as:",
          "in all frames corresponding to one video:": "Figure 2.\nFusion bottleneck.\nThe features of each modality are"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "first compressed to a low-dimensional representation, aggregated"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "P[(l − 1) ∗ M l\n: l ∗ M l, :] = P[(l − 1) ∗ M l\n: l ∗ M l\n:] + Pl.",
          "in all frames corresponding to one video:": "to obtain global representation per sequence,\nthen fused with the"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "complementary modality, and the joint representation is expanded"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "(1)",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "back to the original space, and added to the initial features via a"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "This way, adaptation of the model\nto new data distribu-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "gating mechanism."
        },
        {
          "prompts has been shown to be successful\nin few applica-": "tion happens progressively thereby simplifying the task.",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "Having obtained global low-dimensional representations"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "3.3. Modality fusion bottlenecks",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "for each modality, we fuse them via addition to the opposite"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "To exploit cross-modal dependencies for knowledge extrac-",
          "in all frames corresponding to one video:": "modality (unaggregated), and employ upsampling functions"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "tion, we introduce fusion of modalities at multiple interme-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "to expand the joint representations back to the\nFA and FV"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "diate stages. Our Modality Fusion blocks are based on cre-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "original dimensionality spaces.\nIn our implementation, FA"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "ating a low-dimensional bottleneck where multimodal fea-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "are represented by a Linear\nlayer,\nfollowed by a\nand FV"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "tures are obtained by fusing compressed and normalized",
          "in all frames corresponding to one video:": "GELU activation:"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "representations of each modality, and routing them back to",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "Ua = FA( ˆA + Lv),"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "the corresponding unimodal branches via a gating mech-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "(4)"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "anism.\nThis approach relates to principles of\nlearning to",
          "in all frames corresponding to one video:": "Uv = FV ( ˆV + La)."
        },
        {
          "prompts has been shown to be successful\nin few applica-": "control the amount of transferred information, as well as in-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "Finally,\nthe obtained fused representations are added to"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "formation compression followed by its expansion for high-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "the original ones via gated skip-connections, where we em-"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "lighting the most relevant features,\nthat have been success-",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "ploy a learnable parameter α that controls the strength of"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "fully applied as building blocks of different methods in the",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "multimodal representations:"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "field [2, 4, 16, 21]. Our\nformalization in modality fusion",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "allows\nto flexibly adapt pre-trained unimodal models.\nA",
          "in all frames corresponding to one video:": "˜"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "A = A + tanh(α) ∗ Ua,"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "schematic representation is provided in the Figure 2.",
          "in all frames corresponding to one video:": "(5)"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "˜"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "V = V + tanh(α) ∗ Uv."
        },
        {
          "prompts has been shown to be successful\nin few applica-": "Given a video representation V = {v}t corresponding",
          "in all frames corresponding to one video:": ""
        },
        {
          "prompts has been shown to be successful\nin few applica-": "to t frames, and an audio sequence A, we first project each",
          "in all frames corresponding to one video:": "We initialize α to zero such that\nthe modality encoders re-"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "of them to a low-dimensional\nlatent space to obtain corre-",
          "in all frames corresponding to one video:": "ceive the original (unimodal) feature representations in the"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "sponding low-dimensional representations ˆV and ˆA:",
          "in all frames corresponding to one video:": "first iterations of the training, and the appropriate magnitude"
        },
        {
          "prompts has been shown to be successful\nin few applica-": "",
          "in all frames corresponding to one video:": "of multimodal features is progressively learnt by the model,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: As can be seen, MMA-DFER outperforms the",
      "data": [
        {
          "images lacks such capability. We address this challenge by": "employing a Multimodal Temporal Transformer.",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "We follow the traditional setup of extracting 16 frames to"
        },
        {
          "images lacks such capability. We address this challenge by": "Recall\nthat\nin the image encoder, given a sequence of t",
          "4.2. Experimental setup": "form a video sequence during training [8, 40]. We notice the"
        },
        {
          "images lacks such capability. We address this challenge by": "frames of a video, we concatenate them in a batch manner",
          "4.2. Experimental setup": "discrepancies in image sizes in the experimental setup of ex-"
        },
        {
          "images lacks such capability. We address this challenge by": "and process independently in a parallel manner. Following",
          "4.2. Experimental setup": "isting methods,\ntherefore report results on 112x112 images"
        },
        {
          "images lacks such capability. We address this challenge by": "this stage, we extract\nthe [CLS]\ntoken of every frame cor-",
          "4.2. Experimental setup": "[29], 160x160 images [40], and on 224x224 images [8]. For"
        },
        {
          "images lacks such capability. We address this challenge by": "responding to the same video and concatenate them to form",
          "4.2. Experimental setup": "MAFW, we extract faces with MTCNN [50]. We use learn-"
        },
        {
          "images lacks such capability. We address this challenge by": "a temporal sequence. Therefore,\nthe input\ntensor in the vi-",
          "4.2. Experimental setup": "ing rate of 1e-4 that is annealed via cosine schedule to 0 over"
        },
        {
          "images lacks such capability. We address this challenge by": "sion branch is reshaped from B ∗ t × N × D to B × t × D,",
          "4.2. Experimental setup": "25 epochs. We use batch size of 8 and weight decay of 1e-2,"
        },
        {
          "images lacks such capability. We address this challenge by": "where B is the batch size, t is the temporal length (number",
          "4.2. Experimental setup": "and AdamW optimizer with default parameters. We fix the"
        },
        {
          "images lacks such capability. We address this challenge by": "of frames), N is the sequence length of each frame (num-",
          "4.2. Experimental setup": "random seed to 1. In unimodal encoders, we interpolate the"
        },
        {
          "images lacks such capability. We address this challenge by": "ber of patches), and D is the dimensionality. We addition-",
          "4.2. Experimental setup": "positional embeddings to new sequence lengths and keep"
        },
        {
          "images lacks such capability. We address this challenge by": "ally fuse the corresponding [CLS] token of audio branch to",
          "4.2. Experimental setup": "them tunable to adapt to the new resolutions, the rest of uni-"
        },
        {
          "images lacks such capability. We address this challenge by": "the video sequence via addition, and process the new multi-",
          "4.2. Experimental setup": "modal encoder parameters remain frozen. Therefore,\ntun-"
        },
        {
          "images lacks such capability. We address this challenge by": "modal sequence with a Joint Adaptation Module, which in",
          "4.2. Experimental setup": "able parameters include the Fusion Bottleneck blocks, Mul-"
        },
        {
          "images lacks such capability. We address this challenge by": "our\nimplementation is formalized as a single Linear\nlayer",
          "4.2. Experimental setup": "timodal Temporal Transformer,\nlearnable prompts, classi-"
        },
        {
          "images lacks such capability. We address this challenge by": "(with slightly reduced dimensionality than prior encoders).",
          "4.2. Experimental setup": "fier and positional embeddings,\ntotaling\n7.5M parameters."
        },
        {
          "images lacks such capability. We address this challenge by": "Further, we add learnable temporal embeddings to the mul-",
          "4.2. Experimental setup": "For comparison, S2D that\nfollows a similar\nframework of"
        },
        {
          "images lacks such capability. We address this challenge by": "timodal\ntemporal sequence and concatenate a new [CLS]",
          "4.2. Experimental setup": "adapting pre-trained (for static emotion recognition in their"
        },
        {
          "images lacks such capability. We address this challenge by": "token. We process the new sequence with a Transformer",
          "4.2. Experimental setup": "case) network, includes 9M tunable parameters."
        },
        {
          "images lacks such capability. We address this challenge by": "block, which is now operating on temporal\nlevel on joint",
          "4.2. Experimental setup": "Multimodal Temporal Transformer\nis a 1-layer Trans-"
        },
        {
          "images lacks such capability. We address this challenge by": "multimodal sequence, hence we refer\nto it as Multimodal",
          "4.2. Experimental setup": "former with hidden dimension of 512 and 8 heads. Fusion"
        },
        {
          "images lacks such capability. We address this challenge by": "Temporal Transformer. The [CLS] token is further used as",
          "4.2. Experimental setup": "Bottleneck latent dimensionality is set\nto 128, and 6 learn-"
        },
        {
          "images lacks such capability. We address this challenge by": "input to the classifier.",
          "4.2. Experimental setup": "able prompts are introduced in each modality, and progres-"
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "sive updates are introduced twice in the network, after 1st"
        },
        {
          "images lacks such capability. We address this challenge by": "We\nalso note\nthat\nsome works have\nsuggested adop-",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "and 7th layers, with 3 tokens each."
        },
        {
          "images lacks such capability. We address this challenge by": "tion of\ntemporal adaptation modules on intermediate fea-",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "We\nfollow the\n5-fold\nexperimental\nprotocol\nin\neach"
        },
        {
          "images lacks such capability. We address this challenge by": "ture level [8], with the aim of enriching image features with",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "dataset with the provided splits. We train the models on the"
        },
        {
          "images lacks such capability. We address this challenge by": "temporal information already at the feature extraction stage.",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "train set and report the result of final checkpoint, i.e., at 25th"
        },
        {
          "images lacks such capability. We address this challenge by": "We perform ablation studies on the placement of our Mul-",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "epoch, on the test set. Training is done on a single Nvidia"
        },
        {
          "images lacks such capability. We address this challenge by": "timodal Temporal Transformer with intermediate temporal",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "Tesla V-100-32 GPU, and single training (of\nsingle fold)"
        },
        {
          "images lacks such capability. We address this challenge by": "modules in experimental section, and find our approach to",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "takes approximately 8 hours on resolution 160. We report"
        },
        {
          "images lacks such capability. We address this challenge by": "be beneficial. This can be partially attributed to fusion mod-",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "results both with following the 16-frame uniform sampling"
        },
        {
          "images lacks such capability. We address this challenge by": "ules that already implicitly embed a certain level of tempo-",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "at\ninference time, as well as widely adopted 2-clip average"
        },
        {
          "images lacks such capability. We address this challenge by": "ral\ninformation at\nintermediate stages by fusing an audio",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "results [8, 40], where two clips are sampled uniformly from"
        },
        {
          "images lacks such capability. We address this challenge by": "sequence to image patches.",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "a single video and predictions are averaged. We note that"
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "we do not observe a significant difference between these"
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "two approaches. As prior works, we report Unweighted Av-"
        },
        {
          "images lacks such capability. We address this challenge by": "4. Experiments",
          "4.2. Experimental setup": "erage Recall (UAR) and Weighted Average Recall (WAR)."
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "4.3. Results"
        },
        {
          "images lacks such capability. We address this challenge by": "4.1. Datasets",
          "4.2. Experimental setup": ""
        },
        {
          "images lacks such capability. We address this challenge by": "",
          "4.2. Experimental setup": "The comparison to state-of-the-art methods is provided in"
        },
        {
          "images lacks such capability. We address this challenge by": "We benchmark our method on two popular multimodal dy-",
          "4.2. Experimental setup": "Table 1.\nAs can be seen, MMA-DFER outperforms\nthe"
        },
        {
          "images lacks such capability. We address this challenge by": "namic\nfacial\nexpression recognition in-the-wild datasets,",
          "4.2. Experimental setup": "competing methods.\nSpecifically, MMA-DFER outper-"
        },
        {
          "images lacks such capability. We address this challenge by": "namely DFEW [19]\nand MAFW [29].\nDFEW consists",
          "4.2. Experimental setup": "forms current\nstate-of-the-art of S2D by 1.5% UAR and"
        },
        {
          "images lacks such capability. We address this challenge by": "of 16,000 audiovisual clips split\ninto 5 folds, posing a 7-",
          "4.2. Experimental setup": "WAR on DFEW dataset and 1% on MAFW. We also note"
        },
        {
          "images lacks such capability. We address this challenge by": "class classification task, with classes being emotion labels",
          "4.2. Experimental setup": "that\nfor S2D,\nthe best UAR and WAR are obtained from"
        },
        {
          "images lacks such capability. We address this challenge by": "of\n‘happy’,\n‘sad’,\n‘angry’,\n‘neutral’,\n‘surprise’,\n‘disgust’,",
          "4.2. Experimental setup": "different models/training strategies (with and without over-"
        },
        {
          "images lacks such capability. We address this challenge by": "and ‘fear’. MAFW contains 10,045 clips and follows a",
          "4.2. Experimental setup": "sampling\nof\nunderrepresented\nclasses), while\nin MMA-"
        },
        {
          "images lacks such capability. We address this challenge by": "5-fold split as well.\nIn this dataset\nset of emotions\nfol-",
          "4.2. Experimental setup": "DFER this is achieved by a single model. Compared to the"
        },
        {
          "images lacks such capability. We address this challenge by": "lows 11 classes: ‘anger’, ‘disgust’, ‘fear’, ‘happiness’, ‘sad-",
          "4.2. Experimental setup": "best multimodal model - HiCMAE, we achieve stronger re-"
        },
        {
          "images lacks such capability. We address this challenge by": "ness’, ‘surprise’, ‘contempt’, ‘anxiety’, ‘helplessness’, ‘dis-",
          "4.2. Experimental setup": "sults as well, both on 224 and 160 resolution. With the same"
        },
        {
          "images lacks such capability. We address this challenge by": "appointment’, and ‘neutral’.",
          "4.2. Experimental setup": "image resolution, we obtain 2-3% improvement on DFEW"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: As can be seen,",
      "data": [
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "67.01\n77.51\n58.52\nMMA (ours) 224\n44.11\nAV\n224",
          "the performance is not directly correlated with the number": "of parameters, but more with the placement of the temporal"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "44.25\nMMA (ours) 224*\n66.85\n77.43\n58.45\nAV\n224",
          "the performance is not directly correlated with the number": "module."
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "Table 1. Comparison to SOTA methods. * denotes mean predic-",
          "the performance is not directly correlated with the number": "4.5. Comparison of fusion approaches"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "tion over two uniformly sampled video clips following [8, 40]. M",
          "the performance is not directly correlated with the number": ""
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "We additionally compare our Fusion Bottleneck blocks with"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "denotes the modality, and Res denotes the image resolution.",
          "the performance is not directly correlated with the number": ""
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "other popular multimodal fusion approaches. Specifically,"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "we compare with:"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "• Addition in original space,\ni.e., without compression and"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "and 1.5% on MAFW.",
          "the performance is not directly correlated with the number": ""
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "expansion (ADD);"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "• Multimodal Transformer following [45], where we intro-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "4.4.\nComparison\nof\ntemporal\nadaptation\nap-",
          "the performance is not directly correlated with the number": ""
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "duce 2 multi-head self-attention blocks, audio-to-vision,"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "proaches",
          "the performance is not directly correlated with the number": ""
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "",
          "the performance is not directly correlated with the number": "and vision-to-audio, outputs of which are added to corre-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "In MMA,\nthe Multimodal Temporal Transformer is placed",
          "the performance is not directly correlated with the number": "sponding branches (MULT);"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "following the unimodal branches. However,\nsome works",
          "the performance is not directly correlated with the number": "• Single multimodal Transformer on concatenated audio"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "have suggested a different approach, where temporal adap-",
          "the performance is not directly correlated with the number": "and image features, with global average pooled represen-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "tation happens already in the intermediate features of\nthe",
          "the performance is not directly correlated with the number": "tation added back to each branch (MULT-concat)."
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "model [8]. We perform an experiment where we add such",
          "the performance is not directly correlated with the number": "• No modality fusion;"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "temporal adaptors in the intermediate steps of the network,",
          "the performance is not directly correlated with the number": "For MULT, we utilize 2 heads with total dimensionality of"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "following each Bottleneck Fusion block.\nSuch temporal",
          "the performance is not directly correlated with the number": "128,\nto match the Fusion Bottleneck dimensionality. We"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "adaptors are similar to TMA described in [8] and in prac-",
          "the performance is not directly correlated with the number": "compare the results on 1st fold of DFEW on 160x160 reso-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "tice we apply temporal self-attention over [CLS] tokens of",
          "the performance is not directly correlated with the number": "lution."
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "every frame and fuse the obtained representation back to the",
          "the performance is not directly correlated with the number": "The results can be seen in Table 3.\nAs can be seen,"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "vision branch. We evaluate intermediate temporal adaptors",
          "the performance is not directly correlated with the number": "our approach outperforms the competing methods by a sig-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "(ITA), Multimodal Temporal Transformer (MTM), and the",
          "the performance is not directly correlated with the number": "nificant margin,\nindicating the\neffectiveness of\nthe pro-"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "combination of the two. In the case where only intermediate",
          "the performance is not directly correlated with the number": "posed Fusion Bottlenecks. Poor performance of MULT and"
        },
        {
          "66.51\n77.10\n44.07\n57.85\nAV\n160\nMMA (ours) 160*": "temporal adaptors are applied, we control for the total num-",
          "the performance is not directly correlated with the number": "MULT-concat can be associated with difficulty of drawing"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: As can be seen,",
      "data": [
        {
          "DFEW": "UAR",
          "MAFW": "WAR",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "ITA, d=64\n63.27\n76.59\n1.3"
        },
        {
          "DFEW": "36.15",
          "MAFW": "29.69",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "ITA, d=128\n64.44\n76.80\n0.8"
        },
        {
          "DFEW": "35.98",
          "MAFW": "32.60",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "ITA, d=256\n63.35\n76.59\n4.3"
        },
        {
          "DFEW": "37.78",
          "MAFW": "34.07",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "MTM + ITA-128\n64.66\n77.36\n2.9"
        },
        {
          "DFEW": "53.77",
          "MAFW": "44.15",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "66.52\n77.92\nMTM (ours)\n2.2"
        },
        {
          "DFEW": "-",
          "MAFW": "48.70",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "42.74",
          "MAFW": "42.25",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "Table 2. Comparison of Multimodal Temporal Transformer vs in-"
        },
        {
          "DFEW": "42.79",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "termediate temporal blocks"
        },
        {
          "DFEW": "46.52",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "53.69",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "51.14",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "ber of parameters and therefore each intermediate block is"
        },
        {
          "DFEW": "-",
          "MAFW": "48.18",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "smaller\nthan our Multimodal Temporal Transformer. We"
        },
        {
          "DFEW": "53.43",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "also provide results with different dimensionalities of inter-"
        },
        {
          "DFEW": "54.58",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "mediate temporal adaptors. We compare the results on 1st"
        },
        {
          "DFEW": "54.21",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "54.48",
          "MAFW": "48.83",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "fold of DFEW on 160x160 resolution."
        },
        {
          "DFEW": "55.71",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "The results can be seen in Table 2, where we report UAR,"
        },
        {
          "DFEW": "56.10",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "WAR, and number of parameters that correspond to tempo-"
        },
        {
          "DFEW": "57.56",
          "MAFW": "-",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "ral processing blocks. We note that the variant with ITA of"
        },
        {
          "DFEW": "57.16",
          "MAFW": "51.15",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "dimensionality 128 has less parameters than ITA-64 due to"
        },
        {
          "DFEW": "59.61",
          "MAFW": "52.55",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "internal dimensionality of Bottleneck Fusion already being"
        },
        {
          "DFEW": "62.83",
          "MAFW": "54.28",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "128,\ntherefore no additional\nlayers need to be introduced"
        },
        {
          "DFEW": "63.41",
          "MAFW": "54.31",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "to this variant\nto project\nto new dimensionality space, un-"
        },
        {
          "DFEW": "63.76",
          "MAFW": "56.17",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "like all other variants. We can see that\nthe best\nresult\nis"
        },
        {
          "DFEW": "65.45",
          "MAFW": "57.37",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "achieved by MTM and the second-best by MTM combined"
        },
        {
          "DFEW": "64.24",
          "MAFW": "56.46",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "with ITA. Among ITA, we observe the variant with d=128"
        },
        {
          "DFEW": "64.35",
          "MAFW": "56.60",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "to be outperforming the competing ones. We also note that"
        },
        {
          "DFEW": "66.61",
          "MAFW": "57.90",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "66.51",
          "MAFW": "57.85",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "the performance is not directly correlated with the number"
        },
        {
          "DFEW": "67.01",
          "MAFW": "58.52",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "of parameters, but more with the placement of the temporal"
        },
        {
          "DFEW": "66.85",
          "MAFW": "58.45",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "module."
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "4.5. Comparison of fusion approaches"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "We additionally compare our Fusion Bottleneck blocks with"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "other popular multimodal fusion approaches. Specifically,"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "we compare with:"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "• Addition in original space,\ni.e., without compression and"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "expansion (ADD);"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "• Multimodal Transformer following [45], where we intro-"
        },
        {
          "DFEW": "of",
          "MAFW": "adaptation",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "duce 2 multi-head self-attention blocks, audio-to-vision,"
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": ""
        },
        {
          "DFEW": "",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "and vision-to-audio, outputs of which are added to corre-"
        },
        {
          "DFEW": "the Multimodal Temporal Transformer is placed",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "sponding branches (MULT);"
        },
        {
          "DFEW": "following the unimodal branches. However,",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "• Single multimodal Transformer on concatenated audio"
        },
        {
          "DFEW": "have suggested a different approach, where temporal adap-",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "and image features, with global average pooled represen-"
        },
        {
          "DFEW": "tation happens already in the intermediate features of",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "tation added back to each branch (MULT-concat)."
        },
        {
          "DFEW": "model [8]. We perform an experiment where we add such",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "• No modality fusion;"
        },
        {
          "DFEW": "temporal adaptors in the intermediate steps of the network,",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "For MULT, we utilize 2 heads with total dimensionality of"
        },
        {
          "DFEW": "following each Bottleneck Fusion block.",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "128,\nto match the Fusion Bottleneck dimensionality. We"
        },
        {
          "DFEW": "adaptors are similar to TMA described in [8] and in prac-",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "compare the results on 1st fold of DFEW on 160x160 reso-"
        },
        {
          "DFEW": "tice we apply temporal self-attention over [CLS] tokens of",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "lution."
        },
        {
          "DFEW": "every frame and fuse the obtained representation back to the",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "The results can be seen in Table 3.\nAs can be seen,"
        },
        {
          "DFEW": "vision branch. We evaluate intermediate temporal adaptors",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "our approach outperforms the competing methods by a sig-"
        },
        {
          "DFEW": "(ITA), Multimodal Temporal Transformer (MTM), and the",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "nificant margin,\nindicating the\neffectiveness of\nthe pro-"
        },
        {
          "DFEW": "combination of the two. In the case where only intermediate",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "posed Fusion Bottlenecks. Poor performance of MULT and"
        },
        {
          "DFEW": "temporal adaptors are applied, we control for the total num-",
          "MAFW": "",
          "Temporal adaptation\nUAR\nWAR\nParams (M)": "MULT-concat can be associated with difficulty of drawing"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Here, ‘Pr.’ cor-",
      "data": [
        {
          "fusion methods": "None",
          "UAR": "26.43",
          "WAR": "33.53"
        },
        {
          "fusion methods": "MULT",
          "UAR": "54.34",
          "WAR": "67.15"
        },
        {
          "fusion methods": "MULT-concat",
          "UAR": "66.52",
          "WAR": "77.92"
        },
        {
          "fusion methods": "ADD",
          "UAR": "",
          "WAR": ""
        },
        {
          "fusion methods": "",
          "UAR": "",
          "WAR": ""
        },
        {
          "fusion methods": "Fusion Bottleneck (ours)",
          "UAR": "",
          "WAR": ""
        },
        {
          "fusion methods": "",
          "UAR": "",
          "WAR": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Here, ‘Pr.’ cor-",
      "data": [
        {
          "MULT-concat\n54.96\n65.66": "ADD\n59.97\n71.20",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Table 5. Comparison of each modality encoder to the multimodal"
        },
        {
          "MULT-concat\n54.96\n65.66": "66.52\n77.92\nFusion Bottleneck (ours)",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "MMA-DFER"
        },
        {
          "MULT-concat\n54.96\n65.66": "Table 3. Comparison of modality fusion approaches.",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "latent space d\nUAR\nWAR\nParams (M)"
        },
        {
          "MULT-concat\n54.96\n65.66": "Pr\nPr.Pr.\nMTT\nFB\nUAR\nWAR",
          "66.52\n77.92\nMMA-DFER": "64\n64.57\n77.44\n5.9"
        },
        {
          "MULT-concat\n54.96\n65.66": "-\n-\n-\n-\n56.59\n68.43",
          "66.52\n77.92\nMMA-DFER": "128\n66.52\n77.92\n7.5"
        },
        {
          "MULT-concat\n54.96\n65.66": "+\n-\n-\n-\n57.45\n69.33",
          "66.52\n77.92\nMMA-DFER": "78.17\n256\n66.72\n12.3"
        },
        {
          "MULT-concat\n54.96\n65.66": "+\n+\n-\n-\n58.21\n69.34",
          "66.52\n77.92\nMMA-DFER": "66.77\n512\n78.13\n21.7"
        },
        {
          "MULT-concat\n54.96\n65.66": "+\n+\n+\n-\n59.27\n70.56",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Table 6. Dimensionality of the latent space"
        },
        {
          "MULT-concat\n54.96\n65.66": "+\n+\n-\n+\n61.96\n74.58",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "-\n-\n+\n+\n64.62\n77.40",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Pr.Pr. Num.\nUAR\nWAR"
        },
        {
          "MULT-concat\n54.96\n65.66": "66.52\n77.92\n+\n+\n+\n+",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "12\n65.14\n77.87"
        },
        {
          "MULT-concat\n54.96\n65.66": "Table 4. Ablation studies",
          "66.52\n77.92\nMMA-DFER": "6\n65.44\n77.83"
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "4\n64.41\n77.62"
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "66.52\n77.92\n2"
        },
        {
          "MULT-concat\n54.96\n65.66": "dependencies between individual\nframes and whole audio",
          "66.52\n77.92\nMMA-DFER": "0\n64.62\n77.40"
        },
        {
          "MULT-concat\n54.96\n65.66": "spectrogram.",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Table 7. Ablation on the frequency of progressive prompts"
        },
        {
          "MULT-concat\n54.96\n65.66": "4.6. Ablation studies",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "Further, we ablate each component of our model\nindepen-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "image encoders in ViT-base are 768 [11], and we set our"
        },
        {
          "MULT-concat\n54.96\n65.66": "dently, with the results shown in Table 4. Here,\n‘Pr.’\ncor-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "bottleneck space to 128. Here, we evaluate the effect of"
        },
        {
          "MULT-concat\n54.96\n65.66": "responds\nto learnable prompts\nthat are not updated with",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "larger and smaller dimensionalities of the latent space. The"
        },
        {
          "MULT-concat\n54.96\n65.66": "depth but\nthe initialization is kept;\n‘Pr.Pr.’\ncorresponds to",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "results are shown in Table 6. As can be seen,\nlarger val-"
        },
        {
          "MULT-concat\n54.96\n65.66": "the alternative with the progressive prompts, i.e., new learn-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "ues do increase the performance of the model, but require"
        },
        {
          "MULT-concat\n54.96\n65.66": "able prompts are introduced at different depths; ‘MTT’ cor-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "significantly more parameters. We also find that\nincreas-"
        },
        {
          "MULT-concat\n54.96\n65.66": "responds to Multimodal Temporal Transformer, and ‘FB’",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "ing dimensionality beyond d-128 brings rather diminishing"
        },
        {
          "MULT-concat\n54.96\n65.66": "corresponds to Fusion Bottleneck. When no MTT is em-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "returns in terms of UAR and WAR."
        },
        {
          "MULT-concat\n54.96\n65.66": "ployed, prediction is done on averaged features following",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Finally, we also study the frequency of prompt updates in"
        },
        {
          "MULT-concat\n54.96\n65.66": "Joint Adaptation Module.\nJoint Adaptation Module and a",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "the case of progressive prompting. The results are shown in"
        },
        {
          "MULT-concat\n54.96\n65.66": "classifier are present and unfrozen in all variants. We report",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "Table 7. Here, 12 corresponds to introducing new learnable"
        },
        {
          "MULT-concat\n54.96\n65.66": "the results on first fold of DFEW.",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "prompts every layer, 6 every 2nd layer, 2 every 3rd layer,"
        },
        {
          "MULT-concat\n54.96\n65.66": "As can be seen, performance of plain pre-trained mod-",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "2 every 6th layer\n(our final model case), and 0 not using"
        },
        {
          "MULT-concat\n54.96\n65.66": "els is rather poor, and each of the components progressively",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "learnable prompts at all. As can be seen, any number of"
        },
        {
          "MULT-concat\n54.96\n65.66": "improves the performance. The biggest effect is brought by",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "updates results in better accuracy than no prompts, with the"
        },
        {
          "MULT-concat\n54.96\n65.66": "Fusion Bottleneck which improves WAR by 5% and UAR",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "best result achieved when introducing them twice."
        },
        {
          "MULT-concat\n54.96\n65.66": "by 3.7% if comparing variants without MTT. We also note",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "how performance increases by using MTT by 1.5%, but",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "",
          "66.52\n77.92\nMMA-DFER": "5. Conclusion"
        },
        {
          "MULT-concat\n54.96\n65.66": "when used in conjunction with Fusion Bottlenecks, MTT",
          "66.52\n77.92\nMMA-DFER": ""
        },
        {
          "MULT-concat\n54.96\n65.66": "improves the performance by 3.5%. This shows that fusion",
          "66.52\n77.92\nMMA-DFER": "We have investigated adaptation of pre-trained unimodal"
        },
        {
          "MULT-concat\n54.96\n65.66": "of features on intermediate level helps late-stage adaptation",
          "66.52\n77.92\nMMA-DFER": "models for multimodal dynamic facial expression recogni-"
        },
        {
          "MULT-concat\n54.96\n65.66": "that precedes temporal modeling.",
          "66.52\n77.92\nMMA-DFER": "tion in-the-wild. We identified key limitations associated"
        },
        {
          "MULT-concat\n54.96\n65.66": "We further\nstudy the effect of each modality indepen-",
          "66.52\n77.92\nMMA-DFER": "with adapting pre-trained models for this task, namely intra-"
        },
        {
          "MULT-concat\n54.96\n65.66": "dently. Here,\nin the unimodal cases each branch is frozen",
          "66.52\n77.92\nMMA-DFER": "modality adaptation, cross-modal alignment, and temporal"
        },
        {
          "MULT-concat\n54.96\n65.66": "and only the classifier is updated. The results are shown in",
          "66.52\n77.92\nMMA-DFER": "adaptation, and proposed solutions to address them. Our"
        },
        {
          "MULT-concat\n54.96\n65.66": "Table 5. As can be seen, performance of each individual",
          "66.52\n77.92\nMMA-DFER": "proposed model, MMA-DFER sets a new state-of-the art"
        },
        {
          "MULT-concat\n54.96\n65.66": "modality is significantly lower than the combination.",
          "66.52\n77.92\nMMA-DFER": "on two popular DFER benchmarks DFEW and MAFW. Fu-"
        },
        {
          "MULT-concat\n54.96\n65.66": "We additionally study the dimensionality of\nthe bottle-",
          "66.52\n77.92\nMMA-DFER": "ture work could include\nexperimentation with additional"
        },
        {
          "MULT-concat\n54.96\n65.66": "neck space. Recall\nthat dimensionality of both audio and",
          "66.52\n77.92\nMMA-DFER": "unimodal backbones\nand exploitation of\nfurther modali-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "spaces.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Dirk Weissenborn,\nXiaohua\nZhai,\nThomas Unterthiner,"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "vain Gelly, et al. An image is worth 16x16 words: Trans-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "6. Acknowledgement",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "arXiv\npreprint\nformers\nfor\nimage\nrecognition\nat\nscale."
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "This work was\npartially\nfunded\nby\nthe NSF CBL and",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "arXiv:2010.11929, 2020. 3, 7"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Business\nFinland\nproject AMALIA,\nand\nby\nthe Hori-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[12]\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "zon\nEurope\nprogramme\nPANDORA (GA 101135775).",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "and Marvin Ritter.\nAudio set: An ontology and human-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "labeled dataset\nfor\naudio events.\nIn 2017 IEEE interna-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "References",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "tional conference on acoustics, speech and signal processing"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "(ICASSP), pages 776–780. IEEE, 2017. 3"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[1] Naveed Ahmed, Zaher Al Aghbari,\nand Shini Girija.\nA",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun."
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "systematic survey on multimodal emotion recognition using",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Deep residual\nlearning for\nimage recognition.\nIn Proceed-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "learning algorithms.\nIntelligent Systems with Applications,",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "ings of the IEEE conference on computer vision and pattern"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "17:200171, 2023. 3",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "recognition, pages 770–778, 2016. 6"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[2]\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Miech,\nIain Barr, Yana Hasson, Karel Lenc, Arthur Men-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Doll´ar, and Ross Girshick. Masked autoencoders are scalable"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "sch, Katherine Millican, Malcolm Reynolds, et al. Flamingo:",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "vision learners.\nIn Proceedings of the IEEE/CVF conference"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Advances\na visual\nlanguage model\nfor\nfew-shot\nlearning.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "on computer vision and pattern recognition, pages 16000–"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "in neural\ninformation processing systems, 35:23716–23736,",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "16009, 2022. 3"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "2022. 4",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[15] WN Hsu, B Bolte, YHH Tsai, K Lakhotia, R Salakhutdinov,"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[3] De˘ger Ayata, Yusuf Yaslan, and Mustafa E Kamasak. Emo-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "and AH Mohamed.\nSelf-supervised speech representation"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "tion recognition from multimodal physiological signals for",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "ACM\nlearning by masked prediction of hidden units.\nieee."
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Journal of Medical and\nemotion aware healthcare systems.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Trans. Audio Speech Lang. Process, 29:3451–3460, 2021. 6"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Biological Engineering, 40:149–157, 2020. 1",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[16]\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[4] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "works.\nIn Proceedings of the IEEE conference on computer"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Mao, Gary Cottrell, and Julian McAuley. Rezero is all you",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "vision and pattern recognition, pages 7132–7141, 2018. 4"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "need:\nFast convergence at\nlarge depth.\nIn Uncertainty in",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[17]\nPo-Yao Huang, Hu Xu,\nJuncheng\nLi, Alexei Baevski,"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Artificial Intelligence, pages 1352–1361. PMLR, 2021. 4",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Michael\nAuli, Wojciech\nGaluba,\nFlorian Metze,\nand"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Christoph Feichtenhofer. Masked autoencoders that\nlisten."
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Michael Auli. wav2vec 2.0: A framework for self-supervised",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Advances\nin Neural\nInformation Processing Systems,\n35:"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "learning of speech representations. Advances in neural infor-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "28708–28720, 2022. 3"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "mation processing systems, 33:12449–12460, 2020. 6",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[6] Carmen Bisogni, Aniello Castiglione, Sanoar Hossain, Fabio",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Narducci, and Saiyed Umer.\nImpact of deep learning ap-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "sual prompt\ntuning.\nIn European Conference on Computer"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "proaches on facial expression recognition in healthcare in-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Vision, pages 709–727. Springer, 2022. 3"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "IEEE Transactions on Industrial\ndustries.\nInformatics, 18",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "(8):5619–5627, 2022. 1",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[19] Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Tang, Wanchuang Xia, Cheng Lu, and Jiateng Liu. Dfew: A"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[7]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu,",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "large-scale database for recognizing dynamic facial expres-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "the 28th ACM Interna-\nsions in the wild.\nIn Proceedings of"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Yoshioka, Xiong Xiao,\net\nal.\nWavlm:\nLarge-scale\nself-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "tional Conference on Multimedia, pages 2881–2889, 2020."
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "supervised\npre-training\nfor\nfull\nstack\nspeech\nprocessing.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "1, 2, 5"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "IEEE Journal of Selected Topics\nin Signal Processing, 16",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "(6):1505–1518, 2022. 6",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[20] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Maaz, Salman Khan,\nand Fahad Shahbaz Khan.\nMaple:"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[8] Yin Chen, Jia Li, Shiguang Shan, Meng Wang, and Richang",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Proceedings\nof\nthe\nMulti-modal\nprompt\nlearning.\nIn"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Hong.\nFrom static to dynamic: Adapting landmark-aware",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "IEEE/CVF Conference\non Computer Vision\nand Pattern"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "image models\nfor\nfacial expression recognition in videos.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Recognition, pages 19113–19122, 2023. 3"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "arXiv preprint arXiv:2312.05447, 2023. 1, 2, 3, 5, 6",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": ""
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[9] M Kalpana Chowdary, Tu N Nguyen, and D Jude Hemanth.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[21] Diederik P Kingma and Max Welling. Auto-encoding varia-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Deep learning-based facial emotion recognition for human–",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Neural Computing and\ncomputer\ninteraction applications.",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[22] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "Applications, 35(32):23311–23328, 2023. 1",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal es-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "[10] Kateryna Chumachenko, Alexandros\nIosifidis,\nand Mon-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "timation, expression recognition, action unit detection and"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "cef Gabbouj.\nSelf-attention fusion for\naudiovisual\nemo-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "emotional reaction intensity estimation challenges.\nIn Pro-"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "tion recognition with incomplete data.\nIn 2022 26th Inter-",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "ceedings of\nthe IEEE/CVF Conference on Computer Vision"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "national Conference on Pattern Recognition (ICPR), pages",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "and Pattern Recognition, pages 5888–5897, 2023. 2"
        },
        {
          "ties/sensors,\nsuch as\nlandmarks or vision-language latent": "2822–2828. IEEE, 2022. 3",
          "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,": "[23] Hanting Li, Mingzhe Sui, Zhaoqing Zhu, et al. Nr-dfernet:"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Noise-robust network for dynamic facial expression recogni-": "tion. arXiv preprint arXiv:2206.04975, 2022. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "view of multimodal emotion recognition from datasets, pre-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "processing, features, and fusion methods. Neurocomputing,"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[24] Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "page 126866, 2023. 3"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Intensity-aware loss for dynamic facial expression recogni-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "the AAAI Conference on\ntion in the wild.\nIn Proceedings of",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[38] Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. Mae-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Artificial Intelligence, pages 67–75, 2023. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "dfer: Efficient masked autoencoder\nfor self-supervised dy-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "the\nnamic facial expression recognition.\nIn Proceedings of"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[25] Hanting\nLi,\nHongjing Niu,\nZhaoqing\nZhu,\nand\nFeng",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "31st ACM International Conference on Multimedia, pages"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Zhao.\nCliper:\nA unified vision-language\nframework for",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "6110–6121, 2023. 2, 3, 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "arXiv preprint\nin-the-wild facial\nexpression recognition.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "arXiv:2303.00193, 2023. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[39] Licai Sun, Zheng Lian, Kexin Wang, Yu He, Mingyu Xu,"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Haiyang Sun, Bin Liu,\nand Jianhua Tao.\nSvfap:\nSelf-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[26] Xiang Lisa Li and Percy Liang.\nPrefix-tuning: Optimiz-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "arXiv\npreprint\nsupervised\nvideo\nfacial\naffect\nperceiver."
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "arXiv\npreprint\ning\ncontinuous\nprompts\nfor\ngeneration.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "arXiv:2401.00416, 2023. 2, 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "arXiv:2101.00190, 2021. 3",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[40] Licai Sun, Zheng Lian, Bin Liu,\nand Jianhua Tao.\nHic-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[27]\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "mae: Hierarchical contrastive masked autoencoder for self-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "roaki Hayashi, and Graham Neubig. Pre-train, prompt, and",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "supervised audio-visual emotion recognition. arXiv preprint"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "predict: A systematic survey of prompting methods in nat-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "arXiv:2401.05698, 2024. 1, 2, 3, 5, 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "ural\nlanguage processing. ACM Computing Surveys, 55(9):",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[41] Mani Kumar Tellamekala, Shahin Amiriparian, Bj¨orn W"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "1–35, 2023.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Schuller, Elisabeth Andr´e, Timo Giesbrecht, and Michel Val-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[28] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengx-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "star. Cold fusion: Calibrated and ordinal\nlatent distribution"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "iao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2:\nPrompt",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "fusion for uncertainty-aware multimodal emotion recogni-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "tuning can be comparable to fine-tuning universally across",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "IEEE Transactions on Pattern Analysis and Machine\ntion."
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "scales and tasks. arXiv preprint arXiv:2110.07602, 2021. 3",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Intelligence, 2023. 3"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[29] Yuanyuan Liu, Wei Dai, Chuanxu Feng, Wenbin Wang,",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[42] Aitor Toichoa Eyam, Wael M Mohammed, and Jose L Mar-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Guanghao Yin, Jiabei Zeng, and Shiguang Shan. Mafw: A",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "tinez Lastra. Emotion-driven analysis and control of human-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "large-scale, multi-modal,\ncompound affective database for",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "robot\ninteractions in collaborative applications. Sensors, 21"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "dynamic facial expression recognition in the wild.\nIn Pro-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "(14):4626, 2021. 1"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "ceedings of the 30th ACM International Conference on Mul-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[43] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "timedia (MM’22). 1, 2, 5, 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "and Manohar Paluri. Learning spatiotemporal features with"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[30] Yuanyuan Liu, Chuanxu Feng, Xiaohui Yuan, Lin Zhou,",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "3d convolutional networks. In Proceedings of the IEEE inter-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Wenbin Wang, Jie Qin, and Zhongwen Luo. Clip-aware ex-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "national conference on computer vision, pages 4489–4497,"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "pressive feature learning for video-based facial expression",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "2015. 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "recognition.\nInformation Sciences, 598:182–195, 2022. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[44] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[31] Yuanyuan Liu, Wenbin Wang, Chuanxu Feng, Haoyu Zhang,",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "LeCun, and Manohar Paluri. A closer look at spatiotemporal"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Zhe Chen, and Yibing Zhan. Expression snippet transformer",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "the\nconvolutions for action recognition.\nIn Proceedings of"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "for robust video-based facial expression recognition. Pattern",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "IEEE conference on Computer Vision and Pattern Recogni-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Recognition, 138:109368, 2023. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "tion, pages 6450–6459, 2018. 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[32] Zhentao Liu, Min Wu, Weihua Cao, Luefeng Chen, Jianping",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[45] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Xu, Ri Zhang, Mengtian Zhou, and Junwei Mao. A facial ex-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov."
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "pression emotion recognition based human-robot interaction",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Multimodal\ntransformer for unaligned multimodal\nlanguage"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "system.\nIEEE CAA J. Autom. Sinica, 4(4):668–676, 2017. 1",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "sequences. In Proceedings of the conference. Association for"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[33] Bowen Ma, Wei Zhang, Feng Qiu, and Yu Ding. A unified",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "computational\nlinguistics. Meeting, page 6558. NIH Public"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "approach to facial affect analysis:\nthe mae-face visual\nrep-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Access, 2019. 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "resentation.\nIn Proceedings of the IEEE/CVF conference on",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "computer vision and pattern recognition, pages 5923–5932,",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "2023. 3",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Polosukhin. Attention is all you need. Advances in neural"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[34]\nFuyan Ma, Bin Sun, and Shutao Li. Spatio-temporal\ntrans-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "information processing systems, 30, 2017. 3"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "former for dynamic facial expression recognition in the wild.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[47] Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu,"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "arXiv preprint arXiv:2205.04749, 2022. 6",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Shouhong Ding, and Aimin Zhou. Rethinking the learning"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[35] Bogdan Mocanu, Ruxandra Tapu, and Titus Zaharia. Mul-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "paradigm for dynamic facial expression recognition.\nIn Pro-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "timodal emotion recognition using cross modal audio-video",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "ceedings of\nthe IEEE/CVF Conference on Computer Vision"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Image and\nfusion with attention and deep metric learning.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "and Pattern Recognition, pages 17958–17968, 2023. 6"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Vision Computing, 133:104676, 2023. 3",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": ""
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "[48]\nSeunghyun Yoon, Subhadeep Dey, Hwanhee Lee, and Ky-"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "[36] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "omin\nJung.\nAttentive modality\nhopping mechanism for"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "hoor. Affectnet: A database for facial expression, valence,",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "speech emotion recognition.\nIn ICASSP 2020-2020 IEEE"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "IEEE Transactions on\nand arousal computing in the wild.",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "International Conference on Acoustics, Speech and Signal"
        },
        {
          "Noise-robust network for dynamic facial expression recogni-": "Affective Computing, 10(1):18–31, 2017. 1",
          "[37] Bei Pan, Kaoru Hirota, Zhiyang Jia, and Yaping Dai. A re-": "Processing (ICASSP), pages 3362–3366. IEEE, 2020. 6"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "gang Minker, and Rosalind W Picard. Driver emotion recog-"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "ACM Computing\nnition for\nintelligent vehicles: A survey."
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "Surveys (CSUR), 53(3):1–30, 2020. 1"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "[50] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao."
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "Joint face detection and alignment using multitask cascaded"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "convolutional networks.\nIEEE signal processing letters, 23"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "(10):1499–1503, 2016. 5"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "[51] Xiaoqin Zhang, Min Li, Sheng Lin, Hang Xu, and Guobao"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "Xiao. Transformer-based multimodal emotional perception"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "IEEE\nfor dynamic facial expression recognition in the wild."
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "Transactions on Circuits and Systems for Video Technology,"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "2023. 6"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "[52] Zengqun Zhao and Qingshan Liu.\nFormer-dfer: Dynamic"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "facial expression recognition transformer.\nIn Proceedings"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "of\nthe 29th ACM International Conference on Multimedia,"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "pages 1553–1561, 2021. 6"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "[53] Zengqun Zhao\nand\nIoannis\nPatras.\nPrompting\nvisual-"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "language models for dynamic facial expression recognition."
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "arXiv preprint arXiv:2308.13382, 2023. 6"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "[54] Zengqun Zhao\nand\nIoannis\nPatras.\nPrompting\nvisual-"
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "language models for dynamic facial expression recognition."
        },
        {
          "[49]\nSebastian Zepf, Javier Hernandez, Alexander Schmitt, Wolf-": "arXiv preprint arXiv:2308.13382, 2023. 2, 3"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "Naveed Ahmed",
        "Zaher Al Aghbari",
        "Shini Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "2",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "Deger Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "4",
      "title": "Rezero is all you need: Fast convergence at large depth",
      "authors": [
        "Thomas Bachlechner",
        "Prasad Bodhisattwa",
        "Henry Majumder",
        "Gary Mao",
        "Julian Cottrell",
        "Mcauley"
      ],
      "venue": "Uncertainty in Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Impact of deep learning approaches on facial expression recognition in healthcare industries",
      "authors": [
        "Carmen Bisogni",
        "Aniello Castiglione",
        "Sanoar Hossain",
        "Fabio Narducci",
        "Saiyed Umer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "7",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2006",
      "venue": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "arxiv": "arXiv:2312.05447"
    },
    {
      "citation_id": "9",
      "title": "Deep learning-based facial emotion recognition for humancomputer interaction applications",
      "authors": [
        "Chowdary Kalpana",
        "Tu Nguyen",
        "Jude Hemanth"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "10",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "Kateryna Chumachenko",
        "Alexandros Iosifidis",
        "Moncef Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "11",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "12",
      "title": "Audio set: An ontology and humanlabeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "B Hsu",
        "Yhh Bolte",
        "K Tsai",
        "R Lakhotia",
        "Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "ieee. ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "16",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Masked autoencoders that listen",
      "authors": [
        "Po-Yao Huang",
        "Hu Xu",
        "Juncheng Li",
        "Alexei Baevski",
        "Michael Auli",
        "Wojciech Galuba",
        "Florian Metze",
        "Christoph Feichtenhofer"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Visual prompt tuning",
      "authors": [
        "Menglin Jia",
        "Luming Tang",
        "Bor-Chun Chen",
        "Claire Cardie",
        "Serge Belongie",
        "Bharath Hariharan",
        "Ser-Nam Lim"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2005",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal prompt learning",
      "authors": [
        "Muhammad Uzair Khattak",
        "Hanoona Rasheed",
        "Muhammad Maaz",
        "Salman Khan",
        "Fahad Shahbaz Khan",
        "Maple"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "22",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection and emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "authors": [
        "Hanting Li",
        "Mingzhe Sui",
        "Zhaoqing Zhu"
      ],
      "year": "2022",
      "venue": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "24",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "arxiv": "arXiv:2303.00193"
    },
    {
      "citation_id": "26",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "Prefix-tuning: Optimizing continuous prompts for generation",
      "arxiv": "arXiv:2101.00190"
    },
    {
      "citation_id": "27",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "28",
      "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "authors": [
        "Xiao Liu",
        "Kaixuan Ji",
        "Yicheng Fu",
        "Weng Lam Tam",
        "Zhengxiao Du",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "arxiv": "arXiv:2110.07602"
    },
    {
      "citation_id": "29",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Clip-aware expressive feature learning for video-based facial expression recognition",
      "authors": [
        "Yuanyuan Liu",
        "Chuanxu Feng",
        "Xiaohui Yuan",
        "Lin Zhou",
        "Wenbin Wang",
        "Jie Qin",
        "Zhongwen Luo"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "31",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Yuanyuan Liu",
        "Wenbin Wang",
        "Chuanxu Feng",
        "Haoyu Zhang",
        "Zhe Chen",
        "Yibing Zhan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "A facial expression emotion recognition based human-robot interaction system",
      "authors": [
        "Zhentao Liu",
        "Min Wu",
        "Weihua Cao",
        "Luefeng Chen",
        "Jianping Xu",
        "Ri Zhang",
        "Mengtian Zhou",
        "Junwei Mao"
      ],
      "year": "2017",
      "venue": "IEEE CAA J. Autom. Sinica"
    },
    {
      "citation_id": "33",
      "title": "A unified approach to facial affect analysis: the mae-face visual representation",
      "authors": [
        "Bowen Ma",
        "Wei Zhang",
        "Feng Qiu",
        "Yu Ding"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2022",
      "venue": "Spatio-temporal transformer for dynamic facial expression recognition in the wild",
      "arxiv": "arXiv:2205.04749"
    },
    {
      "citation_id": "35",
      "title": "Multimodal emotion recognition using cross modal audio-video fusion with attention and deep metric learning",
      "authors": [
        "Bogdan Mocanu",
        "Ruxandra Tapu",
        "Titus Zaharia"
      ],
      "year": "2023",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "36",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
      "authors": [
        "Kaoru Bei Pan",
        "Zhiyang Hirota",
        "Yaping Jia",
        "Dai"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "38",
      "title": "Maedfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Svfap: Selfsupervised video facial affect perceiver",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Kexin Wang",
        "Yu He",
        "Mingyu Xu",
        "Haiyang Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Svfap: Selfsupervised video facial affect perceiver",
      "arxiv": "arXiv:2401.00416"
    },
    {
      "citation_id": "40",
      "title": "Hicmae: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2006",
      "venue": "Hicmae: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "arxiv": "arXiv:2401.05698"
    },
    {
      "citation_id": "41",
      "title": "Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W Björn",
        "Elisabeth Schuller",
        "Timo André",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Emotion-driven analysis and control of humanrobot interactions in collaborative applications",
      "authors": [
        "Aitor Toichoa Eyam",
        "M Wael",
        "Jose L Martinez Mohammed",
        "Lastra"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "43",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "44",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "Hanyang Wang",
        "Bo Li",
        "Shuang Wu",
        "Siyuan Shen",
        "Feng Liu",
        "Shouhong Ding",
        "Aimin Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Attentive modality hopping mechanism for speech emotion recognition",
      "authors": [
        "Seunghyun Yoon",
        "Subhadeep Dey",
        "Hwanhee Lee",
        "Kyomin Jung"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "49",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "Sebastian Zepf",
        "Javier Hernandez",
        "Alexander Schmitt",
        "Wolfgang Minker",
        "Rosalind Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "50",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "51",
      "title": "Transformer-based multimodal emotional perception for dynamic facial expression recognition in the wild",
      "authors": [
        "Xiaoqin Zhang",
        "Min Li",
        "Sheng Lin",
        "Hang Xu",
        "Guobao Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "52",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Prompting visuallanguage models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visuallanguage models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "54",
      "title": "Prompting visuallanguage models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "Prompting visuallanguage models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    }
  ]
}