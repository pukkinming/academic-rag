{
  "paper_id": "2508.14920v1",
  "title": "Human Feedback Driven Dynamic Speech Emotion Recognition",
  "published": "2025-08-18T17:25:27Z",
  "authors": [
    "Ilya Fedorov",
    "Dmitry Korobchenko"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work proposes to explore a new area of dynamic speech emotion recognition. Unlike traditional methods, we assume that each audio track is associated with a sequence of emotions active at different moments in time. The study particularly focuses on the animation of emotional 3D avatars. We propose a multi-stage method that includes the training of a classical speech emotion recognition model, synthetic generation of emotional sequences, and further model improvement based on human feedback. Additionally, we introduce a novel approach to modeling emotional mixtures based on the Dirichlet distribution. The models are evaluated based on ground-truth emotions extracted from a dataset of 3D facial animations. We compare our models against the sliding window approach. Our experimental results show the effectiveness of Dirichlet-based approach in modeling emotional mixtures. Incorporating human feedback further improves the model quality while providing a simplified annotation procedure.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) plays an important role in bridging human-computer interaction by analyzing vocal expressions to determine emotional states. Traditionally, SER assigns static emotional labels to speech recordings, facilitating applications for a wide range of sectors, from customer service to healthcare and education. This static approach, while useful, often fails to capture the fluid nature of emotional expressions over time, limiting its applicability in scenarios requiring detailed emotional understanding.\n\nOne such application emerging from recent advancements in Large Language Models (LLMs) is the creation of intelligent conversational agents. These agents, when embodied as 3D digital humans, can serve as advanced non-player characters (NPCs) in video games, providing dynamic and responsive interaction. Additionally, they can function as virtual assistants in educational or therapeutic settings, offering personalized support. This technological leap not only enriches user experience across various domains but also sets a new standard for the integration of emotional intelligence in AI systems.\n\nWe explore the realism of such digital humans with a pretrained NVIDIA Audio2Face network  [1]  designed for animating 3D characters. This neural network generates facial animations, including movements of the lips, eyebrows, eyes, and cheeks, by processing two key inputs: the audio of human speech and its corresponding sequence of emotional states. Building upon this setup, we introduce Dynamic Speech Emotion Recognition (DSER), a novel approach that predicts sequences of emotions over time from audio. DSER extends beyond traditional SER methods, offering a new dimension of emotional intelligence to the animation of 3D characters, achieving a level of realism previously unattainable.\n\nThe development and training of DSER models face significant challenges due to the complex data labeling procedure. For a classic supervised learning approach, manually assigning emotional labels to numerous time stamps for each training audio is required. This is a meticulous process, requiring the annotator to carefully label emotions at each moment in time. Additionally, the ambiguity of emotions further complicates this process.\n\nGiven these data labeling challenges, we opted to fine-tune our models using human feedback. This approach has significantly advanced the field of LLMs, offering a substantial advantage in situations where ground truth labels are ambiguous, making it easier to identify the desired behavior rather than demonstrate it explicitly.\n\nIn this study, we propose a multi-step pipeline for training a Dynamic Speech Emotion Recognition system. The process includes classic SER model training, synthetic sequenceto-sequence data generation for DSER based on the sliding window approach, and fine-tuning of the model with human feedback using the Direct Preference Optimization algorithm  [2] . Additionally, we introduce a novel approach to formalizing the SER problem statement through Dirichlet distribution modeling. Our models are evaluated on a small dataset of emotional sequences extracted via an optimization process from a facial animation dataset. We compare our method with the sliding window approach. The results show that the proposed methods significantly improve the quality of the prediction compared to heuristics.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Our exploration into Dynamic Speech Emotion Recognition (DSER) leverages the Wav2Vec2.0 architecture  [3] , demonstrating significant advancements over traditional neural network models like CNNs and LSTMs in predicting emotions  [4] . Transformer-based architectures, including Wav2Vec2.0 and HuBERT  [5] , have been previously explored for emotion recognition in  [6, 7] .\n\nWe actively utilized the NVIDIA Audio2Face  [1]  neural network for visualizing our results and collecting a dataset of human preferences.\n\nWe utilize Direct Preference Optimization (DPO)  [2] , a technique that advances the concept of Reinforcement Learning From Human Feedback (RLHF)  [8, 9] . This approach enables model training directly on human feedback without the necessity of training a separate reward model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "Our method is multi-staged. In the first stage, we train a neural network to predict a singular emotional representation for the entire audio track. In the second stage, we train a sequence-tosequence model capable of predicting a sequence of emotions in one forward run, utilizing synthetic data generated by the model trained in the first stage. In the final third stage, we further finetune the model from the second stage using human feedback. These stages are illustrated in Figure  1 .\n\nBefore delving into the detailed description of these stages, let us first consider our approach to formalizing the task of emotion recognition. In order to visualize the results of our experiments, we utilized a neural network to generate 3D facial animations based on voice audio and a sequence of emotions. This sequence is represented by numerical vectors ranging from 0 to 1, where each vector corresponds to a continuous description of an emotion at a specific time point. Each scalar within these vectors represents the degree of a particular emotion's activity, such as anger or joy. In this context, our interest lay in creating a probabilistic model for a mixture of emotions rather than probabilities of marginal categories. Unlike traditional classification tasks that presuppose a direct link between objects and distinct classes, we aim to associate an object with a categorical distribution reflecting a mixture of emotions. Formally, our goal is to develop a probabilistic model that estimates not the marginal probabilities of classes, p(emotion | audio), emotion ∈ {anger, sadness, . . . , joy}, but rather the joint probability of a mixture of emotions,\n\nWe propose modeling emotion mixtures using the Dirichlet distribution, an N-dimensional continuous distribution parameterized by N positive values, α = {αi > 0} N i=1 , with its domain being a simplex:\n\nThis formulation implies that a sample from the Dirichlet distribution represents a categorical distribution. Thus, the mixtures of emotions we aim to model are effectively samples from a specific Dirichlet distribution. To define this distribution, we predict its parameters using a neural network f with parameters θ:\n\nThe model f θ is trained by maximizing the likelihood across the training dataset {(audioi, emotioni)} K i=1 :\n\nA model trained in this manner enables the estimation of probabilities for any emotion mixture. To infer a specific prediction, we use the expectation of the Dirichlet distribution:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modeling Singular Emotional Representation",
      "text": "In the first stage, we train the model to predict a singular mixture of emotions for the entire audio track. We solve this problem using classic supervised approach with objective (5). We utilized a dataset consisting of (audio, emotion) pairs, where the emotion is represented as a discrete one-hot encoded label. We used a set of six emotional categories: anger, disgust, fear, joy, neutral, and sadness. This selection was based on the availability of datasets in the public domain.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modeling Sequence Of Emotions",
      "text": "In dynamic speech emotion recognition, we aim to evolve from identifying a singular emotional vector for the entire audio track to predicting a sequence of emotions with corresponding time stamps. The obvious method for doing this transition is through the sliding window approach. This technique involves applying a model, trained at the 1st stage, to small and uniformly spaced segments of the audio, thereby constructing an emotional timeline. However, this method encounters two primary challenges: the limited contextual understanding due to the model processing only short waveform segments; and the inherent assumption of a fixed grid for the windows, restricting the time stamp placement of the emotions and requiring a search procedure to determine the optimal window size and stride.\n\nTo overcome these limitations, we suggest training a model capable of directly predicting an entire emotional sequence in one forward pass. This allows the model to observe the full context of the audio and precisely decide when to change the emotion. Due to the lack of datasets with time-specific emotion annotations, we created synthetic data through the sliding window method for this purpose. We applied the model trained on the first stage to generate such samples and then trained a new model to reproduce this behaviour.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Incorporating Human Feedback",
      "text": "At the final stage, we further fine-tune the model trained on the 2nd stage via learning from the human feedback paradigm. To do this, we generated multiple emotional sequences for each audio track, fed these into a facial animation neural network, and rendered videos showcasing digital humans exhibiting various emotional sequences. We then annotated these video pairs by selecting the preferred ones.\n\nAfter the creation of the dataset, we fine-tuned the sequence-to-sequence model from the 2nd stage using Direct Preference Optimization (DPO) algorithm. The method utilizes the following loss for optimization:\n\nIn this formula, the expectation is taken over triplets (yw, y l , x) -the preferred emotion, the dispreferred one, and the audio accordingly. Dir θ (yw | x) is the model to be trained w.r.t. the parameters θ, while Dirref(y | x) is the reference model explained in detail in the DPO paper. We used the frozen copy of the model from the 2nd stage as the reference model. The β parameter controls how different the trainable model's behavior can be from the reference model. The loss (  7 ) is calculated for each emotion in the sequence and then averaged. This formula makes it clear why the approach with the Dirichlet distribution was developed: if we only had a probabilistic model for marginal emotions, rather than their mixtures, we would have been unable to express the joint probability used in the formula. of audio with an average duration of 3 seconds per audio clip. All recordings are in English and feature a gender balance of approximately 1:1.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "To achieve a better assessment of model accuracy, we employed cross-dataset validation. We independently collected a test dataset consisting of 516 samples, with 86 samples for each emotion. The audio recordings were collected from 7 speakers, each of whom recorded half of the samples in English and the other half in their native language. We used standard classification accuracy as the metric for evaluation.\n\nStage 2. To generate data with emotions that change over time we require an emotionally dynamic dataset. Standard datasets used in the first stage are unsuitable as they feature audio samples with a mostly static emotion. Instead, we utilized the VoxMovies  [13, 14]  collection, which comprises audio tracks of dialogues from movies and TV shows. We used a sliding window with a size of 1.4 sec and a step of 1 sec to predict the sequence of emotions for each audio track in this dataset, yielding a total of 676 sequence-to-sequence samples.\n\nAssessing such a model poses a significant challenge. One approach to do that is to use the standard method of a hold-out sample and compare the predictions with pre-generated data. However, in this scenario, the metric values would only indicate how well the model replicates the sliding window method.\n\nAs previously mentioned, we had a pre-trained facial animation neural network capable of generating 3D digital avatars based on input audio and a sequence of emotions. Additionally, we had a small dataset of 3D facial animations and corresponding audio tracks. To create a ground truth dataset for evaluating our sequence-to-sequence model, we extracted emotional sequences from this dataset by optimizing the NVIDIA Audio2Face network (A2F) w.r.t. the input emotion. Namely, we froze the weights of A2F and found emotional sequences that minimized the Mean Squared Error (MSE) between the ground truth animation and the prediction by A2F. Formally, we solved the following optimization problem for each frame in the animation: emotion * = arg min emotion MSE(A2F(audio, emotion), GT).  (8)  Using this approach, we extracted 40 emotional sequences directly from the ground-truth facial movements. While this data is insufficient for the training of a DSER model, it is enough for assessing its quality. We do not disclose this dataset due to commerical interest.\n\nAs a metric, Mean Absolute Error (MAE) was employed. The use of the Kullback-Leibler divergence was limited by the fact that the animating neural network utilized a different emotional space, notably where the zero emotional vector corresponded to a neutral emotion.\n\nStage 3. To gather the human feedback, we utilized the manually filtered MELD  [15]  dataset. This dataset represents speech recordings from a comedy show. Because of that it contains a lot of samples with background laughter, biasing the prediction towards the joy emotion. We removed those samples, resulting in a total of 400 samples. For each audio track included, we generated 5 distinct sequences of emotions. Randomization involved applying the model from the 1st stage to the samples using the sliding window approach with varying window sizes and strides. These parameters were randomly selected from a range between 0.25 sec to 1.25 sec, under the condition that similar parameters could not repeat, to increase the diversity of generated samples. Subsequently, we rendered videos for each sequence of emotions. For the same audio, the annotator was shown 2 videos. The goal was to choose the one where the sequence of emotions was more preferable. In the result, we obtained 1500 samples of side-by-side comparisons of emotional sequences. We do not disclose this dataset due to commercial interests.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture.",
      "text": "In all our experiments we used Wav2Vec2.0. This model processes the audio waveform to generate a sequence of features.\n\nAt the 1st stage we pool these features by averaging across time, creating a single vector for each audio clip. At the 2nd and 3rd stages we instead apply sliding window based smoothing with kernel=5 and stride=1. After that we apply a fully connected layer to reduce the feature dimensions to 6, matching the number of target emotions. These outputs are then transformed to positive values suitable for the Dirichlet distribution parameters via mapping g(x) = x 2 + 10 -6 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Details",
      "text": "We experimented with two pretrained Wav2Vec2.0 checkpoints available through the Hugging Face  [16]  platform: facebook/wav2vec2-base (94M parameters) and facebook/wav2vec2-large-lv60 (315M parameters). Hereafter, we will refer to these models as Base and Large, respectively.\n\nWe conducted approximately 200 runs to find the optimal hyperparameters for the 1st stage model, as this model was subsequently used for generating synthetic data. Validation was performed on a hold-out portion of our collected dataset. Over- all, the model proved to be quite resilient to changes in hyperparameters. In the best checkpoint, we employed the following setup: 100 epochs, Adam optimizer with learning rate decaying from 5 × 10 -5 to 10 -5 , a batch size of 16, and a weight decay of 10 -4 . Additionally, we utilized augmentations such as audio shifting, cropping and noise injection. For subsequent stages, we used the same parameters. For all the experiments we resampled the input audio samples to 16 kHz.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Tables  2  and 3  show the evaluation of the single emotion and sequence-to-sequence models, respectively. While the traditional cross-entropy loss shows slightly better results in terms of accuracy when predicting emotion for the entire audio, the Dirichlet method results in better metric values for sequential mixture modeling. The incorporation of the human feedback further improves the result, providing state-of-the art quality.\n\nWe examined the effect of the regularization parameter β in the loss  (7) , noting that higher β values align the model more closely with the reference model, as posited by DPO theory. This correlation was confirmed through empirical observation. Furthermore, excessively low β values resulted in the model losing its pretrained characteristics, consequently diminishing performance metrics. Table  4  represents the influence of this parameter for the Large checkpoint. The value β = 0.5 shows the optimal balance between learning from human feedback and utilizing the reference model's knowledge. The reported values represent the best metric scores achieved during training.\n\nDespite the human feedback-based method offering a simpler procedure for annotating and scaling datasets, in practice, it is sometimes challenging to compare the proposed emotional sequences for selection, which reduces the efficiency of labeling. This can be viewed as the method's limitation.\n\nOur approach was trained and evaluated on a single node with an NVIDIA RTX 3090 GPU. The training of each stage takes 1-2 hours. Inference for the largest checkpoint, accelerated with NVIDIA TensorRT in FP16 format, takes about 6 ms for 1 sec of input audio, enabling real-time usage of the model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we introduced Dynamic Speech Emotion Recognition (DSER), a novel approach for over-time speech emotion recognition method suitable for 3D avatar animation. Our approach, leveraging synthetic data generation in conjunction with human feedback fine-tuning, aimed to overcome the challenges in traditional emotion recognition techniques. The results showcase the efficiency of our approach in accurately modeling sequences of emotional mixtures, and highlight the value of human feedback in enhancing model performance. We see further research in scaling datasets and more detailed control over emotion mixture modeling as promising directions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Three stages of the proposed method. At the first stage we train model A to predict a single emotion for the entire track. At",
      "page": 2
    },
    {
      "caption": "Figure 1: Before delving into the detailed description of these stages,",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2NVIDIA, UK": "dkorobchenko@nvidia.com"
        },
        {
          "2NVIDIA, UK": "beyond traditional SER methods,\noffering a new dimension"
        },
        {
          "2NVIDIA, UK": "of emotional\nintelligence to the animation of 3D characters,"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "achieving a level of realism previously unattainable."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "The development and training of DSER models face sig-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "nificant challenges due to the complex data labeling procedure."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "For a classic supervised learning approach, manually assigning"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "emotional labels to numerous time stamps for each training au-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "dio is required. This is a meticulous process, requiring the an-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "notator to carefully label emotions at each moment in time. Ad-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "ditionally,\nthe ambiguity of emotions further complicates this"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "process."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "Given these data labeling challenges, we opted to fine-tune"
        },
        {
          "2NVIDIA, UK": "our models using human feedback. This approach has signif-"
        },
        {
          "2NVIDIA, UK": "icantly advanced the field of LLMs, offering a substantial ad-"
        },
        {
          "2NVIDIA, UK": "vantage in situations where ground truth labels are ambiguous,"
        },
        {
          "2NVIDIA, UK": "making it easier\nto identify the desired behavior\nrather\nthan"
        },
        {
          "2NVIDIA, UK": "demonstrate it explicitly."
        },
        {
          "2NVIDIA, UK": "In this\nstudy, we propose a multi-step pipeline for\ntrain-"
        },
        {
          "2NVIDIA, UK": "ing a Dynamic Speech Emotion Recognition system. The pro-"
        },
        {
          "2NVIDIA, UK": "cess includes classic SER model\ntraining, synthetic sequence-"
        },
        {
          "2NVIDIA, UK": "to-sequence data generation for DSER based on the sliding win-"
        },
        {
          "2NVIDIA, UK": "dow approach, and fine-tuning of the model with human feed-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "back using the Direct Preference Optimization algorithm [2]."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "Additionally, we introduce a novel approach to formalizing the"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "SER problem statement\nthrough Dirichlet distribution model-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "ing. Our models are evaluated on a small dataset of emotional"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "sequences extracted via an optimization process from a facial"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "animation dataset. We compare our method with the sliding"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "window approach. The results show that the proposed methods"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "significantly improve the quality of the prediction compared to"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "heuristics."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "2. Related works"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "Our\nexploration into Dynamic Speech Emotion Recognition"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "(DSER)\nleverages\nthe Wav2Vec2.0 architecture\n[3],\ndemon-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "strating significant advancements over\ntraditional neural net-"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "work models\nlike CNNs and LSTMs\nin predicting emotions"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "[4].\nTransformer-based architectures,\nincluding Wav2Vec2.0"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "and HuBERT [5], have been previously explored for emotion"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "recognition in [6, 7]."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "We actively utilized the NVIDIA Audio2Face [1] neural"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "network for visualizing our\nresults and collecting a dataset of"
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "human preferences."
        },
        {
          "2NVIDIA, UK": ""
        },
        {
          "2NVIDIA, UK": "We utilize Direct Preference Optimization (DPO)\n[2],\na"
        },
        {
          "2NVIDIA, UK": "technique that advances the concept of Reinforcement Learning"
        },
        {
          "2NVIDIA, UK": "From Human Feedback (RLHF) [8, 9]. This approach enables"
        },
        {
          "2NVIDIA, UK": "model\ntraining directly on human feedback without\nthe neces-"
        },
        {
          "2NVIDIA, UK": "sity of training a separate reward model."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the proposed method. At\nFigure 1: Three stages of": "the second stage we use the model A to generate sequential data using the sliding window approach and train model B on this data. At",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "the final stage we further improve the model B by incorporating human feedback.",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "3. Proposed method",
          "the first stage we train model A to predict a single emotion for the entire track. At": "being a simplex:"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "Our method is multi-staged.\nIn the first stage, we train a neural",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "N(cid:89) i\nxαi−1\n,\nx ∈ C,\n(1)\nDir(x | α) ∝"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "network to predict a singular emotional\nrepresentation for\nthe",
          "the first stage we train model A to predict a single emotion for the entire track. At": "i"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "entire audio track.\nIn the second stage, we train a sequence-to-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "=1"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "sequence model capable of predicting a sequence of emotions in",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "one forward run, utilizing synthetic data generated by the model",
          "the first stage we train model A to predict a single emotion for the entire track. At": "N(cid:88) i\n(2)\nxi = 1}.\nC = {(x1, x2, . . . , xN ) ∈ RN | xi > 0,"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "trained in the first stage. In the final third stage, we further fine-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "=1"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "tune the model\nfrom the second stage using human feedback.",
          "the first stage we train model A to predict a single emotion for the entire track. At": "This formulation implies that a sample from the Dirichlet distri-"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "These stages are illustrated in Figure 1.",
          "the first stage we train model A to predict a single emotion for the entire track. At": "bution represents a categorical distribution. Thus,\nthe mixtures"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "Before delving into the detailed description of these stages,",
          "the first stage we train model A to predict a single emotion for the entire track. At": "of emotions we aim to model are effectively samples from a"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "let us first consider our approach to formalizing the task of emo-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "specific Dirichlet distribution.\nTo define this distribution, we"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "tion recognition.\nIn order to visualize the results of our exper-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "predict its parameters using a neural network f with parameters"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "iments, we utilized a neural network to generate 3D facial ani-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "θ:"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "mations based on voice audio and a sequence of emotions. This",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "(3)\nα = fθ(audio)"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "sequence is represented by numerical vectors ranging from 0 to",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "1, where each vector corresponds to a continuous description of",
          "the first stage we train model A to predict a single emotion for the entire track. At": "emotion ∼ Dir(emotion | α)\n(4)"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "an emotion at a specific time point.\nEach scalar within these",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "The model fθ is trained by maximizing the likelihood across the"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "vectors represents the degree of a particular emotion’s activity,",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "training dataset {(audioi, emotioni)}K\ni=1:"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "such as anger or joy.\nIn this context, our interest\nlay in creat-",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "ing a probabilistic model for a mixture of emotions rather than",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "1 K\nK(cid:88) i\nθ∗ = arg max\n(5)\nlog Dir(emotioni\n| fθ(audioi))."
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "probabilities of marginal categories. Unlike traditional classifi-",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "θ"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "=1"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "cation tasks that presuppose a direct\nlink between objects and",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "distinct classes, we aim to associate an object with a categori-",
          "the first stage we train model A to predict a single emotion for the entire track. At": "A model trained in this manner enables the estimation of prob-"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "cal distribution reflecting a mixture of emotions. Formally, our",
          "the first stage we train model A to predict a single emotion for the entire track. At": "abilities for any emotion mixture. To infer a specific prediction,"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "goal\nis to develop a probabilistic model\nthat estimates not\nthe",
          "the first stage we train model A to predict a single emotion for the entire track. At": "we use the expectation of the Dirichlet distribution:"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "marginal probabilities of classes,",
          "the first stage we train model A to predict a single emotion for the entire track. At": "αi"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": ".\n(6)\nemotioni ="
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "(cid:80)N"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "i=1 αi"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "p(emotion | audio),\nemotion ∈ {anger, sadness, . . . , joy},",
          "the first stage we train model A to predict a single emotion for the entire track. At": ""
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "but rather the joint probability of a mixture of emotions,",
          "the first stage we train model A to predict a single emotion for the entire track. At": "3.1. Modeling singular emotional representation"
        },
        {
          "the proposed method. At\nFigure 1: Three stages of": "",
          "the first stage we train model A to predict a single emotion for the entire track. At": "In the first stage, we train the model to predict a singular mixture"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "anger\ndisgust\nfear\njoy\nneutral\nsadness"
        },
        {
          "Table 1: The details of the stage one training dataset.": "CREMA-D[10]\n1271\n1271\n1271\n1271\n1087\n1271\n7442"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "RAVDESS[11]\n192\n192\n192\n192\n96\n192\n1056"
        },
        {
          "Table 1: The details of the stage one training dataset.": "JL[12]\n240\n-\n-\n480\n240\n240\n1200"
        },
        {
          "Table 1: The details of the stage one training dataset.": "Private\n1404\n1404\n1404\n1404\n1404\n1404\n8424"
        },
        {
          "Table 1: The details of the stage one training dataset.": "3107\n2867\n2867\n3347\n2827\n3107\n18122"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "of audio with an average duration of 3 seconds per audio clip."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "All\nrecordings are in English and feature a gender balance of"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "approximately 1:1."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "To achieve a better assessment of model accuracy, we em-"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "ployed cross-dataset validation. We independently collected a"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "test dataset consisting of 516 samples, with 86 samples for each"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "emotion. The audio recordings were collected from 7 speakers,"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "each of whom recorded half of the samples in English and the"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "other half in their native language. We used standard classifica-"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "tion accuracy as the metric for evaluation."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "Stage 2. To generate data with emotions that change over"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "time we\nrequire\nan emotionally dynamic dataset.\nStandard"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "datasets used in the first\nstage are unsuitable as\nthey feature"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "audio samples with a mostly static emotion.\nInstead, we uti-"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "lized the VoxMovies [13, 14] collection, which comprises audio"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "tracks of dialogues from movies and TV shows. We used a slid-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "ing window with a size of 1.4 sec and a step of 1 sec to predict"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "the sequence of emotions for each audio track in this dataset,"
        },
        {
          "Table 1: The details of the stage one training dataset.": "yielding a total of 676 sequence-to-sequence samples."
        },
        {
          "Table 1: The details of the stage one training dataset.": "Assessing such a model poses a significant challenge. One"
        },
        {
          "Table 1: The details of the stage one training dataset.": "approach to do that is to use the standard method of a hold-out"
        },
        {
          "Table 1: The details of the stage one training dataset.": "sample and compare the predictions with pre-generated data."
        },
        {
          "Table 1: The details of the stage one training dataset.": "However, in this scenario, the metric values would only indicate"
        },
        {
          "Table 1: The details of the stage one training dataset.": "how well the model replicates the sliding window method."
        },
        {
          "Table 1: The details of the stage one training dataset.": "As previously mentioned, we had a pre-trained facial ani-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "mation neural network capable of generating 3D digital avatars"
        },
        {
          "Table 1: The details of the stage one training dataset.": "based on input audio and a sequence of emotions. Addition-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "ally, we had a small dataset of 3D facial animations and cor-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "responding audio tracks.\nTo create a ground truth dataset\nfor"
        },
        {
          "Table 1: The details of the stage one training dataset.": "evaluating our sequence-to-sequence model, we extracted emo-"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "tional sequences from this dataset by optimizing the NVIDIA"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "Audio2Face network (A2F) w.r.t.\nthe input emotion. Namely,"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "we froze the weights of A2F and found emotional sequences"
        },
        {
          "Table 1: The details of the stage one training dataset.": "that minimized the Mean Squared Error\n(MSE) between the"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "ground truth animation and the prediction by A2F. Formally,"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "we solved the following optimization problem for each frame"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "in the animation:"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "emotion∗ = arg min\nMSE(A2F(audio, emotion), GT).\n(8)"
        },
        {
          "Table 1: The details of the stage one training dataset.": "emotion"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "Using this approach, we extracted 40 emotional sequences di-"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "rectly from the ground-truth facial movements. While this data"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "is insufficient\nfor\nthe training of a DSER model,\nit\nis enough"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "for assessing its quality. We do not disclose this dataset due to"
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "commerical interest."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "As a metric, Mean Absolute Error (MAE) was employed."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "The use of the Kullback-Leibler divergence was limited by the"
        },
        {
          "Table 1: The details of the stage one training dataset.": "fact that the animating neural network utilized a different emo-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "tional\nspace, notably where the zero emotional vector corre-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "sponded to a neutral emotion."
        },
        {
          "Table 1: The details of the stage one training dataset.": ""
        },
        {
          "Table 1: The details of the stage one training dataset.": "Stage 3.\nTo gather\nthe human feedback, we utilized the"
        },
        {
          "Table 1: The details of the stage one training dataset.": "manually filtered MELD [15] dataset. This dataset\nrepresents"
        },
        {
          "Table 1: The details of the stage one training dataset.": "speech recordings from a comedy show. Because of that it con-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "tains a lot of samples with background laughter, biasing the pre-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "diction towards the joy emotion. We removed those samples, re-"
        },
        {
          "Table 1: The details of the stage one training dataset.": "sulting in a total of 400 samples. For each audio track included,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: represents the influence of this",
      "data": [
        {
          "Table 2: Evaluation of the models predicting single emotion.": "Loss\nBackbone\nAccuracy",
          "Table 4: The influence of the β parameter.": "β\nMAE"
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "0.84\nCross-Entropy\nLarge",
          "Table 4: The influence of the β parameter.": "0.01\n0.215"
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "Dirichlet Likelihood\nLarge\n0.81",
          "Table 4: The influence of the β parameter.": "0.1\n0.197"
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "",
          "Table 4: The influence of the β parameter.": "0.195\n0.5"
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "0.8\nCross-Entropy\nBase",
          "Table 4: The influence of the β parameter.": ""
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "",
          "Table 4: The influence of the β parameter.": "10.0\n0.200"
        },
        {
          "Table 2: Evaluation of the models predicting single emotion.": "Dirichlet Likelihood\nBase\n0.79",
          "Table 4: The influence of the β parameter.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: represents the influence of this",
      "data": [
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "10.0\n0.200"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "all,\nthe model proved to be quite resilient\nto changes in hyper-"
        },
        {
          "0.195\n0.5": "parameters.\nIn the best checkpoint, we employed the following"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "setup: 100 epochs, Adam optimizer with learning rate decay-"
        },
        {
          "0.195\n0.5": "ing from 5 × 10−5 to 10−5, a batch size of 16, and a weight"
        },
        {
          "0.195\n0.5": "decay of 10−4. Additionally, we utilized augmentations such"
        },
        {
          "0.195\n0.5": "as audio shifting, cropping and noise injection. For subsequent"
        },
        {
          "0.195\n0.5": "stages, we used the same parameters. For all\nthe experiments"
        },
        {
          "0.195\n0.5": "we resampled the input audio samples to 16 kHz."
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "4.4. Results and discussion"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "Tables 2 and 3 show the evaluation of\nthe single emotion and"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "sequence-to-sequence models,\nrespectively. While the tradi-"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "tional cross-entropy loss shows slightly better results in terms"
        },
        {
          "0.195\n0.5": "of accuracy when predicting emotion for\nthe entire audio,\nthe"
        },
        {
          "0.195\n0.5": "Dirichlet method results in better metric values for sequential"
        },
        {
          "0.195\n0.5": "mixture modeling.\nThe incorporation of\nthe human feedback"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "further improves the result, providing state-of-the art quality."
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "We examined the effect of the regularization parameter β in"
        },
        {
          "0.195\n0.5": "the loss (7), noting that higher β values align the model more"
        },
        {
          "0.195\n0.5": "closely with the reference model, as posited by DPO theory."
        },
        {
          "0.195\n0.5": "This correlation was confirmed through empirical observation."
        },
        {
          "0.195\n0.5": "Furthermore, excessively low β values\nresulted in the model"
        },
        {
          "0.195\n0.5": "losing its pretrained characteristics, consequently diminishing"
        },
        {
          "0.195\n0.5": "performance metrics.\nTable 4 represents the influence of\nthis"
        },
        {
          "0.195\n0.5": "parameter for the Large checkpoint. The value β = 0.5 shows"
        },
        {
          "0.195\n0.5": "the optimal balance between learning from human feedback and"
        },
        {
          "0.195\n0.5": "utilizing the reference model’s knowledge. The reported values"
        },
        {
          "0.195\n0.5": "represent the best metric scores achieved during training."
        },
        {
          "0.195\n0.5": "Despite the human feedback-based method offering a sim-"
        },
        {
          "0.195\n0.5": "pler procedure for annotating and scaling datasets,\nin practice,"
        },
        {
          "0.195\n0.5": "it is sometimes challenging to compare the proposed emotional"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "sequences for selection, which reduces the efficiency of label-"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "ing. This can be viewed as the method’s limitation."
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "Our approach was trained and evaluated on a single node"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "with an NVIDIA RTX 3090 GPU. The training of each stage"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "takes 1-2 hours.\nInference for the largest checkpoint, acceler-"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "ated with NVIDIA TensorRT in FP16 format, takes about 6 ms"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "for 1 sec of input audio, enabling real-time usage of the model."
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "5. Conclusion"
        },
        {
          "0.195\n0.5": "In this study, we introduced Dynamic Speech Emotion Recog-"
        },
        {
          "0.195\n0.5": ""
        },
        {
          "0.195\n0.5": "nition (DSER), a novel approach for over-time speech emotion"
        },
        {
          "0.195\n0.5": "recognition method suitable for 3D avatar animation. Our ap-"
        },
        {
          "0.195\n0.5": "proach, leveraging synthetic data generation in conjunction with"
        },
        {
          "0.195\n0.5": "human feedback fine-tuning, aimed to overcome the challenges"
        },
        {
          "0.195\n0.5": "in traditional emotion recognition techniques. The results show-"
        },
        {
          "0.195\n0.5": "case the efficiency of our approach in accurately modeling se-"
        },
        {
          "0.195\n0.5": "quences of emotional mixtures, and highlight\nthe value of hu-"
        },
        {
          "0.195\n0.5": "man feedback in enhancing model performance. We see further"
        },
        {
          "0.195\n0.5": "research in scaling datasets and more detailed control over emo-"
        },
        {
          "0.195\n0.5": "tion mixture modeling as promising directions."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "and R. Mihalcea, “MELD: A multimodal multi-party dataset for"
        },
        {
          "6. References": "[1]\nT. Karras,\nT. Aila,\nS.\nLaine,\nA. Herva,\nand\nJ.\nLehtinen,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "emotion recognition in conversations,” in Proceedings of the 57th"
        },
        {
          "6. References": "“Audio-driven facial animation by joint end-to-end learning of",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Annual Meeting of the Association for Computational Linguistics,"
        },
        {
          "6. References": "pose and emotion,” ACM Trans. Graph., vol. 36, no. 4,\njul 2017.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "A. Korhonen, D. Traum, and L. M`arquez, Eds.\nFlorence, Italy:"
        },
        {
          "6. References": "[Online]. Available: https://doi.org/10.1145/3072959.3073658",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Association\nfor\nComputational\nLinguistics,\nJul.\n2019,\npp."
        },
        {
          "6. References": "[2] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "527–536. [Online]. Available: https://aclanthology.org/P19-1050"
        },
        {
          "6. References": "and C. Finn,\n“Direct preference optimization:\nYour\nlanguage",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "[16]",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "T. Wolf,\nL. Debut,\nV\n.\nSanh,\nJ. Chaumond,\nC. Delangue,"
        },
        {
          "6. References": "model\nis secretly a reward model,” in Thirty-seventh Conference",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison,"
        },
        {
          "6. References": "on Neural\nInformation\nProcessing\nSystems,\n2023.\n[Online].",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "S. Shleifer, P. von Platen, C. Ma, Y.\nJernite,\nJ. Plu, C. Xu,"
        },
        {
          "6. References": "Available: https://arxiv.org/abs/2305.18290",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "T. Le Scao, S. Gugger, M. Drame, Q. Lhoest,\nand A. Rush,"
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "“Transformers:\nState-of-the-art\nnatural\nlanguage\nprocessing,”"
        },
        {
          "6. References": "[3] A. Baevski, H. Zhou, A.\nrahman Mohamed,\nand M. Auli,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "the 2020 Conference on Empirical Methods\nin Proceedings of"
        },
        {
          "6. References": "“wav2vec\n2.0:\nA framework\nfor\nself-supervised\nlearning\nof",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "in\nNatural\nLanguage\nProcessing:\nSystem\nDemonstrations,"
        },
        {
          "6. References": "speech\nrepresentations,”\nArXiv,\nvol.\nabs/2006.11477,\n2020.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Q.\nLiu\nand D.\nSchlangen,\nEds.\nOnline:\nAssociation\nfor"
        },
        {
          "6. References": "[Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Computational\nLinguistics, Oct.\n2020,\npp.\n38–45.\n[Online]."
        },
        {
          "6. References": "219966759",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": "Available: https://aclanthology.org/2020.emnlp-demos.6"
        },
        {
          "6. References": "[4]\nL. Pepino, P. Riera, and L. Ferrer, “Emotion Recognition from",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "Interspeech\nSpeech Using wav2vec 2.0 Embeddings,”\nin Proc.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "2021, 2021, pp. 3400–3404.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[5] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "dinov,\nand A. Mohamed,\n“Hubert:\nSelf-supervised\nspeech",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "representation\nlearning\nby\nmasked\nprediction\nof\nhidden",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "IEEE/ACM Trans.\nAudio,\nSpeech\nand\nLang.\nunits,”\nProc.,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "vol.\n29,\np.\n3451–3460,\noct\n2021.\n[Online].\nAvailable:",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "https://doi.org/10.1109/TASLP.2021.3122291",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[6]\nL.\nPepino,\nP.\nE.\nRiera,\nand\nL.\nFerrer,\n“Emotion",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "recognition\nfrom speech\nusing\nwav2vec\n2.0\nembeddings,”",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "ArXiv,\nvol.\nabs/2104.03502,\n2021.\n[Online].\nAvailable:",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "https://api.semanticscholar.org/CorpusID:233181984",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[7] Y. Wang,\nA.\nBoumadane,\nand\nA.\nHeba,\n“A\nfine-tuned",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "wav2vec 2.0/hubert benchmark for speech emotion recognition,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "speaker verification and spoken language understanding,” ArXiv,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "vol.\nabs/2111.02735,\n2021.\n[Online]. Available:\nhttps://api.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "semanticscholar.org/CorpusID:242757022",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[8]\nP.\nF. Christiano,\nJ. Leike,\nT. Brown, M. Martic,\nS. Legg,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "and D. Amodei,\n“Deep\nreinforcement\nlearning\nfrom human",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "in Neural\nInformation Processing\npreferences,”\nin Advances",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "Systems,\nI. Guyon,\nU. V.\nLuxburg,\nS.\nBengio,\nH. Wal-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "lach,\nR.\nFergus,\nS. Vishwanathan,\nand\nR. Garnett,\nEds.,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "vol.\n30.\nCurran\nAssociates,\nInc.,\n2017.\n[Online].\nAvail-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "able:\nhttps://proceedings.neurips.cc/paper files/paper/2017/file/",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[9] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "A. Radford, D. Amodei, and P. Christiano, “Learning to summa-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "the 34th Interna-\nrize from human feedback,” in Proceedings of",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "tional Conference on Neural Information Processing Systems, ser.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "NIPS’20.\nRed Hook, NY, USA: Curran Associates Inc., 2020.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[10] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "actors dataset,” IEEE Transactions on Affective Computing, vol. 5,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "no. 4, pp. 377–390, 2014.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[11]\nS.\nR.\nLivingstone\nand\nF. A.\nRusso,\n“The\nryerson\naudio-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "visual\ndatabase\nof\nemotional\nspeech\nand\nsong\n(ravdess):\nA",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "dynamic, multimodal set of facial and vocal expressions in north",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "american english,” PLOS ONE, vol. 13,\nno. 5,\np.\ne0196391,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "May 2018. [Online]. Available: http://dx.doi.org/10.1371/journal.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "pone.0196391",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[12]\nJ.\nJames, L. Tian,\nand C. Watson,\n“An open source emotional",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "speech corpus for human robot interaction applications,” 09 2018,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "pp. 2768–2772.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[13] A. Brown, J. Huh, A. Nagrani, J. S. Chung, and A. Zisserman,",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "“Playing a part: Speaker verification at\nthe movies,” in Interna-",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "tional Conference on Acoustics, Speech, and Signal Processing",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "(ICASSP), 2021, 2020.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "[14] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Voxceleb:",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "Large-scale speaker verification in the wild,” Computer Science",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        },
        {
          "6. References": "and Language, 2019.",
          "[15]": "",
          "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Audio-driven facial animation by joint end-to-end learning of pose and emotion",
      "authors": [
        "T Karras",
        "T Aila",
        "S Laine",
        "A Herva",
        "J Lehtinen"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph",
      "doi": "10.1145/3072959.3073658"
    },
    {
      "citation_id": "3",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "R Rafailov",
        "A Sharma",
        "E Mitchell",
        "C Manning",
        "S Ermon",
        "C Finn"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Rahman Mohamed",
        "M Auli"
      ],
      "year": "2006",
      "venue": "ArXiv"
    },
    {
      "citation_id": "5",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc",
      "doi": "10.1109/TASLP.2021.3122291"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "8",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "9",
      "title": "Deep reinforcement learning from human preferences",
      "authors": [
        "P Christiano",
        "J Leike",
        "T Brown",
        "M Martic",
        "S Legg",
        "D Amodei"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Learning to summarize from human feedback",
      "authors": [
        "N Stiennon",
        "L Ouyang",
        "J Wu",
        "D Ziegler",
        "R Lowe",
        "C Voss",
        "A Radford",
        "D Amodei",
        "P Christiano"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS'20"
    },
    {
      "citation_id": "11",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "13",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "J James",
        "L Tian",
        "C Watson"
      ],
      "year": "2018",
      "venue": "An open source emotional speech corpus for human robot interaction applications"
    },
    {
      "citation_id": "14",
      "title": "Playing a part: Speaker verification at the movies",
      "authors": [
        "A Brown",
        "J Huh",
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "authors": [
        "A Nagrani",
        "J Chung",
        "W Xie",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "Computer Science and Language"
    },
    {
      "citation_id": "16",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    }
  ]
}