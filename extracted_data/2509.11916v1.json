{
  "paper_id": "2509.11916v1",
  "title": "Neurogaze-Distill: Brain-Informed Distillation And Depression-Inspired Geometric Priors For Robust Facial Emotion Recognition",
  "published": "2025-09-15T13:33:54Z",
  "authors": [
    "Zilin Li",
    "Weiwei Xu",
    "Xuanqi Zhao",
    "Yiran Zhu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial emotion recognition (FER) models trained only on pixels often fail to generalize across datasets because facial appearance is an indirect-and biased-proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal distillation framework that transfers brain-informed priors into an image-only FER student via static Valence-Arousal (V/A) prototypes and a depressioninspired geometric prior (D-Geo). A teacher trained on EEG topographic maps from DREAMER and MAHNOB-HCI produces a consolidated 5×5 V/A prototype grid that is frozen and reused; no EEG-face pairing and no non-visual signals at deployment are required. The student (ResNet-18/50) is trained on FERPlus with conventional CE/KD and two lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with affective findings often reported in depression research (e.g., anhedonia-like contraction in high-valence regions). We evaluate both within-domain (FERPlus validation) and cross-dataset protocols (AffectNet-mini; optional CK+), reporting standard 8-way scores alongside present-only Macro-F1 and balanced accuracy to fairly handle label-set mismatch. Ablations attribute consistent gains to prototypes and D-Geo, and favor 5×5 over denser grids for stability. The method is simple, deployable, and improves robustness without architectural complexity. * First author. † This work was completed while the author was affiliated with the School of Computer Science and Technology. Note on the name. \"NeuroGaze-Distill\" emphasizes neuro-informed distillation. Gaze heatmaps are optional and may be disabled in the final experiments; the \"Gaze\" term survives to reflect the broader privileged-signal design.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Motivation. Human affect is latent; faces are observable but ambiguous. Cross-dataset shifts in demographics, capture conditions, and label conventions substantially degrade FER robustness. In contrast, physiological signals such as EEG encode affective dynamics in a representation less entangled with appearance. However, collecting paired EEG-face data at scale is impractical and undesirable for deployable vision-only systems.\n\nIdea. We learn static neuro-informed prototypes in a continuous Valence-Arousal (V/A) space and distill them into an image-only FER model. A teacher trained on EEG topographic maps (DREAMER, with MAHNOB-HCI as unlabeled support) regresses V/A; its validation embeddings are aggregated into a 5×5 V/A prototype grid, which is then frozen and reused across students. A standard ResNet-18/50 student  [1]  trained on FERPLUS  [17]  receives conventional CE with label smoothing  [8]  and logit KD  [6] , plus two lightweight regularizers: (i) Proto-KD (cosine) to align features with the static prototypes; and (ii) a depression-inspired geometric prior (D-Geo) that softly shapes the embedding geometry in line with affective findings (e.g., anhedonia)  [21, 22] . Deployment remains vision-only.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contributions.",
      "text": "• Static neuro-informed prototypes. A simple, reusable EEG→V/A prototype formation requiring neither paired EEG-face data nor non-visual inputs at test time, grounded in a V/A circumplex space  [20] .\n\n• Minimalist loss cocktail. CE (label smoothing  [8]  + mild class weights) + logit KD  [6]  + Proto-KD (cf. prototype learning  [13, 14] ) + D-Geo, improving cross-dataset generalization without architectural complexity.\n\nArtifacts-only reproducibility. We do not release full source code at submission time, but we provide a minimal repository to reproduce reported tables/figures: https://github.com/Lixeeone/NeuroGaze-Distill.\n\n• Depression-aware perspective. A non-diagnostic geometric prior that regularizes representation structure using insights from affective neuroscience  [21, 22] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Background And Related Work",
      "text": "Affective spaces. Following the circumplex view of affect  [20] , we adopt a continuous V/A space and discretize it for learning. A fixed 5 × 5 grid balances coverage and statistical stability: denser grids (e.g., 7 × 7) increase sparsity and bin collapse in underrepresented regions, while coarser grids lose resolution near decision boundaries. Fig.  2  (left) visualizes the per-bin coverage of our teacher-derived data; Fig.  2  (right) shows the grid centers with marker sizes proportional to counts. This V/A geometry serves as the teacher space for forming static prototypes, while the student remains a categorical FER classifier, decoupling continuous affect structure from the final deployment task.\n\nFER and distribution shift. Deep FER has improved in-domain accuracy, yet cross-dataset robustness remains fragile due to differences in demographics, capture devices, annotation protocols, and class prevalence. To avoid confounding from missing classes on the target set, we report both the standard 8-way metrics and present-only metrics that restrict evaluation to labels actually present in the target dataset, providing a fairer measure of transferability.\n\nPhysiological signals for affect. Physiological channels (EEG, EDA, HRV, gaze) capture affective dynamics with noise and bias characteristics different from pixels. EEG in particular provides time-frequency measurements from distributed sensors; topographic images (\"topomaps\") can be rendered by interpolating per-channel band power onto a 2-D scalp layout. Most prior works either perform EEG-only recognition or require paired multimodal training.\n\nIn contrast, we use EEG only to form a compact prior-static V/A prototypes-and then train a vision-only student without any paired EEG-face examples or non-visual inputs at deployment, using publicly available datasets such as DREAMER  [19]  and MAHNOB-HCI  [18] .\n\nDepression-informed priors. Affective and clinical literature commonly discusses altered reward processing and anhedonia in depressive conditions  [21, 22] . We operationalize this insight as a light D-Geo prior: encourage controlled compactness for features associated with high-valence regions while maintaining separability (margins) elsewhere. The prior is non-diagnostic, applied uniformly across the dataset, and interacts additively with standard objectives.\n\nKnowledge distillation and prototypes. Knowledge distillation (KD) transfers information from a teacher to a student via logit-based soft targets  [6]  (and early model compression  [7] ), feature alignment, or relation matching.\n\nPrototype-based learning summarizes class/region structure with representative vectors  [13, 14] . Our framework combines both: the student receives CE+KD while also aligning, via a cosine objective, to a fixed bank of EEG-derived V/A prototypes. 3 Datasets and Preprocessing\n\n3.1 EEG teacher data DREAMER. We use the public DREAMER affect dataset  [19]  and render EEG topographic images (\"topomaps\") from band power (δ, θ, α, β, γ) computed on stimulus segments. Per subject and per band we z-score and min-max normalize to [0, 1] for visualization consistency; maps are exported at a fixed resolution for training the EEG teacher.\n\nMAHNOB-HCI. We follow the same pipeline for MAHNOB-HCI  [18] . Where gaze is available, we may export a heatmap for analysis only; no non-visual signals are used by the student at training or deployment. For reproducibility we maintain a manifest (mahnob_topomaps_manifest.csv) and a consolidated archive (mahnob_topomaps_all.npz) to reproduce the teacher's validation features used for prototype formation. Illustrative topomap grids are shown in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Face Student Data",
      "text": "FERPlus.\n\nWe use pre-packed NPZs with [N, 48, 48] grayscale images and 8-class label distributions (ferplus_train/valid/test.npz), together with manifests (ferplus_manifest_*.csv) and a class map (class_map_ferplus.json). Images are center-aligned, mean-std normalized, and augmented with safe transforms (random crop/flip and mild color jitter) that do not target identity cues. No face exemplars are displayed in this paper; we report only aggregate metrics and non-identifiable visualizations. We follow the protocol of FERPlus  [17] .\n\nAffectNet-mini. We adopt a reduced AffectNet split with CSV labels (labels_train/valid/test.csv, labels_all.csv) and class_map.json, based on AffectNet  [16] . For cross-dataset transfer we report both standard 8-way metrics and present-only metrics restricted to classes available in the target split (Sec. 2). Optionally, we also evaluate on CK+  [15] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Processing And Binning",
      "text": "EEG → V/A. Teacher networks regress Valence-Arousal (V/A) from topomaps. We linearly map reported V/A to [-1, 1], then discretize the continuous space with a fixed 5 × 5 grid (centers from -0.8 to 0.8 on each axis). Validation embeddings are aggregated per bin to form 25 static prototypes.\n\nFaces (categorical) → student. The student remains an 8-way FER classifier (ResNet-18/50, 256-D feature). Training uses CE with label smoothing  [8]  and mild class weights, logit KD at T =5.0  [6] , Proto-KD (cosine) toward the static prototype bank, and a light depression-inspired geometric prior (D-Geo)  [21, 22] . All figures and tables are exported automatically to viz/ and outs/. 4 Method",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Teacher And Static Prototype Formation",
      "text": "We train a CNN/ViT teacher  [1, 2]  to regress Valence-Arousal (V/A) from EEG topographic maps (Sec. 3). On the teacher validation split, we map continuous V/A into a fixed 5×5 grid (centers in [-0.8, 0.8]), and average the penultimate features within each bin to obtain 25 static prototypes P={p k } 25 k=1 . This yields the consolidated v4 prototype bank (prototypes_dreamer_mahnob_5x5_v4.npz), formed on DREAMER with unlabeled MAHNOB-HCI as a stability supplement; the bank is frozen and reused across all students and datasets (no paired EEG-face samples are required, and no non-visual signals are used at deployment). Fig.  2  visualizes prototype coverage, motivating the 5×5 choice over denser grids for robustness.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Student Network",
      "text": "The student is a standard ResNet-18/50 backbone  [1]  with a 256-D projection and an 8-way classifier. We adopt channels-last memory format, mixed precision (AMP), gradient clipping, and label smoothing (α=0.055)  [8] . Unless otherwise stated, student EMA is disabled (Mean-Teacher style EMA  [10]  underperforms here), and we do not employ additional LDACC-like losses.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Losses",
      "text": "Let x be an input face, f (x) ∈ R 256 the L2-normalized feature, z(x) ∈ R 8 the logits, y the (smoothed) 8-class target distribution, and P the fixed prototype set.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cross-Entropy (Ce).",
      "text": "We use CE with label smoothing (α=0.055)  [8]  and mild class weights to stabilize training on imbalanced emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Logit Distillation (Kd).",
      "text": "We match softened student logits to a vision teacher with temperature T =5.0 using an MSE/KL objective with a medium loss weight  [6] .\n\nPrototype distillation (Proto-KD, cosine). For each sample, we compute cosine similarities s k = cos f (x), p k to all prototypes and obtain a soft bin distribution q stu = softmax(s/τ ) with feature temperature τ =0.90. The prototype prior q pro is the (frozen) per-bin prior induced by P. We minimize D KL (q pro ∥q stu ) with a small weight (cf. prototype learning  [13, 14] ).\n\nDepression-inspired geometric prior (D-Geo). D-Geo is a weak, non-diagnostic regularizer on representation geometry. Concretely, (i) for high-valence categories (we use {happiness, surprise}), we apply a light within-class variance cap; (ii) globally, we encourage inter-class margins to preserve separability. The term is late-activated with a cosine ramp (epochs 20→60) and a small weight, motivated by anhedonia-related findings  [21, 22] .\n\nProto-KD (cos), τ =0.90\n\nWe keep all terms small and stable; together they improve cross-dataset generalization without architectural complexity. Inputs: Teacher T (CNN/ViT); validation set D val = {(M i , v i )} with topomaps M i and V/A v i ∈ [-1, 1] 2 ; grid size G=5; centers C={-0.8, . . . , 0.8} Outputs: Frozen prototype bank P={p k } K k=1 , K=G 2 ; per-bin prior q pro ∈ R K 1: Initialize counts N u,v ← 0, sums S u,v ← 0 for all (u, v) ∈ {1, . . . , G} 2 2: for all (M i , v i ) ∈ D val do ▷ No EEG-face pairing is required 3:\n\nS u,v += e i ; N u,v += 1 6: end for 7: for all (u, v) do end if 13: end for\n\n▷ Laplace smoothing ε≈1\n\n15: SAVENPZ(prototypes_dreamer_mahnob_5x5_v4.npz; P, q pro ) 16: return P, q pro for minibatch {(x b , y b )} B b=1 do 3: Proto-KD:",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Unless otherwise stated, all results in Sec. 5 use the same training recipe. We train with AdamW  [4]  (Adam  [3]  variant) and a cosine schedule  [5] ; base learning rate 2 × 10 -4 , weight decay 0.05, batch size 128, mixed precision (AMP), channels-last, and gradient clipping (1.0). Random seeds are fixed, logs are recorded with TensorBoard, and figure/table/CSV exporters write to viz/ and outs/. All released artifacts (the static prototype bank and student checkpoints) are versioned with SHA-256; filenames and digests are summarized in Table  1 .\n\nModel variants compared in Sec. 5.2. All students share the backbone and the above training recipe; they differ only in loss terms: B0 CE only; B1 CE + logit KD (T =5.0); B2 B1 + Proto-KD (cosine, τ =0.90); B3 B2 + D-Geo (full method, late activation with a small weight). A gaze-augmented EEG teacher can be used when available, but in our final runs gaze is disabled for consistency across datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Protocols And Metrics",
      "text": "We train on FERPlus and evaluate both within-domain and under cross-dataset shift. Within-domain results are reported on the FERPlus validation split with Accuracy (Acc), Macro-F1, and balanced accuracy (bACC; mean of per-class recall)  [24] . Macro-AUROC can be added for completeness. Unless otherwise noted, we report the mean over 3 seeds and 95% confidence intervals via stratified bootstrapping (1,000 resamples)  [23] . Cross-dataset evaluation applies the FERPlus-trained student to AffectNet-mini (and optionally CK+). Following our label-mismatch discussion, we report both: (i) present-only metrics computed using only the classes available in the target set, and (ii) the full 8-way mapping (fixed FER taxonomy).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines And Variants",
      "text": "We compare four progressively augmented students (same backbone and training recipe; Sec. 4.4): B0 CE only; B1 B0 + logit KD (T =5.0)  [6] ; B2 B1 + Proto-KD (cosine; τ =0.90; cf.  [13, 14] ); B3 B2 + D-Geo (full method; late activation with a small weight, motivated by  [21, 22] ). A gaze-augmented EEG teacher can be used when available, but in our final runs gaze is disabled for consistency across datasets. At-a-glance. From Table  3 , adding KD to CE (B0→B1) yields a large gain in Macro-F1, and Proto-KD further improves class balance (bACC). Introducing D-Geo (B3) preserves those gains while nudging high-valence structure; it gives the best or second-best Macro-F1 on FERPlus valid. In cross-dataset tests (Table  2 ), present-only is consistently higher than 8-way on CK+, avoiding penalties for absent classes; on AffectNet-mini the gap is small due to taxonomy alignment.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablations",
      "text": "We ablate the components on FERPLUS (valid). Unless noted, the backbone and recipe are fixed (Sec. 4.2, 4.3).\n\nProto-KD weight. Sweeping λ proto ∈ {0, 0.10, 0.12, 0.15} shows a stable optimum around 0.12.\n\nD-Geo schedule/weight. A late cosine ramp (epochs 20→60) with a small weight performs best; enabling from epoch 0 slightly harms early separability.\n\nGrid size. A 5×5 V/A grid outperforms 7×7 by avoiding sparse/collapsing bins (see Fig.  3 ).\n\nEMA. Teacher EMA helps prototype stability; student EMA underperforms here and is disabled  [10] .    6 Depression-Inspired Prior: Rationale, Scope, and Cautions Rationale. A large body of affective research associates depressive symptoms with blunted positive affect (anhedonia)  [21, 22] . We encode only the shape of this idea as a weak geometric regularizer on the representation: high-valence regions are encouraged to be slightly more compact, while global inter-class margins are preserved.\n\nHow it is applied. The D-Geo term is non-diagnostic and task-agnostic. It never uses clinical labels and produces no clinical signal. At inference time the student is a standard FER model; no EEG or clinical attribute is required. The prototype bank used elsewhere is frozen and anonymized (checksums in Table  1 ).\n\nObserved effect (empirical). Ablations in Sec. 5.3 and Fig.  6  show tighter high-valence clusters and small but consistent late-epoch gains in Macro-F1/Acc when D-Geo is added on top of CE+KD+Proto-KD. Figure  4  further indicates that KD accelerates early learning, while Proto-KD and the late-activated D-Geo improve the final performance.\n\n• Teacher dependence. Prototype quality depends on the teacher trained on EEG topomaps; suboptimal teachers or shifts in V/A calibration may cap the attainable gains.\n\n• Privacy and deployment. Physiological signals (EEG/gaze) are used only during development to form static prototypes; the deployed student is vision-only. We do not display any identifiable face exemplars.\n\nEthics & broader impact. We use only publicly available datasets (CK+, AFFECTNET, FERPLUS/FER2013, DREAMER, MAHNOB-HCI) under their academic licenses. Released artifacts for review are limited to model weights, metrics, and tables; they contain no personally identifiable images.",
      "page_start": 8,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "We presented NEUROGAZE-DISTILL, a brain-informed yet deployment-friendly framework for facial expression recognition (FER). The method couples a static EEG-derived prototype bank with a lightweight depression-inspired geometric prior (D-Geo), and distills both cues into a conventional CNN student via logit and prototype matching. The prototype bank (v4; DREAMER with MAHNOB-HCI as a stability supplement) is formed once on teacher features, frozen thereafter, and-crucially-requires no paired EEG-face data and no non-visual signals at inference. Students remain standard backbones (ResNet-18/50)  [1]  trained with CE+KD+Proto-KD+D-Geo using modest hyperparameters (e.g., T =5, τ =0.90).\n\nEmpirical takeaways. Across datasets (Sec. 5), the full model (B3) improves Accuracy, Macro-F1, and bACC over the CE baseline and intermediate variants (Tables  2  and 3 ). The ablation timelines (Fig.  4 ) show a clear division of labor: KD (B1) accelerates early learning  [6] , while Proto-KD (B2) and the late-activated D-Geo (B3) yield consistent late-epoch gains. Qualitatively, t-SNE/UMAP panels (Fig.  6 ) reveal tighter, better-separated clusters-especially for high-valence categories-without sacrificing low-valence separability  [11, 12] .\n\nPracticality and verification. The recipe uses off-the-shelf training components (AdamW  [4] , cosine schedule  [5] , AMP, channels-last, light clipping) and a single frozen prototype bank shared by all students and datasets. We provide SHA-256-versioned artifacts-prototype bank, the main student checkpoint, and metrics bundles-summarized in Table  1 .\n\nOutlook. Promising directions include: (i) adaptive/data-driven prototype granularity beyond a fixed 5×5 V/A grid; (ii) teacher calibration and stronger students (e.g., ViT backbones  [2] ) while keeping inference vision-only; (iii) broader fairness analyses across demographics and capture conditions; and (iv) additional privileged teachers (speech or physiology) during training under appropriate governance.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: NeuroGaze–Distill overview. (A) FER suffers from distribution shift; robust depression–related affect",
      "page": 2
    },
    {
      "caption": "Figure 2: (right) shows the grid centers with marker sizes",
      "page": 3
    },
    {
      "caption": "Figure 2: Prototype coverage (5 × 5, v4). Left: counts per V/A bin; Right: grid centers with marker size ∝counts.",
      "page": 3
    },
    {
      "caption": "Figure 3: EEG topomap grids used by the teacher (Sec. 3). Left: DREAMER band-power topomaps [19]. Right:",
      "page": 4
    },
    {
      "caption": "Figure 2: visualizes prototype coverage, motivating",
      "page": 5
    },
    {
      "caption": "Figure 4: Ablation timelines on FERPlus valid. Macro–F1 (left) and Accuracy (right) vs epochs for B0→B3. KD (B1)",
      "page": 9
    },
    {
      "caption": "Figure 5: Present-only confusion matrix on AffectNet-mini. Evaluated with the full method (B3: CE+KD+Proto–",
      "page": 9
    },
    {
      "caption": "Figure 6: t-SNE of student features on FERPlus valid across ablations (B0→B3). Proto–KD and D–Geo progres-",
      "page": 10
    },
    {
      "caption": "Figure 4: shows that KD (B1) speeds up early convergence [6], while Proto–KD (B2) and the",
      "page": 10
    },
    {
      "caption": "Figure 7: Training curves. Top: student accuracy, Macro–F1 and training loss on FERPlus; Bottom: teacher V/A CCC",
      "page": 10
    },
    {
      "caption": "Figure 6: show tighter high-valence clusters and small but consis-",
      "page": 10
    },
    {
      "caption": "Figure 4: further indicates",
      "page": 10
    },
    {
      "caption": "Figure 4: ) show a clear division of",
      "page": 12
    },
    {
      "caption": "Figure 6: ) reveal tighter, better-separated clusters—especially for",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Dataset",
          "Column_2": "Presentclasses",
          "Present-only(external)": "Acc(%)",
          "Column_4": "Macro-\nF1(%)",
          "Column_5": "bACC\n(%)",
          "8-way(fixedmapping)": "Acc(%)",
          "Column_7": "Macro-\nF1(%)",
          "Column_8": "bACC\n(%)"
        },
        {
          "Column_1": "AffectNet-\nmini",
          "Column_2": "neutral, happiness,\nsurprise, sadness,\nanger, disgust,\nfear, contempt",
          "Present-only(external)": "76.30",
          "Column_4": "75.60",
          "Column_5": "75.77",
          "8-way(fixedmapping)": "76.30",
          "Column_7": "75.60",
          "Column_8": "75.77"
        },
        {
          "Column_1": "CK+\n[15]",
          "Column_2": "happiness,\nsurprise, sadness,\nanger, disgust,\nfear, contempt",
          "Present-only(external)": "64.93",
          "Column_4": "49.33",
          "Column_5": "52.46",
          "8-way(fixedmapping)": "55.86",
          "Column_7": "39.23",
          "Column_8": "36.25"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "B0: CE onl\nB1: CE + K\nB2: B1 + P\nB3: B2 + D",
          "Column_2": "y\nD\nroto-KD\n-Geo (full)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "B0: CE onl\nB1: CE + K\nB2: B1 + P\nB3: B2 + D",
          "Column_2": "y\nD\nroto-KD\n-Geo (full)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Va",
          "Column_8": "l"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Va",
          "Column_7": "",
          "Column_8": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Va",
          "Column_8": "l"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "Trai",
          "Column_8": "n"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Va",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Va",
          "Column_7": "l"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Va",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Va",
          "Column_7": "l"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Trai",
          "Column_6": "",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "Trai",
          "Column_7": "n"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov"
      ],
      "venue": "ICLR, 2021",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "3",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "ICLR",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "4",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "ICLR",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "5",
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "ICLR",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "6",
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "NeurIPS Deep Learning Workshop",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "7",
      "title": "Rich Caruana, and Alexandru Niculescu-Mizil. Model Compression",
      "authors": [
        "Cristian Buciluȃ"
      ],
      "year": "2006",
      "venue": "KDD"
    },
    {
      "citation_id": "8",
      "title": "When Does Label Smoothing Help?",
      "authors": [
        "Rafael Müller",
        "Simon Kornblith",
        "Geoffrey Hinton"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "9",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning",
      "authors": [
        "Antti Tarvainen",
        "Harri Valpola"
      ],
      "year": "2017",
      "venue": "NeurIPS",
      "arxiv": "arXiv:1703.01780"
    },
    {
      "citation_id": "11",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
      "authors": [
        "Leland Mcinnes",
        "John Healy",
        "James Melville"
      ],
      "year": "2018",
      "venue": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "13",
      "title": "Prototypical Networks for Few-shot Learning",
      "authors": [
        "Jake Snell",
        "Kevin Swersky",
        "Richard Zemel"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "14",
      "title": "No Fuss Distance Metric Learning using Proxies",
      "authors": [
        "Yair Movshovitz-Attias",
        "Alexander Toshev",
        "Thomas Leung",
        "Sergey Ioffe",
        "Saurabh Singh"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "15",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "16",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "17",
      "title": "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhifeng Zhang"
      ],
      "year": "2016",
      "venue": "ACM ICMI",
      "doi": "10.1145/2993148.2993165"
    },
    {
      "citation_id": "18",
      "title": "MAHNOB-HCI: A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Florian Eyben",
        "Markus Kächele"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.34"
    },
    {
      "citation_id": "19",
      "title": "DREAMER: A Database for Emotion Recognition through EEG and ECG Signals from Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "Stamatios Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "20",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "James Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "21",
      "title": "Depression, Stress, and Anhedonia: Toward a Synthesis and Integrated Model",
      "authors": [
        "Diego Pizzagalli"
      ],
      "year": "2014",
      "venue": "Annual Review of Clinical Psychology",
      "doi": "10.1146/annurev-clinpsy-050212-185606"
    },
    {
      "citation_id": "22",
      "title": "Reconceptualizing Anhedonia: Novel Perspectives on Balancing the Anticipation and Experience of Reward",
      "authors": [
        "T Michael",
        "David Treadway",
        "Zald"
      ],
      "year": "2011",
      "venue": "Psychological Bulletin",
      "doi": "10.1037/a0024515"
    },
    {
      "citation_id": "23",
      "title": "An Introduction to the Bootstrap",
      "authors": [
        "Bradley Efron",
        "Robert Tibshirani"
      ],
      "year": "1994",
      "venue": "An Introduction to the Bootstrap"
    },
    {
      "citation_id": "24",
      "title": "The balanced accuracy and its posterior distribution",
      "authors": [
        "H Kay",
        "Brodersen",
        "Soon Cheng",
        "Klaas Ong",
        "Joachim Stephan",
        "Buhmann"
      ],
      "year": "2010",
      "venue": "ICPR"
    }
  ]
}