{
  "paper_id": "2404.04654v1",
  "title": "Eai Endorsed Transactions On Scalable Information Systems",
  "published": "2024-04-06T15:14:25Z",
  "authors": [
    "Rajesh B",
    "Keerthana V",
    "Narayana Darapaneni",
    "Anwesh Reddy P"
  ],
  "keywords": [
    "Facial emotion detection",
    "Resnet50",
    "convolutional neural network",
    "deep learning",
    "region of interest",
    "explainable AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "INTRODUCTION: Music provides an incredible avenue for individuals to express their thoughts and emotions, while also serving as a delightful mode of entertainment for enthusiasts and music lovers. OBJECTIVES: This paper presents a comprehensive approach to enhancing the user experience through the integration of emotion recognition, music recommendation, and explainable AI using GRAD-CAM. METHODS: The proposed methodology utilizes a ResNet50 model trained on the Facial Expression Recognition (FER) dataset, consisting of real images of individuals expressing various emotions. RESULTS: The system achieves an accuracy of 82% in emotion classification. By leveraging GRAD-CAM, the model provides explanations for its predictions, allowing users to understand the reasoning behind the system's recommendations. The model is trained on both FER and real user datasets, which include labelled facial expressions, and real images of individuals expressing various emotions. The training process involves pre-processing the input images, extracting features through convolutional layers, reasoning with dense layers, and generating emotion predictions through the output layer CONCLUSION: The proposed methodology, leveraging the Resnet50 model with ROI-based analysis and explainable AI techniques, offers a robust and interpretable solution for facial emotion detection paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the current digital age, the significance of user experience is pivotal across a wide range of applications.This paper seeks to enhance user experience by integrating three key components: emotion recognition, music recommendation, and explainable AI. The proposed system leverages deep learning techniques and GRAD-CAM for accurate emotion recognition and personalized music recommendations. Recent studies in the domain of music have revealed that music evokes strong emotional reactions in its listeners  [1] . There is a significant correlation between musical preferences and personality traits as well as emotions. The areas of the brain responsible for emotions and mood control the various aspects of music, such as meter, timber, rhythm, and pitch  [2] .\n\nElements such as sex, age  [3] , cultural background  [4] , mood, individual choices, and contextual factors  [5]  collectively influence an individual's emotional reaction to a specific piece of music, taking into account variables like the time of day or the setting. Notwithstanding these external influences, humans possess the ability to consistently classify music into categories such as happiness, sadness, excitement, or calmness. Facial expressions possess considerable potential as indicators of an individual's psychological well-being, serving as the most fundamental and instinctive means of expressing emotions  [6, 7, 2] . Despite the strong association, a majority of current music software lacks the capability to generate playlists aligned with diverse emotional states. Emotion recognition plays a vital role in understanding users' emotional states and tailoring content accordingly. By accurately detecting and classifying facial emotions, the system can adapt its functionality and provide content that resonates with users' emotional needs. This capability opens up new possibilities for interactive applications, such as personalized music recommendation systems. Music recommendation systems have gained immense popularity in recent years, leveraging advanced algorithms to suggest songs based on users' preferences. However, integrating emotion recognition into music recommendation adds a new dimension to the system. By considering the detected emotions, the system can recommend songs that align with users' emotional states, creating a more meaningful and engaging music listening experience.. Explainable AI is another crucial aspect of the proposed system. Conventional machine learning models frequently lack interpretability, posing a challenge for users to comprehend the rationale behind their recommendations. By incorporating the GRAD-CAM technique, the system provides visual explanations for its predictions, enabling users to grasp the underlying features that contribute to the recommended content. This transparency fosters trust, understanding, and user engagement. The primary objective of this study is to create an affordable music application that utilizes real-time video and Convolutional Neural Network (CNN) technology to automatically select songs based on the user's current mood state. The system aims to minimize resource consumption while incorporating an emotion module that analyzes the user's real-time video to evaluate their emotional state. Subsequently, it matches the identified mood with songs from a categorized collection and offers recommendations for a diverse range of songs. By alleviating the burden of manual song selection, this system has the potential to address the existing challenge in finding suitable songs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Survey",
      "text": "The literature survey encompasses a comprehensive analysis of existing research and studies related to emotion recognition, music recommendation systems, and explainable AI. This section highlights key findings, methodologies, and insights from previous works, laying the foundation for the proposed system.\n\nSeveral studies have explored the use of different techniques and methodologies for music recommendation systems based on mood/emotions. In their study, Renu Taneja et al.  [9]  utilized Audio to retrieve audio features such as pace, beats, and RMSE. They then constructed clusters to represent various moods based on these extracted properties. Kee Moe Han et al.  [8] , on the other hand, employed the average emotions from a group of 15 individuals to determine the emotion of a song. They trained a classifier using this data and categorized the music signal's emotion by considering audio parameters such as pitch and timbre. Another research conducted by V. R. Ghule et al.  [10]  centered on the development of a music system that employed facial recognition technology for analyzing emotions.\n\nIn 2005, Wieczorkowska et al. conducted a study aiming to assist users in discovering music aligned with their moods. They employed the K-nearest neighbors (KNN) algorithm to classify a vast dataset of 327,683 songs into six distinct emotions, resulting in an overall accuracy of 37%. Similarly, in 2008, another study  [12]  utilized a regression method for Music Emotion Recognition (MER) and achieved accuracy rates of 64% for arousal and 59% for valence. Yading Song et al.  [13]  explored various facets of music for MER and utilized a labeled dataset of 2,904 songs categorized as \"happy,\" \"sad,\" \"angry,\" or \"relaxed.\" Support Vector Machines (SVM) were employed, with spectral characteristics exhibiting superior performance compared to other acoustic parameters.\n\nIn 1978, Ekman and Friesen  [14]  introduced Action Units (AU), which incorporated both permanent and transient facial traits for emotion recognition. The increasing popularity of employing Convolutional Neural Networks (CNNs) in emotion recognition can be attributed to the continuous advancements in methodologies. Lyrical analysis has also been utilized for music classification  [15] ,  [16] . However, relying exclusively on tokenized methods falls short in achieving accurate song categorization. Additionally, the presence of language barriers poses a limitation to classification within a single language, creating a distinct disadvantage in the overall process.\n\nIn the year 2020, T. Vijayakumar  [17]  introduced a research paper that concentrated on tackling inverse problems utilizing Convolutional Neural Networks (CNNs). The study initially employed CNN and later transitioned to direct inversion using a combination of Filtered Back Projection (FBP) and CNN, known as FBP-C. The approach utilized individual learning and a U-net architecture. The synthetic dataset used in the study consisted of 475 training images and 25 validation images. The backpropagation technique employed in the study produced satisfactory results.\n\nIn a recent study carried out in 2021, Sungheetha, Akey, and Rajesh Sharma  [18]  directed their attention towards image classification using Convolutional Neural Networks (CNN) for the early detection of Diabetic Retinopathy. The conventional methods employed for detecting Hard Exudates (HE) in retinopathy images, which are crucial for assessing diabetes severity, were found to be ineffective. To address this challenge, the study proposed the utilization of CNN to extract relevant features from deep networks, offering a viable solution. Deep learning architectures, including CNN, have demonstrated their effectiveness as powerful tools for image recognition, analysis, classification, and identification within the domain of medical imaging.\n\nIn the year 2021, a survey was conducted by Smys, S., Joy Iong Zong Chen, and Subarna Shakya  [19]  to investigate various architectures and design methodologies employed in neural networks. The study classified deep neural networks into three distinct types: hybrid architectures, generative architectures, and discriminative architectures. The hybrid architecture was presented by integrating Convolutional Neural Networks (CNN) with deep belief networks, while the discriminative architecture predominantly relied on CNN, featuring stacked pooling and convolution layers to construct a deep model. The survey provided insights into the diverse approaches and structures employed.\n\nEmotion recognition has emerged as a highly investigated domain within the fields of computer vision and humancomputer interaction. Researchers have explored various techniques, including facial expression analysis, physiological signals, and audio analysis. In the realm of music psychology, Swathi Swaminathan and E. Glenn Schellenberg  [1]  conducted research to shed light on the current state of emotion research, emphasizing the importance of comprehending emotions within music-related contexts. F. Abdat, C. Maaoui, and A. Pruski  [2]  directed their attention toward human-computer interaction and highlighted the significance of facial cues in emotion detection through facial expression recognition. These studies offer valuable insights into the theoretical foundations and practical applications of techniques utilized in emotion recognition.\n\nMusic recommendation systems have the objective of delivering personalized and pertinent music suggestions to users, taking into account their preferences, context, and emotional states. In the realm of mood classification from musical audio, Kyogu Lee and Minsu Cho  [4]  investigated the utilization of user group-dependent models, highlighting the importance of acknowledging user diversity and individual preferences in the realm of music recommendation. In a distinct perspective, Daniel Wolff, Tillman Weyde, and Andrew MacFarlane  [5]  concentrated on culture-aware music recommendation, recognizing the influence of cultural background on music preferences. Additionally, Mirim Lee and Jun-Dong Cho  [15]  developed a context-based social music recommendation service, underscoring the significance of contextual factors in augmenting music recommendation systems. These studies contribute to the comprehension of music recommendation techniques and the factors that impact user satisfaction and engagement.\n\nExplainable AI has gained considerable attention to enhance transparency and interpretability in AI models. Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel, and Marcus Liwicki  [21]  presented DeXpression, a deep convolutional neural network designed for expression recognition. This network integrates explainable AI techniques to provide visualizations of the specific image regions that contribute to the model's predictions. This study demonstrates the potential of explainable AI in enhancing the interpretability of deep learning models. The literature survey explores the existing research and developments in the field of explainable AI and its relevance to the proposed project.\n\nBy reviewing these studies and research papers, it is evident that emotion recognition, music recommendation, and explainable AI are active areas of research with various approaches and techniques. However, there is still a need to integrate these domains to enhance user experience. The proposed project aims to bridge these gaps and provide a comprehensive system that combines emotion recognition from facial cues, music recommendation based on emotions, and explainable AI using GRAD-CAM visualization..",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "As emphasized in the literature review, Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel, and Marcus Liwicki  [21]  presented DeXpression, a deep convolutional neural network designed for expression recognition. This network integrates explainable AI techniques to provide visualizations of the specific image regions that contribute to the model's predictions.Building upon the insights gained from the extensive literature survey, this paper now proceeds to outline the methodology employed to address the research gaps identified in the existing studies. The methodology section represents a fusion of established techniques from prior research and innovative approaches tailored to the specific objectives of our project. By capitalizing on the strengths and limitations of previous methodologies, we have designed a refined framework that aims to advance the fields of facial emotion recognition and music recommendation. This comprehensive methodology encompasses various stages, including dataset acquisition, preprocessing, model selection, training, and evaluation. While also introducing novel strategies to tackle the unique challenges associated with facial emotion recognition and music recommendation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Description",
      "text": "The proposed system utilizes a dataset consisting of facial images with labeled emotions for training and evaluation purposes. The dataset includes real images of different individuals encompassing a diverse range of emotions. The dataset may be augmented and pre-processed to enhance model performance and generalize well to real-world scenarios. The dataset used for training the facial emotion recognition model consists of two components: the FER dataset and real images of different individuals. The dataset for facial expression recognition (FER) consists of categorized facial expressions, encompassing a range of emotions including anger, disgust, fear, happiness, sadness, surprise, and neutrality. This dataset serves as the foundation for training the deep learning model and enables it to learn patterns associated with various emotions. To enhance the diversity and generalization capability of the model, real images of different individuals are also captured and included in the training dataset. In addition to the FER dataset, a music dataset is utilized for generating personalized music recommendations. The music dataset contains a diverse collection of music tracks from various genres and styles. This information serves as the basis for mapping the detected emotions to appropriate music tracks. The dataset for music can be acquired from diverse sources, such as online music platforms, curated databases, or personalized collections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Description",
      "text": "Facial emotion recognition utilizes the ResNet50 model, a deep CNN architecture renowned for its exceptional performance in image classification assignments It consists of 50 layers, including convolutional layers, shortcut connections, and global average pooling. The input layer of the ResNet50 model takes in facial images as input. The convolutional layers extract meaningful features from the input images, capturing the facial expressions' key characteristics. The dense layers process the extracted features and learn the relationship between facial expressions and emotions. Finally, the output layer provides the predicted emotion based on the learned features.\n\nThe output of the dense layer in the network employs the softmax activation function, which enables the prediction of a multinomial probability distribution. This distribution is well-suited for multiclass classification tasks that involve more than two labels. In this particular project, which involves classifying emotions into seven distinct labels, the output for each class is represented as a probability distribution. The network architecture comprises nine convolutional layers, with a max-pooling layer following every three convolutional layers, and two dense layers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input Layer",
      "text": "The input layer of Resnet50 performs a vital function in processing input images prior to their integration into the network. Given that the input images usually have specific dimensions, any images that deviate from these dimensions are either resized or cropped to meet the expected requirements. Furthermore, preprocessing techniques like normalization and data augmentation may be employed to improve the model's resilience and ability to generalize.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Convolutional Layer",
      "text": "The convolutional layers in Resnet50 are responsible for feature extraction. They consist of filters that slide across the input images, convolving with the pixel values to produce feature maps. Each filter specializes in capturing specific patterns or features, such as edges, textures, and shapes. The depth of the network enables Resnet50 to learn increasingly complex and abstract features as the information passes through multiple convolutional layers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dense Layer",
      "text": "Following the convolutional layers, Resnet50 incorporates dense layers, also known as fully connected layers. These layers receive the extracted features from the previous layers and perform high-level reasoning and decision-making. The dense layers are typically comprised of multiple neurons, with each neuron representing a specific class or emotion in the case of facial emotion detection. Through a series of weighted connections and activation functions, the dense layers transform the extracted features into probability scores or confidence values for each class.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Output Layer",
      "text": "The output layer in Resnet50 represents the final stage, responsible for generating predicted emotions for the input images in facial emotion detection. This layer comprises neurons that correspond to various emotions, including happiness, neutral, anger, surprise, fear, disgust, and sadness. The choice of activation function in the output layer depends on the problem's characteristics. In this scenario, a commonly employed activation function is softmax, which ensures that the predicted emotion probabilities sum up to 1. This property facilitates interpretation and comparison of the predictions.\n\nResnet50 excels in facial emotion detection due to its deep architecture and residual connections. The depth enables the network to learn rich and meaningful representations of facial features, capturing intricate details relevant to emotions. The residual connections help alleviate the vanishing gradient problem, allowing for better training and improved performance.\n\nBy leveraging Resnet50 as the core model for facial emotion detection, this research aims to accurately classify the emotional states conveyed by individuals' facial expressions. The trained model is capable of analyzing facial images and predicting the corresponding emotions, contributing to the creating of advanced systems capable of comprehending and appropriately reacting to human emotion.\n\nTo summarize, Resnet50 serves as a robust deep-learning model that constitutes the foundation of the facial emotion detection system created in this study. It leverages its architecture, including convolutional layers, dense layers, and residual connections, to extract features and make accurate predictions. Through extensive training on the dataset, the model can capture the subtle variations in facial expressions and classify them into different emotions. This model serves as a valuable tool for understanding and analyzing human emotions, opening doors to numerous applications in fields like psychology, human-computer interaction, and affective computing.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Facial Emotion Detection With Roi (Eyes):",
      "text": "In the context of facial emotion detection, the eyes are considered a crucial region for accurate emotion recognition. The eyes exhibit significant changes in various emotional states, and capturing these subtle variations can enhance the performance of emotion classification models. To leverage the distinctive features of the eyes, a specific approach was employed, involving the extraction of the region of interest (ROI) using a Haar cascade classifier.\n\nThe Haar cascade classifier is a popular technique in computer vision for object detection, known for its efficiency and accuracy. In this methodology, the Haar cascade classifier was trained to identify and localize the eyes in facial images. Once the eyes were successfully detected, they were cropped and extracted as separate images, creating a specialized dataset specifically consisting of eye regions. This eye-centric dataset was then utilized for training a facial emotion classification model. The model architecture, based on the Resnet50 deep convolutional neural network (CNN), was employed to learn the intricate patterns and features present in the eye regions. The Resnet50 model has been widely recognized for its exceptional performance in computer vision tasks, making it a suitable choice for this research.\n\nDuring the training process, the model was exposed to the eye images from the specialized dataset, with each eye region associated with a corresponding emotional label. The model learned to analyze the eye features and classify them into different emotional states, including happiness, sadness, anger, fear, disgust, surprise, and neutral.\n\nBy focusing solely on the eyes, the model gained a deeper understanding of the specific eye-related cues and expressions associated with each emotion. This approach allowed for a more fine-grained analysis of the eyes' role in emotion recognition, capturing the nuances and subtleties that contribute to accurate classification.\n\nOnce the model was trained on the eye-centric dataset, it was capable of predicting facial emotions based on new, unseen eye regions. During inference, the Haar cascade classifier was utilized to detect and extract the eyes from facial images in real time. These extracted eye regions were then fed into the trained Resnet50 model, which generated predictions of the corresponding emotional states. This methodology offers several advantages. Firstly, by narrowing the focus to the eyes, the model's attention is concentrated on the most expressive and informative facial region, potentially improving the accuracy of emotion detection. Secondly, working with a specialized dataset of eye regions allows for more targeted training, enabling the model to learn eye-specific features more effectively. Lastly, the use of the Haar cascade classifier for eye detection provides a robust and efficient means of isolating the eyes, ensuring accurate extraction even in real-time scenarios.Overall, the integration of facial emotion detection with ROI (Eyes) using the Haar cascade classifier and the Resnet50 model demonstrates a tailored approach to emotion recognition. By leveraging the distinctive features of the eyes and training on eye-specific datasets, this methodology enhances the precision and granularity of facial emotion detection systems, providing valuable insights into the role of the eyes in expressing and recognizing emotions.\n\nBy focusing solely on the eyes, the model gained a deeper understanding of the specific eye-related cues and expressions associated with each emotion. This approach allowed for a more fine-grained analysis of the eyes' role in emotion recognition, capturing the nuances and subtleties that contribute to accurate classification. Once the model was trained on the eye-centric dataset, it was capable of predicting facial emotions based on new, unseen eye regions. During inference, the Haar cascade classifier was utilized to detect and extract the eyes from facial images in real time. These extracted eye regions were then fed into the trained Resnet50 model, which generated predictions of the corresponding emotional states.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Explainable Ai:",
      "text": "Explainable AI (XAI) is an essential aspect of building trustworthy and interpretable machine learning models. It aims to provide insights into the reasoning behind the predictions made by the model, offering transparency and enabling users to understand and trust the decision-making process. In this project, the GRAD-CAM (Gradient-weighted Class Activation Mapping) technique was employed to achieve explainability in the facial emotion detection model. GRAD-CAM is a visualization technique that helps identify the regions of an image that are influential in a model's decision-making process. It generates heatmaps by highlighting the important areas of an input image that contribute most significantly to the predicted class. By applying GRAD-CAM to the facial emotion detection model, we can gain insights into the regions of the face that contribute to the classification of specific emotions.\n\nTo apply GRAD-CAM, the pre-trained Resnet50 model was utilized. After an input image was fed into the model for prediction, the gradients of the target class (the predicted emotion) were computed with respect to the final convolutional layer. These gradients were then used to weigh the activations of the convolutional layer, creating a heatmap that visually represents the regions that influenced the prediction the most. By visualizing the heatmaps generated by GRAD-CAM, we were able to identify the facial regions, such as the eyes, nose, or mouth, that played a significant role in the model's decision-making process. This information can be invaluable for understanding how the model interprets emotions and which facial features contribute most prominently to each emotion classification.\n\nThe incorporation of Explainable AI techniques like GRAD-CAM enhances the transparency and interpretability of the facial emotion detection model. It allows users to understand why specific emotions were predicted for a given input image, providing them with confidence in the system's decision-making process. This interpretability is especially valuable in real-world applications where users need to trust and comprehend the decisions made by AI systems.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fig2. Explainable Ai With Gradcam",
      "text": "By combining facial emotion detection with ROI (Eyes) and Explainable AI using GRAD-CAM, our methodology not only achieved accurate emotion classification but also provided valuable insights into the visual cues and facial regions that contribute to each emotion. This holistic approach fosters transparency, interpretability, and trust in the model's predictions, paving the way for wider acceptance and application of facial emotion recognition systems in various domains.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Result",
      "text": "The results of our study explains the effectiveness of the proposed methodology for facial emotion detection and music recommendation based on real-time emotion recognition. The system achieved promising performance in accurately classifying facial expressions and generating personalized music playlists based on the detected emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Facial Emotion Detection Performance:",
      "text": "The trained Resnet50 model exhibited impressive accuracy in recognizing facial emotions. On the FER dataset and real images of different individuals, the model achieved an overall accuracy of 86%. This indicates that the model can effectively classify emotions such as anger, disgust, fear, happiness, sadness, surprise, and neutral expressions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Region Of Interest (Roi) Analysis:",
      "text": "By focusing on the eyes as the region of interest, we observed that the model's performance improved in detecting subtle changes in emotions. The eye region, known to convey vital emotional cues, proved to be influential in accurately predicting emotions. The use of Haar cascades for eye detection and a separate dataset consisting solely of eye images contributed to the model's enhanced performance in capturing subtle variations in emotional states.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Music Recommendation:",
      "text": "The integrated music recommendation system successfully generated personalized playlists based on the detected emotions. By mapping emotions to corresponding music tracks, users were provided with a curated selection of songs that matched their emotional state. This personalized approach enhanced user satisfaction and engagement with the music player.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Explainability With Grad-Cam:",
      "text": "The incorporation of GRAD-CAM for explainable AI provided insights into the model's decision-making process. The generated heatmaps visualized the facial regions that played a significant role in the model's classification of emotions. This visualization not only validated the model's attention to relevant facial features but also provided transparency and interpretability to end-users, promoting trust and confidence in the system.\n\nOverall, the results demonstrate the successful implementation of the proposed methodology for facial emotion detection and music recommendation. The accuracy achieved in emotion classification, the focus on the eye region, and the incorporation of explainable AI techniques contribute to the robustness, interpretability, and user-centric nature of the system.\n\nThe outcomes of this study have significant implications for various applications such as personalized music streaming, emotion-aware user interfaces, and affective computing. The combination of facial emotion detection, ROI analysis, music recommendation, and explainable AI provides a comprehensive framework that enhances user experience, engagement, and satisfaction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion And Future Scope",
      "text": "In conclusion, this paper presented a novel approach for facial emotion detection and music recommendation based on real-time emotion recognition. The developed system achieved high accuracy in classifying facial expressions, leveraging the power of the Resnet50 model. By incorporating a region of interest (ROI) analysis focusing on the eyes and utilizing a separate dataset of eye images, the system demonstrated improved performance in capturing subtle emotional cues.\n\nThe integration of music recommendations based on the detected emotions enhanced the user experience, providing personalized playlists that resonated with the user's emotional state. The system's effectiveness was further enhanced by incorporating explainable AI techniques, particularly the GRAD-CAM method, which provided insights into the model's decision-making process and enhanced transparency.\n\nThe results of this paper have several implications for future research and development. Some potential areas for further exploration and improvement include: Expansion of Emotion Categories: While the current system successfully classified emotions into seven categories, future work could involve expanding the range of emotions recognized. This could include more nuanced emotional states or cultural-specific emotions, allowing for a more comprehensive understanding of users' emotional experiences.\n\nMulti-modal Emotion Recognition: Incorporating additional modalities, such as voice or gesture recognition, alongside facial emotion detection, can provide a more holistic understanding of users' emotional states. Multi-modal approaches have the potential to enhance the accuracy and robustness of emotion detection systems.\n\nReal-Time System Deployment: While our system performed real-time emotion recognition, further optimization and deployment on low-latency platforms can ensure its practical usability in real-world scenarios, such as interactive applications or emotion-aware systems.\n\nUser Feedback and Personalization: Integrating user feedback mechanisms can enable the system to adapt and personalize its recommendations based on individual preferences and responses. User feedback loops can contribute to continuous improvement and user satisfaction.\n\nGeneralization to Diverse Populations: Future research should focus on expanding the diversity of the dataset utilised for training the model, encompassing individuals from various demographics, cultures, and age groups. This will ensure the generalizability and inclusiveness of the system across different populations.\n\nIn conclusion, our proposed system demonstrates the potential of combining facial emotion detection, ROI analysis, music recommendation, and explainable AI techniques to create a user-centric, personalized experience. The achieved results, along with the future scope outlined, contribute to the advancement of affective computing and emotion-aware systems, with implications in fields such as entertainment, healthcare, and human-computer interaction.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample images from the dataset.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "INTRODUCTION: Music provides an incredible avenue for individuals to express their thoughts and emotions, while also"
        },
        {
          "Column_1": "serving as a delightful mode of entertainment for enthusiasts and music lovers."
        },
        {
          "Column_1": "OBJECTIVES: This paper presents a comprehensive approach to enhancing the user experience through the integration of"
        },
        {
          "Column_1": "emotion recognition, music recommendation, and explainable AI using GRAD-CAM."
        },
        {
          "Column_1": "METHODS: The proposed methodology utilizes a ResNet50 model trained on the Facial Expression Recognition (FER)"
        },
        {
          "Column_1": "dataset, consisting of real images of individuals expressing various emotions."
        },
        {
          "Column_1": "RESULTS: The system achieves an accuracy of 82% in emotion classification. By leveraging GRAD-CAM, the model"
        },
        {
          "Column_1": "provides explanations for its predictions, allowing users to understand the reasoning behind the system's recommendations."
        },
        {
          "Column_1": "The model is trained on both FER and real user datasets, which include labelled facial expressions, and real images of"
        },
        {
          "Column_1": "individuals expressing various emotions. The training process involves pre-processing the input images, extracting features"
        },
        {
          "Column_1": "through convolutional layers, reasoning with dense layers, and generating emotion predictions through the output layer"
        },
        {
          "Column_1": "CONCLUSION: The proposed methodology, leveraging the Resnet50 model with ROI-based analysis and explainable AI"
        },
        {
          "Column_1": "techniques, offers a robust and interpretable solution for facial emotion detection paper."
        }
      ],
      "page": 1
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Current emotion research in music psychology",
      "authors": [
        "E Swathi Swaminathan",
        "Schellenberg"
      ],
      "year": "2015",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "2",
      "title": "Human-computer interaction using emotion recognition from facial expression",
      "authors": [
        "F Abdat",
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2011",
      "venue": "UKSim 5th European Symposium on Computer Modeling and Simulation"
    },
    {
      "citation_id": "3",
      "title": "How music changes your mood",
      "year": "2017",
      "venue": "How music changes your mood"
    },
    {
      "citation_id": "4",
      "title": "Mood Classification from Musical Audio Using User Group-dependent Models",
      "authors": [
        "Kyogu Lee",
        "Minsu Cho"
      ],
      "venue": "Mood Classification from Musical Audio Using User Group-dependent Models"
    },
    {
      "citation_id": "5",
      "title": "Culture-aware Music Recommendation",
      "authors": [
        "Daniel Wolff",
        "Tillman Weyde",
        "Andrew Macfarlane"
      ],
      "venue": "Culture-aware Music Recommendation"
    },
    {
      "citation_id": "6",
      "title": "Using Animated Mood Pictures in Music Recommendation",
      "authors": [
        "A Lehtiniemi",
        "J Holm"
      ],
      "year": "2012",
      "venue": "16th International Conference on Information Visualization"
    },
    {
      "citation_id": "7",
      "title": "Face Detection and Facial Expression Recognition System",
      "authors": [
        "A Dhavalikar",
        "Dr Kulkarni"
      ],
      "venue": "International Conference on Electronics and Communication Systems"
    },
    {
      "citation_id": "8",
      "title": "Extraction Of Audio Features For Emotion Recognition System Based On Music",
      "authors": [
        "K Han",
        "T Zin",
        "H Tun"
      ],
      "year": "2016",
      "venue": "International Journal Of Scientific & Technology Research"
    },
    {
      "citation_id": "9",
      "title": "Emotion detection of audio files",
      "authors": [
        "R Taneja",
        "A Bhatia",
        "J Monga",
        "P Marwaha"
      ],
      "year": "2016",
      "venue": "IEEE Computing for Sustainable Global Development (INDIACom), 2016 3rd International Conference on"
    },
    {
      "citation_id": "10",
      "title": "Emotion Based Music Player Using Facial Recognition",
      "authors": [
        "V Ghule",
        "A Benke",
        "S Jadhav",
        "S Joshi"
      ],
      "year": "2017",
      "venue": "International Journal of Innovative Research in Computer and Communication Engineering"
    },
    {
      "citation_id": "11",
      "title": "Extracting emotions from music data",
      "authors": [
        "A Wieczorkowska",
        "P Synak",
        "R Lewis",
        "Z Ra≈õ"
      ],
      "year": "2005",
      "venue": "International Symposium on Methodologies for Intelligent Systems"
    },
    {
      "citation_id": "12",
      "title": "A regression approach to music emotion recognition",
      "authors": [
        "Y Yang",
        "Y Lin",
        "Y Su",
        "H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "13",
      "title": "Evaluation of Musical Features for Emotion Classification",
      "authors": [
        "Y Song",
        "S Dixon",
        "M Pearce"
      ],
      "year": "2012",
      "venue": "ISMIR"
    },
    {
      "citation_id": "14",
      "title": "Recognizing lower face action units for facial expression analysis",
      "authors": [
        "Ying-Li Tian",
        "T Kanade",
        "J Cohn"
      ],
      "year": "2000",
      "venue": "Proceedings of the 4th IEEE International Conference on Automatic Face and Gesture Recognition (FG'00)"
    },
    {
      "citation_id": "15",
      "title": "Logmusic: context-based social music recommendation service on mobile device",
      "authors": [
        "Mirim Lee",
        "Jun-Dong Cho"
      ],
      "year": "2014",
      "venue": "Ubicomp'14 Adjunct"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns",
      "authors": [
        "Gil Levi",
        "Tal Hassner"
      ],
      "venue": "Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns"
    },
    {
      "citation_id": "17",
      "title": "Posed Inverse Problem Rectification Using Novel Deep Convolutional Neural Network",
      "authors": [
        "T Vijayakumar"
      ],
      "year": "2020",
      "venue": "Journal of Innovative Image Processing"
    },
    {
      "citation_id": "18",
      "title": "Design an Early Detection and Classification for Diabetic Retinopathy by Deep Feature Extraction based Convolution Neural Network",
      "authors": [
        "A Sungheetha",
        "Rajesh Sharma"
      ],
      "year": "2021",
      "venue": "Journal of Trends in Computer Science and Smart technology (TCSST)"
    },
    {
      "citation_id": "19",
      "title": "Survey on Neural Network Architectures with Deep Learning",
      "authors": [
        "S Smys",
        "Iong Zong Joy",
        "Subarna Chen",
        "Shakya"
      ],
      "year": "2020",
      "venue": "Journal of Soft Computing Paradigm (JSCP)"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised feature learning and deep learning Tutorial",
      "year": "2016",
      "venue": "Unsupervised feature learning and deep learning Tutorial"
    },
    {
      "citation_id": "21",
      "title": "DeXpression: Deep Convolutional Neural Network for Expression Recognition",
      "authors": [
        "Peter Burkert",
        "Felix Trier",
        "Muhammad Zeshan Afzal",
        "Andreas Dengel",
        "Marcus Liwicki"
      ],
      "venue": "DeXpression: Deep Convolutional Neural Network for Expression Recognition"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised feature learning and deep learning Tutorial",
      "year": "2017",
      "venue": "Unsupervised feature learning and deep learning Tutorial"
    },
    {
      "citation_id": "23",
      "title": "Challenges in Representation Learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow"
      ],
      "venue": "Challenges in Representation Learning: A report on three machine learning contests"
    },
    {
      "citation_id": "24",
      "title": "Face recognition: a convolutional neural-network approach",
      "authors": [
        "S Lawrence",
        "C Giles",
        "Ah Chung Tsoi",
        "A Back"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "25",
      "title": "An extended set of haarlike features for rapid object detection",
      "authors": [
        "Rainer Lienhart",
        "Jochen Maydt"
      ],
      "year": "2002",
      "venue": "Proceedings. 2002 International Conference on"
    }
  ]
}