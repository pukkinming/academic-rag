{
  "paper_id": "2408.12124v1",
  "title": "Recording Brain Activity While Listening To Music Using Wearable Eeg Devices Combined With Bidirectional Long Short-Term Memory Networks",
  "published": "2024-08-22T04:32:22Z",
  "authors": [
    "Jingyi Wang",
    "Zhiqun Wang",
    "Guiran Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) signals are crucial for investigating brain function and cognitive processes. This study aims to address the challenges of efficiently recording and analyzing highdimensional EEG signals while listening to music to recognize emotional states. We propose a method combining Bidirectional Long Short-Term Memory (Bi-LSTM) networks with attention mechanisms for EEG signal processing. Using wearable EEG devices, we collected brain activity data from participants listening to music. The data was preprocessed, segmented, and Differential Entropy (DE) features were extracted. We then constructed and trained a Bi-LSTM model to enhance key feature extraction and improve emotion recognition accuracy. Experiments were conducted on the SEED and DEAP datasets. The Bi-LSTM-AttGW model achieved 98.28% accuracy on the SEED dataset and 92.46% on the DEAP dataset in multi-class emotion recognition tasks, significantly outperforming traditional models such as SVM and EEG-Net. This study demonstrates the effectiveness of combining Bi-LSTM with attention mechanisms, providing robust technical support for applications in braincomputer interfaces (BCI) and affective computing. Future work will focus on improving device design, incorporating multimodal data, and further enhancing emotion recognition accuracy, aiming to achieve practical applications in real-world scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The study of Electroencephalography (EEG) signals  [1]  has garnered significant attention in the fields of neuroscience and computer science. EEG signals, which reflect the brain's electrophysiological activity, are crucial tools for investigating brain function and cognitive processes. The advent of deep learning and wearable technology has revolutionized the ability to record and analyze EEG signals in real-time using portable devices  [2] . This advancement not only facilitates research but also expands the applications in brain-computer interfaces (BCI) and affective computing  [3, 4] .\n\nMusic, as a complex auditory stimulus, profoundly influences emotional and cognitive functions of the brain  [5, 6] . Research indicates that different types of music can elicit various neural responses, thereby affecting emotional states. Recording brain activity via EEG while listening to music provides deep insights into the mechanisms by which music influences emotions. This is particularly valuable for applications in music therapy and other practical uses. However, the complexity and high dimensionality of EEG signals pose challenges in efficiently extracting and analyzing useful information  [7] .\n\nThis study addresses the critical need for advanced methods in EEG signal processing to enhance emotion recognition accuracy. Existing methods often struggle with the high dimensionality and complexity of EEG data. By introducing a novel combination of Bi-LSTM and attention mechanisms, this research aims to overcome these challenges and provide a robust solution for real-time emotion recognition using wearable EEG devices. The significance of this study lies in its potential applications in brain-computer interfaces, music therapy, and affective computing, where accurate emotion recognition can greatly enhance user experience and therapeutic outcomes.\n\nBidirectional Long Short-Term Memory (Bi-LSTM) networks  [8] [9] [10] [11] , an advanced type of Recurrent Neural Network (RNN)  [12] [13] [14] [15] [16] , are well-suited for this task. Bi-LSTM networks can leverage both past and future information in time series data, making them adept at capturing long-term dependencies. Applying Bi-LSTM to EEG signal analysis enhances the modeling capabilities for complex time-series data, thereby improving the accuracy and robustness of emotion recognition  [17] [18] [19] [20] [21] . Additionally, incorporating attention mechanisms allows Bi-LSTM to focus on critical features, further boosting model performance.\n\nIn this study, we utilize wearable devices to record EEG signals from participants while they listen to music. We then analyze these signals using a Bi-LSTM model to explore the impact of music on brain activity. The process involves low-pass filtering of EEG signals, feature extraction and selection, and the construction and training of the Bi-LSTM model. Our goal is to achieve efficient recording and precise prediction of brain activity. The results of this study will not only enhance the accuracy of emotion recognition but also provide substantial support for applications in BCI and affective computing.\n\nThis study addresses the critical need for advanced methods in EEG signal processing to enhance emotion recognition accuracy. Existing methods often struggle with the high dimensionality and complexity of EEG data. By introducing a novel combination of Bi-LSTM and attention mechanisms, this research aims to overcome these challenges and provide a robust solution for real-time emotion recognition using wearable EEG devices. The significance of this study lies in its potential applications in brain-computer interfaces, music therapy, and affective computing, where accurate emotion recognition can greatly enhance user experience and therapeutic outcomes.\n\nThe main contributions of our work are as follows:\n\n‚Ä¢ We propose a novel EEG signal processing method using Bi-LSTM and attention mechanisms, significantly enhancing emotion recognition accuracy.\n\n‚Ä¢ Our method enables real-time brain activity recording and analysis under music stimulation using portable EEG devices.\n\n‚Ä¢ The effectiveness of our model is validated on SEED and DEAP datasets, achieving a high accuracy of 98.28% in emotion recognition.\n\nThe remainder of this paper is structured as follows. Section 2 reviews related work on wearable EEG signal monitoring devices, EEG recording and MRI neuroimaging, applications of recurrent neural networks in EEG analysis, and auditory neural stimulation on the brain. Section 3 describes the proposed method, including EEG feature extraction, the construction of a 3D adjacency matrix of graph convolutional neural networks, and the Bi-LSTM model for EEG signal recognition. Section 4 presents relevant experimental results and analysis on the SEED and DEAP datasets. Finally, Section 5 concludes the paper with a summary and future research directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wearable Eeg Signal Monitoring Devices",
      "text": "Wearable EEG signal monitoring devices have gained widespread application in neuroscience, psychology, and biomedical engineering  [22, 23] . Their portability and ease of use make them essential tools in various domains, including affective computing, BCI, sleep management, emotion regulation, depression treatment, and fatigue monitoring. These devices, such as the Emotiv EPOC+ and Muse  [24] , enable real-time recording and analysis of an individual's emotional state by capturing EEG signals and identifying emotional changes, facilitating emotion regulation and psychological therapy  [25] . In BCI technology, EEG devices decode brain signals to help users control external devices, such as enabling individuals with disabilities to operate wheelchairs. In sleep management, these devices analyze brain activity during sleep, providing feedback on sleep quality and promoting better sleep habits. They also aid in the long-term monitoring of emotional changes for diagnosing and treating emotional disorders, and in fatigue monitoring by providing real-time alerts during work or driving to prevent accidents.\n\nWearable EEG devices offer numerous advantages. Their portability and ease of use make them suitable for long-term wear and operation, meeting various application scenarios  [26] . These devices provide real-time monitoring of brain signals, offering immediate feedback and monitoring results. Advanced devices like Mindeep support multiple electrode types, feature long battery life, impedance detection, and WiFi wireless transmission, and can provide high-quality raw data and multi-device synchronous data collection. However, these devices also have limitations. Despite offering higher sampling rates and data quality, many commercial devices still face constraints in sampling rate, signal resolution, and noise control. EEG signals are susceptible to artifacts from eye movements and muscle activity, necessitating complex preprocessing and signal processing algorithms, increasing data analysis complexity  [27] [28] [29] [30] [31] . High-end devices are expensive, and cost considerations remain a factor for average users and some research institutions. Additionally, dry electrode devices may face issues with electrode contact and signal distortion, affecting data accuracy and stability  [32, 33] .\n\nUsing these devices, researchers can efficiently record real-time brain activity data from participants while they listen to music, allowing for accurate analysis and prediction of brain responses to musical stimuli. Bi-LSTM models  [34] [35] [36] [37] [38]  capture long-term dependencies in EEG signals and, combined with attention mechanisms, improve the extraction of key features. This method not only enhances the accuracy of emotion recognition but also provides robust technical support for BCI and affective computing applications. Future improvements in hardware design and signal processing algorithms can address current device limitations, further enhancing EEG signal processing efficiency and accuracy, offering broader applications in neuroscience research and healthcare.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Eeg Recording And Mri Neuroimaging",
      "text": "Electroencephalography (EEG) and magnetic resonance imaging (MRI) are pivotal in neuroscience and biomedical engineering, each offering unique advantages  [39] . EEG involves placing electrodes on the scalp to capture the brain's electrical activity in real-time, boasting high temporal resolution. It is extensively used in affective computing, BCI, and cognitive neuroscience. Functional MRI (fMRI)  [40] , which measures blood oxygen level-dependent (BOLD) signals  [41] , provides high spatial resolution images of brain structure and function, making it invaluable for brain function localization, disease diagnosis, and cognitive research. Combining these techniques allows for multi-faceted brain activity analysis; for example, EEG can monitor real-time emotional changes while fMRI can precisely locate emotionrelated brain activity.\n\nEach method has distinct advantages and limitations. EEG's high temporal resolution enables millisecond-level recording of brain activity, ideal for studying rapid neural processes. Its portability and non-invasiveness make longterm monitoring and convenient operation possible, especially with wearable devices. The relatively low cost of EEG equipment makes it suitable for large-scale and daily monitoring. However, EEG's low spatial resolution limits its ability to reflect deep brain activity, and EEG signals are prone to artifacts, requiring complex preprocessing and signal processing algorithms. On the other hand, MRI's high spatial resolution provides detailed images of brain structures and functions. Its multi-modal imaging capabilities, such as combining structural imaging, functional imaging, and diffusion tensor imaging (DTI)  [42] , offer comprehensive brain information. MRI is also non-invasive, suitable for repeated examinations. Nonetheless, MRI's low temporal resolution makes it challenging to capture rapid neural activity changes. The high cost and operational complexity of MRI equipment, requiring professional maintenance and operation, limit its widespread use. Additionally, MRI is sensitive to motion artifacts, necessitating strict control of the experimental environment.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Applications Of Recurrent Neural Networks In Eeg Analysis",
      "text": "Recurrent Neural Networks (RNNs)  [43] , particularly Long Short-Term Memory (LSTM) and Bidirectional Long Short-Term Memory (Bi-LSTM) networks, are widely applied in EEG signal processing  [44] . RNNs handle time series data, capturing temporal dependencies, making them particularly suitable for EEG signal analysis. LSTM networks address the vanishing gradient problem of traditional RNNs, making them ideal for long-sequence data analysis. Anitha and Hemanth proposed emotion recognition models based on LSTM and Gated Recurrent Unit (GRU) networks  [45] [46] [47] , achieving efficient emotion classification through EEG signal processing, applicable in BCI and affective computing. Hybrid models like CNN-LSTM combine convolutional neural networks (CNN)  [48] [49] [50] [51] [52]  with LSTM  [53] [54] [55] [56] [57]  to extract spatial and temporal features, enhancing emotion recognition accuracy.\n\nRNNs and their variants  [58] [59] [60]  offer significant advantages and disadvantages in EEG signal processing. RNNs excel in handling dynamic time series data, making them ideal for analyzing rapidly changing EEG signals. LSTM networks effectively address the vanishing gradient problem of traditional RNNs, capturing long-term dependencies and improving the modeling of long-sequence data. Bi-LSTM networks further utilize both past and future information in sequences, enhancing the understanding of complex time series, showing excellent performance in emotion recognition and BCI systems. However, RNN models face challenges, including high computational complexity and long training times when processing large-scale data, requiring substantial computational resources. RNNs are also prone to overfitting, especially with high-dimensional data, necessitating regularization techniques and other methods to prevent overfitting. Despite LSTM networks mitigating the vanishing gradient problem, they may still encounter challenges when dealing with extremely long time sequences.\n\nUsing wearable EEG devices to record participants' brain activity in real-time while listening to music allows for accurate analysis and prediction of brain responses to musical stimuli. Bi-LSTM models capture long-term dependencies in EEG signals and, combined with attention mechanisms, improve the extraction of key features. This method not only enhances the accuracy of emotion recognition but also provides robust technical support for BCI and affective computing applications. Future advancements in RNN models and signal processing algorithms can better address current technical limitations, offering broader applications in neuroscience research and healthcare.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Auditory Neural Stimulation On The Brain",
      "text": "Auditory stimuli, particularly music, significantly impact the brain and are widely used in neuroscience and psychology research  [61] . Music is known to activate multiple brain regions, including the auditory cortex, limbic system, and prefrontal cortex, thereby influencing emotional and cognitive functions. In affective computing, emotional responses induced by music are used to study the brain's emotion processing mechanisms  [62] . For instance, by analyzing EEG signals while listening to music, researchers can identify the emotional states elicited by different musical stimuli, which is valuable in music therapy and emotion computing. In cognitive neuroscience, music is used to study cognitive functions such as attention and memory. Additionally, auditory stimuli are applied in BCI systems to decode brain signals induced by auditory stimuli, enabling control of external devices.\n\nAuditory stimulation offers numerous advantages in studying brain activity  [63] . Music, as a complex auditory stimulus, can activate multiple brain regions, providing researchers with a comprehensive understanding of the brain's emotional and cognitive processing mechanisms. Music stimuli are easy to control and standardize, ensuring high repeatability and reliability of experimental results. Furthermore, the effects of music on the brain are apparent and significant, allowing researchers to clearly observe changes in brain activity through EEG signals  [64] .\n\nHowever, auditory stimulation also presents some challenges. Firstly, individual differences in emotional responses to music can significantly affect the generalizability and consistency of experimental results. Secondly, music-induced EEG signals are complex, involving multiple frequency bands and brain regions, requiring sophisticated signal processing and analysis techniques. Additionally, the effects of music on the brain are transient, posing challenges in effectively capturing and analyzing these brief changes.\n\nBy using wearable EEG devices to record brain activity while participants listen to music, researchers can monitor and analyze the brain's response to musical stimuli in real-time. Combining these recordings with Bi-LSTM networks allows for more accurate analysis and prediction of the brain's emotional and cognitive responses to different musical stimuli. Bi-LSTM models capture long-term dependencies in EEG signals and, combined with attention mechanisms, improve the extraction of key features. This combined approach not only enhances emotion recognition accuracy but also provides strong technical support for BCI and affective computing applications. Future research should focus on optimizing auditory stimulus control and EEG signal analysis methods to better understand and utilize the effects of auditory neural stimulation on the brain, providing broader applications in neuroscience and psychology.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "As shown in Figure  1 , the overall experimental model involves extracting features from the EEG and ERP signals of the SEED and DEAP datasets. We processed these signals in segments before applying the Bi-LSTM model  [65] [66] [67] [68] [69] . The Bi-LSTM architecture comprises an enhanced Bi-LSTM layer, an attention weighting layer, two fully attentive layers, and a Softmax classification layer. For feature extraction of the P300 component in ERP data and general EEG signals, we performed channel selection, filtering, and segmentation. However, the extraction of P300 features from ERP data included an additional step of Independent Component Analysis (ICA) to remove prominent signal noise and artifacts.\n\nThe subsequent Bi-LSTM model incorporated our attention gate method, applying attention weighting to both types of signal features. We utilized dropout techniques in the fully connected layers to prevent overfitting. Separate experiments were conducted on the SEED and DEAP datasets. The SEED dataset experiments primarily focused on processing EEG signals, while the DEAP dataset experiments concentrated on ERP signal processing.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Feature Extraction",
      "text": "In the experiment, the differential entropy (DE)  [70]  feature in the frequency domain is used as the input for emotion recognition. The extraction process of the differential entropy (DE) feature in the frequency domain is as follows.\n\nDifferential entropy (DE) extends the Shannon information entropy ùêª(ùëã) = -\n\nto continuous random variables, as shown in Equation  1 .\n\nwhere ùëù(ùë•) represents the continuous probability density function, and [ùëé, ùëè] represents the interval of information extraction. For an EEG signal segment that approximately follows a Gaussian distribution ùëÅ(ùúá, ùúé 2 ), its differential entropy is equal to the logarithm of its energy spectrum in a specific frequency band, as shown in Equation  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Construction Of 3D Adjacency Matrix Of Graph Convolutional Neural Networks",
      "text": "In EEG-based emotion recognition research utilizing graph neural networks  [71] [72] [73] , it is known that describing the relationships between different EEG electrode channels, i.e., constructing an adjacency matrix, is crucial for EEG emotion classification. In this section, we use the spatial distance between EEG nodes to construct the adjacency matrix representing the topology of EEG channels. The specific construction process is as follows:\n\nThe adjacency matrix A ‚àà ‚Ñù ùëõ√óùëõ , where ùëõ denotes the number of channels in the EEG signals. Each entry ùëé ùëñùëó is learnable and represents the connection weight between channel ùëñ and channel ùëó. The international 10-20 EEG electrode system provides the three-dimensional coordinates of each electrode mapped onto a unit sphere. The physical distance between two electrodes is used to measure the connection relationship in the brain space. The farther the distance, the less tightly the channels are connected. Suppose the coordinates of two points on the sphere with radius ùëü are (ùë• ùëñ , ùë¶ ùëñ , ùëß ùëñ ) and (ùë• ùëó , ùë¶ ùëó , ùëß ùëó ), the distance ùëë ùëñùëó between the two points in Cartesian space can be expressed as shown in Equation  3 .\n\nFigure  2  below is a schematic diagram of the 3D spatial relationship of EEG channels used to construct the model's adjacency matrix. The points in the 3D graph represent the electrode positions of the wearable EEG device used for measuring brainwave activity. In sparse fMRI networks, using about 20% of all possible connections typically maximizes the efficiency of network topology. Therefore, for each EEG channel, we retain connections to its nearest ùêæ channels, considering them as connected. The value of ùêæ is chosen based on the number of electrode channels used in the EEG data acquisition equipment. For devices with 62 electrode channels, the value of ùêæ is selected as 62 √ó 20% ‚âà 12.\n\nThe asymmetry of neural activity between the left and right hemispheres is significant in valence and arousal prediction. Therefore, we select certain electrode pairs to initialize the adjacency matrix. To leverage the asymmetry information between the left and right hemispheres, we use 9 global connection pairs and initialize the global interchannel relationships in the adjacency matrix as shown in Equation  4 :\n\nAs shown in Figure  3 , the global channel pairs (ùëñ, ùëó) used for initialization are (FP1, FP2), (AF3, AF4), (F5, F6), (FC5, FC6), (C5, C6), (CP5, CP6), (P5, P6), (PO5, PO6), and (O1, O2).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Bi-Lstm Constructs Eeg Signal Recognition",
      "text": "Bi-LSTM is a deep learning model based on LSTM. LSTM networks, through their unique gating mechanisms, can effectively capture and learn long-term dependencies in sequential data, addressing the issues of gradient vanishing and exploding in traditional RNNs (Recurrent Neural Networks).\n\nAs shown in Figure  3 , unlike traditional unidirectional LSTM, Bi-LSTM comprises two LSTM layers: one processes the sequence forward, and the other processes it backward. This bidirectional mechanism enables Bi-LSTM to leverage both past and future information in the sequence, enhancing the model's understanding of time-series data. In Bi-LSTM, the forward LSTM layer processes EEG samples from time index 1 to ùë°, generating the forward hidden state sequence ‚Éñ‚Éñ ‚Éó ‚Ñé ùë° , while the backward LSTM layer processes EEG samples from time index ùë° + 1 to the end, generating the backward hidden state output ‚Éñ‚Éñ‚Éñ ‚Ñé ùë° . This bidirectional processing mechanism allows Bi-LSTM to capture dynamic changes in EEG signals and more accurately model long-term dependencies in the signals. The left part of Figure  4  illustrates the original architecture of LSTM, where the cell's output state update is related to the previous hidden layer output and the current input. At each time step ùë°, the LSTM unit receives the current input ùë• ùë° , the hidden state ‚Ñé ùë°-1 , and the cell state ùëê ùë°-1 from the previous time step. The LSTM controls information storage and transfer through three gating mechanisms: the input gate, the forget gate, and the output gate. The input gate ùëñ ùë° determines the update of the cell state by the current input, the forget gate ùëì ùë° decides whether to retain the previous cell state ùëê ùë°-1 , and the output gate ùëú ùë° decides whether to transfer the hidden state ‚Ñé ùë°-1 to the next LSTM unit. The candidate cell state cùë° is a nonlinear transformation of the current input and the previous hidden state. The current cell state ùëê ùë° is updated through the forget gate and the input gate, and the current hidden state ‚Ñé ùë° is determined by the output gate and the nonlinear transformation of the current cell state. Here, ùúé represents the sigmoid activation function, tanh represents the tanh activation function, ‚äô denotes elementwise multiplication, ùëä and ùëà are weight matrices, and ùëè is the bias vector. The formulas for the LSTM unit are shown below:\n\nBi-LSTM combines the outputs of the forward and backward LSTM layers to form the final hidden state output ‚Ñé ùë° . The forward hidden state is computed by the forward LSTM layer, and the backward hidden state is computed by the backward LSTM layer. By combining the forward and backward hidden states, Bi-LSTM can more comprehensively understand the temporal information in EEG signals, leading to higher accuracy in emotion recognition. The formulas for the Bi-LSTM unit are shown below:\n\nIn emotion recognition models, Bi-LSTM effectively captures the temporal dynamics in EEG signals through its bidirectional mechanism. Using wearable frontal EEG signal monitoring devices, EEG signals of participants are recorded while they listen to different types of music. The device employs dry electrode technology, simplifying the signal acquisition process and making it suitable for largescale daily applications. The collected EEG signals undergo preprocessing, including denoising, filtering, and feature extraction, to ensure signal quality and analysis accuracy. The Bi-LSTM model processes the preprocessed EEG signals through forward and backward LSTM layers, extracting deep features. By incorporating attention mechanisms to focus on important moments in the signals, this study introduces an improved LSTM method, as shown on the right side of Figure  4 , which incorporates the attention mechanism to capture essential historical information and update the cell state. This ultimately improves the accuracy and robustness of emotion recognition, as represented by Eq. 7:\n\nwhere ùë£ ùëì and ùë§ ùëì are the parameters of the attention mechanism. This method reduces the dimensionality of the training parameters compared to Eq. 5.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "SEED dataset. The SEED dataset  [74] , collected by Shanghai Jiao Tong University, involves participants watching movie clips of various emotional types, with their EEG signals recorded in response to these stimuli. This dataset DEAP dataset. The DEAP dataset  [75] is a multimodal dataset specifically designed for affective analysis, offering a rich collection of electroencephalogram (EEG), physiological signals, and video data. This dataset involves 32 participants who watched 40 one-minute-long music video excerpts, during which their emotional responses were recorded. The first part of the dataset comprises evaluations of the music videos by 14 to 16 participants, who rated the videos based on arousal, valence, and dominance. The second part of the dataset includes ratings, physiological recordings, and facial videos from 32 volunteers while watching the aforementioned 40 music video excerpts. In addition to emotional state ratings, the physiological recordings primarily consist of EEG data. The objective of the DEAP dataset is to provide researchers with a standardized dataset for testing and validating their methods of estimating emotional states, thereby advancing research in affective computing, emotion recognition, and brain-computer interfaces. The EEG preprocessing workflow includes the following steps:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Preprocessing And Feature Extraction",
      "text": "(1) Downsampling: The original EEG signals were downsampled to reduce computational complexity (SEED: 1000 Hz to 200 Hz; DEAP: 512 Hz to 128 Hz). (2) Bad channel detection and removal: Channels with excessive noise or artifacts were identified and removed.\n\n(3) Electrode re-referencing: Signals were re-referenced to a common average reference.\n\n(4) Bandpass filtering: A bandpass filter (0.5-50 Hz) was applied to remove irrelevant frequencies and noise.\n\n(5) Artifact removal using ICA: Independent Component Analysis (ICA) was performed to remove EOG, EMG, and other artifacts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion Evoked Eeg Signal Generation And Acquisition",
      "text": "EEG is a method of recording the electrophysiological activity of the brain's neural tissues on the surface of the cerebral cortex. Neuronal excitation and inhibition in the brain generate voltage fluctuations, typically measured from the scalp in the range of 10 to 100 ùúáV in adults. Information in EEG signals is primarily contained within the frequency spectrum of 0.5 to several tens of Hertz. Based on frequency differences, EEG signals can be classified into five bands: ùõø band (Delta, 1-4 ùêªùëß), ùúÉ band (Theta, 4-8 ùêªùëß), ùõº band (Alpha, 8-12 ùêªùëß), ùõΩ band (Beta, 12-30 ùêªùëß), and ùõæ band (Gamma, > 30 Hz). The rhythmic characteristics of each band are detailed in Table  2 .\n\nEEG signal collection generally employs either dry electrode or wet electrode methods. The current predominant method for EEG data collection is the wet electrode method, which has the advantage of obtaining more distinct EEG data but is inconvenient and less suitable for practical, everyday collection. The use of dry electrodes for EEG signal collection does not require an electrolyte, allowing the electrodes to contact the scalp directly, thereby offering greater convenience. However, due to the higher impedance of the stratum corneum, the EEG signals collected in this manner tend to be weaker. Figure  5  illustrates the layout of the 130-electrode system used in this study, which is both easy to implement and ensures test reproducibility. In the second phase of the experiment, 31 EEG channels were selected (indicated by the blue electrodes in Figure  5 ), with 13 channels located in the prefrontal cortex and 18 channels in the occipital lobe.\n\nEEG signals have low amplitudes and are often contaminated with various noise signals during acquisition. The The EEG preprocessing workflow includes downsampling, bad channel detection, electrode re-referencing, bandpass filtering, and Independent Component Analysis (ICA)  [76]  for artifact removal.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Bi-Lstm Construction And Simulation Parameters",
      "text": "The Bi-LSTM model employed in this study is designed to process EEG signals effectively, leveraging both past and future information to capture long-term dependencies and enhance emotion recognition accuracy. Below are the details of the model construction, hyperparameters, time settings, and experimental parameters:\n\nBi-LSTM Model Architecture. The Bi-LSTM architecture consists of the following components: An enhanced Bi-LSTM layer with 128 hidden units in each direction (forward and backward); An attention weighting layer to emphasize critical features in the EEG signals; Two fully connected layers with 64 neurons each, using ReLU activation functions; A Softmax classification layer to output the probability distribution of emotional states.\n\nHyperparameters. The hyperparameters for the Bi-LSTM model were carefully selected based on empirical experiments and existing literature. The following settings were used: Learning rate: 0.001;Batch size: 64; Epochs: 100; Dropout rate: 0.5 to prevent overfitting; Optimizer: Adam.\n\nTime Settings and Data Preprocessing. For the SEED dataset, EEG signals were recorded at a sampling rate of 1000 Hz and then downsampled to 200 Hz to reduce computational complexity. Each recording session lasted approximately 4 minutes per clip. The EEG signals were segmented into 1-second non-overlapping windows, resulting in 200 data points per segment.\n\nFor the DEAP dataset, EEG signals were recorded at a sampling rate of 512 Hz and then downsampled to 128 Hz. Each music video excerpt lasted 1 minute, and the signals were segmented into 1-second windows with 128 data points per segment.\n\nExperimental Parameters and Settings. The experimental setup included the following key parameters: Feature extraction: Differential Entropy (DE) features were extracted from the segmented EEG signals; Model training: The Bi-LSTM model was trained on 80% of the data and validated on the remaining 20%; Evaluation metrics: Accuracy, precision, recall, and F1-score were used to evaluate the model's performance.\n\nSimulation Process.The simulation of EEG signal fluctuations was conducted as follows: The Data_preprocessed files from the DEAP dataset were fed into a linear filter to map the spectral output of the EEG signals, representing brain neural activity; A receptive field model of the linear filter was created to generate artificial neural responses; The data were downsampled along the time dimension, and the receptive field was used to create an artificial neural response by performing a dot product with the receptive field; The Bi-LSTM model was then used to simulate and predict the time-varying stimulus response of the EEG signals.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "To ensure a fair comparison between the different models, we maintained consistent experimental conditions across all tests. All models were trained and evaluated using the same datasets (SEED and DEAP) and preprocessing techniques. Additionally, we used the same training, validation, and testing splits to provide an equitable basis for performance evaluation. Hyperparameters for each model were tuned individually to achieve optimal performance, ensuring that each model was fairly optimized for comparison. We also employed standard evaluation metrics, such as accuracy, precision, recall, and F1-score, to provide a comprehensive assessment of each model's performance.\n\nThe results demonstrate the superior performance of the Bi-LSTM model with attention mechanisms, highlighting the effectiveness of our proposed approach in emotion recognition using EEG signals.\n\nResults on SEED dataset. We conducted a comparative experiment using EEG-based object detection visual models on the SEED dataset. The models we employed include Support Vector Machine (SVM)  [77] , Pyramid Match Kernel (PMK)  [78] , EEG-Net  [79] , LSTM  [80] , and Bi-LSTM. SVM is a supervised learning model used for classification and regression analysis, effectively handling high-dimensional data. PMK is a kernel function used for image matching by processing image features through a pyramid structure to achieve efficient matching. EEG-Net is a deep learning model specifically designed for processing and analyzing EEG data to improve the classification and recognition performance of EEG signals. LSTM is a type of RNN model that addresses long-term dependency issues through its gating mechanism, making it suitable for handling sequential data. Bi-LSTM consists of forward and backward LSTM layers, enabling the model to utilize both past and future information in the sequence, thus enhancing its understanding of temporal data.\n\nOur proposal and the performance comparison of these models are shown in Table  3 . In addition to these advanced model algorithms, we also compared our model with the incorporation of attention gates and attention weights to explore the effectiveness of Bi-LSTM. As shown in the table, the Bi-LSTM models with attention mechanisms significantly improved accuracy. Specifically, the Bi-LSTM-AttGW (1 layer and 128 hidden size) model achieved the highest accuracy of 98.28% in multiclass tasks. This demonstrates that incorporating attention mechanisms can significantly enhance the performance of models in emotion recognition using EEG signals. This method greatly improves the accuracy of capturing EEG signals.\n\nAs shown in Table  4 , we compared the accuracy and standard deviation of different models in various frequency bands of the SEED dataset. Our proposed model demonstrated superior performance across all frequency bands, particularly in the ùõæ band, where it achieved an accuracy of 98.28% with a standard deviation of 7.42. This highlights To visualize the EEG signal activity of the brain, we applied a low-pass filter at 8 ùêªùëß to the EEG signals from the SEED dataset to remove high-frequency noise. Figure  6  shows the EEG signal fragments in the regions of interest (ROIs) using the 130-electrode system layout. Each row in the figure represents the brain activity recorded for a segment from different movies, illustrating the variations in EEG signals in response to different emotional stimuli.\n\nThe first row represents the brain activity recorded for a segment from the movie Lost in Thailand, showing EEG signal snippets from 50 ms before to 130 ms after each beat mark. We performed baseline correction for each EEG signal band by subtracting the mean value calculated from the 50 ms segments between beat marks. The second row represents brain activity for a segment from the movie Back to 1942, recorded from 100 ms before to 180 ms after each beat mark, with baseline correction applied every 20 ms. The third row shows brain activity for a segment from the movie World Heritage in China, recorded from 55 ms before to 130 ms after each beat mark, with baseline correction applied every 25 ms. This detailed breakdown demonstrates the dynamic changes in brain activity elicited by different emotional stimuli across multiple time points.\n\nWhen recording EEG data, the signal initially shows a negative dip at 0 ms. Any auditory processing related to the music beats occurs shortly afterward. One possible explanation is that the sudden change in the music beat causes a sudden dip in the EEG signal. However, this requires further investigation. It could also be due to inaccuracies in the wearable device used to collect the signal or the sampling rate and tracker precision. For further comparison, Figure  7  shows the EEG signal changes at the imagined time points corresponding to different movie scenes in the SEED dataset. This figure corresponds to the second row in Figure  6 , showing the variations in the ROIs. The amplitude range of these EEG signal changes is relatively large, likely due to the emotional intensity provided by the movie segments. To calculate meaningful fluctuations, precise timing (e.g., sudden events) must be known. However, small changes during EEG signal recording are likely, causing some imprecision in the beat marks. To understand the event-related effects on EEG signals induced by auditory and visual stimuli, we used the DEAP dataset, which contains emotion-related EEG signals. We analyzed the five EEG frequency bands affected by emotional states (as shown in Table  2 ). We employed Event-Related Potentials (ERP) metrics to record these changes. ERP refers to the specific potentials induced by stimuli such as auditory and visual cues, excluding spontaneous EEG signals. With its high temporal resolution, ERP can reflect changes in brain neurophysiological activities during cognitive processes, making it a widely used tool for assessing brain cognitive processing. The features commonly analyzed in ERP data are the latency and peaks of its components.\n\nThe main components of ERP include P100, N100, P200, N200, and P300. Most studies focus on the P300 component of ERP data for brainwave analysis. The P300 component is a positive deflection occurring around 300 milliseconds post-stimulus. Table  5  presents the statistical results of P300 values from ERP data in the DEAP dataset. It is evident from the table that participants exhibit more intense emotional reactions to sad music scenes. We used a Bi-LSTM model to simulate and predict EEG signal fluctuations recorded in the DEAP dataset. The Data_preprocessed files were fed into a linear filter to map the spectral output of the EEG signals, representing brain neural activity. We attempted to recreate the receptive field model of the linear filter to generate this data. The data were downsampled along the time dimension, and this receptive field was used to create an artificial neural response. This involved performing a dot product with the receptive field. As shown in Figure  8",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This study records and analyzes brain activity while listening to music using wearable EEG devices and Bi-LSTM with attention mechanisms to identify emotional states. Data was collected using wearable EEG devices, preprocessed, and DE features were extracted. A Bi-LSTM model with attention mechanisms was then constructed, showing superior performance in emotion recognition, particularly with the SEED and DEAP datasets. The Bi-LSTM-AttGW model achieved the highest accuracy of 98.28% in multi-class tasks.\n\nThe novelty of this study lies in the integration of Bi-LSTM with attention mechanisms for EEG signal processing. This combination significantly improves the accuracy Despite these achievements, there are limitations. Wearable EEG devices have relatively low sampling rates and are prone to artifacts, affecting signal quality. Future research should focus on improving device design and signal processing algorithms. Additionally, larger sample sizes and more diverse experimental conditions are needed to account for individual differences in emotional responses to music. Further integration of multimodal data and various emotional elicitation methods could enhance the accuracy and applicability of emotion recognition in BCI and affective computing.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic Diagram of the Proposed Bi-LSTM Framework for Recording Brain Activity While Listening to Music. EEG Signals Are",
      "page": 4
    },
    {
      "caption": "Figure 1: , the overall experimental model",
      "page": 4
    },
    {
      "caption": "Figure 2: below is a schematic diagram of the 3D spatial",
      "page": 5
    },
    {
      "caption": "Figure 2: 3D location map of EEG channels and location channel",
      "page": 5
    },
    {
      "caption": "Figure 3: , the global channel pairs (ùëñ, ùëó) used",
      "page": 5
    },
    {
      "caption": "Figure 3: , unlike traditional unidirectional",
      "page": 5
    },
    {
      "caption": "Figure 3: Bidirectional Long Short-Term Memory Network Network",
      "page": 5
    },
    {
      "caption": "Figure 4: Single LSTM Cell Diagram. Left: Original LSTM Cell Diagram; Right: LSTM Cell Diagram with Attention Gate.",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the original architec-",
      "page": 6
    },
    {
      "caption": "Figure 4: , which incorporates the attention mechanism to",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates the layout",
      "page": 7
    },
    {
      "caption": "Figure 5: EEG electrode 130 system placement method. The blue",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the EEG signal fragments in the regions of interest",
      "page": 9
    },
    {
      "caption": "Figure 6: Dynamic Changes in EEG Signal Regions of Interest at",
      "page": 10
    },
    {
      "caption": "Figure 7: EEG Signal Variations at Time Points in the Movie Scene",
      "page": 10
    },
    {
      "caption": "Figure 8: , the left side depicts the simulated",
      "page": 10
    },
    {
      "caption": "Figure 8: Neural Response Signal Changes and Bi-LSTM Simulated Signal Variation Trends in EEG Data.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 2: SEED dataset composition. The rhythm of each band of EEG signal.",
      "data": [
        {
          "Number": "1\n2\n3\n4\n5\n6",
          "Name of the clip": "Lost in Thailand\nWorld Heritage in China\nAftershock\nBack to 1942\nFlirting Scholar\nJust Another Pandora‚Äôs Box",
          "Label": "Vigorous\nNeutral\nPassive\nPassive\nVigorous\nVigorous"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: SEED dataset composition. The rhythm of each band of EEG signal.",
      "data": [
        {
          "Band": "Delta (ùõø)",
          "Frequency (ùêªùëß)": "0.1-3.0",
          "Human State": "Deep sleep, disordered,\nhypoxic, comatose states"
        },
        {
          "Band": "Theta (ùúÉ)",
          "Frequency (ùêªùëß)": "4.0-7.0",
          "Human State": "Fatigue, depression,\nlow mood, disappointment"
        },
        {
          "Band": "Alpha (ùõº)",
          "Frequency (ùêªùëß)": "8.0-12.0",
          "Human State": "Relaxed, calm, eyes\nclosed but awake"
        },
        {
          "Band": "Beta (ùõΩ)",
          "Frequency (ùêªùëß)": "12.5-28.0",
          "Human State": "Tense, excited,happy"
        },
        {
          "Band": "Gamma (ùõæ)",
          "Frequency (ùêªùëß)": "29.0-50.0",
          "Human State": "Highly aroused,\nexcited, tense"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: StatisticsontheP300valueofERPdataontheDEAPdataset.",
      "data": [
        {
          "Emotion Change": "Happiness (mean¬±SD)\nSadness (mean¬±SD)\nFear (mean¬±SD)",
          "ERP Change Period\nPost-Stimulus (ms)": "298.45¬±33.25\n316.45¬±30.24\n327.13¬±31.69",
          "P": "0.026\n0.470\n0.020"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Scalp eeg classification using deep bi-lstm network for seizure detection",
      "authors": [
        "X Hu",
        "S Yuan",
        "F Xu",
        "Y Leng",
        "K Yuan",
        "Q Yuan"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "2",
      "title": "The mechanism of the impact of enterprise digital transformation on transaction performance",
      "authors": [
        "Y Wang",
        "S Sun",
        "Q Guo"
      ],
      "year": "2024",
      "venue": "Journal of Xi'an University of Finance and Economics"
    },
    {
      "citation_id": "3",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Ensemble machine learning-based affective computing for emotion recognition using dual-decomposed eeg signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "5",
      "title": "Influence of music liking on eeg based emotion recognition",
      "authors": [
        "D Naser",
        "G Saha"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "6",
      "title": "Neural decoding of music from the eeg",
      "authors": [
        "I Daly"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "7",
      "title": "Recognition of human emotions using eeg signals: A review",
      "authors": [
        "M Rahman",
        "A Sarkar",
        "M Hossain",
        "M Hossain",
        "M Islam",
        "M Hossain",
        "J Quinn",
        "M Moni"
      ],
      "year": "2021",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "8",
      "title": "An attention-based bi-lstm method for visual object classification via eeg",
      "authors": [
        "X Zheng",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "Ai-based nlp section discusses the application and effect of bag-of-words models and tf-idf in nlp tasks",
      "authors": [
        "S Dai",
        "K Li",
        "Z Luo",
        "P Zhao",
        "B Hong",
        "A Zhu",
        "J Liu"
      ],
      "year": "2024",
      "venue": "Journal of Artificial Intelligence General science"
    },
    {
      "citation_id": "10",
      "title": "A machine learning approach for accurate and real-time dna sequence identification",
      "authors": [
        "Y Wang",
        "M Alangari",
        "J Hihath",
        "A Das",
        "M Anantram"
      ],
      "year": "2021",
      "venue": "BMC genomics"
    },
    {
      "citation_id": "11",
      "title": "Reinforcement learning with communication latency with application to stop-andgo wave dissipation",
      "authors": [
        "A Richardson",
        "X Wang",
        "A Dubey",
        "J Sprinkle"
      ],
      "year": "2024",
      "venue": "2024 IEEE Intelligent Vehicles Symposium (IV)"
    },
    {
      "citation_id": "12",
      "title": "A dynamic filtering df-rnn deep-learning-based approach for eeg-based neurological disorders diagnosis",
      "authors": [
        "G Bouallegue",
        "R Djemal",
        "S Alshebeili",
        "H Aldhalaan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Towards robust lidar-camera fusion in bev space via mutual deformable attention and temporal aggregation",
      "authors": [
        "J Wang",
        "F Li",
        "Y An",
        "X Zhang",
        "H Sun"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "14",
      "title": "Dpmpc-planner: A realtime uav trajectory planning framework for complex static environments with dynamic obstacles",
      "authors": [
        "Z Xu",
        "D Deng",
        "Y Dong",
        "K Shimada"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "15",
      "title": "Task allocation planning based on hierarchical task network for national economic mobilization",
      "authors": [
        "P Zhao",
        "K Li",
        "B Hong",
        "A Zhu",
        "J Liu",
        "S Dai"
      ],
      "year": "2024",
      "venue": "Journal of Artificial Intelligence General science"
    },
    {
      "citation_id": "16",
      "title": "Parallel topologyaware mesh simplification on terrain trees",
      "authors": [
        "Y Song",
        "R Fellegara",
        "F Iuricich",
        "L Floriani"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Spatial Algorithms and Systems"
    },
    {
      "citation_id": "17",
      "title": "A motor imagery eeg signal optimized processing algorithm",
      "authors": [
        "X Geng",
        "X Zhang",
        "M Yue",
        "W Hu",
        "L Wang",
        "X Zhang",
        "P Yu",
        "D Long",
        "H Yan"
      ],
      "year": "2024",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "18",
      "title": "Cu-net: a u-net architecture for efficient brain-tumor segmentation on brats",
      "authors": [
        "Q Zhang",
        "W Qi",
        "H Zheng",
        "X Shen"
      ],
      "year": "2019",
      "venue": "Cu-net: a u-net architecture for efficient brain-tumor segmentation on brats",
      "arxiv": "arXiv:2406.13113"
    },
    {
      "citation_id": "19",
      "title": "Lingcn: Structural linearized graph convolutional network for homomorphically encrypted inference",
      "authors": [
        "H Peng",
        "R Ran",
        "Y Luo",
        "J Zhao",
        "S Huang",
        "K Thorat",
        "T Geng",
        "C Wang",
        "X Xu",
        "W Wen"
      ],
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Efficient topologyaware simplification of large triangulated terrains",
      "authors": [
        "Y Song",
        "R Fellegara",
        "F Iuricich",
        "L Floriani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th International Conference on Advances in Geographic Information Systems"
    },
    {
      "citation_id": "21",
      "title": "Classification of dna sequences: Performance evaluation of multiple machine learning methods",
      "authors": [
        "Y Wang",
        "V Khandelwal",
        "A Das",
        "M Anantram"
      ],
      "year": "2022",
      "venue": "2022 IEEE 22nd International Conference on Nanotechnology"
    },
    {
      "citation_id": "22",
      "title": "Real-time eeg-based cognitive workload monitoring on wearable devices",
      "authors": [
        "R Zanetti",
        "A Arza",
        "A Aminifar",
        "D Atienza"
      ],
      "year": "2021",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "23",
      "title": "Support vector machines based predictive seizure care using iot-wearable eeg devices for proactive intervention in epilepsy",
      "authors": [
        "P Srinivas",
        "M Arulprakash",
        "M Vadivel",
        "N Anusha",
        "G Rajasekar",
        "C Srinivasan"
      ],
      "year": "2024",
      "venue": "2024 2nd International Conference on Computer, Communication and Control"
    },
    {
      "citation_id": "24",
      "title": "Validation of emotiv epoc+ for extracting erp correlates of emotional face processing",
      "authors": [
        "K Kotowski",
        "K Stapor",
        "J Leski",
        "M Kotas"
      ],
      "year": "2018",
      "venue": "Biocybernetics and Biomedical Engineering"
    },
    {
      "citation_id": "25",
      "title": "The impact of the digital economy on regional economic development disparities from the perspective of spatial spillovers",
      "authors": [
        "Q Cheng",
        "Y Song"
      ],
      "year": "2023",
      "venue": "Journal of Xi'an University of Finance and Economics"
    },
    {
      "citation_id": "26",
      "title": "Wearable eeg electronics for a brain-ai closedloop system to enhance autonomous machine decision-making",
      "authors": [
        "J Shin",
        "J Kwon",
        "J Kim",
        "H Ryu",
        "J Ok",
        "S Joon Kwon",
        "H Park",
        "T.-I Kim"
      ],
      "year": "2022",
      "venue": "npj Flexible Electronics"
    },
    {
      "citation_id": "27",
      "title": "Effect of age on driving behavior and a neurophysiological interpretation",
      "authors": [
        "T Li",
        "R Zhao",
        "Y Liu",
        "X Liu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "28",
      "title": "Modeling and simulation of dna origami based electronic read-only memory",
      "authors": [
        "A De",
        "H Mohammad",
        "Y Wang",
        "R Kubendran",
        "A Das",
        "M Anantram"
      ],
      "year": "2022",
      "venue": "2022 IEEE 22nd International Conference on Nanotechnology"
    },
    {
      "citation_id": "29",
      "title": "The application of artificial intelligence technology in assembly techniques within the industrial sector",
      "authors": [
        "B Hong",
        "P Zhao",
        "J Liu",
        "A Zhu",
        "S Dai",
        "K Li"
      ],
      "year": "2024",
      "venue": "Journal of Artificial Intelligence General science"
    },
    {
      "citation_id": "30",
      "title": "Visual prompting upgrades neural network sparsification: A datamodel perspective",
      "authors": [
        "C Jin",
        "T Huang",
        "Y Zhang",
        "M Pechenizkiy",
        "S Liu",
        "S Liu",
        "T Chen"
      ],
      "year": "2023",
      "venue": "Visual prompting upgrades neural network sparsification: A datamodel perspective",
      "arxiv": "arXiv:2312.01397"
    },
    {
      "citation_id": "31",
      "title": "The design of autonomous uav prototypes for inspecting tunnel construction environment",
      "authors": [
        "Y Dong"
      ],
      "year": "2024",
      "venue": "The design of autonomous uav prototypes for inspecting tunnel construction environment",
      "arxiv": "arXiv:2408.07286"
    },
    {
      "citation_id": "32",
      "title": "A preliminary evaluation of driver's workload in partially automated vehicles",
      "authors": [
        "R Zhao",
        "Y Liu",
        "T Li",
        "Y Li"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "33",
      "title": "Learning from teaching regularization: Generalizable correlations should be easy to imitate",
      "authors": [
        "C Jin",
        "T Che",
        "H Peng",
        "Y Li",
        "M Pavone"
      ],
      "year": "2024",
      "venue": "Learning from teaching regularization: Generalizable correlations should be easy to imitate",
      "arxiv": "arXiv:2402.02769"
    },
    {
      "citation_id": "34",
      "title": "Dualvd: An adaptive dual encoding model for deep visual understanding in visual dialogue",
      "authors": [
        "X Jiang",
        "J Yu",
        "Z Qin",
        "Y Zhuang",
        "X Zhang",
        "Y Hu",
        "Q Wu"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "35",
      "title": "Evaluate the effect of age and driving experience on driving performance with automated vehicles",
      "authors": [
        "T Li",
        "R Zhao",
        "Y Liu",
        "Y Li",
        "G Li"
      ],
      "year": "2021",
      "venue": "International Conference on Applied Human Factors and Ergonomics"
    },
    {
      "citation_id": "36",
      "title": "Accel-gcn: High-performance gpu accelerator design for graph convolution networks",
      "authors": [
        "X Xie",
        "H Peng",
        "A Hasan",
        "S Huang",
        "J Zhao",
        "H Fang",
        "W Zhang",
        "T Geng",
        "O Khan",
        "C Ding"
      ],
      "year": "2023",
      "venue": "IEEE/ACM International Conference on Computer Aided Design"
    },
    {
      "citation_id": "37",
      "title": "Enhancing visual question answering through ranking-based hybrid training and multimodal fusion",
      "authors": [
        "P Chen",
        "Z Zhang",
        "Y Dong",
        "L Zhou",
        "H Wang"
      ],
      "year": "2024",
      "venue": "Enhancing visual question answering through ranking-based hybrid training and multimodal fusion",
      "arxiv": "arXiv:2408.07303"
    },
    {
      "citation_id": "38",
      "title": "Utilizing deep learning to optimize software development processes",
      "authors": [
        "K Li",
        "A Zhu",
        "W Zhou",
        "P Zhao",
        "J Song",
        "J Liu"
      ],
      "year": "2024",
      "venue": "Utilizing deep learning to optimize software development processes",
      "arxiv": "arXiv:2404.13630"
    },
    {
      "citation_id": "39",
      "title": "Association of clinical, biological, and brain magnetic resonance imaging findings with electroencephalographic findings for patients with covid-19",
      "authors": [
        "V Lambrecq",
        "A Hanin",
        "E Munoz-Musat",
        "L Chougar",
        "S Gassama",
        "C Delorme",
        "L Cousyn",
        "A Borden",
        "M Damiano",
        "V Frazzini"
      ],
      "year": "2021",
      "venue": "JAMA Network Open"
    },
    {
      "citation_id": "40",
      "title": "Functional connectivity of eeg is subject-specific, associated with phenotype, and different from fmri",
      "authors": [
        "M Nentwich",
        "L Ai",
        "J Madsen",
        "Q Telesford",
        "S Haufe",
        "M Milham",
        "L Parra"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "41",
      "title": "Infra-slow fluctuations in electrophysiological recordings, blood-oxygenation-level-dependent signals, and psychophysical time series",
      "authors": [
        "J Palva",
        "S Palva"
      ],
      "year": "2012",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "42",
      "title": "Diffusion tensor imaging indicators of white matter injury are correlated with a multimodal electroencephalography-based biomarker in slow recovering, concussed collegiate athletes",
      "authors": [
        "E Wilde",
        "N Goodrich-Hunsaker",
        "A Ware",
        "B Taylor",
        "B Biekman",
        "J Hunter",
        "R Newman-Norlund",
        "S Scarneo",
        "D Casa",
        "H Levin"
      ],
      "year": "2020",
      "venue": "Journal of neurotrauma"
    },
    {
      "citation_id": "43",
      "title": "Deep learning-based bci for gait decoding from eeg with lstm recurrent neural network",
      "authors": [
        "S Tortora",
        "S Ghidoni",
        "C Chisari",
        "S Micera",
        "F Artoni"
      ],
      "year": "2020",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "44",
      "title": "Regional networks underlying interhemispheric connectivity: an eeg and dti study in healthy ageing and amnestic mild cognitive impairment",
      "authors": [
        "S Teipel",
        "O Pogarell",
        "T Meindl",
        "O Dietrich",
        "D Sydykova",
        "U Hunklinger",
        "B Georgii",
        "C Mulert",
        "M Reiser",
        "H.-J M√∂ller"
      ],
      "year": "2009",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "45",
      "title": "Eeg-based subject-independent emotion recognition using gated recurrent unit and minimum class confusion",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "J Liu",
        "X Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Learning adversarial semantic embeddings for zero-shot recognition in open worlds",
      "authors": [
        "T Li",
        "G Pang",
        "X Bai",
        "J Zheng",
        "L Zhou",
        "X Ning"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning",
      "authors": [
        "H Ran",
        "W Li",
        "L Li",
        "S Tian",
        "X Ning",
        "P Tiwari"
      ],
      "year": "2024",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "48",
      "title": "An efficient cnn based epileptic seizures detection framework using encrypted eeg signals for secure telemedicine applications",
      "authors": [
        "A Ein Shoka",
        "M Dessouky",
        "A El-Sayed",
        "E El-Din Hemdan"
      ],
      "year": "2023",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "49",
      "title": "Cross-task multibranch vision transformer for facial expression and mask wearing classification",
      "authors": [
        "A Zhu",
        "K Li",
        "T Wu",
        "P Zhao",
        "W Zhou",
        "B Hong"
      ],
      "year": "2024",
      "venue": "Cross-task multibranch vision transformer for facial expression and mask wearing classification",
      "arxiv": "arXiv:2404.14606"
    },
    {
      "citation_id": "50",
      "title": "The application of augmented reality (ar) in remote work and education",
      "authors": [
        "K Li",
        "P Xirui",
        "J Song",
        "B Hong",
        "J Wang"
      ],
      "year": "2024",
      "venue": "The application of augmented reality (ar) in remote work and education",
      "arxiv": "arXiv:2404.10579"
    },
    {
      "citation_id": "51",
      "title": "Runtime monitoring of accidents in driving recordings with multi-type logic in empirical models",
      "authors": [
        "Z An",
        "X Wang",
        "T Johnson",
        "J Sprinkle",
        "M Ma"
      ],
      "year": "2023",
      "venue": "International Conference on Runtime Verification"
    },
    {
      "citation_id": "52",
      "title": "An investigation of the impact of autonomous driving on driving behavior in traffic jam",
      "authors": [
        "Y Liu",
        "R Zhao",
        "T Li",
        "Y Li"
      ],
      "year": "2021",
      "venue": "IIE Annual Conference. Proceedings, Institute of Industrial and Systems Engineers (IISE)"
    },
    {
      "citation_id": "53",
      "title": "Recognizing emotions evoked by music using cnn-lstm networks on eeg signals",
      "authors": [
        "S Sheykhivand",
        "Z Mousavi",
        "T Rezaii",
        "A Farzamnia"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "54",
      "title": "Using automated vehicle data as a fitness tracker for sustainability",
      "authors": [
        "X Wang",
        "S Onwumelu",
        "J Sprinkle"
      ],
      "year": "2024",
      "venue": "2024 Forum for Innovative Sustainable Transportation Systems (FISTS)"
    },
    {
      "citation_id": "55",
      "title": "Terrain trees: a framework for representing, analyzing and visualizing triangulated terrains",
      "authors": [
        "R Fellegara",
        "F Iuricich",
        "Y Song",
        "L Floriani"
      ],
      "year": "2023",
      "venue": "GeoInformatica"
    },
    {
      "citation_id": "56",
      "title": "Autorep: Automatic relu replacement for fast private network inference",
      "authors": [
        "H Peng",
        "S Huang",
        "T Zhou",
        "Y Luo",
        "C Wang",
        "Z Wang",
        "J Zhao",
        "X Xie",
        "A Li",
        "T Geng"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "57",
      "title": "Music genre classification with transformer classifier",
      "authors": [
        "Y Zhuang",
        "Y Chen",
        "J Zheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 4th international conference on digital signal processing"
    },
    {
      "citation_id": "58",
      "title": "An investigation of resilience in manual driving and automatic driving in freight transportation system",
      "authors": [
        "R Zhao",
        "Y Liu",
        "Y Li",
        "B Tokgoz"
      ],
      "year": "2021",
      "venue": "IIE Annual Conference. Proceedings, Institute of Industrial and Systems Engineers (IISE)"
    },
    {
      "citation_id": "59",
      "title": "Performance analysis of dna crossbar arrays for highdensity memory storage applications",
      "authors": [
        "A De",
        "H Mohammad",
        "Y Wang",
        "R Kubendran",
        "A Das",
        "M Anantram"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "60",
      "title": "A preliminary comparison of drivers' overtaking behavior between partially automated vehicles and conventional vehicles",
      "authors": [
        "Y Liu",
        "R Zhao",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting"
    },
    {
      "citation_id": "61",
      "title": "Eeg-based diagnostics of the auditory system using cochlear implant electrodes as sensors",
      "authors": [
        "B Somers",
        "C Long",
        "T Francart"
      ],
      "year": "2021",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "62",
      "title": "Entrainment and synchronization of brain oscillations to auditory stimulations",
      "authors": [
        "D Henao",
        "M Navarrete",
        "M Valderrama",
        "M Van Quyen"
      ],
      "year": "2020",
      "venue": "Neuroscience Research"
    },
    {
      "citation_id": "63",
      "title": "Benchmarking real-time algorithms for in-phase auditory stimulation of low amplitude slow waves with wearable eeg devices during sleep",
      "authors": [
        "M Ferster",
        "G Da Poian",
        "K Menachery",
        "S Schreiner",
        "C Lustenberger",
        "A Maric",
        "R Huber",
        "C Baumann",
        "W Karlen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "64",
      "title": "An improved feature extraction algorithms of eeg signals based on motor imagery braincomputer interface",
      "authors": [
        "X Geng",
        "D Li",
        "H Chen",
        "P Yu",
        "H Yan",
        "M Yue"
      ],
      "year": "2022",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "65",
      "title": "Solving the foodenergy-water nexus problem via intelligent optimization algorithms",
      "authors": [
        "Q Deng",
        "Z Fan",
        "Z Li",
        "X Pan",
        "Q Kang",
        "M Zhou"
      ],
      "year": "2024",
      "venue": "Solving the foodenergy-water nexus problem via intelligent optimization algorithms",
      "arxiv": "arXiv:2404.06769"
    },
    {
      "citation_id": "66",
      "title": "Inspection of in-vehicle touchscreen infotainment display for different screen locations, menu types, and positions",
      "authors": [
        "S Patel",
        "Y Liu",
        "R Zhao",
        "X Liu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "International conference on human-computer interaction"
    },
    {
      "citation_id": "67",
      "title": "Traffic smoothing via connected & automated vehicles: A modular, hierarchical control design deployed in a 100-cav flow smoothing experiment",
      "authors": [
        "J Lee",
        "H Wang",
        "K Jang",
        "A Hayat",
        "M Bunting",
        "A Alanqary",
        "W Barbour",
        "Z Fu",
        "X Gong",
        "G Gunter"
      ],
      "year": "2024",
      "venue": "IEEE Control Systems Magazine"
    },
    {
      "citation_id": "68",
      "title": "Image and spectrum based deep feature analysis for particle matter estimation with weather informatio",
      "authors": [
        "Z Zhu",
        "R Zhao",
        "J Ni",
        "J Zhang"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "69",
      "title": "The impact of directional road signs combinations and language unfamiliarity on driving behavior",
      "authors": [
        "Y Liu",
        "R Zhao",
        "T Li",
        "Y Li"
      ],
      "year": "2022",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "70",
      "title": "Automatic epileptic eeg classification based on differential entropy and attention model",
      "authors": [
        "J Zhang",
        "Z Wei",
        "J Zou",
        "H Fu"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "71",
      "title": "Facilitating dnn model adaptivity for efficient private inference in edge computing",
      "authors": [
        "T Zhou",
        "J Zhao",
        "Y Luo",
        "X Xie",
        "W Wen",
        "C Ding",
        "X Xu"
      ],
      "year": "2024",
      "venue": "Facilitating dnn model adaptivity for efficient private inference in edge computing",
      "arxiv": "arXiv:2407.05633"
    },
    {
      "citation_id": "72",
      "title": "Maxk-gnn: Extremely fast gpu kernel design for accelerating graph neural networks training",
      "authors": [
        "H Peng",
        "X Xie",
        "K Shivdikar",
        "M Hasan",
        "J Zhao",
        "S Huang",
        "O Khan",
        "D Kaeli",
        "C Ding"
      ],
      "year": "2024",
      "venue": "Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems"
    },
    {
      "citation_id": "73",
      "title": "Apeer: Automatic prompt engineering enhances large language model reranking",
      "authors": [
        "C Jin",
        "H Peng",
        "S Zhao",
        "Z Wang",
        "W Xu",
        "L Han",
        "J Zhao",
        "K Zhong",
        "S Rajasekaran",
        "D Metaxas"
      ],
      "year": "2024",
      "venue": "Apeer: Automatic prompt engineering enhances large language model reranking",
      "arxiv": "arXiv:2406.14449"
    },
    {
      "citation_id": "74",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "75",
      "title": "Multi-domain feature fusion for emotion classification using deap dataset",
      "authors": [
        "M Khateeb",
        "S Anwar",
        "M Alnowami"
      ],
      "year": "2021",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "76",
      "title": "Classification of eeg using adaptive svm classifier with csp and online recursive independent component analysis",
      "authors": [
        "M Antony",
        "B Sankaralingam",
        "R Mahendran",
        "A Gardezi",
        "M Shafiq",
        "J.-G Choi",
        "H Hamam"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "77",
      "title": "The identification of children with autism spectrum disorder by svm approach on eeg and eyetracking data",
      "authors": [
        "J Kang",
        "X Han",
        "J Song",
        "Z Niu",
        "X Li"
      ],
      "year": "2020",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "78",
      "title": "Eeg-based emotion recognition for hearing impaired and normal individuals with residual feature pyramids network based on time-frequency-spatial features",
      "authors": [
        "F Hou",
        "J Liu",
        "Z Bai",
        "Z Yang",
        "J Liu",
        "Q Gao",
        "Y Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "79",
      "title": "Virtual reality safety training using deep eeg-net and physiology data",
      "authors": [
        "D Huang",
        "X Wang",
        "J Liu",
        "J Li",
        "W Tang"
      ],
      "year": "2022",
      "venue": "The visual computer"
    },
    {
      "citation_id": "80",
      "title": "New lstm deep learning algorithm for driving behavior classification",
      "authors": [
        "N Kadri",
        "A Ellouze",
        "M Ksantini",
        "S Turki"
      ],
      "year": "2023",
      "venue": "Cybernetics and Systems"
    },
    {
      "citation_id": "81",
      "title": "Computational study of the role of counterions and solvent dielectric in determining the conductance of b-dna",
      "authors": [
        "Y Wang",
        "B Demir",
        "H Mohammad",
        "E Oren",
        "M Anantram"
      ],
      "year": "2023",
      "venue": "Physical Review E"
    },
    {
      "citation_id": "82",
      "title": "Aq2pnn: Enabling two-party privacy-preserving deep neural network inference with adaptive quantization",
      "authors": [
        "Y Luo",
        "N Xu",
        "H Peng",
        "C Wang",
        "S Duan",
        "K Mahmood",
        "W Wen",
        "C Ding",
        "X Xu"
      ],
      "year": "2023",
      "venue": "2023 56th IEEE/ACM International Symposium on Microarchitecture"
    }
  ]
}