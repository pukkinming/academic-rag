{
  "paper_id": "2509.18175v1",
  "title": "Erfc: Happy Customers With Emotion Recognition And Forecasting In Conversation In Call Centers",
  "published": "2025-09-17T16:15:49Z",
  "authors": [
    "Aditi Debsharma",
    "Bhushan Jagyasi",
    "Surajit Sen",
    "Priyanka Pandey",
    "Devicharith Dovari",
    "Yuvaraj V. C",
    "Rosalin Parida",
    "Gopali Contractor"
  ],
  "keywords": [
    "Emotion Recognition",
    "Call-center Analytics",
    "Customer Satisfaction",
    "Speaker inter-dependency"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) has been seen to be widely applicable in call-center analytics, opinion mining, finance, retail, healthcare, and other industries. In a call-center scenario, the role of the call center agent is not just confined to receiving calls but to also provide good customer experience by pacifying the frustration or anger of the customers. This can be achieved by maintaining neutral and positive emotion from the agent's end. As in any conversation, the emotion of one speaker is usually dependent on the other speaker's emotion hence the agent's positive emotion accompanied with the right resolution will help in enhancing customer experience. This can change an unhappy customer to a happy one. Imparting the right resolution at right time becomes easier if the agent has the insight of the emotion of future utterances. To predict the emotions of the future utterances we propose a novel architecture Emotion Recognition and Forecasting in Conversation (ERFC). Our proposed ERFC architecture considers multi-modalities, different attributes of emotion, context and the inter-dependencies of the utterances of the speakers in the conversation. Our intensive experiments on the IEMOCAP dataset have shown the feasibility of the proposed ERFC. This approach can provide a tremendous business value for the applications like call center, where the happiness of customer is utmost important.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In any conversation, emotions expressed are highly inter-dependent on the emotions of the other speakers involved. A change in the emotion of one speaker might lead to the change in the emotion of the other speaker over the subsequent utterances. For example, mostly, a consistent positive emotion of one speaker can bring down the intensity of the negative emotion of the other speaker in a conversation and vice versa. Similarly, in any call center there are different emotions observed during conversation with the customer. However, the agent have to maintain a positive emotion irrespective of the emotion of the customer to enhance the customer experience. Typically, an agent should aim to convert an unhappy customer towards a happy customer with the right resolutions along with exhibiting the positive emotions. So, if an agent has an information about the future emotions of the customer, which is going towards the negative direction, it becomes easier for the agent to take corrective actions. In such a scenario, the agent can amplify their positive emotions to empathize towards an unhappy customer, in order to resolve the concerns in a right way.\n\nWith the increasing demand of the application of emotion recognition, there have been significant developments in this area from emotion recognition for an utterance (without context) to Emotion Recognition in Conversation (ERC) where context from previous utterances are also taken into account.\n\nIn literature, for Emotion Recognition in Conversation (ERC), there have been multiple studies to enhance the state-of-the-art accuracy. Most of the studies in ERC have been accomplished by considering context and multimodal features  [5, 10-12, 15, 17]  which is the main difference between ERC and Emotion recognition in utterance. These studies mainly focus on designing the right architecture to achieve higher accuracy. In our paper Varta Rasa  [13] , we have considered context to predict the emotion of the present utterance, however, with a light weight architecture to achieve a comparable accuracy.\n\nSelf dependencies of speaker for a context has been explored in  [6, 7, 14] . In  [9] , the interdependencies between two consecutive turns of utterance between the two speakers have been modeled as Dynamic Bayesian Network Structure for emotion Recognition. When two speakers in a conversation complete their utterance, that defines as one turn.\n\nLately, there have been some work which highlights the importance of emotion forecasting. In  [16] , audio-visual cues are used to forecast the emotion of the next two turns for a single speaker. In the similar lines, there have been some other works which identified the importance of emotion forecasting in different use cases. The authors of  [1, 8]  highlights on the importance of emotion forecasting in human-machine interactions where pre-recognition of user's emotion can result in quality and successful conversation.\n\nIn this paper, we identify a gap in solutions available in literature for call center conversation application where agents do not get enough insights from the emotions of the current utterance. We hence see this scenario in a new light, by focusing on the emotion recognition for the current utterance along with emotion forecasting for the future utterances of the next few turns of both the speakers involved in the conversation. We propose Emotion Recognition and Forecasting in Conversation (ERFC) which fills the gap in the literature and emphasises the utility of this solution for business problems like customer retention in a call center setup. On the proposed ERFC, from a given conversation, apart from emotion recognition for the current utterance, the emotions for the future utterances over a few turns of conversation are forecasted.\n\nIn ERFC, we have considered context, inter-dependency of emotion between speakers in conversation and turns. In addition to this we have also considered the attributal features of emotion which are -Activation, Valence, and Dominance (AVD)  [3] . Activation indicates speaker's excitement level which can vary between excited to calm, Valence is the tone of the speech and it can range between positive and negative, and Dominance is the strength of the voice which varies from strong to weak. The primary reason for considering attributal features along with emotions is that the latter cannot provide information on the intensity of the emotion.\n\nFor the call center application, we aimed to forecast the next few turns of both the speakers. This helps the call center agent to get the insights of the future trend of the conversation.\n\nOverall, the key contributions in this paper are:\n\n‚Ä¢ We propose the Emotion Recognition and Forecasting in Conversation (ERFC) which is designed to be used for the applications such as call-center conversations for current utterance emotion prediction along with emotion forecasts of future utterances, for both the speakers. ‚Ä¢ In the proposed ERFC, we utilize (a) multi-modal features (audio and text) (b) all three dimensional (attribute) featuresvalance, activation and dominance (AVD), of emotions to complement emotion and (c) mutual influence of the speakers in conversation by including speaker turns along with speaker information. ‚Ä¢ We propose a light weight multi-level stacking architecture which is based on our earlier work Varta Rasa  [13]  which was also a light weight architecture, albeit for ERC . ‚Ä¢ We have also done an exhaustive experimental analysis using Interactive Emotional Dyadic Motion Capture (IEMO-CAP) corpus  [3]  to present the results of our proposed ERFC solution.\n\nThe remaining part of the paper is structured as follows: In the next section, the details of the proposed ERFC architecture has been presented; in section Experiments and Results, the Experimental setting and Results of the implementation of ERFC using the IEMOCAP dataset have been presented; in section Business Impact, the value realization of the proposed ERFC for the customer care support application and the other potential applications have been discussed; we finally conclude our work in the section Conclusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Emotion Recognition And Forecasting In Conversation (Erfc) Architecture",
      "text": "In a conversation of two speakers, the emotion of one speaker plays an important role in influencing the emotion of the other speaker. Let's understand this with an example from the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus  [3] . Fig.  2  depicts emotion and valence expressed in a conversation between a customer and a customer care agent where the customer is seen to be excited in her initial utterances due to reaching out to a human agent which she has been trying for long on the IVR (Interactive Voice Response) call. Immediately after that, during utterances 7-11, the customer is seen to be frustrated due to the fact that she has been constantly re-directed to IVR which could not resolve her issue. The customer care agent understands the customer's frustration and tries to give her the solution to avoid IVR in future with his consistent neutral emotion and positivity in his tone indicated by the valance. This provided the customer temporary satisfaction indicated by neutral emotion during the utterance 13-16. However, the customer's primary concern and purpose of contacting the customer care agent is for some incorrect transaction in her card. While expressing and registering this complaint the customer is very anxious and her emotion again goes down towards the negative direction along with her valence during 17-26 utterance with few fluctuations. The agent understands the customer's concern and endeavours to provide the best experience by giving proper solution with his consistent neutral emotion and positivity in the tone. The customer was bit skeptical with the initial solution, however the agent's positive approach and tone reflected through his emotion and valence helped the customer gain some confidence and finally the conversation ends with a happy note reflected through In this paper we propose an Emotion Recognition and Forecasting in Conversation (ERFC) solution which leverages the interdependencies between the speakers to not only predict the emotion for the current utterance but also forecast the emotion for the future utterances. Fig.  3  presents the architecture diagram of the ERFC solution which is based on inter-dependencies between the speakers, context window of past utterances and multi-Model features. The significance of these are explained as below:\n\n‚Ä¢ To capture the mutual influence or inter-dependencies of emotion of one speaker over the emotion of the other, we consider the detection of turn (as in  [9] ) by utilizing speaker identification through diarization in the conversation. ‚Ä¢ To predict the current emotion and forecast the future emotions more accurately, it is important to understand the trend of the emotions from the past utterances; we hence consider the context window as well. ‚Ä¢ Multi model features relevant in a call center scenario are considered like audio, text, speaker information and emotion along with their attributal features of the past utterances. We have not considered video as this is not relevant in a call center scenario.\n\nFig.  3  shows that audio goes as input which then is passed through speaker diarization to extract the speaker information in terms of number of speakers involved in the conversation and their corresponding time stamps with respect to each utterance of the speaker. For example, in a given conversation where [ùë¢ 1 , ùë¢ 2 , ùë¢ 3 , ...., ùë¢ ùëÅ ] are the utterances, speaker diarization identifies the speaker with their corresponding utterance along with their timestamp. The speaker information is further leveraged to label turns in the conversation. A single turns is when both the speakers in a two speaker conversation complete their utterances.\n\nIn our proposed ERFC, we have used multiple feature modalities and computed their respective embeddings to be used as input to the model. For text embeddings, we use pre-trained BERT language model from hugging face transformer library  [13, 18] ; for audio embeddings, we use openSMILE  [4, 13] ; for speaker embeddings, we use Pyannote  [2]   and ùë¢ ùê∏,ùëÜ ùëñ ùë° represent text embeddings, audio embeddings, speaker embeddings, and emotion label for speaker ùëÜ ùëñ 's utterance in turn ùë°. For text, audio and speaker, we consider context window of turn ùë° to ùë° -ùë§ ùë° , turn ùë° to ùë° -ùë§ ùëé and turn ùë° to ùë° -ùë§ ùë† , respectively for both the speakers. Note that, here the utterance for the current turn ùë° is also used as a input. However for emotion, we consider the context window of turn ùë° -1 to ùë° -ùë§ ùëí . Here the emotions for the current turn ùë° is not used as a input as we are predicting for the current utterance emotion.\n\nAs we are combining the utterances in turns to capture the inter-dependencies, the number of training examples will reduce significantly. We hence restore to light weight machine learning architecture based on  [13] , where all these multi-modal embeddings, thus created, are concatenated together and passed to the ensemble learning model. The concatenated vectors which is the output of the Ensemble model, is again concatenated with the embeddings of the text, audio, emotion and speaker to pass it through another ensemble model to predict the emotions of both the speakers for turn ùë° and forecast the emotions till turn ùë° + ùëò. As disclosed in the paper  [13] , the light weight architecture will not only make the training feasible for the small number of training examples, but also will help in reducing the time complexity and memory complexity as compared to the deep learning based models like bc-LSTM. The implementation and the exhaustive experimentation of the proposed ERFC architecture are presented in the following subsections. Through these results we demonstrate the feasibility of the ERFC for the real-world application.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setting",
      "text": "To train and evaluate the proposed ERFC architecture, we used the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [3] . The data collection comprises approximately 12 hours of audio, transcriptions, video, and motion-capture recordings from five sessions. We have targeted 6 emotions (Happy, Excited, Sad, Neutral, Angry and Frustrated) for prediction and forecasting. As in most of the literature using IEMOCAP dataset  [7, 13] , we also used the first four sessions of transcripts from the IEMOCAP dataset as the training set and the last session as the test set. From the available modalities, we use only audio and text from the IEMOCAP dataset for our experiments. This is because we are focusing on call center analytics application, where the voice calls are recorded and transcribed using automatic speech recognition (ASR) model to make the real time transcripts available as text.\n\nHere, for production deployment a turn detection using speaker diarization, where each turn has a part of the speech that belongs to both the speakers. However, in case of IEMOCAP, the transcript as well as speaker information for each utterances are already available and the same has been used in these experiments. For a single speaker in a turn, we may have several distinct utterances. Utterances of the individual speaker in each turn are concatenated together, and then embeddings are computed on the concatenated utterances. The embeddings and emotion labels were created at each speaker-turn combination. Following are the details on the features creation using embeddings and labels for the multi-modal data:\n\n‚Ä¢ Text Embeddings: Pre-trained BERT models from the Huggingface transformers library has been used to extract text embeddings of size 768. ‚Ä¢ Audio Embeddings: We have extracted the audio features from voice data at a 30 Hz frame rate and a sliding window of 100 ms. The openSMILE has been used to extract 6373 audio features which was further reduced to 250 using Principle Component Analysis (PCA).\n\n‚Ä¢ Speaker Embeddings: Using the audio data, we have created Speaker embeddings of the size 512 by using Pyannote which was further reduced to 256 using PCA. ‚Ä¢ Emotions Label: Label encoding has been used for assigning the emotion labels.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "As our focus is on ERFC, our experimental setting is drastically different than the literature  [5-7, 11, 13, 14]  which uses the same dataset IEMOCAP, as used in ERC; hence we would not be comparing the results one-on-one with the past literature.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feasibility Of Emotion Recognition And Forecasting In Conversation (Erfc).",
      "text": "For the first time, we showcase emotion forecasting along with emotion recognition in conversation which would be very useful for call center conversation or any other similar applications leveraging emotions from conversations. In Experiments E1 to E4, we show results for emotion recognition and emotion forecasting for 6 emotion classes (Happy, Sad, Neutral, Angry, Excited, and Frustrated), when AVD data is used as input, with varying context window size from no context (ùë§ = 0) to context window (ùë§ = 3). Here a context window is measured in turns (and not in utterance) where each turn will have atleast one utterance from both the speakers in the two-speaker conversation. To reiterate, this will also provide features from both the speakers to utilize the inter-dependencies between the two speakers. Table  2  presents average test accuracies for Experiments E1 to E4 for current turn emotion recognition as well as future turn emotion forecasting. We also present the validation accuracy which reflect that models were trained appropriately. As the lags are increased from 0 to 3, we observe that the prediction of emotion for current turn becomes more and more accurate. However the same trend is not observed for emotion forecasting for the future turns. For the forecasting of emotions, the presence of context upto ùëúùëõùëí -ùë°ùë¢ùëüùëõ was seen to result in better accuracy in comparison to no-context. However, with further increase in context window, there is no further increase in the accuracy. In Fig.  5 , we expand the future turn forecast and show the results with respect to increase in forecasting horizon. As expected, we notice that as we go ahead in forecasting horizon the accuracy is seen to decrease. Also this shows clear picture of the significance of the context window over the prediction accuracies for current and future turns. While the larger context window ùë§ = 3 is found to be useful for the emotion prediction of the current turn (ùë°), it is loosing its relevance for the future turn forecasting ùë° + 3. This is because of the drastic reduction in the training data set for forecasting for the turns much ahead in the horizon as compared to the prediction    3 .\n\nHere we compare accuracies for experiments E4 (6 Emotions, ùë§=3 and with AVD) with E5 (6 Emotions, ùë§=3 and without AVD) and notice the drop in accuracy for E5 as compared to E4 of around 3% for the prediction of current emotions and around 2% for the forecast of the future emotions. We hence recommend the use of attributal features if available.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Impact Of Number Of Emotions.",
      "text": "In experiment E4, ùë§ = 3 with AVD has been used which shows best result for prediction of current turn emotions. The confusion matrix for E4 is shown in Fig.  6  which shows the prediction classes distribution. This represents 3888 predictions which are results of predicting for both the speakers for 488 test examples for 4 turns (current turn and 3 turns forecasting horizon). The result shows that the model is experiencing confusion between the emotions happy and excited, as well as angry and frustrated. This is also intuitive as these emotions are similar in nature. Hence, we further experimented (E6) by merging the happy and excited categories and naming them happy, and also merging the angry and frustrated categories and naming them as angry and trained a model for 4 emotion recognition and forecasting from the conversations. From Table  4 , it is evident that the overall accuracy for predicting current and future emotions has significantly increased in the case of 4 emotions. In a call center conversation, one of the pressing problems with customer care agents is that how might we pacify the frustration of a customer in a very short time. This requires knowledge of not only what is the real issue but also how frustrated a customer is and if the customer is going to get more angry in the future utterances. To answer this, we plan to divide the conversation in different stages such as -Greetings, Problem Discovery, Problem Resolution, Assurance of the Solution and Concluding the call on a positive note. Here in each stage, a challenge in front of agent is to make the customer feel that they are valued. While the conversation is moving from one stage to the other stage there is a very short time to act. By using the proposed ERFC solution, the forecasted emotions of the customer for the next few turns will come handy in generating useful insights and recommendations for the customer care agent to modulate their emotions in such a way so as to result in an overall positive experience for the customer. This will also lead to an increase in the customer happiness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Other Potential Applications",
      "text": "Towards this end, we also envisage a completely new application of health consultation for a patient who has hearing and speech impairments. The question in front of any hearing and speech impaired individuals when they would like to consult a physician is how might we express our concerns during the diagnosis. The proposed EFRC solution can be applied to this problem as well. Here the patient communicates in the sign language, a computer vision based sign language recognition models can be trained to process the sign language to generate the transcripts as well and the emotions.\n\nTranscripts can be generated using the sign language, however their emotions can be captured using their facial expressions. The transcript and emotions can be displayed to the physician as text.\n\nA physician can comprehend the transcripts and the emotions and converse back in speech. An automatic speech recognition (ASR) model can be used to generate the transcript which can be displayed to the patient or using Generative AI, sign language can as well be generated. Now ERFC can be used for emotion recognition and forecasting in conversation. Here, the physician will not only have an access to the current emotion of the patient but also will have emotion forecasts available for few more turns in advance. This will enable the physician to take the required action to pacify the emotion of the patient and patient will in turn feel confidant with the right words used by the physician. Similarly this can be applied to various other applications like Health care -patient-nurse emotional state counselling, Health Insurance claim; human machine interaction -Emotion aware human robot interactions to help machines respond in a right emotional tone or empathy; and Human Resource Management -recruitment, retention, exit, & difficult conversations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper we have shed a new light in the area of Call-Center Application, wherein we propose Emotion Recognition and Forecasting in Conversation (ERFC) which not only helps in recognizing the emotions for the current utterance but also provides emotion forecasting for the future utterances of the speakers in the conversation. The intensive experimental results from the implementation of our novel ERFC solution using the IEMOCAP dataset shows the feasibility of the solution. The understanding of emotions of future utterance can bring in more empathy and lead to take corrective measures to pacify the negative emotions of the other speaker. The proposed ERFC solution can also be applied to a vast variety of applications where insights of the future emotion of the speakers plays a crucial role.\n\nDisclaimer: This content is provided for general information purposes and is not intended to be used in place of consultation with our professional advisors. The ERFC is the property of Accenture and its affiliates and Accenture be the holder of the copyright or any intellectual property over it. No part of this paper may be reproduced in any manner without the written permission of Accenture. Opinions expressed herein are subject to change without notice.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ERFC in Agent‚Äôs console in Call-center",
      "page": 1
    },
    {
      "caption": "Figure 2: depicts emotion and valence expressed in a conversation between a",
      "page": 2
    },
    {
      "caption": "Figure 2: Mutual interdependence between two speakers from a conversation from IEMOCAP dataset",
      "page": 3
    },
    {
      "caption": "Figure 3: presents the architecture diagram of the ERFC so-",
      "page": 3
    },
    {
      "caption": "Figure 3: shows that audio goes as input which then is passed",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed Emotion Recognition and Forecasting in Conversation (ERFC) architecture",
      "page": 4
    },
    {
      "caption": "Figure 4: Several experiments were carried out as defined in TABLE",
      "page": 5
    },
    {
      "caption": "Figure 4: Input output data in the proposed Emotion Recog-",
      "page": 5
    },
    {
      "caption": "Figure 5: , we expand the future turn forecast and show the results",
      "page": 5
    },
    {
      "caption": "Figure 5: Accuracy for various forecasting horizons for mod-",
      "page": 6
    },
    {
      "caption": "Figure 6: which shows the prediction classes distribution. This rep-",
      "page": 6
    },
    {
      "caption": "Figure 6: Confusion matrix for 6 emotions for experiment E4",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiment": "E1",
          "Number of\nEmotions": "6",
          "Emotion\nAttributes\nAVD": "Yes",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "0"
        },
        {
          "Experiment": "E2",
          "Number of\nEmotions": "6",
          "Emotion\nAttributes\nAVD": "Yes",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "1"
        },
        {
          "Experiment": "E3",
          "Number of\nEmotions": "6",
          "Emotion\nAttributes\nAVD": "Yes",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "2"
        },
        {
          "Experiment": "E4",
          "Number of\nEmotions": "6",
          "Emotion\nAttributes\nAVD": "Yes",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "3"
        },
        {
          "Experiment": "E5",
          "Number of\nEmotions": "6",
          "Emotion\nAttributes\nAVD": "No",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "3"
        },
        {
          "Experiment": "E6",
          "Number of\nEmotions": "4",
          "Emotion\nAttributes\nAVD": "Yes",
          "Context\nWindow\nSize\nfor\nall modali-\nties": "3"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Predicting Evoked Emotions in Conversations",
      "authors": [
        "Enas Altarawneh",
        "Ameeta Agrawal",
        "Michael Jenkin",
        "Manos Papagelis"
      ],
      "year": "2023",
      "venue": "Predicting Evoked Emotions in Conversations",
      "arxiv": "arXiv:2401.00383[cs.CL]"
    },
    {
      "citation_id": "2",
      "title": "Pyannote.Audio: Neural Building Blocks for Speaker Diarization",
      "authors": [
        "Herv√© Bredin",
        "Ruiqing Yin",
        "Juan Manuel Coria",
        "Gregory Gelly",
        "Pavel Korshunov",
        "Marvin Lavechin",
        "Diego Fustes",
        "Hadrien Titeux",
        "Wassim Bouaziz",
        "Marie-Philippe Gill"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP40776.2020.9052974"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Provost",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "4",
      "title": "openSMILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor. MM'10 -Proceedings of the ACM Multimedia 2010 International Conference",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "openSMILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor. MM'10 -Proceedings of the ACM Multimedia 2010 International Conference",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "5",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Ni Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "6",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "7",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "8",
      "title": "Real-time Emotion Pre-Recognition in Conversations with Contrastive Multi-modal Dialogue Pre-training",
      "authors": [
        "Xincheng Ju",
        "Dong Zhang",
        "Suyang Zhu",
        "Junhui Li",
        "Shoushan Li",
        "Guodong Zhou"
      ],
      "year": "2023",
      "venue": "Real-time Emotion Pre-Recognition in Conversations with Contrastive Multi-modal Dialogue Pre-training",
      "doi": "10.1145/3583780.3615024"
    },
    {
      "citation_id": "9",
      "title": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions",
      "authors": [
        "Chi-Chun Lee",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "1983",
      "venue": "Proceedings of the Annual Conference of the International Speech Communication Association",
      "doi": "10.21437/Interspeech.2009-480"
    },
    {
      "citation_id": "10",
      "title": "HiTrans: A Transformer-Based Context-and Speaker-Sensitive Model for Emotion Detection in Conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.370"
    },
    {
      "citation_id": "11",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "12",
      "title": "DialogueTRM: Exploring the Intra-and Inter-Modal Emotional Behaviors in the Conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "DialogueTRM: Exploring the Intra-and Inter-Modal Emotional Behaviors in the Conversation"
    },
    {
      "citation_id": "13",
      "title": "Aditi Debsharma, Pallavi Gawade, and Gopali Contractor. 2023. Varta Rasa -A Simple and Accurate System for Emotion Recognition in Conversations",
      "authors": [
        "Rosalin Parida",
        "Babu Sai",
        "Bhushan Udayagiri",
        "Surajit Jagyasi",
        "Sen"
      ],
      "venue": "Distributed Computing and Intelligent Technology"
    },
    {
      "citation_id": "14",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Audio-Visual Emotion Forecasting: Characterizing and Predicting Future Emotion Using Deep Learning",
      "authors": [
        "Sadat Shahriar",
        "Yelin Kim"
      ],
      "year": "2019",
      "venue": "Gesture Recognition",
      "doi": "10.1109/FG.2019.8756599"
    },
    {
      "citation_id": "17",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Sarthak Tripathi",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "18",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "R√©mi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    }
  ]
}