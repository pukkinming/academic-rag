{
  "paper_id": "2006.05236v2",
  "title": "Audino: A Modern Annotation Tool For Audio And Speech",
  "published": "2020-06-09T13:12:44Z",
  "authors": [
    "Manraj Singh Grover",
    "Pakhi Bamdev",
    "Ratin Kumar Brala",
    "Yaman Kumar",
    "Mika Hama",
    "Rajiv Ratn Shah"
  ],
  "keywords": [
    "audio annotation",
    "open source software",
    "annotation tool",
    "speech transcription",
    "speech labelling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we introduce a collaborative and modern annotation tool for audio and speech: audino. The tool allows annotators to define and describe temporal segmentation in audios. These segments can be labelled and transcribed easily using a dynamically generated form. An admin can centrally control user roles and project assignment through the admin dashboard. The dashboard also enables describing labels and their values. The annotations can easily be exported in JSON format for further analysis. The tool allows audio data and their corresponding annotations to be uploaded and assigned to a user through a key-based API. The flexibility available in the annotation tool enables annotation for Speech Scoring, Voice Activity Detection (VAD), Speaker Diarisation, Speaker Identification, Speech Recognition, Emotion Recognition tasks and more. The MIT open source license allows it to be used for academic and commercial projects. \n CCS CONCEPTS • Applied computing → Annotation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Over the past few years, there has been a dramatic improvement in audio and speech research. The rise and performance of deep neural network have achieved state-of-the-art results on various speech and audio tasks  [5, 7, 11, 18] . These networks are necessary to consume and discover information in large volumes of data being published on the web. This necessitates the need to annotate data efficiently at scale for supervised learning of network. In this paper, we present a flexible and modern web-based annotation tool for audio and speech data called audino. The tool aims to provide a broad set of features required for annotation of speech datasets while focusing on increasing collaboration, project management and accessibility. The annotation tool is permissively licensed MIT 1 allowing it to be freely used for both academic research as well as commercial use. The tool can be downloaded from https://github.com/midas-research/audino.\n\nMany annotation tools already exist for image  [13, 16] , text  [4, 12, 14]  and speech  [2, 6, 9]  modality, where most of them require software installation on annotator's system. Recently, there has been increased interest in developing web-based annotation tools  [3, 10, 12, 14, 15, 17] . Moving annotation tools to the web offer several advantages including data security, management and accessibility. A large number of these tools allow loading data, processing and saving annotations on annotator's web browser, while others offer server-side data loading and annotation storage. For speech modality, however, none of the open-source annotation tools to our knowledge offered complete management of annotation process and other advantages of a server-side annotation tool at the time of development of the project (see Table  1  for comparsion between various web-based audio annotation tools available). With this motivation, we developed audino.\n\nWe share and discuss the salient features of the tool below:\n\n• Accessibility. In contrast to most annotation tools which need to be installed and run on annotator's system, audino is a web-based tool which makes it much easier to access through a web browser remotely. The side-effect of having data on annotator's system and the need to load a new datapoint after every datapoint annotation completion is mitigated.\n\nFeatures VIA  [3]  gecko  [10]  Label Studio  [15]  EMU-webApp  [17]  audino Segmentation and Labelling\n\nComparison between various open source web-based audio annotation tools.\n\n• Centralized control of data allocation, project management, and annotations. In contrast to offline tools available, audino secures data access and simplifies project management through centralization. All labels are controlled centrally, which makes it less prone to errors. The annotations are saved in a central database, making it easier to consume. • Easy setup and deployment. The project makes use of Docker to deliver the software easing the setup process, deployment and also scaling of the tool. • Security. The application implements JSON Web Token  [8]  based authentication and authorization for secure login. An annotator can only view projects they are part of and can only access datapoints assigned to them. The audio filenames of all data points are hashed to prevent remote scraping further increases data security. • Multi-language and emoji support. The tool supports Unicode character set, which enables annotation of multilanguage datasets for tasks like Code-Switched  [1]  Automated Speech Recognition (ASR).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Software Design",
      "text": "audino is a production-ready web application tool. Figure  1  provides a high-level overview of the working of different components in the tool. Its client-side is platform-independent and can run on any modern browser. The server side serves the REST API and static content. All annotations and application data are stored on the server. We describe the software design in detail in the following sections.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Storage",
      "text": "The tool requires three types of data to be stored:\n\n2.1.1 Application data. This includes users, roles, projects, data, labels, and annotations generated. This data is stored in a structured format in a dockerized SQL database. The entity-relationship diagram for the database is shared in documentation 2 . For the current version, the tool supports MySQL database, however, it can easily be extended to other SQL databases available.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "User Session.",
      "text": "To store the current user session, the application uses a dockerized Redis 3 store. The application generates JSON Figure  1 : High level architecture of audino.\n\nWeb Token ID for every user login and saves it in the store (with an expiration time) for future authentication.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Data.",
      "text": "The audio uploaded is saved at a defined path inside the backend docker container. The application generates a unique filename for each uploaded file and stores the name inside the SQL database. The application then serves this file on request. The tool currently supports WAV, MP3 and OGG file formats as all browsers widely support these.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Server Side",
      "text": "In addition to storage components discussed in the previous section, the server-side of the tool also includes an NGINX  4  server and an API server. NGINX is a high-performance web server which can also be used for reverse proxy and HTTP cache. The application utilizes a dockerized NGINX server to serve static client-side content and a REST API using reverse proxy. The REST API server runs in a separate docker container using uWSGI  5  . The tool uses a Pythonbased framework called Flask 6  , and its plugins to provide a RESTful API. This API allows authentication by checking the Redis store for request user's session. The API also enables the client-side to perform CRUD operations on the database. To interact and perform database operations, the API uses SQLAlchemy 7  library. Also, it provides a layer over database allowing easy switching to other SQL databases available. Alembic 8  library is used for versioning and migrating the database.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Client Side",
      "text": "The client-side interface is written mainly in HTML, CSS and JavaScript. The user interface is broken into individual components and developed using React 9  , a JavaScript library for building user interfaces. React allows wiring of these client-side components with respective handlers as well as the REST API. Based on user interactions, the React components are rendered, and API requests are made. To make the application work for all screen size, the interface is styled using Bootstrap 10  CSS framework. The annotation dashboard leverages wavesurfer.js 11  library and its plugins for rendering audios and marking temporal regions. A production build is generated using React build system for NGINX to serve.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Workflow",
      "text": "An admin account is created during setup based on the information provided by the user. Account details of this user should be used to login for the first time. On opening the web application, a login screen is displayed. A user is required to have their account details in order to access the tool. Once logged in, the user dashboard is displayed listing the projects assigned to the user. The user can click on a project name to move to a dashboard which lists audio datapoints assigned to that user for that project in a paginated manner. These datapoints are categorized based on their completion status and whether they are marked for review or not. On clicking on the filename of a datapoint, the annotation panel opens for that audio. We will describe the annotation panel in detail in Section 4.\n\nThe application also provides an admin panel accessible to users with admin role (illustrated in Figure  2 ). This panel allows admins to manage projects and users. An admin can create new users, assign roles and projects to that user through this panel. The panel also allows the creation of new projects, labels and their associated label values for that project. These labels can be single choice select or multi-choice depending on the requirement. For each new project, an API Key is generated, which allows uploading of new datapoints and, optionally, their corresponding reference transcriptions and annotations for that project. This gives flexibility to pre-generate noisy labels or transcription for specific tasks using pre-trained machine learning models, review and correct them using the tool. Post the completion of annotation process, the annotations can be exported through the panel and consumed further for analysis or in any machine learning pipeline.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Annotation Dashboard",
      "text": "Figure  3  illustrates the annotation dashboard. The audio datapoint selected is rendered as a waveform. This component allows users to create multiple temporal segments on audio for annotation. An audio control panel is provided to pause/play, move forward and backwards on the audio timeline. A zoom slider is also provided to control and zoom into a particular audio section for precise segmentation. This is particularly useful for annotating segments of phonemes. A reference transcription is displayed below the control panel, if provided when the datapoint was uploaded. These reference transcriptions could be ASR generated or transcribed by humans. This is specifically useful for tasks like speaker diarisation and ASR, where efficient segmentation and correction of transcriptions is required for improving ASR performance. On segment selection, a form consisting of segment transcript and associated project labels is displayed and is to be filled by the annotator. The annotator can save or delete any segment during the process, and the same will be reflected in the database. Finally, the annotators can mark a datapoint for review. These datapoints are displayed under a separate category on the project's data dashboard.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Use Cases",
      "text": "A primary use of audino is for transcription of human speech. We successfully used audino to transcribe the data collected through the administration of Simulated Oral Proficiency Interview for L2 English speakers by Second Language Testing, Inc., a US-based language testing company, to screen potential employees. The audios were sampled and assigned to two transcribers with an overlap of 20% for quality check. The transcribers segmented the audios and transcribe each segment on the tool. These transcriptions were consumed by a CRON job which tracked and reported Word Error Rate (WER) between the two transcribers. It also flagged the audios with major transcription errors which were later resolved through discussion. Using audino, a total of 65 hours of responses were transcribed which, later, was used to fine-tune a pre-trained ASR. This ASR system was later used for transcription of complete dataset and developing an automated oral proficiency scoring system  [7] . Among other projects, the audino tool has been used for annotating Content, Coherence and Disfluency attributes for the same data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Summary And Roadmap",
      "text": "In this paper, we presented audino, a collaborative web-based modern annotation tool that allows temporal segmentation, transcription and labelling of language and speech aspects. We provide comprehensive documentation and tutorials to get the users started on our GitHub page. The project has been under active development for more than a year now and has been used successfully for large-scale projects at our lab. Open sourcing the tool allows us to discover new possibilities of its utilization while enabling collaboration and easier management of dataset generation task.\n\nThe short-term roadmap of the project includes adding enhancements like providing connectors for usage of cloud storage, keyboard shortcuts, improve mobile experience, exposing complete API through API key authentication, and an analytics dashboard, which can offer insights into the quality of annotations generated, their statistics and agreement between annotators. Some of these features are already in development phase and will be released soon. The long-term roadmap includes improving test coverage of the project, adding continuous integration and delivery to development flow, adding project templates for speech-related tasks enabling more straightforward project setup, and leveraging recent state-of-the-art models for automatic labelling and transcription of audios (reducing overall annotation effort). We welcome everyone to contribute to the project and provide constructive feedback.",
      "page_start": 3,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: High level architecture of audino.",
      "page": 2
    },
    {
      "caption": "Figure 2: Admin panel with marked regions explaining vari-",
      "page": 3
    },
    {
      "caption": "Figure 2: ). This panel allows admins to",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the annotation dashboard. The audio datapoint",
      "page": 3
    },
    {
      "caption": "Figure 3: Screenshot of annotation dashboard showcasing various components.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Features": "Segmentation and Labelling\nProject Management\nUser Management\nLabel Management\nCloud Storage Access\nDatabase Storage\nServer side\nLicense",
          "VIA [3]": "✓\n×\n×\n×\n×\n×\n×\nBSD 2",
          "gecko [10]": "✓\n×\n×\n×\n✓\n×\n×\nBSD 3",
          "Label Studio [15]": "✓\n×\n×\n✓\n✓\n×\n✓\nApache 2.0",
          "EMU-webApp [17]": "✓\n×\n×\n✓\n×\n✓\n✓\nMIT",
          "audino": "✓\n✓\n✓\n✓\n×\n✓\n✓\nMIT"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Code-switching in conversation: Language, interaction and identity",
      "authors": [
        "Peter Auer"
      ],
      "year": "2013",
      "venue": "Code-switching in conversation: Language, interaction and identity"
    },
    {
      "citation_id": "2",
      "title": "Transcriber: Development and use of a tool for assisting speech corpora production",
      "authors": [
        "Claude Barras",
        "Edouard Geoffrois",
        "Zhibiao Wu",
        "Mark Liberman"
      ],
      "year": "2001",
      "venue": "Speech Communication",
      "doi": "10.1016/S0167-6393(00)00067-4"
    },
    {
      "citation_id": "3",
      "title": "The VIA Annotation Software for Images, Audio and Video",
      "authors": [
        "Abhishek Dutta",
        "Andrew Zisserman"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia",
      "doi": "10.1145/3343031.3350535"
    },
    {
      "citation_id": "4",
      "title": "A Webbased Tool for the Integrated Annotation of Semantic and Syntactic Structures",
      "authors": [
        "Richard Eckart De Castilho",
        "Éva Mújdricza-Maydt",
        "Seid Muhie Yimam",
        "Silvana Hartmann",
        "Iryna Gurevych",
        "Anette Frank",
        "Chris Biemann"
      ],
      "year": "2016",
      "venue": "Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities"
    },
    {
      "citation_id": "5",
      "title": "End-to-End Neural Speaker Diarization with Permutation-free Objectives",
      "authors": [
        "Yusuke Fujita",
        "Naoyuki Kanda",
        "Shota Horiguchi",
        "Kenji Nagamatsu",
        "Shinji Watanabe"
      ],
      "year": "2019",
      "venue": "End-to-End Neural Speaker Diarization with Permutation-free Objectives"
    },
    {
      "citation_id": "6",
      "title": "XTrans: A speech annotation and transcription tool",
      "authors": [
        "Meghan Lammie Glenn",
        "Stephanie Strassel",
        "Haejoong Lee"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "7",
      "title": "Multi-modal Automated Speech Scoring using Attention Fusion",
      "authors": [
        "Manraj Singh",
        "Yaman Kumar",
        "Sumit Sarin",
        "Payman Vafaee",
        "Mika Hama",
        "Rajiv Ratn Shah"
      ],
      "year": "2020",
      "venue": "Multi-modal Automated Speech Scoring using Attention Fusion",
      "arxiv": "arXiv:2005.08182[cs.CL]"
    },
    {
      "citation_id": "8",
      "title": "JSON Web Token (JWT)",
      "authors": [
        "Michael Jones",
        "John Bradley",
        "Nat Sakimura"
      ],
      "year": "2015",
      "venue": "JSON Web Token (JWT)",
      "doi": "10.17487/rfc7519"
    },
    {
      "citation_id": "9",
      "title": "Anvil-a generic annotation tool for multimodal dialogue",
      "authors": [
        "Kipp Michael"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "10",
      "title": "GECKO -A Tool for Effective Annotation of Human Conversations",
      "authors": [
        "Golan Levy",
        "Raquel Sitman",
        "Ido Amir",
        "Eduard Golshtein"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Jasper: An End-to-End Convolutional Neural Acoustic Model",
      "authors": [
        "Jason Li",
        "Vitaly Lavrukhin",
        "Boris Ginsburg",
        "Ryan Leary",
        "Oleksii Kuchaiev",
        "Jonathan Cohen",
        "Huyen Nguyen",
        "Ravi Teja"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2019-1819"
    },
    {
      "citation_id": "12",
      "title": "doccano: Text Annotation Tool for Human",
      "authors": [
        "Hiroki Nakayama",
        "Takahiro Kubo",
        "Junya Kamura",
        "Yasufumi Taniguchi",
        "Xu Liang"
      ],
      "year": "2018",
      "venue": "doccano: Text Annotation Tool for Human"
    },
    {
      "citation_id": "13",
      "title": "Web-Based Configurable Image Annotations",
      "authors": [
        "Matthieu Pizenberg",
        "Axel Carlier",
        "Emmanuel Faure",
        "Vincent Charvillat"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia",
      "doi": "10.1145/3240508.3243656"
    },
    {
      "citation_id": "14",
      "title": "brat: a Web-based Tool for NLP-Assisted Text Annotation",
      "authors": [
        "Pontus Stenetorp",
        "Sampo Pyysalo",
        "Goran Topić",
        "Tomoko Ohta",
        "Sophia Ananiadou",
        "Jun'ichi Tsujii"
      ],
      "year": "2012",
      "venue": "Proceedings of the Demonstrations Session at EACL 2012"
    },
    {
      "citation_id": "15",
      "title": "Label Studio: Data labeling software",
      "authors": [
        "Maxim Tkachenko",
        "Mikhail Malyuk",
        "Nikita Shevchenko",
        "Andrey Holmanyuk",
        "Nikolai Liubimov"
      ],
      "year": "2020",
      "venue": "Open source software available from"
    },
    {
      "citation_id": "16",
      "title": "labelme: Image Polygonal Annotation with Python",
      "authors": [
        "Kentaro Wada"
      ],
      "year": "2016",
      "venue": "labelme: Image Polygonal Annotation with Python"
    },
    {
      "citation_id": "17",
      "title": "Introducing a web application for labeling, visualizing speech and correcting derived speech signals",
      "authors": [
        "Raphael Winkelmann",
        "Georg Raess"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "18",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    }
  ]
}