{
  "paper_id": "2112.11718v2",
  "title": "Hybrid Curriculum Learning For Emotion Recognition In Conversation",
  "published": "2021-12-22T08:02:58Z",
  "authors": [
    "Lin Yang",
    "Yi Shen",
    "Yue Mao",
    "Longjun Cai"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC) aims to detect the emotion label for each utterance. Motivated by recent studies which have proven that feeding training examples in a meaningful order rather than considering them randomly can boost the performance of models, we propose an ERCoriented hybrid curriculum learning framework. Our framework consists of two curricula: (1) conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In CC, we construct a difficulty measurer based on \"emotion shift\" frequency within a conversation, then the conversations are scheduled in an \"easy to hard\" schema according to the difficulty score returned by the difficulty measurer. For UC, it is implemented from an emotion-similarity perspective, which progressively strengthens the model's ability in identifying the confusing emotions. With the proposed model-agnostic hybrid curriculum learning strategy, we observe significant performance boosts over a wide range of existing ERC models and we are able to achieve new state-of-the-art results on four public ERC datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) has attracted numerous interests from the NLP community in recent years due to its potential applications in many areas, such as opinion mining in social media  (Chatterjee et al. 2019) , dialogue generation  (Huang et al. 2018 ) and fake news detection  (Guo et al. 2019) . The objective of ERC is to detect emotions expressed by the speakers in each utterance of the conversation. Previous works on ERC usually solve this problem with two steps. At the first step, each utterance is encoded separately into an utterance-level representation, which will be used as the input for sequence-based models  (Majumder et al. 2019; Hazarika et al. 2018a; Jiao et al. 2019)  or graphbased models  (Ghosal et al. 2019; Ishiwatari et al. 2020 ) during the second step. Despite their success, previous works still have a lot of room for improvement  (Poria et al. 2019b) .\n\nCurriculum learning (CL)  (Bengio et al. 2009 ) is a training strategy which imitates the meaningful learning order in human curricula. The core idea of CL is to train the machine learning model with easier data subsets at first, and then gradually increase the difficulty level of data until the whole training dataset. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the overall performance of various models in a wide range of scenarios  (Wang, Chen, and Zhu 2020) . Inspired by the success of CL in other NLP tasks  (Zhou et al. 2020; Liu et al. 2018; Su et al. 2021) , in this paper, we make effort to leverage the spirit of CL to improve the traditional ERC methods. Due to the hierarchical structure of the ERC datasets, we need to construct the curricula from two granularities: one curriculum sorts the conversations in the dataset from easy to hard, and the other sorts the utterances in each conversation from easy to hard.\n\nThe question arises how to measure the difficulty of conversations and utterances. Previous studies  (Majumder et al. 2019; Shen et al. 2021a ) have reported that most ERC methods mainly suffer from two issues: 1) \"emotion shift\" problem. It refers to that these methods cannot efficiently handle scenarios in which emotions of two consecutive utterances are different  (Ghosal et al. 2021) . 2) \"confusing label\" problem. Previous methods  (Ghosal et al. 2019; Shen et al. 2021b ) usually fail to distinguish between similar emotions very well. This is due to the subtle semantic difference between certain emotion labels such as happy and exciting. These two phenomena provide us the key to quantify the difficulty of conversations and utterances in ERC.\n\nIn this paper, we tailor-design a hybrid curriculum learning (HCL) framework for the ERC task. HCL framework consists of two complementary curriculum strategies, conversation-level curriculum (CC) and utterance-level curriculum (UC). In CC, we construct a difficulty measurer based on \"emotion shift\" frequency within a conversation, then the conversations with lower difficulty are presented to the model before harder ones. This way, the model gradually increases its ability to tackle the \"emotion shift\" problem.\n\nWhile in UC, since ERC requires reasoning over multiple utterances in the conversation, we cannot directly schedule the utterances asynchronously in the \"easy to hard\" scheme. As a result, we design an emotion-similarity based curriculum (ESC) to implement utterance-level curriculum learning. Specifically, inspired by the \"confusing label\" problem mentioned above, we believe that in a conversation, those utterances with confusing labels are more difficult than others. Therefore, we make the model focus on the utterances with easily recognizable emotion labels in the early stage, then progressively strengthened the model's capability of identifying the confusing emotions.\n\nMore specifically, based on previous studies  (Plutchik 1982; Mikels et al. 2005)  on psychology, we employ the degree of intersection angle between different emotion labels in Valence-Arousal 2D emotion space  (Guo et al. 2019; Yang et al. 2021)  to measure the similarity between emotion labels.\n\nDuring ESC, instead of one-hot encoding, the target is a probability distribution over all possible emotion labels. The probability of each label is determined by the similarity between current label and the gold label. In other words, besides its true emotion label, each utterance can also belong to similar emotions to a lesser extent. In the beginning of the training process, the targets of utterances with emotions happy and excited should almost be the same, but always be very different from sad. Then the label representation gradually shifted to the one-hot encoding during the following training process. This way, small mistakes are corrected less than serious mistakes at the beginning, it could also be considered as an easy-to-hard curriculum in which broad concepts are taught before those tiny differences are emphasized.\n\nOur hybrid curriculum learning framework is modelagnostic. We evaluate our approach on five representative ERC models. Results on four benchmark datasets demonstrate that the proposed hybrid curriculum learning framework leads to significant performance improvements.\n\nIn summary, our main contributions are as follows:\n\n• We propose a hybrid curriculum learning framework to tackle the task of ERC. At conversation-level curriculum, we utilize an emotion-shift frequency to measure the difficulty of each conversation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Emotion Recognition In Conversation",
      "text": "Emotion recognition in conversations (ERC) has been widely studied due to its potential application prospect. The key point of ERC is how to effectively model the context of each utterance and corresponding speaker. Existing works generally resort to deep learning methods to capture contextual characteristics, which can be divided into sequencebased and graph-based methods. Another direction is to improve the performance of existing models by incorporating various external knowledge, which we classified as knowledge-based methods.\n\nSequence-based Methods Many previous works consider contextual information as utterance sequences. ICON  (Hazarika et al. 2018a ) and CMN  (Hazarika et al. 2018b)  both utilize gated recurrent unit (GRU) to model the utterance sequences. DialogueRNN  (Majumder et al. 2019 ) employs a GRU to capture the global context which is updated by the speaker state GRUs.  Jiao et al. (2019)  propose a hierarchical neural network model that comprises two GRUs for the modelling of tokens and utterances respectively.  Hu, Wei, and Huai (2021)  introduce multi-turn reasoning modules on Bi-directional LSTM to model the ERC task from a cognitive perspective.\n\nGraph-based Methods In this category, some existing works  (Ghosal et al. 2019; Ishiwatari et al. 2020; Zhang et al. 2019 ) utilize various graph neural networks to capture multiple dependencies in the conversation. DialogXL  (Shen et al. 2021a ) modifies the memory block in XLNet  (Yang et al. 2019)  to store historical context and leverages the selfattention mechanism in XLNet to deal with the multi-turn multi-party structure in conversation.  Shen et al. (2021b)  design a directed acyclic graph (DAG) to model the intrinsic structure within a conversation, which achieves the state-ofthe-art performance without considering the introduction of external knowledge.\n\nKnowledge-based Methods KET (Zhong, Wang, and Miao 2019) employs hierarchical transformers with concept representations extracted from the ConceptNet (Speer and Lowry-Duda 2017) for emotion detection, which is the first ERC model integrates common-sense knowledge. COSMIC  (Ghosal et al. 2020 ) adopts a network structure very close to DialogRNN and adds external commonsense knowledge from ATOMIC  (Sap et al. 2019)  to improve its performance. TODKAT  (Zhu et al. 2021 ) leverages an encoder-decoder architecture which incorporates topic representation with commonsense knowledge from ATOMIC for ERC.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Curriculum Learning",
      "text": "Starting from the work by  Bengio et al. (2009) , a variety of curriculum learning approaches  (Wang, Chen, and Zhu 2020; Soviany et al. 2021 ) has been studied. In the field of NLP, curriculum learning has been used for various tasks such as neural machine translation  (Zhou et al. 2020; Liu et al. 2020) , relation extraction  (Huang and Du 2019 ) and natural answer generation  (Liu et al. 2018) . To the best of our knowledge, we leverage curriculum learning for the first time in the ERC task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Framework Task Definition",
      "text": "In ERC, a conversation C contains a sequence of textual utterances {u 1 , u 2 ..., u n }, where n denotes the number of utterances. Each utterance u i = {w i,1 , w i,2 ..., w i,t(ui) } consists of t(u i ) tokens, where t(u i ) is the length of u i . There are m participants P = {p 1 , p 2 ..., p m }(m ≥ 2) in C. Each utterance u i is uttered by one participant in P . Given a predefined emotion label set E = {y 1 , y 2 , ..., y r }, the objective of the ERC task is to predict the emotion label of each utterance in C with the information provided above.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion-Similarity Based Cl Whole Training Dataset",
      "text": "Step 1\n\nStep T\n\nStep t",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview",
      "text": "In curriculum learning, a typical curriculum design consists of two core components: difficulty measurer and training scheduler  (Bengio et al. 2009 ). Difficulty Measurer is used to quantify the relative \"easiness\" of each data example.\n\nThe training scheduler arranges the sequence of data subsets throughout the training process based on the judgment from the difficulty measurer. For ERC oriented curriculum learning, the challenge is how to design suitable difficulty measurer and training scheduler for emotion recognition.\n\nA conversation consists of a sequence of utterances. This hierarchical structure inspired us to construct two curricula for scheduling conversations and utterances respectively. Therefore, our framework consists of two nested curricula, conversation-level curriculum (CC) on the outside and utterance-level curriculum (UC) on the inside.\n\nFor CC, we design an emotion-shift based difficulty measurer. A widely used CL strategy called baby step (Spitkovsky, Alshawi, and Jurafsky 2010) is leveraged as training scheduler. For UC, due to the characteristics of the ERC task, the utterances in the same conversation must be input into a batch simultaneously during the training process. As a result, it is infeasible to employ traditional training scheduler such as baby step to arrange the training order of the utterances. We proposed emotion-similarity based curriculum learning to address this issue.\n\nThe proposed HCL framework is illustrated in Figure  1  and the details of CC and UC are elaborated in following two subsections, respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conversation-Level Curriculum",
      "text": "To design conversation-level curriculum for ERC, we need to answer a question: what kind of conversation is supposed to be easier than other conversations? Since we have mentioned that previous ERC models  (Majumder et al. 2019; Shen et al. 2021a ) tend to suffer from emotion-shift issue, we adopt emotion-shift frequency to measure the difficulty of each conversation. The main idea is that, the more frequent emotion-shift in conversation c i occurs, the more difficult it is. Therefore, the conversation-level difficulty score of c i is defined as\n\nwhere N es (c i ) and N u (c i ) denote the number of emotionshift occurrences in c i and the total number of utterances in c i , respectively. N sp (c i ) is the number of speakers take part in c i and it acts as a smoothing factor.\n\nWe leverage baby step training scheduler (Spitkovsky, Alshawi, and Jurafsky 2010) to arrange conversations and organize the training process. Specifically, the whole training set D is divided into different buckets, i.e. {D 1 , • • • , D T }, in which those conversations with similar difficulty scores are categorized into the same bucket. The training starts from the easiest bucket. After a fixed number of training epochs or convergence, the next bucket is merged into the current training subset. Finally, after all the buckets are merged and used, the whole training process further continues several extra epochs. Our HCL framework is described in Algorithm 1 and the process of CC is illustrated as Line 1-Line 5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Utterance-Level Curriculum",
      "text": "As it is infeasible to employ a traditional CL training scheduler to asynchronously arrange the order of the utterances, the question arises how to measure the difficulty of the utterances and establish a feasible curriculum at utterance-level.\n\nWe address this problem by assuming that the utterances with confusing emotion labels are more difficult for prediction and our utterance-level curriculum is based on the pairwise similarities between the emotion labels.   (Jing, Mao, and Chen 2019; Yang et al. 2021; Toisoul et al. 2021) . The emotions in red color have appeared in previous versions. The emotions in green color is what we added (these emotions only appear in EmoryNLP dataset). θ denotes the intersection angle between happy and excited. The angle between similar emotions will be relatively small. Previous studies  (Plutchik 1982; Mikels et al. 2005 ; Russell 1980) on psychology believe that emotion contains two dimensions: arousal and valence, and they are used to leverage a wheel-like 2D coordinate system to describe emotions. Inspired by these works, we propose a new emotion wheel as Figure  2 , which contains all emotions in the standard ERC datasets. As depicted in Figure  2 , each emotion label can be mapped to a point on the unit circle. Then we calculate the similarity between emotion labels as in Equation  2 .\n\nHere, s ij stands for the similarity of label i and label j. v i denotes the valence value of i. We take the cosine of the included angle θ ij between i and j as their similarity. If θ ij > 90 • (i.e., cosθ ij < 0) the similarity is set to 0. If the valence polarities of i and j are opposite, then the similarity is also set to 0. The similarity between label neutral and other labels is defined as 1/N , where N is the total number of emotions in corresponding datasets.\n\nThe process of emotion-similarity based curruclum learning (ESC) is described as Line 6 -Line 13 in Algorithm 1. We first calculate the similarity between each emotion label pair as Equation 2 and generate the emotion similarity matrix M sim , then M sim is normalized as M target . At the beginning of ESC training, we take the rows of M target as the initial target probability distributions over all possible classes for training, and each row corresponds to an emotion label. That is, instead of solely belonging to its groundtruth label, each input utterance can also belong to similar labels to a lesser extent. During the training process, this label representation is gradually shifted towards the standard one-hot-encoding. We define the update strategy as in Line 9 -Line 11, where m i,j denotes the probability of j-th element of i-th row in M target at training step t. The constant parameter ∈ (0, 1) controls how quickly the label vectors converge to the one-hot-encoded labels. Row-wise normalization is performed after each update. This update strategy leads to a proper label-weighting curriculum.\n\nFor each training step, the predicted probability distribution of utterance u i defined as P ui . Finally, the model is trained with the standard cross-entropy loss function as Equation  3 , where P c ui [k] denotes the predicted probability that the label of u i in conversation c is k. M target [y c ui ] k denotes the target probability of label k in current labelsimilarity matrix at training step t. z is total number of conversations in training set, n is the utterance number of conversation c. In this way, we implement UC through ESC.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Settings Datasets",
      "text": "We evaluate our method on the following four published ERC datasets 1  : IEMOCAP  (Busso et al. 2008) , MELD  (Poria et al. 2019a) , DailyDialog  (Li et al. 2017) , EmoryNLP  (Zahiri and Choi 2018) . The detailed statistics of the datasets are reported in Table  1   2  .\n\nFollowing previous works  (Ghosal et al. 2019; Zhong, Wang, and Miao 2019; Ishiwatari et al. 2020) , the evaluation metrics are chosen as micro-F1 excluding the extremely high majority class (neutral) for DailyDialog and weighted-F1 for other three datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "Since HCL is a model-agnostic framework, we choose the following five ERC models to verify whether HCL is able to further improve the performance of these models.  TODKAT  (Zhu et al. 2021 ) This is the state-of-theart knowledge-based ERC model. Besides commonsense knowledge, it also incorporates topic information.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "All of the baseline models mentioned above have released their source codes. We keep exactly the same settings as reported in the original papers during our experiments. For HCL, the tunable hyperparameters include number of buckets in CC, max training epochs during each baby step, interval steps for training target updating in UC, decay factor in UC. These hyperparameters are manually tuned on each dataset with hold-out validation. The results reported in our experiments are all based on the average score of 5 random runs on the test set. Our experiments are conducted on a single Tesla V100M32 GPU.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis Overall Results",
      "text": "The overall experimental results are reported in Table  2 , where \"X+HCL\" means training the model X with the proposed HCL framework. We can see that HCL has improved the performance of all baseline models, showing the robustness and universality of our approach.\n\nIn general, the performance boosts achieved by HCL on models with simpler feature extractor (i.e., DialogueRNN and DialogueGCN) are more remarkable. An exception is that TODKAT+HCL achieves significant improvements on three datasets. The reason may be that the original TODKAT model does not take account of the speaker information, while our CC has introduced the inter-speaker emotion-shift in the difficulty measurer, which is equivalent to considering speaker information to a certain extent and is beneficial for TODKAT.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "To reveal the individual effects of CC and UC, we try different variants of HCL on TODKAT by removing either CC or UC. The experimental results on IEMOCAP and EmoryNLP are shown in Table  3 , from which we see that both CC and UC make positive contributions to the overall performance when used alone. Although only utilizing UC leads to larger improvements than only using CC, the optimal performance is achieved when CC and UC are combined, indicating that CC and UC are complementary to each other.\n\nIn addition, we also tried another two strategies to combine CC and UC: CC-First (CCF) and UC-First (UCF). CCF performs CC and UC in a pipeline manner. In UCF, the execute order of CC and UC is reversed. The results of CCF and UCF are also outlined in Table  3 . It shows that UCF is better than CCF and HCL outperforms both CCF and UCF. This is intuitive, because HCL makes UC and CC interact with each other during the training process, which is more consist with the hierarchical structure of conversation, so the performance is even better than UCF.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance For Emotion-Shift",
      "text": "To verify the effect of HCL in the emotion-shift scenario, we summarize the results of TODKAT+HCL on different types of utterances. The results are presented in Table  4 , where ES and N-ES denote utterances with emotion-shift and utterances without emotion-shift, respectively. HCL improves    A plausible explanation is that the training set of IEMO-CAP contains much less conversations and the average length of conversations is much longer, so the difficulty scores of conversations in IEMOCAP are usually lower. Therefore, for IEMOCAP, the difficulty discrimination between different buckets in the training scheduler is not as obvious as EmoryNLP.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance On Different Emotions",
      "text": "In this subsection, we aim to verify whether HCL can improve the performance of baseline model on \"confusing labels\". For each pair of emotion labels in ERC dataset, if their similarity (defined in Equation  2 ) is larger than 0, then both of them are regarded as confusing labels in our setting. 3 . We report the results of DAG-ERC and DAG-ERC+HCL on every emotion label in IEMOCAP. There are a total of four confusing labels in this dataset: happy(H), excited(E), sad(S) and frustrated(F). As presented in Table  5 , DAG-ERC+HCL outperforms DAG-ERC on all emotion labels other than neutral and the overall performance on the confusing labels is better ( 69.37 vs 67.88 on weighted-F1). This shows that HCL does strengthen the ability on distinguishing the confusing emotion labels of DAG-ERC. However, the performace is limited by neutral, the reason is that neutral is similar to every other label to some extent as in Equation 2, which increases the difficulty for recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Case Study",
      "text": "Figure  3 (a) shows a conversation passage sampled from the IEMOCAP dataset. The goal is to predict the emotion label of the last utterance in the blue box. Due to emotionshift occurs, all the baseline methods in our experiment are easy to mistakenly identify the emotion as frustrated. Most of our \"X+HCL\" methods are able to recognize the emotion of this utterance correctly, which indicates that HCL alleviates this problem to some extent. Figure  3 (b) depicts a case with confusing labels. The gold emotion label of the last utterance in the red box is excited. Some of the baseline models such as DialogueGCN and DAG-ERC mistook the emotion as happy. After following HCL framework, Di- Right.\n\n[HAPPY]\n\n[NEUTRAL]\n\n[EXCITED]\n\nSpeaker A Speaker B\n\n[EXCITED]\n\n[EXCITED]\n\n[EXCITED]  alogueGCN+HCL and DAG-ERC+HCL successfully identified the emotion as the correct label excited.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Why Curriculum Learning Works?",
      "text": "According to the theory of curriculum learning  (Bengio et al. 2009) , the curriculum will work only if the entropy of data distributions increases during the training process. In HCL, conversation-level curriculum leverages the emotionshift frequency to measure the difficulty. The more frequent the emotion-shift occurs in a conversation, the greater the diversity of the emotion labels, in other words, the higher the entropy. For utterance-level curriculum, since emotionsimilarity based CL does not distinguish similar emotion in the early stage, it is equivalent to merging some emotion labels and could be considered as reducing the diversity of emotions. As a result, it also meets the condition which the entropy should be increased gradually.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose simple but effective hybrid curriculum learning (HCL) for emotion recognition in conversations. HCL is a flexible framework independent of the original training models. During training, HCL simultaneously employs conversation-level and utterance-level curricula to execute the training process as an easy to hard schema. Conversation-level curriculum consists of an emotion-shfit based difficulty measurer and a baby step scheduler. Utterance-level curriculum is implemented as emotion-similarity based CL. Experiments on four benchmark datasets have proved the generality and effectiveness of HCL.\n\nIn the future, we plan to improve our method in three directions. First, we will attempt to seek other suitable features to construct difficulty measurer for ERC. Second, we aim to introduce other training schedulers for CL to further improve the performance. Finally, we aim to apply a learning-based approach to model the similarity between emotion labels.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.",
      "page": 3
    },
    {
      "caption": "Figure 1: and the details of CC and UC are elaborated in following",
      "page": 3
    },
    {
      "caption": "Figure 2: The 2D arousal-valence emotion wheel proposed",
      "page": 4
    },
    {
      "caption": "Figure 2: , which contains all emotions in the standard ERC",
      "page": 4
    },
    {
      "caption": "Figure 2: , each emotion label can be",
      "page": 4
    },
    {
      "caption": "Figure 3: (a) shows a conversation passage sampled from the",
      "page": 6
    },
    {
      "caption": "Figure 3: (b) depicts",
      "page": 6
    },
    {
      "caption": "Figure 3: Two conversation passages from IEMOCAP for case study. The ground-truth emotion label of each utterance is given",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "els and we are able to achieve new state-of-the-art results on": "four public ERC datasets.",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "dle scenarios in which emotions of\ntwo consecutive utter-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "ances are different\n(Ghosal et al. 2021). 2) “confusing la-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "bel” problem. Previous methods (Ghosal et al. 2019; Shen"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "Introduction",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "et al. 2021b) usually fail to distinguish between similar emo-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "Emotion recognition in conversation (ERC) has attracted nu-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "tions very well. This is due to the subtle semantic difference"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "merous interests from the NLP community in recent years",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "between certain emotion labels such as happy and exciting."
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "due to its potential applications in many areas, such as opin-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "These two phenomena provide us the key to quantify the dif-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "ion mining in social media (Chatterjee et al. 2019), dialogue",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "ﬁculty of conversations and utterances in ERC."
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "generation (Huang et al. 2018) and fake news detection (Guo",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "In this paper, we tailor-design a hybrid curriculum learn-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "et al. 2019). The objective of ERC is\nto detect emotions",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "ing\n(HCL)\nframework\nfor\nthe ERC task. HCL frame-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "expressed by the speakers in each utterance of\nthe conver-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "work consists of two complementary curriculum strategies,"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "sation. Previous works on ERC usually solve this problem",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "conversation-level curriculum (CC) and utterance-level cur-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "with two steps. At\nthe ﬁrst step, each utterance is encoded",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "riculum (UC).\nIn CC, we construct a difﬁculty measurer"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "separately into an utterance-level representation, which will",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "based on “emotion shift” frequency within a conversation,"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "be used as the input for sequence-based models (Majumder",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "then the conversations with lower difﬁculty are presented to"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "et al. 2019; Hazarika et al. 2018a; Jiao et al. 2019) or graph-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "the model before harder ones. This way, the model gradually"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "based models (Ghosal et al. 2019; Ishiwatari et al. 2020) dur-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "increases its ability to tackle the “emotion shift” problem."
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "ing the second step. Despite their success, previous works",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "While in UC, since ERC requires reasoning over multiple"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "still have a lot of room for improvement (Poria et al. 2019b).",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "utterances in the conversation, we cannot directly schedule"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "Curriculum learning (CL) (Bengio et al. 2009) is a train-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "the utterances asynchronously in the “easy to hard” scheme."
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "ing strategy which imitates\nthe meaningful\nlearning order",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "As a result, we design an emotion-similarity based curricu-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "in human curricula. The core idea of CL is to train the ma-",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "lum (ESC)\nto implement utterance-level curriculum learn-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "chine learning model with easier data subsets at ﬁrst, and",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "ing. Speciﬁcally, inspired by the “confusing label” problem"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "mentioned above, we believe that in a conversation, those ut-"
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "*These authors contributed equally.",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": ""
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "Copyright © 2022, Association for the Advancement of Artiﬁcial",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "terances with confusing labels are more difﬁcult than others."
        },
        {
          "els and we are able to achieve new state-of-the-art results on": "Intelligence (www.aaai.org). All rights reserved.",
          "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-": "Therefore, we make the model focus on the utterances with"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "whole training dataset. As an easy-to-use plug-in,\nthe CL"
        },
        {
          "Abstract": "Emotion recognition in conversation (ERC) aims\nto detect",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "strategy has demonstrated its power in improving the over-"
        },
        {
          "Abstract": "the emotion label\nfor each utterance. Motivated by recent",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "all performance of various models in a wide range of sce-"
        },
        {
          "Abstract": "studies which have proven that feeding training examples in",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "narios (Wang, Chen, and Zhu 2020). Inspired by the success"
        },
        {
          "Abstract": "a meaningful order\nrather\nthan considering them randomly",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "of CL in other NLP tasks (Zhou et al. 2020; Liu et al. 2018;"
        },
        {
          "Abstract": "can boost\nthe performance of models, we propose an ERC-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "oriented hybrid curriculum learning framework. Our\nframe-",
          "then gradually increase the difﬁculty level of data until\nthe": "Su et al. 2021), in this paper, we make effort to leverage the"
        },
        {
          "Abstract": "work consists of two curricula: (1) conversation-level curricu-",
          "then gradually increase the difﬁculty level of data until\nthe": "spirit of CL to improve the traditional ERC methods. Due"
        },
        {
          "Abstract": "lum (CC); and (2) utterance-level curriculum (UC).\nIn CC,",
          "then gradually increase the difﬁculty level of data until\nthe": "to the hierarchical structure of the ERC datasets, we need to"
        },
        {
          "Abstract": "we construct a difﬁculty measurer based on “emotion shift”",
          "then gradually increase the difﬁculty level of data until\nthe": "construct\nthe curricula from two granularities: one curricu-"
        },
        {
          "Abstract": "frequency within a conversation,\nthen the conversations are",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "lum sorts the conversations in the dataset from easy to hard,"
        },
        {
          "Abstract": "scheduled in an “easy to hard” schema according to the difﬁ-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "and the other sorts the utterances in each conversation from"
        },
        {
          "Abstract": "culty score returned by the difﬁculty measurer. For UC,\nit\nis",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "easy to hard."
        },
        {
          "Abstract": "implemented from an emotion-similarity perspective, which",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "The question arises how to measure the difﬁculty of con-"
        },
        {
          "Abstract": "progressively strengthens\nthe model’s ability in identifying",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "versations and utterances. Previous studies (Majumder et al."
        },
        {
          "Abstract": "the confusing emotions. With the proposed model-agnostic",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "hybrid curriculum learning strategy, we observe signiﬁcant",
          "then gradually increase the difﬁculty level of data until\nthe": "2019; Shen et al. 2021a) have reported that most ERC meth-"
        },
        {
          "Abstract": "performance boosts over a wide range of existing ERC mod-",
          "then gradually increase the difﬁculty level of data until\nthe": "ods mainly suffer from two issues: 1) “emotion shift” prob-"
        },
        {
          "Abstract": "els and we are able to achieve new state-of-the-art results on",
          "then gradually increase the difﬁculty level of data until\nthe": "lem.\nIt\nrefers to that\nthese methods cannot efﬁciently han-"
        },
        {
          "Abstract": "four public ERC datasets.",
          "then gradually increase the difﬁculty level of data until\nthe": "dle scenarios in which emotions of\ntwo consecutive utter-"
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "ances are different\n(Ghosal et al. 2021). 2) “confusing la-"
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "bel” problem. Previous methods (Ghosal et al. 2019; Shen"
        },
        {
          "Abstract": "Introduction",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "et al. 2021b) usually fail to distinguish between similar emo-"
        },
        {
          "Abstract": "Emotion recognition in conversation (ERC) has attracted nu-",
          "then gradually increase the difﬁculty level of data until\nthe": "tions very well. This is due to the subtle semantic difference"
        },
        {
          "Abstract": "merous interests from the NLP community in recent years",
          "then gradually increase the difﬁculty level of data until\nthe": "between certain emotion labels such as happy and exciting."
        },
        {
          "Abstract": "due to its potential applications in many areas, such as opin-",
          "then gradually increase the difﬁculty level of data until\nthe": "These two phenomena provide us the key to quantify the dif-"
        },
        {
          "Abstract": "ion mining in social media (Chatterjee et al. 2019), dialogue",
          "then gradually increase the difﬁculty level of data until\nthe": "ﬁculty of conversations and utterances in ERC."
        },
        {
          "Abstract": "generation (Huang et al. 2018) and fake news detection (Guo",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "In this paper, we tailor-design a hybrid curriculum learn-"
        },
        {
          "Abstract": "et al. 2019). The objective of ERC is\nto detect emotions",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "ing\n(HCL)\nframework\nfor\nthe ERC task. HCL frame-"
        },
        {
          "Abstract": "expressed by the speakers in each utterance of\nthe conver-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "work consists of two complementary curriculum strategies,"
        },
        {
          "Abstract": "sation. Previous works on ERC usually solve this problem",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "conversation-level curriculum (CC) and utterance-level cur-"
        },
        {
          "Abstract": "with two steps. At\nthe ﬁrst step, each utterance is encoded",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "riculum (UC).\nIn CC, we construct a difﬁculty measurer"
        },
        {
          "Abstract": "separately into an utterance-level representation, which will",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "based on “emotion shift” frequency within a conversation,"
        },
        {
          "Abstract": "be used as the input for sequence-based models (Majumder",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "then the conversations with lower difﬁculty are presented to"
        },
        {
          "Abstract": "et al. 2019; Hazarika et al. 2018a; Jiao et al. 2019) or graph-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "the model before harder ones. This way, the model gradually"
        },
        {
          "Abstract": "based models (Ghosal et al. 2019; Ishiwatari et al. 2020) dur-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "increases its ability to tackle the “emotion shift” problem."
        },
        {
          "Abstract": "ing the second step. Despite their success, previous works",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "While in UC, since ERC requires reasoning over multiple"
        },
        {
          "Abstract": "still have a lot of room for improvement (Poria et al. 2019b).",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "utterances in the conversation, we cannot directly schedule"
        },
        {
          "Abstract": "Curriculum learning (CL) (Bengio et al. 2009) is a train-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "the utterances asynchronously in the “easy to hard” scheme."
        },
        {
          "Abstract": "ing strategy which imitates\nthe meaningful\nlearning order",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "As a result, we design an emotion-similarity based curricu-"
        },
        {
          "Abstract": "in human curricula. The core idea of CL is to train the ma-",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "lum (ESC)\nto implement utterance-level curriculum learn-"
        },
        {
          "Abstract": "chine learning model with easier data subsets at ﬁrst, and",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "ing. Speciﬁcally, inspired by the “confusing label” problem"
        },
        {
          "Abstract": "",
          "then gradually increase the difﬁculty level of data until\nthe": "mentioned above, we believe that in a conversation, those ut-"
        },
        {
          "Abstract": "*These authors contributed equally.",
          "then gradually increase the difﬁculty level of data until\nthe": ""
        },
        {
          "Abstract": "Copyright © 2022, Association for the Advancement of Artiﬁcial",
          "then gradually increase the difﬁculty level of data until\nthe": "terances with confusing labels are more difﬁcult than others."
        },
        {
          "Abstract": "Intelligence (www.aaai.org). All rights reserved.",
          "then gradually increase the difﬁculty level of data until\nthe": "Therefore, we make the model focus on the utterances with"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "work leads to signiﬁcant performance improvements.",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "ERC model integrates common-sense knowledge. COSMIC"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "In summary, our main contributions are as follows:",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "(Ghosal et al. 2020) adopts a network structure very close"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "to DialogRNN and adds external commonsense knowledge"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "• We propose a hybrid curriculum learning framework to",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "from ATOMIC (Sap et al. 2019) to improve its performance."
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "tackle the task of ERC. At conversation-level curriculum,",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "TODKAT (Zhu et al. 2021)\nleverages an encoder-decoder"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "we utilize an emotion-shift frequency to measure the dif-",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "architecture which incorporates\ntopic\nrepresentation with"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "ﬁculty of each conversation.",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "commonsense knowledge from ATOMIC for ERC."
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "• We propose emotion-similarity based curriculum learn-",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "ing to achieve utterance-level curriculum learning. It im-",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "Curriculum Learning"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "plements the basic idea that at early stage of training it",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "Starting from the work by Bengio et al.\n(2009), a variety"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "is less important to distinguish between similar emotions",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "of curriculum learning approaches (Wang, Chen, and Zhu"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "compared to separating very different emotions.",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": ""
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "2020; Soviany et al. 2021) has been studied. In the ﬁeld of"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "• We\nconduct\nexperiments\non\nfour\nERC\nbenchmark",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "NLP, curriculum learning has been used for various\ntasks"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "datasets. Empirical\nresults\nshow that our proposed hy-",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "such as neural machine translation (Zhou et al. 2020; Liu"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "brid curriculum learning framework can effectively im-",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "et al. 2020),\nrelation extraction (Huang and Du 2019) and"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "prove the overall performance of various ERC models,",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "natural answer generation (Liu et al. 2018). To the best of"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "including the state-of-the-art.",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "our knowledge, we leverage curriculum learning for the ﬁrst"
        },
        {
          "strate that\nthe proposed hybrid curriculum learning frame-": "",
          "Lowry-Duda 2017) for emotion detection, which is the ﬁrst": "time in the ERC task."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "easily recognizable emotion labels in the early stage,\nthen": "progressively strengthened the model’s capability of identi-",
          "Sequence-based Methods Many previous works\ncon-": "sider contextual\ninformation as utterance sequences. ICON"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "fying the confusing emotions.",
          "Sequence-based Methods Many previous works\ncon-": "(Hazarika et al. 2018a) and CMN (Hazarika et al. 2018b)"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "More\nspeciﬁcally, based on previous\nstudies\n(Plutchik",
          "Sequence-based Methods Many previous works\ncon-": "both utilize gated recurrent unit (GRU) to model\nthe utter-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "1982; Mikels et al. 2005) on psychology, we employ the",
          "Sequence-based Methods Many previous works\ncon-": "ance sequences. DialogueRNN (Majumder et al. 2019) em-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "degree of\nintersection angle between different emotion la-",
          "Sequence-based Methods Many previous works\ncon-": "ploys a GRU to capture the global context which is updated"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "bels in Valence-Arousal 2D emotion space (Guo et al. 2019;",
          "Sequence-based Methods Many previous works\ncon-": "by the speaker state GRUs. Jiao et al. (2019) propose a hi-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "Yang et al. 2021) to measure the similarity between emotion",
          "Sequence-based Methods Many previous works\ncon-": "erarchical neural network model\nthat comprises two GRUs"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "labels.",
          "Sequence-based Methods Many previous works\ncon-": "for the modelling of tokens and utterances respectively. Hu,"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "During ESC,\ninstead of one-hot encoding,\nthe target\nis a",
          "Sequence-based Methods Many previous works\ncon-": "Wei, and Huai (2021) introduce multi-turn reasoning mod-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "probability distribution over all possible emotion labels. The",
          "Sequence-based Methods Many previous works\ncon-": "ules on Bi-directional LSTM to model the ERC task from a"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "probability of each label is determined by the similarity be-",
          "Sequence-based Methods Many previous works\ncon-": "cognitive perspective."
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "tween current\nlabel and the gold label. In other words, be-",
          "Sequence-based Methods Many previous works\ncon-": "Graph-based Methods\nIn this category,\nsome existing"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "sides its true emotion label, each utterance can also belong",
          "Sequence-based Methods Many previous works\ncon-": "works\n(Ghosal et al. 2019;\nIshiwatari et al. 2020; Zhang"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "to similar emotions to a lesser extent.\nIn the beginning of",
          "Sequence-based Methods Many previous works\ncon-": "et al. 2019) utilize various graph neural networks to capture"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "the training process, the targets of utterances with emotions",
          "Sequence-based Methods Many previous works\ncon-": "multiple dependencies in the conversation. DialogXL (Shen"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "happy and excited should almost be the same, but always be",
          "Sequence-based Methods Many previous works\ncon-": "et al. 2021a) modiﬁes the memory block in XLNet\n(Yang"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "very different from sad. Then the label representation grad-",
          "Sequence-based Methods Many previous works\ncon-": "et al. 2019) to store historical context and leverages the self-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "ually shifted to the one-hot encoding during the following",
          "Sequence-based Methods Many previous works\ncon-": "attention mechanism in XLNet\nto deal with the multi-turn"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "training process. This way, small mistakes are corrected less",
          "Sequence-based Methods Many previous works\ncon-": "multi-party structure in conversation. Shen et al. (2021b) de-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "than serious mistakes at the beginning, it could also be con-",
          "Sequence-based Methods Many previous works\ncon-": "sign a directed acyclic graph (DAG) to model\nthe intrinsic"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "sidered as an easy-to-hard curriculum in which broad con-",
          "Sequence-based Methods Many previous works\ncon-": "structure within a conversation, which achieves the state-of-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "cepts are taught before those tiny differences are empha-",
          "Sequence-based Methods Many previous works\ncon-": "the-art performance without considering the introduction of"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "sized.",
          "Sequence-based Methods Many previous works\ncon-": "external knowledge."
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "Our\nhybrid\ncurriculum learning\nframework\nis model-",
          "Sequence-based Methods Many previous works\ncon-": "Knowledge-based Methods KET (Zhong, Wang,\nand"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "agnostic. We evaluate our approach on ﬁve representative",
          "Sequence-based Methods Many previous works\ncon-": "Miao 2019) employs hierarchical transformers with concept"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "ERC models. Results on four benchmark datasets demon-",
          "Sequence-based Methods Many previous works\ncon-": "representations extracted from the ConceptNet\n(Speer and"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "strate that\nthe proposed hybrid curriculum learning frame-",
          "Sequence-based Methods Many previous works\ncon-": "Lowry-Duda 2017) for emotion detection, which is the ﬁrst"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "work leads to signiﬁcant performance improvements.",
          "Sequence-based Methods Many previous works\ncon-": "ERC model integrates common-sense knowledge. COSMIC"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "In summary, our main contributions are as follows:",
          "Sequence-based Methods Many previous works\ncon-": "(Ghosal et al. 2020) adopts a network structure very close"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "to DialogRNN and adds external commonsense knowledge"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "• We propose a hybrid curriculum learning framework to",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "from ATOMIC (Sap et al. 2019) to improve its performance."
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "tackle the task of ERC. At conversation-level curriculum,",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "TODKAT (Zhu et al. 2021)\nleverages an encoder-decoder"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "we utilize an emotion-shift frequency to measure the dif-",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "architecture which incorporates\ntopic\nrepresentation with"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "ﬁculty of each conversation.",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "commonsense knowledge from ATOMIC for ERC."
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "• We propose emotion-similarity based curriculum learn-",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "ing to achieve utterance-level curriculum learning. It im-",
          "Sequence-based Methods Many previous works\ncon-": "Curriculum Learning"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "plements the basic idea that at early stage of training it",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "Starting from the work by Bengio et al.\n(2009), a variety"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "is less important to distinguish between similar emotions",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "of curriculum learning approaches (Wang, Chen, and Zhu"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "compared to separating very different emotions.",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "2020; Soviany et al. 2021) has been studied. In the ﬁeld of"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "• We\nconduct\nexperiments\non\nfour\nERC\nbenchmark",
          "Sequence-based Methods Many previous works\ncon-": "NLP, curriculum learning has been used for various\ntasks"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "datasets. Empirical\nresults\nshow that our proposed hy-",
          "Sequence-based Methods Many previous works\ncon-": "such as neural machine translation (Zhou et al. 2020; Liu"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "brid curriculum learning framework can effectively im-",
          "Sequence-based Methods Many previous works\ncon-": "et al. 2020),\nrelation extraction (Huang and Du 2019) and"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "prove the overall performance of various ERC models,",
          "Sequence-based Methods Many previous works\ncon-": "natural answer generation (Liu et al. 2018). To the best of"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "including the state-of-the-art.",
          "Sequence-based Methods Many previous works\ncon-": "our knowledge, we leverage curriculum learning for the ﬁrst"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "time in the ERC task."
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "Related Work",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "Proposed Framework"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "Emotion Recognition in Conversation",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "",
          "Sequence-based Methods Many previous works\ncon-": "Task Deﬁnition"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "Emotion\nrecognition\nin\nconversations\n(ERC)\nhas\nbeen",
          "Sequence-based Methods Many previous works\ncon-": ""
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "widely studied due to its potential application prospect. The",
          "Sequence-based Methods Many previous works\ncon-": "In ERC, a conversation C contains a sequence of textual ut-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "key point of ERC is how to effectively model the context of",
          "Sequence-based Methods Many previous works\ncon-": "terances {u1, u2..., un}, where n denotes the number of ut-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "each utterance and corresponding speaker. Existing works",
          "Sequence-based Methods Many previous works\ncon-": "terances. Each utterance ui = {wi,1, wi,2..., wi,t(ui)} con-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "generally resort\nto deep learning methods\nto capture con-",
          "Sequence-based Methods Many previous works\ncon-": "sists of t(ui) tokens, where t(ui) is the length of ui. There"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "textual characteristics, which can be divided into sequence-",
          "Sequence-based Methods Many previous works\ncon-": "are m participants P = {p1, p2..., pm}(m ≥ 2) in C. Each"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "based and graph-based methods. Another direction is\nto",
          "Sequence-based Methods Many previous works\ncon-": "is uttered by one participant in P . Given a pre-\nutterance ui"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "improve the performance of existing models by incorpo-",
          "Sequence-based Methods Many previous works\ncon-": "deﬁned emotion label set E = {y1, y2, ..., yr}, the objective"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "rating various external knowledge, which we classiﬁed as",
          "Sequence-based Methods Many previous works\ncon-": "of the ERC task is to predict the emotion label of each utter-"
        },
        {
          "easily recognizable emotion labels in the early stage,\nthen": "knowledge-based methods.",
          "Sequence-based Methods Many previous works\ncon-": "ance in C with the information provided above."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "Overview"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "In curriculum learning, a typical curriculum design consists"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "of\ntwo core components: difﬁculty measurer and training"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "scheduler (Bengio et al. 2009). Difﬁculty Measurer is used"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "to quantify the relative “easiness” of each data example."
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "The training scheduler arranges the sequence of data sub-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "sets throughout\nthe training process based on the judgment"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "from the difﬁculty measurer. For ERC oriented curriculum"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "learning,\nthe challenge is how to design suitable difﬁculty"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "measurer and training scheduler for emotion recognition."
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "A conversation consists of a sequence of utterances. This"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "hierarchical\nstructure inspired us\nto construct\ntwo curric-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "ula for scheduling conversations and utterances respectively."
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "Therefore, our\nframework consists of\ntwo nested curric-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "ula, conversation-level curriculum (CC) on the outside and"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "utterance-level curriculum (UC) on the inside."
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "For CC, we\ndesign\nan\nemotion-shift\nbased\ndifﬁculty"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "baby\nstep\nmeasurer. A widely\nused CL strategy\ncalled"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "(Spitkovsky, Alshawi, and Jurafsky 2010)\nis\nleveraged as"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "training scheduler. For UC, due to the characteristics of the"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "ERC task,\nthe utterances in the same conversation must be"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "input\ninto a batch simultaneously during the training pro-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "cess. As a result,\nit\nis infeasible to employ traditional\ntrain-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "ing scheduler such as baby step to arrange the training or-"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "der of the utterances. We proposed emotion-similarity based"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "curriculum learning to address this issue."
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": ""
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "The proposed HCL framework is illustrated in Figure 1"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "and the details of CC and UC are elaborated in following"
        },
        {
          "Figure 1: The proposed hybrid curriculum learning (HCL) framework for ERC.": "two subsections, respectively."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "of emotions in corresponding datasets."
        },
        {
          "Utterance-level Curriculum": "As it is infeasible to employ a traditional CL training sched-",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "uler to asynchronously arrange the order of the utterances,",
          "other labels is deﬁned as 1/N , where N is the total number": "The process of emotion-similarity based curruclum learn-"
        },
        {
          "Utterance-level Curriculum": "the question arises how to measure the difﬁculty of the utter-",
          "other labels is deﬁned as 1/N , where N is the total number": "ing (ESC) is described as Line 6 - Line 13 in Algorithm 1."
        },
        {
          "Utterance-level Curriculum": "ances and establish a feasible curriculum at utterance-level.",
          "other labels is deﬁned as 1/N , where N is the total number": "We ﬁrst calculate the similarity between each emotion la-"
        },
        {
          "Utterance-level Curriculum": "We address this problem by assuming that\nthe utterances",
          "other labels is deﬁned as 1/N , where N is the total number": "bel pair as Equation 2 and generate the emotion similarity"
        },
        {
          "Utterance-level Curriculum": "with confusing emotion labels are more difﬁcult for predic-",
          "other labels is deﬁned as 1/N , where N is the total number": "matrix Msim, then Msim is normalized as Mtarget. At the"
        },
        {
          "Utterance-level Curriculum": "tion and our utterance-level curriculum is based on the pair-",
          "other labels is deﬁned as 1/N , where N is the total number": "beginning of ESC training, we take the rows of Mtarget as"
        },
        {
          "Utterance-level Curriculum": "wise similarities between the emotion labels.",
          "other labels is deﬁned as 1/N , where N is the total number": "the initial\ntarget probability distributions over all possible"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "classes for\ntraining, and each row corresponds to an emo-"
        },
        {
          "Utterance-level Curriculum": "Arousal",
          "other labels is deﬁned as 1/N , where N is the total number": "tion label. That is, instead of solely belonging to its ground-"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "truth label, each input utterance can also belong to similar"
        },
        {
          "Utterance-level Curriculum": "H\ni\ngh",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "labels to a lesser extent. During the training process, this la-"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "bel representation is gradually shifted towards the standard"
        },
        {
          "Utterance-level Curriculum": "Angry(Mad)\nSurprise",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "one-hot-encoding. We deﬁne the update strategy as in Line"
        },
        {
          "Utterance-level Curriculum": "Excited",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "9 - Line 11, where mi,j denotes the probability of j-th ele-"
        },
        {
          "Utterance-level Curriculum": "Powerful",
          "other labels is deﬁned as 1/N , where N is the total number": "ment of i-th row in Mtarget at training step t. The constant"
        },
        {
          "Utterance-level Curriculum": "Disgust",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "parameter (cid:15) ∈ (0, 1) controls how quickly the label vectors"
        },
        {
          "Utterance-level Curriculum": "Happy",
          "other labels is deﬁned as 1/N , where N is the total number": "converge to the one-hot-encoded labels. Row-wise normal-"
        },
        {
          "Utterance-level Curriculum": "Fear",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "𝜃",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "ization is performed after each update. This update strategy"
        },
        {
          "Utterance-level Curriculum": "Negative\nPositive",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "Valence",
          "other labels is deﬁned as 1/N , where N is the total number": "leads to a proper label-weighting curriculum."
        },
        {
          "Utterance-level Curriculum": "Neutral",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "Frustrated",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "Peaceful",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "z(cid:88) c\nn(cid:88) i\nm(cid:88) k\nL(θ) = −\n[k]\n(3)\nMtarget[yc\n]k log P c"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "ui\nui"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "=1\n=1\n=0"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "For each training step,\nthe predicted probability distri-"
        },
        {
          "Utterance-level Curriculum": "Sad",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "the model\nbution of utterance ui deﬁned as Pui . Finally,"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "is trained with the standard cross-entropy loss function as"
        },
        {
          "Utterance-level Curriculum": "Low",
          "other labels is deﬁned as 1/N , where N is the total number": "Equation 3, where P c\n[k] denotes the predicted probability"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "ui"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "that\nthe label of ui\nin conversation c is k. Mtarget[yc\n]k"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "ui"
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "denotes\nthe target probability of\nlabel k in current\nlabel-"
        },
        {
          "Utterance-level Curriculum": "Figure 2: The 2D arousal-valence emotion wheel proposed",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "similarity matrix at training step t. z is total number of con-"
        },
        {
          "Utterance-level Curriculum": "by us. Each emotion label\nis corresponding to a point on",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "versations in training set, n is the utterance number of con-"
        },
        {
          "Utterance-level Curriculum": "the unit circle. This wheel has integrated the versions from",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        },
        {
          "Utterance-level Curriculum": "",
          "other labels is deﬁned as 1/N , where N is the total number": "versation c. In this way, we implement UC through ESC."
        },
        {
          "Utterance-level Curriculum": "(Jing, Mao, and Chen 2019; Yang et al. 2021; Toisoul et al.",
          "other labels is deﬁned as 1/N , where N is the total number": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "All of\nthe baseline models mentioned above have released"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "their\nsource codes. We keep exactly the same settings as"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "reported in the original papers during our experiments. For"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "HCL, the tunable hyperparameters include number of buck-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "ets in CC, max training epochs during each baby step,\nin-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "terval steps for training target updating in UC, decay factor"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "in UC. These hyperparameters are manually tuned on each"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "dataset with hold-out validation. The results reported in our"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "experiments are all based on the average score of 5 random"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "runs on the test set. Our experiments are conducted on a sin-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "gle Tesla V100M32 GPU."
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "Results and Analysis"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "Overall Results"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "The overall experimental\nresults are reported in Table 2,"
        },
        {
          "Implementation Details": "where “X+HCL” means training the model X with the pro-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "posed HCL framework. We can see that HCL has improved"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "the performance of all baseline models, showing the robust-"
        },
        {
          "Implementation Details": "ness and universality of our approach."
        },
        {
          "Implementation Details": "In general,\nthe performance boosts achieved by HCL on"
        },
        {
          "Implementation Details": "models with simpler\nfeature extractor\n(i.e., DialogueRNN"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "and DialogueGCN) are more remarkable. An exception is"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "that TODKAT+HCL achieves signiﬁcant\nimprovements on"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "three datasets. The reason may be that the original TODKAT"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "model does not\ntake account of\nthe speaker\ninformation,"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "while our CC has introduced the inter-speaker emotion-shift"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "in the difﬁculty measurer, which is equivalent to considering"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "speaker information to a certain extent and is beneﬁcial for"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "TODKAT."
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "Ablation Study"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "To reveal the individual effects of CC and UC, we try differ-"
        },
        {
          "Implementation Details": "ent variants of HCL on TODKAT by removing either CC or"
        },
        {
          "Implementation Details": "UC. The experimental results on IEMOCAP and EmoryNLP"
        },
        {
          "Implementation Details": "are shown in Table 3, from which we see that both CC and"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "UC make positive contributions to the overall performance"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "when used alone. Although only utilizing UC leads to larger"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "improvements than only using CC, the optimal performance"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "is achieved when CC and UC are combined,\nindicating that"
        },
        {
          "Implementation Details": "CC and UC are complementary to each other."
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "In addition, we also tried another two strategies to com-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "bine CC and UC: CC-First (CCF) and UC-First (UCF). CCF"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "performs CC and UC in a pipeline manner. In UCF,\nthe ex-"
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "ecute order of CC and UC is reversed. The results of CCF"
        },
        {
          "Implementation Details": "and UCF are also outlined in Table 3. It shows that UCF is"
        },
        {
          "Implementation Details": "better than CCF and HCL outperforms both CCF and UCF."
        },
        {
          "Implementation Details": "This is intuitive, because HCL makes UC and CC interact"
        },
        {
          "Implementation Details": "with each other during the training process, which is more"
        },
        {
          "Implementation Details": "consist with the hierarchical structure of conversation, so the"
        },
        {
          "Implementation Details": "performance is even better than UCF."
        },
        {
          "Implementation Details": ""
        },
        {
          "Implementation Details": "Performance for Emotion-shift"
        },
        {
          "Implementation Details": "To verify the effect of HCL in the emotion-shift scenario, we"
        },
        {
          "Implementation Details": "summarize the results of TODKAT+HCL on different types"
        },
        {
          "Implementation Details": "of utterances. The results are presented in Table 4, where"
        },
        {
          "Implementation Details": "ES and N-ES denote utterances with emotion-shift and ut-"
        },
        {
          "Implementation Details": "terances without emotion-shift, respectively. HCL improves"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conversations": "",
          "Utterances": ""
        },
        {
          "Conversations": "Val",
          "Utterances": "Val"
        },
        {
          "Conversations": "",
          "Utterances": ""
        },
        {
          "Conversations": "114",
          "Utterances": "1109"
        },
        {
          "Conversations": "99",
          "Utterances": "1344"
        },
        {
          "Conversations": "1000",
          "Utterances": "8069"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DailyDialog\n11118": "",
          "1000\n1000\n87170": "Table 1: The statistics of datasets. avg utt denotes the average number of utterances.",
          "8069\n7740": "",
          "7": "",
          "7.85\nMicro-F1": ""
        },
        {
          "DailyDialog\n11118": "METHOD",
          "1000\n1000\n87170": "IEMOCAP",
          "8069\n7740": "MELD",
          "7": "DailyDialog",
          "7.85\nMicro-F1": "EmoryNLP"
        },
        {
          "DailyDialog\n11118": "DialogueRNN",
          "1000\n1000\n87170": "62.75",
          "8069\n7740": "57.03",
          "7": "-",
          "7.85\nMicro-F1": "-"
        },
        {
          "DailyDialog\n11118": "DialogueGCN",
          "1000\n1000\n87170": "64.18",
          "8069\n7740": "58.10",
          "7": "-",
          "7.85\nMicro-F1": "-"
        },
        {
          "DailyDialog\n11118": "COSMIC",
          "1000\n1000\n87170": "65.28",
          "8069\n7740": "65.21",
          "7": "58.48",
          "7.85\nMicro-F1": "38.11"
        },
        {
          "DailyDialog\n11118": "DAG-ERC",
          "1000\n1000\n87170": "68.03",
          "8069\n7740": "63.65",
          "7": "59.33",
          "7.85\nMicro-F1": "39.02"
        },
        {
          "DailyDialog\n11118": "TODKAT",
          "1000\n1000\n87170": "61.33",
          "8069\n7740": "65.47",
          "7": "58.47",
          "7.85\nMicro-F1": "43.12"
        },
        {
          "DailyDialog\n11118": "DialogueRNN+HCL",
          "1000\n1000\n87170": "64.62 (↑ 1.87)",
          "8069\n7740": "58.30 (↑ 1.27)",
          "7": "-",
          "7.85\nMicro-F1": "-"
        },
        {
          "DailyDialog\n11118": "DialogueGCN+HCL",
          "1000\n1000\n87170": "65.41 (↑ 1.23)",
          "8069\n7740": "59.31 (↑ 1.21)",
          "7": "-",
          "7.85\nMicro-F1": "-"
        },
        {
          "DailyDialog\n11118": "COSMIC+HCL",
          "1000\n1000\n87170": "66.23(↑ 0.95)",
          "8069\n7740": "65.85(↑ 0.64)",
          "7": "59.54(↑ 1.06)",
          "7.85\nMicro-F1": "38.96(↑ 0.85)"
        },
        {
          "DailyDialog\n11118": "DAG-ERC+HCL",
          "1000\n1000\n87170": "68.73(↑ 0.70)",
          "8069\n7740": "63.89(↑ 0.24)",
          "7": "59.64(↑ 0.31)",
          "7.85\nMicro-F1": "39.82(↑ 0.80)"
        },
        {
          "DailyDialog\n11118": "TODKAT+HCL",
          "1000\n1000\n87170": "63.03(↑ 1.70)",
          "8069\n7740": "66.18(↑ 0.71)",
          "7": "59.76(↑ 1.29)",
          "7.85\nMicro-F1": "46.11(↑ 2.99)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "METHOD\nIEMOCAP\nEmoryNLP"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT\n61.33\n43.12"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT+CC\n61.83(↑ 0.50)\n44.20(↑ 1.08)"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT+UC\n62.01(↑ 0.68)\n45.19(↑ 2.07)"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT+CCF\n62.07(↑ 0.74)\n45.06(↑ 1.94)"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT+UCF\n62.76(↑ 1.43)\n45.47(↑ 2.35)"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "63.03(↑ 1.70)\n46.11(↑ 2.99)\nTODKAT+HCL"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "Table 3: Ablation study on TODKAT"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "IEMOCAP\nEmoryNLP"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "METHOD"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "ES\nN-ES\nES\nN-ES"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "(41.2%)\n(58.8%)\n(69.2%)\n(30.8%)"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "TODKAT\n56.24\n64.62\n39.36\n51.51"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "56.91\n67.01\n42.40\n54.02\nTODKAT+HCL"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "Table 4: The performance of TODKAT+HCL on utterances"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "which exhibits emotion-shift. Numbers in parenthesis indi-"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "cate the percentage in the test dataset."
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "the performance of TODKAT on both ES and N-ES of\nthe"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "two datasets. The improvement on ES in EmoryNLP is more"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "signiﬁcant than on ES in IEMOCAP."
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": ""
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "A plausible explanation is that\nthe training set of IEMO-"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "CAP contains much\nless\nconversations\nand\nthe\naverage"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "length of\nconversations\nis much longer,\nso the difﬁculty"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "scores of\nconversations\nin IEMOCAP are usually lower."
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "Therefore,\nfor\nIEMOCAP,\nthe difﬁculty discrimination be-"
        },
        {
          "Table 2: The overall results on different methods on four datasets. The results of baseline methods are from the original papers.": "tween different buckets in the training scheduler\nis not as"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "tiﬁed the emotion as the correct label excited.",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "In this paper, we propose simple but effective hybrid cur-"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "riculum learning (HCL)\nfor\nemotion recognition in con-"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "versations. HCL is\na ﬂexible\nframework independent of"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "Why Curriculum Learning Works?",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "the original\ntraining models. During training, HCL simul-"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "taneously\nemploys\nconversation-level\nand\nutterance-level"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "curricula\nto\nexecute\nthe\ntraining\nprocess\nas\nan\neasy\nto"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "According to the\ntheory of\ncurriculum learning (Bengio",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "hard schema. Conversation-level curriculum consists of an"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "et al. 2009),\nthe curriculum will work only if\nthe entropy",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "emotion-shﬁt\nbased\ndifﬁculty measurer\nand\na\nbaby\nstep"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "of data distributions increases during the training process. In",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "scheduler. Utterance-level\ncurriculum is\nimplemented\nas"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "HCL, conversation-level curriculum leverages the emotion-",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "emotion-similarity based CL. Experiments on four bench-"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "shift frequency to measure the difﬁculty. The more frequent",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "mark datasets have proved the generality and effectiveness"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "the emotion-shift occurs in a conversation,\nthe greater\nthe",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "",
          "Conclusions": "of HCL."
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "diversity of\nthe emotion labels,\nin other words,\nthe higher",
          "Conclusions": ""
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "the entropy. For utterance-level curriculum, since emotion-",
          "Conclusions": "In the future, we plan to improve our method in three di-"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "similarity based CL does not distinguish similar emotion in",
          "Conclusions": "rections. First, we will attempt to seek other suitable features"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "the early stage, it is equivalent to merging some emotion la-",
          "Conclusions": "to construct difﬁculty measurer for ERC. Second, we aim to"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "bels and could be considered as\nreducing the diversity of",
          "Conclusions": "introduce other training schedulers for CL to further improve"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "emotions. As a result,\nit also meets the condition which the",
          "Conclusions": "the performance. Finally, we aim to apply a learning-based"
        },
        {
          "alogueGCN+HCL and DAG-ERC+HCL successfully iden-": "entropy should be increased gradually.",
          "Conclusions": "approach to model the similarity between emotion labels."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Learning for Natural Answer Generation.\nIn IJCAI, 4223–"
        },
        {
          "References": "Bengio, Y.; Louradour,\nJ.; Collobert, R.;\nand Weston,\nJ.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "4229."
        },
        {
          "References": "2009. Curriculum learning.\nIn ICML, 41–48.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Liu, X.; Lai, H.; Wong, D. F.; and Chao, L. S. 2020. Norm-"
        },
        {
          "References": "Busso, C.; Bulut, M.; Lee, C.-C.; Kazemzadeh, A.; Mower,",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Based Curriculum Learning for Neural Machine Transla-"
        },
        {
          "References": "E.; Kim, S.; Chang,\nJ. N.; Lee, S.; and Narayanan, S. S.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "tion.\nIn ACL, 427–436."
        },
        {
          "References": "2008. IEMOCAP: Interactive emotional dyadic motion cap-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "ture database.\nLanguage resources and evaluation, 42(4):",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Majumder, N.; Poria, S.; Hazarika, D.; Mihalcea, R.; Gel-"
        },
        {
          "References": "335–359.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "bukh, A.; and Cambria, E. 2019. Dialoguernn: An attentive"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "rnn for emotion detection in conversations.\nIn AAAI, 6818–"
        },
        {
          "References": "Chatterjee, A.; Narahari, K. N.; Joshi, M.; and Agrawal, P.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "6825."
        },
        {
          "References": "2019. Semeval-2019 task 3: Emocontext contextual emotion",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "detection in text.\nIn SemEval2019, 39–48.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Mikels, J. A.; Fredrickson, B. L.; Larkin, G. R.; Lindberg,"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "C. M.; Maglio, S. J.; and Reuter-Lorenz, P. A. 2005. Emo-"
        },
        {
          "References": "Ghosal, D.; Majumder, N.; Gelbukh, A.; Mihalcea, R.; and",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "tional category data on images from the International Af-"
        },
        {
          "References": "Poria, S. 2020.\nCOSMIC: COmmonSense knowledge for",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "fective Picture System. Behavior research methods, 37(4):"
        },
        {
          "References": "eMotion Identiﬁcation in Conversations.\nIn EMNLP Find-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "626–630."
        },
        {
          "References": "ings, 2470–2481.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Plutchik, R. 1982.\nA psychoevolutionary theory of emo-"
        },
        {
          "References": "Ghosal, D.; Majumder, N.; Mihalcea, R.; and Poria, S. 2021.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "tions. Social sciences information, 21(4-5): 529–553."
        },
        {
          "References": "Exploring the Role of Context\nin Utterance-level Emotion,",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "Act and Intent Classiﬁcation in Conversations: An Empirical",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria,"
        },
        {
          "References": "Study.\nIn ACL Findings, 1435–1449.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "E.; and Mihalcea, R. 2019a. MELD: A Multimodal Multi-"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Party Dataset for Emotion Recognition in Conversations.\nIn"
        },
        {
          "References": "Ghosal, D.; Majumder, N.; Poria, S.; Chhaya, N.; and Gel-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "ACL, 527–536."
        },
        {
          "References": "bukh, A. 2019. DialogueGCN: A Graph Convolutional Neu-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "ral Network for Emotion Recognition in Conversation.\nIn",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Poria, S.; Majumder, N.; Mihalcea, R.; and Hovy, E. 2019b."
        },
        {
          "References": "EMNLP, 154–164.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Emotion recognition in conversation: Research challenges,"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "datasets, and recent advances.\nIEEE Access, 7: 100943–"
        },
        {
          "References": "Guo, C.; Cao,\nJ.; Zhang, X.; Shu, K.; and Liu, H. 2019.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "100953."
        },
        {
          "References": "Dean: Learning dual emotion for fake news detection on so-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "cial media. arXiv preprint arXiv:1903.01728.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Russell, J. A. 1980. A circumplex model of affect. Journal"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "of personality and social psychology, 39(6): 1161."
        },
        {
          "References": "Hazarika, D.; Poria, S.; Mihalcea, R.; Cambria, E.; and Zim-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "mermann, R. 2018a. ICON: interactive conversational mem-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Sap, M.; Le Bras, R.; Allaway, E.; Bhagavatula, C.; Lourie,"
        },
        {
          "References": "ory network for multimodal emotion detection.\nIn EMNLP,",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "N.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y. 2019."
        },
        {
          "References": "2594–2604.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Atomic: An atlas of machine commonsense for if-then rea-"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "soning.\nIn AAAI, 3027–3035."
        },
        {
          "References": "Hazarika, D.; Poria, S.; Zadeh, A.; Cambria, E.; Morency,",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "L.-P.; and Zimmermann, R. 2018b. Conversational memory",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Shen, W.; Chen, J.; Quan, X.; and Xie, Z. 2021a. DialogXL:"
        },
        {
          "References": "network for emotion recognition in dyadic dialogue videos.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "All-in-One XLNet\nfor Multi-Party Conversation Emotion"
        },
        {
          "References": "In NAACL, 2122–2132.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Recognition.\nIn AAAI, 13789–13797."
        },
        {
          "References": "Hu, D.; Wei, L.; and Huai, X. 2021. DialogueCRN: Contex-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Shen, W.; Wu, S.; Yang, Y.; and Quan, X. 2021b. Directed"
        },
        {
          "References": "tual Reasoning Networks for Emotion Recognition in Con-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Acyclic Graph Network for Conversational Emotion Recog-"
        },
        {
          "References": "versations.\nIn ACL, 7042–7052.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "nition.\nIn ACL, 1551–1561."
        },
        {
          "References": "Huang, C.; Zaiane, O. R.; Trabelsi, A.; and Dziri, N. 2018.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Soviany,\nP.;\nIonescu,\nR.\nT.;\nRota,\nP.;\nand\nSebe, N."
        },
        {
          "References": "Automatic dialogue generation with expressed emotions.\nIn",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "arXiv\npreprint\n2021.\nCurriculum learning: A survey."
        },
        {
          "References": "NAACL, 49–54.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "arXiv:2101.10382."
        },
        {
          "References": "Huang, Y.; and Du, J. 2019. Self-attention enhanced CNNs",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Speer, R.;\nand Lowry-Duda,\nJ.\n2017.\nConceptNet\nat"
        },
        {
          "References": "and collaborative curriculum learning for distantly super-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "SemEval-2017 Task 2: Extending Word Embeddings with"
        },
        {
          "References": "vised relation extraction.\nIn EMNLP, 389–398.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Multilingual Relational Knowledge.\nIn SemEval2017, 85–"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "89."
        },
        {
          "References": "Ishiwatari, T.; Yasuda, Y.; Miyazaki, T.; and Goto, J. 2020.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "Relation-aware Graph Attention Networks with Relational",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Spitkovsky, V. I.; Alshawi, H.; and Jurafsky, D. 2010. From"
        },
        {
          "References": "Position Encodings for Emotion Recognition in Conversa-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "baby steps to leapfrog: How “less is more” in unsupervised"
        },
        {
          "References": "tions.\nIn EMNLP, 7360–7370.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "dependency parsing.\nIn NAACL, 751–759."
        },
        {
          "References": "Jiao, W.; Yang, H.; King,\nI.; and Lyu, M. R. 2019.\nHi-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Su, Y.; Cai, D.; Zhou, Q.; Lin, Z.; Baker, S.; Cao, Y.; Shi, S.;"
        },
        {
          "References": "GRU: Hierarchical Gated Recurrent Units\nfor Utterance-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Collier, N.; and Wang, Y. 2021. Dialogue response selection"
        },
        {
          "References": "Level Emotion Recognition.\nIn NAACL, 397–406.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "with hierarchical curriculum learning.\nIn ACL, 1740–1751."
        },
        {
          "References": "Jing, S.; Mao, X.; and Chen, L. 2019.\nAutomatic speech",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Toisoul, A.; Kossaiﬁ, J.; Bulat, A.; Tzimiropoulos, G.; and"
        },
        {
          "References": "discrete labels to dimensional emotional values conversion",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Pantic, M. 2021.\nEstimation of\ncontinuous valence\nand"
        },
        {
          "References": "method.\nIET Biometrics, 8(2): 168–176.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "arousal\nlevels from faces in naturalistic conditions. Nature"
        },
        {
          "References": "",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Machine Intelligence, 3(1): 42–50."
        },
        {
          "References": "Li, Y.; Su, H.; Shen, X.; Li, W.; Cao, Z.;\nand Niu, S.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": ""
        },
        {
          "References": "2017. DailyDialog: A Manually Labelled Multi-turn Dia-",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "Wang, X.; Chen, Y.; and Zhu, W. 2020. A Survey on Cur-"
        },
        {
          "References": "logue Dataset.\nIn IJCNLP, 986–995.",
          "Liu, C.; He, S.; Liu, K.; Zhao, J.; et al. 2018. Curriculum": "riculum Learning. arXiv preprint arXiv:2010.13166."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Circular-Structured Representation for Visual Emotion Dis-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "tribution Learning.\nIn CVPR, 4237–4246."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Yang, Z.; Dai, Z.; Yang, Y.; Carbonell,\nJ.; Salakhutdinov,"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "R.;\nand Le, Q. V. 2019.\nXlnet: Generalized autoregres-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "sive pretraining for language understanding. arXiv preprint"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "arXiv:1906.08237."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Zahiri, S. M.; and Choi, J. D. 2018. Emotion Detection on"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "TV Show Transcripts with Sequence-Based Convolutional"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Neural Networks.\nIn AAAI Workshops."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Zhang, D.; Wu, L.; Sun, C.; Li, S.; Zhu, Q.; and Zhou, G."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "2019. Modeling both Context-and Speaker-Sensitive De-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "pendence for Emotion Detection in Multi-speaker Conver-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "sations.\nIn IJCAI, 5415–5421."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Zhong, P.; Wang, D.;\nand Miao, C. 2019.\nKnowledge-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Enriched Transformer\nfor Emotion Detection\nin Textual"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Conversations.\nIn EMNLP, 165–176."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Zhou, Y.; Yang, B.; Wong, D. F.; Wan, Y.; and Chao, L. S."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "2020. Uncertainty-aware curriculum learning for neural ma-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "chine translation.\nIn ACL, 6934–6944."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Zhu, L.; Pergola, G.; Gui, L.; Zhou, D.; and He, Y. 2021."
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "Topic-Driven and Knowledge-Aware Transformer\nfor Dia-"
        },
        {
          "Yang,\nJ.; Li,\nJ.; Li, L.; Wang, X.; and Gao, X. 2021.\nA": "logue Emotion Detection.\nIn ACL, 1571–1582."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Curriculum learning",
      "authors": [
        "Y Bengio",
        "J Louradour",
        "R Collobert",
        "J Weston"
      ],
      "year": "2009",
      "venue": "ICML"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Semeval-2019 task 3: Emocontext contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi",
        "P Agrawal"
      ],
      "year": "2019",
      "venue": "Semeval-2019 task 3: Emocontext contextual emotion detection in text"
    },
    {
      "citation_id": "4",
      "title": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "EMNLP Findings"
    },
    {
      "citation_id": "5",
      "title": "Exploring the Role of Context in Utterance-level Emotion, Act and Intent Classification in Conversations: An Empirical Study",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2021",
      "venue": "ACL Findings"
    },
    {
      "citation_id": "6",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "7",
      "title": "Dean: Learning dual emotion for fake news detection on social media",
      "authors": [
        "C Guo",
        "J Cao",
        "X Zhang",
        "K Shu",
        "H Liu"
      ],
      "year": "2019",
      "venue": "Dean: Learning dual emotion for fake news detection on social media",
      "arxiv": "arXiv:1903.01728"
    },
    {
      "citation_id": "8",
      "title": "ICON: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "9",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "10",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "11",
      "title": "Automatic dialogue generation with expressed emotions",
      "authors": [
        "C Huang",
        "O Zaiane",
        "A Trabelsi",
        "N Dziri"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "12",
      "title": "Self-attention enhanced CNNs and collaborative curriculum learning for distantly supervised relation extraction",
      "authors": [
        "Y Huang",
        "J Du"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "14",
      "title": "Hi-GRU: Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "15",
      "title": "Automatic speech discrete labels to dimensional emotional values conversion method",
      "authors": [
        "S Jing",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "IET Biometrics"
    },
    {
      "citation_id": "16",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset"
    },
    {
      "citation_id": "17",
      "title": "Curriculum Learning for Natural Answer Generation",
      "authors": [
        "C Liu",
        "S He",
        "K Liu",
        "J Zhao"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "18",
      "title": "Norm-Based Curriculum Learning for Neural Machine Translation",
      "authors": [
        "X Liu",
        "H Lai",
        "D Wong",
        "L Chao"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "19",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "20",
      "title": "Emotional category data on images from the International Affective Picture System",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "21",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1982",
      "venue": "Social sciences information"
    },
    {
      "citation_id": "22",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "25",
      "title": "Atomic: An atlas of machine commonsense for if-then reasoning",
      "authors": [
        "M Sap",
        "R Le Bras",
        "E Allaway",
        "C Bhagavatula",
        "N Lourie",
        "H Rashkin",
        "B Roof",
        "N Smith",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "26",
      "title": "2021a. DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "venue": "AAAI"
    },
    {
      "citation_id": "27",
      "title": "2021b. Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "venue": "ACL"
    },
    {
      "citation_id": "28",
      "title": "Curriculum learning: A survey",
      "authors": [
        "P Soviany",
        "R Ionescu",
        "P Rota",
        "N Sebe"
      ],
      "year": "2021",
      "venue": "Curriculum learning: A survey",
      "arxiv": "arXiv:2101.10382"
    },
    {
      "citation_id": "29",
      "title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with Multilingual Relational Knowledge",
      "authors": [
        "R Speer",
        "J Lowry-Duda",
        "V Spitkovsky",
        "H Alshawi",
        "D Jurafsky"
      ],
      "year": "2010",
      "venue": "NAACL"
    },
    {
      "citation_id": "30",
      "title": "Dialogue response selection with hierarchical curriculum learning",
      "authors": [
        "Y Su",
        "D Cai",
        "Q Zhou",
        "Z Lin",
        "S Baker",
        "Y Cao",
        "S Shi",
        "N Collier",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "31",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "A Survey on Curriculum Learning",
      "authors": [
        "X Wang",
        "Y Chen",
        "W Zhu"
      ],
      "year": "2020",
      "venue": "A Survey on Curriculum Learning",
      "arxiv": "arXiv:2010.13166"
    },
    {
      "citation_id": "33",
      "title": "A Circular-Structured Representation for Visual Emotion Distribution Learning",
      "authors": [
        "J Yang",
        "J Li",
        "L Li",
        "X Wang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "34",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "arxiv": "arXiv:1906.08237"
    },
    {
      "citation_id": "35",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-Based Convolutional Neural Networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "36",
      "title": "Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "37",
      "title": "Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "38",
      "title": "Uncertainty-aware curriculum learning for neural machine translation",
      "authors": [
        "Y Zhou",
        "B Yang",
        "D Wong",
        "Y Wan",
        "L Chao"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "39",
      "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
      "authors": [
        "L Zhu",
        "G Pergola",
        "L Gui",
        "D Zhou",
        "Y He"
      ],
      "year": "2021",
      "venue": "ACL"
    }
  ]
}