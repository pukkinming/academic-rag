{
  "paper_id": "2306.08733v1",
  "title": "Continuous Learning Based Novelty Aware Emotion Recognition System",
  "published": "2023-06-14T20:34:07Z",
  "authors": [
    "Mijanur Palash",
    "Bharat Bhargava"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current works in human emotion recognition follow the traditional closed learning approach governed by rigid rules without any consideration of novelty. Classification models are trained on some collected datasets and expected to have the same data distribution in the real-world deployment. Due to the fluid and constantly changing nature of the world we live in, it is possible to have unexpected and novel sample distribution which can lead the model to fail. Hence, in this work, we propose a continuous learning based approach to deal with novelty in the automatic emotion recognition task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding emotions properly is an important requirement for meaningful human communication. In addition, emotional state affects things like how a person drives a car, how a kid learns in the classroom or how police interact with a criminal on the road etc. Automatic emotion recognition has widespread usage in different areas such as human-computer interactions, law enforcement and surveillance, interactive gaming, consumer behaviour analysis, customer service, education, and health care etc.\n\nResearchers in psychology categorized basic human emotions into anger, happiness, sadness, disgust, fear, contempt and surprise  (Patel et al. 2020)  classes. Human expresses these emotions using various verbal and non-verbal cues. Facial expression analysis is one of the major ways to identify the underlying emotion. Apart from that, various important conclusions for emotion can be drawn from posture and gait. Similarly, speech, writing, brain scan, EEG signal etc. also convey useful emotional information.\n\nThe advancement of machine learning and deep learning enables researchers to design more accurate classifiers for automatic emotion recognition systems. Researchers nowadays frequently use various ML and DL techniques to detect and classify emotions. Popular models include variations of support vector machine (SVM), deep belief network (DBN), ensemble learning, deep neural network (DNN) and convolutional neural network (CNN) etc. These models are trained on various datasets collected from web scrapping or created using volunteers and involve somebody to manually label the data samples. More details about some of the datasets are discussed in section 4.\n\nHowever, this closed learning approach faces challenges in real-world situations where an unexpected sample may appear at any moment. A perfect machine learning model requires a perfect training dataset that truly represents all possible real-world situations. However, creating an allinclusive dataset like this is very hard. Moreover, an emotion detection model trained on acted datasets where the margin between true and false samples are wide and clear may have a hard time correctly identifying emotion in a situation where someone is intentionally obfuscating his emotional state and hence the margin is thin and blurry. Another type of challenge arises when the model encounters a different type of emotion class absent in the training dataset as most of the datasets do not cover all of the emotion classes. In this work we define these types of situations as a novelty.\n\nHowever, there is a lack of proper focus on handling novelty in current emotion recognition research. Researchers are more interested in coming up with newer models and newer datasets to get higher accuracy on the test data. However, we also need to look for answers to the questions such as how to detect a novelty, how to characterize it and how to adapt the systems to handle the novelties etc.\n\nThe main contributions of this work are: 1. We present the novelty aspects of the automatic emotion recognition task. 2. We present a system that addresses novelty in a continuous long learning manner for emotion recognition. 3. We present an emotion recognition classifier that detects emotion from facial image 4. We present a novelty detector that works in parallel with the classifier to detect novelty 5. We present experimental results and discuss potential next steps for this work.\n\n3 Background and Related Work   Jadhav et. al. (Jadhav and Ghadekar 2018 ) used a convolutional neural network (CNN) to detect emotion from the facial expressions on the FER-2013 dataset. They achieved only 63% accuracy on the test dataset. This lower accuracy can be attributed to the simplicity of their model.  Gan et. al. (Gan, Chen, and Xu 2019)  achieved improved accuracy on the FER-2013 dataset using ensemble CNN and a novel label perturbation strategy. Due to the enhanced discrimination ability of the ensemble method, they received higher accuracy of 73.7%.\n\nDhankar et. al.  (Dhankhar 2019 ) used the ensemble method and transfer learning with VGG16 and RESENT-50 to overcome the limitations of the basic CNN method. However, their work did not achieve the best result and only scored 67% accuracy on the FER-2013. This may be attributed to a sub-optimal hyperparameter tuning.\n\nBesides CNN, the support vector machine (SVM) is also popular in facial emotion recognition due to its lightweight architecture compared to CNN. Support Vector Machine (SVM) is a type of machine learning model which tries to identify the maximum margin plane between the classes.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "In literature, several datasets have been proposed for emotion recognition. A summary of notable datasets is presented in table  1 . In this section, we discuss our proposed NAERS which stands for Novelty Aware Emotion Recognition System.\n\nFigure  1  shows a high-level diagram of different parts of this system.\n\nFigure  1 : Proposed NAERS system architecture",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Input",
      "text": "The input of this system is the samples of RGB frames with human subjects. This can be a static photo or a video frame.\n\nLive video can be also used for continuous recognition. In that case, we sample video frames in a suitable frequency such as 10 FPS (frames per second).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition (Er) Model",
      "text": "Face and Background Separation We use the facial image as our main source of the emotional information. There- weak learners. The model variations come from the variation of the initial weights. We used the average of the outputs from the model as the final output\n\nWe do not use the classification output provided by these deep feature generation models. Instead, we use the representation of the input samples at the output of the last layer before the linear layers as deep features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Novelty Detector",
      "text": "We propose a new algorithm for the novelty detector to detect novel and unexpected data samples. It relies on available multimodality of the data samples:\n\nAdditional modality As we discussed in section 2, human emotion can be identified from not only their face but also from posture and gesture extracted from the same input sample. For example, a confident person would appear relaxed while an afraid person would appear tensed and shrunk. For the input sample, we also analyze posture. If both facial and posture network output does not concur we tag that sample as a potential novelty. For posture based detection, we remove the facial area from the input. We use a kinematic human body model to represent the body posture and identify key body joints on the frame using the Blazepose tool. By following a similar idea we used in section 5.2, we create a visible feature set from different joint angles and distances. We use three CNN models (section 5.2) to generate deep features. The final feature set is created by concatenating visible and deep feature sets. The classifier is also a similar 3 layer deep neural network. For the brevity of the manuscript, we do not provide the details of the posture network here.\n\nContext Context is important for novelty detection. A tiger in a forest is a normal sample but the same tiger in a busy city street indicates something is wrong. We create context from the background of the image. At first human body and face is removed from the input frame.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Retraining",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we discuss some of the experiments we have performed so far.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Of Emotion Recognition Model",
      "text": "We got 68%, 71.5% and 75.8% test accuracies for the three versions of our model with different CNN models on FER-2013 dataset. In addition, we compared our results with similar works on this dataset as shown in table 3. Our approach with the ensemble of CNN outperforms the other two models and other existing works. We attribute this improvement to the variance reduction achieved by the ensemble operation. For the rest of this manuscript, we only consider ensemble CNN while talking about our emotion recognition model.\n\nFigure  4  shows the confusion matrix for our emotion recognition model for the FER-2013 dataset. Here actual emotion labels are plotted along the vertical while predicted labels are along the horizontal axis. An entry 'C ij ' of row 'i' and column 'j' represents the fraction of total samples having the true label of row 'i', and predicted label of column The diagonal values are the accuracy of the class and a higher value is represented by a darker colour. We notice the happiness class has higher accuracy implying it is easier to detect. For fear, the accuracy is lower and that's because the system confuses fear with sadness. These two are closely related emotions that may appear together and people may actively try to conceal their fear with other similar emotion classes.\n\nThe results of the facial emotion recognition model on some other datasets are shown in table 4. Our model does well while comparing other works in FER-2013, AffectNet and CAER-S datasets. For the FABO dataset, we outperformed the accuracy of 76.4% reported by the authors of the dataset (?) by a wide margin. For the CK+ dataset, we found the best-reported result to be 98.57% accuracy (Kurup, Ajith, and Ramón 2019). Our accuracy of 98.5% is close to it.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Novelty Detection Experiment",
      "text": "As mentioned earlier in section 5.3, for novelty detection we used posture as an additional modality and check the mismatch between facial based and posture based recognition.\n\nFor training the posture emotion recognition model, We used several posture emotion recognition datasets such as FABO, Emotic and CAER-S.\n\nIn the FABO dataset, our posture model achieved 83.1% accuracy. Results from several other recent posture based work on FABO dataset are shown in table 5. We notice from the table that our model does a good job extracting the underlying emotion from the posture. From this table, we can see there is a mismatch between face and posture emotion recognition accuracies which means not all samples are get-ting similar output labels from both models. Further analysis of the results shows that there is around 12.8% sample that gets different class label output from face and posture recognition. Using our re-weighting and retraining algorithm we could reduce the mismatch close to 5.8%. Further reduction is hard to achieve and we attribute this fact to the idea that the face conveys more information than the posture.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a NAERS-a novelty aware emotion recognition system that deals with novelty in a continuous learning fashion. We showed some of the early experiments with promising results. Some of the components on the system are yet to be completed and we are focusing on those as the next step of this work.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Acknowledgement",
      "text": "This research is supported, in part, by the Defense Advanced Research Projects Agency (DARPA) and 3the Air Force Research Laboratory (AFRL) under the contract number W911NF2020003. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL, or the U.S. Government. We thank our team members on this project for all the discussions to develop this paper. Some of the ideas in this paper are based on our learning from the SAIL-ON meetings",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a high-level diagram of different parts of this",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed NAERS system architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed emotion recognition model architecture",
      "page": 2
    },
    {
      "caption": "Figure 3: Input images are",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed Convolutional Neural Network (CNN)",
      "page": 3
    },
    {
      "caption": "Figure 4: shows the confusion matrix for our emotion",
      "page": 4
    },
    {
      "caption": "Figure 4: Confusion matrix of our emotion recognition",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Different datasets for automatic emotion recogni-",
      "data": [
        {
          "Name": "CK+",
          "# Items": "593",
          "Type": "Video",
          "Setting": "Posed\n&\nsponta-\nneous",
          "Classes": "N, S, Sr, H,\nF, A and D"
        },
        {
          "Name": "FER-2013",
          "# Items": "32,298",
          "Type": "Image",
          "Setting": "Posed",
          "Classes": "N, S, Sr, H,\nF, A and D"
        },
        {
          "Name": "Emotic",
          "# Items": "23,571",
          "Type": "Image",
          "Setting": "Wild",
          "Classes": "N, S, Sr, H,\nF, A, D and\n19\nother\nclasses"
        },
        {
          "Name": "AffectNets",
          "# Items": "450,000",
          "Type": "Image",
          "Setting": "Wild",
          "Classes": "N, S, Sr, H,\nF, A, D and\nC"
        },
        {
          "Name": "CAER-S",
          "# Items": "70,000",
          "Type": "Image",
          "Setting": "TV shows",
          "Classes": "N, S, Sr, H,\nF, A and D"
        },
        {
          "Name": "FABO",
          "# Items": "206",
          "Type": "Video",
          "Setting": "Posed",
          "Classes": "N, S, Sr, H,\nF, A, B, P,\nAx and D"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: While shouting or laughing, people generally 5.3 NoveltyDetector",
      "data": [
        {
          "Feature Type": "Width",
          "Feature Description": "Left eye\nRight eye\nMouth"
        },
        {
          "Feature Type": "Height",
          "Feature Description": "Right eye\nLeft eye\nRight eye\nMouth"
        },
        {
          "Feature Type": "Distance",
          "Feature Description": "Left and right eyes\nEyes to brows\nEyes to mouth\nEyes and nose\nNose and mouth"
        },
        {
          "Feature Type": "Angle",
          "Feature Description": "Left eye with right eye and mouth\nRight Eye with left eye and mouth\nMouth with both eyes\nMouth with both eyes"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Performance comparison of different emotion",
      "data": [
        {
          "Dataset": "CK+",
          "Accuracy(%)": "97.5"
        },
        {
          "Dataset": "FER-2013",
          "Accuracy(%)": "75.8"
        },
        {
          "Dataset": "AffectNets",
          "Accuracy(%)": "66.5"
        },
        {
          "Dataset": "CAER-S",
          "Accuracy(%)": "72.4"
        },
        {
          "Dataset": "FABO",
          "Accuracy(%)": "96.1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance comparison of different emotion",
      "data": [
        {
          "Author": "(Mollahosseini, Chan,\nand Mahoor 2016)",
          "Model": "CNN",
          "Accuracy(%)": "66.0"
        },
        {
          "Author": "(Dhankhar 2019)",
          "Model": "RESNET-50",
          "Accuracy(%)": "67.2"
        },
        {
          "Author": "(Renda et al. 2019)",
          "Model": "Ensemble",
          "Accuracy(%)": "71"
        },
        {
          "Author": "(Gan, Chen,\nand Xu\n2019)",
          "Model": "Ensemble",
          "Accuracy(%)": "73.73"
        },
        {
          "Author": "This work",
          "Model": "Ensemble\nCNN",
          "Accuracy(%)": "75.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Posture emotion recognition accuracy comparison",
      "data": [
        {
          "Author": "(Barros et al.\n2015)",
          "Model": "CNN",
          "Face": "72.07",
          "Pos-\nture": "57.8"
        },
        {
          "Author": "(Chen et al. 2013)",
          "Model": "SVM with the\nRBF kernel",
          "Face": "66.5",
          "Pos-\nture": "66.7"
        },
        {
          "Author": "(Gunes and\nPiccardi 2009)",
          "Model": "Adaboost",
          "Face": "35.2",
          "Pos-\nture": "73.2"
        },
        {
          "Author": "This work",
          "Model": "Ensemble CNN",
          "Face": "92.7",
          "Pos-\nture": "83.1"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal emotional state recognition using sequencedependent deep hierarchical features",
      "authors": [
        "P Barros",
        "D Jirak",
        "C Weber",
        "S Wermter"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "2",
      "title": "Recognizing expressions from face and body gesture by temporal normalized motion and appearance features",
      "authors": [
        "S Chen",
        "Y Tian",
        "Q Liu",
        "D Metaxas"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "3",
      "title": "Integrating geometric and textural features for facial emotion classification using SVM frameworks",
      "authors": [
        "S Datta",
        "D Sen",
        "R Balasubramanian"
      ],
      "year": "2017",
      "venue": "Proceedings of International Conference on Computer Vision and Image Processing"
    },
    {
      "citation_id": "4",
      "title": "ResNet-50 and VGG-16 for recognizing facial emotions",
      "authors": [
        "P Dhankhar"
      ],
      "year": "2019",
      "venue": "International Journal of Innovations in Engineering and Technology (IJIET)"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition boosted by soft label with a diverse ensemble",
      "authors": [
        "Y Gan",
        "J Chen",
        "L Xu"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "6",
      "title": "Automatic Temporal Segment Detection and Affect Recognition From Face and Body Display",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "7",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep Residual Learning for Image Recognition",
      "arxiv": "arXiv:1512.03385"
    },
    {
      "citation_id": "8",
      "title": "Content based facial emotion recognition model using machine learning algorithm",
      "authors": [
        "R Jadhav",
        "P Ghadekar"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Advanced Computation and Telecommunication (ICACAT)"
    },
    {
      "citation_id": "9",
      "title": "Semisupervised facial expression recognition using reduced spatial features and deep belief networks",
      "authors": [
        "A Kurup",
        "M Ajith",
        "M Ramón"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "10",
      "title": "Object recognition with gradient-based learning",
      "authors": [
        "Y Lecun",
        "P Haffner",
        "L Bottou",
        "Y Bengio",
        "Springer",
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C.-L Chang",
        "M Yong",
        "J Lee"
      ],
      "year": "1999",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "11",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "12",
      "title": "Facial sentiment analysis using AI techniques: state-of-the-art, taxonomies, and challenges",
      "authors": [
        "K Patel",
        "D Mehta",
        "C Mistry",
        "R Gupta",
        "S Tanwar",
        "N Kumar",
        "M Alazab"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Comparing ensemble strategies for deep learning: An application to facial expression recognition",
      "authors": [
        "A Renda",
        "M Barsacchi",
        "A Bechini",
        "F Marcelloni"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    }
  ]
}