{
  "paper_id": "2209.10223v1",
  "title": "Dynamic Time-Alignment Of Dimensional Annotations Of Emotion Using Recurrent Neural Networks",
  "published": "2022-09-21T09:38:57Z",
  "authors": [
    "Sina Alisamir",
    "Fabien Ringeval",
    "Francois Portet"
  ],
  "keywords": [
    "Affective computing",
    "Emotion recognition",
    "Dynamic time-alignment",
    "Recurrent neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Most automatic emotion recognition systems exploit time-continuous annotations of emotion to provide fine-grained descriptions of spontaneous expressions as observed in real-life interactions. As emotion is rather subjective, its annotation is usually performed by several annotators who provide a trace for a given dimension, i.e. a time-continuous series describing a dimension such as arousal or valence. However, annotations of the same expression are rarely consistent between annotators, either in time or in value, which adds bias and delay in the trace that is used to learn predictive models of emotion. We therefore propose a method that can dynamically compensate inconsistencies across annotations and synchronise the traces with the corresponding acoustic features using Recurrent Neural Networks. Experimental evaluations were carried on several emotion data sets that include Chinese, French, German, and Hungarian participants who interacted remotely in either noise-free conditions or in-the-wild. The results show that our method can significantly increase inter-annotator agreement, as well as correlation between traces and audio features, for both arousal and valence. In addition, improvements are obtained in the automatic prediction of these dimensions using simple light-weight models, especially for valence in noise-free conditions, and arousal for recordings captured in-the-wild.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic detection of apparent human emotions is of growing interest as it has many real-life applications, touching mostly upon education  Tsatsou et al. [2018] , health  Cummins et al. [2015] , and entertainment  Cosentino et al. [2018] . Affective computing exploits psychological theories of emotion that describe expressions of affect with either a categorical, or a dimensional model: categorical representations view emotion as different classes  Ekman [1993] , such as the basic emotions  Ekman and Cordaro [2011] , whereas dimensional representations describe emotion with different scales  Russel [1980] , such as arousal (ranging from active to passive) and valence (ranging from pleasant to unpleasant). Although some attempts have been made to detail the cognitive processes involved in the emotional experience  Wehrle and Scherer [2001] , the arguably dominant approach in modeling affect relies on dimensional representations of emotion, as they allow a fine differentiation of real-life expressions  Schuller et al. [2012] . Such representation of emotion nonetheless implies some well-known issues, such as biases and delays in annotations, which can vary considerably according to the annotators and the peculiarities of the judged expression  Soroosh and Busso [2013] . In this paper, our aim is to address this problem.\n\nMethods have been proposed to create a unified trace of a set of time-continuous annotations of emotion  Ringeval et al. [2018] , which is usually referred to as 'Gold Standard' (GS). One of the main issues in creating a unified GS that can be reliably used for affect modelling stems from annotator reaction delay, which is defined as the time it takes for an annotator to perceive an acoustic event, evaluate it, and report the value best describing the emotional expression according to the chosen scale  Khorram et al. [2019] .\n\nWhen annotator reaction times are taken into account, a significant performance increase can be observed in the automatic recognition of emotion from speech  Soroosh and Busso [2013] ,  Khorram et al. [2019] ,  Nicolle et al. [2012] . Existing methods, however, make the hypothesis that the annotation delay is constant over a whole interaction sequence, whereas it can vary greatly within the same sequence  Mariooryad and Busso [2014] . In addition, some approaches predict emotions with delays like annotators, which is problematic for conversational systems subject to -almostreal-time constraints or systems that use detected speech segments as input, since annotator delay may vary up to several seconds.\n\nIn this work, we define a tandem of Neural Networks (NNs) that dynamically correct inconsistencies in continuous annotations of emotion, cf. Figure  1 . Whereas a first model has the objective of correcting annotations while preserving their overall shape, another model predicts these corrected annotations from the acoustic features. By jointly learning the two models, the system ultimately provides dynamic corrections of the annotations by aligning them with their corresponding events present in the speech. In effect, the system is forced to provide corrections that match what can be linearly predicted from the data. It is worth to note that our method does not make use of the acoustic features as input of the first model correcting the annotation, as the same features are used to predict emotion.\n\nWe performed various evaluations of the generated GS and compared the results with prior methods on two benchmark data sets (RECOLA and SEWA) that include dimensional annotations of emotion for different cultures (Chinese, French, German, Hungarian), and recording conditions (noise-free, in-the-wild). More specifically, we quantified the inter-annotator agreement, the correlation between the GS and the acoustic data, and evaluated the performance obtained in the automatic recognition of emotion from speech, using either the original GS (simple average of the traces), or the one created by our system.\n\nIn order to deal with issues related to the fact that features and annotations have different frequency bandwidths  Khorram et al. [2017] , we also investigated the interest of a low-pass filter with a cutoff frequency that can be learned for both signals, using a convolutional layer with a Sinc function as kernel. We followed a curriculum in the learning of the model  Bengio et al. [2009] , by starting with a low cutoff frequency, which helps to gradually incorporate more details of the signals during the learning phase.\n\nResults show that: (i) the inter-annotator agreement is either preserved or increased when corrections are applied to the annotations, (ii) corrected annotations have a higher correlation with acoustic features, (iii) the Sinc function helps the system to return predictions with smooth trajectories, and (iv) both the Sinc function and the generated GS can improve the performance obtained in the automatic recognition of emotion from speech.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "As events that trigger changes in the annotation of emotion are present in the data, several studies investigated synchronisation techniques by using audiovisual descriptors to compensate for delays in annotator response.  Nicolle et al. Nicolle et al. [2012]  proposed to maximise the correlation between facial descriptors and annotations to estimate this delay, which is applied as an offset on the features. Other measures were also later exploited, such as mutual information  Mariooryad and Busso [2014] , or Concordance Correlation Coefficient (CCC)  Li [1989] , He et al.  [2015] . A majority vote triplet embedding scheme was then exploited in conjunction with Dynamic Time Warping (DTW) methods to account for inconsistencies in annotations of emotion in  Booth et al. [2018] . Even though DTW methods adjusted the annotations based on a single reference feature, the authors reported improvement in automatic recognition of valence on RECOLA data set.\n\nLong Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs), which can capture long-term temporal dependencies, have been used on individual ratings of emotion to learn dependencies between annotations in a single multi-task problem  Ringeval et al. [2015] . A downsampling/upsampling network based on Convolutional Neural Networks (CNNs) has also shown the interest of exploiting long-term temporal dependencies for continuous emotion recognition from speech  Khorram et al. [2017] . More recently, a Sinc function has been used as the kernel of CNNs to jointly learn annotation delay and emotion  Khorram et al. [2019] . While these methods achieved state-of-the-art performance on various benchmark data sets, and showed the value of compensating inconsistencies in annotations of emotion, they did not allow for dynamic annotation correction.\n\nFigure  1 : Flowchart of the proposed system for dynamic time-alignment of continuous ratings of apparent emotion with corresponding acoustic events. The system is composed of a first model (B-GRU/Linear layers) that performs corrections to the annotations while preserving their original shape, and of a second model (Linear layer) that predicts the generated GS from the acoustic features. The two models are learned jointly with a loss function based on both CCC and Cross-CCC, where the former accounts for the similarity between the generated GS (weighted average of the corrected annotations) and the emotion predicted from the features, and the latter makes sure that the overall shape of the original annotations is preserved when corrections are applied to them; B-GRU: Bidirectional Gated Recurrent Units.\n\nIt is worth to mention that a CNN with a Sinc function as kernel (Sinc layer) has been successfully used with two trainable cutoff frequencies in order to learn band-pass filters for automatic speaker and speech recognition  Ravanelli and Bengio [2018a,b] . Authors have shown that more interpretable filters can be obtained with Sinc layers compared to using generic CNNs. Here, our Sinc layer has only one learnable parameter which is the cutoff frequency to smooth the input and output signals. We start the training with a low cutoff frequency, i.e. a highly smoothed signal, so that the model can automatically and gradually adjust the details needed to increase its performance, thus following a curriculum.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section, We describe the method we developed to align continuous annotations of apparent emotion with associated events present in acoustic signal. The system is based on joint learning of a tandem of NNs that are depicted in Figure  1 .\n\nThe first model is composed of B-GRU (Bidirectional GRU) followed by a linear layer, and has the objective of correcting annotations while preserving their overall shape. We preferred a bidirectional network use past information when correction annotations. Preservation of the original shape of the annotations is obtained by a loss function based on the Cross-CCC, which is computed as the average of the CCCs obtained between delayed versions of the corrected annotations and the original time-series, with steps of 100 ms and a maximum duration of ten seconds, to cover delays up to this value. 100 ms time step is chosen to allow for a further time span while also keeping the training time of the system relatively short. Also, We averaged the results of the Cross-CCC function instead of computing its maximum, as it can be more easily backpropagated during the training of the system.\n\nThe second model is composed of a simple linear layer whose objective is to predict the GS obtained by calculating the weighted average of the corrected annotations, using the acoustic features as input. To ensure that the generated GS is well synchronised with the corresponding acoustic features, we used as loss function the CCC between the output of this model and the generated GS. We needed to use a linear layer here, i.e. without any usage of contextual information, in order not to compromise correction of the annotations in time by the first model.\n\nBecause reliability of the annotation can vary by annotator, we performed a weighted average of the corrected annotations to produce the GS. This step is realised by a trainable weight vector that goes through a Softmax layer to return a probability vector, which is finally multiplied with the corrected annotations to obtain the GS.\n\nSince the two loss functions are jointly learnt, the B-GRU model ideally produces corrections to the annotations that match what can be linearly inferred from the features, and hence provides dynamic time-alignment of the annotations with the corresponding acoustic events. The loss function that is used for jointly training our system is defined as follows:\n\nwhere N is the number of annotations, CrossCCC(., .) n the averaged of the CCCs computed between the original nth annotations and the corrected annotations for different delays, and CCC(., .) is computed between the output of the linear model and the targeted GS with the following equation Li  [1989] :\n\nwhere ρ is the (Pearson's) correlation coefficient, µ x and µ y are the means for the vectors x and y respectively and their variances are defined as σ x and σ y . And cross-CCC is computed through the following equation:\n\nwhere T is a temporal series in seconds (here it is defined as T = {0, 0.1, 0.2, 0.3, . . . , 9.9, 10}), N is the total number of elements in T , x is the original signal and y(t -T (n)) is the shifted version of y(t) in time.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe the acoustic features that were extracted from the speech signals, the data sets we exploited, and the emotion prediction systems along the Sinc layer we used for experimental evaluations. Training and testing conditions for the models are also given, as well as the hyperparameter that was optimized.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Features",
      "text": "We extracted as acoustic descriptors the first 40 log Mel-filterbank (MFB) coefficients from the audio signal using the PYTHON_SPEECH_FEATURES 1 library, as they have proven to perform well with different models in emotion prediction tasks  Le et al. [2017] ,  Khorram et al. [2017] , AlBadawy and Kim [2018],  Khorram et al. [2019] . The feature extraction is realised on a 25 ms window that is shifted forward in time each 10 ms. All features are standardized using mean and variance obtained on the training partition of each data set, which makes them to have a zero mean and unit variance on this partition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Sets",
      "text": "RECOLA  Ringeval et al. [2013]  and SEWA  Kossaifi et al. [2019]  data sets were used for our experiments. A summary of these corpora is given in Table  1 . RECOLA is a well-known corpus for benchmarking emotion recognition systems  Valstar et al. [2016] ,  Ringeval et al. [2018] . It contains 46 audiovisual recordings of spontaneous interactions between French-speaking subjects who solved a collaborative task under remote condition. The recordings were SEWA is a more recent corpus that consists of audiovisual recordings of spontaneous interactions between subjects for several different cultures. The data were recorded in-the-wild, i.e. via a video chat platform the webcams and microphones of each participant at various locations. The conversations were about an advertisement and are from 47 seconds to three minutes. Expressions of apparent emotion were annotated in the dimensions of arousal, valence and (dis)liking intensity (not used in this paper as the associated cues are mostly conveyed by linguistic information) with a sampling rate of 10 Hz. We used the German (GE), Hungarian (HU) and Chinese (CN) cultures with the exact same partitioning as defined in the AVEC 2019 Challenge  Ringeval et al. [2019] , with the exception of the Chinese culture that was solely used for testing, and which is partitioned here with the same rules as those used for the other cultures.\n\nThe data set also provides a GS generated as a consensus between the annotators.\n\nTo have the same length for both the features and annotations, which make the learning of the emotion prediction models easier, we linearly re-sampled all the annotations for all data sets to 100 Hz, which is the sampling frequency of the audio features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Prediction Systems",
      "text": "In order to evaluate the ability of our system to synchronise annotations with features, we based our emotion prediction system on a simple linear layer that only maps a given input to one output, followed by a tangent hyperbolic function to map the output to the annotations' range, which is within [-1, +1]. We used a simple and light-weight model that does not exploit -potentially long-range -contextual dependencies between features and annotations, as recurrent architectures are not necessary to achieve competitive results for time-continuous emotion recognition with NNs  Schmitt et al. [2019] . Benefits of the Sinc layer in emotion recognition from speech are further evaluated by incorporating it in the emotion prediction system. Flowcharts of the two developed emotion prediction systems, i.e. Linear-Tanh (LT), and Sinc-Linear-Tanh-Sinc (SLTS), can be found in Figure  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sinc Layer",
      "text": "The Sinc layer is composed of a convolutional layer with the kernel defined as the Sinc function:\n\nwhere f s is the sampling frequency (here f s =100 Hz), and f c the cutoff frequency, which is a trainable parameter in our case. The kernel size here is set to 20 seconds (20 * f s ) in order to cover relatively low cutoff frequencies. Using the Sinc layer on a signal acts as an ideal low-pass filter, which has a smoothing effect depending on the f c . We initialised this cutoff frequency to f s /1000 with the idea to let the system start the training with a low resolution signal, and go towards a more detailed representation if needed, i.e. for reaching a lower loss. We show the results of this Sinc layer when applied on the input data in Figure  3 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hyper-Parameter",
      "text": "We optimised the hidden size of the B-GRU model used for annotation correction in the GS generator model. We experimented with hidden size ∈ {8, 16, 32, 64, 128, 256, 512} on RECOLA data set and the lowest loss was achieved by hidden size of 128 for arousal and 256 for valence, which were used to generate the GS for all data sets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "In this section, we detail and discuss the qualitative and quantitative results obtained from the analysis of the generated GS.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Time-Alignment With Acoustic Features",
      "text": "Visual comparisons of the alignment of the baseline GS and that obtained with our system with the acoustic features are given in Figure  4 . The plot shows that the generated GS, compared to the baseline GS, is corrected both in time and value with respect to the corresponding acoustic events present in the speech. We further computed the Pearson's correlation coefficient between the first MFB feature (MFB 0 ) and the GS and averaged the results over all subjects of  .47 (.12) SEWA HU .21  (.22)  .14  (.77)  the RECOLA data set. Results show that the correlation increases from .221 to .484 on arousal when corrections are applied to the GS, and from .069 to .283 on valence.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Preservation Of The Original Shape",
      "text": "To ensure that the B-GRU model correcting the annotations does not distort them, we used a specific loss function (c.f. equation 1) that has the objective of preserving the overall shape of the original annotations. We show in Figure  5  that such information is indeed well preserved when our system corrects the annotations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Inter-Annotator Agreement",
      "text": "We report comparisons of the inter-annotator agreement obtained on the original and corrected annotations for RECOLA and SEWA data sets in Table  2 . The annotations of emotion being continuous in value, we computed the pair-wise Cronbach's alpha Cronbach  [1951]  as measure of inter-annotator agreement on each recording. We used the Fisher r-to-z transform to perform statistical comparisons of the inter-annotator agreement with a one-tailed test. Results show that the inter-annotator agreement is either preserved or significantly increased (p < .005) when using our system for all cases except for the Hungarian culture, where the reported inter-annotator agreement is the lowest. This result suggests that a minimum of agreement between the annotators is required to perform time-alignment of the annotations with the corresponding acoustic features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of The Sinc Layer",
      "text": "In Figure  6 , we show the impact of the Sinc layer by comparing predictions obtained with the two emotion prediction systems along with the GS. Results show that the LT model returns predictions that contain very high frequencies, with strong variations over time, whereas the SLTS model provides predictions that have smooth trajectories.\n\nFigure  5 : Comparison of the individual annotations before and after the B-GRU model, which is designed for preserving the original shape of the annotations when applying corrections to them. Thick lines correspond to the corrected annotation of the thin lines with the same color. The plot is for arousal and represents subject \"dev_01\" of RECOLA data set.\n\nFigure  6 : Predictions of arousal (target) obtained with the LT model, which includes a linear layer followed by hyperbolic tangent function, and the SLTS model, which additionally make use of the Sinc layer. The plot is for subject \"dev_01\" on RECOLA data set.\n\nWe report the cutoff frequencies f c that were learned by the Sinc layers for the features and the predictions in Table  3 .\n\nResults show that the details of the acoustic features seem to be useful only for the recognition of arousal in noise-free conditions, as the obtained cutoff frequencies are very low for all other cases. Regarding the learnt cutoff frequencies for predictions, the results vary greatly depending on the dimension processed: whereas arousal does not need to be detailed precisely to reach the best performance, a common cutoff frequency of f s /2 is obtained for valence, meaning that all details present in the GS were found useful for its automatic recognition. One may further note that the Hungarian culture showed the lowest f c value in most of cases, which is in agreement with the results reported in Table  2 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Performance In Affect Sensing",
      "text": "We present the performance obtained by the two emotion prediction systems on the baseline and generated GS, for RECOLA and SEWA data sets, in Table  4 . Results show that the generated GS is more reliable to predict arousal or valence from speech, either in noise-free conditions or for data captured in-the-wild, and that the use of the Sinc layer always resulted in higher performance, except for the valence on the Hungarian culture.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "We compare the results obtained in this study with those of the state-of-the-art in Table  5 . In order to make the comparisons fair on RECOLA data set, we distinguish its two different versions and present our results accordingly: one containing 27 subjects, and another with all 46 subjects. Regarding the results reported on SEWA data set, we used as a reference the performance obtained with the MFCC features in the baseline system developed for AVEC 2019 with intra-cultural models  Ringeval et al. [2019] , for which performance is not reported on the Chinese culture. Results show that the generated GS can perform similar to the state-of-the-art, while using a relatively simpler network, and that improvement can be further reported, especially on arousal for recordings captured in-the-wild, and valence for noise-free conditions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "As there is no ground-truth in the description of human apparent emotion, defining numerical attributes that describe expressions of affect in a way that they can be easily recognised from speech with machine learning methods is a challenging task. Indeed, when humans are confronted with fine-grained annotation of emotion attributes with time-continuous scales, such as arousal and valence, many discrepancies can be found between annotators, either in time or value, adding noise to the definition of targets used for emotion recognition. In this work, we show that audio signals can be used to correct such inconsistencies, and that the resulting GS can provide a higher inter-annotator agreement, and better performance in its automatic recognition from speech. However, it should be noted that because the annotations were performed on the audiovisual recordings, visual information was also available to the annotator to report on the apparent expressions of emotion. As emotion is inherently multimodal, our method needs to be further investigated using facial information in addition to acoustic descriptors. The use of peripheral physiological signals could also be investigated, as well as recordings of the annotators when performing the annotation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed a system that can compensate for the inconsistencies found in continuous emotion annotations with constraints driven from audio features. We showed that the generated GS, which preserves the details of the original annotations, can provide a significant increase in both inter-annotator agreement and performance obtained in its automatic recognition for different cultures and contexts.\n\nWe also investigated the interest of a Sinc layer acting as a low-pass filter with a cutoff frequency that can be learnt for features and labels. The Sinc layer initialised from a low value can help the model follow curriculum learning. The results shows that the method always improved performance and furthermore provided interpretable results.\n\nFuture work will involve exploiting more sophisticated descriptors of speech, such as self-supervised representations, as well as facial descriptors, to perform annotation corrections. We are also interested in modeling each individual annotators' reaction during the annotation of time-continuous attributes of emotion using their own audiovisual recordings.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Whereas a ﬁrst model has the objective of correcting annotations while preserving",
      "page": 2
    },
    {
      "caption": "Figure 1: Flowchart of the proposed system for dynamic time-alignment of continuous ratings of apparent emotion",
      "page": 3
    },
    {
      "caption": "Figure 1: The ﬁrst model is composed of B-GRU (Bidirectional GRU) followed by a linear layer, and has the objective of",
      "page": 3
    },
    {
      "caption": "Figure 2: Flowcharts of the emotion prediction systems.",
      "page": 5
    },
    {
      "caption": "Figure 3: Effect of the Sinc layer with fc=2 Hz on the ﬁrst MFB feature for subject \"train_01\" of RECOLA data set.",
      "page": 6
    },
    {
      "caption": "Figure 4: The plot shows that the generated GS, compared to the baseline GS, is corrected both in time",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparisons of the alignment of the original and generated GS with the ﬁrst MFB feature (divided by 2 for",
      "page": 7
    },
    {
      "caption": "Figure 6: , we show the impact of the Sinc layer by comparing predictions obtained with the two emotion prediction",
      "page": 7
    },
    {
      "caption": "Figure 5: Comparison of the individual annotations before and after the B-GRU model, which is designed for preserving",
      "page": 8
    },
    {
      "caption": "Figure 6: Predictions of arousal (target) obtained with the LT model, which includes a linear layer followed by",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: RECOLA is a well-known corpus for benchmarking emotion recognition",
      "data": [
        {
          "# Female / Male": "# Annotators",
          "27 / 19": "6",
          "103 / 101": "5"
        },
        {
          "# Female / Male": "Duration",
          "27 / 19": "230 minutes",
          "103 / 101": "510 minutes"
        },
        {
          "# Female / Male": "Culture(s)",
          "27 / 19": "French",
          "103 / 101": "Chinese,\nGerman,\nHungarian"
        },
        {
          "# Female / Male": "Partitioning\nTraining-Dev-Test",
          "27 / 19": "16-15-15",
          "103 / 101": "GE: 34-14-16\nHU: 34-14-18\nCN: 30-20-20"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Statistics of the inter-annotator agreement (pair-wise Cronbach’s alpha) obtained on each recording of",
      "data": [
        {
          "Data set": "RECOLA\nSEWA CN\nSEWA GE\nSEWA HU",
          "Baseline": ".48 (.05)\n.35 (.09)\n.25 (.07)\n.11 (.15)",
          "Generated": ".53 (.05)\n.43 (.16)\n.24 (.17)\n.05 (.42)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Statistics of the inter-annotator agreement (pair-wise Cronbach’s alpha) obtained on each recording of",
      "data": [
        {
          "Data set": "RECOLA\nSEWA CN\nSEWA GE\nSEWA HU",
          "Baseline": ".51 (.06)\n.35 (.09)\n.38 (.09)\n.21 (.22)",
          "Generated": ".51 (.09)\n.40 (.17)\n.47 (.12)\n.14 (.77)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: In order to make the",
      "data": [
        {
          "GS": "Baseline\nBaseline\nBaseline\nBaseline",
          "Data set": "RECOLA\nSEWA CN\nSEWA GE\nSEWA HU",
          "fc Feature": "30.7\n0.38\n12.1\n0.02",
          "fc Prediction": "0.05\n0.04\n0.05\n0.02"
        },
        {
          "GS": "Generated\nGenerated\nGenerated\nGenerated",
          "Data set": "RECOLA\nSEWA CN\nSEWA GE\nSEWA HU",
          "fc Feature": "1.45\n0.02\n0.16\n0.01",
          "fc Prediction": "0.10\n0.60\n0.15\n0.60"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: In order to make the",
      "data": [
        {
          "target GS": "Baseline\nBaseline\nBaseline\nBaseline",
          "Data set": "RECOLA\nSEWA GE\nSEWA HU\nSEWA CN",
          "fc Feature": "0.03\n0.07\n0.03\n0.09",
          "fc Prediction": "23.26\n49.62\n49.82\n49.96"
        },
        {
          "target GS": "Generated\nGenerated\nGenerated\nGenerated",
          "Data set": "RECOLA\nSEWA GE\nSEWA HU\nSEWA CN",
          "fc Feature": "0.07\n0.05\n0.08\n0.02",
          "fc Prediction": "49.98\n49.98\n49.97\n48.96"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: In order to make the",
      "data": [
        {
          "Target GS": "Baseline\nBaseline",
          "Model": "LT\nSLTS",
          "RECOLA–French": ".190\n.528",
          "SEWA–Chinese": ".000\n.082",
          "SEWA–German": ".102\n.249",
          "SEWA–Hungarian": ".000\n.130",
          "All": ".073\n.247"
        },
        {
          "Target GS": "Generated\nGenerated",
          "Model": "LT\nSLTS",
          "RECOLA–French": ".482\n.720",
          "SEWA–Chinese": ".054\n.120",
          "SEWA–German": ".367\n.450",
          "SEWA–Hungarian": ".071\n.123",
          "All": ".243\n.353"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: In order to make the",
      "data": [
        {
          "Target GS": "Baseline\nBaseline",
          "Model": "LT\nSLTS",
          "RECOLA–French": ".074\n.289",
          "SEWA–Chinese": ".051\n.137",
          "SEWA–German": ".037\n.134",
          "SEWA–Hungarian": ".005\n.000",
          "All": ".042\n.140"
        },
        {
          "Target GS": "Generated\nGenerated",
          "Model": "LT\nSLTS",
          "RECOLA–French": ".248\n.538",
          "SEWA–Chinese": ".049\n.117",
          "SEWA–German": ".001\n.119",
          "SEWA–Hungarian": ".000\n.066",
          "All": ".074\n.210"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Adaptive learning based on affect sensing",
      "authors": [
        "Dorothea Tsatsou",
        "Andrew Pomazanskyi",
        "Enrique Hortal",
        "Evaggelos Spyrou",
        "Helen Leligou",
        "Stylianos Asteriadis"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Artificial Intelligence in Education (AIED)"
    },
    {
      "citation_id": "2",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "Nicholas Cummins",
        "Stefan Scherer",
        "Jarek Krajewski",
        "Sebastian Schnieder",
        "Julien Epps",
        "Thomas Quatieri"
      ],
      "year": "2015",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Group emotion recognition strategies for entertainment robots",
      "authors": [
        "Sarah Cosentino",
        "Estelle Randria",
        "Jia-Yeu Lin",
        "Thomas Pellegrini",
        "Salvatore Sessa",
        "Atsuo Takanishi"
      ],
      "year": "2018",
      "venue": "International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "4",
      "title": "Paul Ekman and Daniel Cordaro. What is meant by calling emotions basic",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1993",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "5",
      "title": "Towards computational modeling of appraisal theories",
      "authors": [
        "James Russel"
      ],
      "year": "1980",
      "venue": "Appraisal Processes in Emotion: Theory, Methods, Research"
    },
    {
      "citation_id": "6",
      "title": "AVEC 2012 -The continuous Audio/Visual Emotion Challenge",
      "authors": [
        "Björn Schuller",
        "Michel Valstar",
        "Florian Eyben",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "7",
      "title": "Analysis and Compensation of the Reaction Lag of Evaluators in Continuous Emotional Annotations",
      "authors": [
        "Mariooryad Soroosh",
        "Carlos Busso"
      ],
      "year": "2013",
      "venue": "Proceedings of the 5th biannual International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Dennis Lalanne",
        "Adrien Michaud",
        "Elvan Ciftci",
        "Hüseyin Gülec",
        "Albert Ali Salah",
        "Maja Pantic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 8th International Workshop on Audio/Visual Emotion Challenge (AVEC'18)"
    },
    {
      "citation_id": "9",
      "title": "Jointly aligning and predicting continuous emotion annotations",
      "authors": [
        "Soheil Khorram",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Robust continuous prediction of human emotions using multiscale dynamic cues",
      "authors": [
        "Jérémie Nicolle",
        "Vincent Rapp",
        "Kévin Bailly",
        "Lionel Prevost",
        "Mohamed Chetouani"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM International Conference on Multimodal Interaction (ICMI'12)"
    },
    {
      "citation_id": "11",
      "title": "Correcting time-continuous emotional labels by modeling the reaction lag of evaluators",
      "authors": [
        "Soroosh Mariooryad",
        "Carlos Busso"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "arxiv": "arXiv:1708.07050"
    },
    {
      "citation_id": "13",
      "title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks",
      "authors": [
        "Yoshua Bengio",
        "Jérôme Louradour",
        "Ronan Collobert",
        "Jason Weston",
        "Lang He",
        "Dongmei Jiang",
        "Le Yang",
        "Ercheng Pei",
        "Peng Wu",
        "Hichem Sahli"
      ],
      "year": "1989",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge (AVEC'15)"
    },
    {
      "citation_id": "14",
      "title": "Prediction of Asynchronous Dimensional Emotion Ratings from Audiovisual and Physiological Data",
      "authors": [
        "M Brandon",
        "Karel Booth",
        "Shrikanth Mundnich",
        "Narayanan"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2018 Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "15",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Interpretable convolutional filters with sincnet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "Interpretable convolutional filters with sincnet",
      "arxiv": "arXiv:1811.09725"
    },
    {
      "citation_id": "17",
      "title": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network",
      "authors": [
        "Duc Le",
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proceedings INTERSPEECH 2017, 18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "18",
      "title": "Joint discrete and continuous emotion prediction using ensemble and end-to-end approaches",
      "authors": [
        "A Ehab",
        "Yelin Albadawy",
        "Kim"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th International Conference on Multimodal Interaction (ICMI'18)"
    },
    {
      "citation_id": "19",
      "title": "Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Jürgen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2nd International Workshop on Emotion Representation"
    },
    {
      "citation_id": "20",
      "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Björn Schuller",
        "Kam Star",
        "Elnar Hajiyev",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "AVEC 2016 -Depression, Mood, and Emotion Recognition Workshop and Challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge (AVEC'16)"
    },
    {
      "citation_id": "22",
      "title": "AVEC 2019 Workshop and Challenge: State-of-Mind, Depression with AI, and Cross-Cultural Affect Recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Mohammad Soleymani",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Eva-Maria Messner",
        "Leili Tavabi",
        "Siyang Song",
        "Sina Alisamir",
        "Shuo Lui",
        "Ziping Zhao",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International Workshop on Audio/Visual Emotion Challenge (AVEC'19)"
    },
    {
      "citation_id": "23",
      "title": "Continuous Emotion Recognition in Speech -Do We Need Recurrence?",
      "authors": [
        "Maximilian Schmitt",
        "Nicholas Cummins",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings INTERSPEECH 2019, 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "24",
      "title": "Discriminatively Trained Recurrent Neural Networks for Continuous Dimensional Emotion Recognition from Audio",
      "authors": [
        "Felix Weninger",
        "Fabien Ringeval",
        "Erik Marchi",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "25",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Proceedings of the thirty-third Conference on Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "26",
      "title": "Coefficient alpha and the internal structure of tests",
      "authors": [
        "J Lee",
        "Cronbach"
      ],
      "year": "1951",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "27",
      "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
      "authors": [
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings INTERSPEECH 2016, 17th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "28",
      "title": "Bags in Bag: Generating Context-Aware Bags for Tracking Emotions from Speech",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Maximilian Schmitt",
        "Zhao Ren",
        "Fabien Ringeval",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings INTERSPEECH 2018, 19th Annual Conference of the International Speech Communication Association"
    }
  ]
}