{
  "paper_id": "2402.15662v1",
  "title": "Gimefive: Towards Interpretable Facial Emotion Classification",
  "published": "2024-02-24T00:37:37Z",
  "authors": [
    "Jiawen Wang",
    "Leah Kawka"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep convolutional neural networks have been shown to successfully recognize facial emotions for the past years in the realm of computer vision. However, the existing detection approaches are not always reliable or explainable, we here propose our model GiMeFive with interpretations, i.e., via layer activations and gradient-weighted class activation mapping. We compare against the state-of-the-art methods to classify the six facial emotions. Empirical results show that our model outperforms the previous methods in terms of accuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated FER GiMeFive. Furthermore, we explain our work in real-world image and video examples, as well as real-time live camera streams. Our code and supplementary material are available at https: //github.com/werywjw/SEP-CVDL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, facial recognition technology has matured significantly. As an important branch of facial recognition, Facial emotion recognition (FER)  [13, 16, 30, 55]  has received growing attention from researchers. This progress is largely driven by the effective utilization of convolutional neural networks (CNNs). Originally introduced for image classification, CNNs have proven their ability to capture spatial hierarchies, making them ideal for analyzing facial features and expressions. Since the pioneering work by Le-Cun et al.  [21] , CNNs have significantly improved, fueled by larger datasets, powerful GPU implementations, and innovative regularization strategies.\n\nIn the era of deep learning and artificial intelligence (AI), one major focus in the research community has been on designing network architectures and objective functions towards discriminative feature learning  [11, 27, 54] . Simultaneously, there is a strong demand from researchers and general audiences to interpret its successes and failures  [10] , and to understand, improve, and trust the generated decisions by these models. This has led to the development of various tools for visualizing CNNs and dissecting their prediction paths to identify the important visual cues they rely on  [34] .\n\nFacial expressions, a cornerstone of non-verbal communication, hold significant weight in human interactions. Leveraging explainable AI (XAI)  [30, 34, 55] , we shed light on the decision-making process behind our emotion classifier, enhancing its transparency. In this report, we present our best model named GIMEFIVE, besides several deep CNNs to detect and interpret six basic universally recognized and expressed human facial emotions, namely happiness, surprise, sadness, anger, disgust, and fear. The goal of our research is to build a robust and interpretable model for facial emotion recognition tasks.\n\nOur main contributions can be summarized as follows. • We collect, preprocess, and evaluate the training and testing data (both images and videos) from various public databases thoroughly. • We implement all classification models from scratch and optimize them with several techniques in a systematic manner. Meanwhile, we illustrate the classification scores with the confusion matrix heat map. • We give several video demos to show the real-world performance of our best model GIMEFIVE. • We provide qualitative benefits such as interpretability to explain our model with gradient-weighted class activation mapping and face landmarks.\n\nPaper Outline. The structure of the rest of the report is arranged as follows. Section 2 contains the related work of our research. In Section 3, we address the datasets we collected and the model architecture we implemented. An overview of the experimental pipeline of our project is outlined in Figure  2 . The evaluation results of our models are given in Section 4 with interpretability. The test accuracy on the RAF-DB dataset is shown in Figure  1 . Section 5 describes the optimization strategies such as data augmentation and hyperparameter tuning. We provide the conclusion and discussion in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Facial Emotion Recognition. Studies from Ekman and Friesen  [8]  and Li and Deng  [24]  categorize facial expressions into distinct states such as happiness, sadness, or anger, demonstrating their cross-cultural recognition.\n\nWhile emotions can intertwine in specific situations, primary emotions remain broadly identifiable due to their innate nature. Facial emotion recognition capitalizes on this understanding by classifying the emotional states of in-dividuals based solely on their facial expressions. This field has witnessed a shift from hand-crafted features to sophisticated deep learning architectures, achieving remarkable accuracy on benchmark datasets  [9, 26] . Given the widespread use of video streams and FER applications, real-time operation is crucial. However, this must be balanced with fairness, robustness, and scalability, aspects often overlooked in recent research  [40] . Wang et al.  [53]  proposed a benchmark for bias mitigation, while Pham et al.  [37]  presented a novel masking idea to boost the performance on the FER2013  [1]  and Vietnam Emotion (VEMO) dataset.\n\nExplainable AI. Understanding the visual world through machines has been a driving force in computer vision for decades  [14, 29, 36, 46, 48] . Early attempts at peering into the \"black box\" of recognition models included directly visualizing filter responses  [56] , reconstructing inputs from network layers  [57] , and even crafting inputs to activate specific neurons  [33] . Recently, techniques such as Class Activation Mapping (CAM)  [58] , Gradient-weighted CAM (Grad-CAM)  [42] , and Grad-CAM++  [3] , have pushed the boundaries. While CAM leverages global pooling to pinpoint informative regions, Grad-CAM offers a more general framework for visualizing any convolutional filter, revealing insights hidden within neural networks. The quest for meaningful representations in face recognition goes back even further  [5, 20, 35] . Traditional approaches often pri-oritized accuracy over interpretability, building representations from either facial parts  [2, 22]  or facial attributes  [18] . While these methods achieved progress, they lacked transparency in how they captured and utilized facial information, bridging the gap in the field of interpretable representation learning.\n\nInterpretable Emotion Classification. For centuries, the central pursuit in psychology has been studies of the kaleidoscope of human emotions. In the 1960s, Ekman and Friesen  [8]  identified six basic universal expressions: happiness, sadness, anger, fear, surprise, and disgust. This framework later expanded to include contempt and embarrassment, offered a starting point for understanding emotional communication across cultures. However, research has evolved, recognizing the more nuanced nature of emotions. Instead of discrete categories, recent models focus on \"emotional dimensions\": valence (positivity/negativity) and arousal (activation level). Proposed by Russell  [38] , these dimensions offer a broader spectrum, encompassing the vast range of human emotions beyond just eight facial expressions. This dimensional approach has found its way into numerous models of emotion recognition and synthesis, as seen in the work of Kollias et al.  [17]  and Tottenham et al.  [49] . Lately, Yin et al.  [55]  focused on a specific area of interpretable visual recognition by learning from data a structured facial representation. Malik et al.  [30]  proposed an interpretable deep learning-based system, which has been developed for facial emotion recognition tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "All the experiments are implemented in Python. We also use Shell and Jupyter Notebook for generating image and video scripts. The experiment and evaluation are conducted on two MacBook Pros (M1 Pro-Chip with 10-core CPU and 16-core GPU; Intel Core i9 with 2.3 GHz 8-Core).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "To initiate the project, we gathered image databases representing different types of emotional expressions from the Real-world Affective Faces Database (RAF-DB, inthe-wild expression)  [23, 25] , Facial Expression Recognition 2013 (FER2013, real-time wild expression)  [1] , Facial Expression Recognition AffectNet Database (FER Affect-Net, in-the-wild expression)  [32] , Extended Cohn-Kanade Dataset Plus (CK+, posed expressions)  [28] , and Taiwanese Facial Expression Image Database (TFEID, posed expressions)  [4, 26] . These image datasets come in folderstructure classification (See Tab. 1 for statistic details with specific numbers on training, test, and validation set for each emotion class).\n\nFor reviewing explainable AI, we leverage the video dataset Denver Intensity of Spontaneous Facial Action Database (DISFA, spontaneous expressions)  [31] , which comprises two sets of videos, differentiated by capturing various camera angles (left and right). Each set consists of 27 videos, with each video comprising 4844 frames, resulting in 130788 images for each camera angle and a total of 261576 images. The participants in the videos are performing a series of spontaneous facial actions.\n\nImage Preprocessing. The procedure is proceeded from public institutions and kaggle  [7, 41] . To enhance the generalizability and robustness of our model, we aggregate these five FER image benchmarks into a new customized dataset called FER GIMEFIVE  1  . More specifically, the training set of FER GIMEFIVE is aggregated from the training sets of the five image datasets (55073 images in total), while the test set is combined from the test sets of RAF-DB and FER2013 (a total of 8333 images). The validation set is given with equal 100 images per class to test the performance of models.\n\nBuilding upon these image databases, we exclusively analyze human faces representing six emotions. That is, we first generalize a folder structure and append the emotion labels 0 to 5 to the name of each matching image (i.e., 0 for happiness, 1 for surprise, 2 for sadness, 3 for anger, 4 for disgust, and 5 for fear). For implementation, we create a script for generating CSV files to store all the image names and their corresponding labels to later efficiently pass to the model. The images together with the CSV file are loaded and preprocessed for training. Therefore, we manipulate the pixel data through resizing, normalization, augmentation, and conversion to grayscale with three channels at 64 × 64 resolution in the JPG format, as our original CNN is designed to work with three-channel inputs. Typically, we assume that the color of the image does not affect the emotion classification.\n\nVideo Preprocessing. For testing our model, we establish standards for extracted frames from videos or live webcam streams. Each frame, extracted for testing, undergoes scanning for the region of interest, the face(s). To achieve this, we implement the CascadeClassifier [52] incorporating the Viola-Jones algorithm introduced by Viola and Jones  [51]  to power, in our case, face detection, since the Viola-Jones algorithm revolutionizes real-time object detection. As part of the video preprocessing, we run every frame through the FaceClassifier to detect our region of interest and crop the rectangle region. The cropped image is also preprocessed by resizing to 64 × 64 resolution, then converted to greyscale with three channels, after that, a tensor is generated and normalized. Every cropped image is evaluated through our model so that the classification can DATASET SPLIT # happiness # surprise # sadness # anger # disgust # fear # total images/videos RAF-DB  [23, 25]  Train   1 . Overview of the data statistics for each emotion class and the total number of images/videos in our experiment (Note that -denotes the unknown information as the total videos are recorded from the participants randomly acting out their emotions).\n\nbe further used for interpretation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "In order to compare the performance of our GIMEFIVE with other models, we replicate several state-of-the-art CNNs from scratch, which include Visual Geometry Group (VGG)  [44]  and Residual Network (ResNet)  [11]  on two FER benchmarks (i.e., RAF-DB and FER 2013), as well as FER GIMEFIVE (see Tab. 1 for details).\n\nVisual Geometry Group. Developed by Simonyan and Zisserman  [44] , the VGG model represents a milestone in the evolution of CNNs for image recognition tasks. Distinct for its simple architecture, the VGG employs a series of convolutional layers with small 3 × 3 convolution filters, followed by max-pooling layers, fully connected layers, and a softmax layer. This designed option allows the VGG to learn more complex patterns through the depth to 16-19 weight layers. We adapt the VGG by adding batch normalization and dropout to stabilize the learning process. Previous work has shown that the VGG model is effective in classifying large-scale facial emotions, as the representations generalize well to various datasets, where they achieve state-of-the-art results, confirming the importance of depth in visual representations.\n\nResidual Network. In principle, deeper neural networks with more parameters are more difficult to train. ResNet has emerged as a groundbreaking architecture, significantly advancing the performance on various tasks including image classification, object detection, and semantic segmentation. ResNets leverage the power of deep residual learning to achieve remarkable accuracies on benchmark datasets such as ImageNet.\n\nInspired by He et al.  [11] , we implement the ResNet18 and ResNet34 to ease the training process. ResNet18 and ResNet34 containing 18 and 34 layers respectively are specific configurations of the ResNet architecture, which address the vanishing/exploding gradient problem associated with training very deep neural networks through the introduction of residual blocks. These residual blocks allow for the direct propagation of gradients through the network by adding the input of the block to its output, effectively enabling the training of networks that are much deeper than previous architectures. GIMEFIVE. Our model architecture is illustrated in Figure  3  (see Fig.  8  for details). The input of our emotion recognition model is an image with 3 channels at 64 × 64 resolution. The output is six emotion classes. We implement an emotion classifier from scratch with four convolution blocks as our baseline at the very beginning. Despite the larger kernel being able to provide more information and a wider area view due to more parameters, we use a 3 × 3 kernel size for all convolutional layers, which is the same as the VGG model, since it is efficient to train and share the weights without expensive computation. Following each convolutional layer, batch normalization is used for stabilizing the learning by normalizing the input to each layer. Also, the batch normalization ensures forward propagated signals have non-zero variances.\n\nThe convolution stride is fixed to 1 pixel. We interleave with the max pooling layer because it reduces the spatial dimensions of the input volume. Afterward, three linear layers are applied to extract features to the final output. We also add a 50% dropout layer between the first and second linear layers to prevent overfitting. The activation function after each layer is Rectified Linear Unit (ReLU), since it introduces the non-linearity into the model, allowing it to learn more complex patterns. Note that our best model GIME-FIVE is later optimized with the dropout rate at 20% after max pooling layers.\n\nThe softmax function plays a critical role in the final stage of a neural network, which corresponds to different classes by exponentiating each logit and then normalizing these values by the sum of all exponentiated logits. Mathematically, for a given logit z i among n classes, the softmax function S(z i ) is defined as:\n\nwhere the true class has probability 1 and all others have probability 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "For evaluation, we use the metric accuracy to see if our model can classify facial emotions correctly. We report all the training, testing, and validation accuracies in % to compare the performance of our GIMEFIVE with other stateof-the-art methods. By converting logits to probabilities, the softmax function allows for a more interpretable output from the model and facilitates the use of probability-based loss functions. The loss function employed for all models is cross-entropy (CE), which is typically for multi-class classification during training. The CE loss is given by:\n\nwhere y i is the true label and p i is the predicted probability of the i-th class. Here n denotes the total number of classes, in our case, 6.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Results",
      "text": "As seen in Table  2 , the results in terms of accuracies are aggregated from the database RAF-DB  [7]  and FER2013  [39] , as well as the FER GIMEFIVE. We observe that our models achieve the best performance on RAF-DB instead of on our aggregated FER GIMEFIVE. We assume that the reason for this is that the RAF-DB dataset is more balanced, while the FER2013 dataset is more challenging due to the imbalance of the classes. Also verified on the test and validation set, the performance of all our models on FER2013 is worse than on the RAF-DB.\n\nAt the same time, we can see that there is a trade-off between accuracy and the number of parameters. In general, deeper CNNs with more layers and parameters tend to perform better, as they can capture more nuanced facial patterns and features. However, in our experiments, this is not always the case. We hypothesize that more parameters often mean greater learning capacity, but also a higher risk of overfitting and increased computational cost. The models are then too complex and need more resources and time to train. Therefore, we propose our best model GIMEFIVE, the one with 15 layers and 10478086 parameters, which is seven times less than the VGG and half of ResNet34, efficiently achieving the best performance on the both test and validation set on all benchmarks.\n\nIn terms of the implementation details, our GIME-FIVE (marked as a green asterisk ⋆) without random augmentation outperforms the other methods in terms of accuracy, indicating that this kind of random augmentation is not necessarily able to help our model predict the correct label, one reason is that the images on RAF-DB are already aligned and cropped, the RandomHorizontalFlip, RandomRotation, RandomCrop, and RandomErasing might confuse the model by adding more noise to aligned facial images. We further continue to simply apply merely normalization to detect more representative features with respect to different emotions. Previous research from Li et al.  [25] , Verma et al.  [50] , Zeiler and Fergus  [56]  engage similar investigations.\n\nWe run extensive experiments to determine the number of convolutional blocks for a better trade-off between the number of parameters and the performance. Eventually, we find that adding an extra convolutional block (5 blocks with 15 layers in total) from our initial baseline (4 blocks with 13 layers) leads to the best performance. However, six convolutional blocks with 17 layers do not necessarily result in better performance, this is due to the complexity and overfitting of the models. Moreover, we find that the dropout and batch normalization can indeed improve the performance of the model, as such regularization techniques can prevent overfitting by randomly dropping units (along with their connections) from the neural networks during training. Also, batch normalization can help the model reduce the number of training epochs, leading to faster learning progress and thus improving the generalization ability of the model. Unfortunately, the squeeze and excitation block  [12] , a mechanism that adaptively recalibrates channel-wise attention feature re- Table  2 . Accuracies (%) for different models with specific architectures and numbers of parameters and layers in our experiments (Note that ⋆ denotes our best GiMeFive; BN stands for the batch normalization, RB for residual block, SE for the squeeze and excitation block, and DO for dropout; +/-represent with/without respectively). The confusion matrix of the validation set in terms of the number of classified images is shown in Fig.  4 , while the result of the test set on the RAF-DB is given in Fig.  7 .\n\nsponses by explicitly modeling interdependencies between channels, did not improve the performance in this task. One possible interpretation is that emotion classification might benefit more from spatial relationships rather than channelwise dependencies. Notably, ResNet18 is appreciated for its efficiency and lower computational cost, and ResNet34 for offering a balance between depth and performance, making them widely adopted in both academic research and practical applications, achieving good performance on two FER benchmarks in our empirical experiment as well. However, due to the lack of further optimization techniques, they did not significantly outperform our best model GIMEFIVE. We agree that the innovative approach of using residual connections has not only improved the performance of deep neural networks but also inspired a plethora of subsequent research efforts for informative facial feature detections.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Interpretable Results",
      "text": "Classification Scores. To further analyze the probability of each individual image, we write a script that takes a folder path as input and iterates through the images inside a subfolder to record the performance of our best model GIMEFIVE. This CSV file is represented with the corresponding classification scores via the softmax function (see Eq. (  1 )). To have a better review of the total correctly classified images, we also generate a confusion matrix heatmap (see Fig.  4 ) to visualize the performance of the model with respect to each emotional class, where the numbers in the diagonal of this heatmap represent the number of correct predicted images on the validation set. Each line has a sum of 100, as our validation set contains one hundred images with the ground-truth label per class (see Tab. 1).\n\nAs shown in Figure  4 , we observe that our GIMEFIVE can predict the happiness class with the highest accuracy, while the class with the lowest accuracy is disgust. This is understandable since facial emotions such as sadness and anger are similar to disgust, especially when humans sometimes also represent disgusting and angry emotions at the same time. Roughly speaking, happiness is the largest subclass in our training data (see Tab. 1) and the happy emotion is the easiest to detect in principle. Moreover, the confusion matrix illustrates that the model has good performances in terms of classifying surprise, sadness, and anger as well. Overlay Grad-CAM. To understand the decisionmaking process of our model, we aim to explain our model in a more transparent and interpretable way using Grad-CAM  [42] , as Grad-CAM is a generalization of CAM and helps interpret CNN decisions by providing visual cues about the regions that influenced the classification. Grad-CAM highlights the important regions of an image, aiding in the understanding of the behavior of the model, which is especially useful for model debugging and further improvement.  Besides proposing a method to visualize the discriminative regions of a CNN trained for the classification task, we adopt this approach from Zhou et al.  [58]  to localize objects without providing the model with any bounding box anno-tations. The model can therefore learn the classification task with class labels and is then able to localize the object of a specific class in an image. In our case, we leverage the last (i.e., fifth) convolutional layer of our GIMEFIVE to generate the Grad-CAM heatmap. The global average pooling (GAP) layer is utilized to compute the average value of each feature map, resulting in a spatial average of feature maps.\n\nAs seen in Figure  5 , the heatmap in the middle represents the results of our best model GIMEFIVE on the validation set, showing different areas of the face to identify each emotion. The warmer colors (red and yellow) indicate the higher activation or relevance in recognizing facial emotions, while the cooler colors (blue and green) indicate the lower activation. The scale ranges from 0 to 1, indicating the level of activation or confidence in emotion recognition, where 1 means the highest measured activation.\n\nThe right side of each sub-figure is a result of the overlay Grad-CAM. For instance, we can see that the happiness and sadness (see Fig.  5a  and Fig.  5c  respectively) classes show a high level of activation around the mouth region, which is common in uplifting and drooping corners. Surprise (see Fig.  5b ) implies activation around the eyes and eyebrows, which are typically raised in a look in human faces. Other emotions such as anger, disgust, and fear (see Fig.  5d , Fig.  5e , and Fig.  5f  respectively) represent entire distributions, highlighting the complexity of facial expressions from our models.\n\nOverlay Landmark. The GIMEFIVE classifications of every frame are plotted in percentage next to the saliency map  [45] . The most important emotion, extracted from every fifth frame, is displayed above the map to make it easier for our eyes to follow. To plot landmarks on each video frame, we implemented the pre-trained dlib shape predictor, shape predictor 68 face landmarks.dat, constructed using the classic Histogram of Oriented Gradients (HOG)  [6]  feature combined with a linear classifier, an image pyramid, and a sliding window detection scheme  [47] . The pose estimator was created using the dlib from Kazemi and Sullivan  [15] .\n\nWith this pre-trained model, 68 numbered landmarks assist the emotion analysis and facial expression tracking to understand and interpret facial features. Again, illustrated in Figure  6 , happiness emotions (see Figure  6a ) are the easiest to detect in principle.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Optimization Strategies",
      "text": "Data Augmentation. In deep learning and AI, data augmentation  [43]  stands as a transformative technique, empowering algorithms to learn from and adapt to a wider range of data. This is typically beneficial when the dataset is limited, by introducing subtle modifications to existing data points, augmentation effectively expands the dataset,  enabling models to generalize better and achieve enhanced performance. Preliminary, we create various replications of existing photos by randomly altering different properties such as size, brightness, color channels, or perspectives on the RAF-DB dataset. The results, however, are not as expected since aligned and cropped images do not have significant advantages in helping our GIMEFIVE. In the future, we believe this technique can be more effective when applied to the other state-of-the-art models on real-world variations, which can slightly encounter altered versions of familiar data, making more nuanced and robust predictions.\n\nHyperparameter Tuning. To understand and enhance the performance of the model during training and find the best hyperparameter configuration (see Tab. 3 for details) of the model, we utilize the parameter grid  [19] . As a result, The learning rate for all our GIMEFIVE models is set to 0.001 with the stochastic gradient descent optimizer. The momentum = 0.9, meanwhile, we apply the weight decay = 1 × 10 -4 . Additionally, we increase the depth of the network by adding some convolutional layers to learn more complex features. To help the training of deeper neural networks more efficiently, we add the residual connections, as they allow gradients to flow through the network more easily, improving the training for deep architectures. As verified by Barsoum et al.  [1] , the dropout layers are effective in avoiding model overfitting. Potential, more advanced optimization techniques such as random search, bayesian optimization, and genetic algorithms are also able to be further explored to find the best hyperparameters for our GIMEFIVE.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "In this work, we propose a novel emotion classifier GIME-FIVE for facial expression analysis, offering novel insights and methodologies for enhancing the interpretability and accuracy of facial emotion classifications. We run extensive experiments on two FER benchmarks and a five-dataset aggregated FER GIMEFIVE to evaluate the performance of our models, finding that our best model GIMEFIVE outperforms other state-of-the-art models with both efficiency and explainability. In addition, we also provide several demonstration videos and live web camera streams to illustrate the interpretability of our model. We hope that our work can inspire future research in the field of facial emotion classification and multimodal analysis.\n\nLimitations. We acknowledge that our work has several constraints in terms of the accuracy of the models on the leaderboard. The limited time frame allocated for the study might have influenced both the depth and the breadth of the investigation. We believe that the performance of our model can be further improved by incorporating more advanced optimization techniques and loss functions.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Test accuracies (%) of our GIMEFIVE compared to other",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of the experimental pipeline.",
      "page": 2
    },
    {
      "caption": "Figure 2: The evaluation results of our models are",
      "page": 2
    },
    {
      "caption": "Figure 1: Section 5 de-",
      "page": 2
    },
    {
      "caption": "Figure 8: for details). The input of our emotion",
      "page": 4
    },
    {
      "caption": "Figure 3: Overview of the GIMEFIVE model architecture (see Fig. 8 for a detailed version).",
      "page": 5
    },
    {
      "caption": "Figure 4: , while the result of the test set on the RAF-DB is given in Fig. 7.",
      "page": 6
    },
    {
      "caption": "Figure 4: ) to visualize the performance of the model with",
      "page": 6
    },
    {
      "caption": "Figure 4: , we observe that our GIMEFIVE",
      "page": 6
    },
    {
      "caption": "Figure 4: Overview of the confusion matrix evaluated from our",
      "page": 7
    },
    {
      "caption": "Figure 5: Interpreting images with all six facial emotional classes",
      "page": 7
    },
    {
      "caption": "Figure 5: , the heatmap in the middle repre-",
      "page": 7
    },
    {
      "caption": "Figure 5: a and Fig. 5c respectively) classes",
      "page": 7
    },
    {
      "caption": "Figure 5: b) implies activation around the eyes and",
      "page": 7
    },
    {
      "caption": "Figure 5: d, Fig. 5e, and Fig. 5f respectively) represent entire",
      "page": 7
    },
    {
      "caption": "Figure 6: , happiness emotions (see Figure 6a) are the eas-",
      "page": 7
    },
    {
      "caption": "Figure 6: Interpreting video screenshots with grad-CAM and landmarks evaluated from our GIMEFIVE on DISFA dataset.",
      "page": 8
    },
    {
      "caption": "Figure 7: Overview of the confusion matrix on the test set of RAF-",
      "page": 11
    },
    {
      "caption": "Figure 8: Overview of our detailed model architecture.",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Overview of the data statistics for each emotion class and the total number of images/videos in our experiment (Note that - denotes",
      "page": 4
    },
    {
      "caption": "Table 2: , the results in terms of accuracies are ag-",
      "page": 5
    },
    {
      "caption": "Table 2: Accuracies (%) for different models with specific architectures and numbers of parameters and layers in our experiments (Note",
      "page": 6
    },
    {
      "caption": "Table 3: Explored hyperparameter space for our models.",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton-Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2008",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016"
    },
    {
      "citation_id": "2",
      "title": "Face recognition with learning-based descriptor",
      "authors": [
        "Zhimin Cao",
        "Qi Yin",
        "Xiaoou Tang",
        "Jian Sun"
      ],
      "year": "2010",
      "venue": "The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010"
    },
    {
      "citation_id": "3",
      "title": "Grad-cam++: Improved visual explanations for deep convolutional networks",
      "authors": [
        "Aditya Chattopadhay",
        "Anirban Sarkar",
        "Prantik Howlader",
        "Vineeth N Balasubramanian"
      ],
      "year": "2018",
      "venue": "2018 IEEE winter conference on applications of computer vision (WACV)"
    },
    {
      "citation_id": "4",
      "title": "Taiwanese facial expression image database",
      "authors": [
        "L Chen",
        "Y Yen"
      ],
      "year": "2007",
      "venue": "Taiwanese facial expression image database"
    },
    {
      "citation_id": "5",
      "title": "Principle component analysis and its variants for biometrics",
      "authors": [
        "Tsuhan Chen",
        "Jessie Hsu",
        "Xiaoming Liu",
        "Wende Zhang"
      ],
      "year": "2002",
      "venue": "Proceedings of the 2002 International Conference on Image Processing"
    },
    {
      "citation_id": "6",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
    },
    {
      "citation_id": "7",
      "title": "For recognize emotion from facial expression",
      "authors": [
        "Raf-Db Dev-Shuvoalok",
        "Dataset"
      ],
      "year": "2023",
      "venue": "For recognize emotion from facial expression"
    },
    {
      "citation_id": "8",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "Adcorre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "Pourramezan Ali",
        "Mohammad Fard",
        "Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "Ian Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2006",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Samuel Albanie",
        "Gang Sun",
        "Enhua Wu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "13",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "Deepak Kumar Jain",
        "Pourya Shamsolmoali",
        "Paramjit Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognit. Lett"
    },
    {
      "citation_id": "14",
      "title": "Blocks that shout: Distinctive parts for scene classification",
      "authors": [
        "Mayank Juneja",
        "Andrea Vedaldi",
        "C Jawahar",
        "Andrew Zisserman"
      ],
      "year": "2013",
      "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "Vahid Kazemi",
        "Josephine Sullivan"
      ],
      "year": "2014",
      "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "Byoungchul Ko"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "ABAW: valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023 -Workshops"
    },
    {
      "citation_id": "18",
      "title": "Attribute and simile classifiers for face verification",
      "authors": [
        "Neeraj Kumar",
        "Alexander Berg",
        "Peter Belhumeur",
        "Shree Nayar"
      ],
      "year": "2009",
      "venue": "IEEE 12th International Conference on Computer Vision, ICCV 2009"
    },
    {
      "citation_id": "19",
      "title": "Scikit learn developers",
      "year": "2024",
      "venue": "Scikit learn developers"
    },
    {
      "citation_id": "20",
      "title": "Labeled faces in the wild: A survey. Advances in face detection and facial image analysis",
      "authors": [
        "Erik Learned-Miller",
        "Gary Huang",
        "Aruni Roychowdhury",
        "Haoxiang Li",
        "Gang Hua"
      ],
      "year": "2016",
      "venue": "Labeled faces in the wild: A survey. Advances in face detection and facial image analysis"
    },
    {
      "citation_id": "21",
      "title": "Backpropagation applied to handwritten zip code recognition",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1989",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "22",
      "title": "Probabilistic elastic matching for pose variant face verification",
      "authors": [
        "Haoxiang Li",
        "Gang Hua",
        "Zhe Lin",
        "Jonathan Brandt",
        "Jianchao Yang"
      ],
      "year": "2013",
      "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "25",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2006",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Towards east asian facial expression recognition in the real world: A new database and deep recognition baseline",
      "authors": [
        "Shanshan Li",
        "Liang Guo",
        "Jianya Liu"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "27",
      "title": "Learning deep features via congenerous cosine loss for person recognition",
      "authors": [
        "Yu Liu",
        "Hongyang Li",
        "Xiaogang Wang"
      ],
      "year": "2017",
      "venue": "Learning deep features via congenerous cosine loss for person recognition",
      "arxiv": "arXiv:1702.06890"
    },
    {
      "citation_id": "28",
      "title": "The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2010"
    },
    {
      "citation_id": "29",
      "title": "Visualizing deep convolutional neural networks using natural pre-images",
      "authors": [
        "Aravindh Mahendran",
        "Andrea Vedaldi"
      ],
      "year": "2016",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "30",
      "title": "Towards interpretable facial emotion recognition",
      "authors": [
        "Sarthak Malik",
        "Puneet Kumar",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "ICVGIP '21: Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "31",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "Seyed Mohammad Mavadati",
        "Mohammad Mahoor",
        "Kevin Bartlett",
        "Philip Trinh",
        "Jeffrey Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "32",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "authors": [
        "Anh Mai Nguyen",
        "Jason Yosinski",
        "Jeff Clune"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015"
    },
    {
      "citation_id": "34",
      "title": "The building blocks of interpretability",
      "authors": [
        "Chris Olah",
        "Arvind Satyanarayan",
        "Ian Johnson",
        "Shan Carter",
        "Ludwig Schubert",
        "Katherine Ye",
        "Alexander Mordvintsev"
      ],
      "year": "2018",
      "venue": "Distill"
    },
    {
      "citation_id": "35",
      "title": "Face space representations in deep convolutional neural networks",
      "authors": [
        "J O' Alice",
        "Carlos Toole",
        "Connor Castillo",
        "Matthew Parde",
        "Rama Hill",
        "Chellappa"
      ],
      "year": "2018",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "36",
      "title": "Human-debugging of machines",
      "authors": [
        "Devi Parikh",
        "C Zitnick"
      ],
      "year": "2011",
      "venue": "NIPS WCSSWC"
    },
    {
      "citation_id": "37",
      "title": "Facial expression recognition using residual masking network",
      "authors": [
        "Luan Pham",
        "The Vu",
        "Tuan Anh"
      ],
      "year": "2020",
      "venue": "25th International Conference on Pattern Recognition, ICPR 2020"
    },
    {
      "citation_id": "38",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "39",
      "title": "Fer-2013: Learn facial expressions from an image",
      "authors": [
        "Manas Sambare"
      ],
      "year": "2013",
      "venue": "Fer-2013: Learn facial expressions from an image"
    },
    {
      "citation_id": "40",
      "title": "Hsemotion: High-speed emotion recognition library",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Softw. Impacts"
    },
    {
      "citation_id": "41",
      "title": "Facial expressions training data: Fer affectnet database",
      "authors": [
        "Noam Segal"
      ],
      "venue": "Facial expressions training data: Fer affectnet database"
    },
    {
      "citation_id": "42",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Ramprasaath",
        "Michael Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2017"
    },
    {
      "citation_id": "43",
      "title": "A survey on image data augmentation for deep learning",
      "authors": [
        "C Shorten",
        "T Khoshgoftaar"
      ],
      "year": "2019",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "44",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "45",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014"
    },
    {
      "citation_id": "46",
      "title": "Unsupervised discovery of mid-level discriminative patches",
      "authors": [
        "Saurabh Singh",
        "Abhinav Gupta",
        "Alexei Efros"
      ],
      "year": "2012",
      "venue": "Computer Vision -ECCV 2012 -12th European Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Learning hierarchical models of scenes, objects, and parts",
      "authors": [
        "Erik Sudderth",
        "Antonio Torralba",
        "William Freeman",
        "Alan Willsky"
      ],
      "year": "2005",
      "venue": "10th IEEE International Conference on Computer Vision (ICCV 2005)"
    },
    {
      "citation_id": "48",
      "title": "The nimstim set of facial expressions: Judgments from untrained research participants",
      "authors": [
        "Nim Tottenham",
        "James Tanaka",
        "Andrew Leon",
        "Thomas Mccarry",
        "Marcella Nurse",
        "Todd Hare",
        "David Marcus",
        "Alissa Westerlund",
        "Bj J Casey",
        "Charles Nelson"
      ],
      "year": "2009",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "49",
      "title": "Efficient neural architecture search for emotion recognition",
      "authors": [
        "Monu Verma",
        "Murari Mandal",
        "M Satish Kumar Reddy",
        "Yashwanth Reddy Meedimale",
        "Santosh Vipparthi"
      ],
      "year": "2023",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "50",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, pages I-I"
    },
    {
      "citation_id": "51",
      "title": "Towards fairness in visual recognition: Effective strategies for bias mitigation",
      "authors": [
        "Zeyu Wang",
        "Klint Qinami",
        "Christos Ioannis",
        "Kyle Karakozis",
        "Prem Genova",
        "Kenji Nair",
        "Olga Hata",
        "Russakovsky"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020"
    },
    {
      "citation_id": "52",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Yandong Wen",
        "Kaipeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "53",
      "title": "Towards interpretable face recognition",
      "authors": [
        "Bangjie Yin",
        "Luan Tran",
        "Haoxiang Li",
        "Xiaohui Shen",
        "Xiaoming Liu"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)"
    },
    {
      "citation_id": "54",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "Matthew Zeiler",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "Computer Vision -ECCV 2014 -13th European Conference"
    },
    {
      "citation_id": "55",
      "title": "Adaptive deconvolutional networks for mid and high level feature learning",
      "authors": [
        "Matthew Zeiler",
        "Graham Taylor",
        "Rob Fergus"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2011"
    },
    {
      "citation_id": "56",
      "title": "Learning deep features for discriminative localization",
      "authors": [
        "Bolei Zhou",
        "Aditya Khosla",
        "Àgata Lapedriza",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Overview of the confusion matrix on the test set of RAF-DB",
      "venue": "Overview of the confusion matrix on the test set of RAF-DB"
    }
  ]
}