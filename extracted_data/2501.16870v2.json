{
  "paper_id": "2501.16870v2",
  "title": "Experimenting With Affective Computing Models In Video Interviews With Spanish-Speaking Older Adults",
  "published": "2025-01-28T11:42:15Z",
  "authors": [
    "Josep Lopez Camunas",
    "Cristina Bustos",
    "Yanjun Zhu",
    "Raquel Ros",
    "Agata Lapedriza"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding emotional signals in older adults is crucial for designing virtual assistants that support their wellbeing. However, existing affective computing models often face significant limitations: (1) limited availability of datasets representing older adults, especially in non-English-speaking populations, and (2) poor generalization of models trained on younger or homogeneous demographics. To address these gaps, this study evaluates state-ofthe-art affective computing models-including facial expression recognition, text sentiment analysis, and smile detection-using videos of older adults interacting with either a person or a virtual avatar. As part of this effort, we introduce a novel dataset featuring Spanish-speaking older adults engaged in human-to-human video interviews. Through three comprehensive analyses, we investigate (1) the alignment between human-annotated labels and automatic model outputs, (2) the relationships between model outputs across different modalities, and (3) individual variations in emotional signals. Using both the Wizard of Oz (WoZ) dataset and our newly collected dataset, we uncover limited agreement between human annotations and model predictions, weak consistency across modalities, and significant variability among individuals. These findings highlight the shortcomings of generalized emotion perception models and emphasize the need of incorporating personal variability and cultural nuances into future systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "There is growing interest in developing virtual assistants and social assistive robots for older adults, as these technologies can encourage social interaction and promote wellbeing  [2-4, 6, 15, 26] . For these systems to effectively engage older adults, they must be capable of perceiving emotional signals and personalizing interactions to accommodate the unique needs of this demographic  [27, 34] .\n\nDespite advancements in emotion perception technologies for human-robot interaction  [20, 32] , there remains a significant gap in research focused on older adults. This demographic is severely underrepresented in datasets used to train and evaluate affective computing models  [36] , particularly older adults from non-English-speaking populations. As a result, current models often fail to generalize to elderly, leaving their emotional signals poorly understood or inaccurately interpreted. Addressing this gap is crucial, as older adults exhibit unique emotional and behavioral patterns influenced by age-related changes and cultural differences.\n\nIn this study, we investigate the performance of stateof-the-art facial emotion, text sentiment and smile recognition systems when applied to older Spanish-speaking adults, a group underrepresented in both affective computing research and publicly available datasets. To this end, we analyze emotional signals using two datasets: the Wizard of Oz (WoZ) dataset  [15] , which contains videos of older adults interacting with a virtual avatar, and a novel dataset we collected featuring human-to-human interviews with the same demographic. By introducing this new dataset, we provide a more diverse data distribution to enable broader and more generalizable insights into the emotional expressions of older Spanish-speaking adults.\n\nOur work focuses on three key areas of analysis: the alignment between human-annotated labels and automatic model outputs, the relationship between facial emotion signals and speech sentiment, and the impact of individual differences in facial movements and sentiment expression. For this purpose, we leverage state-of-the-art, off-theshelf models for facial expression recognition, smile detection, and text sentiment classification. An overview of our methodology is presented in Fig.  1 , with a detailed description provided in Sec.  4 .\n\nOur findings reveal several challenges in applying generalized emotion perception models to this demographic. Specifically, we observed limited agreement between human annotations and model predictions and significant variability across individuals in facial movements and sentiment",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Small Data Statement",
      "text": "This work focuses on analyzing the performance and output alignment of state-of-the-art affective computing models when applied to older adults, with a particular emphasis on Spanish-speaking populations. These populations are notably underrepresented in the development of machine learning models, which has resulted in systems that often fail to capture their unique emotional and behavioral patterns effectively. Studying this group is important because older adults have distinct needs and behaviors that differ significantly from younger or general adult populations. Factors such as age-related changes in facial expressions, speech patterns, and emotional dynamics necessitate tailored approaches for accurate emotion perception. Furthermore, Spanish-speaking older adults represent a significant portion of the global population, yet their cultural and linguistic nuances are seldom reflected in current datasets or models, which tend to focus on English-speaking demographics. Ensuring that affective computing models work well across all demographic groups, including underrepresented groups like the one studied in this paper, is critical for equitable technological advancements. If these systems are to support human well-being, they must accurately understand and respond to the emotional signals of diverse users. Failing to address these gaps risks perpetuating bias and excluding vulnerable populations from the benefits of such technologies. By focusing on this underrepresented group, the study contributes to developing more inclusive, robust, and effective affective computing systems that cater to the diverse needs of a global user base.\n\nThe goal of this work is to evaluate the performance of state-of-the-art affective computing models on Spanishspeaking older adults, an underrepresented demographic in existing datasets. To address this gap, we relied on the WoZ dataset and, complemented it by collecting a novel dataset to enable broader and more diverse analysis. Our findings reveal limited alignment between human-annotated emotional labels and model outputs, weak agreement between outputs from different modalities, as well as significant variability in facial movements and text sentiment expression across individuals. These results highlight the limitations of generalized models in capturing age-and culture-specific nuances, underscoring the need for inclusive approaches that account for such variability. By identifying these shortcomings, this study contributes to developing robust and equitable emotion perception systems that better serve diverse populations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Interactive technologies, such as virtual agents and social robots, have demonstrated significant benefits in promoting older adults' health and social connection  [40] . These technologies feature non-intrusive, emotionally expressive virtual coaches or social robots that encourage users to adopt healthier lifestyles by improving nutritional habits, promoting physical activity, and facilitating social interactions. By fostering these behaviors, they help seniors reduce the risk of chronic diseases, enabling independent and fulfilling lives while also supporting their caregivers  [6] . Augmented and virtual reality technologies have also been explored to enrich older adults' experiences, including enhancing digital learning  [13] , improving long-term health interventions  [17] , and supporting overall well-being  [38] . Social robots, in particular, have shown promise in providing high-quality care and social support. They have been studied for understanding their roles  [2] , exploring stakeholder experiences with these systems  [1] , and demonstrating their effectiveness in improving older adults' well-being  [16] .\n\nResearch has investigated the generation of empathic behaviors in social robots using estimated affective states  [5, 24, 25] . However, these studies primarily focus on general populations and do not specifically address elderly individuals. Recent work has emphasized the need for actively measuring engagement to drive human-robot interactions. For example, Zhang et al.  [39]  proposed a supervised machine learning approach to estimate the engagement states of older adults in multiparty human-robot interactions. Their method leverages pre-trained models to extract behavioral, affective, and visual signals, achieving effective engagement estimation by incorporating inputs from participants within the interaction group. Specific robots tailored for elderly populations include the Daru-maTO robot  [8] , designed for Japanese-speaking users. It avoids camera use for privacy, relying instead on Speech Emotion Recognition (SER) through CNN and LSTM models to generate suitable facial responses. Similarly, Ryan  [3] , a socially assistive robot for individuals with depression and dementia, uses multimodal emotion perception algorithms, including Residual Neural Networks (ResNet) for Facial Expression Recognition (FER) and natural language processing for SER. While empathic robots like Ryan have been perceived as more engaging and likable, the reported improvements were not statistically significant.\n\nRecent advancements in deep learning methods have enabled multimodal approaches to emotion perception for older adults. Warnants et al.  [37]  introduced a digital platform for health monitoring, companionship, and emotional support. This platform combines SER with CNN-LSTM architectures to facilitate personalized care, highlighting the potential of machine learning to enhance emotional understanding and interaction in elderly populations. Despite these advances, datasets specifically targeting emotion estimation in older adults remain limited. ElderReact  [18]  is a multimodal dataset featuring over 1K annotated video clips of emotional responses from 40 aging participants. It includes visual and audio features, enabling analysis of which modality is more relevant for specific emotions. Unfortunately, this dataset was not accessible for our study. The WoZ dataset  [15] , part of the EMPATHIC project  [6] , offers a unique resource by providing interview-style video clips of older adults interacting with a simulated virtual agent. Palmero et al.  [26]  leveraged this dataset to explore nonverbal emotion expression recognition, applying deep learning models to integrate facial expressions, speech, gaze, and head movements. Their work identified emotional states such as puzzled, calm, and pleased. While previous research has advanced emotion perception in elderly, most studies focus on generalized models and fail to address individual variability or cultural nuances. Our work addresses these gaps by analyzing the alignment between human annotations and model outputs, exploring multimodal relationships, and assessing individual differences.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "This research relies on two datasets focused on understanding emotional signaling in older adults, particularly among Spanish-speaking individuals. The first is the publicly available WoZ dataset, which examines interactions with virtual coaches. The second is the Short Interviews dataset, a resource we developed to provide insights into natural, human-to-human interactions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Woz Dataset",
      "text": "The WoZ (Wizard of Oz) dataset  [15]  is designed to study how older adults interact with virtual coaches in the context of healthy aging. The data collection includes audio and video recordings, alongside dialogue transcriptions from sessions where seniors interacted with a simulated virtual coach. The dataset includes multimodal data-audio, video, and text transcriptions-annotated separately for emotional states across each modality.\n\nFor this study, we focused on a subset of 78 Spanishspeaking participants, consisting of 24 men and 54 women, all aged 65 and above. These individuals were selected from a larger group of 153 recruited through different community centers. Each participant took part in two structured sessions based on the GROW coaching model  [22] , discussing topics such as leisure and nutrition. These sessions produced 4,529 audio-video segments annotated in different modalities like audio, text transcription and facial expressions. In this work, we use the emotional labels of speech transcription (positive, neutral and negative) and facial expressions (neutral, happy, pensive and surprise). Neutral facial expressions were the most common, which aligns with the natural flow of conversations in this context. Annotations of facial expressions in video were carried out by two independent annotators, and any disagreements were resolved collaboratively to ensure reliable labeling. Further details on the data collection procedure and label distribution can be found in  [15, 26] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Our Short Interviews Dataset",
      "text": "The Short Interviews dataset was created to feature older adults engaging in natural, human-to-human conversations. It captures semi-structured interviews between participants and a human interviewer, providing a more organic setting for studying emotional responses. This dataset includes 16 participants (13 women and 3 men) aged 62 and older (average age 77.3 years, standard deviation 7.9). Recruitment took place at a healthcare center offering group sessions focused on reducing social isolation among the elderly. Participants joined the study voluntarily after receiving detailed information about its purpose and procedures. Written informed consent was obtained from all participants, following ethical guidelines.\n\nThe interviews were conducted in a private, quiet room to ensure both comfort and high-quality recordings. Two cameras were used: one positioned on a table to capture a frontal view (recording at 1080p@30fps), and another placed laterally on a tripod (recording at 2160p@30fps). Audio was recorded with a cardioid microphone placed close to the participant to ensure clarity. The discussions covered several neutral topics, including food, music, and books, which were chosen to engage participants and elicit a broad range of emotional responses. This approach resulted in 3.5 hours of video, divided into 107 clips, 43 of which were focused on eliciting mild emotional reactions. While manual annotations have not yet been completed, the dataset provides a solid basis for developing emotion perception models. In contrast to the WoZ dataset-which provides structured, annotated data centered on interactions with a virtual coach-the Short Interviews dataset captures the spontaneity of human-to-human interactions, offering a more natural view of emotional responses. By combining these resources, we can explore both controlled and organic emotional dynamics.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "Our analysis focuses on both verbal (speech transcriptions) and non-verbal (facial expressions and smile) behavioral signals in dyadic interactions. We use front-facing camera recordings that provide detailed views of participants' faces, along with their speech transcriptions. We employ three main components: smile detection, facial expression recognition, and text sentiment analysis. Additionally, we describe our methodology for aligning these different modalities temporally to enable cross-modal analysis.\n\nSmile Detection. Smiles are one of the most significant affective signals in human interactions  [9, 14, 19, 23] , serving as key indicators of emotional states and social engagement. To detect smiles in video sequences, we finetune a Transformer-based video architecture using MAR-LIN  [7]  as the backbone. The model takes a sequence of facial crops as input, extracted using the face detector from  [10, 11] . We trained and tested the model on the CelebV-HQ dataset  [41] , achieving 82% accuracy (F1: 82%) on training and 77% accuracy (F1: 73%) on test sets. The model processes 16-frame sequences and outputs a smile intensity score between 0 and 1. Qualitative evaluation using live webcam sequences highlighted the need to distinguish smiles from speech-related mouth movements. We set the detection threshold to 0.85 based on qualitatively analyzing smile intensities in these sequences, enabling robust separation of smiles from speech articulation while capturing both brief and prolonged smile expressions.\n\nFacial Expression Recognition. For broader emotion analysis, we employ EfficientNet  [33]  and MobileNet  [12]  models trained on AffectNet  [21] , following Savchenko et al.  [31] . These models classify eight expressions: Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness, and Surprise, achieving 63.03% accuracy across all 8 categories and 66.34% when focusing on 7 categories. The models process the same facial crops used for smile detection, providing frame-level expression predictions.\n\nSpeech Transcription and Text Sentiment Analysis. Since the collected Short Interviews dataset lacks humanannotated speech transcriptions, we employed OpenAI's Whisper model  [30]  to transcribe the interviews for this dataset, extracting segmented utterances along with their timestamps. Additionally, we manually annotated the speaker of each utterance to focus exclusively on participant responses. For the WoZ dataset, we used human-annotated speech transcriptions and sentiment labels already included in the dataset. Once the transcriptions were obtained for both datasets, we applied text sentiment analysis using pysentimiento  [29] , a Python toolkit for social NLP tasks. The sentiment analysis for Spanish leverages RoBERTuito  [28] , a state-of-the-art language model trained on Spanish social media text. RoBERTuito produces probabilities for each sentiment class (Positive, Negative, or Neutral), providing the likelihood of each sentiment for every text segment.\n\nTemporal Alignment. To analyze relationships between speech transcriptions and facial behaviors, we align signals using utterances as the temporal unit. For facial signal processing, we downsample video to 10fps to reduce redundancy. The alignment process handles different granularities: text sentiment at utterance-level, smile detection for 16-frame sequences, and facial expressions per-frame. Since facial signals occur more frequently than utterancelevel text sentiment, we oversample text sentiment to match facial signal frequency. For a sentiment score of 0.6 with smile events (0.86, 0.87, 0.865), we replicate the sentiment (0.6, 0.6, 0.6) to maintain correspondence and preserve facial dynamics.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "This section examines facial expressions, text sentiment, and smiles across human-agent and human-human interactions in the WoZ and Short Interviews datasets. Our analysis focuses on three aspects: the relationship between human annotations and model predictions, the correspondence between different modalities, and individual user variations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparing Human Annotations And Model Predictions",
      "text": "We studied the alignment between human-labeled annotations and automatic model predictions across two modalities: facial expressions and text sentiment in the WoZ dataset. To quantify the agreement, we generate a cooccurrence matrix that aggregates results for each category and highlights areas of alignment and disagreement.\n\nFacial Expression. Comparing human and model annotations reveals that the human annotation categories and automatic model's predefined classes do not share the same set of labels, as shown in Figure  2a , where annotators used non-standard and subjective categories, making direct comparisons challenging. While Happiness shows strong agreement across both annotation approaches, misclassifications arise when human annotators used more subjective categories. For example, human-annotated Pensive expressions, which falls outside standard emotion categories, are typically classified by the model as either Sadness or Neutral. Similarly, when annotators labeled expressions as Neutral, the model often interprets these as Sadness.\n\nText Sentiment. The zero-shot performance analysis, presented in Figure  2b , indicates that the model demonstrates a moderate agreement with human annotations for Neutral sentiment. Positive texts are frequently classified as Neutral, but less frequently confused with Negative. Similarly, human-annotated Neutral texts are typically classified as either Neutral or Positive. Negative texts follow a similar pattern, being frequently classified as Neutral, but less frequently confused with Positive. These findings suggest that while the model effectively distinguishes between Positive sentiment against Negative, it struggles to correctly identify Negative sentiment, exhibiting a tendency to shift toward neutral or positive predictions. Additionally, the boundary between neutral and polarized sentiments remains ambiguous, reflecting the model's difficulty in capturing subtle variations in sentiment.  Relationship between Text Sentiment and Facial Expression. We investigate the relationship between facial expressions and speech transcription text sentiment by analyzing their co-occurrence patterns in the WoZ dataset. The modalities were temporally aligned as described in Sec. 4. Figure  3  presents two analyses: (a) model-predicted text sentiment versus human-annotated facial expressions, and (b) human-annotated text sentiment versus model-predicted facial expressions. The analysis reveals consistent patterns in text sentiment across both approaches, with Neutral sentiment being predominant among all facial expres-sions (ranging from 45.5% to 97.4% for model predictions, and 57.5% to 67.4% for human annotations), followed by Positive sentiment, while Negative sentiment remains consistently low (< 9%). Notably, model-predicted facial expressions tend to align with Neutral sentiment, occasionally exhibiting Positive sentiment. This outcome aligns with the nature of WoZ interactions, which typically involve neutral conversations between users and a virtual agent.  Relationship between Detected Smiles and Human Annotated Text Sentiment. We examine the relationship between detected smiles and the text sentiment of speech transcriptions. Specifically, we aligned the predominant text sentiment category, extracted from the utterances as described in Sec. 4, with the smile signals occurring during those same utterances. For utterances spanning multiple smile samples, the text sentiment signal was oversampled to match the smile data frequency. Figure  4  presents the distribution of human-annotated text sentiment in relation to automatically detected smiling and non-smiling states. The results show remarkably similar patterns across both conditions: Neutral sentiment dominates (61.4% for non-smiling and 62.4% for smiling states), followed by Positive sentiment (33.3% and 34.0% respectively), while Negative sentiment remains minimal (< 6%). The high co-occurrence of smiles with Neutral sentiment suggests that smiles in human-agent interactions might function primarily as social signals of engagement or acknowledgment, aligning with interaction patterns where courtesy smiles serve communicative rather than emotional purposes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Anger Contempt Disgust Fear Happiness Neutral Sadness Surprise",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Predictions",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Happiness",
      "text": "In summary, our analyses reveal both alignments and differences between human annotations and model predictions. The data shows clear agreement in specific cases (e.g., Happiness in facial expressions, Neutral in text sentiment), while differences emerge in others. In the context of human-agent interactions, our observations suggest that facial signals like smiles and expressions might serve communicative functions beyond emotional role. These findings provide insights into the patterns of human behavior interpretation in human-agent interaction scenarios.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparing Model Predictions Across Different Modalities",
      "text": "This section compares model predictions across different modalities: facial expressions, text sentiment, and smiles. We examine these relationships across two distinct datasets: WoZ, which captures human-virtual agent interactions, and Short Interviews, which contains human-human conversations. We investigate how different behavioral signals align when interpreted by models, and how these patterns vary across different interaction contexts.\n\nRelationship between Text Sentiment and Facial Expression. We analyze the patterns between model predictions for facial expressions and text sentiment across two datasets. Figure  5  shows the co-occurrence patterns between model-predicted facial expressions and text sentiment, where (a) shows results from the WoZ dataset, and (b) from the Short Interviews dataset. In the WoZ dataset, similar to our findings in 5.1 Neutral sentiment strongly dominates (61.4% to 74.6%) across all facial expressions, with Positive sentiment following as the second most common prediction, particularly for expressions of Happiness (31.3%) and Contempt (29.6%), while Negative sentiment remains consistently low (< 10%). The Short Interviews dataset, in contrast, presents more balanced predictions, with Neutral sentiment ranging from 46.3% to 58.6% and notably more balanced polarities. Several facial expressions in this dataset show higher Negative than Positive sentiment (e.g., Surprise: 31.0% vs 22.7%, Sadness: 28.4% vs 20.5%). These differences in sentiment distributions likely reflect the distinct nature of each dataset: WoZ captures human-virtual agent interactions in controlled settings, while the Short Interviews dataset contains human-to-human interactions across diverse conversational contexts that may evoke a wider range of emotional responses.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Relationship Between Smiles And Text Sentiment",
      "text": "We examine the relationship between detected smiles and predicted text sentiment across both datasets. Figure  6  shows the normalized co-occurrence between model-predicted text sentiment and smile states for the WoZ and Short Interviews datasets. In the WoZ dataset, Neutral sentiment dominates across both smiling and non-smiling states, followed by Positive sentiment. A notable shift is observed in the distribution of Neutral sentiment, which remains more prevalent than other sentiment classes in both conditions. In contrast, the Short Interviews dataset reveals a higher presence of Negative sentiment during non-smiling events, although Neutral sentiment remains dominant across both smile states. Additionally, we observe a low correlation between smile intensity and text sentiment in both datasets. This suggests that smiles may serve broader communicative functions beyond conveying positive emotions, aligning with previous research indicating that smiles play complex roles in human communication and can occur across various emotional contexts  [19] .\n\nOur cross-modal analysis reveals distinct patterns across datasets. While the WoZ dataset shows strong tendencies toward Neutral predictions with occasional Positive sentiment, the Short Interviews dataset exhibits more balanced sentiment distributions. These differences appear consistently across both facial expression and smile analyses, suggesting that the interaction context (human-agent versus human-human) significantly influences model predictions. The weak correlation between smile presence and positive sentiment, observed in both datasets, indicates that behavioral signals in human communication may serve purposes  beyond their typically assumed emotional associations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analyzing Individual Differences.",
      "text": "We analyze the model predictions across different users in WoZ dataset. Our analysis focuses on three aspects: the agreement between model-predicted and human-annotated text sentiment, the distribution of model-predicted facial expression scores and the distribution of smiling intensity score. We aim to understand how model performance and behavioral predictions vary at the individual level.\n\nUser Variation in Text Sentiment Model Agreement. The zero-shot text sentiment analysis reveals substantial variation in model performance across users in WoZ dataset, as shown in Figure  7 . Agreement between model predictions and human annotations ranges from 0.25 to 0.8, with most users falling between 0.4 and 0.7. This wide range of accuracy scores suggests that the model's ability to capture sentiment varies significantly depending on individual speaking patterns and expression styles. While some users' sentiment expressions align well with model predictions, others show consistent disagreement, indicating that individual verbal expression patterns might not be well represented by the model's general understanding of sentiment.   Our analysis reveals two key findings about individual differences. First, the accuracy of text sentiment prediction varies substantially across users, suggesting that models may struggle to capture personal variations in verbal expression. Second, the intensity of facial emotion scores shows distinct patterns for each user, indicating individual styles of non-verbal communication during human-agent interactions. These user-specific patterns in both verbal and non-verbal behavior emphasize the challenges models face in capturing generalizable representations of human expressivity, given the inherent diversity in how individuals communicate and express themselves. This highlights the need to account for individual differences to better understand human behavior.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Our analysis of facial expressions, text sentiment, and smiles in elderly Spanish speakers reveals significant limitations in current affective computing systems. The gap between human annotations and model predictions, particularly evident in facial expression recognition, shows that models often miss the nuanced emotional expressions typical of older adults. These limitations become more pronounced when comparing controlled WoZ interactions with natural conversations in the Short Interviews dataset. In the WoZ dataset, where interactions occur between participants and a virtual agent, model outputs across all modalities consistently skew toward positive emotions. This likely reflects the structured and polite nature of virtual agent interactions, which encourage neutral and positive exchanges. In contrast, the Short Interviews dataset, featuring more spontaneous human-to-human interactions, exhibits a higher presence of negative sentiment outputs. This distinction emphasizes how the type of interaction context shapes emotional expression and the model predictions.\n\nOur cross-modal analysis reveals important findings about emotional expression in human interactions. The weak correlation between smile intensity and text positive sentiment, combined with frequent smiles during neutral speech, indicates that these expressions serve broader communicative functions. This observation of the multiple purposes of smiles is aligned with previous work  [19] . This understanding is crucial for developing virtual assistants, as these social signals may carry different meanings in humanagent interactions compared to natural conversations. Virtual assistants need to consider not just what an expression is, but what it means in a specific interaction context.\n\nIndividual differences emerge as a critical challenge for current systems. Our findings demonstrate substantial variation in personal expression styles, from varying intensities in facial emotions to distinct patterns in sentiment expression. This consistent variability across both datasets suggests that standard emotion perception approaches may oversimplify human expressivity in this demographic.\n\nThese findings have direct implications for developing supportive technologies for older adults. The distinct patterns observed between human-agent and human-human interactions emphasize the importance of context-aware emotion perception. Understanding these nuances is especially important for virtual assistants designed to support elderly well-being, where misinterpreting emotional signals could affect the quality of care. Moreover, the significant individual variations we found suggest the need for more adaptable approaches, especially when working with underrepresented populations like elderly Spanish speakers.\n\nEthical Implications. Our study has considered several ethical guidelines. All participants in the Short Interviews dataset provided informed consent for the use of their identifiable recordings, and ethical approval was granted by the Ethics Committee of the Fundació Sant Joan de Déu healthcare center. To safeguard participant well-being, interviews were carefully designed to elicit only mild emotional responses, avoiding potential distress topics such as personal trauma. This consideration was crucial given the vulnerability of elderly participants, who are more likely to experience feelings of desolation and loneliness. Participants were monitored by an external observer and reminded of their right to withdraw at any time. Although all participants agreed to the use of their data for research purposes, privacy concerns prevent the public sharing of our dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we evaluated state-of-the-art affective computing models-facial expression recognition, text sentiment analysis, and smile detection-on Spanish-speaking older adults using the WoZ and Short Interviews datasets. Our analysis revealed that models struggle to accurately predict the nuanced emotional signals of this demographic. Results highlight the influence of interaction context, showing bias toward neutral/positive emotions in virtual agent interactions, while human-to-human conversations displayed more sentiment variation. The significant individual variability in emotional expression underscores the need for personalized, context-aware models that better account for cultural and contextual factors, ensuring more inclusive emotion perception technologies.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , with a detailed descrip-",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of the analysis pipeline. Video and human annotations are processed through state-of-the-art machine learning models",
      "page": 2
    },
    {
      "caption": "Figure 2: a, where annotators used",
      "page": 5
    },
    {
      "caption": "Figure 2: b, indicates that the model demon-",
      "page": 5
    },
    {
      "caption": "Figure 2: Normalized co-occurrence matrices between human an-",
      "page": 5
    },
    {
      "caption": "Figure 3: presents two analyses: (a) model-predicted text",
      "page": 5
    },
    {
      "caption": "Figure 3: Normalized co-occurrence matrices for the WoZ dataset.",
      "page": 5
    },
    {
      "caption": "Figure 4: presents the dis-",
      "page": 5
    },
    {
      "caption": "Figure 4: Normalized co-occurrence between human-annotated",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the co-occurrence patterns be-",
      "page": 6
    },
    {
      "caption": "Figure 5: Normalized co-occurrence matrices between model-",
      "page": 6
    },
    {
      "caption": "Figure 6: Normalized co-occurrence between model-predicted",
      "page": 7
    },
    {
      "caption": "Figure 7: Agreement between model",
      "page": 7
    },
    {
      "caption": "Figure 7: Per-user zero-shot accuracy comparison between auto-",
      "page": 7
    },
    {
      "caption": "Figure 8: a) show varying median levels",
      "page": 7
    },
    {
      "caption": "Figure 8: Per-user distribution of (a) happiness and (b) smile fa-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5.8": "45.5",
          "8.4": "65.4",
          "8.7": "72.3",
          "0.0": "97.4"
        },
        {
          "5.8": "48.7",
          "8.4": "26.2",
          "8.7": "18.9",
          "0.0": "2.6"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.7": "60.4",
          "2.9": "67.4",
          "5.8": "57.5",
          "6.3": "67.3",
          "3.5": "66.3",
          "4.8": "63.6",
          "6.0": "58.1",
          "4.2": "67.2"
        },
        {
          "4.7": "34.9",
          "2.9": "29.6",
          "5.8": "36.7",
          "6.3": "26.3",
          "3.5": "30.2",
          "4.8": "31.6",
          "6.0": "35.9",
          "4.2": "28.7"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15.8": "2.4",
          "2.4": "15.3",
          "10.5": "4.0",
          "10.0": "6.5",
          "9.1": "25.0",
          "8.3": "5.6",
          "33.1": "13.7",
          "10.8": "27.4"
        },
        {
          "15.8": "13.9",
          "2.4": "2.7",
          "10.5": "6.9",
          "10.0": "2.7",
          "9.1": "18.0",
          "8.3": "11.6",
          "33.1": "40.2",
          "10.8": "4.0"
        },
        {
          "15.8": "2.1",
          "2.4": "0.7",
          "10.5": "6.1",
          "10.0": "0.6",
          "9.1": "79.9",
          "8.3": "1.2",
          "33.1": "7.9",
          "10.8": "1.5"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "23.2": "8.2",
          "58.2": "71.8",
          "18.6": "20.0"
        },
        {
          "23.2": "6.8",
          "58.2": "60.2",
          "18.6": "33.0"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tnemitneS\nNegative 5.3 3.6\nNeutral 61.4 62.4\nPositive txeT 33.3 34.0\nNon-Smiling Smiling\nSmile": "",
          "5.3": "61.4",
          "3.6": "62.4"
        },
        {
          "tnemitneS\nNegative 5.3 3.6\nNeutral 61.4 62.4\nPositive txeT 33.3 34.0\nNon-Smiling Smiling\nSmile": "",
          "5.3": "33.3",
          "3.6": "34.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9.5": "67.3",
          "7.7": "62.7",
          "9.3": "66.2",
          "8.8": "74.6",
          "7.2": "61.4",
          "8.2": "67.9",
          "8.3": "66.7",
          "8.7": "72.7"
        },
        {
          "9.5": "23.2",
          "7.7": "29.6",
          "9.3": "24.5",
          "8.8": "16.6",
          "7.2": "31.3",
          "8.2": "23.9",
          "8.3": "25.0",
          "8.7": "18.6"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "19.6": "58.6",
          "19.0": "50.1",
          "23.2": "53.0",
          "28.6": "51.2",
          "22.1": "49.2",
          "26.4": "49.2",
          "28.4": "51.0",
          "31.0": "46.3"
        },
        {
          "19.6": "21.8",
          "19.0": "30.9",
          "23.2": "23.8",
          "28.6": "20.2",
          "22.1": "28.7",
          "26.4": "24.4",
          "28.4": "20.5",
          "31.0": "22.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8.7": "68.1",
          "6.5": "56.3"
        },
        {
          "8.7": "23.2",
          "6.5": "37.1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "25.4": "51.9",
          "18.0": "53.8"
        },
        {
          "25.4": "22.7",
          "18.0": "28.2"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How do \"robopets\" impact the health and well-being of residents in care homes? a systematic review of qualitative and quantitative evidence",
      "authors": [
        "Rebecca Abbott",
        "Noreen Orr",
        "Paige Mcgill",
        "Rebecca Whear",
        "Alison Bethel",
        "Ruth Garside",
        "Ken Stein",
        "Jo Thompson-Coon"
      ],
      "year": "2019",
      "venue": "International Journal of Older People Nursing"
    },
    {
      "citation_id": "2",
      "title": "Scoping review on the use of socially assistive robot technology in elderly care",
      "authors": [
        "Jordan Abdi",
        "Ahmed Al-Hindawi",
        "Tiffany Ng",
        "Marcela Vizcaychipi"
      ],
      "year": "2018",
      "venue": "BMJ open"
    },
    {
      "citation_id": "3",
      "title": "Artificial emotional intelligence in socially assistive robots for older adults: a pilot study",
      "authors": [
        "Hojjat Abdollahi",
        "Mohammad Mahoor",
        "Rohola Zandie",
        "Jarid Sewierski",
        "Sara Qualls"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "A pilot study on using an intelligent life-like robot as a companion for elderly individuals with dementia and depression",
      "authors": [
        "Hojjat Abdollahi",
        "Ali Mollahosseini",
        "Josh Lane",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)"
    },
    {
      "citation_id": "5",
      "title": "Toward a Reinforcement Learning Based Framework for Learning Cognitive Empathy in Human-Robot Interactions",
      "authors": [
        "Elahe Bagheri",
        "Oliver Roesler",
        "Bram Vanderborght"
      ],
      "year": "2020",
      "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "6",
      "title": "The empathic project: Building an expressive, advanced virtual coach to improve independent healthy-lifeyears of the elderly",
      "authors": [
        "Luisa Brinkschulte",
        "Natascha Mariacher",
        "Stephan Schlögl",
        "Maria Torres",
        "Raquel Justo",
        "Anna Esposito",
        "Gennaro Cordasco",
        "Gérard Chollet",
        "Cornelius Glackin",
        "Colin Pickard"
      ],
      "year": "2018",
      "venue": "Smarter Lives 18"
    },
    {
      "citation_id": "7",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Zhixi Cai",
        "Shreya Ghosh",
        "Kalin Stefanov",
        "Abhinav Dhall",
        "Jianfei Cai",
        "Hamid Rezatofighi",
        "Reza Haffari",
        "Munawar Hayat"
      ],
      "year": "2023",
      "venue": "Marlin: Masked autoencoder for facial video representation learning"
    },
    {
      "citation_id": "8",
      "title": "Composite emotion recognition and feedback of social assistive robot for elderly people",
      "authors": [
        "Yegang Du",
        "Kaiyuan Zhang",
        "Gabriele Trovato"
      ],
      "year": "2023",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "9",
      "title": "String-based audiovisual fusion of behavioural events for the assessment of dimensional affect",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "M Valstar",
        "H Gunes",
        "Björn Schuller",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "Face and Gesture"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Jianzhu Guo",
        "Xiangyu Zhu",
        "Zhen Lei"
      ],
      "year": "2018",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Towards fast, accurate and stable 3d dense face alignment",
      "authors": [
        "Jianzhu Guo",
        "Xiangyu Zhu",
        "Yang Yang",
        "Fan Yang",
        "Zhen Lei",
        "Stan Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "12",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Andrew Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications"
    },
    {
      "citation_id": "13",
      "title": "Empowering autonomous digital learning for older adults",
      "authors": [
        "Jin Xiaofu"
      ],
      "year": "2024",
      "venue": "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "14",
      "title": "Why are you smiling at me? social functions of enjoyment and non-enjoyment smiles",
      "authors": [
        "Lucy Johnston",
        "L Miles",
        "C Macrae"
      ],
      "year": "2010",
      "venue": "The British journal of social psychology"
    },
    {
      "citation_id": "15",
      "title": "Analysis of the interaction between elderly people and a simulated virtual coach",
      "authors": [
        "Raquel Justo",
        "Leila Ben Letaifa",
        "Cristina Palmero",
        "Eduardo Gonzalez-Fraile",
        "Anna Johansen",
        "Alain Vázquez",
        "Gennaro Cordasco",
        "Stephan Schlögl",
        "Begoña Fernández-Ruanova",
        "Micaela Silva"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "16",
      "title": "The effect of cognitive function health care using artificial intelligence robots for older adults: Systematic review and meta-analysis",
      "authors": [
        "Hocheol Lee",
        "Min Chung",
        "Hyeji Kim",
        "Eun Woo"
      ],
      "year": "2002",
      "venue": "JMIR Aging"
    },
    {
      "citation_id": "17",
      "title": "Designing a multisensory vr game prototype for older adults-the acceptability and design implications",
      "authors": [
        "Xiaoxuan Li",
        "Xiangshi Ren",
        "Xin Suzuki",
        "Naoaki Yamaji",
        "Kin Fung",
        "Yasuyuki Gondo"
      ],
      "year": "2024",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "18",
      "title": "Elderreact: a multimodal dataset for recognizing emotional response in aging adults",
      "authors": [
        "Kaixin Ma",
        "Xinyu Wang",
        "Xinru Yang",
        "Mingtong Zhang",
        "Jeffrey Girard",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "2019 international conference on multimodal interaction"
    },
    {
      "citation_id": "19",
      "title": "Smiles as multipurpose social signals",
      "authors": [
        "Jared Martin",
        "Magdalena Rychlowska",
        "Adrienne Wood",
        "Paula Niedenthal"
      ],
      "year": "2008",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "20",
      "title": "A survey on emotion recognition for human robot interaction",
      "authors": [
        "Suhaila Najim",
        "Alia Karim",
        "Abdul Hassan"
      ],
      "year": "2020",
      "venue": "Journal of computing and information technology"
    },
    {
      "citation_id": "21",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "A dialogue-act taxonomy for a virtual coach designed to improve the life of elderly",
      "authors": [
        "César Montenegro",
        "Asier López Zorrilla",
        "Javier Olaso",
        "Roberto Santana",
        "Raquel Justo",
        "Jose Lozano",
        "María Inés Torres"
      ],
      "year": "2019",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "23",
      "title": "Smiles when sharing",
      "authors": [
        "M Méhu",
        "K Grammer",
        "Robin Dunbar"
      ],
      "year": "2007",
      "venue": "Evolution and Human Behavior"
    },
    {
      "citation_id": "24",
      "title": "Fotios Papadopoulos, Sofia Serholt, and Ginevra Castellano. Endowing a robotic tutor with empathic qualities: Design and pilot evaluation",
      "authors": [
        "Mohammad Obaid",
        "Ruth Aylett",
        "Wolmet Barendregt",
        "Christina Basedow",
        "Lee Corrigan",
        "Lynne Hall",
        "Aidan Jones",
        "Arvid Kappas",
        "Dennis Küster",
        "Ana Paiva"
      ],
      "venue": "International Journal of Humanoid Robotics"
    },
    {
      "citation_id": "25",
      "title": "Empathy in virtual agents and robots: A survey",
      "authors": [
        "Ana Paiva",
        "Iolanda Leite",
        "Hana Boukricha",
        "Ipke Wachsmuth"
      ],
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "26",
      "title": "Exploring emotion expression recognition in older adults interacting with a virtual coach",
      "authors": [
        "Cristina Palmero",
        "Mikel Develasco",
        "Mohamed Hmani",
        "Aymen Mtibaa",
        "Leila Ben Letaifa",
        "Pau Buch-Cardona",
        "Raquel Justo",
        "Terry Amorese",
        "Eduardo González-Fraile",
        "Begoña Fernández-Ruanova"
      ],
      "year": "2023",
      "venue": "Exploring emotion expression recognition in older adults interacting with a virtual coach",
      "arxiv": "arXiv:2311.05567"
    },
    {
      "citation_id": "27",
      "title": "Ethical issues in assistive ambient living technologies for ageing well. Multimedia Tools and Applications",
      "authors": [
        "Francesco Panico",
        "Gennaro Cordasco",
        "Carl Vogel",
        "Luigi Trojano",
        "Anna Esposito"
      ],
      "year": "2020",
      "venue": "Ethical issues in assistive ambient living technologies for ageing well. Multimedia Tools and Applications"
    },
    {
      "citation_id": "28",
      "title": "Robertuito: a pre-trained language model for social media text in spanish",
      "authors": [
        "Juan Manuel Pérez",
        "Damián Ariel Furman",
        "Laura Alonso Alemany",
        "Franco M Luque"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "29",
      "title": "Laura Alonso Alemany, and María Vanina Martínez. pysentimiento: A python toolkit for opinion mining and social nlp tasks",
      "authors": [
        "Juan Manuel Pérez",
        "Mariela Rajngewerc",
        "Juan Giudici",
        "Damián Furman",
        "Franco Luque"
      ],
      "year": "2023",
      "venue": "Laura Alonso Alemany, and María Vanina Martínez. pysentimiento: A python toolkit for opinion mining and social nlp tasks"
    },
    {
      "citation_id": "30",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "31",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "Andrey Savchenko",
        "V Lyudmila",
        "Ilya Savchenko",
        "Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "Matteo Spezialetti",
        "Giuseppe Placidi",
        "Silvia Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "33",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "V Quoc",
        "Le",
        "Efficientnet"
      ],
      "year": "2020",
      "venue": "Rethinking model scaling for convolutional neural networks"
    },
    {
      "citation_id": "34",
      "title": "Advanced assistive technologies for elderly people: A psychological perspective on seniors' needs and preferences (part a)",
      "authors": [
        "Alda Troncone",
        "Terry Amorese",
        "Marialucia Cuciniello",
        "Raffaele Saturno",
        "Luca Pugliese",
        "Gennaro Cordasco",
        "Carl Vogel",
        "Anna Esposito"
      ],
      "year": "2020",
      "venue": "Acta Polytechnica Hungarica"
    },
    {
      "citation_id": "35",
      "title": "Robots in elderly care",
      "authors": [
        "Alessandro Vercelli",
        "Innocenzo Rainero",
        "Ludovico Ciferri",
        "Marina Boido",
        "Fabrizio Pirri"
      ],
      "year": "2018",
      "venue": "DigitCult-Scientific Journal on Digital Cultures"
    },
    {
      "citation_id": "36",
      "title": "Towards affective computing that works for everyone",
      "authors": [
        "Tessa Verhoef",
        "Eduard Fosch-Villaronga"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "37",
      "title": "Implementing emotion detection from speech for psychological assessment of elderly people: A comparative study of python-based approaches and existing solutions",
      "authors": [
        "Warnants",
        "Joaquín Tsiogkas",
        "Francisco Roca",
        "Francisco José Ortiz González",
        "I Zaragoza",
        "José Méndez",
        "Vera Alfonso",
        "Repullo",
        "Serna"
      ],
      "year": "2023",
      "venue": "Implementing emotion detection from speech for psychological assessment of elderly people: A comparative study of python-based approaches and existing solutions"
    },
    {
      "citation_id": "38",
      "title": "The role of staff in facilitating immersive virtual reality for enrichment in aged care: an ethic of care perspective",
      "authors": [
        "Jenny Waycott",
        "Ryan Kelly",
        "Steven Baker",
        "Barbara Barbosa Neves",
        "Kong Saoane Thach",
        "Reeva Lederman"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "39",
      "title": "Engagement estimation of the elderly from wild multiparty human-robot interaction",
      "authors": [
        "Zhijie Zhang",
        "Jianmin Zheng",
        "Nadia Magnenat Thalmann"
      ],
      "year": "2022",
      "venue": "Computer Animation and Virtual Worlds"
    },
    {
      "citation_id": "40",
      "title": "Technology deployment for social connection in residential aged care: Care and technology providers' experiences during the covid-19 pandemic",
      "authors": [
        "Wei Zhao",
        "Ryan Kelly",
        "Jenny Waycott"
      ],
      "year": "2023",
      "venue": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "41",
      "title": "CelebV-HQ: A large-scale video facial attributes dataset",
      "authors": [
        "Wayne Hao Zhu",
        "Wentao Wu",
        "Liming Zhu",
        "Siwei Jiang",
        "Li Tang",
        "Ziwei Zhang",
        "Chen Liu",
        "Loy"
      ],
      "year": "2022",
      "venue": "ECCV"
    }
  ]
}