{
  "paper_id": "2303.17611v1",
  "title": "Transformer-Based Self-Supervised Multimodal Representation Learning For Wearable Emotion Recognition",
  "published": "2023-03-29T19:45:55Z",
  "authors": [
    "Yujin Wu",
    "Mohamed Daoudi",
    "Ali Amad"
  ],
  "keywords": [
    "Emotion Recognition",
    "Self-supervised Learning",
    "Transformers",
    "Physiological Signals",
    "Multimodal Fusion Cross Entropy Loss Modality-specific Encoders Modality-specific classifica�on"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, wearable emotion recognition based on peripheral physiological signals has drawn massive attention due to its less invasive nature and its applicability in real-life scenarios. However, how to effectively fuse multimodal data remains a challenging problem. Moreover, traditional fully-supervised based approaches suffer from overfitting given limited labeled data. To address the above issues, we propose a novel self-supervised learning (SSL) framework for wearable emotion recognition, where efficient multimodal fusion is realized with temporal convolution-based modality-specific encoders and a transformer-based shared encoder, capturing both intra-modal and inter-modal correlations. Extensive unlabeled data is automatically assigned labels by five signal transforms, and the proposed SSL model is pre-trained with signal transformation recognition as a pretext task, allowing the extraction of generalized multimodal representations for emotion-related downstream tasks. For evaluation, the proposed SSL model was first pre-trained on a large-scale self-collected physiological dataset and the resulting encoder was subsequently frozen or fine-tuned on three public supervised emotion recognition datasets. Ultimately, our SSL-based method achieved state-of-the-art results in various emotion classification tasks. Meanwhile, the proposed model was proved to be more accurate and robust compared to fully-supervised methods on low data regimes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTIONS are sets of complex physiological, cognitive and behavioral responses that are triggered by internal or external stimuli. Emotion recognition is an emerging field of research which attempts to empower computers with the ability to infer human emotions. In recent years, it has been employed in several practical scenarios such as automated driver assistance  [1] , health care  [2] , social communication  [3] , etc, the majority of which are based on physical or physiological indicators of the human body. In contrast to physical signals such as facial expressions  [4]  and speech  [5] , physiological responses under certain emotional states are involuntary and therefore provide more objective decisions for identification systems  [6] . The physiological modalities primarily consist of Electroencephalography (EEG) signals and a series of peripheral signals. However, the acquisition of EEG data is challenging for implementation in real-life scenarios. With the advance of non-invasive technologies, emotion recognition methods based on multiple peripheral signals captured by smartphones/wearable watches have attracted some attention. Most recent researches focus on deep neural networks, which can automatically extract complex patterns from multimodal signals. However, given that most of them are trained in a supervised manner, it is challenging to obtain generalizable models using limited labeled data, especially in daily life, where standard protocols for obtaining accurate emotion labels are not yet well defined. Besides, each specific supervised task requires training the deep model from scratch and its knowledge transfer ability on other tasks is not satisfactory  [7] . Self-supervised learning (SSL), as an emerging learning paradigm, eliminates the need for extensive manual labeling and has demonstrated comparable or even superior performance to supervised learning methods in areas of computer vision (CV), natural language processing (NLP). Several SSL-based efforts  [8] ,  [9] ,  [10]  have been done for emotion recognition using EEG signal, but they are not suitable for practical scenes. Only one work  [11]  targeted low-frequency wearable peripheral signals, but they ignored the correlation between multimodal signals. In this paper, we propose a self-supervised multimodal representation learning approach for wearable emotion recognition based on peripheral physiological signals. The first stage is model pre-training with the pretext objective of signal transformation classification, where a large amount of unlabeled multimodal data are automatically assigned labels through a series of transformations. Considering the heterogeneity of multimodal signals, temporal convolution-based modality-specific encoders are first employed separately on the transformed unimodal data to extract low-level features, followed by a transformer-based shared encoder deployed to aggregate unimodal features, enabling the modeling of complementary and collaborative properties between multimodal signals. Finally, modalityspecific signal transformation recognition is performed to learn effective multimodal representations for downstream arXiv:2303.17611v1 [cs.HC] 29 Mar 2023 Fig.  1 : Overview of our self-supervised multimodal representation learning framework. The proposed SSL model is first pre-trained with signal transform recognition as the pretext task to learn generalized multimodal representation. The encoder part of the resulting pre-trained model is then served as a feature extractor for downstream tasks which is frozen or fine-tuned on the labeled samples to predict emotion classes. tasks that are robust to perturbations in magnitude or temporal domains. The second stage is supervised emotion recognition, where the SSL pre-trained encoder part is retained as a feature extractor to obtain generalized multimodal representations for classification. The overview of the proposed approach is illustrated in Fig.  1 . To validate the effectiveness of our method and the knowledge transferability across different datasets, we pre-trained the proposed model on a large-scale unsupervised emotion dataset PRESAGE collected in unrestricted real-life scenarios and evaluated its performance on three public emotion recognition datasets. Overall, our contributions can be summarized as follows:\n\n• We proposed a novel self-supervised learning  (SSL)  framework to learn generalized representations from a large number of unlabeled samples to cope with the overfitting problem on small-scale physiological data. • We adopted an intermediate fusion strategy based on temporal convolution and transformer, capable of modeling both the heterogeneity and cross-modal correlation of physiological signals to effectively fuse multimodal data. • We outperformed state-of-the-art supervised or selfsupervised learning-based approaches in various emotion-related classification tasks involving mental stress, affective states, arousal, and valence. Moreover, our model was proven to be more accurate and stable on limited labeled data than fully-supervised models. In addition, multiple ablation studies have been performed to investigate the effectiveness of our method.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fully-Supervised Deep Emotion Recognition Method Based On Peripheral Physiological Signals",
      "text": "Deep learning-based methods have recently gained extensive attention due to their automatic abstract representation learning properties and have been shown to outperform machine learning methods in several studies  [12] ,  [13] ,  [14] ,  [15] . For example, Huynh et al.  [13]  employed a neural architecture search, aiming to obtain the optimal architecture for emotion recognition among 10,000 manually designed deep neural networks for multimodal physiological signals.\n\nLai et al.  [14]  proposed a residual temporal convolutionbased deep neural network to capture the effective features of multimodal signals, resulting in state-of-the-art results for stress detection and emotion recognition tasks. The above results of deep learning-based approaches are encouraging for wearable emotion recognition. However, training a sufficiently accurate and generalizable model commonly depends on a large amount of labeled data, which is challenging for physiological data, as the annotation is timeconsuming, expensive, and requires the intervention of domain experts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Learning (Ssl) For Limited Labelled Data",
      "text": "To solve the overfitting problem introduced by the limited available data for supervised deep learning models, one common solution is data augmentation, i.e., applying different transformations on the original samples to obtain more abundant data. However, performing data augmentation could not introduce inter-subject variability during training  [11] . Alternatively, a technique that does not require the intervention of labeled data is unsupervised representation learning, where a typical model is an autoencoder, which extracts meaningful representations through the compression and reconstruction of the unlabeled data. Several studies have explored the feasibility of this technique for emotion recognition. In  [16] , stacked convolutional autoencoders were applied independently on unlabelled ECG and EDA data to obtain generalized latent representations for arousal classification, achieving better performance than the fully supervised approaches. Though this method effectively modeled the heterogeneity of multimodal signals, i.e. using different models to extract valid unimodal features, however, it neglected the collaborative and complementary nature of multimodality. Different from the previous approach, Zhang et al.  [15]  presented a correlation-based emotion recognition algorithm (CorrNet), where intra-modal features are first obtained with separate convolutional autoencoders, followed by covariance and cross-covariance computation between each pair of modalities to obtain inter-modal features. However, these unsupervised learning methods based on autoencoders did not introduce supervised signals in pre-training, thus may resulting in unsatisfactory performance.\n\nRecently, a compelling branch in the field of unsupervised representation learning is self-supervised learning (SSL), which can effectively address the de-generalization issue posed by insufficient labeled data. Unlike unsupervised learning which does not involve any labelled data, SSL is designed with a series of pretext tasks that introduce self-supervision to unlabelled data, enabling more effective representation learning for downstream tasks. Each unsupervised sample is automatically labeled through inherent dependencies and associations between the data without human intervention  [17] . The SSL model pre-trained on pseudo-labeled data is considered as powerful feature extractor for a variety of downstream tasks. In the domains of computer vision and natural language processing, SSLbased work such as SimCLR  [18] , Word2Vec  [19] , and BERT  [20]  have exhibited competitive and even superior performance on a range of tasks. However, few studies have investigated the performance of SSL models on peripheral physiological signal data. Sarkar et Etemad  [7]  introduced a self-supervised representation learning framework for ECGbased emotion recognition, where the 1DCNN-based multitask deep neural network is pre-trained with the objective of identifying the signal transformation types applied to unlabeled data. Their study indicated that the pretext task based on transformation recognition can enable the model to better cope with potential variation factors in the data. However, not all time steps of a signal sequence are associated with the target event (i.e., a specific emotion). Thus, how to filter out irrelevant information during SSL for downstream tasks is an unsolved problem. Exploiting the synchrony of multimodal emotional responses is a potential solution. More specifically, multimodal physiological signals exhibit correlated or consistent temporal changes when emotions are elicited. In this way, modeling the correlation of multimodal signals in SSL can facilitate the capture of emotionrelated components in unlabeled data. For multimodal emo-tion recognition, Dissanayake et al.  [11]  proposed a selfsupervised contrastive learning approach, which aims to approximate the positive pairs while pushing the negative pairs away from each other. However, their SSL model is obtained by pretraining each modality independently, and thus again ignores the cross-modal correlations. Therefore, more effective multimodal fusion strategies need to be developed for SSL-based wearable emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Data Fusion For Emotion Recognition",
      "text": "Multimodal data fusion strategies can be generally categorized into: early fusion, intermediate fusion, and late fusion. Most existing approaches for multimodal emotion recognition are based on early fusion, where multimodal data are combined as a whole before performing a learning task. Joint representations can be extracted directly from concatenated vectors with deep models such as 1DCNN  [21]  and Bi-LSTM  [22] , which allow for encoding inter-modal correlations. However, since unimodal features are not learned explicitly (i.e., the heterogeneity of the multimodal signal is ignored), this fusion strategy is not effective in capturing intra-modal correlations. Late fusion-based approaches  [14] ,  [23]  integrate the decisions of multiple independent learning models to predict emotion categories. Thus, in contrast to early fusion, this fusion approach ignores the connections and interactions between modalities.\n\nDifferent from the previous fusion approaches, intermediate fusion enables both intra-and inter-modal correlation, where independent feature extractors are first applied to different modalities and the obtained unimodal features are then aggregated in an additional fusion module to further learn the joint representation. A variety of options exist for this fusion module. For example, Shu and Wang  [24]  adopt ed the restricted Boltzmann machine (RBM) model to learn the joint probability distribution of multimodal lowlevel features to encode cross-modal information exchanges. Zhang et al.  [25]  modeled the associations between multimodal features by introducing a regularization term to the objective function. More recently, the transformers have also gained popularity in intermediate fusion-based approaches  [26] ,  [27] ,  [28]  for video, audio and text. Regarding studies on emotion recognition, Wu et al.  [26]  proposed a multimodal Recursive Intermediate Layer Aggregation (RILA) model, which was applied between layers of unimodal deep transformers to capture interactions across modalities through the integration of multimodal intermediate representations. In this work, the transformers were employed to provide valid intermediate features. At the same time, they have also proved to be effective in merging multimodal data  [27] ,  [28] . The attention mechanism can capture advanced patterns shared across modalities, thus exhibiting advantages over naive fusion strategies such as concatenation. In terms of practicality, multimodal emotion recognition based on the video, audio and text may not be well suited to real-life scenarios, as it requires considerable computational resources for long-term video stream analysis. In contrast, wearable physiological signals can consistently predict emotions in a low-cost and objective way. However, the validity of transformer-based models has not been well established for wearable emotion recognition. Meanwhile, video, audio and text-based approaches cannot be directly migrated to physiological data due to differences in data structures. In addition, they are susceptible to overfitting problems as they generally have a relatively deep architecture and follow a fully-supervised setup.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview",
      "text": "Our goal is to employ unlabeled data for capturing generic representations of multimodal physiological signals in order to address the de-generalization problem introduced by a limited number of labeled samples. Hence, we propose a self-supervised learning (SSL) scheme using signal transformation recognition as a pretext objective. An illustration of the proposed approach is shown in Fig.  1 . In our work, three modalities measured by different sensors are considered: electrodermal activity (EDA), blood volume pressure (BVP) and skin temperature (TEMP). More formally, let x m ∈ R N ×1 represent a 1D time-domain signal from one of the M different modalities (in our work, M = 3), where N is the signal length. Given a set of n transform functions T = {T j (•), j ∈ {1, . . . , n}}, the altered multimodal signal dataset can be generated by applying each transformation to individual modality. Based on this, one can easily build a pseudo-labeled dataset L = {(T j (x i m ), y i ), y i ∈ {1, . . . , n}, m ∈ {1, . . . , M }, i ∈ [1, |L|]} for unlabelled samples through self-supervision enabled by signal transformations. Then, the proposed model consisting of a multimodal encoder E and modality-specific classifiers C is pre-trained to predict the type of transformation applied to samples in L. Ultimately, only the encoder part E of the optimal model obtained after pre-training is retained and is expected to produce generalized multimodal representations in a variety of supervised downstream tasks. Details of the proposed SSL framework are as follows.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Self-Supervised Learning Of Multimodal Physiological Signals",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pretext Task: Signal Transformation Recognition",
      "text": "Signal transformation recognition was adopted as the pretext task in SSL, which proved to be effective in learning generalized representations for downstream tasks such as action recognition  [29]  and emotion recognition  [7] . The random transformations used in the previous SSL methods are one of the common data augmentation techniques for time series, which can generally be classified into two categories: magnitude domain transformations and time domain transformations. The former interferes with the signal values while preserving the time step order, whereas the latter mainly affects the time scale. Previous evaluations of SSL models based on individual transformation recognition  [7] ,  [29]  have indicated that Noise addition and Scaling ranked highly for magnitude domain transformations, while Permutation and Time-warping performed outstandingly well among time domain transformations. Meanwhile, according to the review of time series augmentation strategies  [30] ,  [31] , though most of the suggested transformations have been adopted in previous SSL-based work, two transformations have not been thoroughly evaluated: Magnitudewarping and Cropping. Ultimately, we selected the five transformations: Permutation, Time-warping, Noise addition, Magnitude-warping and Cropping for the pretext task. The reason why Scaling was omitted is that Magnitude-warping can be seen as a special variant of Scaling 1 .\n\nThe above signal transformations are performed on all three modalities and the resulting transformed signal data is fed into the proposed SSL model as input along with the original multimodal signal data. Fig.  2  shows the effect of these deformations on a sample of the EDA signal. Details of each transformation are described in subsequent paragraphs. Here, for simplicity, we write the above-mentioned 1D signal x m uniformly as x(t), where t represents the time step.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Magnitude Domain Transformations:",
      "text": "• Gaussian noise addition: The original signal x(t) is disturbed by white Gaussian noise z(t), which can be extracted from a zero-mean normal distribution N (0, σ 2 ). By assigning a preferred signal-to-noise ratio (SNR), the variance σ 2 (i.e., the average power of the noise) of the distribution N can be derived from the following formula 10 (Psig-SN R)/10 , where P sig is the average power of the signal. In the end, the noised signal is calculated as x(t) + z(t). cubic spline interpolation function φ(•). In the end, the transformed signal can be calculated as x(t) • φ(x(t)).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Time Domain Transformations:",
      "text": "• Permutation: The original signal is split into n nonoverlapping segments x(t) = {x 1 , x 2 , ..., x n }, which are then temporally disrupted and eventually recombined together to form the permuted signal x(t) = {x p1 , x p2 , ..., x pn }, where {p1, p2, ..., pn} is a shuffled version of the original order.\n\n• Time-warping: The original signal is divided into n non-overlapping segments x(t) = {x 1 , x 2 , ..., x n }, half of which are randomly selected to be stretched by a linear interpolation function F (x i , k), where k is the stretch factor, and the remaining half of the segments are squeezed by the function F (x i , 1/k), where 1/k is the squeeze factor. The time-warped signal can be concatenated from the transformed segments and finally resized to the original length.\n\n• Cropping: The original signal is divided into n nonoverlapping segments x(t) = {x 1 , x 2 , ..., x n }, one of which is randomly selected and resampled to the original length. By identifying the signal transform types, our model is expected to learn a more robust and generalized representation against disturbances in the magnitude or time domains. For example, Magnitude-warping and Gaussian noise addition can simulate different types of real-world noise, such as measurement errors, signal artefacts caused by the subject's body movements, etc. For time-domain transformations, Permutation perturbs the order of time steps to prompt the model for capturing time-domain dependencies between data points, Time-warping simulates duration variations in emotional responses by stretching or squeezing time steps, and Cropping allows the model to be more robust to changes in the temporal location of emotional events.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Self-Supervised Multimodal Representation Learning Network Architecture",
      "text": "The proposed SSL multimodal deep neural network consists of two key elements, namely the encoder E and the modality-specific transformation classifiers C. The encoder E can be further subdivided into temporal convolutionbased modality-specific encoders E p and transformer-based shared encoder E s , where E p models the heterogeneity of multimodal signals and E s activates cross-modal information exchange. Ultimately, the multimodal features obtained from the encoder are used as input to C for identifying transformation types for each modality. The implementation of these key components is described in the following paragraphs. Modality-specific encoder: Considering the heterogeneity of the multimodal signals, separate encoders are first employed for each modality, with a temporal convolutionbased network acting as the backbone to capture low-level intra-modal correlation information. The temporal convolutional network (TCN)  [32] , in a nutshell, is a combination of dilated causal convolution and residual connections, with parallel computational capability and robust gradients at optimization, thus demonstrating better performance than traditional recurrent networks, such as LSTM and GRU. One basic TCN consists of several residual blocks. The most central components of each block are two dilated causal convolution layers. The causality can be easily achieved when the output at the current moment t depends only on the elements of the past historical moments up to t in the previous layer. Meanwhile, the dilation operation injects holes in the standard convolution map, thereby increasing the reception field. More formally, given the transformed 1D signal of modality m: x m = T j (x m ) ∈ R N ×1 with N time steps, and a filter f of size k, the dilated convolution on time step t can be defined as\n\nwhere d is the dilation factor. Following each convolutional layer is a weight normalization layer for the convolution filter, a rectified linear unit (ReLU) layer and a dropout layer for regularization. In the end, a residual connection is created between the input and output of the block, where a 1 × 1 convolution is introduced to eliminate the mismatch in channel numbers between the input and output. Fig.  3  illustrates the detailed structure of the TCN-based backbone.\n\nThe dilated causal convolution layers in two residual blocks are equipped with 16 filters with a kernel size of 6, where the dilation factors are 1 and 2, respectively. Zero-padding of 5 and 10 are also introduced to ensure that the input and output sequences are of the same length. Subsequently, a modality-specific projection head (i.e., a linear fully connected layer with 128 units) and a layer normalization are then applied to map the low-level features to a higher dimensional embedding space. Finally, the output of the modality-specific encoder E p is:\n\nwhere d is the embedding dimension. Shared encoder: As mentioned in Section 2.2, encoding of the coordination and interaction between multimodal signals is essential in order to learn generic representations related to the downstream emotion recognition tasks. This can be done through the transformer in which each modality identifies components of other modalities that are highly correlated with itself through the attention mechanism for better signal transformation classification. To achieve this, the low-level features z m of each modality are first stacked to form a multimodal embedding\n\nThe scaled dot-product attention proposed in  [33]  is then applied to calculate the dependencies between different modalities:\n\nwhere Q, K, V represent queries, keys and values, respectively. More intuitively, the attention layer acts as a weighted sum of values V , where the attention weight associated with each value is generated by the compatibility of the query with its corresponding key. For our shared encoder E s , queries, keys and values are derived through a linear  mapping of multimodal features z multi , and the resulting output of the attention layer is:\n\nwhere W Q , W K , W V ∈ R d×d are the projection matrices. Fig.  4  presents the process of generating attention weights from multimodal embeddings, where cross-modal communications are activated. For our shared encoder, the one-layer vanilla transformer block proposed in  [33]  with four-head attention is implemented. The feedforward layer dimension is set to 128. ReLU is selected as the activation function for intermediate layers and a rate of 0.2 is used for Dropout operation. In addition, we did not introduce positional coding information for the stacked multimodal inputs. Since the features of each modality are generated by different encoders, the network performance may not benefit from positional encoding in the context of heterogeneous input. This is further explored in the ablation study (Section 5.6.4).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality-Specific Classification Head:",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Emotion Recognition Based On Physiological Signals",
      "text": "After pre-training the proposed SSL model with the pretext task on unlabelled data, only the encoder part E is reserved for extracting efficient multimodal representations in a variety of supervised downstream tasks. In this work, we select emotion recognition as our downstream task. A classification head C emo is applied to the output of the encoder E to generate class probabilities for labeled samples L sup = {(x i m , y i ), , y i ∈ {1, . . . , e}, m ∈ {1, . . . , M }}, where e is the number of the emotion classes. The emotion classification head is constructed in the same way as C m , except that it accepts multimodal features from encoder E. After the multimodal transformer, features from each modality are first passed through the 1D global average pooling layer, then the flattened unimodal features are concatenated and processed successively through a fully-connected layer with 192 hidden units, a Batch Normalization layer, ReLU activation function, a Dropout layer with a rate of 0.2 and a second fully-connected layer with the number of hidden units equal to the number of emotion classes for prediction. Finally, the proposed model is optimized through the minimization of cross entropy loss L sup .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Presage Dataset",
      "text": "The PRESAGE dataset is a large-scale multimodal physiological signal dataset for emotion analysis. collected in the five scenarios for self-supervised multimodal representation learning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Wesad Dataset",
      "text": "The WESAD dataset  [34]  is a multimodal dataset for stress and emotion recognition. Following a study protocol in a restricted laboratory setting, three affective states, namely baseline, stress and amusement, were elicited from 15 subjects during which physiological and motion signals were collected by two separate sensors: RespiBAN (chest-worn device) and Empatica E4 (wrist-worn device). Since we focus on wearable affective computing, only blood volume pressure (BVP, 64 Hz), electrodermal activity (EDA, 4 Hz) and temperature (TEMP, 4 Hz) captured by Empatica E4 were applied to the classification task. According to previous work  [13] ,  [14] ,  [34] , a stress detection task (non-stress vs stress) and a emotion recognition task (baseline vs stress vs amusement) can be performed on the WESAD dataset for supervised learning, where the non-stress class is a combination of the baseline and amusement classes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Case Dataset",
      "text": "The CASE dataset  [35]  is a multimodal emotion recognition dataset with continuous annotations. Eight video clips were employed to stimulate four different emotions: amusing, boring, relaxing and scary from 30 subjects. During the experiment, subjects were required to self-assess their own emotional experiences using an annotation interface based on valence-arousal scores, while six physiological signals were recorded at a frequency of 1000 Hz. In our work, we selected blood volume pressure (BVP), electrodermal activity (EDA) and skin temperature (TEMP) signals as in the self-supervised dataset for the classification task. We adopted the same approach as in the literature  [11] ,  [15]  for the mapping from continuous values of valence and arousal to discrete classes, resulting in a binary (low vs high valence/arousal) and a three-class (low vs medium vs high valence/arousal) classification problem for supervised learning.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "K-Emocon Dataset",
      "text": "The K-EmoCon dataset  [36]  is a multimodal dataset with multiperspective annotations for emotion recognition in social interactions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Data Preprocessing",
      "text": "To eliminate artifacts, we first applied a low-pass Butterworth filter with a cutoff frequency of 0.5 Hz for the EDA and TEMP signals, while the same type of filter with a cutoff frequency of 2 Hz is selected for the BVP signal in PRESAGE, WESAD and K-EmoCon dataset. For the CASE dataset, a low-pass filter with a cutoff frequency of 2 Hz was utilized to clean these three signals. Moreover, we performed z-score normalization as in  [37]  for each signal recording to reduce the variation in physiological responses between different subjects. Since the four datasets involved in the experiments were collected using sensors with different sampling frequencies, we then uniformly downsampled all signals in the different datasets to the most frequently occurring frequency, i.e., 4 Hz. Subsequently, based on previous work  [13] ,  [14] , we segmented the signal recordings of all datasets into windows of length 60 s with 99.5% overlap for PRESAGE and WESAD, 99% and 95% overlap for CASE and K-EmoCon, respectively. If the data in a window corresponds to multiple labels, we adopt the same strategy as in the previous work  [11] , i.e., choosing the one with the majority as the final label. Table  1  concludes the learning tasks corresponding to each dataset and the number of samples created after data segmentation. The last column in the table lists the total size of each dataset, where the first dimension represents the total number of samples, while the second and third dimensions represent the signal length at a frequency of 4 Hz in a 60 s window after segmentation (i.e., 240) and the number of modalities (i.e., three modalities: BVP, EDA and TEMP), respectively.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Implementation And Model Training",
      "text": "The training process of our SSL-based approach consists of two main phases. The first phase is to pre-train the proposed model on the PRESAGE dataset using automatically generated pseudo-labels for signal transformation identification.\n\nA set of transformation parameter vectors (15, 10, 9, 1.05, 4, 0.2) was chosen based on the experimental results of the previous study  [37]  as SNR, magnitude warping variance coefficient, number of permutation segments, number of time warping segments, time-warping stretching coefficient, and number of cropping segments for each modality to generate the five transformations mentioned in Section 3.2.1. The pretraining process of the proposed model took approximately 26 hours on an NVIDIA RTX 6000 GPU. The second phase retains only the encoder part of the pre-trained model to extract valid, generalized representations for emotion recognition on WESAD, CASE and K-EmoCon datasets. We did not introduce these three public datasets into pre-training stage in order to verify the knowledge transfer ability of the learned features across different datasets. Ultimately, the proposed model was installed using Pytorch. The optimal models for the pretext and downstream tasks were obtained by the SGD (Stochastic Gradient Descent) optimizer with weight decay parameter of 5e-7 to avoid overfitting. For the first phase (self-supervised pre-training), learning rate, batch size and the number of epochs are set to 5e-3, 32 and 20, respectively. For the second phase (supervised emotion recognition), the learning rate, batch size and number of epochs are set to 1e-4, 128, 20 on WESAD dataset, while for CASE and K-EmoCon datasets, these parameters were set to 1e-3, 64 and 64, respectively.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Metric And Protocol",
      "text": "For a fair comparison, we adopted the same experimental protocol as in  [11] ,  [13] ,  [14] ,  [15] ,  [34] , i.e. Leave-One-Subject-Out cross validation, which has the benefit of examining the generalization ability of the model to unpresented subject data. Two metrics, accuracy and F1-score applied in  [11] ,  [13] ,  [14] ,  [15] ,  [34]  were selected to evaluate the performance of the proposed approach on the emotion recognition task. Accuracy represents the proportion of correctly classified samples to the total number of samples. F1-score is considered as a harmonic mean of the precision and recall, which is suggested for evaluating imbalanced datasets.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baseline Models",
      "text": "Since the exploration of wearable emotion recognition based on peripheral physiological signals has not been well established, a series of baseline models based on fully-supervised learning, unsupervised learning, and self-supervised learning were implemented in addition to available state-of-theart methods to provide a more comprehensive and reliable performance comparison. The followings are brief descriptions of these models:\n\nSupervised learning-based methods:\n\n• SimpDCNN  [29] : it is a simple convolutional network consisting of three convolutional blocks with kernel sizes of 24, 16 and 8, each followed by a ReLU activation and a dropout layer.\n\n• MulT  [38] : it is a transformer-based multimodal fusion method applied to video, audio and text. The unimodal data is first passed through a temporal convolutional network to obtain low-level features, then transformers based on cross-modal attention and self-attention mechanisms are applied successively for effective fusion.\n\n• ResNet  [39] : it is a 1D convolution-based residual network adapted to physiological signals proposed in  [40] ,  [41] , which is constructed similarly to ResNet-18, consists mainly of 8 residual blocks with batch normalization (BN) operation and ReLU activation function, where each block contains two convolutional layers. The three modalities: BVP, EDA, TEMP are fed into this network as multi-channel signals.\n\n• Ours (Supervised): it is our proposed multimodal network, trained in a fully-supervised manner.\n\nIn addition, three additional supervised methods were applied for the performance comparison on the CASE and K-EmoCon datasets since they lacked baseline results compared to the WESAD dataset.\n\n• DCNN  [21] : it employs a four-layer 1D convolutional neural network to extract modality-specific features, and a three-layer fully connected network connected at the bottom of the network for classification. • Attn-BiLSTM  [42] : it applies a multilayer bidirectional LSTM for capturing valid temporal information for multimodal signals. The attention mechanism was applied to select the most relevant multimodal representation of the emotional state as input for a fully connected layer-based classifier.\n\n• MMResLSTM  [43] : it uses separate four-layer LSTMbased models for multimodal signals with residual connections. Moreover, the weights of the LSTM layers of both modalities are shared to activate cross-modal communication.\n\nUnsupervised learning-based methods:\n\n• Autoencoder: it is an autoencoder with the same encoder part as our proposed model, while the decoder part consists of three transposed convolutional blocks for the reconstruction of the BVP, EDA, TEMP signals.\n\nEach unimodal decoder consists of four-layer transposed convolution with the same parameters as the convolutional layers in the encoder.\n\nSelf-supervised learning-based methods:\n\n• SigRep  [11] : it adopts a similar model architecture to SimCLR  [18] , containing an encoder of four inceptioninspired blocks and a projection head consisting of fully connected layers, where each inception block consists of 1D convolutional layers with different kernel sizes and a maximum pooling layer in parallel. The model is applied independently to each signal modality for contrastive representation learning.\n\n• BENDR  [44] : it is a simpler version of wav2vec 2.0  [45]  that was applied to EEG signals. We adapted it for application to peripheral physiological signals at low frequencies. The multi-channel signal consisting of BVP, EDA, TEMP is first passed through a four-layer convolution with kernel sizes of 3, 2, 2, 2, where the GeLU is chosen as the activation function along with GroupNorm and Dropout operations, and the obtained low-level features are randomly masked and fed to the same transformer as our proposed model. The final output features are used to reconstruct the masked features.\n\nFor a fair comparison, we used the code provided by the authors of the above methods and applied the same experimental setup. If the code is not available, we followed the parameters provided in these works for the model implementation. For those models initially designed for nonperipheral physiological signals, the parameters have been slightly adjusted to match the low-frequency wearable data for proper operation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparision With State-Of-The-Art Methods",
      "text": "Emotion-related classification tasks were performed on WE-SAD, CASE, K-EmoCon datasets to evaluate the performance of the proposed SSL model. Tables  2, 3 , 4 summarize performance comparisons with the state-of-the-art fully supervised, unsupervised, and self-supervised learning-based methods. For the SSL-based approaches, we report the results under two training modes: Frozen (F) and Fine-Tuned (T). The first mode refers to freezing the pre-trained encoder part and updating only the parameters of the classification head in the downstream classification tasks, which is designed to investigate the effectiveness of the learned selfsupervised multimodal features. The second mode employs the pre-trained encoder parameters for model initialization and updates all parameters normally to examine the performance gain relative to the Frozen mode. From the tables, first, it can be observed that our fully-supervised model obtained better performance than other supervised learning approaches in most emotion recognition tasks, confirming the effectiveness of the proposed architecture. Secondly, regarding our SSL model, the comparison results indicated that, under the Frozen mode, our method achieved superior performance over other fully supervised, unsupervised, and self-supervised based approaches on 6 out of 10 tasks, demonstrating the generalization and high discrimination of the representation learned through the SSL pretext task. In addition, the performance of our model was improved in the Fine-Tuned mode, further narrowing the gap with supervised baselines and thus achieving state-of-the-art results in 8 out of 10 tasks. Additionally, it is interesting to note that as the number of supervised samples decreases from WESAD to CASE to K-EmoCon, the higher the performance gain obtained by our SSL-based approach with respect to the supervised approaches. This can be attributed to the fact that supervised learning methods are more prone to overfitting than self-supervised learning methods on low data regimes. Further research on the performance comparison of these two types of methods on limited data is presented in Section 5.5.2. Thirdly, in comparison with non-supervised learning methods, we significantly improved the performance of SigRep and BENDR, especially on the CASE and K-EmoCon datasets. The source of this performance gap may be related to the deployed fusion strategies, in addition to the selected pretext tasks. SigRep  [11]  learned effective representations for each modality independently through contrastive learning, whereas BENDR  [44]  regarded multimodal signals as a whole to reconstruct obscured multimodal features. Thus, these two approaches ignored the encoding of inter-and intra-modal correlations, respectively. The impact of different SSL fusion strategies on downstream performance is later investigated in Section 5.6.1. Furthermore, the results of the Autoencoder are inferior to other SSL methods. This may be due to the unsupervised nature of its pre-training process which results in more redundant patterns being captured that are irrelevant to the downstream tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Self-Supervised Learning Vs Supervised Learning On Limited Labeled Data",
      "text": "In the previous section, our self-supervised approach presented state-of-the-art performance on emotion recognition tasks with all labeled data in the dataset. To further investigate the effectiveness of our fine-tuned model on a limited number of labeled samples, we performed a comparison with four supervised learning models: our proposed model with fully-supervised learning, MulT  [38] , ResNet  [39]  and SimpDCNN  [29] . MulT and ResNet were selected since they share similar structures to our model and are the best-performing supervised models in addition to ours. Besides, SimpDCNN, as a low-complexity model, is not prone to overfitting on limited data, allowing for a more comprehensive performance comparison. We implemented a similar sampling procedure reported in  [8] ,  [9] , i.e., 1, 50, 100, 500, and 1000 samples were randomly selected for each class in the three datasets for training the classification model. This process was executed 50 times independently for different numbers of samples. The resulting average accuracy and the corresponding standard deviation of all compared models are illustrated in Fig.  7 . First, our finetuned model consistently outperforms other supervised learning-based models for sample sizes varying from 1 to 1000 on the emotion recognition tasks of all three datasets. Among supervised learning-based methods, SimpDCNN exhibited the poorest results, over which our SSL model could achieve significant performance gains of 6.84% -21.19% for different downstream tasks. Our fully-supervised model yields the highest results compared to other supervised models, whereas the fine-tuned model initialized by self-supervised learning parameters continues to enhance performance by 5.24% -13.63%. Second, for all downstream tasks, the standard deviation obtained by our fine-tuned model is narrower with respect to the supervised learningbased deep models, demonstrating its superior generalization ability across different samples. The above findings are consistent with those reported in  [46]  that the advantage of the self-supervised learning-based method is its better regularisation on low data regimes to avoid overfitting problems compared to fully-supervised methods. As the amount of available labeled data increases, the difference in performance between the two types of models gradually decreases. Overall, the comparison results suggest that the proposed method can produce more meaningful and robust representations for wearable emotion recognition than fullysupervised methods, offering a potential solution to the problem of little labeled data.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Ablation Studies",
      "text": "Different types of ablation experiments were designed and conducted on the WESAD, CASE, and K-EmoCon datasets to verify the validity of the proposed method. The encoder part of the models involved was trained in freezing mode and the obtained emotion recognition results are reported in the following sections.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study Of Different Fusion Strategies",
      "text": "To demonstrate the effectiveness of the selected fusion strategy, we conducted ablation studies on different SSL fusion strategies. head C all share the same parameter settings as the proposed model (details are given in Section 3.2.2). For the Early fusion setup, we treated the multimodal physiological signal as a whole, i.e. a multichannel signal, from which multimodal representations will be learned directly. For the Late fusion setup, separate encoders were applied to individual modalities to extract unimodal features for classification. In addition, the third variant model has the same fusion strategy as ours, where unimodal features were first captured and then concatenated to learn more advanced multimodal features. The difference, however, is that this model performs classification by multimodal features. This is to verify the necessity of conducting modality-specific classification in the proposed method, and we refer to this setup as Intermediate fusion with an overall loss. Consequently, the corresponding evaluation results are listed in Table  5 . Our model consistently achieved the best performance on all datasets, demonstrating the effectiveness of the selected fusion strategy, i.e., intermediate fusion. In addition, the intermediate fusion-based models performed better than those based on the other two fusions. This can be attributed to the fact that the intermediate fusion simultaneously models the heterogeneity and coordination of multimodal physiological signals, whereas the other two fusion approaches only consider one of these two properties. Furthermore, the third setting Intermediate fusion with an overall loss performs slightly worse than our model, affirming the importance of modality-specific classification.\n\nThe benefit of applying modality-specific loss functions is that it forces the model to learn, for each modality, generic features that are robust to perturbations in the time or magnitude domain, while the application of an overall loss fails to distinguish each modality's contribution to the learned representation.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study Of Different Modalities",
      "text": "We conducted an ablation study of three modalities: EDA, BVP, TEMP and their combinations to explore their performance on emotion recognition tasks. The resulting average accuracies and F1-scores of our model are summarized in Table  6 . First, for the unimodal performance, the EDA signal performs outstandingly well among all the modalities, especially when detecting stress and arousal states. This is consistent with previous findings that EDA is one of the most relevant indicators of stress  [47]  and has even been adopted as ground truth in some studies  [48] ,  [49]  for the stress analysis of other signals. In addition, it has been proven to correlate linearly with arousal  [49] . In the bimodal-based classification, we first observed that the BVP+EDA setup performed better on the stress-related tasks (i.e. S-2 and E-3 on the WESAD dataset) than the other setups. This suggests that the BVP signal and the EDA signal are highly coordinated and correlated when the stress state is elicited, making their combination more effective for detection. This finding is quite reasonable. The BVP signal contains information on heart rate (HR) and heart rate variability (HRV) thus providing a strong correlation with stress states. In  [50] , HRV and EDA were identified as the most relevant physiological indicators for the real-time stress detection task. Secondly, the EDA+TEMP setup achieved the best performance on the classification task regarding arousal level. This finding is supported by previous research  [51]  which indicated that EDA and TEMP had a positive and negative correlation with arousal scores respectively. Lastly, our model achieved performance gains on both bimodal and trimodal data in most cases, confirming again its effectiveness for multimodal fusion.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study Of Missing Modalities",
      "text": "We investigate the robustness of the proposed SSL model when a signal modality is missing in downstream tasks, which is quite common in real-world scenarios. There exist a variety of experimental setups for incomplete modalities.\n\nBased on  [52] , we selected the most challenging one, i.e., a modality is missing in both training and testing of the downstream task, where 50% of the multimodal samples were first randomly selected and subsequently the data values of a specific modality were set to 0 to simulate its absence. The robustness of the proposed SSL model was measured by calculating its difference in performance in two cases: one with all modalities present and one with missing modalities. The smaller the difference, the more robust the model is considered to be. The above experimental procedure was repeated 10 times. Additionally, we benchmarked our model against the SSL baseline models: SigRep and BENDR. Fig.  9  presents the average degradation in accuracy and F1-score of the compared models when a modality is missing in different downstream tasks. A series of t-tests were further conducted on the performance differences for a more systematic robustness comparison.\n\nFrom the evaluation results, we can first observe that the performance drops of our model are significantly lower (p < 0.05) than other SSL models on most tasks. This demonstrates the superiority of the proposed method in   Fig.  9 : Evaluation results of the robustness of the SSL methods in the presence of missing modalities. The horizontal axis of each subplot represents the name of the missing modality, while the vertical axis represents the drops in model performance compared to the case of complete modalities, where the metrics of the vertical axes in the first and second rows are accuracy and F1-score, respectively. (ns: no significant difference; * : p < 0.05, the more asterisks, the more significant the difference.) terms of robustness. Second, we also note that the impact of missing modalities on the robustness of SSL methods is task-dependent. For downstream tasks related to stress and arousal levels, more severe performance declines could be obtained in the absence of the EDA signal, compared to the other two modalities. This result indicates the importance of the EDA signal for identifying these two emotional states. Similarly, missing the TEMP signal also leads to a considerable reduced performance in arousal-based recognition, whereas, in the valence-based tasks, the loss of the BVP signal has the greatest impact on performance. The above results, consistent with those in Section 5.6.2, reconfirm the effect of different modalities on specific emotion recognition.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study Of Different Model Components",
      "text": "We also investigate the impact of different model components on the performance of downstream classification tasks. To validate the contributions of the modality-specific encoder and the shared encoder, we designed two alternative models: No TCN and No Transformer. No TCN eliminates the temporal convolution network (TCN) where unimodal data is passed directly through the projection layer (i.e. a fully connected layer with 128 units) in the modalityspecific encoder shown in Fig.  3  and the resulting unimodal low-level features are then concatenated as a whole and fed into the transformer. No Transformer removes the multimodal transformer, where unimodal features are first extracted by modality-specific encoders and then averaged along the time dimension by the 1D global average pooling (illustrated in Fig.  5 ) for the final classification tasks.  highlighting the importance of capturing the heterogeneity and cross-modal correlation of multimodal signals simultaneously. Subsequently, we examined whether the addition of positional encoding could lead to better performance for the transformers with heterogeneous embedding as input.\n\nWe employed two types of positional encoding (PE): With fixed PE and With learnable PE in the transformer and compared their performance with our PE-free model. With fixed PE added the fixed positional encoding obtained from sine and cosine functions of different frequencies as proposed in  [33]  to the input embedding of the multimodal transformer while With learnable PE adopted the same learnable positional encoding in  [53] . Table  7  also show the classification results of the proposed model with different PE setting. We observed that temporal context information injected by two types of PE did not contribute to model performance on all classification tasks as expected. This can be attributed to the fact that the multimodal embeddings generated by the separate encoders already own different structures, hence the additional positional information introduces redundancy into the model.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Ablation Study Of Different Signal Transformations",
      "text": "We further explored the impact of using individual transformations and their combinations in the pretext task on downstream emotion recognition performance. As mentioned in Section 3.2.1, the five transforms employed can be divided into two classes, i.e., magnitude domain transformations and time domain transformations. Therefore, the types of combinations are arranged accordingly as combinations of transformations within the same domain and combinations of transformations across domains. The evaluation results obtained on different emotion classification tasks are presented in Table  8 . First, we noticed that Permutation and Time-Warping, which perturbed the temporal order and duration of events within the window, performed best among the individual signal transformations, which is consistent with the results in  [7] ,  [29] , demonstrating the necessity to encode the temporal relationships of signals for emotion recognition. Second, the pre-trained models obtained by combining the same domain or cross-domain transformations generally perform better than those based on individual transformations. The performance of these combinations varies depending on the specific task. For the same domain transformation combinations, P+T+C performs better for stress-related tasks, whereas N+M is more appropriate for arousal and valence-based tasks. For the cross-domain combinations, N+T exhibited the best performance on the classification tasks regarding stress, while N+P and M+C performed best in predicting the arousal and valence states. Finally, we found that models based on crossdomain combinations outperformed those based on the same domain combinations in two-thirds of the downstream tasks. Meanwhile, our pre-trained models using the full set of transformations consistently achieved superior performance in the classification tasks. This can be attributed to the fact that different types of transformations inject diverse prior knowledge for multimodal representation learning, thus contributing to the generalizability of the network.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have proposed a self-supervised multimodal representation learning framework for wearable emotion recognition. Signal transformation recognition is defined as a pretext task, where a large amount of unsupervised data is automatically labeled by the imposed signal transformation category for pre-training of the SSL model. Subsequently, the encoder part of the pre-trained model consisting of a temporal convolution network and transformer is maintained to extract effective multimodal representations for the downstream task, i.e. emotion recognition. Eventually, we executed the pre-training on a largescale unrestricted emotion dataset PRESAGE and verified the validity of the proposed method on three public multimodal emotion recognition datasets. Experimental results indicated that our approach surpassed fully-supervised, unsupervised, and self-supervised learning methods, achieving state-of-the-art results in various emotion-related tasks. Additionally, the proposed method performs better than the fully-supervised learning approach on limited labeled data, demonstrating its superior generalization ability to avoid overfitting problems. A series of ablation studies also confirmed the efficiency of the designed model architecture.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our self-supervised multimodal representation learning framework. The proposed SSL model is ﬁrst",
      "page": 2
    },
    {
      "caption": "Figure 1: To validate the ef-",
      "page": 2
    },
    {
      "caption": "Figure 2: The original signal and the disturbed signals after applying ﬁve transformations. For each modality, the raw signal",
      "page": 4
    },
    {
      "caption": "Figure 1: In our work,",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the effect of",
      "page": 4
    },
    {
      "caption": "Figure 3: Modality-speciﬁc backbone based on temporal convolutional network (TCN). Each backbone consists of two residual",
      "page": 6
    },
    {
      "caption": "Figure 4: Shared encoder based on the multimodal transformer. (FC: fully-connected layer with 128 units, LN: layer",
      "page": 6
    },
    {
      "caption": "Figure 4: presents the process of generating attention",
      "page": 6
    },
    {
      "caption": "Figure 5: 1D global average pooling is ﬁrst applied",
      "page": 6
    },
    {
      "caption": "Figure 5: Modality-speciﬁc classiﬁcation head Cm for signal",
      "page": 6
    },
    {
      "caption": "Figure 6: (a-e) shows the images of different scenarios",
      "page": 7
    },
    {
      "caption": "Figure 6: (f)), an invasive",
      "page": 7
    },
    {
      "caption": "Figure 6: Images of different scenarios captured by cameras placed in the simulation training room: (a): Doctor consultation,",
      "page": 8
    },
    {
      "caption": "Figure 7: First, our ﬁne-",
      "page": 10
    },
    {
      "caption": "Figure 8: In all variant models, the TCN-based encoder",
      "page": 11
    },
    {
      "caption": "Figure 7: Performance comparison with state-of-the-art supervised learning-based methods on limited labeled data sampled",
      "page": 12
    },
    {
      "caption": "Figure 8: Different architectures used in the ablation studies of",
      "page": 12
    },
    {
      "caption": "Figure 9: presents the average degradation",
      "page": 12
    },
    {
      "caption": "Figure 9: Evaluation results of the robustness of the SSL methods in the presence of missing modalities. The horizontal axis of",
      "page": 13
    },
    {
      "caption": "Figure 3: and the resulting uni-",
      "page": 13
    },
    {
      "caption": "Figure 5: ) for the ﬁnal classiﬁcation tasks. Table",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The learning tasks assigned to each dataset and the corresponding distribution of samples between classes in",
      "data": [
        {
          "Dataset": "PRESAGE",
          "Type": "P",
          "Task": "Transformation\nRecognition",
          "Category (no. of samples)": "Original version and\nﬁve transformations (681641)",
          "Total Size": "(4089846, 240, 3)"
        },
        {
          "Dataset": "WESAD",
          "Type": "D",
          "Task": "Stress-2\nEmotion-3",
          "Category (no. of samples)": "stress (36279), non-stress (85574)\nbaseline (66859), stress (36279), amusement (18715)",
          "Total Size": "(12185, 240, 3)"
        },
        {
          "Dataset": "CASE",
          "Type": "D",
          "Task": "Arousal-2\nValence-2\nArousal-3\nValence-3",
          "Category (no. of samples)": "low (33211), high (61919)\nnegative(32017), positive (63113)\nlow (4847), medium (26898), high (63385)\nnegative(9312), neutral (56870), positive (28948)",
          "Total Size": "(95130, 240, 3)"
        },
        {
          "Dataset": "K-EmoCon",
          "Type": "D",
          "Task": "Arousal-2\nValence-2\nArousal-3\nValence-3",
          "Category (no. of samples)": "low (3729), high (1488)\nnegative(4050), positive (1167)\nlow (1783), medium (1904), high (1530)\nnegative(1783), neutral (1904), positive (1530)",
          "Total Size": "(5217, 240, 3)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: Performance comparison of different emotion",
      "data": [
        {
          "Type": "",
          "Methods": "",
          "Stress-2": "Acc",
          "Emotion-3": "Acc"
        },
        {
          "Type": "SL",
          "Methods": "LDA [34]\nRF [34]\nSimpDCNN [29]\nMulT [38]\nResNet [39]\nStressNAS [13]\nRes-TCN [14]\nOurs (S)",
          "Stress-2": "86.46\n88.33\n90.12\n91.76\n91.93\n92.87\n94.16\n93.83",
          "Emotion-3": "68.85\n76.17\n78.30\n81.09\n80.85\n81.78\n83.69\n84.81"
        },
        {
          "Type": "UL",
          "Methods": "Autoencoder",
          "Stress-2": "91.51",
          "Emotion-3": "80.39"
        },
        {
          "Type": "SSL",
          "Methods": "SigRep [11] (F)\nSigRep [11] (T)",
          "Stress-2": "92.71\n94.91",
          "Emotion-3": "81.11\n84.27"
        },
        {
          "Type": "",
          "Methods": "BENDR [44] (F)\nBENDR [44] (T)",
          "Stress-2": "92.53\n93.19",
          "Emotion-3": "81.98\n82.44"
        },
        {
          "Type": "",
          "Methods": "Ours (F)\nOurs (T)",
          "Stress-2": "94.81\n96.29",
          "Emotion-3": "83.81\n84.94"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Performance comparison of different emotion recognition tasks with state-of-the-art methods on the CASE",
      "data": [
        {
          "Type": "",
          "Methods": "",
          "Valence-2": "Acc",
          "Valence-3": "Acc",
          "Arousal-2": "Acc",
          "Arousal-3": "Acc"
        },
        {
          "Type": "SL",
          "Methods": "SimpDCNN [29]\nDCNN [21]\nMMResLSTM [43]\nAttn-BiLSTM [42]\nMulT [38]\nResNet [39]\nOurs (S)",
          "Valence-2": "71.33\n72.35\n73.34\n74.25\n74.81\n75.29\n76.94",
          "Valence-3": "59.20\n59.78\n60.78\n61.97\n63.14\n62.89\n64.58",
          "Arousal-2": "67.16\n69.63\n71.12\n70.40\n71.28\n72.35\n74.15",
          "Arousal-3": "56.80\n56.09\n57.41\n58.27\n62.15\n65.46\n66.32"
        },
        {
          "Type": "UL",
          "Methods": "Autoencoder\nCorrNet [15]",
          "Valence-2": "73.23\n76.37",
          "Valence-3": "60.77\n60.15",
          "Arousal-2": "69.16\n74.03",
          "Arousal-3": "60.08\n58.22"
        },
        {
          "Type": "SSL",
          "Methods": "SigRep [11] (F)\nSigRep [11] (T)",
          "Valence-2": "71.74\n73.29",
          "Valence-3": "63.85\n64.63",
          "Arousal-2": "70.79\n72.08",
          "Arousal-3": "63.09\n64.88"
        },
        {
          "Type": "",
          "Methods": "BENDR [44] (F)\nBENDR [44] (T)",
          "Valence-2": "72.94\n72.33",
          "Valence-3": "61.56\n62.15",
          "Arousal-2": "72.04\n71.51",
          "Arousal-3": "62.37\n63.52"
        },
        {
          "Type": "",
          "Methods": "Ours (F)\nOurs (T)",
          "Valence-2": "77.49\n78.57",
          "Valence-3": "65.51\n66.64",
          "Arousal-2": "73.67\n74.98",
          "Arousal-3": "65.09\n66.19"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Performance comparison of different emotion recognition tasks with state-of-the-art methods on the CASE",
      "data": [
        {
          "Type": "",
          "Methods": "",
          "Valence-2": "Acc",
          "Valence-3": "Acc",
          "Arousal-2": "Acc",
          "Arousal-3": "Acc"
        },
        {
          "Type": "SL",
          "Methods": "SimpDCNN [29]\nDCNN [21]\nAttn-BiLSTM [42]\nMMResLSTM [43]\nMulT [38]\nResNet [39]\nOurs (S)",
          "Valence-2": "77.14\n78.72\n79.76\n78.79\n80.13\n80.53\n81.51",
          "Valence-3": "59.67\n61.97\n62.56\n61.25\n63.95\n64.60\n64.07",
          "Arousal-2": "72.48\n73.67\n73.30\n74.31\n74.19\n74.35\n75.17",
          "Arousal-3": "46.49\n49.91\n46.95\n44.68\n49.25\n50.09\n50.42"
        },
        {
          "Type": "UL",
          "Methods": "Autoencoder",
          "Valence-2": "80.58",
          "Valence-3": "63.65",
          "Arousal-2": "71.56",
          "Arousal-3": "48.83"
        },
        {
          "Type": "SSL",
          "Methods": "SigRep [11] (F)\nSigRep [11] (T)",
          "Valence-2": "78.98\n79.14",
          "Valence-3": "63.00\n61.74",
          "Arousal-2": "73.36\n73.94",
          "Arousal-3": "47.85\n48.56"
        },
        {
          "Type": "",
          "Methods": "BENDR [44] (F)\nBENDR [44] (T)",
          "Valence-2": "79.83\n78.73",
          "Valence-3": "61.38\n61.85",
          "Arousal-2": "72.86\n73.82",
          "Arousal-3": "50.68\n52.88"
        },
        {
          "Type": "",
          "Methods": "Ours (F)\nOurs (T)",
          "Valence-2": "82.95\n84.14",
          "Valence-3": "66.97\n68.37",
          "Arousal-2": "74.79\n76.40",
          "Arousal-3": "50.76\n54.60"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: Ablation study of different modalities and their combinations: average accuracy and F1-score obtained with",
      "data": [
        {
          "Type": "",
          "WESAD": "S-2",
          "CASE": "V-2",
          "K-EmoCon": "V-2"
        },
        {
          "Type": "",
          "WESAD": "Acc\nF1",
          "CASE": "Acc\nF1",
          "K-EmoCon": "Acc\nF1"
        },
        {
          "Type": "Early\nLate\nInter w/ ol",
          "WESAD": "91.22\n89.94\n93.02\n91.73\n93.53\n92.77",
          "CASE": "73.01\n72.20\n75.58\n72.27\n76.69\n73.52",
          "K-EmoCon": "79.20\n74.12\n80.94\n76.43\n81.48\n77.22"
        },
        {
          "Type": "Ours",
          "WESAD": "94.81\n93.69",
          "CASE": "77.49\n75.58",
          "K-EmoCon": "82.95\n80.07"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 6: Ablation study of different modalities and their combinations: average accuracy and F1-score obtained with",
      "data": [
        {
          "Modality": "",
          "WESAD": "S-2",
          "CASE": "V-2",
          "K-EmoCon": "V-2"
        },
        {
          "Modality": "",
          "WESAD": "Acc\nF1",
          "CASE": "Acc\nF1",
          "K-EmoCon": "Acc\nF1"
        },
        {
          "Modality": "EDA\nBVP\nTEMP",
          "WESAD": "92.36\n90.58\n87.82\n86.35\n78.15\n76.91",
          "CASE": "75.21\n74.80\n75.90\n75.15\n71.64\n68.66",
          "K-EmoCon": "80.65\n74.60\n80.76\n74.13\n79.02\n72.78"
        },
        {
          "Modality": "EDA + BVP\nEDA + TEMP\nBVP + TEMP",
          "WESAD": "93.73\n92.38\n90.95\n89.62\n84.82\n80.45",
          "CASE": "76.26\n75.13\n76.03\n74.97\n72.35\n71.31",
          "K-EmoCon": "80.87\n75.48\n81.70\n77.77\n80.12\n75.05"
        },
        {
          "Modality": "All",
          "WESAD": "94.81\n93.69",
          "CASE": "77.49\n75.85",
          "K-EmoCon": "82.95\n80.07"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 8: Ablation study of individual signal transformations and their combinations: average accuracy and F1score",
      "data": [
        {
          "Model Variants": "",
          "WESAD": "S-2",
          "CASE": "V-2",
          "K-EmoCon": "V-2"
        },
        {
          "Model Variants": "",
          "WESAD": "Acc",
          "CASE": "Acc",
          "K-EmoCon": "Acc"
        },
        {
          "Model Variants": "No TCN\nNo Transformer",
          "WESAD": "91.09\n92.18",
          "CASE": "64.15\n74.65",
          "K-EmoCon": "79.39\n80.67"
        },
        {
          "Model Variants": "With ﬁxed PE\nWith learnable PE",
          "WESAD": "93.49\n92.68",
          "CASE": "76.37\n76.46",
          "K-EmoCon": "80.32\n81.59"
        },
        {
          "Model Variants": "Our Model",
          "WESAD": "94.81",
          "CASE": "77.49",
          "K-EmoCon": "82.95"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 8: Ablation study of individual signal transformations and their combinations: average accuracy and F1score",
      "data": [
        {
          "Type": "",
          "WESAD\nCASE\nK-EmoCon": "S-2"
        },
        {
          "Type": "Single",
          "WESAD\nCASE\nK-EmoCon": "Acc\nF1"
        },
        {
          "Type": "N\nM\nP\nT\nC",
          "WESAD\nCASE\nK-EmoCon": "90.18\n89.16\n89.74\n87.86\n91.20\n89.33\n91.34\n90.87\n89.48\n88.06"
        },
        {
          "Type": "Same Domain",
          "WESAD\nCASE\nK-EmoCon": "Acc\nF1"
        },
        {
          "Type": "N+M\nP+T+C",
          "WESAD\nCASE\nK-EmoCon": "89.69\n88.43\n93.67\n92.88"
        },
        {
          "Type": "Cross Domain",
          "WESAD\nCASE\nK-EmoCon": "Acc\nF1"
        },
        {
          "Type": "N+P\nM+P\nN+T\nM+T\nN+C\nM+C",
          "WESAD\nCASE\nK-EmoCon": "92.15\n91.12\n91.75\n90.47\n92.95\n91.69\n91.51\n90.18\n91.28\n90.15\n90.08\n89.62"
        },
        {
          "Type": "All",
          "WESAD\nCASE\nK-EmoCon": "94.81\n93.69"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "2",
      "title": "Emotion Recognition from Multimodal Physiological Signals for Emotion Aware Healthcare Systems",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition and affective computing on vocal social media",
      "authors": [
        "W Dai",
        "D Han",
        "Y Dai",
        "D Xu"
      ],
      "year": "2015",
      "venue": "Information & Management"
    },
    {
      "citation_id": "4",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep neural network considering verbal and nonverbal speech sounds",
      "authors": [
        "K Huang",
        "C Wu",
        "Q Hong",
        "M Su",
        "Y Chen"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang",
        "W Zhang"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
      "authors": [
        "H Banville",
        "O Chehab",
        "A Hyvärinen",
        "D.-A Engemann",
        "A Gramfort"
      ],
      "year": "2021",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "9",
      "title": "Self-supervised contrastive learning for eeg-based sleep staging",
      "authors": [
        "X Jiang",
        "J Zhao",
        "B Du",
        "Z Yuan"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "10",
      "title": "Subject-aware contrastive learning for biosignals",
      "authors": [
        "J Cheng",
        "H Goh",
        "K Dogrusoz",
        "O Tuzel",
        "E Azemi"
      ],
      "year": "2020",
      "venue": "Subject-aware contrastive learning for biosignals"
    },
    {
      "citation_id": "11",
      "title": "Sigrep: Toward robust wearable emotion recognition with contrastive representation learning",
      "authors": [
        "V Dissanayake",
        "S Seneviratne",
        "R Rana",
        "E Wen",
        "T Kaluarachchi",
        "S Nanayakkara"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "StressNAS: Affect State and Stress Detection Using Neural Architecture Search",
      "authors": [
        "L Huynh",
        "T Nguyen",
        "T Nguyen",
        "S Pirttikangas",
        "P Siirtola"
      ],
      "year": "2021",
      "venue": "Association for Computing Machinery"
    },
    {
      "citation_id": "14",
      "title": "Intelligent stress monitoring assistant for first responders",
      "authors": [
        "K Lai",
        "S Yanushkevich",
        "V Shmerko"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Corrnet: Fine-grained emotion recognition for video watching using wearable physiological sensors",
      "authors": [
        "T Zhang",
        "A Ali",
        "C Wang",
        "A Hanjalic",
        "P Cesar"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "16",
      "title": "Unsupervised multimodal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "K Ross",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing",
      "doi": "10.1007/s12652-021-03462-9"
    },
    {
      "citation_id": "17",
      "title": "Self-supervised learning: Generative or contrastive",
      "authors": [
        "X Liu",
        "F Zhang",
        "Z Hou",
        "L Mian",
        "Z Wang",
        "J Zhang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "18",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "A simple framework for contrastive learning of visual representations"
    },
    {
      "citation_id": "19",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "20",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "21",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (amigos)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "Arousal-valence classification from peripheral physiological signals using long short-term memory networks",
      "authors": [
        "M Zitouni",
        "C Park",
        "U Lee",
        "L Hadjileontiadis",
        "A Khandoker"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "23",
      "title": "Emotion Recognition from Multimodal Physiological Signals for Emotion Aware Healthcare Systems",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition through integrating eeg and peripheral signals",
      "authors": [
        "Y Shu",
        "S Wang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "26",
      "title": "Leveraging multi-modal interactions among the intermediate representations of deep transformers for emotion recognition",
      "authors": [
        "Y Wu",
        "Z Zhang",
        "P Peng",
        "Y Zhao",
        "B Qin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge, ser. MuSe' 22",
      "doi": "10.1145/3551876.3554813"
    },
    {
      "citation_id": "27",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "arxiv": "arXiv:1906.00295"
    },
    {
      "citation_id": "28",
      "title": "Sparse fusion for multimodal transformers",
      "authors": [
        "Y Ding",
        "A Rich",
        "M Wang",
        "N Stier",
        "M Turk",
        "P Sen",
        "T Höllerer"
      ],
      "year": "2021",
      "venue": "Sparse fusion for multimodal transformers"
    },
    {
      "citation_id": "29",
      "title": "Multi-task self-supervised learning for human activity detection",
      "authors": [
        "A Saeed",
        "T Ozcelebi",
        "J Lukkien"
      ],
      "year": "2019",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol"
    },
    {
      "citation_id": "30",
      "title": "Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural networks",
      "authors": [
        "T Um",
        "F Pfister",
        "D Pichler",
        "S Endo",
        "M Lang",
        "S Hirche",
        "U Fietzek",
        "D Kulić"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction, ser. ICMI '17"
    },
    {
      "citation_id": "31",
      "title": "An empirical survey of data augmentation for time series classification with neural networks",
      "authors": [
        "B Iwana",
        "S Uchida"
      ],
      "year": "2021",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0254841"
    },
    {
      "citation_id": "32",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Information Processing Systems, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "34",
      "title": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction -ICMI '18"
    },
    {
      "citation_id": "35",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "K Sharma",
        "C Castellini",
        "E Van Den Broek",
        "A Albu-Schaeffer",
        "F Schwenker"
      ],
      "year": "2019",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "36",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "C Park",
        "N Cha",
        "S Kang",
        "A Kim",
        "A Khandoker",
        "L Hadjileontiadis",
        "A Oh",
        "Y Jeong",
        "U Lee"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "37",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "39",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Self-supervised contrastive learning for eeg-based sleep staging",
      "authors": [
        "X Jiang",
        "J Zhao",
        "B Du",
        "Z Yuan"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "41",
      "title": "Cardiologist-Level Arrhythmia Detection and Classification in Ambulatory Electrocardiograms Using a Deep Neural Network",
      "authors": [
        "A Hannun",
        "P Rajpurkar",
        "M Haghpanahi",
        "G Tison",
        "C Bourn",
        "M Turakhia",
        "A Ng"
      ],
      "year": "2019",
      "venue": "Nature medicine"
    },
    {
      "citation_id": "42",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia, ser. MM '19"
    },
    {
      "citation_id": "44",
      "title": "Bendr: Using transformers and a contrastive self-supervised learning task to learn from massive amounts of eeg data",
      "authors": [
        "D Kostas",
        "S Aroca-Ouellette",
        "F Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2021.653659"
    },
    {
      "citation_id": "45",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "How useful is self-supervised pretraining for visual tasks",
      "authors": [
        "A Newell",
        "J Deng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "47",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "48",
      "title": "Wearable physiological sensors reflect mental stress state in office-like situations",
      "authors": [
        "J Wijsman",
        "B Grundlehner",
        "H Liu",
        "J Penders",
        "H Hermens"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "49",
      "title": "Under pressure: Sensing stress of computer users",
      "authors": [
        "J Hernandez",
        "P Paredes",
        "A Roseway",
        "M Czerwinski"
      ],
      "year": "2014",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ser. CHI '14"
    },
    {
      "citation_id": "50",
      "title": "Towards an automatic early stress recognition system for office environments based on multimodal measurements: A review",
      "authors": [
        "A Alberdi",
        "A Aztiria",
        "A Basarab"
      ],
      "year": "2016",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "51",
      "title": "Physiological correlates of subjective emotional valence and arousal dynamics while viewing films",
      "authors": [
        "W Sato",
        "T Kochiyama",
        "S Yoshikawa"
      ],
      "year": "2020",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "52",
      "title": "Smil: Multimodal learning with severely missing modality",
      "authors": [
        "M Ma",
        "J Ren",
        "L Zhao",
        "S Tulyakov",
        "C Wu",
        "X Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "A transformer-based framework for multivariate time series representation learning",
      "authors": [
        "G Zerveas",
        "S Jayaraman",
        "D Patel",
        "A Bhamidipaty",
        "C Eickhoff"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, ser. KDD '21"
    }
  ]
}