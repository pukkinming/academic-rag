{
  "paper_id": "2409.05243v1",
  "title": "Mamba-Enhanced Text-Audio-Video Alignment Network For Emotion Recognition In Conversations",
  "published": "2024-09-08T23:09:22Z",
  "authors": [
    "Xinran Li",
    "Xiaomao Fan",
    "Qingyang Wu",
    "Xiaojiang Peng",
    "Ye Li"
  ],
  "keywords": [
    "Emotion recognition in conversations",
    "Multimodal Fusion",
    "Mamba",
    "Emotion classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERCs) is a vital area within multimodal interaction research, dedicated to accurately identifying and classifying the emotions expressed by speakers throughout a conversation. Traditional ERC approaches predominantly rely on unimodal cues-such as text, audio, or visual data-leading to limitations in their effectiveness. These methods encounter two significant challenges: 1)Consistency in multimodal information. Before integrating various modalities, it is crucial to ensure that the data from different sources is aligned and coherent. 2)Contextual information capture. Successfully fusing multimodal features requires a keen understanding of the evolving emotional tone, especially in lengthy dialogues where emotions may shift and develop over time. To address these limitations, we propose a novel Mamba-enhanced Text-Audio-Video alignment network (MaTAV) for the ERC task. MaTAV is with the advantages of aligning unimodal features to ensure consistency across different modalities and handling long input sequences to better capture contextual multimodal information. The extensive experiments on the MELD and IEMOCAP datasets demonstrate that MaTAV significantly outperforms existing state-of-the-art methods on the ERC task with a big margin. The source code is available at URL(https://github.com/Alena-Xinran/MaTAV).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversations (ERC) is a critical area of research in the field of multimodal interaction, which focuses on accurately identifying the emotions of speakers throughout various utterances within a conversation. Emotions play a significant role in human communication, influencing decision-making, social interactions, and personal well-being. Hence, developing a reliable ERC system is essential for applications in social media analysis  [1] ,  [2] , customer service  [3] ,  [4] , and mental health monitoring  [5] -  [7] .\n\nThis work is partially supported by the National Natural Science Foundation of China (No.62473267), and the Natural Science Foundation of Top Talent of SZTU (No.GDRC202318).\n\n* Xiaomao Fan is the corresponding author.\n\nTraditional ERC methods  [8] ,  [9]  have predominantly relied on unimodal cues such as text, audio, or visual data to classify emotions. While these methods have shown promising results, they often fail to capture the full spectrum of emotional nuances present in conversations. Recent advanced multimodal ERC methods  [10] -  [14]  have been introduced to address this limitation by integrating information from multiple modalities. However, these multimodal methods still face significant challenges in two main areas: 1)Achieving consistency across different modalities is challenging because the way emotions are expressed through text can be quite different from how they are conveyed through audio or visual cues. e.g., \"I feel something is a bit off.\" In the text modality, this phrase might be interpreted as mild concern, while in the audio modality, a trembling tone and rapid breathing could convey a stronger sense of fear, and the tense facial expression in the visual modality further intensifies this emotion. 2)Effectively capturing the contextual information is crucial for accurate emotion classification. Existing methods  [15] ,  [16]  often struggle to incorporate this contextual information during the fusion process, leading to a loss of critical emotional cues. e.g., in the MELD dataset, which features scenes from the TV show Friends, there's a moment where Ross confesses to Rachel at the airport, saying, \"I'm still in love with you.\" A fixed context window might interpret this as neutral, but by dynamically integrating contextual information can help model accurately captures Ross's underlying sadness. This is because it considers his prior experiences and fears that this confession won't change Rachel's decision.\n\nTo address the aforementioned issues, we propose MaTAV, a novel Mamba-enhanced text-audio-video alignment network designed for the ERC task. Inspired by the ALBEF framework  [17] , we propose a multimodal emotion contrastive loss (MEC-Loss) to align unimodal features and ensure consistency in multimodal information, alleviating the problem of discrepancy between modalities. Furthermore, we leverage the Mamba network architecture to address the challenge of effectively capturing contextual information during the fusion of multimodal features. Unlike traditional approaches that rely on a fixed context window, the Mamba network dynamically incorporates information from a broader context, making it particularly well-suited for processing long sequences. Additionally, Mamba is optimized for faster inference, allowing it to handle the complexities of long sequences more efficiently. This adaptability and speed make it especially effective in the ERC task, where the emotional tone can evolve over time. We conducted extensive experiments on two widelyused ERC datasets, MELD  [18]  and IEMOCAP  [19] , to evaluate the effectiveness of our proposed MaTAV network. The experiment results demonstrate that MaTAV significantly outperforms the existing state-of-the-art methods in the ERC task. The main contributions of this work can be summarized as follows:\n\n• We first introduce the Mamba network within the ERC task, which significantly enhances the ability to efficiently process lengthy multimodal sequences involving text, audio, and video. • We propose a novel multimodal emotion contrastive loss, i.e. MEC-Loss, designed to tackle the discrepancies between modalities in ERC tasks. The MEC-Loss focuses on aligning unimodal features, ensuring consistency across multimodal information. • The extensive experimental results on the MELD and IEMOCAP datasets reveal that the MaTAV framework significantly outperforms existing state-of-the-art methods by a considerable margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Overview",
      "text": "The MaTAV framework, as illustrated in Fig.  1 , consists of four key components: a text-audio-video encoders (TAV-encoders) module, a text-audio-video alignment (TAV-Alignment) module, a multimodal fusion module, and an emotion classifier. The main objective of MaTAV is to accurately identify the emotion label of each utterance from a predefined set of emotion categories. Specifically, the primary input to MaTAV is a dialogue comprising n utterances, which include text (T ), audio (A), and video (V ) data. For each utterance, specialized encoders-including RoBERTa  [20]  for text, WavLM  [21]  for audio, and VisExtNet  [10]  for video-are employed to extract relevant features from their respective modalities. Building upon the ALBEF approach  [17] , we introduce the multimodal emotion contrastive loss (MEC-Loss) to effectively align these extracted features prior to the fusion process.During the multimodal fusion phase, the Mamba network  [22]  is utilized to capture contextualized information from the integrated multimodal features, facilitating a more nuanced understanding of the emotional context within the dialogue. Finally, an emotion classifier, implemented as a Softmax layer, determines the emotion category associated with each utterance. The following subsections will provide an in-depth exploration of each module within the MaTAV framework, detailing their specific functions and interactions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Tav-Encoders",
      "text": "In this section, TAV-encoders utilize three specified encoders of RoBERTa  [20] , WavLM  [21] , and VisExtNet  [10]  to process input from three modalities of text, audio, and video, respectively. For text encoding, we leverage the RoBERTa which excels in sequence modeling and can be fine-tuned for emotion recognition tasks. RoBERTa transforms the text content {T 1 , T 2 , . . . , T n } into 256-dimensional feature vectors, denoted as {t 1 , t 2 , . . . , t n }. In the realm of audio processing, we utilize WavLM as the audio encoder. WavLM is particularly adept at handling adverse conditions, having been trained on extensive unlabeled speech data. The audio content {A 1 , A 2 , . . . , A n } is transformed into 1024-dimensional feature vectors, represented as {a 1 , a 2 , . . . , a n }. For video encoding, we adopt VisExtNet, which effectively captures facial expressions while minimizing the inclusion of extraneous visual information. This approach helps to mitigate redundancy in scene-related data. The video content {V 1 , V 2 , . . . , V n } is processed to yield 1000-dimensional features, denoted as {a 1 , a 2 , . . . , a n }.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Tav-Alignment",
      "text": "Inspired by the work of ALBEF  [17] , we introduce a novel text-audio-video alignment network called TAV-alignment as shown in Fig.  2 . This network incorporates a modality contrast loss, referred to as MEC-loss, to facilitate effective alignment among text, audio, and video modalities. By leveraging MECloss, TAV-alignment enhances the coherence and synchronization of these diverse data types, improving the ERC performance. Specifically, to compute the MEC-Loss, we first define the similarity scores with cosine between different modalities as follows:\n\nwhere k, q ∈ {T, A, V }. g q (•), and g k (•) represent projection heads that map the corresponding embeddings into normalized 256-dimensional representations. Besides, we follow the work of MoCo  [23] , maintaining three queues to store the most recent M representations of text, audio, and video from the momentum unimodal encoders, denoted as g ′ t (t ′ ), g ′ a (a ′ ), and g ′ v (v ′ ). The similarity calculations involving momentum encoders are defined as\n\nTo facilitate effective alignment of the text, audio, and video embeddings in the 256-dimensional space, we present the MEC-Loss aiming to maximize similarity scores for matching pairs while minimizing similarity scores for non-matching pairs, ensuring effective alignment of text, audio, and video embeddings:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Multimodal Fusion",
      "text": "Given the limitations of existing methods in the ERC task, we employ the Mamba Network  [22]  to effectively handle long sequence multimodal data within dialogues. The aligned features of T , A, and V from the different modalities are fed into a six-layer cross-attention mechanism. The cross-attention is formulated as follows:\n\nwhere W Q , W K , and W V are the weight matrices for the query, key, and value transformations, respectively. The attention computation via the cross-attention mechanism is expressed as:\n\nd k is the dimensional size of K. After calculating the attention, the fused features are combined using:\n\n, where W F is the weight matrix for merging the features. Prior to inputting these features into the Mamba Network module, they undergo transformation through a fully connected (FC) layer, yielding outputs denoted as {Z 1 , Z 2 , . . . , Z n }. To standardize the input dimensions and enhance the model's ability to learn complex relationships within the multimodal data, the features are adjusted to a suitable dimensionality for the Mamba network:z i = FC(f i ). The resulting output features are concatenated into a sequence:\n\nThis sequence is then input into the Mamba network, enabling comprehensive analysis and interpretation of the multimodal data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Emotion Classifier",
      "text": "The Mamba network is a streamlined end-to-end neural network architecture that operates without attention mechanisms or multi-layer perceptron (MLP) blocks. Following the paradigm of Mamba network, a Softmax layer is employed in the emotion classifier (Clf) to generate a probability distribution across the set of emotion categories. The emotion label with the highest probability is selected as the predicted emotion ŷi for the i-th utterance. This process can be mathematically expressed as:\n\nIn the equation (  8 ), ŷi represents the predicted emotion label for the i-th utterance, while W and b denote the weights and biases of the Softmax layer, respectively. Here, F i refers to the i-th feature in the output sequence F .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experimental Settings",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Datasets",
      "text": "In this study, we utilize two public avaliable datasets of IEMOCAP and MELD to evaluate the MaTAV performance. IEMOCAP  [19]  includes around 12 hours of video recordings of dyadic conversations. These videos are segmented into 7,433 utterances and 151 dialogues. Each utterance is annotated with one of six emotion labels: happiness, sadness, neutral, anger, excitement, and frustration. MELD  [18]  is a multi-party dataset derived from the TV series Friends. It consists of 13,708 utterances and 1,433 dialogues. Each utterance is annotated with one of seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Hyperparameter Settings: In this paper, the proposed MEMO-Memba is implemented on an Dell-Precision-T7920-Tower workstation using PyTorch 1.8.0, with a Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz, 250 GB memory, and an NVIDIA Quadro RTX 6000 GPU with 24 GB VRAM. The parameters for the model training are configured as follows: The batch size is set to 64, and the training will run for 100 epochs. The learning rate is specified at 0.0001, with a weight Evaluation Metrics: we employ the widely used weightedaverage F1 score (W F 1) as the evaluation metric for the emotion recognition performance of MaTAV.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results And Discussion",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Comparison With Baseline Models",
      "text": "Compared to existing methods, our MaTAV model achieves state-of-the-art results on both the IEMOCAP and MELD datasets, as illustrated in Tables I and II. On the IEMOCAP dataset, MaTAV demonstrates notable advantages across multiple emotion categories, particularly excelling in the Neutral category with the highest W F 1 score of 74.88%. The model also shows impressive performance in the Sadness and Frustration categories, achieving W F 1 scores of 85.26% and 70.59%, respectively. On the MELD dataset, MaTAV stands out in the Neutral, Joy, and Anger emotions, with W F 1 scores of 82.13%, 64.52%, and 56.28%, respectively. It performs well in the Surprise and Sadness categories as well, with scores of 55.64% and 38.21%. The superior performance of MaTAV in these categories can be attributed to its advanced dialogue understanding capabilities, which enable it to effectively capture nuanced expressions and variations in emotional tones. This allows for a more accurate and comprehensive emotional analysis in conversational contexts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Ablation Study",
      "text": "The ablation study focuses on demonstrating the performance contributions of two components of the Mamaba network and MEC-Loss, as shown in Table  I  and Table  II . When the Mamba network component is removed, there is a noticeable drop in W F 1 scores across various emotion categories. Specifically, in the IEMOCAP dataset, the W F 1 scores for Happiness, Neutral, and Excitement are significantly lower compared to the full MaTAV model. This indicates that the Mamba network component plays a crucial role in accurately capturing these positive emotions. Similarly, the removal of the MEC-Loss component also affects the performance of the model, particularly in the Anger category, where the W F 1 score drops considerably. This suggests that the MEC-Loss component is essential for the precise classification of more intense emotions such as anger. In the MELD dataset, the absence of the Mamba network component results in lower W F 1 scores in Surprise, Sadness, and Anger, indicating its importance in handling diverse emotional expressions. The removal of the MEC-Loss component leads to lower scores in Surprise, Fear, and Sadness, underscoring its significance in these negative emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this work, we present MaTAV, an innovative multimodal framework specifically designed to enhance performance in the ERC task. By incorporating MEC-Loss for effective alignment of features across text, audio, and video, and utilizing the Mamba Network for dynamic contextual integration, MaTAV adeptly addresses two critical challenges: ensuring consistency in multimodal information and capturing contextual nuances. Experimental evaluations on the MELD and IEMOCAP datasets demonstrate that MaTAV significantly surpasses stateof-the-art methods, resulting in substantial improvements in emotion recognition across various modalities and dialogue turns.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall network architecture of MaTAV. It consists of four",
      "page": 2
    },
    {
      "caption": "Figure 2: The network architecture of Text-Audio-Video Alignment (TAV-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Shenzhen, China": "ye.li@siat.ac.cn"
        },
        {
          "Shenzhen, China": "Traditional ERC methods [8], [9] have predominantly relied"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "on unimodal cues such as text, audio, or visual data to classify"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "emotions. While these methods have shown promising results,"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "they\noften\nfail\nto\ncapture\nthe\nfull\nspectrum of\nemotional"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "nuances present in conversations. Recent advanced multimodal"
        },
        {
          "Shenzhen, China": "ERC methods [10]–[14] have been introduced to address this"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "limitation by integrating information from multiple modali-"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "ties. However,\nthese multimodal methods still\nface significant"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "challenges in two main areas: 1)Achieving consistency across"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "different modalities is challenging because the way emotions"
        },
        {
          "Shenzhen, China": "are\nexpressed through text\ncan be quite different\nfrom how"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "they are conveyed through audio or visual cues. e.g., ”I\nfeel"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "something\nis\na\nbit\noff.”\nIn\nthe\ntext modality,\nthis\nphrase"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "might\nbe\ninterpreted\nas mild\nconcern, while\nin\nthe\naudio"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "modality, a trembling tone and rapid breathing could convey"
        },
        {
          "Shenzhen, China": "a stronger sense of fear, and the tense facial expression in the"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "visual modality further\nintensifies\nthis emotion. 2)Effectively"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "capturing the contextual\ninformation is crucial\nfor accurate"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "emotion classification. Existing methods [15], [16] often strug-"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "gle to incorporate this contextual information during the fusion"
        },
        {
          "Shenzhen, China": "process,\nleading\nto\na\nloss\nof\ncritical\nemotional\ncues.\ne.g.,"
        },
        {
          "Shenzhen, China": "in\nthe MELD dataset, which\nfeatures\nscenes\nfrom the TV"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "show Friends,\nthere’s\na moment where Ross\nconfesses\nto"
        },
        {
          "Shenzhen, China": "Rachel\nat\nthe\nairport,\nsaying,\n”I’m still\nin love with you.”"
        },
        {
          "Shenzhen, China": "A fixed context window might\ninterpret\nthis as neutral, but by"
        },
        {
          "Shenzhen, China": "dynamically integrating contextual information can help model"
        },
        {
          "Shenzhen, China": "accurately captures Ross’s underlying sadness. This is because"
        },
        {
          "Shenzhen, China": "it considers his prior experiences and fears that this confession"
        },
        {
          "Shenzhen, China": "won’t change Rachel’s decision."
        },
        {
          "Shenzhen, China": "To address the aforementioned issues, we propose MaTAV,"
        },
        {
          "Shenzhen, China": "a novel Mamba-enhanced text-audio-video alignment network"
        },
        {
          "Shenzhen, China": "designed for\nthe ERC task.\nInspired by the ALBEF frame-"
        },
        {
          "Shenzhen, China": "work [17], we propose a multimodal emotion contrastive loss"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "(MEC-Loss)\nto\nalign\nunimodal\nfeatures\nand\nensure\nconsis-"
        },
        {
          "Shenzhen, China": ""
        },
        {
          "Shenzhen, China": "tency in multimodal\ninformation,\nalleviating the problem of"
        },
        {
          "Shenzhen, China": "discrepancy\nbetween modalities.\nFurthermore, we\nleverage"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "outperforms the existing state-of-the-art methods in the ERC": "task. The main contributions of this work can be summarized",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "as follows:",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "an in-depth exploration of\neach module within the MaTAV"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "• We first\nintroduce the Mamba network within the ERC",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "framework, detailing their specific functions and interactions."
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "task, which significantly enhances the ability to efficiently",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "process\nlengthy multimodal\nsequences\ninvolving\ntext,",
          "module, and an emotion classifier.": "B. TAV-Encoders"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "audio, and video.",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "In this section, TAV-encoders utilize three specified encoders"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "• We\npropose\na\nnovel multimodal\nemotion\ncontrastive",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "of RoBERTa [20], WavLM [21], and VisExtNet\n[10]\nto pro-"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "loss,\ni.e. MEC-Loss, designed to tackle the discrepancies",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "cess\ninput\nfrom three modalities of\ntext,\naudio,\nand video,"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "between modalities\nin ERC tasks. The MEC-Loss\nfo-",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "respectively. For\ntext\nencoding, we\nleverage\nthe RoBERTa"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "cuses on aligning unimodal features, ensuring consistency",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "which\nexcels\nin\nsequence modeling\nand\ncan\nbe fine-tuned"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "across multimodal\ninformation.",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "for emotion recognition tasks. RoBERTa transforms\nthe text"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "• The\nextensive\nexperimental\nresults\non\nthe MELD and",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "content {T1, T2, . . . , Tn} into 256-dimensional feature vectors,"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "IEMOCAP datasets\nreveal\nthat\nthe MaTAV framework",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "In the realm of audio processing,\ndenoted as {t1, t2, . . . , tn}."
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "significantly outperforms\nexisting state-of-the-art meth-",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "we\nutilize WavLM as\nthe\naudio\nencoder. WavLM is\npar-"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "ods by a considerable margin.",
          "module, and an emotion classifier.": ""
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "ticularly adept\nat handling adverse\nconditions, having been"
        },
        {
          "outperforms the existing state-of-the-art methods in the ERC": "",
          "module, and an emotion classifier.": "trained on extensive unlabeled speech data. The\naudio con-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Mamba network architecture to address\nthe challenge of": "effectively capturing contextual\ninformation during the fusion"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "of multimodal features. Unlike traditional approaches that rely"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "on a fixed context window,\nthe Mamba network dynamically"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "incorporates\ninformation from a broader\ncontext, making it"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "particularly well-suited for processing long sequences. Addi-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "tionally, Mamba is optimized for\nfaster\ninference, allowing it"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "to handle the complexities of long sequences more efficiently."
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "This\nadaptability\nand\nspeed make\nit\nespecially\neffective\nin"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "the ERC task, where\nthe\nemotional\ntone\ncan\nevolve\nover"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "time. We\nconducted\nextensive\nexperiments\non\ntwo widely-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "used ERC datasets, MELD [18]\nand\nIEMOCAP\n[19],\nto"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "evaluate the effectiveness of our proposed MaTAV network."
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "The experiment\nresults demonstrate that MaTAV significantly"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "outperforms the existing state-of-the-art methods in the ERC"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "task. The main contributions of this work can be summarized"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "as follows:"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "• We first\nintroduce the Mamba network within the ERC"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "task, which significantly enhances the ability to efficiently"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "process\nlengthy multimodal\nsequences\ninvolving\ntext,"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "audio, and video."
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "• We\npropose\na\nnovel multimodal\nemotion\ncontrastive"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "loss,\ni.e. MEC-Loss, designed to tackle the discrepancies"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "between modalities\nin ERC tasks. The MEC-Loss\nfo-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "cuses on aligning unimodal features, ensuring consistency"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "across multimodal\ninformation."
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "• The\nextensive\nexperimental\nresults\non\nthe MELD and"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "IEMOCAP datasets\nreveal\nthat\nthe MaTAV framework"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "significantly outperforms\nexisting state-of-the-art meth-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "ods by a considerable margin."
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "II. METHODOLOGY"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "A. Overview"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "The MaTAV framework,\nas\nillustrated\nin\nFig.\n1,\ncon-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "sists\nof\nfour\nkey\ncomponents:\na\ntext-audio-video\nencoders"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "(TAV-encoders) module,\na\ntext-audio-video alignment\n(TAV-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "Alignment) module, a multimodal fusion module, and an emo-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "tion classifier. The main objective of MaTAV is to accurately"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "identify the emotion label of each utterance from a predefined"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "set of\nemotion categories. Specifically,\nthe primary input\nto"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "MaTAV is a dialogue comprising n utterances, which include"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "text\n(T ),\naudio\n(A),\nand\nvideo\n(V )\ndata.\nFor\neach\nutter-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "ance, specialized encoders—including RoBERTa [20] for text,"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "WavLM [21]\nfor\naudio,\nand VisExtNet\n[10]\nfor video—are"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "employed\nto\nextract\nrelevant\nfeatures\nfrom their\nrespective"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "modalities. Building\nupon\nthe ALBEF\napproach\n[17], we"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "introduce\nthe multimodal\nemotion\ncontrastive\nloss\n(MEC-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "Loss)\nto\neffectively\nalign\nthese\nextracted\nfeatures\nprior\nto"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "the\nfusion process.During the multimodal\nfusion phase,\nthe"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "Mamba network [22]\nis utilized to capture contextualized in-"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "formation from the integrated multimodal features, facilitating"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "a more nuanced understanding of the emotional context within"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": ""
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "the dialogue. Finally,\nan emotion classifier,\nimplemented as"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "a Softmax layer, determines\nthe emotion category associated"
        },
        {
          "the Mamba network architecture to address\nthe challenge of": "with each utterance. The following subsections will provide"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dk is the dimensional size of K. After calculating the attention,": "the fused features are combined using: f = WF [t||a||v], where"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "is\nthe weight matrix for merging the\nfeatures. Prior\nto\nWF"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "inputting these features into the Mamba Network module, they"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "undergo transformation through a fully connected (FC)\nlayer,"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "yielding outputs denoted as {Z1, Z2, . . . , Zn}. To standardize"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "the\ninput\ndimensions\nand\nenhance\nthe model’s\nability\nto"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "learn complex relationships within the multimodal data,\nthe"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "features\nare\nadjusted\nto\na\nsuitable\ndimensionality\nfor\nthe"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "resulting output\nfeatures\nMamba network:zi = FC(fi). The"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "are concatenated into a sequence: x = [Z1, Z2, . . . , Zn]. This"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "sequence\nis\nthen\ninput\ninto\nthe Mamba\nnetwork,\nenabling"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "comprehensive analysis and interpretation of\nthe multimodal"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "data."
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "E. Emotion Classifier"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "The Mamba\nnetwork\nis\na\nstreamlined\nend-to-end\nneural"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "network architecture\nthat operates without\nattention mecha-"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "nisms or multi-layer perceptron (MLP) blocks. Following the"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "paradigm of Mamba network,\na Softmax layer\nis\nemployed"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "in the emotion classifier\n(Clf)\nto generate a probability dis-"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "tribution across\nthe\nset of\nemotion categories. The\nemotion"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "label with the highest probability is selected as the predicted"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "for\nthe i-th utterance. This process can be mathe-\nemotion ˆyi"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "matically expressed as:"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "(8)\nyi = argmax(Softmax(W Fi + b))"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "In the equation (8), ˆyi represents the predicted emotion label"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "for\nthe i-th utterance, while W and b denote the weights and"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "biases of\nthe Softmax layer,\nrefers\nto\nrespectively. Here, Fi"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "the i-th feature in the output sequence F ."
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "III. EXPERIMENTAL SETTINGS"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "A. Datasets"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "In this\nstudy, we utilize\ntwo public\navaliable datasets of"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "IEMOCAP and MELD to evaluate the MaTAV performance."
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "IEMOCAP [19]\nincludes around 12 hours of video record-"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "ings\nof\ndyadic\nconversations. These\nvideos\nare\nsegmented"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "into 7,433 utterances\nand 151 dialogues. Each utterance\nis"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "annotated with one of six emotion labels: happiness, sadness,"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "neutral, anger, excitement, and frustration."
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "MELD [18]\nis\na multi-party\ndataset\nderived\nfrom the TV"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "series\nFriends.\nIt\nconsists\nof\n13,708\nutterances\nand\n1,433"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "dialogues. Each\nutterance\nis\nannotated with\none\nof\nseven"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "emotion categories: anger, disgust,\nfear,\njoy, neutral, sadness,"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "and surprise."
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "B.\nImplementation Details"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "Hyperparameter\nSettings:\nIn\nthis\npaper,\nthe\nproposed"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "MEMO-Memba is implemented on an Dell-Precision-T7920-"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "Tower workstation\nusing\nPyTorch\n1.8.0, with\na\nIntel(R)"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "Xeon(R) Gold 6248R CPU @ 3.00GHz, 250 GB memory, and"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "an NVIDIA Quadro RTX 6000 GPU with 24 GB VRAM. The"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "parameters\nfor\nthe model\ntraining are configured as\nfollows:"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "The batch size is set\nto 64, and the training will\nrun for 100"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": "epochs. The learning rate is specified at 0.0001, with a weight"
        },
        {
          "dk is the dimensional size of K. After calculating the attention,": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Neutral"
        },
        {
          "TABLE I": "68.38"
        },
        {
          "TABLE I": "61.73"
        },
        {
          "TABLE I": "63.80"
        },
        {
          "TABLE I": "61.50"
        },
        {
          "TABLE I": "67.08"
        },
        {
          "TABLE I": "74.88"
        },
        {
          "TABLE I": "63.77"
        },
        {
          "TABLE I": "77.89"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Fear"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "10.40"
        },
        {
          "TABLE II": "29.67"
        },
        {
          "TABLE II": "25.91"
        },
        {
          "TABLE II": "26.26"
        },
        {
          "TABLE II": "13.14"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "82.13\n50.35\n13.14\nMaTAVw/oM EC−Loss",
          "37.13\n51.47\n25.85\n54.26\n63.54": "28.63\n66.38\n16.14\n53.12\n64.83"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "decay of 0.00001. The model\nincorporates 6 layers\nfor\nthe",
          "37.13\n51.47\n25.85\n54.26\n63.54": "the Mamba network component\nis removed,\nthere is a notice-"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "cross-attention mechanism. Additionally, the parameter for the",
          "37.13\n51.47\n25.85\n54.26\n63.54": "able drop in W F 1 scores across various emotion categories."
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "MEC loss is set\nto 0.3.",
          "37.13\n51.47\n25.85\n54.26\n63.54": "Specifically,\nin the IEMOCAP dataset,\nthe W F 1 scores\nfor"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "Evaluation Metrics: we employ the widely used weighted-",
          "37.13\n51.47\n25.85\n54.26\n63.54": "Happiness, Neutral,\nand Excitement\nare\nsignificantly\nlower"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "average F1\nscore\n(W F 1)\nas\nthe\nevaluation metric\nfor\nthe",
          "37.13\n51.47\n25.85\n54.26\n63.54": "compared to the full MaTAV model. This\nindicates\nthat\nthe"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "emotion recognition performance of MaTAV.",
          "37.13\n51.47\n25.85\n54.26\n63.54": "Mamba network component plays a crucial\nrole in accurately"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "capturing these positive\nemotions. Similarly,\nthe\nremoval of"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "IV. RESULTS AND DISCUSSION",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "the MEC-Loss component also affects the performance of the"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "A. Comparison with Baseline Models",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "model, particularly in the Anger\ncategory, where\nthe W F 1"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "Compared to existing methods, our MaTAV model achieves",
          "37.13\n51.47\n25.85\n54.26\n63.54": "score drops\nconsiderably. This\nsuggests\nthat\nthe MEC-Loss"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "state-of-the-art\nresults\non\nboth\nthe\nIEMOCAP and MELD",
          "37.13\n51.47\n25.85\n54.26\n63.54": "component\nis essential\nfor\nthe precise classification of more"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "datasets, as\nillustrated in Tables\nI and II. On the IEMOCAP",
          "37.13\n51.47\n25.85\n54.26\n63.54": "intense\nemotions\nsuch as\nanger.\nIn the MELD dataset,\nthe"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "dataset, MaTAV demonstrates notable advantages across mul-",
          "37.13\n51.47\n25.85\n54.26\n63.54": "absence of\nthe Mamba network component\nresults\nin lower"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "tiple emotion categories, particularly excelling in the Neutral",
          "37.13\n51.47\n25.85\n54.26\n63.54": "W F 1 scores\nin Surprise, Sadness, and Anger,\nindicating its"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "category with the highest W F 1 score of 74.88%. The model",
          "37.13\n51.47\n25.85\n54.26\n63.54": "importance\nin\nhandling\ndiverse\nemotional\nexpressions. The"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "also shows impressive performance in the Sadness and Frustra-",
          "37.13\n51.47\n25.85\n54.26\n63.54": "removal of\nthe MEC-Loss component\nleads\nto lower\nscores"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "tion categories, achieving W F 1 scores of 85.26% and 70.59%,",
          "37.13\n51.47\n25.85\n54.26\n63.54": "in Surprise, Fear, and Sadness, underscoring its\nsignificance"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "respectively. On\nthe MELD dataset, MaTAV stands\nout\nin",
          "37.13\n51.47\n25.85\n54.26\n63.54": "in these negative emotions."
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "the Neutral,\nJoy, and Anger emotions, with W F 1 scores of",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "V. CONCLUSION"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "82.13%, 64.52%, and 56.28%,\nrespectively.\nIt performs well",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "In this work, we present MaTAV, an innovative multimodal"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "in the Surprise\nand Sadness\ncategories\nas well, with scores",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "framework specifically designed to enhance performance in the"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "of 55.64% and 38.21%. The superior performance of MaTAV",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "ERC task. By incorporating MEC-Loss for effective alignment"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "in these categories can be attributed to its advanced dialogue",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "of\nfeatures\nacross\ntext,\naudio,\nand\nvideo,\nand\nutilizing\nthe"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "understanding capabilities, which enable it\nto effectively cap-",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "Mamba Network for dynamic contextual\nintegration, MaTAV"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "ture nuanced expressions\nand variations\nin emotional\ntones.",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "adeptly\naddresses\ntwo\ncritical\nchallenges:\nensuring\nconsis-"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "This allows for a more accurate and comprehensive emotional",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "tency in multimodal\ninformation and capturing contextual nu-"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "analysis in conversational contexts.",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "ances. Experimental evaluations on the MELD and IEMOCAP"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "B. Ablation Study",
          "37.13\n51.47\n25.85\n54.26\n63.54": ""
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "",
          "37.13\n51.47\n25.85\n54.26\n63.54": "datasets demonstrate that MaTAV significantly surpasses state-"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "The\nablation\nstudy\nfocuses\non\ndemonstrating\nthe\nperfor-",
          "37.13\n51.47\n25.85\n54.26\n63.54": "of-the-art methods,\nresulting in substantial\nimprovements\nin"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "mance contributions of\ntwo components of\nthe Mamaba net-",
          "37.13\n51.47\n25.85\n54.26\n63.54": "emotion recognition across various modalities\nand dialogue"
        },
        {
          "80.86\n54.23\n26.26\nMaTAVw/oM amba": "work and MEC-Loss, as shown in Table I and Table II. When",
          "37.13\n51.47\n25.85\n54.26\n63.54": "turns."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "[9] Xianxun Zhu, Yao Huang, Xiangyang Wang, and Rui Wang, “Emotion"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "recognition\nbased\non\nbrain-like multimodal\nhierarchical\nperception,”"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Multimedia Tools and Applications, vol. 83, no. 18, pp. 56039–56057,"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "2024."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Tao\nShi\nand\nShao-Lun\nHuang,\n“Multiemo:\nAn\nattention-based"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "correlation-aware multimodal fusion framework for emotion recognition"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "the 61st Annual Meeting of\nthe\nin conversations,”\nin Proceedings of"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Association for Computational Linguistics\n(Volume 1: Long Papers),"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "2023, pp. 14752–14766."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Tao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin Li,\n“Deep im-"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "balanced learning for multimodal emotion recognition in conversations,”"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "IEEE Transactions on Artificial\nIntelligence, 2024."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Zhanjiang Yang, Meiyu Qiu, Xiaomao Fan, Genan Dai, Wenjun Ma,"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Xiaojiang Peng, Xianghua Fu, and Ye Li, “cvan: A novel sleep staging"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "method via cross-view alignment network,” IEEE Journal of Biomedical"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "and Health Informatics, 2024."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Feiyu Chen, Jie Shao, Shuyuan Zhu, and Heng Tao Shen, “Multivariate,"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "multi-frequency and multimodal: Rethinking graph neural networks for"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "the IEEE/CVF\nemotion recognition in conversation,”\nin Proceedings of"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Conference\non Computer Vision\nand Pattern Recognition,\n2023,\npp."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "10761–10770."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Tao Meng, Fuchen Zhang, Yuntao Shou, Hongen Shao, Wei Ai,\nand"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Keqin Li,\n“Masked graph learning with recurrent alignment\nfor multi-"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "IEEE/ACM Transactions\nmodal emotion recognition in conversation,”"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "on Audio, Speech, and Language Processing, 2024."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Zhiwei Yang,\nJing Ma, Hechang Chen, Yunke Zhang, and Yi Chang,"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "“Hitrans:\na hierarchical\ntransformer network for nested named entity"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "the Association for Computational Linguis-\nrecognition,” in Findings of"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "tics: EMNLP 2021, 2021, pp. 124–132."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "and Yekun Chai,\n“Coin: Conversational\ninteractive"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "networks for emotion recognition in conversation,” in Proceedings of the"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Third Workshop on Multimodal Artificial Intelligence, 2021, pp. 12–18."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Junnan Li, Ramprasaath\nSelvaraju, Akhilesh Gotmare,\nShafiq\nJoty,"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Caiming Xiong,\nand Steven Chu Hong Hoi,\n“Align before\nfuse: Vi-"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "sion and language representation learning with momentum distillation,”"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Advances in neural\ninformation processing systems, vol. 34, pp. 9694–"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "9705, 2021."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Soujanya\nPoria, Devamanyu Hazarika, Navonil Majumder, Gautam"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Naik, Erik Cambria, and Rada Mihalcea,\n“Meld: A multimodal multi-"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "arXiv preprint\nparty dataset\nfor emotion recognition in conversations,”"
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "arXiv:1810.02508, 2018."
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": ""
        },
        {
          "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "database,”\nLanguage resources and evaluation, vol. 42, pp. 335–359,"
        },
        {
          "REFERENCES": "[1] Ge Wang, Li Tan, Ziliang Shang, and He Liu, “Multimodal dual emotion",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "2008."
        },
        {
          "REFERENCES": "with fusion of visual sentiment\nfor\nrumor detection,” Multimedia Tools",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[20]\nTaewoon Kim and\nPiek Vossen,\n“Emoberta:\nSpeaker-aware\nemo-"
        },
        {
          "REFERENCES": "and Applications, vol. 83, no. 10, pp. 29805–29826, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "arXiv\npreprint\ntion\nrecognition\nin\nconversation\nwith\nroberta,”"
        },
        {
          "REFERENCES": "[2]\nZhiwei Liu, Tianlin Zhang, Kailai Yang, Paul Thompson, Zeping Yu, and",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "arXiv:2108.12009, 2021."
        },
        {
          "REFERENCES": "Sophia Ananiadou,\n“Emotion detection for misinformation: A review,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[21]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu,"
        },
        {
          "REFERENCES": "Information Fusion, p. 102300, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "Zhuo Chen,\nJinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao,"
        },
        {
          "REFERENCES": "[3] Duan Chen, Huang Zhengwei, Tan Yiting, Min\nJintao,\nand Ribesh",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "et al.,\n“Wavlm: Large-scale self-supervised pre-training for\nfull\nstack"
        },
        {
          "REFERENCES": "Khanal,\n“Emotion\nand\nsentiment\nanalysis\nfor\nintelligent\ncustomer",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "speech processing,” IEEE Journal of Selected Topics in Signal Process-"
        },
        {
          "REFERENCES": "service conversation using a multi-task ensemble framework,” Cluster",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "ing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "REFERENCES": "Computing, vol. 27, no. 2, pp. 2099–2115, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[22] Albert Gu and Tri Dao,\n“Mamba: Linear-time sequence modeling with"
        },
        {
          "REFERENCES": "[4] Yiting Guo, Yilin Li, De Liu, and Sean Xin Xu,\n“Measuring service",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "selective state spaces,” arXiv preprint arXiv:2312.00752, 2023."
        },
        {
          "REFERENCES": "quality\nbased\non\ncustomer\nemotion: An\nexplainable\nai\napproach,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick,"
        },
        {
          "REFERENCES": "Decision Support Systems, vol. 176, pp. 114051, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "“Momentum contrast\nfor unsupervised visual\nrepresentation learning,”"
        },
        {
          "REFERENCES": "[5]\nZifan Jiang, Salman Seyedi, Emily Griner, Ahmed Abbasi, Ali Bahrami",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "the\nIEEE/CVF conference on computer\nvision and\nin Proceedings of"
        },
        {
          "REFERENCES": "Rad, Hyeokhyen Kwon, Robert O Cotes, and Gari D Clifford,\n“Multi-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "pattern recognition, 2020, pp. 9729–9738."
        },
        {
          "REFERENCES": "modal mental health digital biomarker analysis from remote interviews",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[24]\nJiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang Zeng,\n“Ga2mif:"
        },
        {
          "REFERENCES": "using facial, vocal, linguistic, and cardiovascular patterns,” IEEE Journal",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "graph\nand\nattention\nbased\ntwo-stage multi-source\ninformation\nfusion"
        },
        {
          "REFERENCES": "of Biomedical and Health Informatics, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "IEEE Transactions on affective\nfor conversational emotion detection,”"
        },
        {
          "REFERENCES": "[6] Xiaomao Fan, Xianhui Chen, Wenjun Ma, and Weidong Gao,\n“Bafnet:",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "computing, 2023."
        },
        {
          "REFERENCES": "Bottleneck attention based fusion network for\nsleep apnea detection,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[25]\nJingwen Hu, Yuchen Liu,\nJinming Zhao,\nand Qin\nJin,\n“Mmgcn:"
        },
        {
          "REFERENCES": "IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 5, pp.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "Multimodal\nfusion\nvia\ndeep\ngraph\nconvolution\nnetwork\nfor\nemotion"
        },
        {
          "REFERENCES": "2473–2484, 2023.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "recognition in conversation,” arXiv preprint arXiv:2107.06779, 2021."
        },
        {
          "REFERENCES": "[7] Xiaoyue\nJi, Zhekang Dong, Yifeng Han, Chun Sing Lai, Guangdong",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[26] Minjie Ren, Xiangdong Huang, Wenhui Li, Dan Song,\nand Weizhi"
        },
        {
          "REFERENCES": "Zhou,\nand Donglian Qi,\n“Emsn: An energy-efficient memristive\nse-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "Nie,\n“Lr-gcn: Latent\nrelation-aware graph convolutional network for"
        },
        {
          "REFERENCES": "quencer\nnetwork\nfor\nhuman\nemotion\nclassification\nin mental\nhealth",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "conversational emotion recognition,” IEEE Transactions on Multimedia,"
        },
        {
          "REFERENCES": "monitoring,”\nIEEE Transactions on Consumer Electronics, vol. 69, no.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "vol. 24, pp. 4422–4432, 2021."
        },
        {
          "REFERENCES": "4, pp. 1005–1016, 2023.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "[27] Wei Ai, Yuntao Shou, Tao Meng, and Keqin Li,\n“Der-gcn: Dialog and"
        },
        {
          "REFERENCES": "[8] K Ezzameli and H Mahersia,\n“Emotion recognition from unimodal\nto",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "event relation-aware graph convolutional neural network for multimodal"
        },
        {
          "REFERENCES": "multimodal analysis: A review,”\nInformation Fusion, p. 101847, 2023.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "IEEE Transactions on Neural Networks\ndialog emotion recognition,”"
        },
        {
          "REFERENCES": "[9] Xianxun Zhu, Yao Huang, Xiangyang Wang, and Rui Wang, “Emotion",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": "and Learning Systems, 2024."
        },
        {
          "REFERENCES": "recognition\nbased\non\nbrain-like multimodal\nhierarchical\nperception,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Multimedia Tools and Applications, vol. 83, no. 18, pp. 56039–56057,",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[10]\nTao\nShi\nand\nShao-Lun\nHuang,\n“Multiemo:\nAn\nattention-based",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "correlation-aware multimodal fusion framework for emotion recognition",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "the 61st Annual Meeting of\nthe\nin conversations,”\nin Proceedings of",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Association for Computational Linguistics\n(Volume 1: Long Papers),",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "2023, pp. 14752–14766.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[11]\nTao Meng, Yuntao Shou, Wei Ai, Nan Yin, and Keqin Li,\n“Deep im-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "balanced learning for multimodal emotion recognition in conversations,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "IEEE Transactions on Artificial\nIntelligence, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[12]\nZhanjiang Yang, Meiyu Qiu, Xiaomao Fan, Genan Dai, Wenjun Ma,",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Xiaojiang Peng, Xianghua Fu, and Ye Li, “cvan: A novel sleep staging",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "method via cross-view alignment network,” IEEE Journal of Biomedical",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "and Health Informatics, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[13]\nFeiyu Chen, Jie Shao, Shuyuan Zhu, and Heng Tao Shen, “Multivariate,",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "multi-frequency and multimodal: Rethinking graph neural networks for",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "the IEEE/CVF\nemotion recognition in conversation,”\nin Proceedings of",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Conference\non Computer Vision\nand Pattern Recognition,\n2023,\npp.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "10761–10770.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[14]\nTao Meng, Fuchen Zhang, Yuntao Shou, Hongen Shao, Wei Ai,\nand",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Keqin Li,\n“Masked graph learning with recurrent alignment\nfor multi-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "IEEE/ACM Transactions\nmodal emotion recognition in conversation,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "on Audio, Speech, and Language Processing, 2024.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[15]\nZhiwei Yang,\nJing Ma, Hechang Chen, Yunke Zhang, and Yi Chang,",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "“Hitrans:\na hierarchical\ntransformer network for nested named entity",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "the Association for Computational Linguis-\nrecognition,” in Findings of",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "tics: EMNLP 2021, 2021, pp. 124–132.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[16] Haidong Zhang\nand Yekun Chai,\n“Coin: Conversational\ninteractive",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "networks for emotion recognition in conversation,” in Proceedings of the",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Third Workshop on Multimodal Artificial Intelligence, 2021, pp. 12–18.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[17]\nJunnan Li, Ramprasaath\nSelvaraju, Akhilesh Gotmare,\nShafiq\nJoty,",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Caiming Xiong,\nand Steven Chu Hong Hoi,\n“Align before\nfuse: Vi-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "sion and language representation learning with momentum distillation,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Advances in neural\ninformation processing systems, vol. 34, pp. 9694–",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "9705, 2021.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[18]\nSoujanya\nPoria, Devamanyu Hazarika, Navonil Majumder, Gautam",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Naik, Erik Cambria, and Rada Mihalcea,\n“Meld: A multimodal multi-",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "arXiv preprint\nparty dataset\nfor emotion recognition in conversations,”",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "arXiv:1810.02508, 2018.",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "[19] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        },
        {
          "REFERENCES": "Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S",
          "Narayanan,\n“Iemocap:\nInteractive\nemotional\ndyadic motion\ncapture": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal dual emotion with fusion of visual sentiment for rumor detection",
      "authors": [
        "Ge Wang",
        "Li Tan",
        "Ziliang Shang",
        "He Liu"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "2",
      "title": "Emotion detection for misinformation: A review",
      "authors": [
        "Zhiwei Liu",
        "Tianlin Zhang",
        "Kailai Yang",
        "Paul Thompson",
        "Zeping Yu",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Emotion and sentiment analysis for intelligent customer service conversation using a multi-task ensemble framework",
      "authors": [
        "Duan Chen",
        "Huang Zhengwei",
        "Tan Yiting",
        "Min Jintao",
        "Ribesh Khanal"
      ],
      "year": "2024",
      "venue": "Cluster Computing"
    },
    {
      "citation_id": "4",
      "title": "Measuring service quality based on customer emotion: An explainable ai approach",
      "authors": [
        "Yiting Guo",
        "Yilin Li",
        "De Liu",
        "Sean Xu"
      ],
      "year": "2024",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "5",
      "title": "Multimodal mental health digital biomarker analysis from remote interviews using facial, vocal, linguistic, and cardiovascular patterns",
      "authors": [
        "Zifan Jiang",
        "Salman Seyedi",
        "Emily Griner",
        "Ahmed Abbasi",
        "Ali Bahrami Rad",
        "Hyeokhyen Kwon",
        "Robert Cotes",
        "Gari D Clifford"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "6",
      "title": "Bafnet: Bottleneck attention based fusion network for sleep apnea detection",
      "authors": [
        "Xiaomao Fan",
        "Xianhui Chen",
        "Wenjun Ma",
        "Weidong Gao"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "7",
      "title": "Emsn: An energy-efficient memristive sequencer network for human emotion classification in mental health monitoring",
      "authors": [
        "Xiaoyue Ji",
        "Zhekang Dong",
        "Yifeng Han",
        "Chun Lai",
        "Guangdong Zhou",
        "Donglian Qi"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition based on brain-like multimodal hierarchical perception",
      "authors": [
        "Xianxun Zhu",
        "Yao Huang",
        "Xiangyang Wang",
        "Rui Wang"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "10",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "cvan: A novel sleep staging method via cross-view alignment network",
      "authors": [
        "Zhanjiang Yang",
        "Meiyu Qiu",
        "Xiaomao Fan",
        "Genan Dai",
        "Wenjun Ma",
        "Xiaojiang Peng",
        "Xianghua Fu",
        "Ye Li"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "13",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Jie Shao",
        "Shuyuan Zhu",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Masked graph learning with recurrent alignment for multimodal emotion recognition in conversation",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Hongen Shao",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Hitrans: a hierarchical transformer network for nested named entity recognition",
      "authors": [
        "Zhiwei Yang",
        "Jing Ma",
        "Hechang Chen",
        "Yunke Zhang",
        "Yi Chang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "16",
      "title": "Coin: Conversational interactive networks for emotion recognition in conversation",
      "authors": [
        "Haidong Zhang",
        "Yekun Chai"
      ],
      "year": "2021",
      "venue": "Proceedings of the Third Workshop on Multimodal Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "authors": [
        "Junnan Li",
        "Ramprasaath Selvaraju",
        "Akhilesh Gotmare",
        "Shafiq Joty",
        "Caiming Xiong",
        "Steven Chu",
        "Hong Hoi"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multiparty dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "21",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "23",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Ga2mif: graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "26",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "Minjie Ren",
        "Xiangdong Huang",
        "Wenhui Li",
        "Dan Song",
        "Weizhi Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    }
  ]
}