{
  "paper_id": "2510.13906v1",
  "title": "Switchboard-Affect: Emotion Perception Labels From Conversational Speech",
  "published": "2025-10-14T21:23:04Z",
  "authors": [
    "Amrit Romana",
    "Jaya Narain",
    "Tien Dung Tran",
    "Andrea Davis",
    "Jason Fong",
    "Ramya Rasipuram",
    "Vikramjit Mitra"
  ],
  "keywords": [
    "affective corpus",
    "conversational speech",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding the nuances of speech emotion dataset curation and labeling is essential for assessing speech emotion recognition (SER) model potential in real-world applications. Most training and evaluation datasets contain acted or pseudo-acted speech (e.g., podcast speech) in which emotion expressions may be exaggerated or otherwise intentionally modified. Furthermore, datasets labeled based on crowd perception often lack transparency regarding the guidelines given to annotators. These factors make it difficult to understand model performance and pinpoint necessary areas for improvement. To address this gap, we identified the Switchboard corpus as a promising source of naturalistic conversational speech, and we trained a crowd to label the dataset for categorical emotions (anger, contempt, disgust, fear, sadness, surprise, happiness, tenderness, calmness, and neutral) and dimensional attributes (activation, valence, and dominance). We refer to this label set as Switchboard-Affect (SWB-Affect). In this work, we present our approach in detail, including the definitions provided to annotators and an analysis of the lexical and paralinguistic cues that may have played a role in their perception. In addition, we evaluate state-of-the-art SER models, and we find variable performance across the emotion categories with especially poor generalization for anger. These findings underscore the importance of evaluation with datasets that capture natural affective variations in speech. We release the labels for SWB-Affect to enable further analysis in this domain.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) has the potential to enhance human-computer interaction, improve our ability to monitor mental health and well-being  [1] ,  [2] , and better understand customer service, entertainment, and education experiences  [3] ,  [4] . However, our understanding of SER ©2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. DOI: 979-8-3315-8015-5 models is limited by training and evaluation data, which often does not include spontaneous speech. In this work, we release emotion perception labels for the Switchboard corpus, a widely analyzed dataset of conversations  [5] .\n\nA barrier in SER is the lack of labeled naturalistic conversational speech datasets. The majority of datasets contain recordings of actors portraying emotions. IEMOCAP includes actors performing emotional scripts and improvising scenarios  [6] . CREMA-D  [7] , RAVDESS  [8] , and MEAD  [9]  capture actors performing lexically matched sentences with a variety of target emotions. While these datasets allow for understanding of emotion expression under controlled conditions, models trained on this data do not generalize to emotion expressions in-the-wild. This is in part because there are differences between spontaneous and acted speech, including in the lexical content  [10]  and paralinguistics  [11] , but also because emotion expression in spontaneous speech tends to be more subtle  [12] .\n\nLotfian et. al introduced MSP-Podcast  [13]  to address the need for non-acted speech affect datasets. They mined potentially emotional audio from existing podcast recordings, and trained a crowd to label the speech segments for categorical  [14]  and dimensional  [15]  attributes. Similarly, Zadeh et al. introduced CMU-MOSEI  [16]  in which they mined segments from YouTube and crowd-sourced emotion perception labels. While podcast and YouTube speech contains characteristics closer to spontaneous speech, content creators are often intending to convey a pre-determined story, message, or point of view, and in doing so, they may alter their speaking style and expressions to appear more engaging  [17] . Due to these intentional expression modifications, we can consider podcast and YouTube speech to be pseudo-spontaneous  [18]  and evaluation with only those domains may not provide a true representation of SER model performance in-the-wild.\n\nA small body of work has started to explore emotion expression and perception in more naturalistic conversational settings. Namely, Mooriyad et al. explored emotion in Fisher English Training, an LDC conversational dataset  [19] . They crowd-sourced categorical and dimensional annotations from multiple annotators per segment. They found over half of the random sentences selected contained emotional traits but the labeled data were not released for further analysis. Later, Lu et al. investigated emotion in Switchboard, another LDC conversational dataset  [20] . They released Switchboard-Sentiment with positive, neutral, and negative labels, but these categories do not capture the full range of emotion labels relevant to broader SER applications. Furthermore, the annotation protocol lacked transparency, including whether multiple annotators were involved and how they were instructed to label the data. Kossaifi et al. released SEWA DB, in which participants watched adverts selected to elicit emotions and discussed the advert with their conversation partner  [21] . While these segments were annotated with multiple graders and arousal and valence attributes, they were not annotated for categorical emotions which have been increasingly of interest to the community  [22] . In this work, we address the need for emotion perception labels, both categorical and dimensional, pertaining to naturalistic conversational speech.\n\nWe introduce affect annotations for Switchboard. Our contributions include:\n\n• The release of affect annotations, including categorical labels (anger, contempt, disgust, fear, sadness, surprise, happiness, tenderness, calmness, and neutral) and dimensional labels (activation, valence, and dominance) 1  • An overview of the training process used to prepare graders and an analysis of data trends and distributions • An investigation into the lexical and paralinguistic cues corresponding to emotion perception in the dataset • An analysis of state-of-the-art SER model performance on the newly annotated data In addition to the existing audio files, Switchboard contains highly accurate transcripts and numerous metadata files corresponding to each call and speaker, including speaker sex, age, and dialect. We hope the release of the these labels will encourage others to explore the relationship between speech and emotion in spontaneous speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Swb-Affect Annotation",
      "text": "We selected 10,000 segments from the Switchboard corpus, amounting to roughly 25 hours of speech, and we trained a crowd to annotate the data for categorical and dimensional emotion traits. This section provides background on the Switchboard corpus as well as details of our segment selection, grader training and certification process, and quality analysis.\n\nSwitchboard corpus. Switchboard is a widely analyzed LDC dataset of telephone conversations. We worked with Switchboard-1 Release 2 which contains 260 hours of speech from 543 speakers  [5] . Switchboard was collected using a computer-driven robot operator system which paired participants on the phone and prompted them to discuss a specific topic (e.g., care of the elderly, public education, taxes). The data were segmented  [23] , transcribed, and labeled for echo, static, and background noise. The data were also collected with speaker information, including speaker sex, age, and dialect. The naturalistic conversations in Switchboard have allowed for a range of analyses, including topic classification  [24] , dialogue act prediction  [25] , and disfluency detection  [26] ,  [27] . The prompt topics and flow of conversation also have the potential to evoke emotions, which we aimed to explore with the introduction of annotations in Switchboard-Affect.\n\nSegment selection. We selected segments for annotation (10,000 segments) and a pilot study (100 segments) by first filtering out segments that were more likely to have low quality audio (echo, static, or background noise ratings of 4), not enough content for emotion perception (less than 5 seconds or 5 words), or shifting emotions (greater than 15 seconds). Then, following guidance from the labeling of MSP-Podcast  [13] , we reduced the proportion of neutral samples by mining the candidate samples for emotional content. Specifically, we ran an in-house SER model trained for valence, activation, and dominance prediction. We binned the predictions for each dimension into three levels (e.g., low valence, medium valence, high valence) and evenly sampled segments with each of the 27 valence-activation-dominance level combinations.\n\nAnnotation questions and options. The annotation tool presented graders with one random audio segment at a time and a series of five questions, similar to the tool used in  [13] .\n\nThe first two questions pertained to labeling categorical emotions, and asked the grader to select all emotions they perceived followed by the primary emotion they perceived. The options were presented in a list: anger, contempt, disgust, sadness, fear, surprise, happiness, tenderness, calmness, neutral, or other. Note that the first seven emotions in this list come from Ekman's universal set  [14]  and are commonly used in speech affect labeling  [13] . We added tenderness and calmness as options to increase granularity in the positivevalence space. Tenderness promotes prosocial behavior  [28] , calmness is linked to self-regulation  [29] ,  [30] , and both have significance in well-being research.\n\nThe next three questions asked the graders to rate the speech for valence, activation, and dominance. For these questions we presented the graders with a 5-point Likert scale. In the valence scale, we indicated that 1=negative, 3=neutral, and 5=positive. In the activation scale, we indicated that 1=drained, 3=neutral, and 5=energized. In the dominance scale, we indicated that 1=weak, 3=neutral, and 5=strong.\n\nAnnotation guidelines. We leveraged a wide body of literature on emotion and its expression in speech to develop a set of training material for this task  [14] ,  [15] ,  [31] -  [34] . We presented the material in a self-paced slide deck that included an overview of the task, guidelines for annotating categorical emotions, guidelines for annotating dimensional emotions, and several graded examples with reasoning.\n\nThe overview introduced the task and tool, and it explained a key concept: emotion can be conveyed in both what a speaker says (lexical content) and in how they say it (paralinguistics). These cues may align and suggest the same emotion, or they may conflict, for example if the speaker is being sarcastic or recalling a story. We emphasized that when they conflict, the TABLE I: Summary of guidelines provided to graders. These guidelines helped to anchor how graders perceived emotional speech relative to neutral speech, but we emphasized that the graders should rate the speech based on their overall perception.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "(A) Guidelines For Categorical Grading",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Description Associated Feelings Voice Descriptors",
      "text": "Anger is the feeling of being blocked in our progress or when a personal value or boundary is violated. It also arises in response to perceived provocations or threats.\n\nRage, Bitterness, Frustration, Annoyance Harsh, Sharp, Terse\n\nContempt is the feeling of deep dislike or disrespect toward someone or something you consider unworthy or inferior.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Scorn, Disdain Smug, Disapproving",
      "text": "Disgust is the feeling of distaste or revulsion toward something that is considered offensive, unpleasant, or gross.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Revulsion, Aversion, Dislike",
      "text": "Negative, Nasal\n\nFear is a response to a perceived threat or danger, causing feelings of worry or the instinct to protect oneself.\n\nPanic, Anxiety, Nervousness, Overwhelm",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Strained, Erratic",
      "text": "Sadness is the feeling of sorrow or unhappiness, often in response to loss, disappointment, or a difficult situation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Grief, Disappointment Dull, Flat, Somber",
      "text": "Surprise is triggered by sudden and unexpected information or stimuli from the internal or external environment -like hearing surprising news or having a sudden stomach pain.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Amazement, Distraction Sudden Increase In Pitch",
      "text": "Happiness is the feeling of pleasure and is often experienced when things are going well or when a person feels fulfilled.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Excitement, Passion, Joy Positive, Bright",
      "text": "Tenderness is triggered by the feeling of affection, care, and gentle kindness towards oneself, someone or something. This often involves a sense of closeness and compassion. High dominance (confident, strong) may be expressed with direct or concise speech, and perhaps a steady speaking rate or a higher volume.\n\nLow dominance (uncertain, weak) may be expressed with wavering or hesitant speech, and perhaps a choppy speaking rate or lower volume.\n\nValence relates to how positive versus negative a speaker sounds.\n\nHigh valence (positive) may be expressed with a pleasant or upbeat tone, and perhaps a higher pitch or speaking rate.\n\nLow valence (negative) may by expressed with a gloomy or harsh tone, and perhaps a lower speaking rate or pitch.\n\nActivation relates to how energized versus drained a speaker sounds.\n\nHigh activation (energy) may be expressed with a fast (or variable) speaking rate, higher (or variable) volume, or intentional emphasis in speech.\n\nLow activation (drained) may be expressed with a monotone voice, a slow speech rate, or low volume.\n\ngraders should make their selections based on how the speaker currently sounds in the segment.\n\nThe guidelines for annotating categorical emotion included emotion definitions, associated feelings, and voice descriptors. In addition, we manually selected audio examples to serve as examples, and in doing so we ensured examples for each emotion included male and female speakers, as well as low and high intensities of expression. The guidelines for annotating dimensional emotion included definitions with vocal cues and audio examples for the low-, mid-, and high-points of the scale. Table  I  summarizes the guidelines. These guidelines helped promote uniform understanding of emotion expression among graders. Specifically, it helped to define and differentiate between related emotions, such as anger, contempt, and disgust. It also clarified that they were expected to label a wide range of intensities, e.g., annoyance and rage are both types of anger, and disappointment and grief are both types of sadness. Ultimately, we found that providing these guidelines helped the graders align their perception of emotion to accepted definitions. At the same time, we emphasized that the guidelines were not all-encompassing and in the end the annotators should make their selections based on their overall perception of the speech.\n\nGold set creation. We used a small set of 100 segments to run pilot and internal analyses. The pilot analysis included approximately 60 crowd-sourced graders and the internal analysis included 3 graders from the author list, all of whom independently graded the 100 segments. These analyses allowed our team to iterate on the guidelines and create a gold set. The gold set consisted of 20 segments representing all emotion categories with both male and female speakers. These segments were specifically selected because all internal graders agreed on the primary emotion, suggesting that the speaker was clearly expressing one, valid emotion. We assigned valid secondary emotions to each of these segments if any of the graders in the pilot analysis selected that emotion. We assigned valid dimensional emotion attributes based on the majority response during the internal analysis.\n\nGrader certification. After reviewing the training material, graders from a selected crowd attempted the gold set. In order to pass, graders had to 1.) select valid secondary emotions for > 70% of segments, 2.) select valid primary emotions for > 70% of segments, and 3.) select valid dimensional attributes for > 70% segments. The loose thresholds allowed for variation due to subjective, individual perception, while still maintaining some alignment with the definitions and examples in the guidelines. Ultimately, 29 out of 35 graders passed certification, and these graders worked through the annotation set over the course of four weeks. Graders took on average 30 seconds to label one audio segment, and six graders independently labeled each audio segment.\n\nAffect label Quality Analysis (QA) Our QA process first focused on grader agreement. For primary emotion and dimensional selections, we calculated Krippendorff's alpha coefficient. For secondary emotion selections, we calculated agreement as the average jaccard similarity of selections between pairs of annotators. We found agreement scores of 0.25 and 0.62 for primary and secondary emotion selection, respectively. We found agreement scores of 0.40, 0.53, and 0.48 for valence, activation, and dominance ratings, respectively. We note emotion grading is notoriously challenging given the complexity of emotional expression in spontaneous speech, and the selection of a primary emotion is especially subjective. Figure  2  shows the pairwise primary emotion cooccurrence across annotators, illustrating that while agreement for this question was low, annotators were most likely to conflate emotional speech with neutral speech or with closely related emotions (for example contempt and anger).\n\nIn addition, our QA team regularly spot checked annotations and identified several consistent patterns in the labels. First, the emotions skewed toward lower intensity, subtly expressed emotions. This is an expected artifact of the one-on-one conversational data. Secondly, high inter-annotator agreement on a segment tended to indicate more clearly expressed emotions, whereas low agreement corresponded with more ambiguous expressions. Thirdly, for the most part graders appropriately weighted paralinguistic cues, but in some cases graders tagged a segment as emotional when a speaker was recalling an emotional incident instead of actively expressing the emotion. These are ongoing challenges in perceptual grading.\n\nWe release the data in multiple formats, consistent with  [13] . We provide consensus labels, where we find consensus primary emotions for 67% of segments. In addition, we provide detailed annotator-level labels. While the bulk of analysis in this paper focuses on the consensus labels, the detailed labels provide a promising avenue for future work as they may more accurately reflect the nuances in emotion expression and perception, leading to a more precise assessment of SER model performance  [35] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Swb-Affect Analysis",
      "text": "A. Data distributions.\n\nFigure  1  illustrates the distribution of labels in SWB-Affect. We conduct chi-square tests of independence to understand the impact of speaker demographics, namely sex and age, in the emotion labels. We find statistically significant (p value < 0.05) associations between sex and emotion labels, with males having more samples labeled as calmness or neutral and females having more samples labeled as happiness or tenderness. In addition, we find statistically significant (p value < 0.05) associations between age and emotion labels, with younger individuals (20-30 year olds) having more samples labeled as calmness and older individuals (40+ year olds) having more samples labeled as happiness. These relationships may result from the underlying data collection protocol, or they may reflect biases in the data mining procedure, training material, or graders themselves. Ultimately, these findings underscore the importance of analyzing SER model performance by emotion type and demographic group.\n\nWe also conduct an analysis of the relationship between dimensional and categorical annotations, illustrated in Figure  3 . We find many expected relationships, for example, happiness is associated with high valence, activation, and dominance, whereas calmness is linked to slightly lower valence, noticeably lower activation, but similar dominance. Sadness and fear are both associated with low valence and low dominance, and fear is related to higher activation. Anger, contempt, and disgust cluster nearby but with key differences. Anger and contempt are associated with higher dominance compared to disgust, and anger is associated with higher activation compared to both contempt and disgust. Although we did not explicitly provide graders with information connecting the categorical and dimensional labeling schemes, the findings are closely aligned with emotion definitions in the literature which highlights the graders strong understanding of the task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Investigating Factors In Speech Emotion Perception",
      "text": "Understanding the cues that graders rely on when labeling is important to ensure that SER models evaluated with the data are being assessed based on their ability to learn a holistic and accurate representation of emotion perception. In this section, we present an approach to quantify the influence of lexical and paralinguistic factors in the labeling of SWB-Affect.\n\nLexical factors. We use the publicly available OpenAI GPT-4o model to understand lexical importance in emotion perception. Previously, Niu et al. have shown that GPT-4 excels at emotion recognition from text, even outperforming human graders at this task  [36] ,  [37] . In this work, we use text-based GPT-4o emotion recognition probabilities to quantify the extent to which human graders may have selected emotion tags based on the lexical content. We modify the prompt from  [36]  to have GPT-4o label emotional transcripts in SWB-Affect; our version of the prompt is in Table  III . Note that the latest API does not support extracting specific probabilities. As a workaround, we prompt the model ten times with temperature=1 to encourage varying responses. We take the mean probability associated with the human-labeled consensus emotion. If, within those multiple requests, GPT does not respond with the human-labeled consensus emotion, we assume the probability is 0.\n\nTable  IV  lists the text-based emotion detection probabilities from GPT-4o. We find unweighted and weighted average probabilities of 0.276 and 0.193, respectively. The weighted average suffers due to lower average probability of detecting Fig.  1 : Distribution of affect labels Fig.  2 : Co-occurence of affect labels Fig.  3 : Relationship between dimensional and categorical labels calmness, tenderness, and happiness from text alone (averages of 0.048, 0.082, and 0.174, respectively). This suggests paralinguistic cues may have played a larger role when annotators were distinguishing these emotions from neutral speech. Most other emotions are detected from text with an average probability range of 0.3-0.4. However, fear samples are detected with a relatively high probability (average=0.586) suggesting that the lexical content of the segments may have been more relevant to annotators' perception of fear. Overall the low probabilities of emotion detection from text alone, even with a state-of-the-art GPT-4o model, underscore the importance of acoustic modeling in SER.\n\nParalinguistic factors. Previous work has found numerous prosodic and spectral features relevant for distinguishing between neutral and emotional speech, as well as between emotion types  [34] ,  [38] ,  [39] . In this work, we extract and compare acoustic features from speaker-matched emotional and neutral speech segments. Specifically we select a small number of commonly explored, interpretable features:\n\n• Pitch: log F0, mean and standard deviation • Loudness: log energy, mean and standard deviation • Rhythm: number of pauses and words per minute • Spectral centroid: mean and standard deviation We extract the features using the corpus audio, transcripts, and Librosa  [40] . To reduce the influence of specific matches, we select up to 10 neutral speaker-matched samples for comparison to each emotional sample. We conduct the Wilcoxon test with two alternatives to evaluate which, if any, of these features can be used to distinguish between emotional and neutral speech in the dataset. We use the Benjamini-Hochberg correction to adjust p values. We also repeat the analysis with MSP-Podcast Test1 and Test2 sets to validate our findings, although MSP-Podcast does not include pause annotations, tenderness labels, or calmness labels.\n\nTable  II  illustrates our findings on the relevant paralinguistic cues. We find several consistent trends across all three datasets, particularly with respect to pitch and loudness. While we find some disagreements, the within-corpus differences between MSP-Podcast test sets suggest that some variation is expected due to variability in emotion expression, emotion perception, and the broad nature of these emotion categories.\n\nOverall, we find acoustic features in emotionally-tagged segments in SWB-Affect align with previous work in this space. For example, samples tagged as happy or surprise have TABLE II: Affect labels in relation to acoustic features. Note: ↑ and ↓ indicate the feature from emotionally-tagged samples is significantly higher or lower, respectively, than the feature from neutral speaker-matched samples in MSP-Podcast Test1, MSP-Podcast Test2, and SWB-Affect. The Wilcoxon test was used to compare the paired data, and significance was assessed with a p value < 0.1 after Benjamini-Hochberg correction. MSP-Podcast does not include pause annotations, tenderness labels, or calmness labels. Otherwise, blank cells indicate no significant differences between the emotional and neutral samples.\n\nTABLE III: GPT-4o prompt, adapted from  [36] , to estimate the probabilities of detecting each emotion from lexical content.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "System",
      "text": "You are an emotionally-intelligent and empathetic agent. You will be given a transcript from a speaker, and your task is to identify the primary emotion the speaker is expressing within the text. If there is no emotion, then the primary emotion is neutral. Classify the transcript into one of the following categories: anger, contempt, disgust, sadness, fear, surprise, happiness, neutral. Respond with only one category and keep your responses to the category name as written and nothing else.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "User",
      "text": "Transcript: <segment transcript>",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Assistant",
      "text": "The primary emotion is a higher pitch and loudness, whereas samples tagged as sad have a lower pitch and loudness  [41] . Furthermore, samples tagged as angry have a higher spectral centroid mean compared to samples tagged as sad  [42] . We also observe that the relevant cues found for anger, contempt, and disgust reveal some commonalities and differences between the often confused emotions. All three correspond to higher loudness, but only anger and contempt are associated with higher pitch and higher speaking rates compared to neutral speech. On the other hand, contempt and disgust co-occur with more pauses compared to neutral speech. Lastly, anger and contempt are linked to higher and lower spectral centroid means, respectively, compared to neutral speech. This analysis identifies key paralinguistic cues that are relevant to capture in SER modeling, particularly for naturalistic conversational speech.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Speech Emotion Recognition Benchmarking",
      "text": "We evaluate existing categorical and dimensional SER models on SWB-Affect to understand their generalizability to spontaneous, conversational speech. We evaluate the categorical SER models on all segments that have a consensus label of neutral or one of Ekman's seven universal emotions (anger, contempt, disgust, sadness, fear, surprise, happiness) as these are the most common output categories for existing models. We evaluate the dimensional SER models on all segments. Before running the evaluations, we upsample the audio from 8kHz to 16kHz.\n\nSER models. We compare performance from four previously published SER models, and one zero-shot approach:\n\n• Emotion2Vec  [43] . A universal speech representation model pre-trained with unlabeled emotional data and finetuned with IEMOCAP categorical labels. • Audeering W2V2  [44] . A model based on wav2vec 2.0 (W2V2,  [45] ), which was pruned to 12 transformer layers and fine-tuned with MSP-Podcast dimensional labels.\n\n• Odyssey  [46] . A model based on WavLM  [47] , which has been fine-tuned with MSP-Podcast consensus categorical and dimensional labels. • Whisper-GRU  [48] . A model that uses frozen Whisper embeddings  [49]  as input to a GRU network trained with MSP-Podcast categorical soft labels (distributions of perception, not consensus labels) and dimensional consensus labels. • GPT-4o (audio preview)  [50] . A GPT-based model that accepts audio inputs and prompts but has not been trained with speech emotion targets. When prompting GPT-4o to select an emotion given an audio sample, we use a prompt similar to that in Table  III .\n\nNote that Emotion2Vec only provides categorical predictions (and does not include contempt), Audeering only pro-  Metrics. We present results separately for categorical and dimensional SER models. For the former, we report F1-score and recall for primary emotion prediction at the class-level as well as unweighted averages. For dimensional SER performance, we report Lin's Concordance correlation coefficient (CCC) between predicted and ground truth labels.\n\nResults. Table V lists the categorical emotion prediction results by model. We find GPT-4o performs best in terms of unweighted average F1-score (0.391 vs 0.300-0.326) and significantly outperforms other models for most classes (e.g., anger, sadness, fear) while underperforming for happiness. Odyssey performs best in terms of unweighted average recall (0.365 vs 0.319-0.358), and the recall of most classes. However, its recall of neutral, the majority class, is significantly lower than with other models (0.291 vs 0.776-0.951).\n\nFor most models, fear is one of the hardest emotions to detect while happiness is the easiest, which mirrors the class frequencies, but this performance trend is the opposite of our findings with lexical-based emotion predictions. This suggests the current models would benefit by increasing fear representation during training, or alternatively, by considering lexical content more explicitly when classifying fear. We also find lower probabilities for surprise, which we find is often misclassified as happiness, potentially due to the paralinguistic similarities between the two emotions. Similarly, we find confusion between anger, contempt, and disgust, but future work may benefit from training or prompting models with distinguishing paralinguistic cues in mind. For example, Perez et al. have previously demonstrated that pause information can improve lexical-based SER  [51] , and our analysis suggests the potential of additional prosody or spectral features in this type of multimodal framework.\n\nTable VI lists the dimensional emotion prediction results by model and attribute. We find Odyssey performs best for valence prediction (CCC=0.689) but other models perform similarly (0.648-0.674). Audeering performs best for activation and dominance prediction (CCC=0.455 and 0.424, respectively) and other models vary considerably in their performance (0.210-0.400).\n\nWhen comparing these results to results on MSP-Podcast Test1 and Test2, we find a few consistent trends. First, all models detect sadness and happiness with higher accuracy in SWB-Affect compared to MSP-Podcast, but at the same time, all models detect anger with lower accuracy in SWB-Affect compared to MSP-Podcast. We suspect this is related to how speakers express each emotion in different contexts. For example, speakers may be more overtly expressive of sadness or happiness in one-on-one conversations with strangers, but more overtly expressive of anger in podcasts. Second, we find that all models predict valence comparably across SWB-Affect and MSP-Podcast, but that they predict activation and dominance with lower accuracy in SWB-Affect compared to MSP-Podcast. We attribute this again to the change in domain where podcast speech likely has different activation and dominance qualities given that speakers are presenting to an audience. As a result, MSP-Podcast may have been labeled with different representations of these dimensions in mind. These findings underscore the need to evaluate SER models with speech from a variety of contexts.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "This work introduces a new set of emotion labels for the existing Switchboard corpus. The Switchboard corpus includes speech segments from naturalistic conversations and it has been widely transcribed and annotated. In this work, we describe our method for training a crowd to label emotion perception in Switchboard, and we explore trends in the annotated data. We find low agreement for primary emotion selection, but that the disagreements may meaningfully point to mixed or ambiguous emotions. Future work may benefit from evaluation with individual annotator labels or distributions. In addition, we investigate the lexical and paralinguistic cues that graders may have perceived when labeling the data. We find the labels for fear are linked to lexical content, whereas the labels for happiness correspond more to paralinguistic features. Future work may benefit from considering both text and audio-based inputs to capture the complexity of emotion expression. Lastly, we explore the performance of state-of-the-art SER models on the newly labeled data. While SER performance drops for anger, it increases for happiness and sadness in SWB-Affect. This likely points to the variability of emotion expression in different domains and underscores the need for diverse evaluation datasets. Ultimately, the affective labeling of Switchboard opens many future analysis directions, including improving SER for low-intensity, conversational emotions, as well as analyses on the relationships between emotion perception and turn-taking, dialogue acts, or speech disfluencies.\n\nV. ETHICAL IMPACT STATEMENT SWB-Affect contributes to the evaluation of SER models, which may be used in a range of applications including healthcare, customer satisfaction, entertainment, or education. Any assessments of SER model performance on SWB-Affect may be influenced by underlying biases in the data. Perceptual emotion annotation is a difficult, subjective task. In addition, emotion expression varies from person to person and can be especially subtle in conversational speech. The speakers in this task are US-based English speakers, and the findings may not generalize to other languages or accents. In addition, the graders employed in this task are US-based English speakers, and the labels may not generalize to perceptions of emotion that are common in other languages or cultures. Lastly, the labels may be influenced by insufficient context, individual annotator biases, or biases that shaped the overall annotation process. Specifically, biases may have played a role in segment selection, in the simplified emotion definitions, in the voice descriptors and audio examples, or in the gold set we used to certify annotators. For transparency, we provide detailed summaries of our training guidelines and processes. Future annotation work should include a larger and more diverse group of annotators to minimize risk of individual biases. In addition, future annotation work should explore continuous grading with context as has been done recently in other datasets  [21] ,  [52] ,  [53] .\n\nWe also acknowledge the limitations of the analysis section. The investigation on lexical and paralinguistic cues provides a post-hoc analysis of the different factors that may have played a role in annotators' perception of emotion. The list of cues analyzed is not exhaustive, and future work may improve on this process by collecting annotator reasoning during the labeling. In addition, our evaluation of state-of-the-art SER models may by limited by audio quality or label reliability, as mentioned above. However, the overall trends we see across all models highlight the importance of evaluation with datasets that capture natural affective variations in speech.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: shows the pairwise primary emotion co-",
      "page": 4
    },
    {
      "caption": "Figure 1: illustrates the distribution of labels in SWB-Affect.",
      "page": 4
    },
    {
      "caption": "Figure 3: We find many expected relationships, for example, happiness",
      "page": 4
    },
    {
      "caption": "Figure 1: Distribution of affect labels",
      "page": 5
    },
    {
      "caption": "Figure 2: Co-occurence of affect labels",
      "page": 5
    },
    {
      "caption": "Figure 3: Relationship between dimensional and categorical labels",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "N Cummins",
        "S Scherer",
        "J Krajewski",
        "S Schnieder",
        "J Epps",
        "T Quatieri"
      ],
      "year": "2015",
      "venue": "Speech communication"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in the real-world: passively collecting and estimating emotions from natural speech data of individuals with bipolar disorder",
      "authors": [
        "E Provost",
        "S Sperry",
        "J Tavernor",
        "S Anderau",
        "A Yocum",
        "M Mcinnis"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Ordinal learning for emotion recognition in customer service calls",
      "authors": [
        "W Han",
        "T Jiang",
        "Y Li",
        "B Schuller",
        "H Ruan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Predicting student emotions in computer-human tutoring dialogues",
      "authors": [
        "D Litman",
        "K Forbes-Riley"
      ],
      "year": "2004",
      "venue": "Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04"
    },
    {
      "citation_id": "5",
      "title": "Switchboard: Telephone speech corpus for research and development",
      "authors": [
        "J Godfrey",
        "E Holliman",
        "J Mcdaniel"
      ],
      "year": "1992",
      "venue": "ieee international conference on"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "9",
      "title": "Mead: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "Mead: A large-scale audio-visual dataset for emotional talking-face generation"
    },
    {
      "citation_id": "10",
      "title": "Scripted dialogs versus improvisation: lessons learned about emotional elicitation techniques from the iemocap database",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Real vs. acted emotional speech",
      "authors": [
        "J Wilting",
        "E Krahmer",
        "M Swerts"
      ],
      "year": "2006",
      "venue": "Proceedings of the international conference on spoken language processing"
    },
    {
      "citation_id": "12",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "13",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "15",
      "title": "Segregated neural representation of distinct emotion dimensions in the prefrontal cortex-an fmri study",
      "authors": [
        "S Grimm",
        "C Schmidt",
        "F Bermpohl",
        "A Heinzel",
        "Y Dahlem",
        "M Wyss",
        "D Hell",
        "P Boesiger",
        "H Boeker",
        "G Northoff"
      ],
      "year": "2006",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "16",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Exploring audio-based stylistic variation in podcasts",
      "authors": [
        "K Martikainen",
        "J Karlgren",
        "K Truong"
      ],
      "year": "2022",
      "venue": "Exploring audio-based stylistic variation in podcasts"
    },
    {
      "citation_id": "18",
      "title": "Spontaneous speech",
      "authors": [
        "B Tucker",
        "Y Mukai"
      ],
      "year": "2023",
      "venue": "Spontaneous speech"
    },
    {
      "citation_id": "19",
      "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora",
      "authors": [
        "S Mariooryad",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2014",
      "venue": "Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C.-C Chiu",
        "J Fan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "22",
      "title": "Speechverse: A large-scale generalizable audio language model",
      "authors": [
        "N Das",
        "S Dingliwal",
        "S Ronanki",
        "R Paturi",
        "Z Huang",
        "P Mathur",
        "J Yuan",
        "D Bekal",
        "X Niu",
        "S Jayanthi"
      ],
      "year": "2024",
      "venue": "Speechverse: A large-scale generalizable audio language model",
      "arxiv": "arXiv:2405.08295"
    },
    {
      "citation_id": "23",
      "title": "Resegmentation of switchboard",
      "authors": [
        "N Deshmukh",
        "A Ganapathiraju",
        "A Gleeson",
        "J Hamaker",
        "J Picone"
      ],
      "year": "1998",
      "venue": "ICSLP"
    },
    {
      "citation_id": "24",
      "title": "Approaches to topic identification on the switchboard corpus",
      "authors": [
        "J Mcdonough",
        "K Ng",
        "P Jeanrenaud",
        "H Gish",
        "J Rohlicek"
      ],
      "year": "1994",
      "venue": "Proceedings of ICASSP'94. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Guiding attention in sequence-to-sequence models for dialogue act prediction",
      "authors": [
        "P Colombo",
        "E Chapuis",
        "M Manica",
        "E Vignon",
        "G Varni",
        "C Clavel"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "26",
      "title": "Toward a multimodal approach for disfluency detection and categorization",
      "authors": [
        "A Romana",
        "K Koishida"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Automatic disfluency detection from untranscribed speech",
      "authors": [
        "A Romana",
        "K Koishida",
        "E Provost"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Is tenderness a basic emotion?",
      "authors": [
        "J Kalawski"
      ],
      "year": "2010",
      "venue": "Motivation and emotion"
    },
    {
      "citation_id": "29",
      "title": "A tale of two emotions: The diverging salience and health consequences of calmness and excitement in old age",
      "authors": [
        "J Hamm",
        "C Wrosch",
        "M Barlow",
        "U Kunzmann"
      ],
      "year": "2021",
      "venue": "Psychology and Aging"
    },
    {
      "citation_id": "30",
      "title": "Contentment and self-acceptance: wellbeing beyond happiness",
      "authors": [
        "D Cordaro",
        "Y Bai",
        "C Bradley",
        "F Zhu",
        "R Han",
        "D Keltner",
        "A Gatchpazian",
        "Y Zhao"
      ],
      "year": "2024",
      "venue": "Journal of Happiness Studies"
    },
    {
      "citation_id": "31",
      "title": "Atlas of emotions",
      "venue": "Atlas of emotions"
    },
    {
      "citation_id": "32",
      "title": "The bright side of relational communication: Interpersonal warmth as a social emotion",
      "authors": [
        "P Andersen",
        "L Guerrero"
      ],
      "year": "1996",
      "venue": "Handbook of communication and emotion"
    },
    {
      "citation_id": "33",
      "title": "Emotion in speech: The acoustic attributes of fear, anger, sadness, and joy",
      "authors": [
        "C Sobin",
        "M Alpert"
      ],
      "year": "1999",
      "venue": "Journal of psycholinguistic research"
    },
    {
      "citation_id": "34",
      "title": "Universal vocal signals of emotion",
      "authors": [
        "D Sauter",
        "F Eisner",
        "P Ekman",
        "S Scott"
      ],
      "year": "2009",
      "venue": "31st Annual Meeting of the Cognitive Science Society"
    },
    {
      "citation_id": "35",
      "title": "The whole is bigger than the sum of its parts: Modeling individual annotators to capture emotional variability",
      "authors": [
        "J Tavernor",
        "Y El-Tawil",
        "E Provost"
      ],
      "year": "2024",
      "venue": "The whole is bigger than the sum of its parts: Modeling individual annotators to capture emotional variability",
      "arxiv": "arXiv:2408.11956"
    },
    {
      "citation_id": "36",
      "title": "Rethinking emotion annotations in the era of large language models",
      "authors": [
        "M Niu",
        "Y El-Tawil",
        "A Romana",
        "E Provost"
      ],
      "year": "2024",
      "venue": "Rethinking emotion annotations in the era of large language models",
      "arxiv": "arXiv:2412.07906"
    },
    {
      "citation_id": "37",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "M Niu",
        "M Jaiswal",
        "E Provost"
      ],
      "year": "2024",
      "venue": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "arxiv": "arXiv:2408.17026"
    },
    {
      "citation_id": "38",
      "title": "Feature type relevance for the discrimination of emotion pairs. The role of prosody in affective speech",
      "authors": [
        "B Schuller",
        "M Wöllmer",
        "F Eyben",
        "G Rigoll"
      ],
      "year": "2009",
      "venue": "Feature type relevance for the discrimination of emotion pairs. The role of prosody in affective speech"
    },
    {
      "citation_id": "39",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "40",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "SciPy"
    },
    {
      "citation_id": "41",
      "title": "Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion",
      "authors": [
        "I Murray",
        "J Arnott"
      ],
      "year": "1993",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "42",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "43",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Proc. ACL 2024 Findings"
    },
    {
      "citation_id": "44",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "45",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "46",
      "title": "Odyssey2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop)"
    },
    {
      "citation_id": "47",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Modeling speech emotion with label variance and analyzing performance across speakers and unseen acoustic conditions",
      "authors": [
        "V Mitra",
        "A Romana",
        "D Tran",
        "E Azemi"
      ],
      "year": "2025",
      "venue": "ICLR Workshop. I Can't Believe It's Not Better: Challenges in Applied Deep Learning"
    },
    {
      "citation_id": "49",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "50",
      "title": "Openai api documentation",
      "venue": "Openai api documentation"
    },
    {
      "citation_id": "51",
      "title": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition",
      "authors": [
        "M Perez",
        "M Jaiswal",
        "M Niu",
        "C Gorrostieta",
        "M Roddy",
        "K Taylor",
        "R Lotfian",
        "J Kane",
        "E Provost"
      ],
      "year": "2022",
      "venue": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition"
    },
    {
      "citation_id": "52",
      "title": "The mspconversation corpus",
      "authors": [
        "L Martinez-Lucas",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2020",
      "venue": "The mspconversation corpus"
    },
    {
      "citation_id": "53",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    }
  ]
}