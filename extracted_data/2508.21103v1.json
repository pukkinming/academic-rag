{
  "paper_id": "2508.21103v1",
  "title": "Spatiotemporal Eeg-Based Emotion Recognition Using Sam Ratings From Serious Games With Hybrid Deep Learning",
  "published": "2025-08-28T08:25:19Z",
  "authors": [
    "Abdul Rehman",
    "Ilona Heldal",
    "Jerry Chun-Wei Lin"
  ],
  "keywords": [
    "Electroencephalography",
    "Electrodes",
    "Transformers",
    "Feature extraction",
    "Brain modeling",
    "Emotion recognition",
    "Sensors",
    "Sliding Window",
    "Deep learning",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition üò† üòä üò¢ üòê .5Abstract-Recent advancements in EEG-based emotion recognition have shown promising outcomes using both deep learning and classical machine learning approaches; however, most existing studies focus narrowly on binary valence prediction or subjectspecific classification, which limits generalizability and deployment in real-world affective computing systems. To address this gap, this paper presents a unified, multigranularity EEG emotion classification framework built on the GAMEEMO dataset, which consists of 14-channel EEG recordings and continuous self-reported emotion ratings (boring, horrible, calm, and funny) from 28 subjects across four emotioninducing gameplay scenarios. Our pipeline employs a structured preprocessing strategy that comprises temporal window segmentation, hybrid statistical and frequency-domain feature extraction, and z-score normalization to convert raw EEG signals into robust, discriminative input vectors. Emotion labels are derived and encoded across three complementary axes: (i) binary valence classification based on the averaged polarity of positive (funny, calm) and negative (boring, horrible) emotion ratings, and (ii) Multi-class emotion classification, where the presence of the most affective state is predicted. (iii) Fine-grained multi-label representation via binning each emotion into 10 ordinal classes. We evaluate a broad spectrum of models, including Random Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM, LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently outperforms the others, achieving an F1-score of 0.932 in the binary valence task and 94.5% and 90.6% in both multi-class and Multi-Label emotion classification. In contrast to prior work, our framework delivers high-resolution affect modeling with generalized classification capacity and strong subject-independent reproducibility, offering a scalable solution for future real-time EEG-based emotion recognition applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition plays a critical role in affective computing that enables various intelligent systems to recognize, infer, and interpret interpersonal interactions, knowledge insight, perception, and human reactions/states to improve user experiences as well as enhance their outcomes  [1] -  [3] . Recent advances in Artificial Intelligence (AI) have significantly enhanced the ability to detect and interpret emotions, revolutionizing fields such as human-computer interaction  [4] ,  [5]  and mental health  [6] . Notably, deep learning models have enhanced the accuracy of emotion detection by effectively analyzing complex data, such as facial expressions, voice tones, and text  [7] ,  [8] . Transfer learning has further accelerated progress by enabling models trained on large datasets to be adapted to specific tasks with smaller datasets, improving generalization across different populations and contexts  [9] .\n\nEmotion recognition plays a critical role in affective com-A. Rehman, I. Heldal, and J. C.-W. Lin are with the Department of Computer Science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, 5020 Bergen, Norway (e-mail: {arj, ilona.heldal, jerry.chun-wei.lin}@hvl.no).\n\nCorresponding author: Abdul Rehman, Jerry Chun-wei Lin (e-mail: arj@hvl.no, jerry.chun-wei.lin}@hvl.no).\n\nputing, enabling the interpretation of human reactions  [10] . Emotions vary significantly across different age groups due to psychological, social, and physiological changes that occur throughout a person's life. As children learn to navigate new experiences and manage their emotions, they often exhibit rapid fluctuations between different emotional states, such as happiness, anger, or melancholy  [11] ,  [12] . Adults are better at controlling their emotions and understanding them more deeply, even though they still occasionally feel stressed or anxious about their life obligations. The Elderly often experience changes in emotional regulation and expression as they age, which can be influenced by life experiences, cognitive changes, and social circumstances  [13] . For instance, older adults may express emotions more subtly or with different facial and vocal cues compared to younger adults. The emotional priorities of older people may also change; they may exhibit less severe outward manifestations of unpleasant emotions, such as sadness or anger, instead choosing to concentrate more on preserving social ties and positive emotions.\n\nWhile there exist some studies on emotion recognition, they are often limited to only identifying and categorizing emotion  [2] ,  [3] ,  [14] . Since different emotions are difficult to generalize, they cannot work effectively on datasets that are too diverse or too large  [15] . Most existing studies have focused on multiclass classification (4 classes) only. In this paper, we build upon existing research, considering both binary classification and multi-label classification. This paper makes the following contributions:\n\n‚Ä¢ We present a unified, multigranularity EEG emotion classification framework built on the GAMEEMO dataset, which consists of 14-channel EEG recordings and continuous self-reported emotion ratings (boring, horrible, calm, and funny) from 28 subjects across four emotion-inducing gameplay scenarios. In contrast to prior work, our framework delivers highresolution affect modeling with generalized classification capacity and strong subject-independent reproducibility, offering a scalable solution for future real-time EEGbased emotion recognition applications. This paper is organized as follows: Section II provides the related work on emotion recognition using AI. Utilized ML and DL models, data preprocessing methods, dataset preliminaries, and the technical components of the proposed framework are all covered in detail in Section III. In Section IV, the experimental analysis provides a thorough examination and draws conclusions based on the proposed work. Section V then presents the discussion. Finally, Section VI concludes the paper and leads to future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section provides the related work on emotion recognition. EEG emotion detection research has been exciting for a long time, and several papers have proposed different approaches to creating systems that can identify emotions in humans.\n\nShahzad et al.  [2]  focused on using EEG data from 28 subjects, utilizing the GAMEEMO dataset, for emotion recognition in a gaming environment. They developed an EEGdriven framework for emotion recognition during gameplay, utilizing Random Forest to detect emotions (boring, calm, horror, and funny) with an accuracy of 98.21%. This framework is validated on EEG data collected during gaming, demonstrating its efficacy in recognizing emotional states with applications in adaptive gaming and affective computing. Gosala et al.  [3]  introduced a hybrid convolutional neural network (CNN) model utilizing the GAMEEMO extended dataset, which comprises EEG signals recorded during gameplay of four emotionally distinct games (boring, calm, horror, and funny). They highlight the effectiveness of hybrid CNN architectures in processing EEG data for emotion detection. Chen et al.  [10]  presented a multimodal emotion identification approach using a combination of EEG and ECG and the Dempster-Shafer evidence theory. The SVM classifier is utilised for EEG feature classification. They utilise the bi-directional long shortterm memory for ECG. Superior performance over singlemodal models is achieved by fusing the outcomes using evidence theory. By 2.64% in arousal and 2.75% in valence, the multimodal model outperforms EEG-based models; in comparison, it outperforms ECG-based models by 7.37% and 8.73%, respectively. An enhanced method for EEG-based emotion recognition on the publicly accessible VREED dataset was presented in  [16] . Two emotional states were automatically classified by five machine learning classifiers using DE characteristics. Additionally, they observed that the gamma band yielded the highest average accuracy score. Alslaity et al.  [17]  examined emotion recognition in a virtual reality setting using machine learning and explainable machine learning methods. Fang et al.  [18]  recognized emotions using EEG data by employing a multi-feature input deep forest (MF-DF) model, which analyzed the DEAP dataset's two valence and arousal labels. Ahirwal et al.  [19]  introduced a novel channel selection method for emotion categorisation using EEG signals produced by audio-visual stimulation from 40 participants. Three methods are used: artificial neural networks (ANN), naive bayes (NB), and SVM. Diah et al.  [20]  created an EEG-based emotion identification system with a feature extraction and classifier subsystem. They employ and analyze nine characteristics extracted from the EEG signal's temporal and frequency domains. To categorise the subject's emotional state, they applied Random Forest and SVM techniques, and they contrasted the outcomes with those of other machine learning techniques. The results of the experiment indicate that the Random Forest approach yields the maximum recognition accuracy, measuring 62.58%.\n\nAlgumaei et al.  [21]  introduced a model for emotion recognition using wavelet packet energy characteristics and EEG data. The emotional states were detected using four conventional classifiers: naive Bayes, k-nearest neighbor, SVM, and linear discriminant analysis. When listening to bilingual (English and Urdu) music, Zainba et al.  [22]  used EEG signals to identify four distinct emotions: joyful, sad, angry, and calm. To create the hybrid feature vector, which classifiers then use to identify emotional reactions, frequency and time-domain characteristics are merged. Happiness is the most prevalent and easily identifiable emotion, and hybrid characteristics have been found to produce superior outcomes than separate domains. Bardak et al.  [23]  aimed to improve the recognition accuracy of three and four emotions by using the Neuro-Fuzzy Inference System (ANFIS). The investigation's results demon-strate that the proposed model efficiently detects emotions and that it performs well in terms of categorization. Zhong et al.  [24]  proposed a Regularised Graph Neural Network (RGNN) using an EEG to recognize emotions. It suggests two regularizers to better handle noisy labeling and cross-subject EEG variability. Three emotion classes are used to thoroughly analyze the proposed RGNN model: neutral, negative, and positive.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Methodology",
      "text": "Figure  1  presents the overall architecture of the proposed EEG-based emotion classification framework designed for cognitive assessment during gameplay, leveraging the structured GAMEEMO dataset. This dataset comprises EEG signals from 28 participants across four gameplay sessions, with each EEG record containing 14 structured features per time point and accompanied by self-reported emotion ratings (scale 1-10) for four affective states: horrible, boring, calm, and funny. To address the non-stationary nature of EEG data, a comprehensive preprocessing pipeline is employed. First, a sliding window technique segments the continuous EEG time series into overlapping epochs, enabling both temporal feature preservation and efficient batch processing. For each window, a range of statistical descriptors, such as mean, standard deviation, and entropy, is extracted to construct robust input vectors. Fig.  2  Corresponding emotion labels are transformed into three task-specific formats: (i) binary valence classification based on the averaged polarity of positive (funny, calm) and negative (boring, horrible) emotion ratings, and (ii) Multiclass emotion classification, where the presence of the most affective state is predicted. (iii) Fine-grained multi-label representation via binning each emotion into 10 ordinal classes. These representations feed into parallel classification pipelines comprising both deep learning architectures (LSTM, LSTM-GRU, LSTM-CNN) and classical machine learning models (Logistic Regression, SVM, Random Forest, XGBoost). Each path is independently trained and evaluated using metrics such as accuracy, F1-score, and confusion matrix analysis. This modular and extensible framework supports the detailed decoding of emotions from EEG, enabling both fine-grained and generalized predictions of emotional states across diverse affective dimensions.\n\nAlgorithm 1 outlines a unified and structured five-phase framework designed for EEG-based emotion recognition using the GAMEEMO dataset, targeting both multi-label multiclass and binary valence classification objectives. The pipeline begins by segmenting raw EEG signals collected from 28 subjects across four gameplay sessions into overlapping temporal windows (500 ms, 50% overlap), thereby preserving temporal dynamics while standardizing input lengths for downstream processing. From each window, a rich set of statistical (mean, variance, entropy) and frequency-domain features (e.g., FFT, bandpower) is extracted to form discriminative representations of brain activity. These feature vectors are then normalized using z-score standardization to minimize inter-subject and inter-session variability. Emotion annotations, derived from synchronized PDF-based self-reports, are transformed into two distinct supervised targets: a multi-class format with 10-level binning for each of the four emotions (boring, horrible, calm, funny), and a binary valence label, computed by thresholding the average intensity of positive (funny, calm) versus negative (boring, horrible) affective scores. These labeled datasets feed into parallel learning pipelines consisting of both deep models (LSTM, LSTM-GRU, LSTM-CNN) and classical machine learning algorithms (Logistic Regression, SVM, Random Forest, XGBoost). Each branch is optimized using the AdamW optimizer with cosine annealing learning rate scheduling and warm-up strategy, along with regularization techniques such as dropout (0.3), weight decay (10 -4 ), and gradient clipping (norm ‚â§ 1.0). Model training spans 100 epochs with a batch size of 32, logging accuracy, precision, recall, F1-score, and confusion matrices per epoch for comprehensive evaluation. Final models are evaluated on held-out test sets, and outputs, along with visualizations, are exported via TensorBoard or saved as high-resolution plots. This modular and scalable design facilitates robust emotion decoding from EEG signals, supporting both fine-grained modeling of emotional states and high-level valence analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset Collection And Pre-Processing",
      "text": "The proposed EEG-based emotion classification framework utilizes the structured GAMEEMO dataset, which comprises EEG recordings from 28 participants participating in four emotionally evocative gaming scenarios. Each session is annotated with self-reported emotion scores for four affective states: boring, horrible, calm, and funny, rated on a continuous scale from 0 to 10. EEG data, recorded across 14 channels, undergoes a robust multi-stage pre-processing pipeline to retain temporal dynamics and enhance feature discriminability. Initially, raw signals are segmented into overlapping temporal windows (e.g., 500 ms with 50% overlap), allowing for localized temporal analysis of neural fluctuations. Within each segment, comprehensive handcrafted features are extracted per channel, including time-domain statistics (mean, standard deviation, entropy, skewness, kurtosis) and frequency-domain features (band powers across delta, theta, alpha, and beta bands), forming a high-dimensional representation of the subject's cognitive-emotional state at fine granularity. This feature extraction process follows structured principles similar to those in physics-constrained data-driven modeling, where domainspecific transformations improve representation fidelity and learning robustness  [25] . Labels are constructed using two parallel strategies: (1) in the multi-class setting, each emotion is discretized into 11 ordinal bins (0-10), resulting in a multihot 4-dimensional label vector that supports nuanced modeling of co-occurring affective states; and (2) in the binary classification setup, a valence score is computed by averaging positive (funny, calm) and negative (boring, horrible) ratings, followed by thresholding to assign a binary label reflecting emotional polarity. To ensure consistency and model convergence, zscore normalization is applied to all features, and padding is used where necessary to maintain uniform segment lengths. Additionally, the framework accommodates a Multi-Label Emotion classification mode, where all four discrete emotions",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Analysis, Results, And Discussion",
      "text": "The comparative results in Table . I highlight the superior performance of deep learning models, particularly the LSTM-GRU architecture, across all three classification paradigms: binary valence, multi-class emotion recognition, and Multi-Label Emotion classification. In the binary setup where emotional valence is computed based on the balance between positive and negative emotions, the LSTM-GRU model achieved a leading F1-score of 0.932 and an accuracy of 93.3%, notably outperforming the classical Random Forest model, which attained an accuracy of 85%. For the multi-class task, which independently predicts the four emotions \"boring,\" \"horrible,\" \"calm,\" and \"funny,\" LSTM-GRU again led with F1-scores above 0.92 for each class. At the same time, the best-performing Random Forest model yielded F1-scores between 0.738 and 0.792, suggesting challenges in learning finer affective nuances. In the more complex Multi-Label Emotion classification scenario, where the model must simultaneously determine the dominant emotional state from a set of four classes, the LSTM-GRU model achieved 90.6% accuracy and an F1-score of 0.909, substantially outperforming the Random Forest's 79%. These results collectively validate the importance of temporal sequence modeling and feature-rich preprocessing in the fine-grained decoding of affective EEG signals. The deep models consistently demonstrated stronger generalization and robustness across subject-independent evaluations, making them ideal candidates for real-time EEG-based emotion recognition systems.\n\nThis paragraph presents a comprehensive visualization of confusion matrices for the highest-performing models across all three classification tasks: binary valence detection, multiclass emotion recognition, and multi-label emotion classification. In the binary classification task, the Random Forest model (Fig.  3a ) produces balanced and moderately accurate predictions, characterized by a relatively dense diagonal and few off-diagonal misclassifications, indicating a solid separation between low and high valence states. The LSTM-GRU model (Fig.  3b ), however, further strengthens this separation by yielding a sharper diagonal and fewer false positives and negatives, highlighting its ability to capture fine-grained temporal dependencies in EEG data.\n\nFor the multi-class task, the confusion matrix of Random Forest (Fig.  4a ) shows moderate classification accuracy, particularly excelling in certain classes, such as \"funny\" and \"calm,\" but struggles with overlapping emotion scores due to the inherent ambiguity and class imbalance in higher label bins. In contrast, the LSTM-GRU model (Fig.  4b ) consistently preserves high fidelity in emotion intensity discrimination, showing dominant diagonal entries with minimal spillover, thus validating its superior generalization capacity in complex affective modeling.\n\nAdditionally, the Multi-Label Emotion classification results further illustrate the disparity between classical and deep  learning paradigms. The confusion matrix for Random Forest (Fig.  5a ) shows that while it performs reasonably across classes such as \"funny\" and \"calm,\" it exhibits increased confusion between negative affective states like \"boring\" and \"horrible,\" suggesting limited discriminability. In contrast, the LSTM-GRU model (Fig.  5b ) demonstrates a clearly defined and balanced diagonal structure with substantially fewer crossclass mispredictions, indicating a strong capacity for jointly learning all four affective labels with temporal consistency and high inter-class separability. Overall, these comparative insights reinforce the effectiveness of temporal deep learning models in EEG-based emotion recognition while acknowledging the interpretability and simplicity of ensemble-based classical approaches.\n\nThis paragraph provides the training dynamics of the LSTM-GRU model across binary, multi-class, and Multi-Label Emotion EEG-based emotion classification tasks. In Figs.  6a  and 6b , the binary valence classification exhibits rapid   In the more complex Multi-Label Emotion classification task depicted in Fig.  8a  and 8b , where a single label is assigned from four exclusive emotion classes per sample, the model continues to demonstrate robust learning behavior, maintaining accuracy beyond 90% with consistent and stable loss reduction. These results collectively affirm the model's temporal learning capability and adaptability to varying emotional modeling strategies, from coarse binary distinctions to fine-grained and joint emotion predictions. Similarly, in the multi-class task, LSTM-GRU surpassed all classical models, achieving F1-scores exceeding 0.927 across all emotion categories, and offering robust detection of cooccurring emotional states, including high-arousal responses such as \"funny.\" In the more complex Multi-Label Emotion setup, where a single dominant emotion is predicted per instance, the model continued to demonstrate high accuracy ( 90.6%) and reliable generalization, outperforming Random Forest (79%). Confusion matrix analysis revealed sharper diagonals and reduced misclassifications for LSTM-GRU in all three tasks, especially in distinguishing similar affective states. Training curves further confirmed stable optimization with smooth convergence and no overfitting across tasks. Collectively, these results underscore the effectiveness of combining temporally aware deep models with rigorous preprocessing, supporting a robust and generalizable EEG-based framework for real-time emotion recognition in interactive and neuroadaptive applications.\n\nTable II presents a comparative analysis of studies, providing an overview of how they analyze emotion recognition using EEG data and related techniques. Shahzad et al.  [2]  and Gosala et al.  [3]  proposed random forest and hybrid CNN for emotion recognition, achieving an accuracy of 98.21% and 95%, respectively. On the other hand, the proposed framework achieves 86% accuracy for binary and 99% accuracy for multiclass classification problems, while demonstrating the versatility of using transfer learning and outperforming previously conducted works.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "This work introduced a robust and extensible multi-branch EEG-based emotion recognition framework tailored to decode affective responses elicited during game-based cognitive experiences, using the GAMEEMO dataset as a foundational testbed. At its core lies a meticulously engineered preprocessing pipeline that encompasses the synchronized acquisition of 14-channel EEG signals, the extraction of self-reported emotion labels, overlapping temporal windowing, and comprehensive handcrafted feature transformation across both statistical and frequency domains. The framework's strength stems from its tri-format label encoding strategy: (i) binary valence classification based on the averaged polarity of positive (funny, calm) and negative (boring, horrible) emotion ratings, and (ii) Multi-class emotion classification, where the presence of the most affective state is predicted. (iii) Fine-grained multilabel representation via binning each emotion into 10 ordinal classes, thereby supporting comprehensive benchmarking of learning paradigms. Classical models, such as Random Forest, offered competitive performance with 85% binary accuracy and macro F1-scores of nearly 0.79 for Multi-Label Emotion classification, demonstrating interpretability and robustness despite class imbalance. However, the LSTM-GRU deep sequence model consistently outperformed all baselines, achieving a 0.932 F1-score in binary tasks and exceeding 0.92 across multi-class and Multi-Label Emotion setups, aided by its temporal modeling of EEG sequences. Confusion matrix analysis confirmed LSTM-GRU's superior discriminative power, showing dense diagonals and minimal off-class predictions in all classification schemes. Training curves further validated the smooth convergence and strong generalization achieved through effective regularization (dropout, weight decay) and scheduler tuning. Notwithstanding its success, the framework opens up promising future directions, such as integrating domain-aware spectral descriptors (e.g., entropy, wavelets), enhancing subject-independent generalization through metalearning or transfer learning, and deploying real-time emotion inference on lightweight edge or BCI platforms. These encan pave the way toward scalable, interpretable, and emotionally intelligent neuroadaptive systems in gaming and beyond.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Place Photo Here",
      "text": "Abdul Rehman is currently pursuing the Ph.D. degree at Western Norway University of Applied Sciences, Bergen, Norway. His research interests include affective computing, EEG-based emotion recognition, deep learning, and the application of serious games in education and healthcare.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Place Photo Here",
      "text": "Ilona Heldal is a professor at Western Norway University of Applied Sciences, aiming to define basic requirements and to develop a better understanding for more effective and enjoyable collaboration via new technologies, especially by utilizing visualization and telecommunication to support work.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Place Photo Here",
      "text": "Jerry Chun-Wei Lin Jerry Chun-Wei Lin (Senior Member, IEEE) received the Ph.D. degree from the Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, in 2010. He has published over 600 research articles in top-tier, refereed journals and conferences. His research interests encompass data mining, soft computing, artificial intelligence, social computing, multimedia and image processing, as well as privacypreserving and security technologies.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents the overall architecture of the proposed",
      "page": 3
    },
    {
      "caption": "Figure 2: Corresponding emotion labels are transformed into",
      "page": 3
    },
    {
      "caption": "Figure 1: Proposed framework for EEG-based emotion classification leveraging the GAMEEMO dataset.",
      "page": 4
    },
    {
      "caption": "Figure 3: a) produces balanced and moderately accurate",
      "page": 4
    },
    {
      "caption": "Figure 3: b), however, further strengthens this separation",
      "page": 4
    },
    {
      "caption": "Figure 4: a) shows moderate classification accuracy, par-",
      "page": 4
    },
    {
      "caption": "Figure 4: b) consistently",
      "page": 4
    },
    {
      "caption": "Figure 2: High Level Illustration of sliding window for Spatiotemporal EEG-based Emotion Recognition",
      "page": 5
    },
    {
      "caption": "Figure 5: a) shows that while it performs reasonably across",
      "page": 5
    },
    {
      "caption": "Figure 5: b) demonstrates a clearly defined",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion matrices for Binary Classification",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrices for Multi-class Emotion Classifi-",
      "page": 6
    },
    {
      "caption": "Figure 5: Confusion matrices for Multi-label Classification.",
      "page": 6
    },
    {
      "caption": "Figure 6: Binary Classification",
      "page": 6
    },
    {
      "caption": "Figure 7: Multi-class Classification",
      "page": 6
    },
    {
      "caption": "Figure 8: Multi-Label Emotion Classification",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "Binary (Valence)",
          "Model Type": "ML - Random Forest\nDL - LSTM-GRU",
          "Emotion Class": "Positive / Negative\nPositive / Negative",
          "Accuracy": "0.85\n0.933",
          "Precision": "0.85 (avg)\n0.933",
          "F1-Score": "0.85 (avg)\n0.932"
        },
        {
          "Task": "Multi-Class (Per Emotion)",
          "Model Type": "ML - Random Forest",
          "Emotion Class": "boring\nhorrible\ncalm\nfunny\nMacro Average",
          "Accuracy": "0.808\n0.796\n0.791\n0.790\n0.796",
          "Precision": "0.892\n0.841\n0.832\n0.808\n0.843",
          "F1-Score": "0.740\n0.738\n0.763\n0.792\n0.758"
        },
        {
          "Task": "",
          "Model Type": "DL - LSTM-GRU",
          "Emotion Class": "boring\nhorrible\ncalm\nfunny\nMacro Average",
          "Accuracy": "0.955\n0.953\n0.938\n0.934\n0.945",
          "Precision": "0.942\n0.948\n0.929\n0.934\n0.938",
          "F1-Score": "0.939\n0.943\n0.927\n0.935\n0.936"
        },
        {
          "Task": "Multi-Label\n(Joint Emotion Labels)",
          "Model Type": "ML - Random Forest\nDL - LSTM-GRU",
          "Emotion Class": "boring, horrible, calm,\nfunny (1-10)\nboring, horrible, calm,\nfunny (1-10)",
          "Accuracy": "0.79\n0.906",
          "Precision": "0.79\n0.909",
          "F1-Score": "0.79\n0.909"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 93: 3% emotionlabels,overlappingtemporalwindowing,andcompre-",
      "data": [
        {
          "Paper & Year": "Shahzad et\nal.\n(2024)\n[2]",
          "Approach": "Random\nForest\nfor\nEmotion\nRecognition",
          "Dataset": "GAMEEMO",
          "Classes": "Multi-class",
          "Findings": "98.21%"
        },
        {
          "Paper & Year": "Gosala\net\nal.\n[3]",
          "Approach": "Hybrid\nCNNs\nfor\nemotion\nrecognition",
          "Dataset": "GAMEEMO",
          "Classes": "Multi-class",
          "Findings": "95%"
        },
        {
          "Paper & Year": "Proposed\nFramework",
          "Approach": "Transfer\nlearning",
          "Dataset": "GAMEEMO",
          "Classes": "Binary,\nmulti-class,\nmulti-label\nclassifica-\ntion\nusing\nsliding\nwindow",
          "Findings": "93.3%,\n94.5%,\n90.6%"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Transformers for eegbased emotion recognition: A hierarchical spatial information learning model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "2",
      "title": "An eeg-driven framework for emotion recognition during gameplay",
      "authors": [
        "K Shahzad",
        "Z Ali",
        "U Rauf",
        "A Rehman",
        "S Khan",
        "S Noorani"
      ],
      "year": "2024",
      "venue": "2024 5th International Conference on Advancements in Sciences (ICACS)"
    },
    {
      "citation_id": "3",
      "title": "Hybrid convolutional neural networks for multi-emotion classification using gameemo",
      "authors": [
        "B Gosala",
        "B Jagwani",
        "M Gupta"
      ],
      "year": "2024",
      "venue": "International Conference on Advanced Communications and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "An eeg data processing approach for emotion recognition",
      "authors": [
        "G Li",
        "D Ouyang",
        "Y Yuan",
        "W Li",
        "Z Guo",
        "X Qu",
        "P Green"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "A Saxena",
        "A Khanna",
        "D Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "6",
      "title": "Enhancing mental health with artificial intelligence: Current trends and future prospects",
      "authors": [
        "D Olawade",
        "O Wada",
        "A Odetayo",
        "A David-Olawade",
        "F Asaolu",
        "J Eberhardt"
      ],
      "year": "2024",
      "venue": "Journal of medicine"
    },
    {
      "citation_id": "7",
      "title": "Integrating artificial intelligence to assess emotions in learning environments: a systematic literature review",
      "authors": [
        "A Vistorte",
        "A Deroncele-Acosta",
        "J Ayala",
        "A Barrasa",
        "C L√≥pez-Granero",
        "M Mart√≠-Gonz√°lez"
      ],
      "year": "2024",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition and classification based on audio data using ai",
      "authors": [
        "S Bekenova",
        "A Bekenova"
      ],
      "year": "2023",
      "venue": "Emotion recognition and classification based on audio data using ai"
    },
    {
      "citation_id": "9",
      "title": "Tltd: Transfer learning for tabular data",
      "authors": [
        "M Bragilovski",
        "Z Kapri",
        "L Rokach",
        "S Levy-Tzedek"
      ],
      "year": "2023",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition based on fusion of long short-term memory networks and svms",
      "authors": [
        "T Chen",
        "H Yin",
        "X Yuan",
        "Y Gu",
        "F Ren",
        "X Sun"
      ],
      "year": "2021",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Toward user-independent emotion recognition using physiological signals",
      "authors": [
        "A Albraikan",
        "D Tob√≥n",
        "A Saddik"
      ],
      "year": "2018",
      "venue": "IEEE sensors Journal"
    },
    {
      "citation_id": "12",
      "title": "Reactivity of emotions in adolescents-caregivers' tool (react): Development and validation of a novel parent-rated measure for assessing emotional dysregulation in youth",
      "authors": [
        "G Sesso",
        "F Guccione",
        "L Conti",
        "E Valente",
        "A Narzisi",
        "S Berloffa",
        "P Fantozzi",
        "V Viglione",
        "G Masi",
        "A Milone"
      ],
      "year": "2024",
      "venue": "Clinical Neuropsychiatry"
    },
    {
      "citation_id": "13",
      "title": "Emotional regulation and well-being among elderly",
      "authors": [
        "E Yadav",
        "S Chanana"
      ],
      "year": "2018",
      "venue": "International Journal of Scientific and Research Publications"
    },
    {
      "citation_id": "14",
      "title": "Ensemble machine learning-based affective computing for emotion recognition using dual-decomposed eeg signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "15",
      "title": "Fear generalization and anxiety: behavioral and neural mechanisms",
      "authors": [
        "J Dunsmoor",
        "R Paz"
      ],
      "year": "2015",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "16",
      "title": "Use of differential entropy for automated emotion recognition in a virtual reality environment with eeg signals",
      "authors": [
        "H Uyanƒ±k",
        "S Ozcelik",
        "Z Duranay",
        "A Sengur",
        "U Acharya"
      ],
      "year": "2022",
      "venue": "Diagnostics"
    },
    {
      "citation_id": "17",
      "title": "Machine learning techniques for emotion detection and sentiment analysis: current state, challenges, and future directions",
      "authors": [
        "A Alslaity",
        "R Orji"
      ],
      "year": "2024",
      "venue": "Behaviour & Information Technology"
    },
    {
      "citation_id": "18",
      "title": "Multi-feature input deep forest for eeg-based emotion recognition",
      "authors": [
        "Y Fang",
        "H Yang",
        "X Zhang",
        "H Liu",
        "B Tao"
      ],
      "year": "2021",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "19",
      "title": "Audio-visual stimulation based emotion classification by correlated eeg channels",
      "authors": [
        "M Ahirwal",
        "M Kose"
      ],
      "year": "2020",
      "venue": "Health and Technology"
    },
    {
      "citation_id": "20",
      "title": "Exploring the feature selection of the eeg signal time and frequency domain features for knn and weighted k-nn",
      "authors": [
        "K Diah",
        "A Faqih",
        "B Kusumoputro"
      ],
      "year": "2019",
      "venue": "2019 IEEE R10 Humanitarian Technology Conference (R10-HTC"
    },
    {
      "citation_id": "21",
      "title": "Wavelet packet energy features for eeg-based emotion recognition",
      "authors": [
        "M Algumaei",
        "I Hettiarachchi",
        "R Veerabhadrappa",
        "A Bhatti"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition based on eeg signals in response to bilingual music tracks",
      "authors": [
        "R Zainab",
        "M Majid"
      ],
      "year": "2021",
      "venue": "Int. Arab J. Inf. Technol"
    },
    {
      "citation_id": "23",
      "title": "Adaptive neuro-fuzzy based hybrid classification model for emotion recognition from eeg signals",
      "authors": [
        "F Bardak",
        "M Seyman",
        "F Temurtas"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "24",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Physics Constrained Data-Driven Technique for Reservoir Proxy Model and Model Order Reduction",
      "authors": [
        "A Bao"
      ],
      "year": "2019",
      "venue": "Physics Constrained Data-Driven Technique for Reservoir Proxy Model and Model Order Reduction"
    }
  ]
}