{
  "paper_id": "2210.00263v1",
  "title": "Fine-Tuning Wav2Vec For Vocal-Burst Emotion Recognition",
  "published": "2022-10-01T12:03:27Z",
  "authors": [
    "Dang-Khanh Nguyen",
    "Sudarshan Pant",
    "Ngoc-Huynh Ho",
    "Guee-Sang Lee",
    "Soo-Huyng Kim",
    "Hyung-Jeong Yang"
  ],
  "keywords": [
    "Vocal Burst",
    "Emotion Recognition",
    "Wav2vec"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The ACII Affective Vocal Bursts (A-VB) competition introduces a new topic in affective computing, which is understanding emotional expression using the non-verbal sound of humans. We are familiar with emotion recognition via verbal vocal or facial expression. However, the vocal bursts such as laughs, cries, and signs, are not exploited even though they are very informative for behavior analysis. The A-VB competition comprises four tasks that explore non-verbal information in different spaces. This technical report describes the method and the result of SclabCNU Team for the tasks of the challenge. We achieved promising results compared to the baseline model provided by the organizers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Language and facial expression are strong indicators for behavior analysis. There is numerous research trying to solve the emotion recognition problem based on these data. However, the non-linguistic vocalizations are understudied even though they are very useful information. Analyzing and applying these signals are interesting topics and require more attention from researchers. The A-VB competition was conducted for that reason and is expected to explore advanced improvements in emotion science. The competition  [2]  consists of 4 individual tasks as below:\n\n• High-dimension task (A-VB-HIGH): a multi-output regression task generating 10 values in the range of [0,1] corresponding to levels of Awe, Excitement, Amusement, Awkwardness, Fear, Horror, Distress, Triumph, Sadness, and Surprise. The greater the score is, the better the model performs. We should also note that all the results in this paper are in percentage.\n\nIn this paper, we propose a straightforward approach using a pre-trained Wav2vec network to resolve the problem. The model accomplishes a noticeable improvement compared to the baseline provided by the organizers. Because of its simplicity, our model can be considered a new baseline for all tasks in the competition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In the baseline paper  [2] , the authors introduce two different approaches, which are feature-based and end-to-end approaches. In the feature-based option, the OpenSMILE toolkit  [11]  is leveraged to extract the COMPARE (COMputational PARalinguistics ChallengE  [12] ) feature and EGEMAPS (The extended Geneva Minimalistic Acoustic Parameter Set  [13] ) feature from an input sample. The features are then fed to a 3-layer fully-connected neural network. Mean Squared Error (MSE) loss is used for regression tasks while the classification task applies the Cross-entropy (CE) loss function.\n\nIn the end-to-end manner, the baseline model uses End2You  [14] , a multimodal profiling toolkit that is capable of training and evaluating models from raw input. Particularly, Emo-18 architecture  [15]  is chosen for the competition. The model includes 1-D Convolutional Neural Network (CNN) layers to derive the features from audio frames and a Recurrent Neural Network to learn the temporal information.\n\nFor the speaker recognition task, Nik Vaessen and David A. van Leeuwen  [4]  conducted fine-tuning the Wav2vec2 model by using a shared fully-connected layer. Their model and ours have one thing in common: exploiting the pretrained Wav2vec. However, there are considerable differences between the two methods. Basically, speaker recognition is a classification problem so the authors optimize the model with CE or Binary Cross-entropy (BCE) loss. In our method, we consider two loss options for the regression tasks, which are MSE and CCC loss. Additionally, besides using a shared fullyconnected layer, we also take advantage of the RNN to explore the temporal behavior.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Method",
      "text": "The sequence embeddings are obtained from the waveform signal by the audio extractor. They are then fed into an RNN to enrich the sequence information. Afterward, a fully connected layer changes the embeddings' dimension to the output sizes depending on the particular task. Finally, a pooling layer is used to reduce variable-length sequence embeddings to fixsize speaker embedding. The dimension of the final prediction would be 2, 10, 40, or 8 corresponding to A-VB-TWO, A-VB-HIGH, A-VB-CULTURE or A-VB-TYPE task, respectively. Figure  1  describes the architecture of our method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Audio Extractor",
      "text": "We take advantage of the Pre-trained Wav2vec 2.0 models  [3]  provided by Pytorch. They are trained with large unlabeled audio corpora so they can effectively capture the audio features. We conducted experiments with 4 versions of the Wav2vec2 model described below:\n\n• BASE: use the base configuration of transformer trained with 960 hours of unlabeled audio from LibriSpeech  [5] . • LARGE: use the large configuration of transformer trained with 960 hours of unlabeled audio from Lib-riSpeech  [5] .\n\n• LARGE-LV60K: use the large configuration of transformer trained with 60,000 hours of unlabeled audio from Libri-Light  [6] . • XLSR53: use base configuration of transformer  [10]  trained with 56,000 hours of unlabeled audio from multiple datasets (Multilingual LibriSpeech  [7] , CommonVoice  [8]  and BABEL  [9] ).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Pooling Method",
      "text": "Inspired by the model of  [4] , we use 4 options of pooling to fix the length of embedding: first (take the first sequence embedding to be the speaker embedding), last (take the last sequence embedding to be the speaker embedding), max, and average pooling. The performance of models with various types of pooling layers is recorded to study their impact on the result. The operation of the pooling can be described as:\n\nwhere s i is the speaker embedding of i th sample; e 1 , e 2 , ..., e mi are the temporal embeddings and m i is the sequence length of corresponding sample.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Loss Function",
      "text": "We use the CE loss for the classification task. In the remaining tasks, we want to test the effect of the loss function on the performance of the model so we did the experiments and compared the result of the model using Mean Square Error (MSE) and Concordance Correlation Coefficient (CCC) loss. The CCC loss is formulated as below:\n\nwhere x and y are the mean values of ground truth and predicted values, respectively, s x and s x are corresponding variances and s xy is the covariance value.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Dataset And Experiments A. Dataset",
      "text": "The database used for the competition is the HUME-VB dataset  [1]  which consists of 59201 audio files and is split into 3 sets (training, validation, and test) of similar size. Each file has 53 labels corresponding to 4 tasks, one label is a categorical label that is used in the classification task and the remains are values in the range [0,1] representing the emotional level. The organizers provide 2 versions of the HUME-VB dataset: the raw version sampled at 48kHz and the processed version where audio files are converted to 16kHz and normalized to -3 decibels. In our experiments, we take the processed version as the input of our model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Experiments",
      "text": "Our model was implemented using the Pytorch framework. The experiments were conducted on a machine with NVIDIA RTX 2080 Ti GPU. All scenarios were run in 20 epochs, and the model with the best performance on the validation set was recorded. The batch size is 16 and the initial learning rate is 1e -4. We used Adam optimizer with the weight decay coefficient of 0.0625.\n\nIn our setting, we take the output from the 12 th layer of the Wav2vec network to be the sequence embeddings. The input size of the RNN network depends on the configuration of the pre-trained audio extractor, which is 768 for base configuration and 1024 for large configuration. It includes 2 LSTM layers and the hidden size is fixed to 512.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Discussion",
      "text": "We tested the performance of the model with various audio extractors to explore their effect. Table  I  shows the performance on four tasks of the competition with 4 pre-trained Wav2vec extractors. As a result, the XLSR53 pre-trained model achieves the best performance in A-VB-TWO and A-VB-TYPE when LARGE-LV60K attains the highest scores in A-VB-HIGH and A-VB-CULTURE. In the meanwhile, the BASE model produces the lowest score in A-VB-TWO and A-VB-TYPE due to its simple architecture.\n\nRegarding the pooling method, we examined their influence on the results in A-VB-HIGH. As shown in Table  II , in both LARGE-LV60K and XLSR53 model, the Last pooling outperforms the other options while the First pooling gets the lowest CCC score among the four methods. The result of Avg pooling is slightly better than Max pooling in both LARGE-LV60K and XLSR53 scenarios. It can be inferred that the last embedding of the sequence contains the most useful information for the prediction when using other embeddings or combining them may degrade the accuracy.\n\nNext, we conducted the training processes with MSE and CCC to explore their advantage. As a consequence, the model trained with CCC loss gives better performance on the validation set compared to the one trained with MSE loss. The detail is shown in Table  III . In addition, we carried out the ablation study to analyze the role of the RNN. According to Table  IV , using the RNN can significantly boost the accuracy of the model in all four tasks. It can be explained by the capability of learning temporal information of the LSTM, which can enhance the overall operation of the model.\n\nAfter conducting the above experiments, we concluded that the best configuration of our model is combining either LARGE-LV60K or XLSR53 pre-trained model with last pooling method and utilizing CCC loss. This setting was used to train separated model for each task in order to obtain unbiased evaluation on test set. We decided to choose LARGE-LV60K for A-VB-HIGH and A-VB-CULTURE, XLSR53 for A-VB-TWO and A-VB-TYPE. This time we trained each model for 50 epochs and applied early stopping by monitoring the validation result. Our best models and their evaluations on test set and validation set are listed in Table  V .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper presents our proposed method for all subchallenges of the A-VB competition. Particularly, we finetuned the pre-trained Wav2vec and combined it with basic neural networks and a proper pooling method. The CCC loss and Last pooling show the best performance on four tasks among the other options. Our model outperforms the baseline of the organizer on the test set, with the CCC score of 62.02 for A-VB-TWO, 66.77 for A-VB-HIGH, 54.95 for A-VB-CULTURE and UAR metric of 49.70 for A-VB-TYPE.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: describes the architecture of our method.",
      "page": 2
    },
    {
      "caption": "Figure 1: Block diagram of our proposed model.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "End2You Baseline",
          "TWO": "49.88",
          "HIGH": "56.38",
          "CULTURE": "43.59",
          "TYPE": "41.66"
        },
        {
          "Model": "BASE",
          "TWO": "54.65",
          "HIGH": "58.69",
          "CULTURE": "47.18",
          "TYPE": "41.65"
        },
        {
          "Model": "LARGE",
          "TWO": "55.42",
          "HIGH": "58.00",
          "CULTURE": "47.12",
          "TYPE": "43.96"
        },
        {
          "Model": "LARGE-LV60K",
          "TWO": "61.59",
          "HIGH": "65.41",
          "CULTURE": "53.39",
          "TYPE": "47.22"
        },
        {
          "Model": "XLSR53",
          "TWO": "61.94",
          "HIGH": "65.32",
          "CULTURE": "52.50",
          "TYPE": "49.89"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task\nname": "",
          "Pre-trained\nAudio Extractor": "",
          "Performance": "Val set\nTest set"
        },
        {
          "Task\nname": "TWO",
          "Pre-trained\nAudio Extractor": "XLSR53",
          "Performance": "61.94\n62.02"
        },
        {
          "Task\nname": "HIGH",
          "Pre-trained\nAudio Extractor": "LARGE-LV60K",
          "Performance": "66.76\n66.77"
        },
        {
          "Task\nname": "CULTURE",
          "Pre-trained\nAudio Extractor": "LARGE-LV60K",
          "Performance": "54.93\n54.95"
        },
        {
          "Task\nname": "TYPE",
          "Pre-trained\nAudio Extractor": "XLSR53",
          "Performance": "49.89\n49.70"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The hume vocal burst competition dataset (H-VB) -raw data",
      "authors": [
        "Alan Cowen",
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Michael Opara",
        "Lauren Kim",
        "Jeff Brooks",
        "Jacob Metrick"
      ],
      "year": "2022",
      "venue": "The hume vocal burst competition dataset (H-VB) -raw data"
    },
    {
      "citation_id": "2",
      "title": "The ACII 2022 Affective Vocal Bursts Workshop & Competition: Understanding a critically understudied modality of emotional expression",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Anton Batliner",
        "Björn Schuller",
        "Dacher Keltner",
        "Alan Cowen"
      ],
      "year": "2022",
      "venue": "The ACII 2022 Affective Vocal Bursts Workshop & Competition: Understanding a critically understudied modality of emotional expression",
      "arxiv": "arXiv:2207.03572"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Fine-tuning wav2vec2 for speaker recognition",
      "authors": [
        "Nik Vaessen",
        "David Van Leeuwen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Libri-light: a benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Mls: a large-scale multilingual dataset for speech research",
      "authors": [
        "Qiantong Vineel Pratap",
        "Anuroop Xu",
        "Gabriel Sriram",
        "Ronan Synnaeve",
        "Collobert"
      ],
      "year": "2020",
      "venue": "Mls: a large-scale multilingual dataset for speech research"
    },
    {
      "citation_id": "8",
      "title": "Common voice: a massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2020",
      "venue": "Common voice: a massively-multilingual speech corpus"
    },
    {
      "citation_id": "9",
      "title": "Speech recognition and keyword spotting for low-resource languages: babel project research at cued",
      "authors": [
        "Mark John",
        "Francis Gales",
        "Kate Knill",
        "Anton Ragni",
        "Shakti Prasad"
      ],
      "year": "2014",
      "venue": "Speech recognition and keyword spotting for low-resource languages: babel project research at cued"
    },
    {
      "citation_id": "10",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "Alexis Conneau",
        "Alexei Baevski",
        "Ronan Collobert",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition"
    },
    {
      "citation_id": "11",
      "title": "Opensmile: the Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "J Hirschberg",
        "J Burgoon",
        "A Baird",
        "A Elkins",
        "Y Zhang",
        "E Coutinho",
        "K Evanini"
      ],
      "year": "2016",
      "venue": "Proceedings of INTERSPEECH"
    },
    {
      "citation_id": "13",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "arxiv": "arXiv:1802.01115"
    },
    {
      "citation_id": "15",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    }
  ]
}