{
  "paper_id": "2501.04904v1",
  "title": "Jelly: Joint Emotion Recognition And Context Reasoning With Llms For Conversational Speech Synthesis",
  "published": "2025-01-09T01:32:44Z",
  "authors": [
    "Jun-Hyeok Cha",
    "Seung-Bin Kim",
    "Hyung-Seok Oh",
    "Seong-Whan Lee"
  ],
  "keywords": [
    "Conversational speech synthesis",
    "dialogue",
    "emotional context reasoning",
    "large language model",
    "low-rank adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context. To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules. We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech. The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech. The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation. Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Conversational speech synthesis (CSS) focuses on generating speech that has contextually appropriate prosody within a conversation. The primary distinction between CSS and text-to-speech (TTS)  [1] -  [5]  tasks is that conventional TTS does not consider prior conversation history. In contrast, CSS reflects the interactions between speakers within a conversation. Earlier CSS approaches  [6] -  [10]  explored various strategies for utilizing conversation history to generate natural speech in a conversational context. The GRU-based approach  [6]  focuses on textual information of dialogue, extracting utterance-level semantic features to generate conversational speech. However, this approach has overlooked the information in speech, leading to limitations in fully capturing conversational context.\n\nTo address these limitations,  [7]  enhances speaking style through graph-based multi-modal context modeling. M2CTTS  [8]  proposes a method that leverages both textual and acoustic information, addressing coarse-and fine-grained features simultaneously within each domain. CONCSS  [9]  introduces a contrastive learning-based method that generates context-sensitive representations for dialogueappropriate prosody. Despite these advancements and progress in deep learning  [11] -  [16] , fully capturing expressive conversational contexts like emotional speech  [17] -  [20]  remains challenging.\n\nIn human conversations, even when the literal content is identical, emotional context varies depending on the emotions conveyed by ‚Ä† Corresponding author. This each speaker, as shown in Fig.  1 . Speech that is mismatched with the emotional context feels unnatural or awkward, emphasizing the importance of considering the emotional context in the CSS task. ECSS  [21]  demonstrates the importance of considering emotion in the CSS task through a heterogeneous graph-based approach that predicts appropriate emotional expressions by using ground-truth emotion labels of each utterance within a conversation history.\n\nStill, understanding emotional context remains challenging in the CSS task, as it requires recognizing each utterance's emotion and integrating it with the conversation content. Both steps are complex, even for humans. Moreover, the combined task of inferring emotional context and generating contextually appropriate speech requires highquality emotional conversational datasets, which are scarce.\n\nTo address these challenges, we introduce a novel framework, JELLY that incorporates Joint Emotion recognition and context reasoning with a Large Language model for conversational speech sYnthesis. Our framework consists of three stages that enhance emotional context inference for CSS and alleviate the scarcity of emotional conversational speech datasets. To infer the emotional context, we leverage the reasoning ability of LLM through the partial LoRA (PLoRA) approach  [22] ,  [23] . To align speech emotions with text, we propose an Emotion-aware Q-former encoder (EQ-former). Integrating it with Whisper as a speech encoder enables the LLM to perceive emotional states from speech and infer the emotional context accordingly. By leveraging this speech encoder, JELLY infers the emotional context for CSS from speech alone during inference, making it more suitable to real-world scenarios.\n\nOur contributions are summarized as follows:\n\n‚Ä¢ We introduce JELLY, a novel CSS framework designed to synthesize emotionally appropriate speech in conversational contexts, mitigating the scarcity of emotional conversational data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion-Aware Q-Former Encoder",
      "text": "Comprehending the emotional context of a conversation requires recognizing the emotional state of the speaker in each utterance. Accordingly, we propose EQ-former as an emotion perception module for each utterance, as described in Fig.  2 . The EQ-former consists of the Time and Layer-Wise  [24]  Transformer (TLTR) for emotion feature extraction, the Querying Transformer (Q-former)  [25]  for emotion alignment, and a projection layer.\n\nWe adopt a Whisper encoder  [26]  and the TLTR module to extract emotional information from speech. The Whisper encoder representations encompass various types of speech information, such as paralinguistic cues  [24] ,  [27] . We leverage these rich paralinguistic representations to capture emotion-related information. The Whisper encoder takes the waveform as input, and the intermediate representations from the 32 layers of the Whisper encoder are passed to the TLTR. We specifically apply the attention mechanism to enable the model to focus on the layers that contain more relevant emotional information through the TLTR.\n\nAdditionally, we utilize the Q-former module to align emotion representations with text representations. Emotion features extracted by TLTR interact with a fixed set of learnable query embeddings through cross-attention layers in the Q-former module. The query tokens pass through a self-attention layer and then interact with the emotion features via the cross-attention layer, aligning the query outputs that hold emotional information with the text representation. This cross-modal alignment is crucial for enabling the LLM to reason about the emotional context by jointly understanding emotional information of each utterance and the content of the dialogue. Finally, we apply a linear layer to project the query outputs to match the embedding dimension of the LLM, ensuring that the projection layer is trainable.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Llm With Multiple Plora Modules",
      "text": "To efficiently handle both emotion and text embeddings, we follow the partial LoRA (PLoRA)  [22] ,  [23]  approach, which is designed to adapt the LLM's parameters to partial inputs. This approach preserves the LLM's integrity in encoding and generating text tokens while providing additional modeling capacity to address gaps between different modalities.\n\nIn our task, we fine-tune the LLM for emotional context reasoning, guiding its behavior depending on the input modality by employing multiple PLoRA modules. Specifically, PLoRA is applied to emotion embeddings (PLoRA-E) from the EQ-former and to text embeddings (PLoRA-T) from the LLM's tokenizer. By applying distinct PLoRA modules for each modality, we selectively adapt the LLM's parameters, effectively reducing the gap between emotion and text while fine-tuning the LLM for emotional context reasoning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Three-Stage Learning Pipeline",
      "text": "We designed a three-stage learning pipeline that enables 1) recognition of emotions in each utterance, 2) appropriate emotion inference based on both the perceived emotions and the conversation content, and 3) generation of speech that aligns with the inferred emotional context, mitigating the scarcity of emotional conversational datasets.\n\n1) Emotion-Text Alignment: The goal of the first stage is to train the EQ-former to extract emotional information from an utterance and align the emotion features with the text embedding space. The emotion-text alignment stage is designed to enable the LLM to perceive the emotional states of each utterance in the dialogue history. The EQ-former is trained using the LLM's next-token prediction task to extract emotion representations. We adapt the LLM with multiple PLoRA modules to determine the emotional states of the target speech, using emotion embeddings from the EQ-former and text embeddings from the instruction prompt, as shown in the left panel of Fig.  2 . The EQ-former and PLoRA modules are trained to align text with speech emotions, leveraging the more readily available emotional speech data. During training, the Whisper encoder and LLM are frozen.\n\n2) Emotional Context Reasoning: The emotional context reasoning stage aims to infer the emotional context of the dialogue by considering both the content of each utterance and the perceived emotions. We consider N utterances in a conversation, denoted as {u0, u1, . . . , u k , . . . uN }, where u k represents the k-th utterance in the conversation history. Each utterance u k is associated with a corresponding ground-truth transcript t k , speaker ID s k , and textual emotion label e k . We fine-tune the LLM with multiple PLoRA modules to predict the appropriate emotional state of the target utterance uc within the given dialogue history, where c ‚àà [2, N ] represents the current turn. For this process, we generate the instruction embedding I and the k-th utterance embedding U k , corresponding to the kth utterance u k in the dialogue history, where k ‚àà [0, c]. These embeddings are generated from the EQ-former pre-trained in the first stage and the LLM's tokenizer for the LLM's input, as illustrated in Fig.  2  (a) and (b). Each utterance embedding U k is formed by concatenating the prefix embedding P k , emotion embedding √äk , and transcript embedding T k along the temporal dimension. The prefix embedding P k and transcript embedding T k are text embeddings generated from the prefix text p k and the ground-truth transcript t k using the LLM's tokenizer. The prefix text p k indicates the order in which speaker s k is speaking. We utilize the Whisper decoder during inference to generate T k from speech. The emotion embedding √äk , which represents the recognized emotional state of the k-th utterance, is derived from the EQ-former. For generating the current utterance embedding Uc, only Pc and Tc are used. The instruction embedding I, generated from the text instruction prompt by using the LLM's tokenizer, guides the model to predict the emotion and intensity for the current utterance, based on U0:c and I.\n\nTo address the scarcity of emotional conversation datasets for emotional context reasoning, we adopt a pre-training strategy in the second stage. This strategy focuses on training the PLoRA-T module using only textual data, including emotion labels e k , from DailyTalk  [29]  and DailyDialog  [30] . In this pre-training, the emotion embedding E k is derived from the ground-truth emotion label e k in text form and converted into a text embedding using the LLM's tokenizer. By deriving emotion embeddings from text, the PLoRA-T module can be trained on large-scale textual datasets, allowing the LLM to better infer emotional context in conversations. During this pre-training process, we train only the PLoRA-T adapter. We then fine-tune the LLM on the DailyTalk dataset to predict the emotional state of target utterance, initializing it with the pre-trained PLoRA-T module from this pre-training stage, along with the PLoRA-E module and EQ-former trained in the first stage. This approach alleviates the limitation of insufficient emotional conversation datasets and enhances the model's ability for emotional context reasoning.\n\n3) Emotional Context-Aware Speech Synthesis: The emotional context-aware speech synthesis stage generates speech with appropriate emotional expressions based on the inferred emotional state. We use FastSpeech 2  [2]  as the backbone model, incorporating an emotion encoder and an intensity encoder to generate target speech that appropriately reflects the predicted emotion and intensity based on the emotional context, as depicted in the top-right panel of Fig.  2 . The models for the emotional context reasoning stage and emotional context-aware speech synthesis are trained separately and concatenated during inference.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "For the first stage, we use the DailyTalk  [29] , CREMA-D  [31] , EmoV-DB  [32] , IEMOCAP  [33] , MEAD  [34] , and TESS  [35]  datasets, with a total of 80.6 hours of speech. We selected data corresponding to seven emotion categories (happy, sad, surprise, angry, fear, disgust, and neutral) and three intensity levels (weak, medium, and strong) available in each dataset. For the second stage, we use the DailyDialog  [30]  text dataset with 13,118 dialogues and the DailyTalk  [29]  dataset, which contains 20 hours of speech across 2,541 dialogues derived from DailyDialog. We added three intensity levels from ECSS  [21]  to the DailyTalk dataset, which originally did not include intensity labels. All audio samples are re-sampled at 16 kHz during both the text-emotion alignment and emotional context reasoning stages. To train the third stage, we use the DailyTalk  [29]  dataset. The audio is converted to Mel-spectrogram using short-timefourier transform with an FFT size of 1024, a hop size of 256, and a window size of 1024, applying an 80-bins mel filter. All audio samples in this stage are down-sampled at 22.05 kHz for speech synthesis, and the data was split into training, validation, and test sets with a ratio of 8:1:1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We use the official pre-trained Whisper Large v3  [26]  and Vicuna-7B LLM  [36] , which is a LLaMA model  [37]  fine-tuned to follow instructions using the Vicuna instruction set. For Q-former, we employ 25 trainable query tokens with a dimension of 768. Multiple PLoRA adapters are injected into the projection layers for all queries and values in every LLaMA self-attention layer, with a rank of 8 and a scaling factor of 4.0 for both modules. In the first and second stages, we apply the AdamW optimizer  [38]  with Œ≤1 = 0.9, Œ≤2 = 0.999, weight decay of 0.05, and a cosine learning rate decay, peaking at 3 √ó 10 -5 with a linear warmup of 3k steps and a minimum learning rate of 1 √ó 10 -5 . During training, only the parameters of the EQformer and the PLoRA modules are updated, while the TLTR module is frozen in the second stage. In the third stage, the AdamW optimizer settings are Œ≤1 = 0.9, Œ≤2 = 0.98, and HiFi-GAN  [28]  is used as the vocoder. All models are trained for 180k steps in the first stage, 30k steps in the second stage, and 275k steps in the third stage, using 4 NVIDIA RTX A6000 GPUs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "We conduct two mean opinion score (MOS) tests to subjectively evaluate the naturalness in dialogue context (N-DMOS) and the alignment of emotional expression with the dialogue's emotional context (E-DMOS). For MOS evaluation, we use Amazon MTurk to crowdsource 20 listeners, and the results are reported with a 95% confidence interval. To evaluate whether synthesized speech contains appropriate emotional expressions, we use the emotion2vec plus large model  [39] , a speech emotion recognition foundation model finetuned on approximately 42,000 hours of emotional data, to measure emotional classification accuracy (ECA). To evaluate pronunciation accuracy, we calculate the word error rate (WER) and phoneme error rate (PER) using Whisper  [26]  and wav2vec 2.0  [40] , respectively. To evaluate prosody, we compute the pitch error (RMSE f 0 ), periodicity error (RMSEp), F1 score of voiced/unvoiced classification (F1 v/uv ), and the Mel-cepstral distortion (MCD). We conducted a duration prediction performance evaluation using the average absolute differences of the utterance duration (DDUR)  [41] . To evaluate the emotional context reasoning, we calculate the accuracy of emotion and intensity predictions using the weighted accuracy (WA), unweighted average accuracy (UA), and macro F1 score (F1). These values are reported as percentages.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "We compared our JELLY framework with other CSS frameworks, including ECSS  [21]  and a GRU-based approach  [6]  as baselines.\n\nFor ECSS, we used the official implementation for training. To verify that JELLY infers emotional context and generates contextually appropriate speech from speech alone, we also evaluated JELLY (speech-only), which performs inference using only speech through the EQ-former and Whisper. Additionally, we compared it with our TTS backbone model, FastSpeech 2  [2] , without context modeling.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Emotional Context-Aware Speech Synthesis",
      "text": "We conducted both subjective and objective evaluations to assess whether the generated speech appropriately reflects the emotional context. Table  I  shows that JELLY outperforms the other baseline models across nearly all metrics, particularly in terms of N-DMOS, E-DMOS, ECA, MCD, and DDUR. The high E-DMOS and ECA scores validate that our framework generates natural speech with emotional expressions closely aligned with the emotional context. Specifically, the results show that JELLY, unlike other CSS baselines, effectively understands the emotional context and generates speech that is appropriate for the emotional context.\n\nThe low WER and PER scores indicate that JELLY generates speech with reasonably accurate pronunciation, and the low DDUR score highlights JELLY's ability to model duration within the emotional context. Low error rates in terms of RMSEp and MCD, along with the high scores in N-DMOS and F1 v/uv , demonstrate that our framework models prosody appropriate for conversational contexts.\n\nJELLY (speech-only) also demonstrates that our framework can generate emotionally context-aware speech without relying on transcripts or emotion labels during inference, as reflected in its high E-DMOS and ECA scores. Notably, JELLY (speech-only) outperforms the comparison models in MCD, DDUR, and F1 v/uv , further showing that our framework, through its outstanding emotional context reasoning, generates speech with prosody well-suited to conversational contexts using only speech.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Emotional Context Reasoning",
      "text": "We conducted comparative experiments on WA, UA, and F1. ECSS  [21]  was used as the baseline, and its emotion and intensity predictors were employed for comparison with our framework. Table  II  presents the results, demonstrating that JELLY outperformed the baseline across all metrics for both emotion and intensity predictions. Although ECSS exhibited inaccuracies in prediction, the prosody predictor in ECSS, which predicts prosody based on previous conversations, appears to compensate for these errors, as reflected in the high N-DMOS and low RMSE f 0 results shown in Table  I . JELLY infers emotional context from recognized emotions using the proposed EQformer, even without ground truth emotion labels, leading to more accurate predictions of the target utterance's emotional state.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Ablation Study",
      "text": "As detailed in Table  II , we conducted ablation studies to verify the performance of each module in our proposed method. We assessed the impact of TLTR by replacing it with the last hidden layer representations of Whisper encoder, which led to performance declines in ECA, WA, and F1 macro in the emotion category and across all intensity metrics. This emphasizes the role of TLTR in extracting emotion features, including detailed information such as emotion intensity, and improving emotional context reasoning.\n\nNext, removing Q-former and directly feeding emotion features from TLTR into the LLM through a projection layer led to a significant performance drop across all metrics, confirming that the Qformer plays a crucial role in bridging the modality gap between text and emotion, which is essential for the emotional context reasoning.\n\nWhen we replaced the multiple PLoRA modules with the standard LoRA adapter  [42] , the decline in ECA and intensity metrics shows that the multiple PLoRA modules contribute to more effective modeling of intensity, ensuring alignment with the conversational context.\n\nAdditionally, we conducted an experiment where emotional inference was performed directly in the second stage, without the first stage. The decreases across all metrics indicate that EQ-former pretrained in the first stage is effective for inferring emotional context and demonstrates the benefits of the value of using relatively available emotional speech data.\n\nFinally, skipping the pre-training of the PLoRA-T module in the second stage also led to lower performance across all metrics, highlighting the importance of pre-training with text data to address the scarcity of emotional conversational datasets and enhance the LLM's ability to reason the emotional context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we proposed JELLY, a novel CSS framework that integrates joint emotion recognition and context reasoning with a large language model. We introduced the EQ-former as an emotion perception module for emotion-text alignment and designed a threestage learning pipeline to mitigate the scarcity of emotional conversational datasets. Experimental results demonstrate that JELLY generates emotionally aligned and natural speech in conversational contexts, outperforming existing models-even without transcripts or emotion labels. By leveraging the reasoning capabilities of the LLM through multiple LoRA modules and a distinct pre-training strategy in the first and second stages, JELLY improves predictions of the target speaker's emotional states and synthesizes speech that is more appropriate for the emotional context. However, challenges remain in bridging the gap between CSS and real-world scenarios. In future work, we plan to address this by considering overlapped speech and larger speaker groups.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of different emotional contexts that can arise from the same",
      "page": 1
    },
    {
      "caption": "Figure 1: Speech that is mismatched with",
      "page": 1
    },
    {
      "caption": "Figure 2: The overview of JELLY framework.",
      "page": 2
    },
    {
      "caption": "Figure 2: The EQ-former consists",
      "page": 2
    },
    {
      "caption": "Figure 2: The EQ-former and PLoRA modules are trained to",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) and (b).",
      "page": 3
    },
    {
      "caption": "Figure 2: The models for the emotional context reasoning stage and",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Dialogue History\nCase 1\nCase 2"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "Abstract‚ÄîRecently,\nthere has been a growing demand for\nconversa-",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "tional\nspeech synthesis\n(CSS)\nthat\ngenerates more natural\nspeech by",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Medium happy\nMedium sad\nùíñùüè: ‚ÄúI just heard the news about the team.‚Äù"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "considering\nthe\nconversational\ncontext. To\naddress\nthis, we\nintroduce",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Medium happy\nWeak sad\nùíñùüê: ‚ÄúYeah, it‚Äôs pretty unexpected.‚Äù"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "JELLY, a novel CSS framework that\nintegrates emotion recognition and",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "context reasoning for generating appropriate speech in conversation by",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Weak surprise\nMedium disgust\nùíñùüë: ‚ÄúI know, I didn‚Äôt expect that at all.‚Äù"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "fine-tuning a large language model\n(LLM) with multiple partial LoRA",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "modules. We propose an Emotion-aware Q-former encoder, which enables",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Weak happy\nWeak sad\nùíñùüí: ‚ÄúWell, We‚Äôll just see how it goes‚Äù"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "the LLM to\nperceive\nemotions\nin\nspeech. The\nencoder\nis\ntrained\nto",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "align speech emotions with text, utilizing datasets of emotional\nspeech.",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Fig. 1. Examples of different emotional contexts that can arise from the same"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "The entire model\nis\nthen fine-tuned with conversational\nspeech data to",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "conversation content."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "infer emotional context for generating emotionally appropriate speech in",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "conversation. Our experimental results demonstrate that JELLY excels in",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "each speaker, as\nshown in Fig. 1. Speech that\nis mismatched with"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "emotional context modeling, synthesizing speech that naturally aligns with",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "the emotional context\nfeels unnatural or awkward, emphasizing the"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "conversation, while mitigating the\nscarcity of\nemotional\nconversational",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "importance of\nconsidering the\nemotional\ncontext\nin the CSS task."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "speech datasets.",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "ECSS [21] demonstrates the importance of considering emotion in the"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "Index Terms‚ÄîConversational\nspeech\nsynthesis,\ndialogue,\nemotional",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "context reasoning,\nlarge language model,\nlow-rank adaptation",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "CSS task through a heterogeneous graph-based approach that predicts"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "appropriate\nemotional\nexpressions\nby\nusing\nground-truth\nemotion"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "I.\nINTRODUCTION",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "labels of each utterance within a conversation history."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Still, understanding emotional context\nremains challenging in the"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "Conversational\nspeech\nsynthesis\n(CSS)\nfocuses\non\ngenerating",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "CSS task,\nas\nit\nrequires\nrecognizing each utterance‚Äôs\nemotion and"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "speech that has\ncontextually appropriate prosody within a\nconver-",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "integrating it with the conversation content. Both steps are complex,"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "sation. The\nprimary\ndistinction\nbetween CSS\nand\ntext-to-speech",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "even for humans. Moreover, the combined task of inferring emotional"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "(TTS)\n[1]‚Äì[5]\ntasks\nis\nthat\nconventional TTS\ndoes\nnot\nconsider",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "context and generating contextually appropriate speech requires high-"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "prior conversation history.\nIn contrast, CSS reflects\nthe interactions",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "quality emotional conversational datasets, which are scarce."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "between speakers within a conversation. Earlier CSS approaches [6]‚Äì",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "To\naddress\nthese\nchallenges, we\nintroduce\na\nnovel\nframework,"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "[10] explored various strategies for utilizing conversation history to",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "JELLY that\nincorporates\nJoint Emotion\nrecognition\nand\ncontext"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "generate natural speech in a conversational context. The GRU-based",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "reasoning with a Large Language model\nfor conversational\nspeech"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "approach [6]\nfocuses on textual\ninformation of dialogue, extracting",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "sYnthesis. Our\nframework\nconsists\nof\nthree\nstages\nthat\nenhance"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "utterance-level\nsemantic features\nto generate conversational\nspeech.",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "emotional\ncontext\ninference\nfor CSS and alleviate\nthe\nscarcity of"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "However,\nthis\napproach has overlooked the\ninformation in speech,",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "emotional\nconversational\nspeech\ndatasets. To\ninfer\nthe\nemotional"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "leading to limitations in fully capturing conversational context.",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "context, we leverage the reasoning ability of LLM through the partial"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "To address these limitations,\n[7] enhances speaking style through",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "LoRA (PLoRA) approach [22],\n[23]. To align speech emotions with"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "graph-based multi-modal context modeling. M2CTTS [8] proposes",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "text, we propose an Emotion-aware Q-former encoder\n(EQ-former)."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "a method\nthat\nleverages\nboth\ntextual\nand\nacoustic\ninformation,",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Integrating it with Whisper as a speech encoder enables\nthe LLM"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "addressing coarse-\nand fine-grained features\nsimultaneously within",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "to perceive\nemotional\nstates\nfrom speech and infer\nthe\nemotional"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "each domain. CONCSS [9]\nintroduces a contrastive learning-based",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "context accordingly. By leveraging this speech encoder, JELLY infers"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "method that generates context-sensitive representations for dialogue-",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "the emotional context\nfor CSS from speech alone during inference,"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "appropriate\nprosody. Despite\nthese\nadvancements\nand\nprogress\nin",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "making it more suitable to real-world scenarios."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "deep\nlearning\n[11]‚Äì[16],\nfully\ncapturing\nexpressive\nconversational",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "Our contributions are summarized as follows:"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "contexts like emotional speech [17]‚Äì[20]\nremains challenging.",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "‚Ä¢ We\nintroduce\nJELLY,\na\nnovel CSS\nframework\ndesigned\nto"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "In human conversations, even when the literal content\nis identical,",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "synthesize emotionally appropriate speech in conversational con-"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "emotional\ncontext varies depending on the\nemotions\nconveyed by",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "texts, mitigating the scarcity of emotional conversational data."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "‚Ä¢ Our method\neffectively\ninfers\nemotional\ncontext\nsolely\nfrom"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "‚Ä†Corresponding author. This work was partly supported by the\nInstitute",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "of Information & Communications Technology Planning & Evaluation (IITP)",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "speech, unlike existing CSS models that\nrequire emotion labels"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "grant funded by the Korea government (MSIT) (No. RS-2019-II190079, Arti-",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "or\ntranscripts of dialogue history during inference."
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "ficial Intelligence Graduate School Program (Korea University), No. RS-2021-",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "‚Ä¢ Experimental\nresults exhibit\nthat\nJELLY outperforms compari-"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "II-212068, Artificial Intelligence Innovation Hub, No. RS-2024-00336673, AI",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "son models in emotional context reasoning and speech synthesis"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "Technology for Interactive Communication of Language Impaired Individuals,",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": ""
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "and No. RS-2024-00436857, Information Technology Research Center (ITRC)",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "based on the conversational context. Audio samples and code are"
        },
        {
          "jh cha@korea.ac.kr\nsb-kim@korea.ac.kr": "support program).",
          "hs oh@korea.ac.kr\nsw.lee@korea.ac.kr": "available at https://jh-cha-prml.github.io/JELLY."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "Intensity Encoder",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "Large Language Model",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "Large Language Model",
          "weak happy": "",
          "Emotion Encoder": "Speaker Encoder",
          "3. Emotional Context-Aware": "Target Speech ùíñùíÑ"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "PLoRA-E",
          "medium surprise": "PLoRA-T",
          "2. Emotional Context Reasoning": "PLoRA-E",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "Text Encoder",
          "3. Emotional Context-Aware": "Acoustic Decoder"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "‚ãØ",
          "medium surprise": "‚ãØ",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "Whisper",
          "3. Emotional Context-Aware": "‚ãØ"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "‚ãØ",
          "weak happy": "ùë∞‚ãØ\n‚ãØ",
          "Emotion Encoder": "Encoder",
          "3. Emotional Context-Aware": "ùë¨ùíå"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "ùëºùüè",
          "weak happy": "ùëºùíÑ",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "Projection",
          "medium surprise": "Text Embedding",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "Whisper",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": "‚ãØ"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "Decoder",
          "3. Emotional Context-Aware": "ùëªùíå"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "Text Tokenizer",
          "2. Emotional Context Reasoning": "",
          "weak happy": "Text Tokenizer &",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "Q-former",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "Text Embedding",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": "‚ãØ"
        },
        {
          "1. Emotion-Text Alignment": "",
          "medium surprise": "",
          "2. Emotional Context Reasoning": "",
          "weak happy": "",
          "Emotion Encoder": "",
          "3. Emotional Context-Aware": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotional Speech": "Target Speaker\nonly in pre-training"
        },
        {
          "Emotional Speech": "(Medium surprise)\n(b) Details of k-th Utterance Embedding"
        },
        {
          "Emotional Speech": "Fig. 2.\nThe overview of JELLY framework."
        },
        {
          "Emotional Speech": "II. METHODS\nwhile providing additional modeling capacity to address gaps between"
        },
        {
          "Emotional Speech": "different modalities."
        },
        {
          "Emotional Speech": "A. Emotion-aware Q-former Encoder"
        },
        {
          "Emotional Speech": "In our task, we fine-tune the LLM for emotional context reasoning,"
        },
        {
          "Emotional Speech": "Comprehending the emotional context of a conversation requires"
        },
        {
          "Emotional Speech": "guiding its behavior depending on the input modality by employing"
        },
        {
          "Emotional Speech": "recognizing the emotional state of the speaker in each utterance. Ac-"
        },
        {
          "Emotional Speech": "multiple PLoRA modules. Specifically, PLoRA is applied to emotion"
        },
        {
          "Emotional Speech": "cordingly, we propose EQ-former as an emotion perception module"
        },
        {
          "Emotional Speech": "embeddings (PLoRA-E) from the EQ-former and to text embeddings"
        },
        {
          "Emotional Speech": "for each utterance, as described in Fig. 2. The EQ-former consists"
        },
        {
          "Emotional Speech": "(PLoRA-T)\nfrom the LLM‚Äôs tokenizer. By applying distinct PLoRA"
        },
        {
          "Emotional Speech": "of\nthe Time and Layer-Wise [24] Transformer\n(TLTR)\nfor emotion"
        },
        {
          "Emotional Speech": "modules for each modality, we selectively adapt\nthe LLM‚Äôs param-"
        },
        {
          "Emotional Speech": "feature\nextraction,\nthe Querying Transformer\n(Q-former)\n[25]\nfor"
        },
        {
          "Emotional Speech": "eters, effectively reducing the gap between emotion and text while"
        },
        {
          "Emotional Speech": "emotion alignment, and a projection layer."
        },
        {
          "Emotional Speech": "fine-tuning the LLM for emotional context\nreasoning."
        },
        {
          "Emotional Speech": "We\nadopt\na Whisper\nencoder\n[26]\nand\nthe TLTR module\nto"
        },
        {
          "Emotional Speech": "extract\nemotional\ninformation\nfrom speech. The Whisper\nencoder"
        },
        {
          "Emotional Speech": "C. Three-Stage Learning Pipeline"
        },
        {
          "Emotional Speech": "representations encompass various types of speech information, such"
        },
        {
          "Emotional Speech": "We designed a three-stage learning pipeline that enables 1) recog-\nas paralinguistic cues [24], [27]. We leverage these rich paralinguistic"
        },
        {
          "Emotional Speech": "nition of emotions in each utterance, 2) appropriate emotion inference\nrepresentations to capture emotion-related information. The Whisper"
        },
        {
          "Emotional Speech": "based on both the perceived emotions and the conversation content,\nencoder\ntakes the waveform as input, and the intermediate represen-"
        },
        {
          "Emotional Speech": "and 3) generation of\nspeech that aligns with the inferred emotional\ntations from the 32 layers of\nthe Whisper encoder are passed to the"
        },
        {
          "Emotional Speech": "context, mitigating the scarcity of emotional conversational datasets.\nTLTR. We specifically apply the attention mechanism to enable the"
        },
        {
          "Emotional Speech": "1) Emotion-Text Alignment: The goal of\nthe first stage is to train\nmodel\nto focus on the layers\nthat contain more relevant emotional"
        },
        {
          "Emotional Speech": "the EQ-former\nto extract\nemotional\ninformation from an utterance\ninformation through the TLTR."
        },
        {
          "Emotional Speech": "and align the emotion features with the text embedding space. The\nAdditionally, we\nutilize\nthe Q-former module\nto\nalign\nemotion"
        },
        {
          "Emotional Speech": "emotion-text\nalignment\nstage\nis\ndesigned\nto\nenable\nthe LLM to\nrepresentations with text\nrepresentations. Emotion features extracted"
        },
        {
          "Emotional Speech": "perceive the emotional states of each utterance in the dialogue history.\nby TLTR interact with a fixed set of\nlearnable query embeddings"
        },
        {
          "Emotional Speech": "The EQ-former\nis\ntrained\nusing\nthe LLM‚Äôs\nnext-token\nprediction\nthrough cross-attention layers\nin the Q-former module. The query"
        },
        {
          "Emotional Speech": "task\nto\nextract\nemotion\nrepresentations. We\nadapt\nthe LLM with\ntokens pass through a self-attention layer and then interact with the"
        },
        {
          "Emotional Speech": "multiple PLoRA modules\nto determine the emotional\nstates of\nthe\nemotion\nfeatures\nvia\nthe\ncross-attention\nlayer,\naligning\nthe\nquery"
        },
        {
          "Emotional Speech": "target\nspeech, using emotion embeddings\nfrom the EQ-former and\noutputs that hold emotional\ninformation with the text representation."
        },
        {
          "Emotional Speech": "text embeddings\nfrom the instruction prompt, as\nshown in the left\nThis cross-modal alignment is crucial for enabling the LLM to reason"
        },
        {
          "Emotional Speech": "panel of Fig. 2. The EQ-former and PLoRA modules are trained to\nabout\nthe\nemotional\ncontext\nby\njointly\nunderstanding\nemotional"
        },
        {
          "Emotional Speech": "align text with speech emotions, leveraging the more readily available\ninformation of each utterance and the content of the dialogue. Finally,"
        },
        {
          "Emotional Speech": "emotional\nspeech data. During training,\nthe Whisper\nencoder\nand\nwe\napply a\nlinear\nlayer\nto project\nthe query outputs\nto match the"
        },
        {
          "Emotional Speech": "LLM are frozen.\nembedding dimension of the LLM, ensuring that\nthe projection layer"
        },
        {
          "Emotional Speech": "is trainable.\n2) Emotional Context Reasoning: The emotional context\nreason-"
        },
        {
          "Emotional Speech": "ing stage\naims\nto infer\nthe\nemotional\ncontext of\nthe dialogue by"
        },
        {
          "Emotional Speech": "B. LLM with Multiple PLoRA Modules"
        },
        {
          "Emotional Speech": "considering\nboth\nthe\ncontent\nof\neach\nutterance\nand\nthe\nperceived"
        },
        {
          "Emotional Speech": "To efficiently handle both emotion and text embeddings, we follow\nemotions. We consider N utterances\nin a conversation, denoted as"
        },
        {
          "Emotional Speech": "the partial LoRA (PLoRA)\n[22],\n[23] approach, which is designed\nrepresents the k-th utterance in\n{u0, u1, . . . , uk, . . . uN }, where uk"
        },
        {
          "Emotional Speech": "to\nadapt\nthe LLM‚Äôs\nparameters\nto\npartial\ninputs. This\napproach\nthe\nconversation\nhistory. Each\nis\nassociated with\na\nutterance uk"
        },
        {
          "Emotional Speech": "preserves the LLM‚Äôs integrity in encoding and generating text\ntokens\ncorresponding ground-truth transcript tk, speaker\nID sk, and textual"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "emotion label ek. We fine-tune the LLM with multiple PLoRA mod-",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "III. EXPERIMENTS"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "ules to predict\nthe appropriate emotional state of the target utterance",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "A. Experimental Setup"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "uc within the given dialogue history, where c ‚àà [2, N ] represents the",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "For\nthe first\nstage, we use the DailyTalk [29], CREMA-D [31],"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "current\nturn. For this process, we generate the instruction embedding",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "EmoV-DB [32],\nIEMOCAP\n[33], MEAD [34],\nand\nTESS\n[35]"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "I\nand the k-th utterance\ncorresponding to the k-\nembedding Uk,",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "datasets, with\na\ntotal\nof\n80.6\nhours\nof\nspeech. We\nselected\ndata"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "th\nin\nthe\ndialogue\nhistory, where k ‚àà [0, c]. These\nutterance uk",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "corresponding\nto\nseven\nemotion\ncategories\n(happy,\nsad,\nsurprise,"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "embeddings are generated from the EQ-former pre-trained in the first",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "angry,\nfear, disgust,\nand neutral)\nand three\nintensity levels\n(weak,"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "stage and the LLM‚Äôs tokenizer\nfor\nthe LLM‚Äôs input, as illustrated in",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "medium, and strong) available in each dataset. For the second stage,"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "Fig. 2 (a) and (b).",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "we use the DailyDialog [30]\ntext dataset with 13,118 dialogues and"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "Each utterance embedding Uk is formed by concatenating the pre-",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "the DailyTalk [29] dataset, which contains 20 hours of speech across"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "fix embedding Pk, emotion embedding ÀÜEk, and transcript embedding",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "2,541 dialogues derived from DailyDialog. We added three intensity"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "along\nthe\ntemporal\ndimension. The\nprefix\nand\nTk\nembedding Pk",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "levels from ECSS [21] to the DailyTalk dataset, which originally did"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "transcript\nare\ntext\nembeddings\ngenerated\nfrom the\nembedding Tk",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "not\ninclude intensity labels. All audio samples are re-sampled at 16"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "and the ground-truth transcript\nusing the LLM‚Äôs\nprefix text pk\ntk",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "kHz during both the text-emotion alignment and emotional context"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "tokenizer. The prefix text pk indicates the order in which speaker sk is",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "reasoning stages. To train the third stage, we use the DailyTalk [29]"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "speaking. We utilize the Whisper decoder during inference to generate",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "ÀÜ",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "dataset. The audio is converted to Mel-spectrogram using short-time-"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "from speech. The emotion embedding\nTk\nEk, which represents the",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "fourier\ntransform with an FFT size of 1024, a hop size of 256, and"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "recognized emotional state of\nthe k-th utterance,\nis derived from the",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "a window size of 1024,\napplying an 80-bins mel filter. All\naudio"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "EQ-former. For generating the current utterance embedding Uc, only",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "samples\nin this\nstage\nare down-sampled at 22.05 kHz\nfor\nspeech"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "are used. The instruction embedding I, generated from\nPc\nand Tc",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "synthesis,\nand the data was\nsplit\ninto training, validation,\nand test"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "the text\ninstruction prompt by using the LLM‚Äôs tokenizer, guides the",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "sets with a ratio of 8:1:1."
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "model\nto predict\nthe emotion and intensity for\nthe current utterance,",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "based on U0:c and I.",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "B.\nImplementation Details"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "To\naddress\nthe\nscarcity\nof\nemotional\nconversation\ndatasets\nfor",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "emotional\ncontext\nreasoning, we\nadopt\na\npre-training\nstrategy\nin",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "We use the official pre-trained Whisper Large v3 [26] and Vicuna-"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "the\nsecond stage. This\nstrategy focuses on training the PLoRA-T",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "7B LLM [36], which is a LLaMA model\n[37] fine-tuned to follow"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "module using only textual data,\nfrom\nincluding emotion labels ek,",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "instructions\nusing\nthe Vicuna\ninstruction\nset.\nFor Q-former, we"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "DailyTalk [29] and DailyDialog [30]. In this pre-training, the emotion",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "employ 25 trainable query tokens with a dimension of 768. Multiple"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "embedding Ek\nis derived from the ground-truth emotion label ek",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "PLoRA adapters are injected into the projection layers for all queries"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "in text\nform and converted into a text embedding using the LLM‚Äôs",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "and values in every LLaMA self-attention layer, with a rank of 8 and"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "tokenizer. By deriving emotion embeddings from text,\nthe PLoRA-T",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "a scaling factor of 4.0 for both modules. In the first and second stages,"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "module can be trained on large-scale textual datasets, allowing the",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "we apply the AdamW optimizer\n[38] with Œ≤1 = 0.9, Œ≤2 = 0.999,"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "LLM to better\ninfer emotional context\nin conversations. During this",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "weight decay of 0.05, and a cosine learning rate decay, peaking at"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "pre-training process, we train only the PLoRA-T adapter. We then",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "3 √ó 10‚àí5 with a linear warmup of 3k steps and a minimum learning"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "fine-tune the LLM on the DailyTalk dataset\nto predict\nthe emotional",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "rate of 1 √ó 10‚àí5. During training, only the parameters of\nthe EQ-"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "state of target utterance,\ninitializing it with the pre-trained PLoRA-T",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "former and the PLoRA modules are updated, while the TLTR module"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "module from this pre-training stage, along with the PLoRA-E module",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "is frozen in the second stage. In the third stage, the AdamW optimizer"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "and EQ-former\ntrained in the first\nstage. This\napproach alleviates",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "settings are Œ≤1 = 0.9, Œ≤2 = 0.98, and HiFi-GAN [28] is used as the"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "the\nlimitation\nof\ninsufficient\nemotional\nconversation\ndatasets\nand",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "vocoder. All models are trained for 180k steps in the first stage, 30k"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "enhances the model‚Äôs ability for emotional context\nreasoning.",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "steps in the second stage, and 275k steps in the third stage, using 4"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "3) Emotional Context-Aware\nSpeech\nSynthesis:\nThe\nemotional",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "NVIDIA RTX A6000 GPUs."
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "context-aware speech synthesis\nstage generates\nspeech with appro-",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "C. Evaluation Metrics"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "priate emotional expressions based on the inferred emotional\nstate.",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": ""
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "We use FastSpeech 2 [2] as\nthe backbone model,\nincorporating an",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "We conduct\ntwo mean opinion score (MOS)\ntests\nto subjectively"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "emotion encoder and an intensity encoder\nto generate target speech",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "evaluate\nthe\nnaturalness\nin\ndialogue\ncontext\n(N-DMOS)\nand\nthe"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "that appropriately reflects the predicted emotion and intensity based",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "alignment\nof\nemotional\nexpression with\nthe\ndialogue‚Äôs\nemotional"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "on\nthe\nemotional\ncontext,\nas\ndepicted\nin\nthe\ntop-right\npanel\nof",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "context\n(E-DMOS). For MOS evaluation, we use Amazon MTurk"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "Fig. 2. The models\nfor\nthe\nemotional\ncontext\nreasoning stage\nand",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "to crowdsource 20 listeners, and the results are reported with a 95%"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "emotional context-aware speech synthesis are trained separately and",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "confidence interval. To evaluate whether synthesized speech contains"
        },
        {
          "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01": "concatenated during inference.",
          "3.059\n28.95\n23.48\n0.4759\n0.7292\n0.2194": "appropriate emotional expressions, we use the emotion2vec plus large"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "SUBJECTIVE AND OBJECTIVE EVALUATION RESULTS. THE N-DMOS AND E-DMOS SCORES ARE PRESENTED WITH 95% CONFIDENCE INTERVALS."
        },
        {
          "TABLE I": "Method\nN-DMOS (‚Üë)\nE-DMOS (‚Üë)\nECA (‚Üë)\nWER (‚Üì)"
        },
        {
          "TABLE I": "GT\n3.900 ¬± 0.045\n4.063 ¬± 0.039\n56.16\n5.72"
        },
        {
          "TABLE I": "Vocoded [28]\n3.871 ¬± 0.043\n4.013 ¬± 0.033\n57.05\n5.73"
        },
        {
          "TABLE I": "FastSpeech 2 [2]\n3.788 ¬± 0.045\n3.910 ¬± 0.043\n56.38\n6.06"
        },
        {
          "TABLE I": "GRU-based [6]\n3.792 ¬± 0.043\n3.944 ¬± 0.034\n56.38\n5.91"
        },
        {
          "TABLE I": "5.78\n3.802 ¬± 0.045\n3.914 ¬± 0.038\n55.72\nECSS [21]"
        },
        {
          "TABLE I": "3.847 ¬± 0.042\n3.987 ¬± 0.028\n58.60\nJELLY\n5.88"
        },
        {
          "TABLE I": "JELLY (speech-only)\n3.790 ¬± 0.043\n3.983 ¬± 0.027\n57.05\n6.01"
        },
        {
          "TABLE I": "emotion label ek. We fine-tune the LLM with multiple PLoRA mod-"
        },
        {
          "TABLE I": "ules to predict\nthe appropriate emotional state of the target utterance"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "uc within the given dialogue history, where c ‚àà [2, N ] represents the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "current\nturn. For this process, we generate the instruction embedding"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "I\nand the k-th utterance\ncorresponding to the k-\nembedding Uk,"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "th\nin\nthe\ndialogue\nhistory, where k ‚àà [0, c]. These\nutterance uk"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "embeddings are generated from the EQ-former pre-trained in the first"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "stage and the LLM‚Äôs tokenizer\nfor\nthe LLM‚Äôs input, as illustrated in"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Fig. 2 (a) and (b)."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Each utterance embedding Uk is formed by concatenating the pre-"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "fix embedding Pk, emotion embedding ÀÜEk, and transcript embedding"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "along\nthe\ntemporal\ndimension. The\nprefix\nand\nTk\nembedding Pk"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "transcript\nare\ntext\nembeddings\ngenerated\nfrom the\nembedding Tk"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "and the ground-truth transcript\nusing the LLM‚Äôs\nprefix text pk\ntk"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "tokenizer. The prefix text pk indicates the order in which speaker sk is"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "speaking. We utilize the Whisper decoder during inference to generate"
        },
        {
          "TABLE I": "ÀÜ"
        },
        {
          "TABLE I": "from speech. The emotion embedding\nTk\nEk, which represents the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "recognized emotional state of\nthe k-th utterance,\nis derived from the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "EQ-former. For generating the current utterance embedding Uc, only"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "are used. The instruction embedding I, generated from\nPc\nand Tc"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "the text\ninstruction prompt by using the LLM‚Äôs tokenizer, guides the"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "model\nto predict\nthe emotion and intensity for\nthe current utterance,"
        },
        {
          "TABLE I": "based on U0:c and I."
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "To\naddress\nthe\nscarcity\nof\nemotional\nconversation\ndatasets\nfor"
        },
        {
          "TABLE I": "emotional\ncontext\nreasoning, we\nadopt\na\npre-training\nstrategy\nin"
        },
        {
          "TABLE I": "the\nsecond stage. This\nstrategy focuses on training the PLoRA-T"
        },
        {
          "TABLE I": "module using only textual data,\nfrom\nincluding emotion labels ek,"
        },
        {
          "TABLE I": "DailyTalk [29] and DailyDialog [30]. In this pre-training, the emotion"
        },
        {
          "TABLE I": "embedding Ek\nis derived from the ground-truth emotion label ek"
        },
        {
          "TABLE I": "in text\nform and converted into a text embedding using the LLM‚Äôs"
        },
        {
          "TABLE I": "tokenizer. By deriving emotion embeddings from text,\nthe PLoRA-T"
        },
        {
          "TABLE I": "module can be trained on large-scale textual datasets, allowing the"
        },
        {
          "TABLE I": "LLM to better\ninfer emotional context\nin conversations. During this"
        },
        {
          "TABLE I": "pre-training process, we train only the PLoRA-T adapter. We then"
        },
        {
          "TABLE I": "fine-tune the LLM on the DailyTalk dataset\nto predict\nthe emotional"
        },
        {
          "TABLE I": "state of target utterance,\ninitializing it with the pre-trained PLoRA-T"
        },
        {
          "TABLE I": "module from this pre-training stage, along with the PLoRA-E module"
        },
        {
          "TABLE I": "and EQ-former\ntrained in the first\nstage. This\napproach alleviates"
        },
        {
          "TABLE I": "the\nlimitation\nof\ninsufficient\nemotional\nconversation\ndatasets\nand"
        },
        {
          "TABLE I": "enhances the model‚Äôs ability for emotional context\nreasoning."
        },
        {
          "TABLE I": "3) Emotional Context-Aware\nSpeech\nSynthesis:\nThe\nemotional"
        },
        {
          "TABLE I": "context-aware speech synthesis\nstage generates\nspeech with appro-"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "priate emotional expressions based on the inferred emotional\nstate."
        },
        {
          "TABLE I": "We use FastSpeech 2 [2] as\nthe backbone model,\nincorporating an"
        },
        {
          "TABLE I": "emotion encoder and an intensity encoder\nto generate target speech"
        },
        {
          "TABLE I": "that appropriately reflects the predicted emotion and intensity based"
        },
        {
          "TABLE I": "on\nthe\nemotional\ncontext,\nas\ndepicted\nin\nthe\ntop-right\npanel\nof"
        },
        {
          "TABLE I": "Fig. 2. The models\nfor\nthe\nemotional\ncontext\nreasoning stage\nand"
        },
        {
          "TABLE I": "emotional context-aware speech synthesis are trained separately and"
        },
        {
          "TABLE I": "concatenated during inference."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "EVALUATION AND ABLATION STUDY RESULTS FOR PREDICTION OF THE",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "We\nconducted\ncomparative\nexperiments\non WA, UA,\nand\nF1."
        },
        {
          "TABLE II": "EMOTION AND INTENSITY IN EMOTIONAL CONTEXT REASONING STAGE.",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "ECSS [21] was used as\nthe baseline, and its emotion and intensity"
        },
        {
          "TABLE II": "Emotion\nIntensity",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "predictors were employed for comparison with our framework. Table"
        },
        {
          "TABLE II": "ECA\nWA (‚Üë)\nUA (‚Üë)\nF1 (‚Üë)\nWA (‚Üë)\nUA (‚Üë)\nF1 (‚Üë)\nMethod",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "II presents\nthe results, demonstrating that\nJELLY outperformed the"
        },
        {
          "TABLE II": "ECSS [21]\n55.72\n43.51\n15.06\n13.66\n60.38\n33.33\n25.10",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "58.60\n78.54\n77.21\n52.78\nJELLY\n59.09\n60.17\n51.61",
          "B. Emotional Context Reasoning": "baseline across all metrics for both emotion and intensity predictions."
        },
        {
          "TABLE II": "60.14\nw/o TLTR\n57.71\n77.77\n59.81\n75.88\n52.09\n50.87",
          "B. Emotional Context Reasoning": "Although ECSS exhibited\ninaccuracies\nin\nprediction,\nthe\nprosody"
        },
        {
          "TABLE II": "w/o Q-former\n55.38\n47.01\n14.81\n10.43\n59.62\n36.10\n32.46",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "62.52\nw/o PLoRA modules\n57.05\n77.77\n59.24\n76.33\n51.71\n50.83",
          "B. Emotional Context Reasoning": "predictor in ECSS, which predicts prosody based on previous conver-"
        },
        {
          "TABLE II": "w/o stage 1\n57.38\n76.99\n57.60\n58.12\n74.00\n52.03\n49.80",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "sations, appears to compensate for these errors, as reflected in the high"
        },
        {
          "TABLE II": "53.03\nw/o PT in stage 2\n56.60\n67.15\n24.73\n25.73\n74.89\n51.55",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "results shown in Table I. JELLY infers\nN-DMOS and low RMSEf 0"
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "emotional context from recognized emotions using the proposed EQ-"
        },
        {
          "TABLE II": "model\n[39],\na\nspeech emotion recognition foundation model fine-",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "former, even without ground truth emotion labels,\nleading to more"
        },
        {
          "TABLE II": "tuned on approximately 42,000 hours of emotional data,\nto measure",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "accurate predictions of\nthe target utterance‚Äôs emotional state."
        },
        {
          "TABLE II": "emotional classification accuracy (ECA). To evaluate pronunciation",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "C. Ablation Study"
        },
        {
          "TABLE II": "accuracy, we calculate the word error rate (WER) and phoneme error",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "rate (PER) using Whisper [26] and wav2vec 2.0 [40], respectively. To",
          "B. Emotional Context Reasoning": "As detailed in Table II, we conducted ablation studies\nto verify"
        },
        {
          "TABLE II": "evaluate prosody, we compute the pitch error (RMSEf 0), periodicity",
          "B. Emotional Context Reasoning": "the\nperformance\nof\neach module\nin\nour\nproposed method. We"
        },
        {
          "TABLE II": "error\n(RMSEp), F1 score of voiced/unvoiced classification (F1v/uv),",
          "B. Emotional Context Reasoning": "assessed the\nimpact of TLTR by replacing it with the\nlast hidden"
        },
        {
          "TABLE II": "and the Mel-cepstral distortion (MCD). We conducted a duration pre-",
          "B. Emotional Context Reasoning": "layer\nrepresentations of Whisper encoder, which led to performance"
        },
        {
          "TABLE II": "diction performance evaluation using the average absolute differences",
          "B. Emotional Context Reasoning": "declines\nin ECA, WA, and F1 macro in the emotion category and"
        },
        {
          "TABLE II": "of\nthe utterance duration (DDUR)\n[41]. To evaluate\nthe\nemotional",
          "B. Emotional Context Reasoning": "across\nall\nintensity metrics. This\nemphasizes\nthe\nrole of TLTR in"
        },
        {
          "TABLE II": "context reasoning, we calculate the accuracy of emotion and intensity",
          "B. Emotional Context Reasoning": "extracting emotion features,\nincluding detailed information such as"
        },
        {
          "TABLE II": "predictions using the weighted accuracy (WA), unweighted average",
          "B. Emotional Context Reasoning": "emotion intensity, and improving emotional context\nreasoning."
        },
        {
          "TABLE II": "accuracy (UA), and macro F1 score (F1). These values are reported",
          "B. Emotional Context Reasoning": "Next,\nremoving Q-former\nand directly feeding emotion features"
        },
        {
          "TABLE II": "as percentages.",
          "B. Emotional Context Reasoning": "from TLTR into\nthe LLM through\na\nprojection\nlayer\nled\nto\na"
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "significant performance drop across all metrics, confirming that the Q-"
        },
        {
          "TABLE II": "IV. RESULTS",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "former plays a crucial role in bridging the modality gap between text"
        },
        {
          "TABLE II": "We compared our JELLY framework with other CSS frameworks,",
          "B. Emotional Context Reasoning": "and emotion, which is essential\nfor\nthe emotional context\nreasoning."
        },
        {
          "TABLE II": "including ECSS [21]\nand a GRU-based approach [6]\nas baselines.",
          "B. Emotional Context Reasoning": "When we replaced the multiple PLoRA modules with the standard"
        },
        {
          "TABLE II": "For ECSS, we\nused\nthe\nofficial\nimplementation\nfor\ntraining. To",
          "B. Emotional Context Reasoning": "LoRA adapter\n[42],\nthe decline in ECA and intensity metrics shows"
        },
        {
          "TABLE II": "verify that JELLY infers emotional context and generates contextually",
          "B. Emotional Context Reasoning": "that\nthe multiple PLoRA modules contribute to more effective mod-"
        },
        {
          "TABLE II": "appropriate\nspeech\nfrom speech\nalone, we\nalso\nevaluated\nJELLY",
          "B. Emotional Context Reasoning": "eling of intensity, ensuring alignment with the conversational context."
        },
        {
          "TABLE II": "(speech-only), which performs inference using only speech through",
          "B. Emotional Context Reasoning": "Additionally, we conducted an experiment where emotional\ninfer-"
        },
        {
          "TABLE II": "the EQ-former and Whisper. Additionally, we compared it with our",
          "B. Emotional Context Reasoning": "ence was performed directly in the\nsecond stage, without\nthe first"
        },
        {
          "TABLE II": "TTS backbone model, FastSpeech 2 [2], without context modeling.",
          "B. Emotional Context Reasoning": "stage. The decreases across all metrics indicate that EQ-former pre-"
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "trained in the first\nstage is effective for\ninferring emotional context"
        },
        {
          "TABLE II": "A. Emotional Context-Aware Speech Synthesis",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "and demonstrates the benefits of the value of using relatively available"
        },
        {
          "TABLE II": "We conducted both subjective and objective evaluations to assess",
          "B. Emotional Context Reasoning": "emotional speech data."
        },
        {
          "TABLE II": "whether\nthe\ngenerated\nspeech\nappropriately\nreflects\nthe\nemotional",
          "B. Emotional Context Reasoning": "Finally,\nskipping\nthe\npre-training\nof\nthe\nPLoRA-T module\nin"
        },
        {
          "TABLE II": "context. Table\nI\nshows\nthat\nJELLY outperforms\nthe other baseline",
          "B. Emotional Context Reasoning": "the second stage also led to lower performance across all metrics,"
        },
        {
          "TABLE II": "models across nearly all metrics, particularly in terms of N-DMOS,",
          "B. Emotional Context Reasoning": "highlighting the importance of pre-training with text data to address"
        },
        {
          "TABLE II": "E-DMOS, ECA, MCD,\nand DDUR. The high E-DMOS and ECA",
          "B. Emotional Context Reasoning": "the\nscarcity of\nemotional\nconversational datasets\nand enhance\nthe"
        },
        {
          "TABLE II": "scores\nvalidate\nthat\nour\nframework\ngenerates\nnatural\nspeech with",
          "B. Emotional Context Reasoning": "LLM‚Äôs ability to reason the emotional context."
        },
        {
          "TABLE II": "emotional\nexpressions\nclosely aligned with the\nemotional\ncontext.",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "V. CONCLUSION"
        },
        {
          "TABLE II": "Specifically, the results show that JELLY, unlike other CSS baselines,",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "effectively understands\nthe emotional context and generates\nspeech",
          "B. Emotional Context Reasoning": "In this paper, we proposed JELLY, a novel CSS framework that"
        },
        {
          "TABLE II": "that\nis appropriate for\nthe emotional context.",
          "B. Emotional Context Reasoning": "integrates\njoint\nemotion recognition and context\nreasoning with a"
        },
        {
          "TABLE II": "The\nlow WER and PER scores\nindicate\nthat\nJELLY generates",
          "B. Emotional Context Reasoning": "large language model. We introduced the EQ-former as an emotion"
        },
        {
          "TABLE II": "speech with reasonably accurate pronunciation, and the low DDUR",
          "B. Emotional Context Reasoning": "perception module for emotion-text alignment and designed a three-"
        },
        {
          "TABLE II": "score highlights JELLY‚Äôs ability to model duration within the emo-",
          "B. Emotional Context Reasoning": "stage\nlearning pipeline\nto mitigate\nthe\nscarcity of\nemotional\ncon-"
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "versational datasets. Experimental\nresults demonstrate\nthat\nJELLY"
        },
        {
          "TABLE II": "rates in terms of RMSEp and MCD, along",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "generates emotionally aligned and natural\nspeech in conversational"
        },
        {
          "TABLE II": "with the high scores in N-DMOS and F1v/uv, demonstrate that our",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "framework models prosody appropriate for conversational contexts.",
          "B. Emotional Context Reasoning": "contexts, outperforming existing models‚Äîeven without\ntranscripts or"
        },
        {
          "TABLE II": "JELLY (speech-only)\nalso demonstrates\nthat our\nframework can",
          "B. Emotional Context Reasoning": "emotion labels. By leveraging the reasoning capabilities of the LLM"
        },
        {
          "TABLE II": "generate emotionally context-aware speech without\nrelying on tran-",
          "B. Emotional Context Reasoning": "through multiple LoRA modules and a distinct pre-training strategy"
        },
        {
          "TABLE II": "scripts or emotion labels during inference, as reflected in its high E-",
          "B. Emotional Context Reasoning": "in the first\nand second stages,\nJELLY improves predictions of\nthe"
        },
        {
          "TABLE II": "DMOS and ECA scores. Notably, JELLY (speech-only) outperforms",
          "B. Emotional Context Reasoning": "target speaker‚Äôs emotional states and synthesizes speech that\nis more"
        },
        {
          "TABLE II": "",
          "B. Emotional Context Reasoning": "appropriate for\nthe emotional context. However, challenges\nremain"
        },
        {
          "TABLE II": "the comparison models in MCD, DDUR, and F1v/uv, further showing",
          "B. Emotional Context Reasoning": ""
        },
        {
          "TABLE II": "that our\nframework,\nthrough its outstanding emotional context\nrea-",
          "B. Emotional Context Reasoning": "in bridging the gap between CSS and real-world scenarios. In future"
        },
        {
          "TABLE II": "soning, generates speech with prosody well-suited to conversational",
          "B. Emotional Context Reasoning": "work, we plan to address this by considering overlapped speech and"
        },
        {
          "TABLE II": "contexts using only speech.",
          "B. Emotional Context Reasoning": "larger speaker groups."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "SPEECH), 2024."
        },
        {
          "REFERENCES": "[1]\nJonatha Shen et al.,\n‚ÄúNatural TTS Synthesis by Conditioning Wavenet",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[20] Hyoung-Seok Oh, Sang-Hoon Lee, Deok-Hyun Cho, and Seong-Whan"
        },
        {
          "REFERENCES": "on Mel Spectrogram Predictions,‚Äù\nin IEEE Int. Conf. Acoust. Speech",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Lee,\n‚ÄúDurFlex-EVC: Duration-Flexible Emotional Voice Conversion"
        },
        {
          "REFERENCES": "Signal Process.\n(ICASSP), 2018.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "with Parallel Generation,‚Äù arXiv preprint arXiv:2401.08095, 2024."
        },
        {
          "REFERENCES": "[2] Yi Ren et al.,\n‚ÄúFastSpeech 2: Fast and High-Quality End-to-End Text",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[21] Rui Liu, Yifan Hu, Yi Ren, Xiang Yin,\nand Haizhou Li,\n‚ÄúEmotion"
        },
        {
          "REFERENCES": "Int. Conf. Learn. Represent.\nto Speech,‚Äù\nin Proc.\n(ICLR), 2021.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "rendering for conversational speech synthesis with heterogeneous graph-"
        },
        {
          "REFERENCES": "[3]\nSang-Hoon Lee\net\nal.,\n‚ÄúHierSpeech: Bridging the Gap between Text",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "based context modeling,‚Äù in Proc. AAAI Conf. Artif. Intell. (AAAI), 2024."
        },
        {
          "REFERENCES": "and Speech by Hierarchical Variational\nInference using Self-supervised",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[22] Xiaoyi Dong et al.,\n‚ÄúInternlm-xcomposer2: Mastering free-form text-"
        },
        {
          "REFERENCES": "Representations for Speech Synthesis,‚Äù in Adv. Neural Inf. Process. Syst.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "image composition and comprehension in vision-language large model,‚Äù"
        },
        {
          "REFERENCES": "(NeurIPS), 2022.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv preprint arXiv:2401.16420, 2024."
        },
        {
          "REFERENCES": "¬¥\n[4]\nShivam Mehta, Ruibo Tu, Jonas Beskow,\nEva Sz¬¥ekely, and Gustav Eje",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[23] Chen Wang, Minpeng Liao, Zhongqiang Huang,\nand\nJiajun Zhang,"
        },
        {
          "REFERENCES": "Henter,\n‚ÄúMatcha-TTS: A fast TTS Architecture with Conditional Flow",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "‚ÄúBLSP-KD: Bootstrapping Language-Speech Pre-training\nvia Knowl-"
        },
        {
          "REFERENCES": "Matching,‚Äù in IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP),",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "edge Distillation,‚Äù arXiv preprint arXiv:2405.19041, 2024."
        },
        {
          "REFERENCES": "2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[24] Yuan Gong,\nSameer Khurana, Leonid Karlinsky,\nand\nJames Glass,"
        },
        {
          "REFERENCES": "[5] Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu,\n‚ÄúVoice-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "‚ÄúWhisper-AT: Noise-Robust Automatic Speech Recognizers\nare Also"
        },
        {
          "REFERENCES": "Flow: Efficient Text-To-Speech with Rectified Flow Matching,‚Äù in IEEE",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int.\nSpeech\nStrong General Audio Event Taggers,‚Äù\nin Ann. Conf."
        },
        {
          "REFERENCES": "Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Commun. Assoc.\n(INTERSPEECH), 2023."
        },
        {
          "REFERENCES": "[6] Haohan Guo, Shaofei Zhang, Frank K. Soong, Lei He,\nand Lei Xie,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[25]\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, ‚ÄúBLIP-2: Boot-"
        },
        {
          "REFERENCES": "‚ÄúConversational End-to-End TTS for Voice Agents,‚Äù in IEEE Workshop",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "strapping Language-Image Pre-training with Frozen Image Encoders and"
        },
        {
          "REFERENCES": "Spok. Lang. Technol.\n(SLT), 2021.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Mach. Learn.\nLarge Language Models,‚Äù\nin Proc.\n(ICML),"
        },
        {
          "REFERENCES": "[7]\nJingbei Li et al., ‚ÄúEnhancing Speaking Styles in Conversational Text-to-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2023."
        },
        {
          "REFERENCES": "Speech Synthesis with Graph-Based Multi-Modal Context Modeling,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[26] Alec Radford et al., ‚ÄúRobust Speech Recognition via Large-Scale Weak"
        },
        {
          "REFERENCES": "in IEEE Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2022.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Mach. Learn.\nSupervision,‚Äù\nin Proc.\n(ICML), 2023."
        },
        {
          "REFERENCES": "[8]\nJinlong Xue et al.,\n‚ÄúM2-CTTS: End-to-End Multi-Scale Multi-Modal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[27] Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky,\nand"
        },
        {
          "REFERENCES": "Conversational Text-to-Speech Synthesis,‚Äù\nin IEEE Int. Conf. Acoust.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "James Glass, ‚ÄúJoint Audio and Speech Understanding,‚Äù in IEEE Autom."
        },
        {
          "REFERENCES": "Speech Signal Process.\n(ICASSP), 2023.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Speech Recognit. Underst. Workshop (ASRU), 2023."
        },
        {
          "REFERENCES": "[9] Yayue Deng et al., ‚ÄúConcss: Contrastive-based Context Comprehension",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[28]\nJungil Kong,\nJaehyeon Kim,\nand Jaekyoung Bae,\n‚ÄúHiFi-GAN: Gen-"
        },
        {
          "REFERENCES": "for Dialogue-Appropriate Prosody in Conversational Speech Synthesis,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "erative Adversarial Networks\nfor Efficient\nand High Fidelity Speech"
        },
        {
          "REFERENCES": "in IEEE Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Inf. Process. Syst.\nSynthesis,‚Äù\nin Adv. Neural\n(NeurIPS), 2020."
        },
        {
          "REFERENCES": "[10] Kangdi Mei et al.,\n‚ÄúConsidering Temporal Connection between Turns",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[29] Keon Lee, Kyumin Park,\nand Daeyoung Kim,\n‚ÄúDailyTalk: Spoken"
        },
        {
          "REFERENCES": "for Conversational Speech Synthesis,‚Äù in IEEE Int. Conf. Acoust. Speech",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dialogue Dataset for Conversational Text-to-Speech,‚Äù in IEEE Int. Conf."
        },
        {
          "REFERENCES": "Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Acoust. Speech Signal Process.\n(ICASSP), 2023."
        },
        {
          "REFERENCES": "[11]\nSeong-Whan Lee and Hee-Heon Song, ‚ÄúA new recurrent neural-network",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[30] Yanran\nLi\net\nal.,\n‚ÄúDailyDialog: A Manually\nLabelled Multi-turn"
        },
        {
          "REFERENCES": "architecture for visual pattern recognition,‚Äù IEEE Transactions on Neural",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dialogue Dataset,‚Äù in Int. Jt. Conf. Nat. Lang. Process. (IJCNLP), 2017."
        },
        {
          "REFERENCES": "Networks, vol. 8, pp. 331‚Äì340, 1997.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[31] Houwei Cao et al., ‚ÄúCREMA-D: Crowd-Sourced Emotional Multimodal"
        },
        {
          "REFERENCES": "[12]\nJi-Hoon Jeong, Baek-Woon Yu, Dae-Hyeok Lee, and Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Actors Dataset,‚Äù IEEE Trans. Affect. Comput., vol. 5, pp. 377‚Äì390, 2014."
        },
        {
          "REFERENCES": "‚ÄúClassification of drowsiness\nlevels based on a deep spatio-temporal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[32] Adaeze Adigwe, No¬¥e Tits, Kevin El Haddad, Sarah Ostadabbas,\nand"
        },
        {
          "REFERENCES": "convolutional bidirectional LSTM network using electroencephalogra-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Thierry Dutoit,\n‚ÄúThe emotional voices database: Towards controlling"
        },
        {
          "REFERENCES": "phy signals,‚Äù Brain Sci., vol. 9, pp. 348, 2019.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv preprint\nthe\nemotion dimension in voice generation systems,‚Äù"
        },
        {
          "REFERENCES": "[13]\nSeo-Hyun\nLee, Minji\nLee,\nJi-Hoon\nJeong,\nand\nSeong-Whan\nLee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv:1806.09514, 2018."
        },
        {
          "REFERENCES": "‚ÄúTowards\nan EEG-based\nintuitive BCI\ncommunication\nsystem using",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[33] Carlos Busso et al.,\n‚ÄúIEMOCAP:\nInteractive emotional dyadic motion"
        },
        {
          "REFERENCES": "IEEE Int. Conf. Syst.\nimagined speech and visual\nimagery,‚Äù\nin Proc.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "capture database,‚Äù Lang. Resour. Eval., vol. 42, pp. 335‚Äì359, 2008."
        },
        {
          "REFERENCES": "Man Cybern.\n(SMC), 2019.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[34] Kaisiyuan Wang et al., ‚ÄúMEAD: A Large-scale Audio-visual Dataset for"
        },
        {
          "REFERENCES": "[14] Ravikiran Mane, Neethu Robinson, A Prasad Vinod, Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Emotional Talking-face Generation,‚Äù in Eur. Conf. Comput. Vis. (ECCV),"
        },
        {
          "REFERENCES": "and Cuntai Guan, ‚ÄúA multi-view cnn with novel variance layer for motor",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2020."
        },
        {
          "REFERENCES": "Int. Conf.\nIEEE Eng. Med.\nimagery brain computer\ninterface,‚Äù\nin Proc.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[35] Kate Dupuis and M Kathleen Pichora-Fuller, ‚ÄúToronto emotional speech"
        },
        {
          "REFERENCES": "Biol. Soc.\n(EMBC), 2020.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "set\n(tess)-younger\ntalker happy,‚Äù 2010."
        },
        {
          "REFERENCES": "[15] Dong-Ok Won, Klaus-Robert M¬®uller,\nand\nSeong-Whan\nLee,\n‚ÄúAn",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[36] Wei-Lin Chiang et al.,\n‚ÄúVicuna: An Open-Source Chatbot\nImpressing"
        },
        {
          "REFERENCES": "adaptive deep reinforcement\nlearning framework enables curling robots",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "GPT-4 with 90%* ChatGPT Quality,‚Äù 2023."
        },
        {
          "REFERENCES": "with human-like performance in real-world conditions,‚Äù Sci. Robot., vol.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[37] Hugo Touvron et al.,\n‚ÄúLLama: Open and efficient\nfoundation language"
        },
        {
          "REFERENCES": "5, pp. eabb9764, 2020.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "models,‚Äù arXiv preprint arXiv:2302.13971, 2023."
        },
        {
          "REFERENCES": "[16]\nSang-Hoon Lee, Ha-Yeong Choi, and Seong-Whan Lee,\n‚ÄúPeriodwave:",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[38]\nIlya Loshchilov and Frank Hutter,\n‚ÄúDecoupled Weight Decay Regular-"
        },
        {
          "REFERENCES": "Multi-period\nflow matching\nfor\nhigh-fidelity waveform generation,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "ization,‚Äù\nin Int. Conf. Learn. Represent.\n(ICLR), 2019."
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2408.07547, 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[39] Ziyang Ma\net\nal.,\n‚Äúemotion2vec:\nSelf-Supervised\nPre-Training\nfor"
        },
        {
          "REFERENCES": "[17] Chae-Bin Im, Sang-Hoon Lee, Seung-Bin Kim, and Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Speech Emotion Representation,‚Äù\nin Annu. Meet. Assoc. Comput."
        },
        {
          "REFERENCES": "‚ÄúEmoq-tts: Emotion intensity quantization for fine-grained controllable",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Linguist.\n(ACL), 2024."
        },
        {
          "REFERENCES": "emotional\ntext-to-speech,‚Äù\nin IEEE Int. Conf. Acoust. Speech Signal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[40] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael"
        },
        {
          "REFERENCES": "Process.\n(ICASSP), 2022, pp. 6317‚Äì6321.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Auli,\n‚Äúwav2vec\n2.0: A Framework\nfor Self-Supervised Learning\nof"
        },
        {
          "REFERENCES": "[18] Hyung-Seok Oh, Sang-Hoon Lee, and Seong-Whan Lee, ‚ÄúDiffprosody:",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Inf. Process. Syst.\nSpeech Representations,‚Äù\nin Adv. Neural\n(NeurIPS),"
        },
        {
          "REFERENCES": "Diffusion-based latent prosody generation for expressive speech synthe-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2020."
        },
        {
          "REFERENCES": "IEEE/ACM Trans.\nsis with prosody conditional\nadversarial\ntraining,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[41]\nJing-Xuan Zhang, Zhen-Hua Ling, Li-Juan Liu, Yuan Jiang, and Li-Rong"
        },
        {
          "REFERENCES": "Audio, Speech, Lang. Process., vol. 32, pp. 2654‚Äì2666, 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dai, ‚ÄúSequence-to-Sequence Acoustic Modeling for Voice Conversion,‚Äù"
        },
        {
          "REFERENCES": "[19] Deok-Hyeon Cho, Hyung-Seok Oh, Seung-Bin Kim, Sang-Hoon Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, pp. 631‚Äì644,"
        },
        {
          "REFERENCES": "and Seong-Whan Lee, ‚ÄúEmoSphere-TTS: Emotional Style and Intensity",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2019."
        },
        {
          "REFERENCES": "Modeling\nvia\nSpherical Emotion Vector\nfor Controllable Emotional",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[42] Edward J Hu et al.,\n‚ÄúLoRA: Low-Rank Adaptation of Large Language"
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Learn. Represent.\nModels,‚Äù\nin Proc.\n(ICLR), 2022."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "SPEECH), 2024."
        },
        {
          "REFERENCES": "[1]\nJonatha Shen et al.,\n‚ÄúNatural TTS Synthesis by Conditioning Wavenet",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[20] Hyoung-Seok Oh, Sang-Hoon Lee, Deok-Hyun Cho, and Seong-Whan"
        },
        {
          "REFERENCES": "on Mel Spectrogram Predictions,‚Äù\nin IEEE Int. Conf. Acoust. Speech",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Lee,\n‚ÄúDurFlex-EVC: Duration-Flexible Emotional Voice Conversion"
        },
        {
          "REFERENCES": "Signal Process.\n(ICASSP), 2018.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "with Parallel Generation,‚Äù arXiv preprint arXiv:2401.08095, 2024."
        },
        {
          "REFERENCES": "[2] Yi Ren et al.,\n‚ÄúFastSpeech 2: Fast and High-Quality End-to-End Text",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[21] Rui Liu, Yifan Hu, Yi Ren, Xiang Yin,\nand Haizhou Li,\n‚ÄúEmotion"
        },
        {
          "REFERENCES": "Int. Conf. Learn. Represent.\nto Speech,‚Äù\nin Proc.\n(ICLR), 2021.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "rendering for conversational speech synthesis with heterogeneous graph-"
        },
        {
          "REFERENCES": "[3]\nSang-Hoon Lee\net\nal.,\n‚ÄúHierSpeech: Bridging the Gap between Text",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "based context modeling,‚Äù in Proc. AAAI Conf. Artif. Intell. (AAAI), 2024."
        },
        {
          "REFERENCES": "and Speech by Hierarchical Variational\nInference using Self-supervised",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[22] Xiaoyi Dong et al.,\n‚ÄúInternlm-xcomposer2: Mastering free-form text-"
        },
        {
          "REFERENCES": "Representations for Speech Synthesis,‚Äù in Adv. Neural Inf. Process. Syst.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "image composition and comprehension in vision-language large model,‚Äù"
        },
        {
          "REFERENCES": "(NeurIPS), 2022.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv preprint arXiv:2401.16420, 2024."
        },
        {
          "REFERENCES": "¬¥\n[4]\nShivam Mehta, Ruibo Tu, Jonas Beskow,\nEva Sz¬¥ekely, and Gustav Eje",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": ""
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[23] Chen Wang, Minpeng Liao, Zhongqiang Huang,\nand\nJiajun Zhang,"
        },
        {
          "REFERENCES": "Henter,\n‚ÄúMatcha-TTS: A fast TTS Architecture with Conditional Flow",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "‚ÄúBLSP-KD: Bootstrapping Language-Speech Pre-training\nvia Knowl-"
        },
        {
          "REFERENCES": "Matching,‚Äù in IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP),",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "edge Distillation,‚Äù arXiv preprint arXiv:2405.19041, 2024."
        },
        {
          "REFERENCES": "2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[24] Yuan Gong,\nSameer Khurana, Leonid Karlinsky,\nand\nJames Glass,"
        },
        {
          "REFERENCES": "[5] Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu,\n‚ÄúVoice-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "‚ÄúWhisper-AT: Noise-Robust Automatic Speech Recognizers\nare Also"
        },
        {
          "REFERENCES": "Flow: Efficient Text-To-Speech with Rectified Flow Matching,‚Äù in IEEE",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int.\nSpeech\nStrong General Audio Event Taggers,‚Äù\nin Ann. Conf."
        },
        {
          "REFERENCES": "Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Commun. Assoc.\n(INTERSPEECH), 2023."
        },
        {
          "REFERENCES": "[6] Haohan Guo, Shaofei Zhang, Frank K. Soong, Lei He,\nand Lei Xie,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[25]\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, ‚ÄúBLIP-2: Boot-"
        },
        {
          "REFERENCES": "‚ÄúConversational End-to-End TTS for Voice Agents,‚Äù in IEEE Workshop",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "strapping Language-Image Pre-training with Frozen Image Encoders and"
        },
        {
          "REFERENCES": "Spok. Lang. Technol.\n(SLT), 2021.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Mach. Learn.\nLarge Language Models,‚Äù\nin Proc.\n(ICML),"
        },
        {
          "REFERENCES": "[7]\nJingbei Li et al., ‚ÄúEnhancing Speaking Styles in Conversational Text-to-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2023."
        },
        {
          "REFERENCES": "Speech Synthesis with Graph-Based Multi-Modal Context Modeling,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[26] Alec Radford et al., ‚ÄúRobust Speech Recognition via Large-Scale Weak"
        },
        {
          "REFERENCES": "in IEEE Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2022.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Mach. Learn.\nSupervision,‚Äù\nin Proc.\n(ICML), 2023."
        },
        {
          "REFERENCES": "[8]\nJinlong Xue et al.,\n‚ÄúM2-CTTS: End-to-End Multi-Scale Multi-Modal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[27] Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky,\nand"
        },
        {
          "REFERENCES": "Conversational Text-to-Speech Synthesis,‚Äù\nin IEEE Int. Conf. Acoust.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "James Glass, ‚ÄúJoint Audio and Speech Understanding,‚Äù in IEEE Autom."
        },
        {
          "REFERENCES": "Speech Signal Process.\n(ICASSP), 2023.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Speech Recognit. Underst. Workshop (ASRU), 2023."
        },
        {
          "REFERENCES": "[9] Yayue Deng et al., ‚ÄúConcss: Contrastive-based Context Comprehension",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[28]\nJungil Kong,\nJaehyeon Kim,\nand Jaekyoung Bae,\n‚ÄúHiFi-GAN: Gen-"
        },
        {
          "REFERENCES": "for Dialogue-Appropriate Prosody in Conversational Speech Synthesis,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "erative Adversarial Networks\nfor Efficient\nand High Fidelity Speech"
        },
        {
          "REFERENCES": "in IEEE Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Inf. Process. Syst.\nSynthesis,‚Äù\nin Adv. Neural\n(NeurIPS), 2020."
        },
        {
          "REFERENCES": "[10] Kangdi Mei et al.,\n‚ÄúConsidering Temporal Connection between Turns",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[29] Keon Lee, Kyumin Park,\nand Daeyoung Kim,\n‚ÄúDailyTalk: Spoken"
        },
        {
          "REFERENCES": "for Conversational Speech Synthesis,‚Äù in IEEE Int. Conf. Acoust. Speech",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dialogue Dataset for Conversational Text-to-Speech,‚Äù in IEEE Int. Conf."
        },
        {
          "REFERENCES": "Signal Process.\n(ICASSP), 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Acoust. Speech Signal Process.\n(ICASSP), 2023."
        },
        {
          "REFERENCES": "[11]\nSeong-Whan Lee and Hee-Heon Song, ‚ÄúA new recurrent neural-network",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[30] Yanran\nLi\net\nal.,\n‚ÄúDailyDialog: A Manually\nLabelled Multi-turn"
        },
        {
          "REFERENCES": "architecture for visual pattern recognition,‚Äù IEEE Transactions on Neural",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dialogue Dataset,‚Äù in Int. Jt. Conf. Nat. Lang. Process. (IJCNLP), 2017."
        },
        {
          "REFERENCES": "Networks, vol. 8, pp. 331‚Äì340, 1997.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[31] Houwei Cao et al., ‚ÄúCREMA-D: Crowd-Sourced Emotional Multimodal"
        },
        {
          "REFERENCES": "[12]\nJi-Hoon Jeong, Baek-Woon Yu, Dae-Hyeok Lee, and Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Actors Dataset,‚Äù IEEE Trans. Affect. Comput., vol. 5, pp. 377‚Äì390, 2014."
        },
        {
          "REFERENCES": "‚ÄúClassification of drowsiness\nlevels based on a deep spatio-temporal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[32] Adaeze Adigwe, No¬¥e Tits, Kevin El Haddad, Sarah Ostadabbas,\nand"
        },
        {
          "REFERENCES": "convolutional bidirectional LSTM network using electroencephalogra-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Thierry Dutoit,\n‚ÄúThe emotional voices database: Towards controlling"
        },
        {
          "REFERENCES": "phy signals,‚Äù Brain Sci., vol. 9, pp. 348, 2019.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv preprint\nthe\nemotion dimension in voice generation systems,‚Äù"
        },
        {
          "REFERENCES": "[13]\nSeo-Hyun\nLee, Minji\nLee,\nJi-Hoon\nJeong,\nand\nSeong-Whan\nLee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "arXiv:1806.09514, 2018."
        },
        {
          "REFERENCES": "‚ÄúTowards\nan EEG-based\nintuitive BCI\ncommunication\nsystem using",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[33] Carlos Busso et al.,\n‚ÄúIEMOCAP:\nInteractive emotional dyadic motion"
        },
        {
          "REFERENCES": "IEEE Int. Conf. Syst.\nimagined speech and visual\nimagery,‚Äù\nin Proc.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "capture database,‚Äù Lang. Resour. Eval., vol. 42, pp. 335‚Äì359, 2008."
        },
        {
          "REFERENCES": "Man Cybern.\n(SMC), 2019.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[34] Kaisiyuan Wang et al., ‚ÄúMEAD: A Large-scale Audio-visual Dataset for"
        },
        {
          "REFERENCES": "[14] Ravikiran Mane, Neethu Robinson, A Prasad Vinod, Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Emotional Talking-face Generation,‚Äù in Eur. Conf. Comput. Vis. (ECCV),"
        },
        {
          "REFERENCES": "and Cuntai Guan, ‚ÄúA multi-view cnn with novel variance layer for motor",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2020."
        },
        {
          "REFERENCES": "Int. Conf.\nIEEE Eng. Med.\nimagery brain computer\ninterface,‚Äù\nin Proc.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[35] Kate Dupuis and M Kathleen Pichora-Fuller, ‚ÄúToronto emotional speech"
        },
        {
          "REFERENCES": "Biol. Soc.\n(EMBC), 2020.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "set\n(tess)-younger\ntalker happy,‚Äù 2010."
        },
        {
          "REFERENCES": "[15] Dong-Ok Won, Klaus-Robert M¬®uller,\nand\nSeong-Whan\nLee,\n‚ÄúAn",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[36] Wei-Lin Chiang et al.,\n‚ÄúVicuna: An Open-Source Chatbot\nImpressing"
        },
        {
          "REFERENCES": "adaptive deep reinforcement\nlearning framework enables curling robots",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "GPT-4 with 90%* ChatGPT Quality,‚Äù 2023."
        },
        {
          "REFERENCES": "with human-like performance in real-world conditions,‚Äù Sci. Robot., vol.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[37] Hugo Touvron et al.,\n‚ÄúLLama: Open and efficient\nfoundation language"
        },
        {
          "REFERENCES": "5, pp. eabb9764, 2020.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "models,‚Äù arXiv preprint arXiv:2302.13971, 2023."
        },
        {
          "REFERENCES": "[16]\nSang-Hoon Lee, Ha-Yeong Choi, and Seong-Whan Lee,\n‚ÄúPeriodwave:",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[38]\nIlya Loshchilov and Frank Hutter,\n‚ÄúDecoupled Weight Decay Regular-"
        },
        {
          "REFERENCES": "Multi-period\nflow matching\nfor\nhigh-fidelity waveform generation,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "ization,‚Äù\nin Int. Conf. Learn. Represent.\n(ICLR), 2019."
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2408.07547, 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[39] Ziyang Ma\net\nal.,\n‚Äúemotion2vec:\nSelf-Supervised\nPre-Training\nfor"
        },
        {
          "REFERENCES": "[17] Chae-Bin Im, Sang-Hoon Lee, Seung-Bin Kim, and Seong-Whan Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Speech Emotion Representation,‚Äù\nin Annu. Meet. Assoc. Comput."
        },
        {
          "REFERENCES": "‚ÄúEmoq-tts: Emotion intensity quantization for fine-grained controllable",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Linguist.\n(ACL), 2024."
        },
        {
          "REFERENCES": "emotional\ntext-to-speech,‚Äù\nin IEEE Int. Conf. Acoust. Speech Signal",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[40] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael"
        },
        {
          "REFERENCES": "Process.\n(ICASSP), 2022, pp. 6317‚Äì6321.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Auli,\n‚Äúwav2vec\n2.0: A Framework\nfor Self-Supervised Learning\nof"
        },
        {
          "REFERENCES": "[18] Hyung-Seok Oh, Sang-Hoon Lee, and Seong-Whan Lee, ‚ÄúDiffprosody:",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Inf. Process. Syst.\nSpeech Representations,‚Äù\nin Adv. Neural\n(NeurIPS),"
        },
        {
          "REFERENCES": "Diffusion-based latent prosody generation for expressive speech synthe-",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2020."
        },
        {
          "REFERENCES": "IEEE/ACM Trans.\nsis with prosody conditional\nadversarial\ntraining,‚Äù",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[41]\nJing-Xuan Zhang, Zhen-Hua Ling, Li-Juan Liu, Yuan Jiang, and Li-Rong"
        },
        {
          "REFERENCES": "Audio, Speech, Lang. Process., vol. 32, pp. 2654‚Äì2666, 2024.",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Dai, ‚ÄúSequence-to-Sequence Acoustic Modeling for Voice Conversion,‚Äù"
        },
        {
          "REFERENCES": "[19] Deok-Hyeon Cho, Hyung-Seok Oh, Seung-Bin Kim, Sang-Hoon Lee,",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, pp. 631‚Äì644,"
        },
        {
          "REFERENCES": "and Seong-Whan Lee, ‚ÄúEmoSphere-TTS: Emotional Style and Intensity",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "2019."
        },
        {
          "REFERENCES": "Modeling\nvia\nSpherical Emotion Vector\nfor Controllable Emotional",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "[42] Edward J Hu et al.,\n‚ÄúLoRA: Low-Rank Adaptation of Large Language"
        },
        {
          "REFERENCES": "",
          "Int. Speech Commun. Assoc.\n(INTER-\nText-to-Speech,‚Äù\nin Ann. Conf.": "Int. Conf. Learn. Represent.\nModels,‚Äù\nin Proc.\n(ICLR), 2022."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Natural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions",
      "authors": [
        "Jonatha Shen"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)"
    },
    {
      "citation_id": "2",
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
      "authors": [
        "Yi Ren"
      ],
      "venue": "Proc. Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "3",
      "title": "HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis",
      "authors": [
        "Sang-Hoon Lee"
      ],
      "venue": "Adv. Neural Inf. Process. Syst. (NeurIPS)"
    },
    {
      "citation_id": "4",
      "title": "Matcha-TTS: A fast TTS Architecture with Conditional Flow Matching",
      "authors": [
        "Shivam Mehta",
        "Ruibo Tu",
        "Jonas Beskow",
        "√âva Sz√©kely",
        "Gustav Henter"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "5",
      "title": "Voice-Flow: Efficient Text-To-Speech with Rectified Flow Matching",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Ziyang Ma",
        "Xie Chen",
        "Kai Yu"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "6",
      "title": "Conversational End-to-End TTS for Voice Agents",
      "authors": [
        "Haohan Guo",
        "Shaofei Zhang",
        "Frank Soong",
        "Lei He",
        "Lei Xie"
      ],
      "venue": "IEEE Workshop Spok. Lang. Technol. (SLT)"
    },
    {
      "citation_id": "7",
      "title": "Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-Based Multi-Modal Context Modeling",
      "authors": [
        "Jingbei Li"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "8",
      "title": "M2-CTTS: End-to-End Multi-Scale Multi-Modal Conversational Text-to-Speech Synthesis",
      "authors": [
        "Jinlong Xue"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "9",
      "title": "Concss: Contrastive-based Context Comprehension for Dialogue-Appropriate Prosody in Conversational Speech Synthesis",
      "authors": [
        "Yayue Deng"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "10",
      "title": "Considering Temporal Connection between Turns for Conversational Speech Synthesis",
      "authors": [
        "Kangdi Mei"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "11",
      "title": "A new recurrent neural-network architecture for visual pattern recognition",
      "authors": [
        "Seong-Whan Lee",
        "Hee-Heon Song"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "Classification of drowsiness levels based on a deep spatio-temporal convolutional bidirectional LSTM network using electroencephalography signals",
      "authors": [
        "Ji-Hoon Jeong",
        "Baek-Woon Yu",
        "Dae-Hyeok Lee",
        "Seong-Whan Lee"
      ],
      "year": "2019",
      "venue": "Brain Sci"
    },
    {
      "citation_id": "13",
      "title": "Towards an EEG-based intuitive BCI communication system using imagined speech and visual imagery",
      "authors": [
        "Seo-Hyun Lee",
        "Minji Lee",
        "Ji-Hoon Jeong",
        "Seong-Whan Lee"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Int. Conf. Syst. Man Cybern"
    },
    {
      "citation_id": "14",
      "title": "A multi-view cnn with novel variance layer for motor imagery brain computer interface",
      "authors": [
        "Ravikiran Mane",
        "Neethu Robinson",
        "Prasad Vinod",
        "Seong-Whan Lee",
        "Cuntai Guan"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf"
    },
    {
      "citation_id": "15",
      "title": "An adaptive deep reinforcement learning framework enables curling robots with human-like performance in real-world conditions",
      "authors": [
        "Dong-Ok Won",
        "Klaus-Robert M√ºller",
        "Seong-Whan Lee"
      ],
      "year": "2020",
      "venue": "Sci. Robot"
    },
    {
      "citation_id": "16",
      "title": "Periodwave: Multi-period flow matching for high-fidelity waveform generation",
      "authors": [
        "Sang-Hoon Lee",
        "Ha-Yeong Choi",
        "Seong-Whan Lee"
      ],
      "year": "2024",
      "venue": "Periodwave: Multi-period flow matching for high-fidelity waveform generation",
      "arxiv": "arXiv:2408.07547"
    },
    {
      "citation_id": "17",
      "title": "Emoq-tts: Emotion intensity quantization for fine-grained controllable emotional text-to-speech",
      "authors": [
        "Chae-Bin Im",
        "Sang-Hoon Lee",
        "Seung-Bin Kim",
        "Seong-Whan Lee"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "18",
      "title": "Diffprosody: Diffusion-based latent prosody generation for expressive speech synthesis with prosody conditional adversarial training",
      "authors": [
        "Hyung-Seok Oh",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process"
    },
    {
      "citation_id": "19",
      "title": "EmoSphere-TTS: Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech",
      "authors": [
        "Deok-Hyeon Cho",
        "Hyung-Seok Oh",
        "Seung-Bin Kim",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "venue": "Ann. Conf. Int. Speech Commun. Assoc. (INTER-SPEECH"
    },
    {
      "citation_id": "20",
      "title": "DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation",
      "authors": [
        "Hyoung-Seok Oh",
        "Sang-Hoon Lee",
        "Deok-Hyun Cho",
        "Seong-Whan Lee"
      ],
      "year": "2024",
      "venue": "DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation",
      "arxiv": "arXiv:2401.08095"
    },
    {
      "citation_id": "21",
      "title": "Emotion rendering for conversational speech synthesis with heterogeneous graphbased context modeling",
      "authors": [
        "Rui Liu",
        "Yifan Hu",
        "Yi Ren",
        "Xiang Yin",
        "Haizhou Li"
      ],
      "venue": "Proc. AAAI Conf. Artif. Intell. (AAAI)"
    },
    {
      "citation_id": "22",
      "title": "Internlm-xcomposer2: Mastering free-form textimage composition and comprehension in vision-language large model",
      "authors": [
        "Xiaoyi Dong"
      ],
      "year": "2024",
      "venue": "Internlm-xcomposer2: Mastering free-form textimage composition and comprehension in vision-language large model",
      "arxiv": "arXiv:2401.16420"
    },
    {
      "citation_id": "23",
      "title": "BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation",
      "authors": [
        "Chen Wang",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Jiajun Zhang"
      ],
      "year": "2024",
      "venue": "BLSP-KD: Bootstrapping Language-Speech Pre-training via Knowledge Distillation",
      "arxiv": "arXiv:2405.19041"
    },
    {
      "citation_id": "24",
      "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
      "authors": [
        "Yuan Gong",
        "Sameer Khurana",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "venue": "Ann. Conf. Int. Speech Commun. Assoc. (INTERSPEECH)"
    },
    {
      "citation_id": "25",
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "venue": "Proc. Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "26",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "Alec Radford"
      ],
      "venue": "Proc. Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "27",
      "title": "Joint Audio and Speech Understanding",
      "authors": [
        "Yuan Gong",
        "Alexander Liu",
        "Hongyin Luo",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "venue": "IEEE Autom. Speech Recognit. Underst. Workshop (ASRU)"
    },
    {
      "citation_id": "28",
      "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
      "authors": [
        "Jungil Kong",
        "Jaehyeon Kim",
        "Jaekyoung Bae"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst. (NeurIPS)"
    },
    {
      "citation_id": "29",
      "title": "DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech",
      "authors": [
        "Keon Lee",
        "Kyumin Park",
        "Daeyoung Kim"
      ],
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "30",
      "title": "DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Yanran Li"
      ],
      "year": "2017",
      "venue": "Int. Jt. Conf. Nat. Lang. Process. (IJCNLP)"
    },
    {
      "citation_id": "31",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "Houwei Cao"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "32",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "Adaeze Adigwe",
        "No√© Tits",
        "Kevin Haddad",
        "Sarah Ostadabbas",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "33",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "34",
      "title": "MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation",
      "authors": [
        "Kaisiyuan Wang"
      ],
      "year": "2020",
      "venue": "Eur. Conf. Comput. Vis. (ECCV)"
    },
    {
      "citation_id": "35",
      "title": "Toronto emotional speech set (tess)-younger talker happy",
      "authors": [
        "Kate Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (tess)-younger talker happy"
    },
    {
      "citation_id": "36",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "Wei-Lin Chiang"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "citation_id": "37",
      "title": "LLama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron"
      ],
      "year": "2023",
      "venue": "LLama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "38",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Int. Conf. Learn. Represent. (ICLR)"
    },
    {
      "citation_id": "39",
      "title": "emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation",
      "authors": [
        "Ziyang Ma"
      ],
      "venue": "Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "40",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst. (NeurIPS)"
    },
    {
      "citation_id": "41",
      "title": "Sequence-to-Sequence Acoustic Modeling for Voice Conversion",
      "authors": [
        "Jing-Xuan Zhang",
        "Zhen-Hua Ling",
        "Li-Juan Liu",
        "Yuan Jiang",
        "Li-Rong Dai"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "42",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": [
        "J Edward",
        "Hu"
      ],
      "venue": "Proc. Int. Conf. Learn. Represent. (ICLR)"
    }
  ]
}