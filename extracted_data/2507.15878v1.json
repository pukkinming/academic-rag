{
  "paper_id": "2507.15878v1",
  "title": "Salience Adjustment For Context-Based Emotion Recognition",
  "published": "2025-07-17T20:55:20Z",
  "authors": [
    "Bin Han",
    "Jonathan Gratch"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in dynamic social contexts requires an understanding of the complex interaction between facial expressions and situational cues. This paper presents a salience-adjusted framework for context-aware emotion recognition with Bayesian Cue Integration (BCI) and Visual-Language Models (VLMs) to dynamically weight facial and contextual information based on the expressivity of facial cues. We evaluate this approach using human annotations and automatic emotion recognition systems in prisoner's dilemma scenarios, which are designed to evoke emotional reactions. Our findings demonstrate that incorporating salience adjustment enhances emotion recognition performance, offering promising directions for future research to extend this framework to broader social contexts and multimodal applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automatic expression recognition traditionally treats facial expressions as signifying the emotional state of the expresser. Recently, there has been growing appreciation that observer perceptions differ from self-reported emotions  [6]  (people can seem happy when experiencing negative emotions  [2] ,  [22] ), yet these (mis)perceptions are crucial for explaining human social behavior. Perceived emotions build trust and cooperation  [8] ,  [41] , shape partner decisions  [13] , and help others to regulate their own emotions  [32] , regardless of the expresser's true feelings. Automatically recognizing perceived emotion can improve theories of human social behavior  [21]  and human-machine interaction  [14] .\n\nResearch into how observers interpret facial expressions highlights the crucial role of context. Interpretations from face alone differ dramatically from how expressions are interpreted in the context of other modalities  [34] , background faces  [30] , co-occurring actions  [4] , or recent events  [20] . Automated approaches to such \"context-based emotion recognition\" include standard machine learning  [24] ,  [26]  and methods grounded in psychological theory  [40] ,  [31] ,  [20] . In this paper, we focus on psychologically motivated approaches as our interest is in uncovering fundamental mechanisms that shape emotion perception, though we argue that such approaches can still yield state-of-the-art performance.\n\nBayesian Cue Integration (BCI) is a prominent psychological theory of how people infer emotion in context  [33] ,  [19] . From the perspective of automatic recognition, BCI is a cognitively-plausible late-fusion approach that can be applied to any state-of-the-art context-free emotion recognition method. It works by post-processing its output into predictions that better align with context-based annotations (see  [20] ). BCI argues people form separate emotion judgments from expressions and from situations, then integrate them with Bayesian inference. The theory behind BCI has been validated across a diverse range of social situations  [36] ,  [38] ,  [33] ,  [43] . While the theory highlights the importance of face and situation in determining perception, it also suggests that additional factors should be incorporated into the model. For instance, a recent study found that contextual information may dominate facial information in certain cases  [19] , suggesting the need to weigh the relative contribution of face versus situational context.\n\nIn this paper, we propose that the visual salience of a facial expression is an important factor that determines the relative importance of face versus context when observers form context-based emotion judgments. We first discuss prior psychological research that suggests that strong facial expressions capture attention. We next examine how facial and contextual cues interact to shape human emotion perception in a social task with real-world consequences.\n\nWe make several contributions to Bayesian Cue Integration and its application to automatic expression recognition. First, we find evidence that observers place greater weight on facial expressions than contextual cues when facial movement is salient. Second, we show that BCI's accuracy improves by incorporating expression salience. Third, we show that adjusting for expression salience enhances the accuracy of automatic recognition systems inspired by BCI. Finally, we demonstrate that even non-BCI approaches, such as the use of Vision-Language Models, achieve higher accuracy by adjusting for visual salience.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Expression Salience",
      "text": "Several studies find that emotional faces capture visual attention. Emotional faces \"pop out\" of a scene  [11] . Strong expressions, especially smiles  [10]  and frowns  [17] , attract attention and are recognized faster than more neutral faces or other visual information. The urge to attend to strong expressions is difficult to suppress, meaning that expressive faces cannot be ignored as effectively as other sources of information  [7] . Previous studies on BCI also found that the relative contribution of face and context varies, but did not incorporate a mechanism to address this variance  [19] .\n\nBuilding on these findings, we hypothesize that facial expressivity is one mechanism that modulates the relative importance of facial versus situational cues when forming an overall emotion judgment:\n\n• Expression-Salience Hypothesis: When facial expressiveness is low, perceivers give more weight to situational cues; when facial expressiveness is high, perceivers prioritize facial cues. To test this hypothesis, we analyzed how the salience of the expression influences the relative attention to facial arXiv:2507.15878v1 [cs.CV] 17 Jul 2025 versus situational cues in a social task with financial consequences. We first verify the hypothesis using purely human judgments before exploring whether the hypothesis affects the accuracy of automatic recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dataset: Usc Split-Steal",
      "text": "We examine the impact of expression salience using the USC Split-Steal corpus  [27]  as this has been previously used to illustrate the relevance of BCI to automatic expression recognition  [20] . The Split-Steal corpus contains videos and meta-data on participants engaged in a 10-round prisoner's dilemma  [35] . This task creates tension between cooperation and competition. On each round, participants can choose to cooperate (C) by offering to split a pot of lottery tickets, or they can defect (D) by trying to steal the entire pot. Their earnings are determined by the joint decision (i.e., they can steal the entire pot if they choose D and their partner chooses C). The game creates an incentive to cooperate but a temptation to steal and fear of being exploited. Prior research shows that people pay attention to their partner's emotional expressions after each round to predict their partner's intentions and determine their own actions  [15] .\n\nWe analyzed 100 videos from the Split-Steal corpus that had previously been annotated with context-free and contextbased labels  [20]  -25 from each joint outcome (CC, CD, DC, DD). These illustrate a player's facial reaction upon learning the outcome in a round (7-second video). Human annotators (N=141) provided 20 ratings per video of valence (5-point Likert scale  [9] ), and Basic Emotion (anger, disgust, fear, joy, sadness, surprise or neutral). Basic emotion labels were chosen as these have been used in prior psychological research on the prisoner's dilemma (e.g.,  [15] ) and BCI  [33] , but we also add valence as a second measure of emotion.\n\nAnnotations were collected under three conditions: the video and a description of the game and outcome As BCI treats emotions judgments as a probability distribution over possible labels. This was estimated using the 20 annotations per video. Following  [3] , valence was discretized into five categories using the points of the Likert scale.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Measuring Facial Expressivity",
      "text": "Facial expressivity was quantified using visual features inspired by prior work  [27] . OpenFace 2.0  [5]  is used to extract 12 Facial Action Units (AUs) 1 , focusing on frequently co-occurring AUs  [37] . Optical flow is calculated using ZFace  [23] , which tracks movements of 512 dense facial landmarks between frames. Head pose direction vectors, gaze direction vectors, and gaze angles are also included. Expressivity was calculated by combining facial, gaze, head 1 The selected AUs are  AU 1, 2, 4, 6, 7, 10, 12, 14, 15, 17, 25, and 26.  movement, and optical flow metrics. Each metric was standardized and equally weighted.\n\nWe validate the calculated expressivity with human annotations for subset of dataset  (24 videos) . Human annotations use a 7-point Likert scale. The results show a correlation of 0.61 (p < 0.001), comparable to the association found in  [27] . This suggests that the automatic expressivity score is a reasonably proxy for the expressivity perceived by a human observer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Impact Of Expressivity On Human Judgments",
      "text": "BCI argues that context-based judgments reflect an equal integration of face-only and situation-only judgments of emotion.  2  In contrast, Expression-Salience Hypothesis claims that highly expressive faces will capture attention, thereby assigning larger weight to the face.\n\nTo test this, we analyze if the context-based judgments are closer to situation-only judgments or closer to face-only judgments, measured as a function of facial expressivity. Videos were grouped into tertiles based on their level of expressivity. We then measure the proportion of contextbased judgments that were closer to the face-only judgments versus situation-only judgments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Experiment 1: Bayesian Cue Integration",
      "text": "Although BCI was developed to explain human judgments, Han and colleagues showed it can be adapted for automatic context-based recognition  [20] . Rather than using human annotations, they used standard facial expression recognition to predict context-free judgments and large langue models to predict context-only judgments. When integrated with BCI, the system quite accurately predicted human context-based judgments. Here, we see if these results can be improved further by incorporating expression salience. We test this for both basic emotion and valence recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Tasks And Evaluation Metrics",
      "text": "We test our approach on both valence and basic emotion recognition. For valence, we evaluate the results using Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Pearson Correlation. Basic Emotion Recognition, treated as a distribution task, is evaluated using KL Divergence (KLD), RMSE, and Pearson Correlation. Specifically, we use KLD to measure the distance between the predicted and ground truth probability distributions  [46] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Bayesian Cue Integration With Salience Adjustment",
      "text": "Section II showed that facial expressivity modulates how humans integrate these cues. To account for this, we introduce a salience adjustment mechanism into BCI. The original BCI equation is shown in Eq. 1  [33] , while the salienceadjusted version is defined in Eq. 2, where w represents the weight assigned to the facial cue based on expressivity. P(e| f ) represents the probability of emotion given only the face (Context-Free), P(e|c) represents the probability of emotion given only the game outcome (Context-Only), and P(e|c, f ) represents Context-Based prediction.\n\nWe calculate the weight w based on the Expressivity Score (Section II) and rescale it to the range [0.5, 1.0] using a linear mapping, approximating the findings in Figure  1 . Linear weighting is a common approach in cue integration models  [16] , and in our case, it provides a practical adjustment method, aligning with prior psychological findings on attentional biases toward expressive faces  [10] ,  [19] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Validation Using Human Annotations",
      "text": "We first evaluate salience adjustment by seeing if it improves BCI's fit to the human annotations: i.e., using the human annotations to estimate P(e| f ), P(e|c) and P(e|c, f ), does Equation (  2 ) better predict the distribution of P(e|c, f ) than Equation  (1) . Table  I  verifies that salience adjustment improves the accuracy of BCI for both valence and basic emotion prediction, averaging across the 100 videos.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Validation Of Fully Automatic Recognition",
      "text": "We next validate salience adjustment using automatic methods to estimate the emotion distributions and compare the automatic context-based prediction against the emotion distribution of the human context-based labels.  We use pre-trained models to estimate P(e| f ) (i.e., the context-free emotion predictions). EmoNet  [39]  and Blueskeye (blueskeye.com) to estimate emotional valence. Facet  [29]  and EAC  [45]  estimate basic emotions. To compare with  [20] , we also fine-tune a LSTM modeling following their protocol. We use GPT-4 to estimate P(e|c) (contextonly emotion predictions) following  [20] , specifically \"gpt-4-o-mini\". We prompt 20 times per description to ensure reliability and average the results.  The results show that salience adjustment consistently improves performance in both human and automatic emotion recognition tasks. By dynamically weighting facial and contextual cues, the proposed method better reflects human perception patterns and enhances performance IV. EXPERIMENT 2: VISION-LANGUAGE MODELS With recent advancements in Vision-Language Models (VLMs)  [28] , it is possible to fuse context and face information together in the same model, rather that using specialized models for each inference. As a last evaluation, we examine if VLMs could produce more accurate context-based emotion predictions by incorporating expression salience. We test this  with gpt-4o-mini-2024-0718 due to its efficient reasoning capabilities and support for multimodal inputs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Setup And Frame Selection",
      "text": "As GPT-4 cannot process video directly, we extract frames from videos and use them as input. While prior work  [28]  experimented with only 2-3 frames, we expand this range to 2-6 frames to evaluate the model's performance across varying frame counts. We tested multiple frame counts within this range and found that performance was highest when using 4 frames. After determining the number of frames, we uniformly sample from the original video, ensuring consistent representation of temporal information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Prompt Design",
      "text": "To combine facial and cues, we design a multistep prompt process (Figure  2 ), inspired Chain-of-Thought reasoning  [42]  in Vision-Language Models  [44] ,  [18] . GPT performs three tasks sequentially: 1) Face-Only emotion recognition (valence and basic emotion) based on the selected video frames; 2) Context-Only emotion recognition using the joint outcome (e.g., CC or DC); and 3) Face and Context integration based on both facial and contextual cues. The model also considers the facial expressivity score (W ), where higher W values prioritize facial expressions, and lower W values emphasize game context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results: Vlm-Context-Integration",
      "text": "The result is shown in Table  IV . First, when comparing the Context-Free and Context-Based approaches for both tasks, we observe that the addition of contextual information consistently improves performance (consist with BCI approach). Furthermore, the adoption of the Salience Adjustment method further enhances performance, demonstrating the effectiveness of dynamically weighting facial and contextual cues to align with human perception patterns.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "Our results provide strong support for the Expression-Salience Hypothesis and its utility for improving automatic emotion recognition. Within the prisoner's dilemma task, when the face shows strong emotions, observers attend more to the face and discount the results of the dilemma. Incorporating this \"salience adjustment\" into BCI-based recognition methods improved recognition accuracy. These improvements were found for both basic emotion recognition and valence recognition. Salience adjustment also improved the accuracy of an approach using a Visual Language Model.\n\nThere are several next steps for this research. These results must be replicated on other social tasks. For example, the Split-Steal corpus contains a large number of smiles. Although this aligns with previous findings that smiles are the most common nonverbal signals in social settings  [25] , our results might not generalize to corpora with more diverse emotion distributions. This study also focused exclusively on perceived emotion so a promising next step is to consider the accuracy of these perceptions compared to self-report as some research suggests context-based perceptions accurately reflect self-report  [12] . For example, automated methods have shown some success in distinguishing enjoyment smiles from other types of smiles  [1] , and it would be useful to explore how automatically-recognized distinctions shape observer perceptions in context. Finally, our approach is a heuristic modification of BCI. The \"proper\" Bayesian solution would be to incorporate expressivity as an explicit variable and estimate its conditional effect on inferences via Bayes rule (which would require a larger corpus than we evaluated here). Again, extending these results to other domains will be a crucial next step.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proportion of context-based annotations that are best approximated",
      "page": 2
    },
    {
      "caption": "Figure 1: shows clear support for for the Expression-",
      "page": 2
    },
    {
      "caption": "Figure 1: Linear weighting is a common approach in cue integration",
      "page": 3
    },
    {
      "caption": "Figure 2: Salience Adjustment Prompt Template for GPT Visual Reasoning.",
      "page": 4
    },
    {
      "caption": "Figure 2: ), inspired Chain-of-Thought",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bin Han and Jonathan Gratch": "Computer Science, University of Southern California, Los Angeles, USA"
        },
        {
          "Bin Han and Jonathan Gratch": "Abstract— Emotion recognition in dynamic\nsocial\ncontexts"
        },
        {
          "Bin Han and Jonathan Gratch": "requires\nan\nunderstanding\nof\nthe\ncomplex\ninteraction\nbe-"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "tween\nfacial\nexpressions\nand\nsituational\ncues.\nThis\npaper"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "presents a salience-adjusted framework for context-aware emo-"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "tion\nrecognition with Bayesian Cue\nIntegration\n(BCI)\nand"
        },
        {
          "Bin Han and Jonathan Gratch": "Visual-Language Models\n(VLMs)\nto dynamically weight\nfacial"
        },
        {
          "Bin Han and Jonathan Gratch": "and contextual\ninformation based on the expressivity of\nfacial"
        },
        {
          "Bin Han and Jonathan Gratch": "cues. We evaluate this approach using human annotations and"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "automatic emotion recognition systems\nin prisoner’s dilemma"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "scenarios, which are designed to evoke emotional reactions. Our"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "findings\ndemonstrate\nthat\nincorporating\nsalience\nadjustment"
        },
        {
          "Bin Han and Jonathan Gratch": "enhances emotion recognition performance, offering promising"
        },
        {
          "Bin Han and Jonathan Gratch": "directions\nfor\nfuture\nresearch\nto\nextend\nthis\nframework\nto"
        },
        {
          "Bin Han and Jonathan Gratch": "broader social contexts and multimodal applications."
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "I.\nINTRODUCTION"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "Automatic expression recognition traditionally treats facial"
        },
        {
          "Bin Han and Jonathan Gratch": "expressions as signifying the emotional state of the expresser."
        },
        {
          "Bin Han and Jonathan Gratch": "Recently,\nthere has been growing appreciation that observer"
        },
        {
          "Bin Han and Jonathan Gratch": "perceptions differ\nfrom self-reported emotions\n[6]\n(people"
        },
        {
          "Bin Han and Jonathan Gratch": "can seem happy when experiencing negative emotions\n[2],"
        },
        {
          "Bin Han and Jonathan Gratch": "[22]), yet\nthese (mis)perceptions are crucial\nfor explaining"
        },
        {
          "Bin Han and Jonathan Gratch": "human social behavior. Perceived emotions build trust and"
        },
        {
          "Bin Han and Jonathan Gratch": "cooperation\n[8],\n[41],\nshape\npartner\ndecisions\n[13],\nand"
        },
        {
          "Bin Han and Jonathan Gratch": "help others\nto regulate their own emotions\n[32],\nregardless"
        },
        {
          "Bin Han and Jonathan Gratch": "of\nthe\nexpresser’s\ntrue\nfeelings. Automatically recognizing"
        },
        {
          "Bin Han and Jonathan Gratch": "perceived\nemotion\ncan\nimprove\ntheories\nof\nhuman\nsocial"
        },
        {
          "Bin Han and Jonathan Gratch": "behavior\n[21] and human-machine interaction [14]."
        },
        {
          "Bin Han and Jonathan Gratch": "Research\ninto\nhow observers\ninterpret\nfacial\nexpres-"
        },
        {
          "Bin Han and Jonathan Gratch": "sions highlights\nthe\ncrucial\nrole of\ncontext.\nInterpretations"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "from face\nalone differ dramatically from how expressions"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "are\ninterpreted\nin\nthe\ncontext\nof\nother modalities\n[34],"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "background faces\n[30],\nco-occurring actions\n[4], or\nrecent"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "events\n[20]. Automated approaches\nto such “context-based"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "emotion recognition” include standard machine learning [24],"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "[26]\nand methods grounded in psychological\ntheory [40],"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "[31],\n[20].\nIn this paper, we focus on psychologically mo-"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "tivated approaches\nas our\ninterest\nis\nin uncovering funda-"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "mental mechanisms\nthat\nshape emotion perception,\nthough"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "we argue that such approaches can still yield state-of-the-art"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "performance."
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "Bayesian Cue\nIntegration (BCI)\nis\na prominent psycho-"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "logical\ntheory of how people infer emotion in context\n[33],"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "[19]. From the\nperspective\nof\nautomatic\nrecognition, BCI"
        },
        {
          "Bin Han and Jonathan Gratch": ""
        },
        {
          "Bin Han and Jonathan Gratch": "is\na\ncognitively-plausible\nlate-fusion approach that\ncan be"
        },
        {
          "Bin Han and Jonathan Gratch": "applied to any state-of-the-art\ncontext-free\nemotion recog-"
        },
        {
          "Bin Han and Jonathan Gratch": "nition method.\nIt works by post-processing its output\ninto"
        },
        {
          "Bin Han and Jonathan Gratch": "predictions\nthat better align with context-based annotations"
        },
        {
          "Bin Han and Jonathan Gratch": "(see [20]). BCI argues people form separate emotion judg-"
        },
        {
          "Bin Han and Jonathan Gratch": "ments\nfrom expressions and from situations,\nthen integrate"
        },
        {
          "Bin Han and Jonathan Gratch": "them with Bayesian inference. The theory behind BCI has"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "versus situational cues in a social\ntask with financial conse-": "quences. We first verify the hypothesis using purely human",
          "movement, and optical flow metrics. Each metric was stan-": "dardized and equally weighted."
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "judgments before\nexploring whether\nthe hypothesis\naffects",
          "movement, and optical flow metrics. Each metric was stan-": "We validate the calculated expressivity with human anno-"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "the accuracy of automatic recognition.",
          "movement, and optical flow metrics. Each metric was stan-": "tations for subset of dataset (24 videos). Human annotations"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "use\na 7-point Likert\nscale. The\nresults\nshow a\ncorrelation"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "A. Dataset: USC Split-Steal",
          "movement, and optical flow metrics. Each metric was stan-": "of 0.61 (p < 0.001),\ncomparable\nto the\nassociation found"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "in [27]. This\nsuggests\nthat\nthe automatic expressivity score"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "We examine the impact of expression salience using the",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "is\na\nreasonably proxy for\nthe\nexpressivity perceived by a"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "USC Split-Steal corpus [27] as this has been previously used",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "human observer."
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "to illustrate\nthe\nrelevance of BCI\nto automatic\nexpression",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "recognition [20]. The Split-Steal corpus contains videos and",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "C.\nImpact of Expressivity on Human Judgments"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "meta-data on participants engaged in a 10-round prisoner’s",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "BCI\nargues\nthat\ncontext-based\njudgments\nreflect\nan"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "dilemma [35]. This task creates tension between cooperation",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "equal\nintegration of\nface-only and situation-only judgments"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "and competition. On each round, participants can choose to",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "of\nemotion.2\nIn\ncontrast, Expression-Salience Hypothesis"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "cooperate\n(C) by offering to split\na pot of\nlottery tickets,",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "claims\nthat highly expressive\nfaces will\ncapture\nattention,"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "or\nthey\ncan\ndefect\n(D)\nby\ntrying\nto\nsteal\nthe\nentire\npot.",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "thereby assigning larger weight\nto the face."
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "Their earnings are determined by the joint decision (i.e., they",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "To test\nthis, we\nanalyze\nif\nthe\ncontext-based judgments"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "can steal\nthe entire pot\nif\nthey choose D and their partner",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "are closer\nto situation-only judgments or closer\nto face-only"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "chooses C). The game creates an incentive to cooperate but a",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "judgments, measured\nas\na\nfunction\nof\nfacial\nexpressivity."
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "temptation to steal and fear of being exploited. Prior research",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "Videos were grouped into tertiles based on their\nlevel of"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "shows that people pay attention to their partner’s emotional",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "expressivity. We\nthen measure\nthe\nproportion\nof\ncontext-"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "expressions after each round to predict\ntheir partner’s inten-",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "based judgments that were closer to the face-only judgments"
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "tions and determine their own actions [15].",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "",
          "movement, and optical flow metrics. Each metric was stan-": "versus situation-only judgments."
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "We analyzed 100 videos from the Split-Steal corpus that",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "had previously been annotated with context-free and context-",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "based labels\n[20] – 25 from each joint outcome (CC, CD,",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "DC, DD). These\nillustrate\na player’s\nfacial\nreaction upon",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "learning the outcome in a round (7-second video). Human",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "annotators (N=141) provided 20 ratings per video of valence",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "(5-point Likert scale [9]), and Basic Emotion (anger, disgust,",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "fear,\njoy, sadness, surprise or neutral). Basic emotion labels",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "were chosen as these have been used in prior psychological",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "research on the prisoner’s dilemma (e.g., [15]) and BCI [33],",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "but we also add valence as a second measure of emotion.",
          "movement, and optical flow metrics. Each metric was stan-": ""
        },
        {
          "versus situational cues in a social\ntask with financial conse-": "Annotations were collected under\nthree conditions:",
          "movement, and optical flow metrics. Each metric was stan-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "reliability and average the results."
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "Salience"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "-"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "w/o"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "w/"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "-"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "w/o"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": "w/"
        },
        {
          "4-o-mini”. We prompt 20 times per description to ensure": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": "P(e)"
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": "f )w · P(e | c)1−w"
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": "P(e)"
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": "the"
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": ""
        },
        {
          "P(e| f )P(e|c)": "it provides a practical adjust-"
        },
        {
          "P(e| f )P(e|c)": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": ""
        },
        {
          "Valence": "MSE(↓)\nRMSE(↓)\nCorrelation(↑)"
        },
        {
          "Valence": "BCI\n(w/o Salience)\n0.199\n0.446\n0.743"
        },
        {
          "Valence": "0.108\n0.328\n0.870\nBCI\n(w/ Salience)"
        },
        {
          "Valence": "Basic Emotion"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "KLD(↓)\nRMSE(↓)\nCorrelation(↑)"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "BCI\n(w/o Salience)\n0.308\n0.122\n0.873"
        },
        {
          "Valence": "0.146\n0.093\n0.889\nBCI\n(w/ Salience)"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "TABLE I"
        },
        {
          "Valence": "PERFORMANCE OF BCI WITH HUMAN-GENERATED EMOTION"
        },
        {
          "Valence": ""
        },
        {
          "Valence": ""
        },
        {
          "Valence": ""
        },
        {
          "Valence": "P(e| f )\nWe\nuse\npre-trained models\nto\nestimate\n(i.e.,"
        },
        {
          "Valence": "the\ncontext-free\nemotion\npredictions).\nEmoNet\n[39]\nand"
        },
        {
          "Valence": "Blueskeye\n(blueskeye.com)\nto estimate\nemotional valence."
        },
        {
          "Valence": "Facet\n[29] and EAC [45] estimate basic emotions. To com-"
        },
        {
          "Valence": "pare with [20], we also fine-tune a LSTM modeling following"
        },
        {
          "Valence": "their protocol. We use GPT-4 to estimate P(e|c)\n(context-"
        },
        {
          "Valence": "only emotion predictions)\nfollowing [20], specifically “gpt-"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "4-o-mini”. We prompt 20 times per description to ensure"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "reliability and average the results."
        },
        {
          "Valence": ""
        },
        {
          "Valence": ""
        },
        {
          "Valence": "Model\nSalience\nMSE(↓)\nRMSE(↓)\nCorrelation(↑)"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "EmoNet\n-\n0.516\n0.718\n0.425"
        },
        {
          "Valence": "EmoNet+GPT\nw/o\n0.481\n0.694\n0.528"
        },
        {
          "Valence": "EmoNet+GPT\nw/\n0.474\n0.688\n0.584"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "Blueskeye\n-\n0.308\n0.555\n0.556"
        },
        {
          "Valence": "0.653\nBlueskeye+GPT\nw/o\n0.274\n0.523"
        },
        {
          "Valence": "0.267\n0.517\nBlueskeye+GPT\nw/\n0.620"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "TABLE II"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "PERFORMANCE OF BCI WITH MACHINE-GENERATED VALENCE"
        },
        {
          "Valence": ""
        },
        {
          "Valence": ""
        },
        {
          "Valence": ""
        },
        {
          "Valence": "Model\nSalience\nKLD(↓)\nRMSE(↓)\nCorrelation(↑)"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "Facet\n-\n2.437\n0.203\n0.059"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "Facet+GPT\nw/o\n2.065\n0.199\n0.238"
        },
        {
          "Valence": "Facet+GPT\nw/\n1.975\n0.197\n0.640"
        },
        {
          "Valence": "EAC\n-\n1.525\n0.270\n0.141"
        },
        {
          "Valence": "EAC+GPT\nw/o\n0.967\n0.232\n0.141"
        },
        {
          "Valence": "EAC+GPT\nw/\n0.938\n0.190\n0.603"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "LSTM\n-\n0.581\n0.154\n0.675"
        },
        {
          "Valence": "LSTM+GPT\nw/o\n0.537\n0.139\n0.725"
        },
        {
          "Valence": "0.347\n0.109\n0.725\nLSTM+GPT\nw/"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "TABLE III"
        },
        {
          "Valence": "PERFORMANCE OF BCI WITH MACHINE-GENERATED BASIC EMOTIONS"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "The\nresults\nshow that\nsalience\nadjustment\nconsistently"
        },
        {
          "Valence": "improves performance in both human and automatic\nemo-"
        },
        {
          "Valence": "tion recognition tasks. By dynamically weighting facial and"
        },
        {
          "Valence": "contextual cues,\nthe proposed method better\nreflects human"
        },
        {
          "Valence": "perception patterns and enhances performance"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "IV. EXPERIMENT 2: VISION-LANGUAGE MODELS"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "With\nrecent\nadvancements\nin Vision-Language Models"
        },
        {
          "Valence": ""
        },
        {
          "Valence": "(VLMs) [28],\nit\nis possible to fuse context and face informa-"
        },
        {
          "Valence": "tion together in the same model, rather that using specialized"
        },
        {
          "Valence": "models for each inference. As a last evaluation, we examine"
        },
        {
          "Valence": "if VLMs could produce more accurate context-based emotion"
        },
        {
          "Valence": "predictions by incorporating expression salience. We test this"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. Results: VLM-Context-Integration": "The result\nis\nshown in Table IV. First, when comparing"
        },
        {
          "C. Results: VLM-Context-Integration": "the Context-Free\nand Context-Based\napproaches\nfor\nboth"
        },
        {
          "C. Results: VLM-Context-Integration": "tasks, we observe\nthat\nthe\naddition of\ncontextual\ninforma-"
        },
        {
          "C. Results: VLM-Context-Integration": "tion consistently improves performance\n(consist with BCI"
        },
        {
          "C. Results: VLM-Context-Integration": "approach). Furthermore,\nthe\nadoption of\nthe Salience Ad-"
        },
        {
          "C. Results: VLM-Context-Integration": "justment method further enhances performance, demonstrat-"
        },
        {
          "C. Results: VLM-Context-Integration": "ing the\neffectiveness of dynamically weighting facial\nand"
        },
        {
          "C. Results: VLM-Context-Integration": "contextual cues to align with human perception patterns."
        },
        {
          "C. Results: VLM-Context-Integration": "V. CONCLUSION"
        },
        {
          "C. Results: VLM-Context-Integration": "Our\nresults\nprovide\nstrong\nsupport\nfor\nthe Expression-"
        },
        {
          "C. Results: VLM-Context-Integration": "Salience Hypothesis and its utility for\nimproving automatic"
        },
        {
          "C. Results: VLM-Context-Integration": "emotion\nrecognition. Within\nthe\nprisoner’s\ndilemma\ntask,"
        },
        {
          "C. Results: VLM-Context-Integration": "when\nthe\nface\nshows\nstrong\nemotions,\nobservers\nattend"
        },
        {
          "C. Results: VLM-Context-Integration": "more to the face and discount\nthe results of\nthe dilemma."
        },
        {
          "C. Results: VLM-Context-Integration": "Incorporating\nthis\n“salience\nadjustment”\ninto\nBCI-based"
        },
        {
          "C. Results: VLM-Context-Integration": "recognition methods\nimproved recognition accuracy. These"
        },
        {
          "C. Results: VLM-Context-Integration": "improvements were found for both basic emotion recognition"
        },
        {
          "C. Results: VLM-Context-Integration": "and valence recognition. Salience adjustment also improved"
        },
        {
          "C. Results: VLM-Context-Integration": "the accuracy of an approach using a Visual Language Model."
        },
        {
          "C. Results: VLM-Context-Integration": "There\nare\nseveral\nnext\nsteps\nfor\nthis\nresearch.\nThese"
        },
        {
          "C. Results: VLM-Context-Integration": "results must be replicated on other social tasks. For example,"
        },
        {
          "C. Results: VLM-Context-Integration": "the Split-Steal\ncorpus\ncontains\na\nlarge\nnumber\nof\nsmiles."
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "Although this aligns with previous findings\nthat\nsmiles are"
        },
        {
          "C. Results: VLM-Context-Integration": "the most common nonverbal signals in social settings [25],"
        },
        {
          "C. Results: VLM-Context-Integration": "our results might not generalize to corpora with more diverse"
        },
        {
          "C. Results: VLM-Context-Integration": "emotion distributions. This study also focused exclusively on"
        },
        {
          "C. Results: VLM-Context-Integration": "perceived emotion so a promising next\nstep is\nto consider"
        },
        {
          "C. Results: VLM-Context-Integration": "the accuracy of these perceptions compared to self-report as"
        },
        {
          "C. Results: VLM-Context-Integration": "some research suggests context-based perceptions accurately"
        },
        {
          "C. Results: VLM-Context-Integration": "reflect\nself-report\n[12].\nFor\nexample,\nautomated methods"
        },
        {
          "C. Results: VLM-Context-Integration": "have shown some success in distinguishing enjoyment smiles"
        },
        {
          "C. Results: VLM-Context-Integration": "from other\ntypes\nof\nsmiles\n[1],\nand\nit would\nbe\nuseful"
        },
        {
          "C. Results: VLM-Context-Integration": "to explore how automatically-recognized distinctions\nshape"
        },
        {
          "C. Results: VLM-Context-Integration": "in\nobserver\nperceptions\ncontext.\nFinally,\nour\napproach\nis"
        },
        {
          "C. Results: VLM-Context-Integration": "a\nheuristic modification\nof BCI. The\n“proper” Bayesian"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "solution would be to incorporate expressivity as an explicit"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "variable\nand\nestimate\nits\nconditional\neffect\non\ninferences"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "via Bayes\nrule\n(which would require\na\nlarger\ncorpus\nthan"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "we evaluated here). Again, extending these results\nto other"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "domains will be a crucial next step."
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "REFERENCES"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "[1]\nZ. Ambadar,\nJ. F. Cohn, and L.\nI. Reed. All\nsmiles are not created"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "equal: Morphology and timing of smiles perceived as amused, polite,"
        },
        {
          "C. Results: VLM-Context-Integration": ""
        },
        {
          "C. Results: VLM-Context-Integration": "and embarrassed/nervous.\nJournal of nonverbal behavior, 33:17–34,"
        },
        {
          "C. Results: VLM-Context-Integration": "2009."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Personality\nand\nsocial\npsychology\nturned\nupside\ndown.\nbulletin,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "conference on computer vision, pages 10143–10152, 2019."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "33(6):763–775, 2007.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[27]\nS. Lei and J. Gratch.\nEmotional expressivity is a reliable signal of"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[3]\nS. Anzellotti, S. D. Houlihan, S. Liburd Jr, and R. Saxe. Leveraging",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "IEEE Transactions\nsurprise.\non Affective Computing,\n14(4):2913–"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "facial\nexpressions\nand contextual\ninformation to investigate opaque",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "2924, 2023."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "representations of emotions. Emotion, 21(1):96, 2021.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[28]\nZ. Lian, L. Sun, H. Sun, K. Chen, Z. Wen, H. Gu, B. Liu, and J. Tao."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[4] H. Aviezer, R. R. Hassin, J. Ryan, C. Grady, J. Susskind, A. Anderson,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "M. Moscovitch, and S. Bentin. Angry, disgusted, or afraid? studies",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "recognition.\nInformation Fusion, 108:102367, 2024."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Psychological\non\nthe malleability\nof\nemotion\nperception.\nscience,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[29] G. Littlewort, J. Whitehill, T. Wu, I. Fasel, M. Frank, and J. Movellan."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "19(7):724–732, 2008.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "The computer expression recognition toolbox (cert).\nIn 2011 IEEE"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[5]\nT. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency. Openface",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "International Conference on Automatic Face & Gesture Recognition"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "2.0: Facial behavior analysis toolkit.\nIn 2018 13th IEEE international",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "(FG), pages 298–305, 2011."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "conference on automatic face & gesture recognition (FG 2018), pages",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[30]\nT. Masuda, P. C. Ellsworth, B. Mesquita,\nJ. Leu, S. Tanida,\nand"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "59–66.\nIEEE, 2018.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "E. Van de Veerdonk. Placing the face in context: cultural differences"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[6]\nL. F. Barrett, B. Mesquita,\nand M. Gendron.\nContext\nin emotion",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "in the perception of facial emotion. Journal of personality and social"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Current directions\nin psychological\nperception.\nscience, 20(5):286–",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "psychology, 94(3):365, 2008."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "290, 2011.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[31] M. Mortillaro, B. Meuleman, and K. R. Scherer. Advocating a com-"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[7]\nE. Blagrove and D. G. Watson.\nIgnoring real faces: Effects of valence,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "International\nponential appraisal model\nto guide emotion recognition."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "threat, and salience. Attention, Perception, & Psychophysics, 76:725–",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Journal of Synthetic Emotions (IJSE), 3(1):18–32, 2012."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "745, 2014.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[32]\nP. M. Niedenthal, M. Brauer, L. Robin, and\nA. H.\nInnes-Ker. Adult"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[8] R. T. Boone and R. Buck. Emotional expressivity and trustworthiness:",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "attachment and the perception of facial expression of emotion. Journal"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "The role of nonverbal behavior in the evolution of cooperation. Journal",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "of personality and social psychology, 82(3):419, 2002."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "of Nonverbal Behavior, 27:163–182, 2003.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[33] D. C. Ong,\nJ. Zaki,\nand N. D. Goodman.\nAffective\ncognition:"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[9] M. M. Bradley and P. J. Lang. Measuring emotion: the self-assessment",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Exploring lay theories of emotion. Cognition, 143:141–162, 10 2015."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "manikin and the semantic differential. Journal of behavior therapy and",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[34]\nS.\nPoria, D. Hazarika, N. Majumder, G. Naik, E. Cambria,\nand"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "experimental psychiatry, 25(1):49–59, 1994.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "R. Mihalcea. Meld: A multimodal multi-party dataset\nfor\nemotion"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[10] M. G. Calvo\nand L. Nummenmaa.\nDetection\nof\nemotional\nfaces:",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "recognition in conversations. arXiv preprint arXiv:1810.02508, 2018."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Journal\nof\nsalient\nphysical\nfeatures\nguide\neffective\nvisual\nsearch.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Prisoner’s dilemma: A study in\n[35] A. Rapoport and A. M. Chammah."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Experimental Psychology: General, 137(3):471, 2008.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "conflict and cooperation, volume 165. University of Michigan press,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[11] M. G. Calvo, L. Nummenmaa,\nand\nP. Avero.\nVisual\nsearch\nof",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "1965."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "emotional\nfaces: Eye-movement assessment of component processes.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[36] R. Saxe and S. D. Houlihan. Formalizing emotion concepts within a"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Experimental Psychology, 55(6):359–370, 2008.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "bayesian model of\ntheory of mind.\nCurrent opinion in Psychology,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[12]\nJ. A. Coan and J. M. Gottman.\nThe\nspecific\naffect\ncoding system",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "17:15–21, 2017."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Handbook of\n(spaff).\nemotion elicitation and assessment, 267:285,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[37] G. Stratou, J. Van Der Schalk, R. Hoegen, and J. Gratch. Refactoring"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "2007.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "facial expressions: An automatic analysis of natural occurring facial"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[13] C. M. De Melo, P. J. Carnevale, S. J. Read, and J. Gratch. Reading",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "expressions in iterative social dilemma.\nIn 2017 Seventh International"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "people’s minds\nfrom emotion expressions in interdependent decision",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Conference on Affective Computing and Intelligent Interaction (ACII),"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Journal\nof\npersonality\nand\nsocial\nmaking.\npsychology,\n106(1):73,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "pages 427–433, 2017."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "2014.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[38]\nJ. B. Tenenbaum, C. Kemp, T. L. Griffiths,\nand N. D. Goodman."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[14] C. M. de Melo, J. Gratch, and P. J. Carnevale. Humans versus com-",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "How to grow a mind: Statistics,\nstructure, and abstraction.\nscience,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "puters:\nImpact of emotion expressions on people’s decision making.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "331(6022):1279–1285, 2011."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "IEEE Transactions on Affective Computing, 6(2):127–136, 2014.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[39] A. Toisoul,\nJ. Kossaifi, A. Bulat, G. Tzimiropoulos, and M. Pantic."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[15] C. M. de Melo, K. Terada, and F. C. Santos.\nEmotion expressions",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Estimation of\ncontinuous valence\nand arousal\nlevels\nfrom faces\nin"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "shape human social norms and reputations.\nIscience, 24(3), 2021.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "naturalistic conditions. Nature Machine Intelligence, 3(1):42–50, 2021."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[16] M. O. Ernst and M. S. Banks. Humans\nintegrate visual and haptic",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[40]\nE. Troiano, L. Oberl¨ander, and R. Klinger. Dimensional modeling of"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "information in a statistically optimal fashion. Nature, 415(6870):429–",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "emotions in text with appraisal\ntheories: Corpus creation, annotation"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "433, 2002.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "reliability, and prediction. Computational Linguistics, 49(1):1–72, 03"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[17]\nE. Fox, V. Lester, R. Russo, R. Bowles, A. Pichler, and K. Dutton. Fa-",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "2023."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "cial expressions of emotion: Are angry faces detected more efficiently?",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[41] G. A. Van Kleef, C. K. De Dreu,\nand A. S. Manstead.\nThe\ninter-"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Cognition & emotion, 14(1):61–92, 2000.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "personal effects of emotions in negotiations: a motivated information"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[18]\nJ. Ge, H. Luo, S. Qian, Y. Gan,\nJ. Fu,\nand S. Zhang.\nChain\nof",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "processing approach.\nJournal of personality and social psychology,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "arXiv\npreprint\nthought\nprompt\ntuning\nin\nvision\nlanguage models.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "87(4):510, 2004."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "arXiv:2304.07919, 2023.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[42]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[19]\nS. Goel,\nJ.\nJara-Ettinger, D. C. Ong,\nand M. Gendron.\nFace\nand",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "context\nintegration in emotion inference is limited and variable across",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "language models. Advances in neural\ninformation processing systems,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "categories and individuals. Nature Communications, 15(1):2443, 2024.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "35:24824–24837, 2022."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[20] B. Han, C. Yau,\nS. Lei,\nand\nJ. Gratch.\nKnowledge-based\nemo-",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[43]\nJ. Zaki. Cue integration: A common framework for social cognition"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "arXiv\npreprint\ntion\nrecognition\nusing\nlarge\nlanguage models.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "Perspectives\non Psychological\nand\nphysical\nperception.\nScience,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "arXiv:2408.04123, 2024.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "8(3):296–312, 2013."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[21] R. Heesen, M. A. Szenteczki, Y. Kim, M. E. Kret, A. P. Atkinson,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[44] R. Zhang, B. Zhang, Y. Li, H. Zhang, Z. Sun, Z. Gan, Y. Yang,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Z. Upton, and Z. Clay.\nImpact of social context on human facial and",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "R. Pang,\nand Y. Yang.\nImprove\nvision\nlanguage model\nchain-of-"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "gestural emotion expressions.\niScience, 27(11), 2024.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "thought\nreasoning. arXiv preprint arXiv:2410.16198, 2024."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[22] M. Hladk`y, R. R. Guerra, X. L. Cang, K. E. MacLean, P. Gebhard, and",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[45] Y. Zhang, C. Wang, X. Ling, and W. Deng. Learn from all: Erasing"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "T. Schneeberger. Modeling the ‘kiss my ass’-smile: Appearance and",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "attention consistency for noisy label\nfacial expression recognition.\nIn"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "functions of smiles in negative social situations.\nIn 12th International",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "European Conference on Computer Vision, pages 418–434. Springer,"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Conference on Affective Computing and Intelligent Interaction (ACII).",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "2022."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Glasgow, UK, 2024.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "[46]\nS. Zhao, G. Ding, Y. Gao,\nand\nJ. Han.\nApproximating\ndiscrete"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[23]\nL. A. Jeni, J. F. Cohn, and T. Kanade. Dense 3d face alignment from 2d",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "probability distribution of\nimage\nemotions by multi-modal\nfeatures"
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "videos in real-time.\nIn 2015 11th IEEE international conference and",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": "fusion. Transfer, 1000(1):4669–4675, 2017."
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "workshops on automatic face and gesture recognition (FG), volume 1,",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "pages 1–8.\nIEEE, 2015.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[24] R. Kosti,\nJ. M. Alvarez, A. Recasens,\nand A. Lapedriza.\nContext",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "IEEE transactions on\nbased emotion recognition using emotic dataset.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "pattern analysis and machine intelligence, 42(11):2755–2766, 2019.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[25] R. E. Kraut and R. E.\nJohnston.\nSocial and emotional messages of",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "Journal of personality and social\nsmiling: an ethological approach.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "psychology, 37(9):1539, 1979.",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        },
        {
          "[2] M. E. Ansfield.\nSmiling when distressed: When a smile is a frown": "[26]\nJ. Lee, S. Kim, S. Kim, J. Park, and K. Sohn. Context-aware emotion",
          "the IEEE/CVF international\nrecognition networks.\nIn Proceedings of": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "All smiles are not created equal: Morphology and timing of smiles perceived as amused, polite, and embarrassed/nervous",
      "authors": [
        "Z Ambadar",
        "J Cohn",
        "L Reed"
      ],
      "year": "2009",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "2",
      "title": "Smiling when distressed: When a smile is a frown turned upside down",
      "authors": [
        "M Ansfield"
      ],
      "year": "2007",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "3",
      "title": "Leveraging facial expressions and contextual information to investigate opaque representations of emotions",
      "authors": [
        "S Anzellotti",
        "S Houlihan",
        "S Liburd",
        "R Saxe"
      ],
      "year": "2021",
      "venue": "Emotion"
    },
    {
      "citation_id": "4",
      "title": "Angry, disgusted, or afraid? studies on the malleability of emotion perception",
      "authors": [
        "H Aviezer",
        "R Hassin",
        "J Ryan",
        "C Grady",
        "J Susskind",
        "A Anderson",
        "M Moscovitch",
        "S Bentin"
      ],
      "year": "2008",
      "venue": "Psychological science"
    },
    {
      "citation_id": "5",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "6",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "7",
      "title": "Ignoring real faces: Effects of valence, threat, and salience. Attention, Perception, & Psychophysics",
      "authors": [
        "E Blagrove",
        "D Watson"
      ],
      "year": "2014",
      "venue": "Ignoring real faces: Effects of valence, threat, and salience. Attention, Perception, & Psychophysics"
    },
    {
      "citation_id": "8",
      "title": "Emotional expressivity and trustworthiness: The role of nonverbal behavior in the evolution of cooperation",
      "authors": [
        "R Boone",
        "R Buck"
      ],
      "year": "2003",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "9",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "10",
      "title": "Detection of emotional faces: salient physical features guide effective visual search",
      "authors": [
        "M Calvo",
        "L Nummenmaa"
      ],
      "year": "2008",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "11",
      "title": "Visual search of emotional faces: Eye-movement assessment of component processes",
      "authors": [
        "M Calvo",
        "L Nummenmaa",
        "P Avero"
      ],
      "year": "2008",
      "venue": "Experimental Psychology"
    },
    {
      "citation_id": "12",
      "title": "The specific affect coding system (spaff). Handbook of emotion elicitation and assessment",
      "authors": [
        "J Coan",
        "J Gottman"
      ],
      "year": "2007",
      "venue": "The specific affect coding system (spaff). Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "13",
      "title": "Reading people's minds from emotion expressions in interdependent decision making",
      "authors": [
        "C De Melo",
        "P Carnevale",
        "S Read",
        "J Gratch"
      ],
      "year": "2014",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "14",
      "title": "Humans versus computers: Impact of emotion expressions on people's decision making",
      "authors": [
        "C De Melo",
        "J Gratch",
        "P Carnevale"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Emotion expressions shape human social norms and reputations",
      "authors": [
        "C De Melo",
        "K Terada",
        "F Santos"
      ],
      "venue": "Iscience"
    },
    {
      "citation_id": "16",
      "title": "Humans integrate visual and haptic information in a statistically optimal fashion",
      "authors": [
        "M Ernst",
        "M Banks"
      ],
      "year": "2002",
      "venue": "Nature"
    },
    {
      "citation_id": "17",
      "title": "Facial expressions of emotion: Are angry faces detected more efficiently?",
      "authors": [
        "E Fox",
        "V Lester",
        "R Russo",
        "R Bowles",
        "A Pichler",
        "K Dutton"
      ],
      "year": "2000",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "18",
      "title": "Chain of thought prompt tuning in vision language models",
      "authors": [
        "J Ge",
        "H Luo",
        "S Qian",
        "Y Gan",
        "J Fu",
        "S Zhang"
      ],
      "year": "2023",
      "venue": "Chain of thought prompt tuning in vision language models",
      "arxiv": "arXiv:2304.07919"
    },
    {
      "citation_id": "19",
      "title": "Face and context integration in emotion inference is limited and variable across categories and individuals",
      "authors": [
        "S Goel",
        "J Jara-Ettinger",
        "D Ong",
        "M Gendron"
      ],
      "year": "2024",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "20",
      "title": "Knowledge-based emotion recognition using large language models",
      "authors": [
        "B Han",
        "C Yau",
        "S Lei",
        "J Gratch"
      ],
      "year": "2024",
      "venue": "Knowledge-based emotion recognition using large language models",
      "arxiv": "arXiv:2408.04123"
    },
    {
      "citation_id": "21",
      "title": "Impact of social context on human facial and gestural emotion expressions",
      "authors": [
        "R Heesen",
        "M Szenteczki",
        "Y Kim",
        "M Kret",
        "A Atkinson",
        "Z Upton",
        "Z Clay"
      ],
      "venue": "iScience"
    },
    {
      "citation_id": "22",
      "title": "Modeling the 'kiss my ass'-smile: Appearance and functions of smiles in negative social situations",
      "authors": [
        "M Hladkỳ",
        "R Guerra",
        "X Cang",
        "K Maclean",
        "P Gebhard",
        "T Schneeberger"
      ],
      "year": "2024",
      "venue": "12th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "23",
      "title": "Dense 3d face alignment from 2d videos in real-time",
      "authors": [
        "L Jeni",
        "J Cohn",
        "T Kanade"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "24",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "25",
      "title": "Social and emotional messages of smiling: an ethological approach",
      "authors": [
        "R Kraut",
        "R Johnston"
      ],
      "year": "1979",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "26",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "27",
      "title": "Emotional expressivity is a reliable signal of surprise",
      "authors": [
        "S Lei",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "29",
      "title": "The computer expression recognition toolbox (cert)",
      "authors": [
        "G Littlewort",
        "J Whitehill",
        "T Wu",
        "I Fasel",
        "M Frank",
        "J Movellan"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "30",
      "title": "Placing the face in context: cultural differences in the perception of facial emotion",
      "authors": [
        "T Masuda",
        "P Ellsworth",
        "B Mesquita",
        "J Leu",
        "S Tanida",
        "E Van De Veerdonk"
      ],
      "year": "2008",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "31",
      "title": "Advocating a componential appraisal model to guide emotion recognition",
      "authors": [
        "M Mortillaro",
        "B Meuleman",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "32",
      "title": "Adult attachment and the perception of facial expression of emotion",
      "authors": [
        "P Niedenthal",
        "M Brauer",
        "L Robin",
        "Å Innes-Ker"
      ],
      "year": "2002",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Affective cognition: Exploring lay theories of emotion",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2015",
      "venue": "Cognition"
    },
    {
      "citation_id": "34",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "35",
      "title": "Prisoner's dilemma: A study in conflict and cooperation",
      "authors": [
        "A Rapoport",
        "A Chammah"
      ],
      "year": "1965",
      "venue": "Prisoner's dilemma: A study in conflict and cooperation"
    },
    {
      "citation_id": "36",
      "title": "Formalizing emotion concepts within a bayesian model of theory of mind",
      "authors": [
        "R Saxe",
        "S Houlihan"
      ],
      "year": "2017",
      "venue": "Current opinion in Psychology"
    },
    {
      "citation_id": "37",
      "title": "Refactoring facial expressions: An automatic analysis of natural occurring facial expressions in iterative social dilemma",
      "authors": [
        "G Stratou",
        "J Van Der Schalk",
        "R Hoegen",
        "J Gratch"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "38",
      "title": "How to grow a mind: Statistics, structure, and abstraction. science",
      "authors": [
        "J Tenenbaum",
        "C Kemp",
        "T Griffiths",
        "N Goodman"
      ],
      "year": "2011",
      "venue": "How to grow a mind: Statistics, structure, and abstraction. science"
    },
    {
      "citation_id": "39",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Dimensional modeling of emotions in text with appraisal theories: Corpus creation, annotation reliability, and prediction",
      "authors": [
        "E Troiano",
        "L Oberländer",
        "R Klinger"
      ],
      "year": "2023",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "The interpersonal effects of emotions in negotiations: a motivated information processing approach",
      "authors": [
        "G Van Kleef",
        "C De Dreu",
        "A Manstead"
      ],
      "year": "2004",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "42",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "Cue integration: A common framework for social cognition and physical perception",
      "authors": [
        "J Zaki"
      ],
      "year": "2013",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "44",
      "title": "Improve vision language model chain-ofthought reasoning",
      "authors": [
        "R Zhang",
        "B Zhang",
        "Y Li",
        "H Zhang",
        "Z Sun",
        "Z Gan",
        "Y Yang",
        "R Pang",
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Improve vision language model chain-ofthought reasoning",
      "arxiv": "arXiv:2410.16198"
    },
    {
      "citation_id": "45",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "X Ling",
        "W Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Approximating discrete probability distribution of image emotions by multi-modal features fusion",
      "authors": [
        "S Zhao",
        "G Ding",
        "Y Gao",
        "J Han"
      ],
      "year": "2017",
      "venue": "Transfer"
    }
  ]
}