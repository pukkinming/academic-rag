{
  "paper_id": "2302.14638v1",
  "title": "Speechformer++: A Hierarchical Efficient Framework For Paralinguistic Speech Processing",
  "published": "2023-02-27T11:48:54Z",
  "authors": [
    "Weidong Chen",
    "Xiaofen Xing",
    "Xiangmin Xu",
    "Jianxin Pang",
    "Lan Du"
  ],
  "keywords": [
    "Transformer",
    "paralinguistic speech processing",
    "speech emotion recognition",
    "neurocognitive disorder detection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Paralinguistic speech processing is important in addressing many issues, such as sentiment and neurocognitive disorder analyses. Recently, Transformer has achieved remarkable success in the natural language processing field and has demonstrated its adaptation to speech. However, previous works on Transformer in the speech field have not incorporated the properties of speech, leaving the full potential of Transformer unexplored. In this paper, we consider the characteristics of speech and propose a general structure-based framework, called SpeechFormer++, for paralinguistic speech processing. More concretely, following the component relationship in the speech signal, we design a unit encoder to model the intra-and inter-unit information (i.e., frames, phones, and words) efficiently. According to the hierarchical relationship, we utilize merging blocks to generate features at different granularities, which is consistent with the structural pattern in the speech signal. Moreover, a word encoder is introduced to integrate word-grained features into each unit encoder, which effectively balances fine-grained and coarse-grained information. SpeechFormer++ is evaluated on the speech emotion recognition (IEMOCAP & MELD), depression classification (DAIC-WOZ) and Alzheimer's disease detection (Pitt) tasks. The results show that SpeechFormer++ outperforms the standard Transformer while greatly reducing the computational cost. Furthermore, it delivers superior results compared to the state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH has been used over thousands of years in human society and is able to convey the most information in the simplest way  [1] . Paralinguistic speech processing (PSP), which aims to extract information beyond the linguistic content of speech, such as sentiment, depression and neurocognition, has a wide range of applications in different areas. Consequently, it is of increasing interest to the research community. Weidong Chen and Xiaofen Xing are with the School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510640, China (e-mail: eewdchen@mail.scut.edu.cn; xfxing@scut.edu.cn). Xiangmin Xu is with the School of Future Technology, South China University of Technology, Guangzhou 511442, China, and also with Pazhou Laboratory, Guangzhou 510330, China (e-mail: xmxu@scut.edu.cn). Jianxin Pang is with UBTECH Research, UBTECH Robotics Corporation, Shenzhen 518055, China (e-mail: walton@ubtrobot.com). Lan Du is with iFLYTEK Research, iFLYTEK Corporation, Hefei 230088, China (e-mail: landu@iflytek.com). Modeling speech signals in PSP is a great challenge because the pronunciation information and the dynamic changes of speech are well understood by humans but are difficult for models to comprehend. Over the last three decades, numerous machine learning algorithms, such as hidden Markov models  [2] -  [4] , decision trees  [5] ,  [6]  and restricted Boltzmann machines  [7] -  [9] , have been proposed to capture paralinguistic information in speech. Recently, deep learning methods have delivered superior performance for PSP tasks owing to their remarkable modeling capabilities. For example, convolutional neural networks (CNNs)  [10] -  [16] , graph neural networks (GNNs)  [17] ,  [18] , recurrent neural networks (RNNs)  [19] -  [21]  and two popular variants of the RNNs named long shortterm memory (LSTM)  [22] -  [24]  and gated recurrent units (GRUs)  [25]  have achieved promising results in PSP domain.\n\nTransformer  [26] , is the latest deep learning technique and was originally used in the natural language processing field (NLP). It has achieved great success using full attention to model the long-range dependencies in a sequence. Currently, there is a growing body of literature that recognises the value of Transformer, especially in the computer vision domain (CV)  [27] -  [32] . When researchers adapt the Transformer from language to vision, the characteristics of the input signal (i.e. image) are considered and used as a guide to modify the attention mechanism. In essence, the information contained in a language signal is dense, while that in an image is sparse. Hence, full attention in the standard Transformer is no longer appropriate. As shown in Fig.  1(a ) and 1(b), to bridge the gap between language and vision, researchers in vision prefer to use local windows to capture the local information in images. However, using local windows may cause the same object in the image to be separated. To allow connection between different windows, prior works have mainly explored two directions:  (1)  Shift the attention windows between layers such that the neighboring windows are linked together  [28] .\n\n(2) Cluster the input tokens and perform local attention in feature space  [29] ,  [30] . For the PSP task, there has also been extensive research regarding the use of Transformer  [33] -  [38] . Despite the improvements made, little attention has been paid to the characteristic of the input signal (i.e. speech), as has been done in computer vision, which is however crucial for extracting paralinguistic information. Therefore, there is an urgent need to exploit the potential of Transformer in PSP by incorporating the features of speech signals. Meanwhile, the standard Transformer is computationally expensive due to its full attention, which makes it difficult to use in practice.\n\nTo address these drawbacks, we should rethink the natural structure of speech signals first. As shown in Fig.  1 (c), we note that a speech signal can be perceived from different perspectives, allowing the information to have different granularities. We refer to this property as the hierarchical relationships of speech. The basic acoustic units that construct the speech signal, from fine to coarse, are frames, phones and words. On the other hand, a word consists of several consecutive phones, and a phone is composed of several consecutive frames. This connection between frames, phones and words is termed the component relationship. Armed with these implicit relationships, we propose a novel attention mechanism for the PSP task, which is illustrated in Fig.  1(c ). First, we perform, with high computational efficiency, local attention to model the adjacent tokens that belong to the same unit. Statistical durations of phones and words are used to ensure that each local window completely covers the central unit, allowing the boundary information of the central unit to be completely preserved and not separated. The interactions between neighboring units are also considered to comprehensively simulate the inter-unit information. After sufficient learning has been performed in the current stage, we utilize a merging block to aggregate the feature tokens and feed them into the next stage, thus enabling our framework to follow the hierarchical structure of the speech signal. Moreover, to further enrich the information contained in each token, we introduce several learnable word tokens to appropriately incorporate the coarse-grained features into the fine-grained features. The contributions of this paper can be summarized as follows:\n\n• Based on the component relationship in speech, we propose a unit encoder to capture the intra-and inter-unit information efficiently. To further enhance the extracted features, we utilize a word encoder to effectively integrate the coarse-and fine-grained information. • Based on the hierarchical relationship in speech, we construct a hierarchical backbone, called SpeechFormer++, for paralinguistic speech processing. To the best of our knowledge, this is the first study that leverages the characteristics of speech to exploit the potential of Transformer.\n\n• We evaluate our method on four benchmark datasets and demonstrate that our SpeechFormer++ substantially outperforms the standard Transformer in terms of performance and computational efficiency. Moreover, Speech-Former++ achieves superior results compared to the stateof-the-art approaches. Our code is publicly available at https://github.com/HappyColor/SpeechFormer2. A preliminary version of this work was published in  [39] . We have extended our conference version as follows. In terms of the model structure,  (1)  we introduce an additional encoder to balance the fine-grained and the coarse-grained information efficiently and achieve superior performance; (2) we investigate the sensitivity of SpeechFormer++ to the statistical durations and offer guidance on applying SpeechFormer++ to various scenarios. In terms of verifying the effectiveness of the framework, (1) we report results of SpeechFormer++ with pretrained features and hand-crafted features to demonstrate the adaptability of our approach; (2) we compare Speech-Former++ and other approaches using the same set of features to release the impact of input features; (3) we compare the performances of finetuning the pretrained model directly with several dense layers and learning further deep representation with SpeechFormer++ to prove the importance of our framework; (4) we adopt attention mechanisms from computer vision to demonstrate the necessity of considering the features of speech when modeling speech signal. In terms of verifying the effectiveness of every module, (1) we conduct a comprehensive ablation study to analyze the indispensability of each module in SpeechFormer++. In terms of model understanding and interpretation, (1) we visualize the attention weights of Transformer and SpeechFormer++ to determine the reasons behind the improvements; (2) we add more evaluation metrics to each dataset for better justification of results.\n\nThe rest of this paper is organized as follows: In Section II, we provide a brief literature review on Transformer and structure-based paralinguistic speech processing. In Section III, we elaborate on the proposed SpeechFormer++ framework. In Section IV, we describe the experimental corpora and setup in detail. In Section V, we present our experimental results and analyses. Finally, we draw our conclusions in Section VI.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we systematically introduce the applications of Transformer in different fields and the related research on structure-based paralinguistic speech modeling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Transformer In Language Processing And Computer Vision",
      "text": "The original Transformer is designed to tackle machine translation tasks in the natural language processing field  [26] . As a sequence learning model, Transformer is excellent at modeling the long-range dependencies, while being purely based on the attention mechanism, it dispenses with the recursion and convolution and computes hidden representations in parallel. In general, the raw text signal is first converted into a word embedding sequence via the word and position embedding layers. Then, the output is delivered to a stack of Transformer encoders to produce the final embedding, followed by several Transformer decoders or a task-specific classifier. The Transformer has been applied to various NLP tasks, including question answering  [40] , named entity recognition  [41] , natural language inference  [42] , semantic textual similarity  [43]  and document classification  [44] .\n\nIn computer vision, an image is first split into several fixedsize (e.g. 16 × 16) patches, followed by a linear projection and a positional embedding layer to yield the input for the Transformer  [27] . Inspired by CNNs that can be improved by stacking more convolutional layers, researchers have attempted to increase the depth of vision Transformer and solve the attention collapse issue encountered when the model goes deeper  [45] . Nevertheless, the self-attention operation scales quadratically with the sequence length, making Transformer computationally expensive and unable to handle numerous tokens in high-resolution images. Consequently, the bulk of the literature has been devoted to enhancing the attention mechanism used to exploit the potential of Transformer in computer vision  [28] -  [32] . Typically, Rao et al.  [32]  evaluated the importance of each token and dropped the useless tokens dynamically and progressively. Liu et al.  [28]  proposed a hierarchical Transformer and performed attention within each shifted window, which greatly reduced the computational cost while also allowing for cross-window interactions. Although this method boosts efficiency, it fails to capture the relationships between distant but similar patches in the image due to the constraint of the window size. To address this limitation,  [29]  first clustered the tokens and then computed self-attention among the related tokens in feature space.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Transformer In Paralinguistic Speech Processing",
      "text": "There has also been much interest in applying the standard Transformer in the speech domain. Generally, the given raw speech signal is segmented into multiple overlapping frames. Then, the spectral or deep learning-based features are extracted from each frame and used as input for the Transformer  [34] -  [36] ,  [46] . In  [35] , stacked multiple Transformer layers were explored to enhance the features extracted for speech emotion recognition. In  [36] , researchers followed the structure of Swin Transformer  [28]  and cut the spectrogram into different patch tokens for sound classification and detection. Although these works demonstrate their effectiveness, they mainly adopt Transformer directly, ignoring the characteristics of speech and task. To overcome this problem, efficient emotion recognition was implemented in  [34]  by utilizing a sparse Transformer to focus more on the emotion-related information. In  [47] , an auditory saliency mechanism was studied and applied in a Transformer to adjust the feature embeddings. Additionally, Transformer has been employed for Alzheimer's disease (AD) detection  [48] ,  [49]  and depression classification  [50] . For example, Ilias et al.  [48]  utilized a pretrained vision Transformer  [27]  to extract acoustic features and achieved remarkable results for dementia detection. Later, Zhu et al.  [49]  sought an effective integration of semantic and non-semantic speech information. Both  [48]  and  [49]  encouraged the use of pretrained models. In  [50] , a Transformer-based network was utilized to extract long-term temporal context information for depression estimation. Based on Transformer, various selfsupervised speech representation learning approaches have also been proposed, including wav2vec  [51] , wav2vec 2.0  [52]  and HuBERT  [53] . Built on the pretrained self-supervised models, several researches have delivered promising results in the literature  [33] ,  [34] ,  [39] ,  [49] ,  [54] -  [57] . Typically, Monica et al.  [33]  fine-tuned the pretrained HuBERT model for AD detection and achieved competitive performance. In addition to the paralinguistic tasks, Transformer is also broadly used in speech recognition field  [58] -  [60] . Typically, Wang et al.  [59]  explored the potential of Transformer-based acoustic models on hybrid speech recognition and achieved significant word error rate improvement over the conventional baselines. Gulati et al.  [60]  novelly proposed a convolution-augmented Transformer, called Conformer, to learn both global interactions and local features effectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Structure-Based Paralinguistic Speech Modeling",
      "text": "The speech signal is structured by different basic units, from fine to coarse, which are frames, phones and words. This natural structure is unique to speech and contains extensive paralinguistic information such as fluency, articulation, prolongation and rhythm. Some previous works have taken advantage of these speech structures to improve system performance  [39] ,  [61] -  [65] . For example, Zhao et al.  [61]  trained a hierarchical network for depression severity measurement, where frame-level and sentence-level representations were learned explicitly. However, human speech has more than these two levels. Other levels such as phone-level and word-level can better reflect the pronunciation. To address this problem, a vast majority of works are explored toward hierarchical multigranularity learning. For example,  [62]  utilized a hierarchical attention structure with word-level alignment for emotion recognition. In  [63] , phone-and word-level representations were captured through a GRU network  [66] , using the groundtruth timestamps of every unit.  [64]  aggregated the acoustic embedding for each word based on its corresponding speech frames for information fusion. Although the above methods improve the recognition performance, the requirement of exact timestamps of phones or words in their systems makes them unsuitable for practical applications. Additionally, current studies have not sufficiently explored the hierarchy of speech signal, leaving the full potential of Transformer in the speech domain unexplored. Recently, researchers demonstrated that deep neural networks can integrate the individual basic units across multiple timescales via different integration windows, the sizes of which were yoked to the duration of the units  [67] . This finding also indicates that the basic units in speech are instructing the model learning. However, existing works on Transformer have not comprehensively considered the audio properties, which is remedied in this paper.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The proposed framework, as shown in Fig  2 , consists of four stages and three key modules. The unit encoder and word encoder are used for structure-based speech unit learning, and a merging block is employed for structure-based speech unit aggregation. We first clarify the guidelines for model design. Afterwards, we elaborate on the proposed SpeechFormer++.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Guiding Principles Of Model Design",
      "text": "The statistical duration of the speech unit is the basis for the design of our framework. Therefore, we first estimate the durations of phones and words on the corpora used in this paper by P2FA  [68]  toolkit. Since the distribution of the unit duration is similar for each corpus, we illustrate in Fig.  3  the statistical results obtained by combining all the audio files from four corpora. We note that more than 80% of phones vary from 50 to 200 ms, and we therefore approximate the shortest and longest durations of phones to be 50 and 200 ms, respectively. Similarly, almost 90% of words range between 250 to 1000 ms, which we regard as the shortest and longest durations of words. Additionally, we note that the duration of the frame is literally the frame length used when extracting acoustic features, which can be set manually. In addition, the hierarchical pattern in the speech signal aggregates the consecutive units progressively, which sheds new light on the design of the hierarchical framework.\n\nB. Structure-Based Speech Unit Learning 1) Unit Encoder: Given a speech signal, we first extract its acoustic representations x 1 ∈ R T1×d1 , where T 1 is the number of frames and d 1 is the dimension of each frame embedding. To capture the information about consecutive frames in the frame stage, we employ a unit encoder with window T w1 to learn the frame-grained features in x 1 . Specifically, the framegrained input feature x 1 is split into T 1 overlapping segments:\n\nwhere subscript i denotes the different stages in The subscript i is equal to 1 because it is currently in the frame stage. Zero padding is employed when the segment is out of range (e.g., when a < 0 or b > T i ). The value of T w1 is set to the number of tokens that can be contained within 50 ms (the shortest duration of phones) of input x 1 . Thus, the interactions of nearby frames are learnt. Specifically, the attention in each segment can be written as:\n\nwhere\n\nand xi are the updated values of x (j) i and x i , respectively. M SA(Q, K, V ) represents the Multi-Head Self-Attention (MSA) mechanism with inputs query Q, key K and value V . More details of MSA can be found in  [26] . N orm(•) represents the layer normalization  [69]  throughout the paper. When performing attention, the query x (j) i denotes the central feature token in the current overlapping segment.\n\nIn the phone stage, we assume the phone-grained input feature to be x 2 ∈ R T2×d2 , where T 2 denotes the number of phone tokens and d 2 denotes the dimensions of phone embeddings. Each token contained in x 2 is the representation of a phone or subphoneme, which is produced by the merging block using the output of the frame stage (described in III-C) and fed into the phone stage. To model a phone and the interactions with its neighbors, the value of window T w2 is set to the number of tokens that can be contained within 400 ms (twice the longest duration of phones) of x 2 . Thus, each segment covers consecutive phones, and the central phone is unbroken. Finally, the attention calculation in the phone stage follows Eqs. 1-3 with i = 2.\n\nSimilarly, in the word stage, its word-grained input feature is x 3 ∈ R T3×d3 , where T 3 and d 3 represent the number of word tokens and the dimensions of word embeddings, respectively. It is produced by a merging block using the output of the phone stage (described in III-C). To capture the intra-and inter-word information, the window size T w3 in the word stage is set to the number of tokens that can be contained within 2000 ms (twice the longest duration of words) of x 3 . The attention mechanism is then invoked in the overlapping segments, each of which contains a central word and its surrounding context. The computational process follows Eqs. 1-3 with i = 3.\n\n2) Word Encoder: The proposed unit encoder is able to model the fine-grained features efficiently. However, its receptive field is limited by the size of the attention window. To take the coarse-grained information into account, we propose a word encoder (Fig.  4(c )) to inject the coarse-gained information into each unit encoder. We first create several learnable word tokens z 1 ∈ R Tz×d1 for the frame stage, where T z indicates the approximate number of words in the utterance.\n\nConcretely, the value of T z is equal to the total duration of the utterance divided by 1000 ms (the longest duration of words). z 2 ∈ R Tz×d2 for the phone stage and z 3 ∈ R Tz×d3 for the word stage are produced by the merging block (described in III-C). Then, the input x i is evenly grouped into T z nonoverlapping segments. Each learnable word token is required to learn the coarse-grained features about the corresponding segment. The learning process is as follows:\n\nwhere EvenSeg(•) denotes the non-overlapping segmentation, s ij is the j-th non-overlapping segment of x i and j ∈ [1, T z ], z (j) i denotes the j-th learnable word token in z i and z(j) i is the updated value of z (j) i . Since the interactions between words are modeled by the word stage of SpeechFormer++, we perform non-overlapping segmentation in the word encoder. Note that the number of non-overlapping segments is always identical to that of the learnable word tokens and remains constant across different stages.\n\nThen, we pass the learnt zi to each unit encoder in the i-th stage, allowing the unit encoders to take the coarsegrained information into consideration while modeling locally. As shown in Fig.  4 (b), each acoustic segment is enhanced by its corresponding learnable word token, which is then fed into the MSA and FFN layers. The complete calculation flow in the unit encoder (Fig.  4(b) ) is as follows:\n\ni , e jk i , e jk i ) + x\n\nwhere e jk i ∈ R (1+Twi)×di is the enhanced segment, Ceil[•] rounds a number upward to its nearest integer and j ∈ [1, T i ]; F F N (•) denotes the feed-forward network and xi denotes the final output of the unit encoder. The parameters in the MSA are shared between the unit encoder and the word encoder, keeping the size of the model unchanged. In addition, a unit encoder and a word encoder constitute a basic SpeechFormer++ block. Multiple SpeechFormer++ blocks are stacked to form a stage in our proposed framework.\n\nC. Structure-Based Speech Unit Aggregation 1) Merging Block: Inspired by the hierarchical property of speech signals that can be progressively categorized into frames, phones and words, we propose a merging block to generate the relevant features under the instruction of the statistical durations of the speech units. As shown in Fig.  2 , merging blocks are used between each stage. Initially, the acoustic input of the frame stage x 1 represents the features of each frame from the original speech signal. To provide the phone-grained input to the phone stage, we apply average pooling over the output of the frame stage x1 with a merging scale M 1 of 50  ms (the shortest duration of phones). Then, a linear projection and layer normalization are performed to create the phonegrained feature x 2 . The information contained every 50 ms is aggregated into a token in x 2 such that each token in x 2 represents the information of a subphoneme. Analogously, the merging scale M 2 is set to 250 ms (the shortest duration of words) when attempting to generate the word-grained input x 3 for the word stage, making each token in x 3 a representation of a subword. Finally, the last merging block is applied to the output of the word stage x3 while merging scale M 3 is set to 1000 ms (the longest duration of words) to roughly simulate the number of words in the utterance sample. The learnable word tokens z i represent the coarse-grained features in words, and thus, we do not have to aggregate them. Formally, the merging block is defined as:\n\nwhere AvgP ool(x, M ) represents an average pooling layer performed on x with window size and stride equal to M ; W i ∈ R di×di+1 and b ∈ R di+1 are to be learned parameters; xi and zi denote the outputs of the i-th stage and x i+1 and z i+1 denote the inputs of the next stage, i ∈ {1, 2, 3}.\n\nThe outputs of the third merging block are concatenated together and fed into the utterance stage, which is a stack of standard Transformer encoders, to model the speech signal globally. The overview of the computational flow in our proposed method is illustrated in Fig.  5 . The acoustic tokens are aggregated progressively to imitate the structural pattern in the speech signal, and the attention is guided by the characteristics of speech. The final output of the utterance stage is pooled in the temporal dimension and is passed to a classifier, which is composed of two linear projections with an activation function in between, to yield the final classification results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Loss Function",
      "text": "We choose the categorical cross-entropy loss (CCE) as the objective function in this paper. Suppose we have S samples and C possible categories. The CCE can be represented as:\n\nwhere ŷsc ∈ R 1  denotes the predicted probability that the s-th sample belongs to class c and y sc ∈ R 1 is 1 when c is equal to the ground-truth label and 0 otherwise.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Complexity Analysis",
      "text": "Supposing inputs x ∈ R T ×d , z ∈ R Tz×d and window size is T w . The computational complexities of the MSA in Transformer and SpeechFormer++ (S-MSA) are:\n\n)\n\nNote that we omit softmax computation in determining complexity. When T w and T z are fixed, Ω(S-M SA) scales linearly with the sequence length T , while Ω(M SA) in the standard Transformer scales quadratically. Moreover, the values of T w and T z are much smaller than that of T in practice. When features go through a merge block, the number of tokens is greatly reduced, enabling the computational cost of the later layers in SpeechFormer++ to become fairly low. The cost of the merging block is negligible compared to the total complexity and the model size.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets And Evaluation Metric",
      "text": "IEMOCAP  [70]  is the most commonly used dataset in the speech emotion recognition field. It contains 12 hours of audio data and consists of five sessions, each of which has one male speaker and one female speaker. 5,531 utterances from four emotion categories: angry, neutral, happy 1 and sad, are considered in this work. To train and test the model, we conduct experiments in the leave-one-session-out crossvalidation strategy. Specifically, samples from 4 sessions are used for training, and the remaining session is regarded as the testing set, which is repeated 5 times until all different sessions are used for training and testing. We evaluate at each epoch the model on the testing set and the reported results are the average scores of the 5-fold experiments.\n\nMELD  [71]  is the second dataset we used for emotion recognition. The dataset contains 13,708 utterances from the Friends TV series, divided into 7 emotion classes: anger, disgust, sadness, joy, neutral, surprise and fear. Since this dataset has been officially divided into training, validation and testing sets, we use the validation set for hyperparameter turning. The model with the best performance on the validation set across epochs is evaluated on the testing set. Finally, the results on the testing set are reported.\n\nPitt  [72]  is a classical dataset used in the AD detection field. To the narrative speech recordings, the AD patients and healthy controls are asked to take the \"Cookie Theft\" picture description task from the Boston Diagnostic Aphasia Examination  [73] . To evaluate the model on Pitt dataset, the speaker-independent 10-fold cross-validation technique is implemented. Similar to IEMOCAP, we evaluate at each epoch the model on the testing set and the reported results are the average scores of the 10-fold experiments DAIC-WOZ  [74] , used in AVEC 2017  [75] , is a subset of the Distress Analysis Interview Corpus (DAIC)  [74] . This dataset contains training, validation and testing sets originally, and a label depressed/not depressed is assigned to each clinical interview recording in the training and validation sets, but the labels of the test data are not provided. Therefore, we randomly select 20% of the training data for hyperparameter turning and checkpoint selection. Finally, the results on its original validation set are reported.\n\nFollowing previous works  [33] ,  [37] , we apply four evaluation metrics to evaluate the performance of different learning algorithms: weighted accuracy (WA), unweighted accuracy (UA), weighted average F1 (WF1) and macro average F1 (MF1). The above criteria can be formulated as:\n\nwhere S c denotes the number of samples of the c-th category and Acc(c) and F 1(c) are the classification accuracy and F1 score of the c-th category, respectively. For speech emotion recognition on IEMOCAP and MELD, we aim to predict the discrete emotion labels for each individual utterance. While conducting neurocognitive disorder analyses (i.e., namely, Alzheimer's disease detection on Pitt and depression classification on DAIC-WOZ), we first receive a dialogue, and we then crop out the utterances of the participant based on the provided transcription timestamps. Subsequently, the utterances are processed and predicted, and a majority vote is applied to yield a subject-level prediction, which is used for final evaluation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Details",
      "text": "1) Acoustic Features: Encouraged by the success of selfsupervised learning models in various speech tasks, we utilize the pretrained HuBERT-large  [53]  model to extract the acoustic features. Specifically, the duration of each frame processed in HuBERT is 25 ms, and the hop length used when yielding the overlapping frames is 20 ms. The overlap between consecutive frames is 5 ms. In total, 1024-dimensional frame-grained features are extracted for each utterance sample. Recently, it has been reported that the output from the middle layer has the most pronunciation-related features  [76] . Hence, we use the output from the 12-th layer of the 24-layer Transformer encoder in HuBERT. Unless otherwise stated, the pretrained self-supervised models are only used to extract the acoustic features and will not be involved in the training procedure. The max sequence lengths are set to 326, 224, 328 and 426 for IEMOCAP, MELD, Pitt and DAIC-WOZ, respectively, because 80% of samples in each dataset are shorter than the corresponding set sequence lengths. We also report the results of SpeechFormer++ with hand-crafted features, such as 80dimensional log-mel filter bank coefficients (FBANK). Unless otherwise stated, Hubert features are used in SpeechFormer++.\n\n2) Training Details: We train SpeechFormer++ in an endto-end manner using a Nvidia GeForce RTX 2080 Ti GPU. The total number of training epochs are set to 120, 120, 80 and 60 for IEMOCAP, MELD, Pitt and DAIC-WOZ, respectively, and their initial learning rates are set to 0.0005, 0.0005, 0.001 and 0.0001, respectively. The learning rate gradually drops to 1% of the original by cosine annealing. The batch size is set to 32. The model is updated by SGD with momentum 0.9. The number of attention heads used in MAS is set to 8. For the sake of simplicity, the dimensions of x i and z i , i ∈ {1, 2, 3, 4}, are all set to 1024. Unless otherwise stated, the number of layers employed in the frame stage N 1 , phone stage N 2 and word stage N 3 are 2, 2 and 4, respectively, and the number of Transformer encoders used in the utterance stage N 4 is 4. As a result, the total number of layers of SpeechFormer++ is 12.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "In this section, we report the experiments conducted on four corpora, including three recognition tasks. First, we compare the proposed SpeechFormer++ with the standard Transformer architecture in terms of performance and computational efficiency. To be consistent with the settings of SpeechFormer++,  a total of 12 Transformer encoders are used in the standard Transformer framework. Second, we present a comparison with previous works. Finally, we perform extensive ablation studies to better understand the effectiveness of each module.  I  presents the results of Transformer and the proposed SpeechFormer++ on IEMO-CAP. Our SpeechFormer++ has a slightly larger model size due to the addition of the merging blocks. However, the theoretical computational complexity (FLOPs) of SpeechFormer++ is greatly reduced (by 71.67%) compared to Transformer. Meanwhile, our model boosts performance consistently (0.705 vs. 0.685 in WA, 0.715 vs. 0.701 in UA and 0.707 vs. 0.692 in WF1), meaning that our model is efficient and effective.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Speech Emotion Recognition On Iemocap 1) Comparison To Transformer: Table",
      "text": "2) Comparison to Previous State-of-the-Art: Table  II  lists the results of SpeechFormer++ and existing works on IEMO-CAP. Our model with HuBERT fetures achieves 0.705 WA, 0.715 UA and 0.707 WF1, surpassing the previous best results. Our SpeechFormer++ with hand-crafted features outperforms STC  [14]  (0.645 vs. 0.613 in WA, 0.658 vs. 0.604 in UA and 0.649 vs. 0.617 in WF1) and achieves comparable results to LSTM-GIN  [17]  (0.645 vs. 0.647 in WA and 0.658 vs. 0.655 in UA) under the same experimental setup. SpeechFormer++ obtains inferior results compared to ISNet  [15] . We suspect this is because ISNet is equipped with a carefully designed individual benchmark to alleviate the problem of interindividual emotion confusion. In addition, speaker information is used in ISNet. Our SpeechFormer++ is a general backbone and can be employed in ISNet for further improvement.   size of SpeechFormer++ is slightly larger, the computational effort of SpeechFormer++ is reduced from 15.33G to 4.51G, which is a 70.58% relative reduction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Speech Emotion Recognition On Meld",
      "text": "2) Comparison to Previous State-of-the-Art: Table  IV  compares SpeechFormer++ with previous state-of-the-art models on MELD. It can be observed that SpeechFormer++ with HuBERT features noticeably outperforms the previous works by a large margin of +3.1% WF1 and +1.2% WA. When compared under the hand-crafted features, SpeechFormer++ outperforms ConGCN  [78] , MMFA-RNN  [19] , MM-DFN  [18]  and CTNet  [37]  in terms of WF1. Note that SpeechFormer++ is simply applied in MELD and does not utilize the context and speaker information. This demonstrates the potential of SpeechFormer++ and the possibility of further improvement.\n\nC. Alzheimer's Disease Detection on Pitt 1) Comparison to Transformer: As shown in Table  V , our SpeechFormer++ once again beats the standard Transformer framework on Pitt in terms of WA and UA while having much lower FLOPs. In detail, the results of SpeechFormer++ are +2.4 WA, +2.6 UA and +2.8 WF1 superior to the Transformer with a comparable model size (66.79M vs. 63.64M) and a significantly lower computational burden (6.58G vs. 23.28G).\n\n2) Comparison to Previous State-of-the-Art: Table  VI  gives the comparison among SpeechFormer++ with existing works on Pitt. Our method using HuBERT features outperforms other comparisons with promising gains: +7.7% WA over  [10] , +8.2% WA over  [11] , +8.8% WF1 over  [79] , +7.4% (+17.5%) WA (UA) over  [20]  and +7.3% (+7.6%) WA (UA) over  [33] . Also, our method using FBANK outperforms the competitors using hand-crafted features in terms of WA and UA.     VIII . Our method with HuBERT features outperforms currently advanced approaches by a considerable margin in all metrics. Additionally, SpeechFormer++ using FBANK features achieves state-of-the-art compared to other hand-crafted feature-based methods, drawing the improvement of 0.3%∼15.7% on WA, 7.4%∼16.8% on UA and 5.3%∼18.1% on MF1.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Depression Classification On Daic-Woz",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Comparison Under The Hubert Features",
      "text": "To release the impact of input features, we compare Speech-Former++ with other approaches using HuBERT features. Experimental results in Table IX demonstrate that Speech-Former++ shows superior performance on four corpora. For IEMOCAP, our method shows an absolute improvement of 2.9%∼4.8% on WA, 4.6%∼5.6% on UA and 4.5%∼6.4% on WF1 over other competitors. For MELD, our method shows an absolute improvement of 1.0%∼3.3% on WA, 1.9%∼3.0% on UA and 1.2%∼2.4% on WF1 over other competitors. For Pitt, our method shows an absolute improvement of 1.9%∼7.3% on WA, 2.5%∼7.6% on UA and 3.4%∼6.3% on WF1 over other competitors. For DAIC-WOZ, our method shows an absolute improvement of 5.7%∼8.5% on WA, 2.3%∼2.5% on UA and 1.5%∼3.3% on MF1 over other competitors. The reason lies in that other methods ignore the structural features of the speech signal, which is remedied in SpeechFormer++. These results verify the effectiveness of the proposed method.   [13]  0.714 0.703 0.694 ‡ EmoAudioNet  [12]  0.686 0.701 0.676 SpeechFormer++ 0.771 0.726 0.709 † Source code is not provided. We reproduce the method by ourselves and replace the input features with HuBERT. ‡ We reproduce the method using the code provided in the original paper and replace the input features with HuBERT.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "F. Ablation Study",
      "text": "In this section, we conduct a comprehensive ablation study on the four corpora to determine the role of each module in our SpeechFormer++. All ablation studies are implemented in the same configuration, except for the module under investigation. In addition, we investigate the sensitivity of SpeechFormer++ to the statistical phone and word durations. Finally, we compare SpeechFormer++ with finetuning of HuBERT to verify the importance of the downstream model.\n\n1) Effectiveness of Unit Encoder: We first replace the unit encoder with the standard Transformer encoder, which means each layer in the modified model always applies full attention among all the acoustic tokens. The merging blocks are preserved in the modified model. Since the word encoder is used to enhance the unit encoder, we also remove the word encoder when the unit encoder is disabled. The results are reported in Table  X . The computational complexity is also listed for comparison. Supported by the unit encoder, SpeechFormer++ performs better than its counterpart by +2.77% (+3.03%) WA (UA) on IEMOCAP, +2.84% WF1 on MELD, +2.14% (+1.62%) WA (UA) on pitt and +4.26% MF1 on DAIC-WOZ. Generally, it demonstrates that modeling structure-based acoustic information under the instruction of the characteristics of speech can distinctly improve the performance of recognition. In addition, the attention mechanism in the proposed unit encoder scales linearly with the sequence length, which allows our SpeechFormer++ to achieve better performance at lower computational cost.\n\n2) Effectiveness of Word Encoder: To verify the potency of the word encoder, we discard all learnable word tokens in SpeechFormer++ such that the unit encoder can only perceive the local information within the window. The results are summarized in Table  XI . The performance of the counterpart is weaker than SpeechFormer++ on four corpora, especially for IEMOCAP (0.696 vs. 0.705 in WA), MELD (0.464 vs. 0.470 in WF1), and DAIC-WOZ (0.672 vs. 0.709 in MF1). This indicates that the coarse-gained information cannot be neglected   even if the fine-gained feature is effective. The word encoder presents an efficient way to help each unit encoder consider the coarse-gained information when modeling local segments. Note that the parameters in the word encoder and unit encoder are shared, enabling the introduction of the word encoder to not increase the model size but only the computational cost.\n\nIn particular, the relative increments at FLOPs are +7.73%, +7.89%, +7.52% and +7.57% on IEMOCAP, MELD, Pitt and DAIC-WOZ, respectively.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "3) Effectiveness Of Merging Block:",
      "text": "To analyze the indispensability of the merging block, we implement a modified model with solely the unit encoder and the word encoder, where the number of input tokens is the same for each layer. In SpeechFormer++, each time the acoustic feature goes through a merging block, the number of tokens is greatly reduced, which significantly reduces the computational burden for the following layers. More concretely, as shown in  which further confirms the necessity of the merging block. 4) Sensitivity to the Statistical Durations: SpeechFormer++ is performed under the instruction of the statistical durations of speech. To investigate the sensitivity of SpeechFormer++ to the statistical phone and word durations, we intentionally set the phone and word durations longer or shorter than the statistics described in Section III-A. The experimental results are shown in Fig.  6 . When the value of the x-axis mismatch in Fig.  6  is larger than 1, the durations of phone and word used in the system are mismatch times longer than the respective statistics. On the contrary, if the mismatch is less than 1, the durations used are shorter than the statistical durations. The durations are consistent with the statistics only if the mismatch is equal to 1. The durations determine the window size in the encoder and the merging scale in the merging block, which further impact the performance and computational complexity. As shown in Fig.  6 , the FLOPs gradually decreases when the durations used increase. The accuracy of SpeechFormer++ remains generally robust when the mismatch lies between 0.9 and 1.1, suggesting that we can apply SpeechFormer++ directly to other English datasets with similar statistics. When the mismatch is larger than 1.3 or less than 0.7, the performance starts to break, especially on DAIC-WOZ. These results suggest that we should recalculate the duration of each speech unit when processing different languages or language dialects. 5) Comparison to Finetuning of Pretrained Model: When the input feature is obtained from an already pretrained model (HuBERT in this paper), the proposed SpeechFormer++ can be viewed as a downstream model for the downstream task. To investigate the importance of the downstream model, we conduct experiments to compare the performances of finetuning the pretrained model with a simple MLP (3 dense layers) and learning further deep representation with SpeechFormer++.  In addition, we implement Transformer and SpeechFormer++ with only 4 layers to investigate the impact of model size on small datasets. The experimental results in Table  XIII  show that finetuning the pretrained model with the simple MLP obtains inferior results compared to the SpeechFormer++. The reason lies in that the pretrained model utilizes general selfsupervised tasks, which do not consider the characteristics of speech. For results of SpeechFormer++ with 4 layers, we observe that it outperforms the 12 layers SpeechFormer++ on DAIC-WOZ. This is mainly because DAIC-WOZ is a smallscale dataset. Thus, only a small number of parameters are needed to fit the data. On the other three datasets, Speech-Former++ with 12 layers delivers superior performance. Note that SpeechFormer++ also outperforms Transformer when both employ only 4 layers. Our work provides a new perspective on modeling speech signals. In this paper, we choose to use 12 layers of Transformer and SpeechFormer++ across four datasets merely for the sake of simple, straightforward and consistent comparisons. In real-world applications, the number of layers employed in each stage can be tuned for optimal performance according to the training dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "G. Adopting Attention From Computer Vision",
      "text": "We have confirmed that the standard full attention mechanism from NLP is unsuitable for the PSP task. Furthermore, we are interested in the performance of adopting attention methods from computer vision, which are optimized according to the characteristics of the images. Typically, the shifted window-based algorithm Swin-T 2  [28]  and the cluster-based algorithm BOAT-T 3  [29]  are considered in this paper. Note that 2 Codes of Swin: https://github.com/microsoft/Swin-Transformer 3 Codes of BOAT: https://github.com/mahaoyuHKU/pytorch-boat we follow the same configurations as their original papers. The recognition results on four corpora are reported in Table  XIV .\n\nComputational costs are also provided for the sake of fair comparison. Unsurprisingly, the use of vision algorithms causes a significant drop in performance in the PSP tasks, as well as an increased computational burden. The results indicate that we cannot simply adapt the attention methods from other domains to speech, but instead, we need to make our own improvements based on the characteristics of the speech signal. Thus the proposed SpeechFormer++ presents a solution to fill the gap in the literature.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "H. Visualization Of Attention Weights",
      "text": "From the experimental results discussed above, we conclude that SpeechFormer++ provides better results than the standard Transformer with less computational cost. It also outperforms the previous state-of-the-art approaches by a large margin on four commonly used corpora. To further understand the model and determine the reasons behind the improvements, we consider two utterance samples in IEMOCAP and compare the attention weights in Transformer and SpeechFormer++ by visualization. Here, the attention weights indicate the importance of each token in the model, which are obtained by adding up all the weights of the same value vector in MSA. Note that the merging block in SpeechFormer++ aggregates the acoustic tokens, resulting in a different number of input tokens in different layers and difficulties in comparison. For that reason, we visualize the attention weights in the first layer of SpeechFormer++ and Transformer, where the number of input tokens is the same for both models. The visualization results of the two utterance samples are illustrated in Fig.  7 . The attention weights are limited to the range (0, 1) by softmax function. In addition, we manually mark out the content of the samples for better comprehension and analysis. For the first sample, forced breathing appears in the segment of the left bounding box, which is conducive for recognition. However, the attention weights in the Transformer indicate that the Transformer is not interested in that area and assigns relatively low weights to the corresponding tokens. Similarly, the right bounding box of the first sample is the prolongation of a particular word, which is essential for recognition but is omitted by Transformer. Our SpeechFormer++ is able to alleviate the above issues by assigning reasonable attention weights to the tokens in sample 1, where no informative content is neglected. For the second sample in Fig.  7 , the left bounding box outlines an imperceptible sigh that is completely ignored by the Transformer and accurately captured by our SpeechFormer++. With this sigh signal, the model can accomplish the recognition effectively. The right bounding box of sample 2 shows a continuous utterance in which multiple words are spoken in quick succession. The attention weights in Transformer are relatively stable, while those in SpeechFormer++ fluctuate. This is because our method is capable of capturing more detailed information. In other words, SpeechFormer++ applies rapidly changing attention weights to model the fine-grained features in a cost-effective manner.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we exploit the potential of Transformer by considering the essence of the audio properties. We reveal the implicit relationships in speech and propose a structurebased framework, called SpeechFormer++, for paralinguistic speech processing. SpeechFormer++ takes the intra-and interunit features into account while preserving the coarse-grained information to further boost the performance. In addition, the merging blocks are applied to imitate the hierarchical structure in the speech signal. Experimental results on four speechrelated corpora demonstrate that our method substantially surpasses the standard Transformer with respect to performance and efficiency. Additionally, the comparison to state-of-theart also confirms the superiority of SpeechFormer++. In the future, we intend to make use of the lexical information and develop a unified textual-audio framework. In addition, we intend to consider the semantic information in SpeechFormer++ for solving speech recognition tasks.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) (b) Graphical illustration of two typical attention variants in",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) and 1(b), to bridge the gap",
      "page": 1
    },
    {
      "caption": "Figure 1: (c), we note",
      "page": 2
    },
    {
      "caption": "Figure 1: (c). First, we perform, with",
      "page": 2
    },
    {
      "caption": "Figure 2: , consists of",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the proposed SpeechFormer++. It mainly consists of four stages and three merging blocks. Raw audio samples and randomly initialized",
      "page": 4
    },
    {
      "caption": "Figure 3: Overall duration statistics of phones (left) and words (right) in",
      "page": 4
    },
    {
      "caption": "Figure 4: Architectures of the (a) Transformer encoder, (b) unit encoder and",
      "page": 5
    },
    {
      "caption": "Figure 4: (c)) to inject the coarse-gained",
      "page": 5
    },
    {
      "caption": "Figure 4: (b), each acoustic segment is enhanced by",
      "page": 5
    },
    {
      "caption": "Figure 4: (b)) is as follows:",
      "page": 5
    },
    {
      "caption": "Figure 5: Overview of the calculation process in SpeechFormer++. In practice,",
      "page": 6
    },
    {
      "caption": "Figure 5: The acoustic tokens are",
      "page": 6
    },
    {
      "caption": "Figure 6: Performance of SpeechFormer++ under different degrees of mismatch.",
      "page": 10
    },
    {
      "caption": "Figure 6: When the value of the x-axis mismatch",
      "page": 10
    },
    {
      "caption": "Figure 6: is larger than 1, the durations of phone and word",
      "page": 10
    },
    {
      "caption": "Figure 6: , the FLOPs",
      "page": 10
    },
    {
      "caption": "Figure 7: Visualization of the normalized attention weights obtained by the",
      "page": 11
    },
    {
      "caption": "Figure 7: The attention weights are limited to the range (0, 1) by softmax",
      "page": 12
    },
    {
      "caption": "Figure 7: , the left bounding box outlines",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nFeatures\nYear": "STC [14]\nH/C\n2021\n† ISNet\n[15]\nH/C\n2022\nLSTM-GIN [17]\nH/C\n2021\nSUPERB [77]\nw2v2\n2021\nSUPERB [77]\nHuBERT\n2021\nCA-MSER [54]\nH/C + w2v2\n2022",
          "WA\nUA\nWF1": "0.613\n0.604\n0.617\n0.704\n0.650\n-\n0.647\n0.655\n-\n0.656\n-\n-\n0.676\n-\n-\n0.698\n0.711\n-"
        },
        {
          "Method\nFeatures\nYear": "SpeechFormer++\nH/C\n2022\nSpeechFormer++\nHuBERT\n2022",
          "WA\nUA\nWF1": "0.645\n0.658\n0.649\n0.705\n0.715\n0.707"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nFeatures\nYear": "GCNN [10]\nH/C\n2018\nMakiuchi\n[11]\nH/C\n2021\nAutoencoder\n[20]\nH/C\n2022\nP´erez-Toro [79]\nw2v2\n2022\nMonica [33]\nHuBERT\n2022",
          "WA\nUA\nWF1": "0.736\n-\n-\n0.731\n0.731\n0.732\n0.739\n0.641\n0.621\n-\n-\n0.720\n0.740\n0.740\n0.745"
        },
        {
          "Method\nFeatures\nYear": "SpeechFormer++\nH/C\n2022\nSpeechFormer++\nHuBERT\n2022",
          "WA\nUA\nWF1": "0.742\n0.738\n0.727\n0.813\n0.816\n0.808"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nFeatures\nYear": "† ConGCN [78]\nH/C\n2019\nMMFA-RNN [19]\nH/C\n2020\n† MM-DFN [18]\nH/C\n2022\n† CTNet\n[37]\nH/C\n2021\nSharma [57]\nw2v2\n2022",
          "WA\nUA\nWF1": "-\n-\n0.422\n0.488\n-\n0.423\n-\n-\n0.427\n0.469\n-\n0.382\n0.498\n-\n-"
        },
        {
          "Method\nFeatures\nYear": "SpeechFormer++\nH/C\n2022\nSpeechFormer++\nHuBERT\n2022",
          "WA\nUA\nWF1": "0.480\n0.236\n0.429\n0.510\n0.273\n0.470"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "SUPERB [77]\n† STC [14]\n† LSTM-GIN [17]",
          "WA\nUA\nF1": "0.676\n-\n-\n0.657\n0.659\n0.643\n0.661\n0.669\n0.662"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer++",
          "WA\nUA\nF1": "0.705\n0.715\n0.707"
        },
        {
          "Dataset": "MELD",
          "Method": "‡ MM-DFN [18]\n† MMFA-RNN [19]",
          "WA\nUA\nF1": "0.477\n0.254\n0.458\n0.500\n0.243\n0.446"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer++",
          "WA\nUA\nF1": "0.510\n0.273\n0.470"
        },
        {
          "Dataset": "Pitt",
          "Method": "Monica [33]\n† Makiuchi\n[11]",
          "WA\nUA\nF1": "0.740\n0.740\n0.745\n0.794\n0.791\n0.774"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer++",
          "WA\nUA\nF1": "0.813\n0.816\n0.808"
        },
        {
          "Dataset": "DAIC-WOZ",
          "Method": "SIMSIAM-S [82]\n† FVTC-CNN [13]\n‡ EmoAudioNet\n[12]",
          "WA\nUA\nF1": "0.703\n-\n-\n0.714\n0.703\n0.694\n0.686\n0.701\n0.676"
        },
        {
          "Dataset": "",
          "Method": "SpeechFormer++",
          "WA\nUA\nF1": "0.771\n0.726\n0.709"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method\nFeatures\nYear": "FVTC-CNN [13]\nH/C\n2020\nEmoAudioNet\n[12]\nH/C\n2021\nSaidi\n[80]\nH/C\n2020\nSolieman [81]\nH/C\n2021\nSIMSIAM-S [82]\nHuBERT\n2022\nTOAT [56]\nw2v2\n2022",
          "WA\nUA\nMF1": "0.735\n0.656\n0.640\n0.732\n0.649\n0.653\n0.680\n0.680\n0.680\n0.660\n0.615\n0.610\n0.703\n-\n-\n0.717\n0.429\n0.480"
        },
        {
          "Method\nFeatures\nYear": "SpeechFormer++\nH/C\n2022\nSpeechFormer++\nHuBERT\n2022",
          "WA\nUA\nMF1": "0.754\n0.733\n0.743\n0.771\n0.726\n0.709"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "† MLP (FT)\n(cid:92) Transformer",
          "IEMOCAP\nWA\nUA": "0.677\n0.689\n0.680\n0.699",
          "MELD\nWF1": "0.455\n0.457",
          "Pitt\nWA\nUA": "0.793\n0.789\n0.798\n0.793",
          "DAIC-WOZ\nMF1": "0.676\n0.665"
        },
        {
          "Method": "‡ SpeechFormer++\nSpeechFormer++",
          "IEMOCAP\nWA\nUA": "0.695\n0.702\n0.705\n0.715",
          "MELD\nWF1": "0.468\n0.470",
          "Pitt\nWA\nUA": "0.805\n0.805\n0.813\n0.816",
          "DAIC-WOZ\nMF1": "0.746\n0.709"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Introduction. The perception of speech: from sound to meaning",
      "authors": [
        "B Moore",
        "L Tyler",
        "W Marslen-Wilson"
      ],
      "year": "2008",
      "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences"
    },
    {
      "citation_id": "2",
      "title": "Speech parameter generation from HMM using dynamic features",
      "authors": [
        "K Tokuda",
        "T Kobayashi",
        "S Imai"
      ],
      "year": "1995",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Wavelet-based statistical signal processing using hidden Markov models",
      "authors": [
        "M Crouse",
        "R Nowak",
        "R Baraniuk"
      ],
      "year": "1998",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 International Conference on Multimedia and Expo. ICME '03. Proceedings"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in speech signal using emotion-extracting binary decision trees",
      "authors": [
        "J Cichosz",
        "K Slot"
      ],
      "year": "2007",
      "venue": "Proceedings of affective computing and intelligent interaction"
    },
    {
      "citation_id": "6",
      "title": "Decision tree based depression classification from audio video and language information",
      "authors": [
        "L Yang",
        "D Jiang",
        "L He",
        "E Pei",
        "M Oveneke",
        "H Sahli"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, ser. AVEC '16"
    },
    {
      "citation_id": "7",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Conditional restricted Boltzmann machine for voice conversion",
      "authors": [
        "Z Wu",
        "E Chng",
        "H Li"
      ],
      "year": "2013",
      "venue": "2013 IEEE China Summit and International Conference on Signal and Information Processing"
    },
    {
      "citation_id": "9",
      "title": "Acoustic emotion recognition based on fusion of multiple feature-dependent deep Boltzmann machines",
      "authors": [
        "K Poon-Feng",
        "D.-Y Huang",
        "M Dong",
        "H Li"
      ],
      "year": "2014",
      "venue": "The 9th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Detecting Alzheimer's disease using gated convolutional neural network from audio data",
      "authors": [
        "T Warnita",
        "N Inoue",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Speech paralinguistic approach for detecting dementia using gated convolutional neural network",
      "authors": [
        "M Makiuchi",
        "T Warnita",
        "N Inoue",
        "K Shinoda",
        "M Yoshimura",
        "M Kitazawa",
        "K Funaki",
        "Y Eguchi",
        "T Kishimoto"
      ],
      "year": "2021",
      "venue": "IEICE Transactions on Information and Systems"
    },
    {
      "citation_id": "12",
      "title": "Towards robust deep neural networks for affect and depression recognition from speech",
      "authors": [
        "A Othmani",
        "D Kadoch",
        "K Bentounes",
        "E Rejaibi",
        "R Alfred",
        "A Hadid"
      ],
      "year": "2021",
      "venue": "Pattern Recognition. ICPR International Workshops and Challenges"
    },
    {
      "citation_id": "13",
      "title": "Exploiting vocal tract coordination using dilated CNNS for depression detection in naturalistic environments",
      "authors": [
        "Z Huang",
        "J Epps",
        "D Joachim"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "ISNet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Adaptive domain-aware representation learning for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "D Huang"
      ],
      "venue": "Adaptive domain-aware representation learning for speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "Graph isomorphism network for speech emotion recognition",
      "authors": [
        "J Liu",
        "H Wang"
      ],
      "year": "2021",
      "venue": "Graph isomorphism network for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "An automatic Alzheimer's disease classifier based on spontaneous spoken English",
      "authors": [
        "F Bertini",
        "D Allevi",
        "G Lutero",
        "L Calzà",
        "D Montesi"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "21",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Long short term memory recurrent neural network based multimodal dimensional emotion recognition",
      "authors": [
        "L Chao",
        "J Tao",
        "M Yang",
        "Y Li",
        "Z Wen"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, ser. AVEC '15"
    },
    {
      "citation_id": "23",
      "title": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "M Niu",
        "M Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop, ser. AVEC'18"
    },
    {
      "citation_id": "24",
      "title": "Multichannel speech separation with recurrent neural networks from high-order ambisonics recordings",
      "authors": [
        "L Perotin",
        "R Serizel",
        "E Vincent",
        "A Guérin"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "A novel attention-based gated recurrent unit and its efficacy in speech emotion recognition",
      "authors": [
        "S Rajamani",
        "K Rajamani",
        "A Mallol-Ragolta",
        "S Liu",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "BOAT: Bilateral local attention vision transformer",
      "authors": [
        "T Yu",
        "G Zhao",
        "P Li",
        "Y Yu"
      ],
      "year": "2022",
      "venue": "BOAT: Bilateral local attention vision transformer",
      "arxiv": "arXiv:2201.13027"
    },
    {
      "citation_id": "30",
      "title": "Not all tokens are equal: Human-centric visual analysis via token clustering transformer",
      "authors": [
        "W Zeng",
        "S Jin",
        "W Liu",
        "C Qian",
        "P Luo",
        "W Ouyang",
        "X Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "ResT: An efficient transformer for visual recognition",
      "authors": [
        "Q Zhang",
        "Y.-B Yang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "DynamicViT: Efficient vision transformers with dynamic token sparsification",
      "authors": [
        "Y Rao",
        "W Zhao",
        "B Liu",
        "J Lu",
        "J Zhou",
        "C.-J Hsieh"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "A comparison of feature-based classifiers and transfer learning approaches for cognitive impairment recognition in language",
      "authors": [
        "G Monica",
        "M Rafael"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence in Neuroscience: Affective Analysis and Health Applications"
    },
    {
      "citation_id": "34",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "X Wang",
        "M Wang",
        "W Qi",
        "W Su",
        "X Wang",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection",
      "authors": [
        "K Chen",
        "X Du",
        "B Zhu",
        "Z Ma",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "SpeechFormer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "SpeechFormer: A hierarchical efficient framework incorporating the characteristics of speech"
    },
    {
      "citation_id": "40",
      "title": "Teaching machines to read, answer and explain",
      "authors": [
        "Y Cui",
        "T Liu",
        "W Che",
        "Z Chen",
        "S Wang"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Improving multimodal named entity recognition via entity span detection with unified multimodal transformer",
      "authors": [
        "J Yu",
        "J Jiang",
        "L Yang",
        "R Xia"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Gaussian transformer: A lightweight approach for natural language inference",
      "authors": [
        "M Guo",
        "Y Zhang",
        "T Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "SBERT-WK: A sentence embedding method by dissecting BERT-based word models",
      "authors": [
        "B Wang",
        "C.-C Kuo"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "44",
      "title": "Hi-Transformer: Hierarchical interactive transformer for efficient and effective long document modeling",
      "authors": [
        "C Wu",
        "F Wu",
        "T Qi",
        "Y Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "45",
      "title": "DeepViT: Towards deeper vision transformer",
      "authors": [
        "D Zhou",
        "B Kang",
        "X Jin",
        "L Yang",
        "X Lian",
        "Z Jiang",
        "Q Hou",
        "J Feng"
      ],
      "year": "2021",
      "venue": "DeepViT: Towards deeper vision transformer",
      "arxiv": "arXiv:2103.11886"
    },
    {
      "citation_id": "46",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "An audio-saliency masking transformer for audio emotion classification in movies",
      "authors": [
        "Y.-T Wu",
        "J.-L Li",
        "C.-C Lee"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Detecting dementia from speech and transcripts using transformers",
      "authors": [
        "L Ilias",
        "D Askounis",
        "J Psarras"
      ],
      "year": "2021",
      "venue": "Detecting dementia from speech and transcripts using transformers",
      "arxiv": "arXiv:2110.14769"
    },
    {
      "citation_id": "49",
      "title": "WavBERT: Exploiting semantic and non-semantic speech using wav2vec and BERT for dementia detection",
      "authors": [
        "Y Zhu",
        "A Obyat",
        "X Liang",
        "J Batsis",
        "R Roth"
      ],
      "year": "2021",
      "venue": "WavBERT: Exploiting semantic and non-semantic speech using wav2vec and BERT for dementia detection"
    },
    {
      "citation_id": "50",
      "title": "Multi-modal adaptive fusion transformer network for the estimation of depression level",
      "authors": [
        "H Sun",
        "J Liu",
        "S Chai",
        "Z Qiu",
        "L Lin",
        "X Huang",
        "Y Chen"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "51",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "52",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "53",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "54",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "56",
      "title": "A topic-attentive transformerbased model for multimodal depression detection",
      "authors": [
        "Y Guo",
        "C Zhu",
        "S Hao",
        "R Hong"
      ],
      "year": "2022",
      "venue": "A topic-attentive transformerbased model for multimodal depression detection",
      "arxiv": "arXiv:2206.13256"
    },
    {
      "citation_id": "57",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Self-attention transducers for end-to-end speech recognition",
      "authors": [
        "Z Tian",
        "J Yi",
        "J Tao",
        "Y Bai",
        "Z Wen"
      ],
      "year": "2019",
      "venue": "Self-attention transducers for end-to-end speech recognition"
    },
    {
      "citation_id": "59",
      "title": "Transformer-based acoustic modeling for hybrid speech recognition",
      "authors": [
        "Y Wang",
        "A Mohamed",
        "D Le",
        "C Liu",
        "A Xiao",
        "J Mahadeokar",
        "H Huang",
        "A Tjandra",
        "X Zhang",
        "F Zhang",
        "C Fuegen",
        "G Zweig",
        "M Seltzer"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Conformer: Convolutionaugmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu",
        "R Pang"
      ],
      "venue": "Conformer: Convolutionaugmented transformer for speech recognition"
    },
    {
      "citation_id": "61",
      "title": "Automatic assessment of depression from speech via a hierarchical attention transfer network and attention autoencoders",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang",
        "J Deng",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Y Gu",
        "K Yang",
        "S Fu",
        "S Chen",
        "X Li",
        "I Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "63",
      "title": "WISE: Word-level interaction-based multimodal fusion for speech emotion recognition",
      "authors": [
        "G Shen",
        "R Lai",
        "R Chen",
        "Y Zhang",
        "K Zhang",
        "Q Han",
        "H Song"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "64",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2021",
      "venue": "Learning fine-grained cross modality excitement for speech emotion recognition"
    },
    {
      "citation_id": "65",
      "title": "Learning mutual correlation in multimodal transformer for speech emotion recognition",
      "authors": [
        "Y Wang",
        "G Shen",
        "Y Xu",
        "J Li",
        "Z Zhao"
      ],
      "year": "2021",
      "venue": "Learning mutual correlation in multimodal transformer for speech emotion recognition"
    },
    {
      "citation_id": "66",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "C Gulcehre",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "67",
      "title": "Understanding adaptive, multiscale temporal integration in deep speech recognition systems",
      "authors": [
        "M Keshishian",
        "S Norman-Haignere",
        "N Mesgarani"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "68",
      "title": "Speaker identification on the scotus corpus",
      "authors": [
        "J Yuan",
        "M Liberman"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "69",
      "title": "Layer normalization",
      "authors": [
        "J Lei Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "70",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "71",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "72",
      "title": "The natural history of Alzheimer's disease: Description of study cohort and accuracy of diagnosis",
      "authors": [
        "J Becker",
        "F Boiler",
        "O Lopez",
        "J Saxton",
        "K Mcgonigle"
      ],
      "year": "1994",
      "venue": "Archives of Neurology"
    },
    {
      "citation_id": "73",
      "title": "The Boston diagnostic aphasia examination",
      "authors": [
        "K Goodglass"
      ],
      "year": "1983",
      "venue": "Lea & Febinger"
    },
    {
      "citation_id": "74",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella",
        "D Traum",
        "S Rizzo",
        "L.-P Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "75",
      "title": "AVEC 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "76",
      "title": "What all do audio transformer models hear? Probing acoustic representations for language delivery and its structure",
      "authors": [
        "J Shah",
        "Y Singla",
        "C Chen",
        "R Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? Probing acoustic representations for language delivery and its structure",
      "arxiv": "arXiv:2101.00387"
    },
    {
      "citation_id": "77",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H Yi Lee"
      ],
      "venue": "SUPERB: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "78",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "79",
      "title": "Alzheimer's detection from English to Spanish using acoustic and linguistic embeddings",
      "authors": [
        "P Pérez-Toro",
        "P Klumpp",
        "A Hernandez",
        "T Arias",
        "P Lillo",
        "A Slachevsky",
        "A García",
        "M Schuster",
        "A Maier",
        "E Noeth",
        "J Orozco-Arroyave"
      ],
      "year": "2022",
      "venue": "Alzheimer's detection from English to Spanish using acoustic and linguistic embeddings"
    },
    {
      "citation_id": "80",
      "title": "Hybrid CNN-SVM classifier for efficient depression detection system",
      "authors": [
        "A Saidi",
        "S Othman",
        "S Saoud"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Systems and Emergent Technologies"
    },
    {
      "citation_id": "81",
      "title": "The detection of depression using multimodal models based on text and voice quality features",
      "authors": [
        "H Solieman",
        "E Pustozerov"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering"
    },
    {
      "citation_id": "82",
      "title": "On combining global and localized self-supervised models of speech",
      "authors": [
        "S Dumpala",
        "C Sastry",
        "R Uher",
        "S Oore"
      ],
      "year": "2022",
      "venue": "On combining global and localized self-supervised models of speech"
    },
    {
      "citation_id": "83",
      "title": "South China University of Technology. His research interests include speech emotion recognition, multimodal emotion recognition, and deep learning in speech processing",
      "year": "2001",
      "venue": "Xiaofen Xing (Member, IEEE) received the B.S., M.S., and Ph.D. degrees from South China University of Technology"
    }
  ]
}