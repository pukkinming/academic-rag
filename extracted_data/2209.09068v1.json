{
  "paper_id": "2209.09068v1",
  "title": "Audio-Visual Fusion For Emotion Recognition In The Valence-Arousal Space Using Joint Cross-Attention",
  "published": "2022-09-19T15:01:55Z",
  "authors": [
    "R Gnana Praveen",
    "Eric Granger",
    "Patrick Cardinal"
  ],
  "keywords": [
    "Dimensional Emotion Recognition",
    "Deep Learning",
    "Multimodal Fusion",
    "Joint Representation",
    "Cross-Attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition (ER) has recently gained lot of interest due to its potential in many real-world applications. In this context, multimodal approaches have been shown to improve performance (over unimodal approaches) by combining diverse and complementary sources of information, providing some robustness to noisy and missing modalities. In this paper, we focus on dimensional ER based on the fusion of facial and vocal modalities extracted from videos, where complementary audio-visual (A-V) relationships are explored to predict an individual's emotional states in valence-arousal space. Most state-of-the-art fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. To address this problem, we introduce a joint cross-attentional model for A-V fusion that extracts the salient features across A-V modalities, that allows to effectively leverage the inter-modal relationships, while retaining the intra-modal relationships. In particular, it computes the cross-attention weights based on correlation between the joint feature representation and that of the individual modalities. By deploying the joint A-V feature representation into the cross-attention module, it helps to simultaneously leverage both the intra and inter modal relationships, thereby significantly improving the performance of the system over the vanilla cross-attention module. The effectiveness of our proposed approach is validated experimentally on challenging videos from the RECOLA and AffWild2 datasets. Results indicate that our joint cross-attentional A-V fusion model provides a cost-effective solution that can outperform state-of-the-art approaches, even when the modalities are noisy or absent. Code is available at https://github.com/praveena2j/Joint-Cross-Attention-for-Audio-Visual-Fusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A UTOMATIC recognition and analysis of human emo- tions has drawn much attention over the past few decades. It has been extensively researched in various fields such as neuroscience, psychology, cognitive science and computer science, leading to the advancement of a wide range of applications in various fields, such as health care (e.g., assessment of anger, fatigue, depression and pain), robotics (human-machine interaction), driver assistance (assessment of driver's state), etc  [1] . Emotion recognition (ER) is a challenging problem since the expressions linked to human emotions are extremely diverse in nature across individuals and cultures. Ekman conducted cross-cultural study on human emotions, and categorized the basic emotions into six categories -anger, disgust, fear, happy, sad, and surprise  [2] . Subsequently, contempt has been added to these six basic emotions  [3] . The categorical model of ER has been explored extensively in the field of affective computing due to its simplicity and universality  [4] . Recently, real-world applications have driven a shift of affective computing research from laboratory-controlled environments to more realistic natural settings. This shift has further led to the analysis of wide range of subtle, continuous emotional and health Fig.  1:  The valence-arousal space. Valence denotes the range of emotions from being very sad (negative) to very happy (positive) and arousal reflects the energy or intensity of emotions from very passive to very active.\n\nstates that are elicited in real-world settings. Conventionally, the estimation of continuous ER states are formulated as the dimensional ER problem, where complex human emotions can be represented in a dimensional valence-arousal space. Figure  7  illustrates the use of a two-dimensional space to represent emotional states, where valence and arousal are employed as dimensional axes  [5] . Valence reflects the wide range of emotions in the dimension of pleasantness from arXiv:2209.09068v1 [cs.CV] 19 Sep 2022\n\nbeing negative (sad) to positive (happy), whereas arousal spans the range of intensities from passive (sleepiness) to active (high excitement)  [6] . Recognizing such fine-grained emotional states is beneficial in various applications, such as assessing driver fatigue, estimating the level of depression or pain in health-care, assessing customer engagement in marketing, etc. Given the growing need for continuous ER in real-world applications, this paper focuses on dimensional ER in the valence-arousal space. Human emotions can be conveyed through various modalities such as face, voice, text and physiology (electroencephalogram, electrocardiogram, etc.), which typically carry complementary information among them. Although human emotions can be expressed through various modalities, vocal and facial modalities are the predominant contactfree channels, which carries complementary information  [7] . Audio-visual (A-V) fusion has also been widely explored for various applications including identity verification  [8] , event localization  [9] , action recognition  [10] , etc. Efficiently leveraging the complementary nature of A-V relationships captured in videos can play a crucial role in improving the performance of multimodal systems over unimodal systems  [11] . Techniques for multimodal fusion can be broadly categorized as model-agnostic or model-based  [12] . In modelbased approaches, fusion is performed using specialized models to cope with the diverse information in multimodal data. Depending on the type of model used for fusion, these techniques are typically classified further as kernel methods, graphical models, or neural networks  [12] . Unlike model based fusion, model agnostic fusion can be achieved using almost any uni-modal classifier or regressor. They do not rely on any specialized model for fusion. Most of the existing fusion models belongs to this category, where fusion is often performed by concatenating the features or individual modal predictions. Model agnostic approaches can be further classified as three major strategies: decision-, feature-, hybrid-level fusion  [13] . In decision-level fusion (late fusion), multiple modalities are trained end-to-end independently, and then the predictions obtained from the individual modalities are fused to obtain the final predictions. Although decision-level fusion is easy to implement, and requires less training, it neglects the interactions across the individual modalities, thereby resulting in limited improvement over uni-modal approaches. Conventionally, featurelevel fusion (early fusion) is achieved by concatenating the features of A-V modalities immediately after they are extracted, which is further used for predicting the final outputs. Hybrid fusion takes advantage of both decisionlevel and feature-level fusion by combining outputs from both feature-level fusion and decision level fusion. Though feature level fusion is conventionally done by aggregating or concatenating the features immediately after they are extracted, it can also be performed by learning the interactions between the modalities for better feature representations before concatenating the features  [14] ,  [15] . In this work, we explore feature-level fusion based on joint cross-attention, where the A and V features extracted from videos are further modeled using a joint cross-attention model prior to concatenation.\n\nDeep learning (DL) models provide state-of-the-art performance in many V recognition applications, such as im-age classification, object detection, action recognition, etc. Inspired by their performance, several ER approaches have been proposed for video-based dimensional ER using CNNs to obtain the deep features, and a recurrent neural network to capture the temporal dynamics  [16] ,  [17] . Deep models have also been widely explored for vocal emotion recognition, typically using spectrograms with 2D-CNNs  [16] ,  [18] , or raw wave forms with 1D-CNNs  [17] . In most of the existing approaches  [17] ,  [19]  for dimensional ER, A-V fusion is performed by concatenating the deep features extracted from individual facial and vocal modalities, and fed to LSTM for predicting valence and arousal. Although LSTM based fusion models can improve the system performance by leveraging the intra-modal relationships, it does not effectively capture the inter-modal relationships across the individual modalities. We therefore investigate the prospect of extracting more comprehensive salient features that can effectively exploit the complementary relationships across the A and V modalities.\n\nAttention mechanisms have recently gained much interest in the areas of computer vision and machine learning as they allow extracting task relevant features, thereby improving system performance. This has been extensively explored for various applications, such as event/action recognition  [20] , ER  [21] , etc. Most of the existing attention based approaches for dimensional ER explore the intramodal relationships  [21] . Although a few approaches  [19] ,  [22]  attempt to capture the cross-modal relationships using cross-attention based on transformers, they fail to effectively leverage the complementary relationship of A-V modalities. Indeed, their computation of attention weights does not consider the correlation across the A and V features.\n\nA preliminary version of the cross-attentional (CA) A-V fusion model for dimensional ER was presented in our previous work  [15] . In this work, we further extend our previous work, where a joint A-V feature representation is deployed in the CA model in order to jointly capture both intra and inter modal relationships. In this previous paper  [15] , the attention weights are computed based on the correlation across A and V modalities, which depends only on intermodal relationships. Instead of using individual feature representations across the modalities to generate the attention weights, we introduce joint A-V feature representations to capture the relationships within the same modality as well as other modality, thereby leveraging both inter-and intra-modal relationships to obtain the attention weights. Using the joint feature representation drastically reduces the heterogeneity across the A and V features, which further helps to provide robust A-V feature representations. Specifically, we obtain the cross-correlation matrix across the deep joint feature representation and features of individual modalities to obtain the attention weights for the A and V modalities. Therefore, the attention weights of each modality is obtained not only using the features of itself but also from the other modality, resulting in more informative features. Besides providing improved performance over individual modalities, a benefit of our joint A-V representation is its ability to perform well even when a modality is noisy or absent. Finally, we have also explored the impact on JCA performance of feature-level fusion, where multiple divers backbones are combined for the A and V modalities.\n\nThe main contributions of the paper are: (1) A joint cross-attentional (JCA) model for A-V fusion is introduced to effectively exploit the complementary relationship across modalities for dimensional ER in valence-arousal space. Contrary to the prior approaches, the proposed model simultaneously leverages both intra and inter modal relationships to effectively capture the complementary relationships. (2) By deploying the joint feature representation it also helps to reduce the heterogeneity across A and V features, thereby resulting in robust AV feature representations. (3) An extensive set of experiments on the challenging RECOLA and Affwild2 datasets indicate that our proposed JCA fusion model can outperform related state-of-the-art fusion models for dimensional ER. Our visual interpretation of the fusion process shows that JCA can efficiently leverages the complementary intermodal relationships, while retaining the intramodal relationships.\n\nThe rest of this paper is organized as follows. Section II provides a critical analysis of the relevant literature on dimensional ER, and attention models for A-V fusion. Section III describes the proposed JCA A-V fusion model in detail. Section IV presents the experimental methodology for the backbones of the individual modalities, and the experimental settings used in our fusion model. Finally, the results obtained with the proposed approach with RECOLA and Affwild2 datasets are presented and discussed in Section V.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work 2.1 A-V Fusion For Dimensional Emotion Recognition:",
      "text": "One of the early approaches using DL models for A-V fusion based dimensional ER was proposed by Tzirakis et al.  [17] , where A and V features are obtained using ResNet50 and 1D-CNN respectively. The obtained features are then concatenated and fed to Long short-term memory model (LSTM) for the prediction of valence and arousal. Juan et al.  [23]  investigated an empirical study of fine-tuning pretrained CNN models by freezing various convolutional layers. Schonevald et al.  [16]  explored knowledge distillation using teacher-student model for V modality and CNN model for A modality using spectrograms. The deep feature representations are combined using model-based fusion strategy, where RNNs are used to capture the temporal dynamics. Inspired by the deep auto-encoders, Nguyen et al.  [24]  investigated the prospect of how to simultaneously learn compact representative features from A and V modalities using deep auto-encoders. They have proposed a deep model of two-stream auto-encoders and LSTM for efficiently integrating V and A streams for dimensional ER.\n\nDeng et al  [25]  proposed iterative self distillation method for modeling the uncertainties in the labels in a multitask framework. They have trained a model with multiple task labels, which is further used to distill iteratively to several student models. They have shown that iterative distillation significantly improves the performance of the system. Kuhnke et al.  [26]  proposed two stream A-V network, where V features are extracted from R(2plus1)D model  [27]  pretrained from action recognition dataset and A features are obtained from Resnet18 model  [28] . The obtained features are further concatenated for final prediction of valence and arousal. Wang et al  [18]  further improved their approach  [26]  by introducing teacher-student model in a semi-supervised learning framework. The teacher model is trained on the available labels, which is further used to obtain pseudo labels for unlabeled data. The pseudo labels are finally used to train the student model, which is used for final prediction. Though the above mentioned approaches have shown significant improvement for dimensional ER, they fail to effectively capture the inter-modal relationships and relevant salient features specific to the task. Therefore, we have focused on capturing the comprehensive features in a complementary fashion using attention mechanisms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Attention Models For A-V Fusion:",
      "text": "Attention mechanisms are widely used in the context of multimodal fusion with various modalities such as A and text  [29] ,  [30] , V and text  [31] ,  [32] , etc. Zhao et al.  [33]  proposed an end-to-end architecture for emotion classification by integrating spatial, channel-wise and temporal attentions into V network and temporal attention into A network. Esam et al.  [34]  explored attention to weigh the time windows of a video sequence to efficiently exploit the temporal interactions between the A-V modalities. They used transformer  [35]  based encoders to obtain the attention weights through self attention for emotion classification. Lee et al.  [21]  proposed spatiotemporal attention for the V modality to focus on emotional salient parts using Convolutional LSTM (ConvLSTM) modules and a temporal attention network using deep networks for A modality. Then the attended features are concatenated and fed to the regression network for the prediction of valence and arousal. However, these approaches focused on modeling the intra-modal relationships and failed to effectively exploit the inter-modal relationship of the A-V modalities.\n\nWang et al.  [36]  investigated the prospect of exploiting the implicit contextual information along with the A and V modalities. They have proposed an end-to-end architecture using cross-attention based on transformers for A-V group ER. Srinivas et al.  [22]  also explored transformers with cross-modal attention for dimensional ER, where crossattention is integrated along with self attention. Tzirakis et al.  [19]  investigated various fusion strategies along with attention mechanisms for A-V fusion based dimensional ER. They have further explored self attention as well as crossattention fusion based on transformers in order to enable the extracted features of different modalities to attend to each other. Although these approaches have explored crossmodal attention with transformers, they fail to leverage semantic relevance among the A-V features based on crosscorrelation.\n\nZhang et al.  [14]  investigated the prospect of improving the fusion performance over individual modalities and proposed leader-follower attentive fusion for dimensional ER. The obtained features are encoded and attention weights are obtained by combining the encoded A and V features. The attention weights are further attended on the V features and concatenated to the original V features for final prediction. Zhang et al.  [37]  proposed attentive fusion mechanism, where V features are obtained from 3D-CNNs and A features from spectrograms fed to 2D-CNN. The obtained A and V features are further re-weighted using weights, obtained from scoring functions based on the relevant information in the individual modalities. Wang et al.  [38]  addressed the problem of multi-modal feature fusion along with frame alignment issues between A and V modalities using cross-attention for speech recognition. Luo et al.  [39]  investigated the potential of joint representation learning using Convolutional Recurrent Neural Networks (CRNN) for vocal ER. They have also shown that the impact of time interval significantly impacts the performance of the system. Hu et al  [40]  proposed dense multi-modal fusion by densely integrating the representation at multiple shared layers to capture hierarchical correlations across the modalities. Vedran et al  [41]  proposed a cross-modal deep network architecture, where the weights of two deep networks are enforced to be symmetry, yielding joint representation in a common feature space. In this work, we have used simple joint representation of feature concatenation of A and V modalities in our JCA framework.\n\nUnlike prior approaches, we advocate for a simple yet efficient JCA model based on joint modeling of intra and inter modal relationships between A and V modalities. Cross-attention has been successfully applied in several applications, such as weakly-supervised action localization  [10] , and few-shot classification  [42] . The similar idea of exploiting the complementary relationships for better audiovisual fusion has also been explored for person verification  [11] , where an attention layer is used for the fusion of A and V modalities. In most of these cases, cross-attention has been applied across the individual modalities. However, we have explored joint attention between individual and combined AV-features. By deploying the joint AV feature representation, we can effectively capture the intra and inter-modal relationships simultaneously by allowing interactions across the modalities as well as within oneself. Recently, joint co-attention has been explored by Duan et al.  [9]  in a recursive fashion for A-V event localization. They have shown that recursive training of joint co-attention yields more discriminant and robust feature representations for multimodal fusion. In this paper, joint (combined) A-V features are extracted through cross-attention (instead of co-attention) for dimensional ER. Specifically, the features of each modality attend to themselves, as well as those of the other modality, through cross-correlation of the concatenated A-V features, and features of individual modalities. By effectively leveraging the joint modeling of intra-and inter-modal relationships, the proposed approach can significantly improve system performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Network:",
      "text": "Facial expressions from videos involve both appearance and temporal dynamics of video sequences. Efficient modeling of these spatial and temporal dynamics play a crucial role in extracting discriminant and robust features, which inturn improves the overall system performance. State-ofthe-art performance is typically achieved using 2D-CNN in combination with Recurrent Neural Networks (RNN) to capture the effective latent appearance representation, along with temporal dynamics  [43] . Several approaches have been explored for dimensional facial ER based on 2D-CNNs and LSTMs  [6] ,  [44] . However, 3D-CNNs are found to be efficient in capturing the spatiotemporal dynamics in videos  [45] , and have also been explored for dimensional facial ER. For instance, in  [26] , they have shown that R3D  [27]  pretrained on the Kinetics-400 action recognition dataset  [46]  has outperformed conventional 2D-CNNs for dimensional ER on Affwild2 dataset. Inspired by the performance of 3D-CNNs, we consider Inflated 3D-CNN  [47] , to extract spatiotemporal features of the facial clips from a video sequence. Initially, proposed by Carreira et al.  [47]  for action recognition, the Inflated 3D (I3D) CNN model can efficiently capture the spatiotemporal dynamics of the V modality while optimizing fewer parameters than that of conventional 3D-CNNs. I3D model is obtained by inflating the filters and pooling kernels of 2D ConvNet, expanding to 3D CNN. Therefore, it allows leveraging existing common pretrained 2D-CNNs, which are trained on large-scale image datasets for facial expressions, thereby improving the spatial discrimination for videos. Though I3D model has been primarily explored for action recognition, it has also been used for other applications in the field of affective computing, like in videobased pain localization  [48] , etc. In the proposed approach, we train the I3D model to extract spatio-temporal features for the facial modality (see implementation details in Section 4.2).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Audio Network:",
      "text": "The para-lingual information of vocal signals was found to convey significant information on the emotional state of a person. Even though vocal ER has been widely explored using the conventional handcrafted features, such as Melfrequency cepstral coefficients (MFCCs)  [49] , there has been a significant improvement over the recent years with the introduction of DL models. Though deep vocal ER models can be explored using spectrograms with 2D-CNNs  [16] ,  [18] , as well as raw A signal with 1D-CNNs  [17] , spectrograms are found to carry significant para-lingual information pertaining to the affective state of a person  [50] ,  [51] . Spectrograms have been explored with various 2D-CNNs in the literature for ER  [52] ,  [53] . Therefore, we consider spectrograms in the proposed framework along with 2D-CNN models to extract A features. In particular, Resnet18  [28]  was used for Affwild2 dataset, and the A model as shown in Table  I  for RECOLA dataset. Given the differences in the size of the datasets, we have used different 2D-CNN models for RECOLA and Affwild2 in order to avoid over-fitting. (see implementation details in Section 4.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature-Level Fusion Of Multiple Backbones:",
      "text": "We have also explored the fusion of features extracted from multiple backbones for both A and V modalities. Deploying multiple backbones for each modality can allow to capture diverse information for a same modality. Specifically, we have extracted V features from I3D, R3D, and 2D CNN in conjunction with Long Short Term Memory (LSTM). I3D and R3D are 3D CNN models, used to simultaneously capture the spatiotemporal relationships, which is efficient at capturing the short-term temporal relations. 2D CNN with LSTM extracts spatial features, and performs temporal modeling, which captures the long-term temporal relationships. Similarly, for A modality, combined features from a 2D CNN trained on spectrograms, and conventional handcrafted MFCC features, widely used in speech processing for many applications.\n\nThen we have considered two different feature-level fusion strategies to obtain a feature representation for each modality. First, we concatenate the features from all the backbones, followed by fully connected layer in order to produce a compact joint representation based on multiple diverse backbones. Feature concatenation followed by fully connected layer has been widely used in the literature for many applications. The second strategy is a more specialized feature stacking approach, where the features extracted from of multiple divers backbones and from a sequence are assembled into a block of features, and then processed by the A-V fusion model. This approach eliminates the need for training an additional fully connected layer to combine features, as all features are trained within the fusion model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Joint Cross-Attentional (Jca) Av-Fusion:",
      "text": "Though A-V fusion can be achieved through unified multimodal training, it was found that multimodal performance often declines over that of individual modalities  [54] . This has been attributed to a number of factors, such as differences in learning dynamics for A and V modalities  [54] , different noise topologies, with some modality streams containing more or less information for the task at hand, as well as specialised input representations  [55] . Therefore, we have trained DL models for the individual A and V modalities independently in order to extract A and V features, which is further fed to the JCA fusion model for A-V fusion that outputs final valence and arousal prediction.\n\nFor a given video sequence, the V modality carries relevant information in some video clips, whereas A modality might be more relevant for others. Since, multiple modalities convey diverse and complementary information for valence and arousal than a single modality, their complementarity can be effectively through A and V fusion. In order to reliably fuse these modalities, we rely on cross-attention based fusion mechanism to efficiently encode the intermodal information, while preserving the intra-modal characteristics. Though cross-attention has been conventionally applied across the features of individual modalities, we have explored cross-attention in a joint framework. Specifically, our joint A-V feature representation is obtained by concatenating the A and V features to attend to the individual A and V features. By using the joint representation, features of each modality attend to oneself, as well as the other modality, helping to capture the semantic inter-modal relationships across A and V. The heterogeneity among the A and V modalities can also be drastically reduced by using the combined feature representation in the cross-attentional module, which further improves system performance. A block diagram of the proposed model is shown in Figure  2 .\n\nA) Training mode: Let X a and X v represents two sets of deep feature vectors extracted for the A and V modalities, in response to a given input video sub-sequence S of fixed size, where\n\nL denotes the number of non overlapping fixed-size clips sampled uniformly from S, d a and d v represents the dimension of the A and V feature representations, respectively, and x l a and x l v denotes the A and V feature vectors, respectively, for l = 1, 2, ..., L clips. Instead of applying cross-attention across the features of individual A and V modalities, we use cross-attention in a joint learning framework. The joint representation of A-V features, J , is obtained by concatenating the A and V feature vectors:\n\nwhere d = d a + d v denotes the feature dimension of concatenated features.\n\nThe concatenated A-V feature representations (J ) of the given video sub-sequence (S) is now used to attend to unimodal feature representations X a and X v . The joint correlation matrix C a across the A features X a , and the combined A-V features J are given by:\n\nwhere W ja ∈ R L×L represents learnable weight matrix across the A and combined A-V features, and T denotes transpose operation. Similarly, the joint correlation matrix for V features are given by:\n\nThe joint correlation matrices C a and C v for A and V modalities provide a semantic measure of relevance not only across the modalities but also within the same modality. Higher correlation coefficient of the joint correlation matrices C a and C v shows that the corresponding samples are strongly correlated within the same modality as well as other modality. Therefore, the proposed approach is able to efficiently leverage the complementary nature of A and V modalities (i.e., inter-modal relationship) as well as intramodal relationships, thereby improving the performance of the system. After computing the joint correlation matrices, the attention weights of A and V modalities are estimated.\n\nSince the dimensions of joint correlation matrices (R da×d ) and the features of corresponding modality (R L×da ) differ, we rely on a different learnable weight matrices corresponding to features of the individual modalities, and the corresponding joint correlation matrices, in order to compute attention weights of the modalities. For the A modality, the joint correlation matrix C a and the corresponding A features X a are combined using the learnable weight matrices W ca and W a respectively to compute the attention weights of A modality, which is given by\n\nwhere W ca ∈ R k×d , W a ∈ R k×L and H a represents the attention maps of the A modality. Similarly, the attention maps (H v ) of V modality are obtained as\n\nwhere\n\nIn our experiments, we have considered k to be 32. Finally, the attention maps are used to compute the attended features of A and V modalities. These features are obtained as:\n\nwhere W ha ∈ R k×L and W hv ∈ R k×L denote the learnable weight matrices, respectively. The attended A and V features, X att,a and X att,v are further concatenated to obtain the A-V feature representation, which is given by:\n\nFinally, the A-V features are fed to the fully connected layers for the predictions of valence and arousal.\n\nThe Concordance Correlation Coefficient (ρ c ) has been widely used in the literature to measure the level of agreement between the predictions (x) and ground truth (y) annotations for dimensional ER  [17] . Let µ x and µ y represents the mean of predictions and ground truth, respectively. Similarly, if σ 2\n\nx and σ 2 y denotes the variance of predictions and ground truth, respectively, then ρ c between the predictions and ground truth is:\n\nwhere σ 2 xy denotes the predictions -ground truth covariance. Although MSE has been widely used as a loss function for regression models, we use L = 1 -ρ c since it is standard and conventional loss function in the literature of dimensional ER literature  [17] . The parameters of our A-V fusion model (W ca , W a , W cv , W v , W ha , and W hv ) are optimized according to this loss. B) Test mode: As shown in Figure  2 , we assume that a continuous video sequence is input to our model during inference. Feature representations x a and x v are extracted by A and V backbones for successive input clips and spectrograms, and fed to the fusion model for the prediction of valence and arousal.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Methodology",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets:",
      "text": "The proposed architecture is validated on REmote COLlaborative and Affective (RECOLA)  [56]  and AffWild2  [57] . RECOLA: In total, this dataset consists of 9.5 hours of multimodal recordings, which is recorded by 46 French -speaking participants, performing a collaborative task during a video conference. Among the participants, 17 are French, 3 are German and 3 are Italian. The video sequences are divided into sequences of 5 minutes each, which is annotated with a regressed intensity value for every 40 msec by 6 French speaking annotators (three male and three female). The dataset is split into three partitions: train (16 subjects), validation (15 subjects) and test (15 subjects) by balancing the age and gender of the speakers. Due to the uncontrolled spontaneous nature of expressions of the subjects, the dataset has been widely used by the research community in affective computing for various challenges such as AVEC 2015  [58] , AVEC 2016  [59] , etc. Most of the existing approaches in the literature, e.g.,  [16] ,  [17] , have validated on the dataset used for AVEC 2016  [59]  challenge, which consists of 9 subjects for training, and 9 subjects for validation. Therefore, we have also validated our proposed approach on the dataset used in AVEC 2016 challenge. Affwild2: Affwild2 is the largest dataset in the field of affective computing, consisting of 564 videos collected from YouTube, all captured in-the-wild  [57] . Sixteen of these videos display two subjects, both of which have been annotated. The annotations are provided by four experts using a joystick and the final annotations are obtained as the average of the four raters. In total, there are 2, 816, 832 frames with 455 subjects, out of which 277 are male and 178 female. The annotations for valence and arousal are provided continuously in the range of [-1, 1]. The dataset is split into the training, validation and test sets. The partitioning is done in a subject independent manner, so that every subject's data will present in only one subset. The partitioning produces 341, 71, and 152 videos for the training, validation, and test sets respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details:",
      "text": "RECOLA: For the V modality, the faces are extracted and pre-processed from the video sequences of the dataset using MTCNN model  [60] , a deep cascaded multi-task framework of face detection and alignment. Faces are resized to 224 × 224 to be fed to the I3D model  [47] . In order to generate more samples, the videos of the dataset are converted to sequences of 128 frames with a subsequence length of 16, resulting in 21,284 training samples and 16,177 validation samples. I3D uses the Inception v1 architecture as the base model as shown in Table  1 , which is pretrained on Kinetics-400 dataset  [46] , and the inflated to a 3D-CNN using RECOLA videos of facial expressions. Typically, pooling operation is performed on the last convolutional layer(512 × 7 × 7) to reduce the spatial dimension to size 1 (7 → 1), however, it may leave out useful information. Therefore, scaled dot product of audio and visual features are performed to smoothly reduce the dimension of raw visual features as in  [9] . For regularizing the network, dropout is used with p = 0.8 on the linear layers. The initial learning rate of the network was set to be 1e -4 and the momentum of 0.9 is used for Stochastic Gradient Descent (SGD). Also weight decay of 5e -4 is used. Due to the hardware limitations and memory constraints, the batch size of the network is set to be 8. Data augmentation is performed on the training data by random cropping, which produces scale invariant model. The number of epochs is set to be 50 and early stopping is used to obtain the best network weights. The A network is composed of 3 blocks where the initial weights of the network are initialized with values from normal distribution. The number of epochs are set to be 100, and early stopping is used. The network is optimized using SGD with momentum of 0.9. The initial learning rate is set to be 0.001 and batch size is fixed to be 16. Due to the limited data, the network might be prone to over-fitting. Therefore, in order to prevent the network from over-fitting, dropout is used with p = 0.5 after the last linear layer. Also weight decay of 5e -4 is used for all the experiments. For the A-V fusion network, the size of A-V features are set to be 1024. In the joint cross-attention module, the initial weights of the cross-attention matrix is initialized with Xavier method  [61] , and the weights are updated using Adam optimizer. The initial learning rate is set to be 0.001 and batch size is fixed to be 16. Also, dropout of 0.5 is applied on the attended A-V features and weight decay of 5e-4 is used for all the experiments. Due to the spontaneity of the expressions, the annotations are also found to be highly stochastic in nature. Therefore, post processing steps are applied to predictions and labels. A rigorous analysis on some of the post processing steps for annotations appears in  [62] . Tzirakis et al.  [17]  explored a series of post processing steps for validating their architecture on the RECOLA. Inspired by their approach, we have followed similar post processing steps to validate our architecture: (i) median filtering with the window size ranging from 0.4sec to 20sec; (ii) centering the predicted values by computing the bias between annotated (ground truth) values and predicted values; (iii) matching the scaling of predicted values and annotations using the ratio of standard deviation of annotated values and predicted values. (iv) time shifting the annotations forward in time with values ranging from 0.04 to 10sec to compensate for delay in human annotations (delay in correspondence between the annotated values and the video frames). Affwild2: For the V modality, we have used the cropped and aligned images provided by the challenge organizers  [63] . For the missing frames in the V modality, we have considered black frames (i.e., zero pixels). Faces are resized to 224x224 to be fed to the I3D network. The subsequence length and the sequence length of the videos are considered to be 8 and 64 respectively, obtained by down-sampling a sequence of 256 frames by 4. Therefore, we have 8 subsequences in each sequence, resulting in 1,96,265 training samples and 41,740 validation samples and 92,941 test samples. Similar to RECOLA dataset, I3D model was pretrained on Kinetics-400 dataset  [46] , and inflated to a 3D-CNN using expressions. Instead of conventional pooling layer after the last convolutional layer, we have used scaled dot product of audio and visual features similar to that of  [9] . To regularize the network, dropout is used with p = 0.8 on the linear layers. The initial learning rate was set to be 1e -3, and the momentum of 0.8 is used for SGD. Weight decay of 5e -4 is used.\n\nHere again, the batch size of the network is set to be 8. Data augmentation is performed on the training data by random cropping, which produces scale invariant model. The number of epochs is set to be 50 and early stopping is used to obtain the best weights of the network.\n\nFor the A modality, the vocal signal is extracted from the corresponding video, and re-sampled to 44100Hz, which is further segmented to short vocal segments corresponding a sub-sequence of 256 frames of the V network. The spectrogram is obtained using Discrete Fourier Transform (DFT) of length 1024 for each short segment, where the window length is considered to be 20 msec and the hop length to be 10 msec. Following aggregation of short-time spectra, we obtain the spectrogram of 64 x 107 corresponding to each sub-sequence of the V modality. Now a normalization step is performed on the obtained spectrograms. The spectrogram is converted to log-power-spectrum, expressed in dB. Finally, mean and variance normalization is performed on the spectrogram. Now the obtained spectrograms are fed to the Resnet18  [28]  to obtain the A features. Due to the availability of the large number of samples in the Affwild2 dataset, we trained the Resnet18 model from scratch. In order to adapt to the number of channels of the spectrogram, the first conv. layer in the Resnet18 model is replaced by single channel. The network is trained with an initial learning rate of 0.001 and weights are optimized using Adam optimizer. The batch size is considered to be 64 and early stopping is used to obtain the best model for prediction. For the A-V fusion network, we have used the similar training strategy as with the RECOLA dataset. RECOLA: Table  3  presents the results of our ablation study on the RECOLA validation dataset. In order to analyze the performance of our joint cross-attention model for A-V fusion, we compare with various fusion strategies widely used in the literature. One of those fusion strategies is LSTM based fusion, where the A and V features are concatenated and fed to the LSTM followed by linear layers.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results And Discussion",
      "text": "We have extracted V features (frame-level) using VGG 2D-CNN architecture, pretrained on FER dataset similar to  [23] , and further fine-tuned on RECOLA. Initially, we compare the proposed approach without LSTM, where the A and V features are concatenated and directly fed to linear layers. LSTM model-based fusion is evaluated by feeding the concatenated features to LSTM layer followed by fully connected layers. Given the temporal modeling of the concatenated features, the fusion performance improves over the non-LSTM based fusion strategy. We also compare the performance to I3D using baseline concatenation, where the A-V features are concatenated without attention, and fed to linear layers for valence/arousal prediction (similar to that of the fusion in  [23] ). We have further compared the performance improvement of joint cross-attention fusion over that of conventional CA fusion  [15] . In case of conventional CA fusion, attention weights are computed based on the cross-correlation across the A and V modalities. The attention weights encode the semantic relevance across the A and V features. However, they do not allow the features to interact within the same modality, thereby failing to capture the temporal modeling within the same modality. Though temporal modeling across the modalities captures inter-modal relationships, and can improve the state-of-art accuracy, retaining the temporal modeling within the same modality also plays a pivotal role to capture intra-modal relationships. Therefore, we have integrated the modeling within A and V modalities, along with modeling of intermodal relationships, and further improve system performance. Since we have introduced joint feature representation in the proposed JCA fusion model, it simultaneously captures both intra-and inter-modal relationships, and thereby outperforms the conventional CA fusion in  [15] , along with most of the widely used fusion strategies. Affwild2: Table  4  presents the results of our ablation study on the Affwild2 validation dataset. The performance of out proposed JCA fusion was compared using various A and V backbones and A-V fusion strategies. Since we used I3D for the V modality, we have compared against a V backbone based on 3D (2plus1d) CNNs  [23] . First, we implemented the backbone of TSAV  [23]  with simple feature concatenation, where the extracted A and V features are concatenated, and fed to fully connected layers for valence and arousal prediction. Our proposed model provides a significant improvement in the performance. We have also analyzed when our V backbone (I3D) is used with baseline feature concatenation and leader-follower fusion based attention  [16]  with our backbones, and found that there is significant improvement in the performance over that of baseline feature concatenation. We have also implemented the conventional CA fusion  [15]  with I3D backbone. Although its performance improves over that of baseline feature concatenation, it shows lower performance than leader follower attention  [16] . Finally, we have compared the proposed JCA fusion with I3D, and found that it outperforms other fusion strategies in the literature on Affwild2. By allowing the features of each modality to interact with itself and other modality, we can be effectively capture the semantic relevance of intra-and inter-modal relationships of A and V modalities for dimensional ER. We can also observe that the performance of our proposed A-V fusion model with TSAV  [23]  slightly outperforms that of JCA fusion with I3D. We have further validated the proposed fusion model with multiple backbones of V and A modalities and showed further improvement in the performance of the system. We have also explored multiple backbones for A and V modalities along with the proposed fusion model and further improved the performance of the system. As discussed in Section 3.1 and 3.2 for V and A modalities respectively, we have used I3D, R3D and 2D CNN in conjunction with LSTM to obtain spatiotemporal features for V modality. Similarly, we have used MFCCs and spectrograms with 2D CNNs for A modality. The features of multiple backbones are fused using feature concatenation followed by fully connected layer and stacking of features in order to obtain comprehensive features for both A and V modalities. Features from multiple backbones helps to obtain diverse information of each modality and thereby improves the performance of the system. The proposed AV JCA fusion model is validated with the fusion of features from multiple backbones for both A and V modalities and the results are shown in Table  5 . We have also evaluated our proposed approach for the case where a growing proportion of A is replaced by background noise in test mode. Specifically, we have randomly replaced some segments/spectrograms to reflect background noise in the video. We have tested our system on Affwild2 with a video named \"16-30-1920x1080.mp4\" with 5475 frames, and varied the percentage of missing spectrograms by 10, 25, 50 and 100%. Even though spectrograms are noisy and absent, we can clearly observe that there is modest minimal decline in CCC performance (see in  Fig 3) . In particular, since we can effectively encode the complementary relationship across modalities (by jointly modeling of intra-and inter-modal relationships), our models can sustain a high level of performance for valence.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Comparison To The State-Of-Art:",
      "text": "RECOLA: Table  6  presents our comparative results against state-of-the-art A-V fusion models on the RECOLA development set. He et al.  [64]  explored handcrafted LPQ-TOP features for V, and low-level descriptors (LLDs) such as MFCC, energy, etc. for A, along with physiological modalities like electro-cardiogram (ECG) and electro-dermal activity (EDA). Given the use of additional physiological modalities, and more LLD descriptors in A, as well as additional geometric features of V, the fusion performance provides significant improvement. Han et al.  [65]  explored LLD features for A, and facial landmark features (only geometric) for V, combined in a hierarchical fashion to leverage the individual advantages of support vector regressor (SVR) and Bidirectional Long Short-Term Memory Networks (BLSTM), and improved the performance for valence. Inspired by performance of DL models, Tzirakis et al.  [17]  explored Resnet50 2D-CNN for V, and 1D-CNN on raw data for A. However, the features are directly concatenated, and fed to  LSTMs. This results in a decline in CCC performance over individual modalities. The performance has been further improved by Juan et al  [23] , where they pre-train a CNN on FER for V, and LLD for A. Recently, Schoneval et al.  [16]  used knowledge distillation for V, and a VGG network on spectrograms for A. Instead of direct concatenation, they rely on two independent CNNs before concatenating them, and showed that their fusion outperforms over individual modalities. Though deep models have improved the performance over handcrafted features, they fail to effectively leverage the complementary nature of the A-V modalities. By effectively leveraging the intra and inter-modal relationships of A and V features, the proposed model outperforms state-of-the-art approaches using joint cross-attention.\n\nAffwild2: Table  7  shows our comparative results against relevant state-of-the-art A-V fusion models on the Affwild2 dataset. In the recent years, most of the existing work on the Affwild2 dataset have been submitted to the Affective Behavior Analysis in-the-wild (ABAW) challenges  [66] ,  [67] . Therefore, we compare our proposed approach with that of the top relevant approaches appearing in ABAW challenges for A-V fusion. However, the experimental protocol and training data varies widely among these approaches. We therefore re-implemented these approaches according to our experimental protocol, and analyzed the results on Affwild2 validation set for fair comparison. Similar to our A and V backbones, Kuhnke et al  [26]  also used 3D-CNNs, where R(2plus1)D model is used for visual modality and Resnet18 is used for audio modality. However, they perform simple feature concatenation without any specialized fusion model for the prediction of valence and arousal. So the fusion performance was not significantly improved over the uni-modal performance. Zhang et al  [14]  explored leader follower attention model for fusion and showed minimal improvement of fusion performance over uni-modal perfor-mances. Though they have shown significant performance for arousal than valence, it is highly attributed to the visual backbone. In our proposed approach, we have shown significant improvement for fusion especially for valence than arousal. Even with vanilla CA fusion  [15] , we have shown that fusion performance for valence has been improved better than  [14]  and  [26] . By deploying joint representation into the cross attententional fusion model, the fusion performance of valence has been significantly improved further.\n\nIn case of arousal, though the fusion performance is lower than that of  [14]  and  [26] , we can observe that it has been improved better than that of uni-modal visual performance. Therefore, the proposed approach is effective in capturing the variations spanning over a wide-range of emotions (valence) than that of the intensities of the emotions (arousal). Table  8  shows the results of our approach against relevant state-of-the-art A-V fusion models on the Affwild2 test set. In the recent years, several challenges such as FG2020  [66] , ICCV2021  [67]  have been performed on the Affwild2 dataset as it has been the largest in-the-wild dataset in the field of affective computing. Kuhnke et al.  [23]  proposed Fig.  4 : Visualization of attention scores of our proposed A-V fusion (JCA) and CA  [15]  models on video named \"317\" of Affwild2 dataset. a two stream A-V network by using R(2plus1)D  [27]  for V stream, and Resnet18  [28]  for A stream. They have also used additional masks as external inputs to guide the spatial attention of the V modality and label filtering based on multi task labels to deal with the noisy annotations of valence and arousal. Wang et al  [18]  further extended their approach to perform semi-supervised learning. However they use the annotations of other ABAW challenge tasks (expression classification and action unit classification) to filter the noisy labels of valence and arousal, as well as to estimate pseudo labels for the unlabeled samples. Deng et al.  [25]  proposed an iterative distillation method for modeling the uncertainty of annotations of valence and arousal and showed significant improvement in the performance. However, they have used iterative distillation of student models, which is computationally expensive as well as labels of other tasks to model the uncertainty of valence/arousal labels. Zhang et al.  [14] , Meng et al.  [68]  and Vincent et al.  [69]  are the only approaches, which does not use the labels of additional tasks. Meng et al.  [68]  has shown significant improvement in the performance by using three external datasets along with multiple backbones of A and V modalities, whereas  [14]  and  [69]  uses only Affwild2 dataset similar to ours. The proposed approach performs at par with that of  [14]  and better than that of  [69]  in terms of valence.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visual Analysis",
      "text": "We have further validated the proposed approach using interpretability analysis by visualizing the attention scores of A and V modalities. In the proposed approach, we have primarily exploited the temporal attention within the same modality, as well as across the A-V modalities. So the cliplevel attention scores help us to intuitively understand the important clips in the video, where the fusion attention model is focuses on the temporal sequence of A and V modalities. In order to highlight the improvement of the proposed approach w.r.t. that of the vanilla CA model  [15] , we have also plotted the attention scores of the proposed JCA model along with that of  [15] . It can be observed that the proposed JCA model is able to effectively capture the importance modalities, as well as the temporal importance within the modalities. For instance, as shown in tion score for clips when the person elicits knitted brows and significant facial muscle movement near his mouth, whereas  [15]  fails to capture those important clips of the V modality. In both cases, we can observe that JCA assigns higher attention score to the corresponding modality when there is significant temporal variation (i.e., facial expression or tone changes), whereas the vanilla CA model  [15]  fails to focus on the some of the important clips of V modality. Since the proposed JCA model leverages both the intramodal and intermodal relationships, it is able to effectively leverage the contextual information among A and V modalities.\n\nTherefore, the proposed model can efficiently exploit the importance of modalities as well as temporal importance within the modalities, resulting in better performance than that of  [15] .\n\nThough the proposed JCA fusion model can outperform  [15] , we observe lower performance for arousal than with valence, on both RECOLA and Affwild2 datasets. Since the proposed model considers the intra and inter variations in computing the attention scores, JCA fusion sometimes becomes mislead by assigning higher attention scores for neutral frames, and lower attention scores for more relevant clips when there is significant occlusion, blur or pose variations in the temporal sequence of the V modality. For instance, as shown in Fig  6 , the proposed model assigns higher attention score for neutral clips, but lower attention scores for clips with more relevant facial expressions due to blur and strong pose variations. In addition to the visualization of attention scores of A and V modalities, we also visualize the valence and arousal predictions over time for videos of the Affwild2 dataset. The proposed JCA model is able to capture the contextual relationships between A and V modalities better than that of  [15] , which helps to achieve better performance. As shown in Fig  7 , we can observe that both the JCA and vanilla cross-attention models  [15]  are able to track the ground truth for valence and arousal. Yet, when a fully frontal face is not available (due to pose variations), the predictions of proposed JCA model closely follow the ground truth more closely than that of  [15] , especially for valence.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, JCA A-V fusion model is explored for videobased dimensional ER. Contrary to the prior approaches, we leverage the intra-and inter-modal relationships across the A and V features in a unified framework. Specifically, the complementary relationship between A and V features are efficiently captured based on the correlation between the joint A-V feature representations and individual A and V features while retaining the intra-modal relationships. By jointly modeling the inter and inter-modal relationships, features of each modality attend to the other modality as well as itself, resulting in robust A and V feature representations. With the proposed model, A and V backbones are first trained individually for facial (V) and vocal (A) modalities. Then, an attention mechanism based on correlation between joint and individual features are applied to obtain the attended A and V features. Finally, the attention weighted features are concatenated, and fed to linear connected layers to predict valence and arousal values. The proposed A-V fusion model is validated experimentally on the challenging RECOLA and Affwild2 video datasets, using different A and V backbones, and different proportions of missing A segments during testing mode. Results show that the proposed model is a cost-effective approach that can outperform the state-of-the-art. It encodes inter-modal relationships, while sustaining a high level of performance, even when A segments are noisy and absent. Although the JCA AV fusion model has been proposed for dimensional emotion recognition, it can also be explored for other applications pertinent to audio visual fusion such as identity verification, event localization, etc.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The valence-arousal space. Valence denotes the",
      "page": 1
    },
    {
      "caption": "Figure 7: illustrates the use of a two-dimensional space to",
      "page": 1
    },
    {
      "caption": "Figure 2: Joint cross-attention model proposed for A-V fusion (in testing mode).",
      "page": 5
    },
    {
      "caption": "Figure 2: A) Training mode: Let Xa and Xv represents two sets of",
      "page": 5
    },
    {
      "caption": "Figure 2: , we assume that a continuous video sequence",
      "page": 6
    },
    {
      "caption": "Figure 3: ). In particular, since we can effectively encode the",
      "page": 9
    },
    {
      "caption": "Figure 3: Performance of our proposed A-V fusion (JCA)",
      "page": 9
    },
    {
      "caption": "Figure 4: Visualization of attention scores of our proposed A-V fusion (JCA) and CA [15] models on video named ”317” of",
      "page": 11
    },
    {
      "caption": "Figure 5: Visualization of attention scores of our proposed A-V fusion (JCA) and CA [15] models on video named ”video92”",
      "page": 11
    },
    {
      "caption": "Figure 5: , we observe that the proposed model assigns higher atten-",
      "page": 11
    },
    {
      "caption": "Figure 6: Visualization of attention scores of our proposed A-V fusion (JCA) and CA [15] models on video named",
      "page": 12
    },
    {
      "caption": "Figure 7: Visualization of valence and arousal predictions over time for our proposed A-V fusion (JCA) and Cross-Attention",
      "page": 12
    },
    {
      "caption": "Figure 6: , the proposed model assigns",
      "page": 12
    },
    {
      "caption": "Figure 7: , we can observe that",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , which is pre-",
      "data": [
        {
          "Input": "Block 1",
          "-": "Conv : 64, 5x5, 1x2\nMax pool : 4x4, 4x4",
          "1 x 128 x 129": "64 x 31 x 15"
        },
        {
          "Input": "Block 2",
          "-": "Conv : 128, 5x5, 1x2\nConv : 256, 3x3, 1x1\nMax pool : 2x2, 2x2",
          "1 x 128 x 129": "256 x 15 x 4"
        },
        {
          "Input": "Block 3",
          "-": "Conv : 512, 5x5, 1x1\nConv : 1024, 3x3, 1x1\nAvg pool : 13x2, 1x1",
          "1 x 128 x 129": "1024 x 1 x 1"
        },
        {
          "Input": "Block 4",
          "-": "Linear : in = 1024, out = 256",
          "1 x 128 x 129": "256x1"
        },
        {
          "Input": "Block 5",
          "-": "Linear : in = 256, out = 1",
          "1 x 128 x 129": "1x1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: , which is pre-",
      "data": [
        {
          "Input": "Block 1",
          "-": "Conv : 64, 7x7x7, 2x2x2\nMax pool : 1x3x3, 1x2x2",
          "3 x 8 x 224 x 224": "64 x 7 x 112 x 112"
        },
        {
          "Input": "Block 2",
          "-": "Conv : 192, 3x3x3, 1x2x2\nMax pool : 3x3x3, 1x2x2",
          "3 x 8 x 224 x 224": "192 x 7 x 56 x 56"
        },
        {
          "Input": "Block 3",
          "-": "2 x Inception Module\nMax pool : 3x3x3, 1x2x2",
          "3 x 8 x 224 x 224": "480 x 6 x 28 x 28"
        },
        {
          "Input": "Block 4",
          "-": "5 x Inception Module\nMax pool : 3x3x3, 1x2x2",
          "3 x 8 x 224 x 224": "832 x 2 x 14 x 14"
        },
        {
          "Input": "Block 5",
          "-": "2 x Inception Module\nAvg pool : 2x7x7, 1x2x2",
          "3 x 8 x 224 x 224": "1024 x 1 x1 x 1"
        },
        {
          "Input": "Block 6",
          "-": "Linear : in = 1024, out = 256",
          "3 x 8 x 224 x 224": "256x1"
        },
        {
          "Input": "Block 7",
          "-": "Linear : in = 256, out = 1",
          "3 x 8 x 224 x 224": "1x1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2D-CNN + Feature Concatenation": "2D-CNN + LSTM",
          "0.538": "0.552",
          "0.680": "0.697"
        },
        {
          "2D-CNN + Feature Concatenation": "I3D + Feature Concatenation",
          "0.538": "0.579",
          "0.680": "0.732"
        },
        {
          "2D-CNN + Feature Concatenation": "I3D + Cross-Attention [15]",
          "0.538": "0.687",
          "0.680": "0.831"
        },
        {
          "2D-CNN + Feature Concatenation": "I3D + Joint Cross-Attention (JCA)",
          "0.538": "0.728",
          "0.680": "0.842"
        },
        {
          "2D-CNN + Feature Concatenation": "I3D; R3D; 2DCNN + JCA",
          "0.538": "0.762",
          "0.680": "0.891"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: presents our comparative results against",
      "data": [
        {
          "TSAV [23] + Feature Concatenation": "TSAV [23] + Joint Cross-Attention (Ours)",
          "0.531": "0.642",
          "0.493": "0.592"
        },
        {
          "TSAV [23] + Feature Concatenation": "I3D + Feature Concatenation",
          "0.531": "0.498",
          "0.493": "0.452"
        },
        {
          "TSAV [23] + Feature Concatenation": "I3D + Leader-Follower Fusion [16]",
          "0.531": "0.592",
          "0.493": "0.521"
        },
        {
          "TSAV [23] + Feature Concatenation": "I3D + Cross-Attention [15]",
          "0.531": "0.541",
          "0.493": "0.517"
        },
        {
          "TSAV [23] + Feature Concatenation": "I3D + Joint Cross-Attention (Ours)",
          "0.531": "0.657",
          "0.493": "0.580"
        },
        {
          "TSAV [23] + Feature Concatenation": "I3D; R3D; 2DCNN + JCA",
          "0.531": "0.725",
          "0.493": "0.614"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 7: shows our comparative results against",
      "data": [
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Han et al. [65], IVU 2017 – A: LLDs + SM; V: geometric features + S.M.",
          "0.400": "0.480",
          "0.441": "0.592",
          "0.609": "0.554",
          "0.800": "0.760",
          "0.587": "0.350",
          "0.747": "0.685"
        },
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Tzirakis et al. [17], JSTSP 2017 – A: 1D-CNN; V: Resnet50",
          "0.400": "0.428",
          "0.441": "0.637",
          "0.609": "0.502",
          "0.800": "0.786",
          "0.587": "0.371",
          "0.747": "0.731"
        },
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Ortega et al. [23], SMC 2019 – A:LLDs; V: 2D-CNN",
          "0.400": "-",
          "0.441": "-",
          "0.609": "0.565",
          "0.800": "-",
          "0.587": "-",
          "0.747": "0.749"
        },
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Schoneval et al. [16], PRL 2021 – A: Finetuned VGGish; V: Distilled CNN",
          "0.400": "0.460",
          "0.441": "0.550",
          "0.609": "0.630",
          "0.800": "0.800",
          "0.587": "0.570",
          "0.747": "0.810"
        },
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Rajasekhar et al [15], FG 2021 – A: 2D-CNN; V: I3D",
          "0.400": "0.463",
          "0.441": "0.642",
          "0.609": "0.687",
          "0.800": "0.822",
          "0.587": "0.582",
          "0.747": "0.831"
        },
        {
          "He et al. [64], AVEC 2015 – A: LLDs; V: LLDs": "Joint Cross-Attention (Ours) – A: 2D-CNN; V: I3D",
          "0.400": "0.463",
          "0.441": "0.642",
          "0.609": "0.728",
          "0.800": "0.822",
          "0.587": "0.582",
          "0.747": "0.842"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: shows our comparative results against",
      "data": [
        {
          "Meng et al. [68]": "Kuhnke et al. [26]",
          "0.606": "0.448",
          "0.596": "0.417",
          "0.601": "0.432"
        },
        {
          "Meng et al. [68]": "Zhang et al. [14]",
          "0.606": "0.463",
          "0.596": "0.492",
          "0.601": "0.477"
        },
        {
          "Meng et al. [68]": "Wang et al. [18]",
          "0.606": "0.478",
          "0.596": "0.498",
          "0.601": "0.488"
        },
        {
          "Meng et al. [68]": "Deng et al. [25]",
          "0.606": "0.533",
          "0.596": "0.454",
          "0.601": "0.493"
        },
        {
          "Meng et al. [68]": "Vincent et al. [69]",
          "0.606": "0.418",
          "0.596": "0.407",
          "0.601": "0.413"
        },
        {
          "Meng et al. [68]": "JCA (Ours)",
          "0.606": "0.451",
          "0.596": "0.389",
          "0.601": "0.420"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A Kołakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wr Óbel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications"
    },
    {
      "citation_id": "2",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "3",
      "title": "More evidence for the universality of a contempt expression",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "4",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif Intell Rev"
    },
    {
      "citation_id": "5",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "6",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Audiovisual information fusion in human-computer interfaces and intelligent environments: A survey",
      "authors": [
        "S Shivappa",
        "M Trivedi",
        "B Rao"
      ],
      "year": "2010",
      "venue": "Proc. of the IEEE"
    },
    {
      "citation_id": "8",
      "title": "Audio-visual person verification",
      "authors": [
        "S Ben-Yacoub",
        "J Luttin",
        "K Jonsson",
        "J Matas",
        "J Kittler"
      ],
      "year": "1999",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Audiovisual event localization via recursive fusion by joint co-attention",
      "authors": [
        "B Duan",
        "H Tang",
        "W Wang",
        "Z Zong",
        "G Yang",
        "Y Yan"
      ],
      "year": "2021",
      "venue": "WACV"
    },
    {
      "citation_id": "10",
      "title": "Cross-attentional audiovisual fusion for weakly-supervised action localization",
      "authors": [
        "J.-T Lee",
        "M Jain",
        "H Park",
        "S Yun"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "11",
      "title": "Noise-tolerant audio-visual online person verification using an attention-based neural network fusion",
      "authors": [
        "S Shon",
        "T.-H Oh",
        "J Glass"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "12",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA Trans. on Signal and Information Processing"
    },
    {
      "citation_id": "14",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "S Zhang",
        "Y Ding",
        "Z Wei",
        "C Guan"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "15",
      "title": "Cross attentional audio-visual fusion for dimensional emotion recognition",
      "authors": [
        "G Rajasekhar",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "16",
      "title": "Leveraging recent advances in deep learning for audio-visual emotion recognition",
      "authors": [
        "L Schoneveld",
        "A Othmani",
        "H Abdelkawy"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "17",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE J. of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "A multi-task mean teacher for semi-supervised facial affective behavior analysis",
      "authors": [
        "L Wang",
        "S Wang",
        "J Qi",
        "K Suzuki"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "19",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Weakly-supervised action localization by generative attention modeling",
      "authors": [
        "B Shi",
        "Q Dai",
        "Y Mu",
        "J Wang"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "Audio-visual attention networks for emotion recognition",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "K Sohn"
      ],
      "year": "2018",
      "venue": "Workshop on Audio-Visual Scene Understanding for Immersive Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "J Ortega",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "SMC"
    },
    {
      "citation_id": "24",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "D Deng",
        "L Wu",
        "B Shi"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "26",
      "title": "Two-stream auralvisual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "FG Workshop"
    },
    {
      "citation_id": "27",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text",
      "authors": [
        "Y Lee",
        "S Yoon",
        "K Jung"
      ],
      "year": "2020",
      "venue": "Multimodal Speech Emotion Recognition Using Cross Attention with Aligned Audio and Text"
    },
    {
      "citation_id": "30",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "A Patil"
      ],
      "year": "2020",
      "venue": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks"
    },
    {
      "citation_id": "31",
      "title": "Visual question answering with memory-augmented networks",
      "authors": [
        "C Ma",
        "C Shen",
        "A Dick",
        "Q Wu",
        "P Wang",
        "A Hengel",
        "I Reid"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "Multi-modality cross attention network for image and sentence matching",
      "authors": [
        "X Wei",
        "T Zhang",
        "Y Li",
        "Y Zhang",
        "F Wu"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "33",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "34",
      "title": "Multimodal attentionmechanism for temporal emotion recognition",
      "authors": [
        "E Ghaleb",
        "J Niehues",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "ICIP"
    },
    {
      "citation_id": "35",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "36",
      "title": "Implicit knowledge injectable cross attention audiovisual model for group emotion recognition",
      "authors": [
        "Y Wang",
        "J Wu",
        "P Heracleous",
        "S Wada",
        "R Kimura",
        "S Kurihara"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "37",
      "title": "Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Y.-H Zhang",
        "R Huang",
        "J Zeng",
        "S Shan"
      ],
      "year": "2020",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "38",
      "title": "Wavenet with crossattention for audiovisual speech recognition",
      "authors": [
        "H Wang",
        "F Gao",
        "Y Zhao",
        "L Wu"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "39",
      "title": "Investigation on Joint Representation Learning for Robust Feature Extraction in Speech Emotion Recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "40",
      "title": "Dense multimodal fusion for hierarchically joint representation",
      "authors": [
        "D Hu",
        "C Wang",
        "F Nie",
        "X Li"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "41",
      "title": "Bidirectional joint representation learning with symmetrical deep neural networks for multimodal and crossmodal applications",
      "authors": [
        "V Vukotić",
        "C Raymond",
        "G Gravier"
      ],
      "year": "2016",
      "venue": "ICMR"
    },
    {
      "citation_id": "42",
      "title": "Cross attention network for few-shot classification",
      "authors": [
        "R Hou",
        "H Chang",
        "B Ma",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "NIPS"
    },
    {
      "citation_id": "43",
      "title": "Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition",
      "authors": [
        "D Kim",
        "W Baddar",
        "J Jang",
        "Y Ro"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Lstm-modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Öllmer",
        "M Kaiser",
        "F Eyben",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "45",
      "title": "Deep domain adaptation with ordinal regression for pain assessment using weaklylabeled videos",
      "authors": [
        "G Rajasekhar",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2021",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "46",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev",
        "M Suleyman",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "47",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "48",
      "title": "Deep weakly supervised domain adaptation for pain localization in videos",
      "authors": [
        "R Gnana Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2020",
      "venue": "FG"
    },
    {
      "citation_id": "49",
      "title": "Speech based emotion recognition",
      "authors": [
        "V Sethu",
        "J Epps",
        "E Ambikairajah"
      ],
      "year": "2015",
      "venue": "Speech and Audio Processing for Coding, Enhancement and Recognition"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Emotion recognition from variable-length speech segments using deep learning on spectrograms"
    },
    {
      "citation_id": "51",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition from speech using spectrograms and shallow neural networks",
      "authors": [
        "A Slimi",
        "M Hamroun",
        "M Zrigui",
        "H Nicolas"
      ],
      "year": "2020",
      "venue": "ICAMCM"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Emotion recognition in speech using cross-modal transfer in the wild"
    },
    {
      "citation_id": "54",
      "title": "What makes training multimodal classification networks hard?",
      "authors": [
        "W Wang",
        "D Tran",
        "M Feiszli"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "55",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab",
        "C Schmid",
        "C Sun"
      ],
      "year": "2021",
      "venue": "NIPS"
    },
    {
      "citation_id": "56",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Introducing the recola multimodal corpus of remote collaborative and affective interactions"
    },
    {
      "citation_id": "57",
      "title": "Deep affect prediction inthe-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "58",
      "title": "Av+ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "AVEC"
    },
    {
      "citation_id": "59",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge"
    },
    {
      "citation_id": "60",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "61",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "ICAIS"
    },
    {
      "citation_id": "62",
      "title": "An investigation of annotation delay compensation and output-associative fusion for multimodal continuous emotion prediction",
      "authors": [
        "Z Huang",
        "T Dang",
        "N Cummins",
        "B Stasak",
        "P Le",
        "V Sethu",
        "J Epps"
      ],
      "year": "2015",
      "venue": "AVEC"
    },
    {
      "citation_id": "63",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "64",
      "title": "Multimodal affective dimension prediction using deep bidirectional long shortterm memory recurrent neural networks",
      "authors": [
        "L He",
        "D Jiang",
        "L Yang",
        "E Pei",
        "P Wu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "AVEC"
    },
    {
      "citation_id": "65",
      "title": "Strength modelling for real-world automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Image Vision Comput"
    },
    {
      "citation_id": "66",
      "title": "Analysing affective behavior in the first abaw competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "Analysing affective behavior in the first abaw competition"
    },
    {
      "citation_id": "67",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "ICCVW"
    },
    {
      "citation_id": "68",
      "title": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "C Liu",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "Valence and arousal estimation based on multimodal temporal-aware features for videos in the wild"
    },
    {
      "citation_id": "69",
      "title": "Time-continuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition",
      "authors": [
        "V Karas",
        "M Tellamekala",
        "A Mallol-Ragolta",
        "M Valstar",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Time-continuous audiovisual fusion with recurrence vs attention for in-the-wild affect recognition"
    }
  ]
}