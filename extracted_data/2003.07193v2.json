{
  "paper_id": "2003.07193v2",
  "title": "Tf-Idfc-Rf: A Novel Supervised Term Weighting Scheme For Sentiment Analysis",
  "published": "2020-03-12T21:31:46Z",
  "authors": [
    "Flavio Carvalho",
    "Gustavo Paiva Guedes"
  ],
  "keywords": [
    "Supervised Term Weighting Scheme",
    "Sentiment Analysis",
    "Affective"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sentiment Analysis is a branch of Affective Computing usually considered a binary classification task. In this line of reasoning, Sentiment Analysis can be applied in several contexts to classify the attitude expressed in text samples, for example, movie reviews, sarcasm, among others. A common approach to represent text samples is the use of the Vector Space Model to compute numerical feature vectors consisting of the weight of terms. The most popular term weighting scheme is TF-IDF (Term Frequency -Inverse Document Frequency). It is an Unsupervised Weighting Scheme (UWS) since it does not consider the class information in the weighting of terms. Apart from that, there are Supervised Weighting Schemes (SWS), which consider the class information on term weighting calculation. Several SWS have been recently proposed, demonstrating better results than TF-IDF. In this scenario, this work presents a comparative study on different term weighting schemes and proposes a novel supervised term weighting scheme, named as TF-IDFC-RF. The effectiveness of TF-IDFC-RF is validated with SVM (Support Vector Machine) and NB (Naive Bayes) classifiers on four commonly used Sentiment Analysis datasets. TF-IDFC-RF shows promising results, outperforming all other weighting schemes on two datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sentiment Analysis (SA) has attracted much attention in recent years  [1] .\n\nIt is a branch of Affective Computing usually considered a binary classification task  [2] . The goal of SA is to classify the attitude expressed in text samples (e.g., positive or negative) rather than some facts (e.g., entertainment or sport)  [2, 3, 4] . It can be useful in several contexts, for example, to detect subjectivity  [5, 6] , irony/sarcasm  [7, 8, 9] , sentiment in movie reviews  [5, 10, 11] , among others.\n\nA usual approach to represent text documents in the scope of SA is the use of the Vector Space Model (VSM), initially introduced in  [12] . The main idea behind VSM is to represent each document as a numerical feature vector, consisting of the weight of terms extracted from the text corpus  [13] . The weight of each term is considered the key component of document representation in VSM  [4] . Thereby, the choice of the term weighting scheme to represent documents directly affects the classification accuracy  [14, 13, 15] .\n\nThe weighting schemes can be divided into two main categories, based on the usage of class information in training documents  [14] . The first one is the unsupervised term weighting (UTW), which does not use class information to generate weights. The most popular unsupervised scheme is TF-IDF (Term Frequency -Inverse Document Frequency)  [16, 17] . It has been used effectively in information retrieval studies; however, it is not very well suited for text classification tasks  [18] . The second main category of weighting schemes is referred to as supervised term weighting (STW), which was firstly proposed by Debole and Sebastiani  [19] . STW schemes embrace class information from training dataset to compute term weighting  [20] , which leads researchers to believe that these schemes have superior performance than UTW  [21] .\n\nFollowing this line of reasoning, several researches has focused on the devel-opment of new supervised weighting schemes (e.g., TF-RF  [21] , TF-IDF-ICF  [16] ). Recently, Chen et al.  [13]  introduced a new term weighting based in inverse gravity moment, named as TF-IGM. The authors state that TF-IGM outperforms the state-of-the-art supervised term weighting schemes. Dogan and Uysal  [15]  proposed TF-IGM imp , which is an improved version of TF-IGM.\n\nExperiments indicated that TF-IGM imp outperforms TF-IGM.\n\nAlthough there are several supervised term weighting schemes, the experiments are usually conducted on multi-class datasets  [22, 13] . Based on this premise, Chen et al.  [13]  constructed ten two-class subsets from the Reuters-21578 corpus to conduct the experiments in their study. However, Reuters-21578 is not an original two-class dataset, and the scope of this work is to study STW schemes in sentiment analysis, more specifically, in two-class datasets.\n\nThe main contributions of this work are: (i) the proposal of a novel STW scheme named as TF-IDFC-RF; (ii) the evaluation of ten weighting schemes (two UWS and eight SWS) on four two-class sentiment analysis datasets; we selected broadly available datasets to facilitate replication of the experiments.\n\nAccording to the experimental results described in the next sections, TF-IDFC-RF outperforms all compared schemes in two datasets. These results are achieved in four two-class sentiment analysis datasets.\n\nThe remainder of this study is structured as follows: In Section 2 we present the concepts of nine STW schemes and the proposed scheme; In Section 3, we describe the datasets used in carrying out experiments; In Section 4, we discuss the experimental setup used to execute the experiments; In Section 5, we present the experiments conducted with seven STW schemes, two UTW schemes and the proposed approach. Finally, in Section 6, we conclude and open discussion for further research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Term Weighting Schemes",
      "text": "As previously mentioned, VSM represents each document as a numerical feature vector (weights), where each dimension corresponds to a separate term (words, keywords, or longer phrases). The process of assigning a weight to each term is known as term weighting. There are several term weighting schemes in the literature, and the adoption of each of them leads to different results in text classification tasks  [23, 24] . This section describes relevant concepts for text classification and discusses nine term weighting schemes in order to compare them with the proposed term weighting. Section 2.1 reviews two of the most common unsupervised term weighting schemes, which are commonly considered as the baseline schemes.\n\nSection 2.2 presents seven supervised term weighting schemes used throughout this work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Unsupervised Term Weighting",
      "text": "Unsupervised term weighting schemes compute term weights considering information such as the frequency of terms in documents or the number of times that a term appears in a collection  [25] . In unsupervised term weighting approaches, the class information of the documents is not used to generate weights  [26] . Section 2.1.1 and 2.1.2 present the UTW used in this work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Term Weighting Based On Tf",
      "text": "Term frequency (TF) is the number of times a particular term t i occurs in a document d j , as indicated in Eq. 1. It is one of the most important term weighting schemes in document analysis  [27] . However, it is widely recognized that TF puts too much weight on repeated occurrences of a term  [28] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Term Weighting Based On Tf-Idf",
      "text": "TF-IDF  [12]  is one of the earliest and common unsupervised weighting methods  [18] . The intuition behind TF-IDF is that, for some context, some terms are more important than others to describe documents. For example, a term that appears in all documents does not have substantial relevance to help identifying documents. Eq. 2 describes TF-IDF, where N is the number of documents in the corpus and DF (t i ) corresponds to the frequency of documents that term t i appears in the collection. TF-IDF and TF are considered unsupervised term weighting schemes as they do not take into account class information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Supervised Term Weighting",
      "text": "This section describes seven supervised term weights schemes and the proposed weighting scheme. As already stated, STW schemes weight terms by exploiting the known class information in training corpus. The fundamental elements of supervised term weighting are depicted in Table  1 . In this representation, the importance of a term t i for a class c k is represented as follows:\n\n• A represents the number of documents in class c k where the term t i occurs at least once;\n\n• C represents the number of documents not belonging to class c k where the term t i occurs at least once;\n\n• B represents the number of documents belonging to class c k where the term t i does not occur;\n\n• D represents the number of documents not belonging to class c k where the term t i does not occur;\n\n• N is the total number of documents in the corpus; N = A + B + C + D;\n\n• N p is the number of documents in the positive class; N p = A + B;\n\n• N n is the number of documents in the negative class; N n = C + D.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Term Weighting Based On Delta Tf-Idf",
      "text": "Delta TF-IDF was proposed by Martineau and Finin  [29] . It computes the difference of TF-IDF scores in the positive and negative classes to improve accuracy  [29] . As an STW, it considers the distribution of features between the two classes before classification, recognizing and heightening the effect of distinguishing terms. Delta TF-IDF boosts the importance of words that are unevenly distributed between the positive and the negative class.\n\nIn this work, we use the smoothed version, as indicated in Eq. 3, since it achieved higher accuracy in Paltoglou and Thelwall  [30] . N p and N n are, respectively, the number of documents in positive and negative classes. A and C represent the document frequency of term t i in positive and negative classes, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Term Weighting Based On Tf-Idf-Icf",
      "text": "TF-IDF-ICF is a supervised weighting scheme based on traditional TF-IDF.\n\nHowever, it adds Inverse Class Frequency (ICF) factor  [16]  to give higher weighting values to rare terms that occur in fewer documents (IDF) and classes (ICF).\n\nIn Eq. 4, M is the number of classes in the collection and CF (t i ) corresponds to the frequency of classes that term t i appears in the collection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Term Weighting Based On Tf-Rf",
      "text": "TF-RF (Term Frequency -Relevance Frequency) was proposed in  [21] . Similar to Delta TF-IDF, TF-RF takes into account terms distribution in positive and negative classes. However, only the documents containing the term are considered, that is, the Relevance Frequency (RF) of the terms. TF-RF is indicated in Eq. 5, where the minimal denominator is 1 to avoid division by zero.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Term Weighting Based On Tf-Igm",
      "text": "Term Frequency -Inverse Gravity Moment (TF-IGM)  [13]  is proposed to measure the non-uniformity or concentration of terms inter-class distribution, which reflects the terms class distinguishing power. The standard IGM equation assign ranks (r) based on the inter-class distribution concentration of a term, which is analogous to the concept of \"gravity moment\" (GM) from the physics.\n\nIGM is indicated in Eq. 6, where f ir (r = 1, 2, ..., M ) indicates the number of documents containing the term t i in the r-th class, which are sorted in descending order. Thus, f i1 represents the frequency of t i in the class which it appears most often.\n\nTF-IGM term weighting is then defined based on IGM(t i ), as shown in Eq.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "7.",
      "text": "λ is an adjustable coefficient used to maintain the relative balance between the global and local factors in the weight of a term. The λ coefficient has a default value of 7.0 and can be set as a value between 5.0 and 9.0  [13] . Eq. 8\n\npresents SQRT TF-IGM, which calculates the square root of TF, as a technique to obtaining a more reasonable term weighting by reducing the effect of high TF  [13] .\n\nTo enhance the weighting process of TF-IGM for extreme scenarios, Dogan\n\nand Uysal  [15]  proposed IGM imp , an improvement of IGM . IGM imp is used in two new term weighting schemes, TF-IGM imp and SQRT TF-IGM imp , which were also proposed in  [15] . IGM imp is described in Eq. 9, where D total (t i max )\n\nindicates the total number of documents in the class that the term t i occurs most. TF-IGM imp and SQRT TF-IGM imp are defined in Eq. 10 and Eq. 11,\n\nrespectively. Dogan and Uysal  [15]  report IGM imp produces better results than IGM.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Novel Term Weighting Scheme",
      "text": "The proposed term weighting scheme is based on the IDF concept. However, it calculates the inverse document frequency of terms in classes (IDFC). It is also inspired in TF-RF since it calculates the Relevance Factor of a term.\n\nEq. 12 describes the proposed supervised term weighting scheme, named as TF-IDFC-RF. To avoid division by zero, we adjust the denominators with (A + 1) for IDFC and (C + 1) for RF as in  [21] . In the RF part, we also adjust the numerator with (A + 1) to avoid log(0).\n\nTo illustrate the properties of different term weighting measures and to obtain a more solid understanding of TF-IDFC-RF, consider the fundamental elements presented in  In order to investigate the effect produced by these schemes, Table  4  summarizes the scores for both terms t 1 and t 2 . When comparing IDFC-RF with RF, it is possible to note that IDFC-RF seems to be less discriminative between the terms. For example, the ratio between the terms (i.e., t 1 and t 2 ) is less prominent in IDFC-RF. Therefore, IDFC-RF considers intra-class and inter-class distribution since both are even taken as equally important in STW  [13] .\n\nWeighting",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "This section describes the datasets used to produce the experiments. All datasets are commonly used in Sentiment Analysis studies.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Polarity",
      "text": "The Polarity dataset consists of 1, 000 positive and 1, 000 negative movie reviews. It was first introduced by Pang and Lee  [5] . It is used as a baseline dataset in several sentiment analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Amazon Sarcasm",
      "text": "The Amazon Sarcasm dataset was introduced by Filatova  [8] . It consists of an unbalanced dataset with 437 sarcastic reviews and 817 regular reviews from Amazon (http://www.amazon.com). The reviews were labeled using crowdsourcing.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Subjectivity",
      "text": "The Subjectivity dataset was introduced by Pang and Lee  [5] . It consists of 5, 000 subjective sentences and 5, 000 objective sentences. The subjective sentences were collected from www.rottentomatoes.com. The objective sentences were extracted from summaries available from the Internet Movie Database (www.imdb.com).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Movie Review",
      "text": "Movie Review dataset contains 10, 662 movie-reviews \"snippets\" (a striking extract usually one sentence long) with positive and negative labels  [31] . The movie-reviews were collected from www.rottentomatoes.com. It consists of 5, 331 negative snippets and 5, 331 positive snippets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experimental Setup",
      "text": "This section describes the experimental setup used to present the experimental results. In Section 4.1 we discuss the classification process adopted in this work. In Section 4.2 we review concepts from the learning algorithms considered to produce the experiments. Finally, in Section 4.3 we describe the evaluation measures used in the experimental study.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Classification Process",
      "text": "All documents in each dataset were preprocessed with lowercase conversion, punctuation removal, and tokenization. In the classification process, we applied the stratified 5-fold cross-validation technique to present the classification performance. The process adopted to execute the experiments is based on the training-and-testing paradigm described in  [32] . The procedure followed for each fold is illustrated in Fig.  1 . As depicted in Fig.  1  Feature extraction: In the scope of this study, in the \"feature extraction\" step, we extracted the features with the following schemes: TF-IDFC- Next, as in  [13, 15] , we adopted χ 2  or chi-square statistics (CHI2)\n\nfor feature selection 2 . The weighting schemes are tested with top 500, 1,000, 2,000, 4,000, 6,000, 8,000, 10,000, 12,000, and 14,000 terms scored and sorted descending order by CHI2 max for all datasets. Parameters setting: Lambda parameters were configured with λ = 7 for TF-IGM, STF-IGM, TF-IGM imp and STF-IGM imp , as it is considered the default value  [13] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Learning Algorithms",
      "text": "To evaluate the effectiveness of weighting schemes, we conducted the experiments with Support Vector Machines (SVM), since it is the best learning approach in text categorization  [25, 21, 20] . We used the scikit-learn implementation of SVC  [35]  trained with default values.\n\nWe also executed the experiments with the Naive Bayes algorithm (NB), since it is also often used as a baseline for text categorization and sentiment analysis  [36] . We also used a Sklearn implementation of NB, named as Multi-nomialNB.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Performance Measures",
      "text": "We calculated the effectiveness of the weighting schemes using weighted F 1 measure, as described in Chavarriaga et al.  [37] . The weighted F 1 measure is calculated considering the class size and the precision and recall for each class.\n\nPrecision is defined as the fraction of all positive predictions that are actual positives, as defined in Eq. 13. Recall is the fraction of all actual positives that are predicted to be positive, as indicated in Eq. 14.\n\nprecision = T P T P + F P (13)\n\nConsidering the two equations above, Weighted F 1 measure is defined as the Eq. 15.\n\nIn Eq. 15, i is the class index and w i = n i /N is the proportion of samples of class i. N indicates the total number of samples and n i denotes the number of samples of the i th class.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "In this section, we evaluate the performance of the unsupervised term weighting scheme proposed in this work, named as TF-IDFC-RF. To accomplish this goal, we compared TF-IDFC-RF with 9 other weighting schemes on Polarity, Amazon Sarcasm, Subjectivity and Movie Review datasets.   Detailed results on Sarcasm dataset are reported in Tables  7  and 8 . It is evident the superiority of TF-IDFC-RF over all other weighting schemes with SVM. DTF-IDF has the best overall performance with NB, reaching an F 1 score of 66.80% with 14, 000 features, as indicated in Table  7 . With the same feature size, TF-IDFC-RF achieves an F 1 score of 65.13%. It is clear that when discussing the performance of TF-IDFC-RF and NB, it is not so meaningful as the result reported by the TF-IDFC-RF and SVM, which achieved 79.93% of F 1 with 14, 000 features. However, TF-IDFC-RF presents the second-best result with NB classifier.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Performance Comparisons On The Polarity Dataset",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Performance Comparisons On The Sarcasm Dataset",
      "text": "Although TF-IDFC-RF was inferior to DTF-IDF considering the NB classifier, it is possible to notice that all F 1 values achieved by TF-IDFC-RF and SVM are higher than the values reported by DTF-IDF and NB. This evidence points out that, when considering the Sarcasm dataset, TF-IDFC-RF obtains better results than DTF-IDF.\n\nFeat.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Weighted",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Performance Comparisons On The Subjectivity Dataset",
      "text": "Subjectivity dataset is also a balanced dataset, however, it consists of 10, 000 sentences, as pointed out in Section 3. Tables 9 and 10 report the detailed F 1 results obtained with Subjectivity dataset. Except DTF-IDF scheme, in most cases, the weighting schemes presented better F 1 results considering the NB classifier. TF-IDF generally shows the best performance with both classifiers. Table  9  shows that TF-IGM imp and STF-IGM imp achieved the best result when feature sizes are 500 and 1, 000 respectively.\n\nAs indicated in Table  10 , TF-IDFC-RF performs better than TF-IDF when feature sizes are 500 and 1, 000. This table also present that STF-IGM achieved the best result when the feature size is 6, 000. The F 1 score difference between TF-IDF and TF-IDFC-RF is lower than 0.2 percentage points in all feature sizes with SVM and NB.\n\nFeat.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Weighted",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Discussion",
      "text": "The performance assessment of different term weighting schemes in classification tasks was executed with four two-class datasets. Results are generally better with SVM classifier considering two datasets (Polarity and Sarcasm) and better with NB on the other two datasets (Subjectivity and Movie Review).\n\nPrior work reported TF-IGM imp and STF-IGM imp generally outperform TF-IGM and STF-IGM as well as STF-IGM imp generally outperforms TF-IGM imp  [15] . However, on all datasets considered in this work, this pattern is not found.\n\nFor example, on the Polarity dataset, not always STF-IGM imp is better than STF-IGM. The same pattern occurs on the Subjectivity dataset. This is an important finding, since, as the authors know, this is the first study to conduct experiments with IGM based schemes and polarity datasets.\n\nThe results obtained in previous studies indicated good results with TF-RF  [21, 13] . As reported in  [13] , sometimes TF-RF outperforms TF-IGM and STF-IGM. However, our results revealed that TF-RF is almost always worse than all IGM based schemes with NB and SVM on all four datasets. On the other hand, TF-IDFC-RF presented results equal to or better than TF-RF in all experiments.\n\nTF and TF-ICF did not achieve the best results considering all four datasets.\n\nTF-IDF presented the best results on the Subjectivity dataset, and DTF-IDF showed the best results with the Sarcasm dataset and NB classifier.\n\nOur results provide compelling evidence that TF-IDFC-RF achieves better results than the other nine weighting schemes on two datasets with NB and SVM. It is important to note that when considering IGM schemes, no one scheme performs better on a specific dataset. When considering different feature sizes, one time, the best can be a squared IGM based scheme, another time a non-squared IGM scheme. Since there is only one version of TF-IDFC-RF, it presents more consistent results.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this work, we have proposed a novel supervised term weighting scheme named TF-IDFC-RF to be used in Sentiment Analysis tasks, more specifically, in the binary classification problem. The proposed scheme is based on two other schemes: TF-IDF and TF-RF. TF-IDFC-RF is inspired by the fact that the IDF factor can be used for each class, referred to as the Inverse Document Frequency in Class (IDFC). On the other hand, since TF-RF has produced good results in the literature, we were also inspired TF-IDFC-RF on it. The most important concept of TF-IDFC-RF is that it aims to consider intra-class and inter-class distribution to weight the terms.\n\nThe performance of TF-IDFC-RF is compared with nine other term weighting schemes, including TF-IDF and TF-RF. These schemes also encompass the IGM based schemes, since they outperformed several other schemes in recent work  [13, 15] . It is important to stress that, as stated in  [13] , TF-IGM schemes are especially suitable for multi-class text classification applications, however, they can be used for binary classification. SVM and NB classifiers were utilized to perform the experiments with different feature sizes.\n\nThe experiments show that TF-IDFC-RF outperforms all schemes with NB and SVM on two datasets. IGM based schemes achieved the best results in one dataset. Finally, TF-IDF presented performed better than all other schemes in one dataset. In future work, we will conduct comparative studies with TF-IDFC-RF in multi-class datasets. We also plan to produce experiments with larger datasets.",
      "page_start": 22,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: As depicted in Fig. 1(a), during the training phase,",
      "page": 11
    },
    {
      "caption": "Figure 1: (b), during prediction, the",
      "page": 11
    },
    {
      "caption": "Figure 1: Supervised classiﬁcation process. Adapted from He [33].",
      "page": 12
    },
    {
      "caption": "Figure 2: Weighted-F1 scores obtained using NB and SVM classiﬁers with 10 term weighting",
      "page": 14
    },
    {
      "caption": "Figure 3: Weighted-F1 scores obtained using NB and SVM classiﬁers with 10 term weighting",
      "page": 16
    },
    {
      "caption": "Figure 4: Weighted-F1 scores obtained using NB and SVM classiﬁers with 10 term weighting",
      "page": 18
    },
    {
      "caption": "Figure 5: Weighted-F1 scores obtained using NB and SVM classiﬁers with 10 term weighting",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Suppose a training dataset containing 100 doc-",
      "data": [
        {
          "cp\ncn": "10\n25\nt2\n20\n45\nt2"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sentiment analysis using product review data",
      "authors": [
        "X Fang",
        "J Zhan"
      ],
      "venue": "Journal of Big Data",
      "doi": "10.1186/s40537-015-0015-2"
    },
    {
      "citation_id": "2",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2017.02.003"
    },
    {
      "citation_id": "3",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2016.31"
    },
    {
      "citation_id": "4",
      "title": "A study of supervised term weighting scheme for sentiment analysis",
      "authors": [
        "Z.-H Deng",
        "K.-H Luo",
        "H.-L Yu"
      ],
      "year": "2014",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2013.10.056"
    },
    {
      "citation_id": "5",
      "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2004",
      "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)",
      "doi": "10.3115/1218955.1218990"
    },
    {
      "citation_id": "6",
      "title": "Affective representations for sarcasm detection",
      "authors": [
        "A Agrawal",
        "A An"
      ],
      "year": "2018",
      "venue": "st International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": "10.1145/3209978.3210148"
    },
    {
      "citation_id": "7",
      "title": "Sarcasm classification: A novel approach by using content based feature selection method",
      "authors": [
        "H Kumar",
        "B Harish"
      ],
      "year": "2018",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "8",
      "title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing",
      "authors": [
        "E Filatova"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "9",
      "title": "Towards a contextual pragmatic model to detect irony in tweets",
      "authors": [
        "J Karoui",
        "F Zitoune",
        "V Moriceau",
        "N Aussenac-Gilles",
        "L Belguith"
      ],
      "year": "2015",
      "venue": "ACL-IJCNLP 2015 -53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Convolutional sentence kernel from word embeddings for short text categorization",
      "authors": [
        "J Kim",
        "F Rousseau",
        "M Vazirgiannis"
      ],
      "year": "2015",
      "venue": "Conference Proceedings -EMNLP 2015: Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "A discourse-aware neural network-based text model for document-level text classification",
      "authors": [
        "K Lee",
        "S Han",
        "S.-H Myaeng"
      ],
      "year": "2018",
      "venue": "Journal of Information Science",
      "doi": "10.1177/0165551517743644"
    },
    {
      "citation_id": "12",
      "title": "A vector space model for automatic indexing",
      "authors": [
        "G Salton",
        "A Wong",
        "C Yang"
      ],
      "year": "1975",
      "venue": "Communications of the ACM",
      "doi": "10.1145/361219.361220"
    },
    {
      "citation_id": "13",
      "title": "Turning from tf-idf to tf-igm for term weighting in text classification",
      "authors": [
        "K Chen",
        "Z Zhang",
        "J Long",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2016.09.009"
    },
    {
      "citation_id": "14",
      "title": "A comparison of term weighting schemes for text classification and sentiment analysis with a supervised variant of tf.idf",
      "authors": [
        "G Domeniconi",
        "G Moro",
        "R Pasolini",
        "C Sartori"
      ],
      "year": "2015",
      "venue": "conference of 4th International Conference on Data Management Technologies and Applications",
      "doi": "10.1007/978-3-319-30162-4_4"
    },
    {
      "citation_id": "15",
      "title": "Improved inverse gravity moment term weighting for text classification",
      "authors": [
        "T Dogan",
        "A Uysal"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2019.04.015"
    },
    {
      "citation_id": "16",
      "title": "Class-indexing-based term weighting for automatic text classification",
      "authors": [
        "F Ren",
        "M Sohrab"
      ],
      "year": "2013",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2013.02.029"
    },
    {
      "citation_id": "17",
      "title": "A supervised term weighting scheme for multi-class text categorization",
      "authors": [
        "Y Gu",
        "X Gu"
      ],
      "year": "2017",
      "venue": "conference of 13th International Conference on Intelligent Computing",
      "doi": "10.1007/978-3-319-63315-2_38"
    },
    {
      "citation_id": "18",
      "title": "On term frequency factor in supervised term weighting schemes for text classification",
      "authors": [
        "T Dogan",
        "A Uysal"
      ],
      "venue": "Arabian Journal for Science and Engineer",
      "doi": "10.1007/s13369-019-03920-9"
    },
    {
      "citation_id": "19",
      "title": "Supervised term weighting for automated text categorization",
      "authors": [
        "F Debole",
        "F Sebastiani"
      ],
      "year": "2004",
      "venue": "Text Mining and its Applications"
    },
    {
      "citation_id": "20",
      "title": "A study on term weighting for text categorization: A novel supervised variant of tf.idf",
      "authors": [
        "G Domeniconi",
        "G Moro",
        "R Pasolini",
        "C Sartori"
      ],
      "year": "2015",
      "venue": "DATA 2015 -4th International Conference on Data Management Technologies and Applications"
    },
    {
      "citation_id": "21",
      "title": "Supervised and traditional term weighting methods for automatic text categorization",
      "authors": [
        "M Lan",
        "C Tan",
        "J Su",
        "Y Lu"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2008.110"
    },
    {
      "citation_id": "22",
      "title": "Supervised term weighting centroid-based classifiers for text categorization",
      "authors": [
        "T Nguyen",
        "K Chang",
        "S Hui"
      ],
      "year": "2013",
      "venue": "Knowledge and Information Systems",
      "doi": "10.1007/s10115-012-0559-9"
    },
    {
      "citation_id": "23",
      "title": "Modified frequency-based term weighting schemes for text classification",
      "authors": [
        "T Sabbah",
        "A Selamat",
        "M Selamat",
        "F Al-Anzi",
        "E Viedma",
        "O Krejcar",
        "H Fujita"
      ],
      "year": "2017",
      "venue": "Applied Soft Computing Journal"
    },
    {
      "citation_id": "24",
      "title": "Experiments in term weighting for novelty mining",
      "authors": [
        "F Tsai",
        "A Kwee"
      ],
      "year": "2011",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2011.04.218"
    },
    {
      "citation_id": "25",
      "title": "Machine learning in automated text categorization",
      "authors": [
        "F Sebastiani"
      ],
      "year": "2002",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/505282.505283"
    },
    {
      "citation_id": "26",
      "title": "Learn to weight terms in information retrieval using category information",
      "authors": [
        "R Jin",
        "J Chai",
        "L Si"
      ],
      "year": "2005",
      "venue": "Proceedings of the 22nd international conference on Machine learning"
    },
    {
      "citation_id": "27",
      "title": "Categorical term frequency probability based feature selection for document categorization",
      "authors": [
        "Q Li",
        "L He",
        "X Lin"
      ],
      "year": "2013",
      "venue": "2013 International Conference on Soft Computing and Pattern Recognition",
      "doi": "10.1109/SOCPAR.2013.7054103"
    },
    {
      "citation_id": "28",
      "title": "International Conference on Information and Knowledge Management, Proceedings",
      "authors": [
        "Y Lv",
        "C Zhai"
      ],
      "year": "2011",
      "venue": "International Conference on Information and Knowledge Management, Proceedings",
      "doi": "10.1145/2063576.2063584"
    },
    {
      "citation_id": "29",
      "title": "An improved feature space for sentiment analysis",
      "authors": [
        "J Martineau",
        "T Finin",
        "Delta Tfidf"
      ],
      "year": "2009",
      "venue": "ICWSM"
    },
    {
      "citation_id": "30",
      "title": "A study of information retrieval weighting schemes for sentiment analysis",
      "authors": [
        "G Paltoglou",
        "M Thelwall"
      ],
      "year": "2010",
      "venue": "ACL 2010 -48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference"
    },
    {
      "citation_id": "31",
      "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2005",
      "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics"
    },
    {
      "citation_id": "32",
      "title": "Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing",
      "authors": [
        "D Jurafsky",
        "J Martin"
      ],
      "venue": "Speech and language processing: An introduction to speech recognition, computational linguistics and natural language processing"
    },
    {
      "citation_id": "33",
      "title": "Text mining and IRT for psychiatric and psychological assessment",
      "authors": [
        "Q He"
      ],
      "year": "2013",
      "venue": "Text mining and IRT for psychiatric and psychological assessment"
    },
    {
      "citation_id": "34",
      "title": "A comparative study on feature selection in text categorization",
      "authors": [
        "Y Yang",
        "J Pedersen"
      ],
      "year": "1997",
      "venue": "Proceedings of the Fourteenth International Conference on Machine Learning, ICML '97"
    },
    {
      "citation_id": "35",
      "title": "API design for machine learning software: experiences from the scikit-learn project",
      "authors": [
        "L Buitinck",
        "G Louppe",
        "M Blondel",
        "F Pedregosa",
        "A Mueller",
        "O Grisel",
        "V Niculae",
        "P Prettenhofer",
        "A Gramfort",
        "J Grobler",
        "R Layton",
        "J Van-Derplas",
        "A Joly",
        "B Holt",
        "G Varoquaux"
      ],
      "year": "2013",
      "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "Baselines and bigrams: Simple, good sentiment and topic classification",
      "authors": [
        "S Wang",
        "C Manning"
      ],
      "year": "2012",
      "venue": "th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "The opportunity challenge: A benchmark database for onbody sensor-based activity recognition",
      "authors": [
        "R Chavarriaga",
        "H Sagha",
        "A Calatroni",
        "S Digumarti",
        "G Trster",
        "J Milln",
        "D Roggen"
      ],
      "year": "2013",
      "venue": "Pattern Recognition Letters"
    }
  ]
}