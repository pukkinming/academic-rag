{
  "paper_id": "2103.08310v1",
  "title": "Emonet: A Transfer Learning Framework For Multi-Corpus Speech Emotion Recognition",
  "published": "2021-03-10T19:12:37Z",
  "authors": [
    "Maurice Gerczuk",
    "Shahin Amiriparian",
    "Sandra Ottl",
    "Björn Schuller"
  ],
  "keywords": [
    "deep learning",
    "transfer learning",
    "multi-domain learning",
    "multi-corpus",
    "cross-corpus",
    "speech emotion recognition",
    "computational paralinguistics",
    "computer audition",
    "audio processing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this manuscript, the topic of multi-corpus Speech Emotion Recognition (SER) is approached from a deep transfer learning perspective. A large corpus of emotional speech data, EMOSET, is assembled from a number of existing SER corpora. In total, EMOSET contains 84 181 audio recordings from 26 SER corpora with a total duration of over 65 hours. The corpus is then utilised to create a novel framework for multi-corpus speech emotion recognition, namely EMONET. A combination of a deep ResNet architecture and residual adapters is transferred from the field of multi-domain visual recognition to multi-corpus SER on EMOSET. Compared against two suitable baselines and more traditional training and transfer settings for the ResNet, the residual adapter approach enables parameter efficient training of a multi-domain SER model on all 26 corpora. A shared model with only 3.5 times the number of parameters of a model trained on a single database leads to increased performance for 21 of the 26 corpora in EMOSET. Measured by McNemar's test, these improvements are further significant for ten datasets at p < 0.05 while there are just two corpora that see only significant decreases across the residual adapter transfer experiments. Finally, we make our EMONET framework publicly available for users and developers at https://github.com/EIHW/EmoNet. EMONET provides an extensive command line interface which is comprehensively documented and can be used in a variety of multi-corpus transfer learning settings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "W Ith recent advancements in the field of machine learning and the widespread availability of computationally powerful consumer devices, such as smartphones, many people are now already interacting with artificial intelligence (AI) technology on a daily basis. A prominent example are voice controlled personal assistants, such as Alexa and Siri  [1] ,  [2] , which are becoming increasingly popular. These products are a first step towards conversational AI, which integrates a variety of research fields, such as automatic speech recognition (ASR), natural language understanding (NLU), and context modelling  [3] . While their current functionality demonstrates their capabilities for task oriented ASR, they are still a far way off of being able to converse freely with humans  [4]  and disregard other important aspects of interpersonal communication, such as the expression and understanding of emotions.\n\nIn contrast to an early view of emotions being a disruption of organised rational thought and to be controlled by an individual  [5] -  [7] , today, emotional intelligence is accepted as a central and guiding function of cognitive ability that meets standards for an intelligence  [8] -  [11] . In this context, it has been argued that designing conversational AIs with both the ability to comprehend and to express emotions will improve on the quality and effectiveness of human machine interaction  [12] ,  [13] . Emotion recognition capabilities can furthermore serve a purpose for the integration of machine learning and AI technologies into health-care where an affective computing approach can support patient in-home monitoring  [14] , help with early detection of psychiatric diseases  [15]  or be applied to support diagnosis and treatment in military healthcare  [16] . In this regard, SER is furthermore highly related to automatic speech based detection of clinical depression  [17] -  [20] , where emotional content can further improve the performance of recognition approaches  [21] . Here, monitoring of patients using only audio signals could be seen as less intrusive than video or physiological based methods.\n\nWhile SER can serve a wide range of purposes from an application stand-point, there are a couple of fundamental characteristics of the field that make it a hard task up to this day  [22] . Collecting and annotating sufficiently large amounts of data that is suitable to the target application is time consuming  [22] . Emotional speech recordings can for example be obtained in laboratory setting by recording professional actors or in the wild  [23] , e. g. , from movie scenes or online videos  [24] . Moreover, after suitable material has been collected, annotation is not straightforward and has to consider an additional modelling step in the different ways human emotions can be classified and interpreted  [22] . Here, choosing between a categorical and dimensional approach and defining how to represent the temporal dynamics of emotion are two examples of important steps that have to be taken to arrive at a fitting an emotion model  [24] ,  [25] . The actual process of annotation then has to find ways to deal with the inherent subjectivity of human emotion, e. g. , by employing multiple annotators, measuring inter-rater agreement and removing data samples containing emotion portrayals that were too ambiguous  [22] . For these reasons, unlike in many fields that have seen leaps in the state-of-theart due to the paradigm of deep learning, truly large-scalei. e. , more than one million samples -SER corpora do not exist up to this day. Rather, SER research brought forth a large number of small-scale corpora which showcase a high degree of diversity in their nature.\n\nThe sparsity of large SER datasets on the one hand, and the availability of a large number of smaller datasets on the other, are key motivating factors for our work which applies state-of-the-art paradigms of both transfer and deep learn-ing for the task of multi-corpus SER. For this, we develop a multi-corpus framework called EMONET based based on residual adapters  [26] ,  [27] . We evaluate this framework on a large collection of existing SER corpora assembled and organised as EMOSET . Further, all experiments are compared to suitable baseline SER systems. Finally, we release the source code of our framework together with a comprehensive command line interface on GitHub 1 , to be freely used by any researcher interested in multi-domain audio analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Three general research areas are of interest for the topic of this manuscript. First, the field of SER deals with the general problem of recognising human emotions from speech recordings, which is the task that should be solved on a multi-corpus basis by the presented approaches. Second, the field of deep learning where large amounts of data are used to train neural network architectures that are able to learn high-level feature representations from raw input data, gives the direction for the choice of machine learning models. Here, the DEEP SPECTRUM feature extraction system is of special importance, as it will serve as one of the baselines against which the transfer learning experiments are compared. Finally, the field of domain adaptation and multi-domain training investigates ways of efficiently transferring knowledge between domains which is important for the multi-corpus SER problem.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "SER describes the task of automatically detecting emotional subtext from spoken language and is a research field that has increasingly been of interest for more than two decades  [29] . Traditionally, to develop a computational emotion recognition model from speech data, the following steps should be considered. Initially, unlike for many other current machine learning tasks, e. g. , visual object recognition or Automatic Speech Recognition (ASR), no single definitive method of representing human emotion is applicable to every scenario  [22] . Furthermore, a decision has to be made about the temporal granularity of annotating and detecting emotions from continuous speech signals  [22] . For the first aspect, two main approaches are used in the literature  [30] . In the categorical approach, emotions can be grouped into discrete classes  [31] , mostly with respect to the Ekman 'Big Six'  [32] ,  [33] , including anger, disgust, fear, happiness, sadness and surprise. On the other hand, emotions can be analysed from a continuous and dimensional perspective. Here, an emotion is annotated on multiple axes each describing a different aspect. Most often used dimensions are arousal, valence and dominance. Arousal describes the intensity or activation of an emotion, while valence represents the intrinsic positivity or negativity of a feeling  [34] . Finally, emotions can also be placed on a continuous dominance-submissiveness scale which, for example, allows discrimination between anger and fear (both low valence high arousal emotions)  [35] . Translation between categories and dimensions is possible, 1. https://github.com/EIHW/EmoNet e. g. , using a mapping, such as the circumplex model of affect  [36] .\n\nFor the actual automatic computational recognition of emotions from audio speech signals, many traditional approaches rely on the extraction of either brute-forced or handcrafted sets of features computed from frame-level low-level descriptors (LLDs), such as energy or spectral information, by applying a range of statistical functionals, such as mean, standard deviation, and extreme values over a defined longer segment, e. g. , a speaker turn or utterance  [37] ,  [38] . More recently, the field is being influenced by the trend of deep learning and methods of directly learning models for emotion recognition from raw speech data are being investigated  [22] ,  [39] . These works are touched upon in Section 2.2.\n\nIn the computational and machine learning sphere, previous works have shown the suitability of cross-corpus training for SER  [40] ,  [41] . Aspects that were researched include how to effectively select data from multiple corpora  [40] ,  [41]  or the effects of adding unlabelled data for training  [42] . Schuller et al.  [43]  have investigated various strategies to cope with inter corpus variance by evaluating multiple normalisation strategies, such as per-speaker and per-corpus feature normalisation, for cross-corpus testing on six databases while Kaya et al.  [44]  applied cascaded normalisation on standard acoustic feature sets. Many of the above approaches utilise hand-crafted or brute-forced feature sets computed from LLDs of the audio content. The approach taken in this manuscript differs from the previous research into cross-corpus SER, in that the focus here lies on harnessing the power of deep learning when applied to raw input features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Deep Learning Based Ser",
      "text": "For computer audition, deep learning has made an impact -especially in the ASR domain where large datasets of recorded speech and corresponding transcriptions could be utilised to train accurate models  [45] ,  [46] .\n\nIn comparison to image recognition where training samples can be collected in a large-scale fashion from the internet and annotation is relatively straight forward, many areas of audio recognition do not have this advantage. In an attempt to improve the situation for audio recognition, AudioSet  [47]  which is a large ontology of collected YouTube clips can serve as basis for training deep, general purpose network architectures. These networks can later be used as feature extractors and transfer models for various recognition tasks. Hershey et al. investigated the viability of adapting standard CNN architectures as used for ImageNet classification, such as VGG  [48] , Inception  [49] , and ResNet  [50] ,  [51] , for large-scale audio recognition  [52] . Unlike in image recognition, where the raw pixels of (resized) images can serve as input to the Deep Neural Networks (DNNs), a suitable data representation for audio content has to be chosen manually. Additionally, when the machine learning models cannot handle variable size inputs, e. g. , CNNs with fully connected layers, audio has to be chunked into segments of fixed duration. Tzirakis et al.  [53]  took a slightly different approach by introducing a Convolutional Recurrent Neural Network (CRNN) architecture that transforms the audio signal to a suitable representation using one-dimensional convolutional layers. This time-continuous representation is then processed by a Recurrent Neural Network (RNN) which makes predictions for the learning task at hand. The whole model can be trained in an end-to-end fashion from raw audio signals  [54] ,  [55] .\n\nMel-spectrograms are often used as input features for the DNNs  [56] -  [59] . As not all time-frequency regions of speech samples contain high emotional saliency, methods have been investigated that learn to focus on the most important parts of a given input. RNNs, for example, have the inherent capability to deal with sequences of variable lengths. When combined with a self attention mechanism, emotionally informative time-segments of an input can be highlighted  [60] . Mirsamadi et al. used an attention RNN for SER on Interactive Emotional Dyadic Motion Capture (IEMOCAP) while Gorrostieta et al. applied a similar model with low-level spectral features as input for the ComParE self-asessed affect  [61]  sub-challenge  [62] . More recently, combining CNN feature extractors with attention based RNNs has been shown to be a highly competitive approach to SER  [63] ,  [64] . Taking inspiration from the combination of Fully Convolutional Networks (FCNs) and two-dimensional attention pooling for visual online object tracking  [65] , Neumann and Vu evaluate an attentive CNN for SER on different input features and find log Mel-spectrograms to work best. Their work highlights another advantage an attention pooling method has compared to a more traditional CNN architecture with fully connected layers: Sequences of variable lengths can be handled by the same model without having to adapt parts of the architecture. This is especially important for SER where utterances often differ in duration, and analysing only a short segment of a long utterance often leads to worse results. While the works outlined above demonstrate the efficacy of deep learning based methods for SER on various databases, hand-crafted features still play an important role in the field  [66] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Spectrum Feature Extraction",
      "text": "First introduced for the task of snore sound classification  [67] , DEEP SPECTRUM 2 is a deep feature extraction toolkit for audio-based on image classification CNNs. In the system, knowledge obtained by CNNs for the task of large-scale object recognition on the ImageNet  [68]  database 2. https://github.com/DeepSpectrum is transferred to the audio domain by applying the learnt feature extractors to spectrogram representations. Specifically, an audio sample is first converted to a suitable 2dimensional time-frequency format -most often a (Mel-)spectrogram or a chromagram -by means of Fourier transformation and application of various filter and scaling operations. Afterwards, an RGB image representation conforming with the input specifications of an ImageNet pre-trained CNN, e. g. , AlexNet  [69] , VGG16  [48] , or ResNets  [50] , is created by plotting and resizing the spectrogram, mapping power to a certain pre-defined colour scale. This image is then forwarded through the CNN and the learnt filter operations are applied to the spectrogram. A specific layer of the extractor network can finally be chosen to serve as a deep feature descriptor, i. e. , the activations of this layer are flattened into a (large) feature vector which can then be used as input for various machine learning models. The whole process is illustrated in Figure  1 .\n\nThe features extracted by the DEEP SPECTRUM system have been shown to provide state-of-the-art results for a range of audio analysis tasks in preceding research  [28] ,  [39] ,  [70] -  [75] . The results imply that convolutional filters trained on natural image classification can extract useful features for audio analysis tasks if applied to spectral representations of the content. This motivates using DEEP SPECTRUM as one of the baseline systems to evaluate on EMOSET .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Domain Adaptation And Multi-Domain Learning",
      "text": "As part of the larger field of transfer learning, domain adaptation aims to transfer knowledge learnt on a fully labelled source domain to a target domain where labels are either unavailable or sparse by mitigating the negative impact of domain shift  [76] . Specifically, it deals with the problem that machine learning models trained on largescale datasets are sensitive to dataset bias  [77] -  [79] . As deep learning approaches can be seen as the state-of-the-art in many machine learning research areas, current domain adaptation research focuses on learning to map deep feature representations into a shared space  [76] . One of the most prominent examples can be found in the work of Ganin et al.  [80] , where a shared feature extractor base serves as input for both a source domain label predictor and a domain discriminator. Similarly, Tzeng et al.  [76]   While most of the work done for domain adaptation focuses on transferring models and feature representations from a single source domain to a new target domain, the setting of multi-corpus SER as found in this manuscript is different. Due to unavailability of truly large-scale databases in the field, an approach that is able to learn from multiple domains at once and in the process increases performance for individual tasks is more desirable. Following work into the area of domain adaptation training, the model of residual adapters was first introduced in  [26]  for the task of multidomain learning in the visual domain. The approach tries to find a middle ground between using large pre-trained networks as feature extractors and finetuning the networks on a new task. While using pre-trained networks as feature extractors might have performance drawbacks, finetuning the whole network is very parameter inefficient and can easily lead to catastrophic forgetting of the model's pretraining. Instead, in their approach, not a single universal multi-domain model is trained, but families of networks that are highly related to one another, sharing a large portion of their parameters while still containing task specialised modules. In their work on the Visual Decathlon  [27]  challenge, the authors experimented with different placements of and configurations for the adapter modules. They found that for the best performance, adapters should be used throughout the whole deep network in a parallel configuration. In general, this method reaches or even surpasses traditional finetuning strategies while only requiring about 10 % of task specific parameters. Due to the highly promising results on different domains of visual recognition and the capabilities of CNNs for various speech recognition tasks -as touched upon in Section 2.3 and Section 2.2 -the model is adapted for multi-corpus SER for this manuscript.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emoset -Collection Of Speech Emotion Recognition Corpora",
      "text": "For this manuscript, a large collection of 26 emotional speech corpora has been assembled and organised. This collection, from hereon EMOSET, contains published SER databases that have been used for research and experiments in the field. Additionally, some unpublished speech databases of the Chair of Embedded Intelligence for Healthcare and Wellbeing (University of Augsburg) are used to further augment the training data in order to improve the generalisation capabilities and minimise the overfitting problems of deep learning models. The individual datasets had to be structured for training. Here, speaker-independent training, development, and test partitions had to be constructed manually, in the case they did not already exist.\n\nAn overview of each corpus can be found in Table  1 . For published corpora, descriptions can be found in the referenced papers whereas unpublished databases are described below. These corpora all include categorical emotion labels.\n\nOverall, there are 84 161 samples with a total duration of 65.6 hours and the mean duration of all audio recording is 2.81 seconds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Anger Detection (Ad)",
      "text": "Anger Detection (AD) is a corpus of angry and neutral speech recorded in the setting of phone calls. The corpus is split over 9 calls, each containing speech segments for both classes. In total, there are 660 samples with an average duration of 10.5 seconds. Two calls are separated from the training data, one for the development partition and one for the held-out test set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ppmmk-Emo",
      "text": "PPMMK-EMO is a database of German emotional speech recorded at the University of Passau covering the four basic classes angry, happy, neutral, and sad. It has a total of 3 154 samples averaging 2.5 seconds in length recorded from 36 speakers. For the test set, 8 speakers' recordings are set aside, while 20 % of the training data is randomly sampled to form the development partition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Turkish Emotion",
      "text": "This Turkish SER corpus contains 484 samples of emotional speech recorded from 11 subjects (7 female and 4 male) covering the basic classes anger, joy, neutrality, and sadness. The samples have an average duration of 2.3 seconds. Two female and two male speakers' samples are used set aside for the development and test partition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Exploratory Data Analysis",
      "text": "While a number of statistics for each EMOSET corpus can be found in Table  1 , further data exploration can lead to higher level insights into the nature and composition of EMOSET. How much data is provided by each individual database, both in terms of number of samples and total duration of audio content, is highly variable. Here, a few datasets stand out by including a comparatively large amount of data. DEMoS, FAU Aibo, IEMOCAP, MELD, and SIMIS all contain more than 5 000 samples. When looking at these corpora from the perspective of their total duration, the variability of sample lengths shows its effect. While SIMIS includes more samples than IEMOCAP (almost twice as many), they are a lot shorter on average, leading to a smaller combined corpus duration. A similar picture is found for FAU Aibo and MELD, where the latter contains very long samples while the former's speech recordings are shorter on average. In this context, it is further of interest to take a look at the distribution of the length of audio recordings in the whole EMOSET database. Figure  3  shows a histogram of all sample durations from 0 to 10 seconds -there are a few much longer samples, but they are excluded for keeping the histogram readable. From the plot, it can be seen that most of the samples are between 1 and 5 seconds long, i. e. , when choosing a window size for emotion classification later on, 5 seconds should be enough to capture a sufficiently large portion of the the input samples in their entirety.\n\nObservations about the samples in each dataset regarding their duration can be made on a more granular level from Figure  2  which shows a boxplot for each of the 26 corpora. Here, most datasets exhibit relatively narrow duration ranges in the order of a few seconds. However, the variability of sample durations depends on the recording setting and nature of the databases, with acted and scripted emotion portrayals having a very narrow inner-quartile range while natural or induced emotional speech samples vary more heavily in their length. Thus, it is interesting to look at the databases which stick out from the pack. First of all, ABC has samples that are 10 seconds long on average but shows a large inner-quartile range and minimum durations of under 2 seconds. The variability here can be explained by the nature of the dataset. ABC contains unscripted induced emotional speech recorded in the simulated setting of an airplane. Emotional speech segments were annotated and extracted by experts after the recording, leading to no bounds on sample lengths. Similarly for AD, the speaker turns of angry and neutral speech during phone conversations are generally longer than for other datasets at 10 seconds. This seems reasonable considering that in phone calls, reliance on the actual content of speech is greater to convey one's intentions than in face to face conversations, where additional information is transported through gestures and facial expressions. DES can be seen as a further exception here. While the corpus contains only acted and scripted emotion portrayals, the fact that there are different types of spoken content -short utterances, sentences and fluent passages of text -leads to a very large range of sample durations. The popular benchmark dataset IEMOCAP also shows high variability in this context for another reason. In the database, there are both scripted and improvised sessions of conversations between professional actors. While sample durations for scripted portrayals can be kept quite consistent in length, speaker turns in natural conversations do not conform to any restrictions. MELD which consist of scenes from the popular TV series 'Friends' sitcks out by containing very long samples -some well over a minute long while finally SmartKom shows the highest variability in duration from only a small fraction of a second to minute long recordings captured from the test subjects during interaction with a simulated human-machine interface.\n\nFor the implemented deep learning methods, sample duration variability can be a problem. The DEEP SPECTRUM model needs a fixed length inputs for extracting feature representations from its fully connected layer, resizing the generated spectrograms might lead to a loss in information and make it harder to learn emotionally salient information across databases. For this reason, the approach works with fixed windows of 1 second. While the 2D attention model used in the ResNet based approach enables variable size input spectrograms, duration differences might lead to problems here as well -and similarly for the fixed window in DEEP SPECTRUM.\n\nA very long sample which has been annotated with only one emotional category as a whole might not exhibit this emotion uniformly across its entirety. This can introduce noise into the learning process which might limit the learning capabilities of the investigated models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines",
      "text": "Two baseline systems using unadapted DEEP SPECTRUM and eGeMAPS  [104]  features for emotion recognition are included to compare the performance of transfer learning models trained on EMOSET. Additionally, for the experiments with a residual adapter CNN model, a ResNet is trained from scratch on each of the tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Egemaps",
      "text": "The first baseline system uses the extended version of the Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) extracted with the help of openSMILE. Afterwards, zero mean and unit standardisation is applied to the feature sets. Here, the normalisation parameters are learnt on the training partitions and then fixed to later be applied to the validation and test splits. A linear SVM is then trained for the individual classification tasks. Its complexity parameter is optimised on the validation partitions of each EMOSET corpus on a logarithmic scale from 1 to 10 -9 . With optimised parameters, the SVM is fit again on the combined training and development splits and then evaluated on the test partition of each dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Deep Spectrum",
      "text": "The second baseline is constructed from the DEEP SPECTRUM system. Features are extracted from Mel-spectrogram plots (128 Mel-filters) using VGG16 pre-trained on ImageNet as feature extractor. The plots use the magma colourmap to convert the 2D spectrogram representation to an RGB colour-coded image (see Section 5.1 for more details). No windowing of the input audio samples is applied, i. e. , Mel-spectrogram plots are created from the full duration of each sample. Afterwards, this image is resized to 224 × 224 to conform with the training images of the ImageNet database. As with the eGeMAPs features, a linear SVM classifier is used with the DEEP SPECTRUM features and its parameters are optimised as in Section 4.1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Resnet From Scratch",
      "text": "The performance of transfer experiments with the residual adapters model will be compared against an identically structured ResNet which has been trained from scratch on each of EMOSET's corpora, individually. The description of the model's architecture can therefore be found in Section 5.2. The model is optimised in batches of 64 by Stochastic Gradient Descent (SGD) with a momentum of 0.9 and a learning rate decay of 10 -6 to the weights of all layers. Further, class weighted cross entropy is used as loss to counteract the class imbalance in many datasets, and a l2 regularisation loss term is added with factor 10 -6 . As batch normalisation (BN) is used throughout the network, the training starts with a large initial learning rate of 0.1 which is exponentially decayed by factor 10 when no improvement in validation Unweighted Average Recall (UAR) occurs for 50 epochs. This learning rate decay step happens twice and afterwards the model is trained until no UAR improvement is seen for another 50 epochs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Deep Learning Architectures",
      "text": "For the transfer learning experiments, two deep learning architectures are considered. An ImageNet pre-trained VGG16 network as used in the DEEP SPECTRUM system is finetuned on EMOSET and the approach of parallel residual adapters as described in Section 2.4 and previously used for multidomain visual recognition is adapted and evaluated on the multi-corpus SER problem posed by EMOSET.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Preprocessing",
      "text": "For the transfer learning experiments, the input audio content of the SER data is transformed and pre-processed. Mel-Spectrograms are derived from the spectrogram audio representation. Typically, for speech recognition tasks a window size of around 25 ms is chosen  [22] ,  [52]  for the fast Fourier transform (FFT). For efficient computation, it is best to chose a power of two (in number of samples) as window size, i. e. , an FFT window with 512 samples leads to a close 32 ms for the EMOSET databases that have a fixed (resampled) sampling rate of 16 000 Hz. The windows are further shifted with a hop size of 256 samples along the input audio signal, leading to a window overlap of 50 %.\n\nMel-Spectrograms apply dimensionality reduction to the log-magnitude spectrum with the help of a mel-filter. In the transfer learning experiments with residual adapter models, 64 mel-filters are chosen, as it has been found that a larger number of filters often leads to worse results when used as input for a CNN  [105] ,  [106] . The spectrograms do not have to be resized, as the usage of a two dimensional attention module, which is attached to the fully convolutional feature extractor base, enables handling of variable size input. Nevertheless, an upper bound of 5 s is set on the length of the audio segments from which spectrograms are extracted. This is well above the average duration of an EMOSET sample, but some datasets contain samples which are longer than 30 s. In the case of such a sample, a random 5 s chunk is chosen for feature extraction. As the extraction is done on the fly and on a GPU, this serves as a form of augmentation and should help against overfitting. Shorter samples are converted to spectrograms as they are, and zero-padding is only performed on a per-batch basis to the maximum sample length within this batch.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Parallel Residual Adapters",
      "text": "The second Deep Learning model investigated for this manuscript is built upon the concept of residual adapters, as described in Section 2.4. A residual CNN based on the popular Wide ResNet  [107]  model with the parallel adapter configuration as used in  [26]  is trained on EMOSET.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Architecture",
      "text": "All ResNet models trained on EMOSET contain three submodules (stacks) of residual blocks chained in sequence. Before those submodules, however, an additional convolutional block is applied. This block consists of a convolutional layer with 32 filters of size 3 × 3, which are convolved with the input using a stride of 1. Additionally, BN is applied to the outputs of the convolution. Afterwards, the output is fed through the submodules. The number of filters in the convolutional layers within these submodules doubles for each consecutive one. The first one uses 64, the second 128, and the third one 256 filters of shape 3 × 3. Each of the modules contains two residual blocks, where the blocks are structured as follows. In the very first block, a convolutional layer with a stride of 2 is applied to the input followed by BN and a Rectified Linear Unit (ReLU) activation. A second convolution, this time with a stride of 1, is placed directly afterwards, its output again batch normalised. As the number of filters in the block's first convolution increases compared to the input received from the very first convolution in the network (from 32 to 64), a shortcut connection is needed to add the residual (input to the block) back to the output received after the two convolutions. This is done by first applying average pooling with patches of 2 × 2 and stride 2 to the block input and concatenating this pooled residual with zeroes along the filter dimension. The resulting residual is then added to the output of the second convolution in the block and a ReLU activation is applied. The second block differs only in that it uses a stride of 1 for both of its convolutional layers and further has no need for the shortcut connection as the number of filters does not increase. For the other two submodules, it works the same but the number of filters are increased to 128 and 256, respectively. As for the first module, only the first block in each of the other two modules has convolutions with stride 2 and needs a shortcut connection. A final BN and ReLU activation are placed after the last block of the third module.\n\nInstead of the standard global pooling and fully connected layers applied on top of the convolutional feature extractor base, a 2D self attention layer as proposed in  [105] ,  [106]  is utilised. This layer projects the variable size spatiotemporal output of the CNN to a fixed length weighted sum representation. By doing so, the weights are learnt based on the emotional content that can be found in a specific timefrequency region of the spectrogram. The computation of the weighted sum works as follows (adapted from  [105] ): The output of the convolutional ResNet feature extractor base is three dimensional with size N f × N t × N c , where N f is the number of frequency bins at the output -for an input with 64 mel-filters 8 bins remain after the pooling operations of the chosen architecture have been applied. N t is the number of time-steps in the input spectrogram again downsampled by 8, while N c is the number of channels (256) of the last convolutional layer in the ResNet. The first two dimensions of this representation are now flattened such that a sequence of N c -dimensional vectors x i with length N = N f • N t remains:\n\nEach of these vectors is then transformed into an intermediate learnt representation by a fully connected layer with 256 units and hyperbolic tangent activation. Afterwards, an importance vector is calculated by the inner product of the layer output and a learnable vector u, again of size 256:\n\nThe attention weight for each individual vector is then computed as a smoothed softmax over all e i s:\n\nThe factor λ defines a smoothing of the attention weights.\n\nIf λ = 0, then each of vector gets an equal weight of 1/N, while with λ = 1, no smoothing of the computed attention weights is performed. The first case can be considered as a global average pooling the ResNet output. A value of λ = 0.3 has been found to work well for SER on the IEMOCAP database by Zhao et al. in  [105] ,  [106] , and is adopted here as well. Finally, the output of the module computes the attention weighted sum of the flattened input vectors x 1,... ,N as:\n\nSimilar to the weighted sum of fixed size vectors, c has a fixed size of 256. Compared to standard global average pooling, the attention layer introduces trainable parameters that aim to learn the importance of a specific time-frequency region's features to the training task at hand. Further, this module allows for training with variable length audio sequences. A stack of fully connected layer, BN and dropout is finally put on top of the attention module before the softmax classification layer. The attention layer can contain corpus specific parameters or be shared across datasets while the fully connected stack always serves as a domain specific classifier, i. e. , is not shared between databases, such as the adapter modules. Figure  5  gives a visual overview of the whole model. For transfer learning, residual adapters for each target database are used throughout the whole depth of the network and are applied in parallel to the shared convolutions. The adapter modules are convolutional layers which all contain small 1 × 1 kernels. They share the rest of their settings (strides and number of filters) with the corresponding convolution. The output of each adapter is then added to that of the parallel convolution and fed through the BN layer (cf. Figure  6 ). Further, all BN layers contain parameters which are trained for each database. Depending on the number of neurons in the classification softmax layer, the described architecture has around 3 million parameters in total of which 300 000 are domain specific.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Training Procedure",
      "text": "The experiments can be divided into two parts. For the first set of experiments, the transfer capabilities of a ResNet with adapter modules which are trained on a single main task is evaluated for all tasks in EMOSET. All parameters of the base model are trained on either DEMoS, IEMOCAP, FAU Aibo, or GEMEP. Subsequently, the pre-trained model is transferred by freezing all parameters apart from the task specific classifier head and adapters. Afterwards, the remaining parameters are trained on the new task. The second set of experiments takes multiple corpora into account for training the base model. Here, a model is constructed which includes adapter modules and classifier heads for each of the training tasks while sharing the rest of the parameters.\n\nAfter the training process is finished, the shared parameters are frozen and transferred to a new task from EMOSET , while adapters and classifier are reinitialised. For both kinds of experiments, the following hyperparameters were chosen for the training procedure. The models are optimised via SGD with momentum of 0.9 in batches of 64 examples. Further, as BN is used throughout the model, the initial learning rate is set to a high value of 0.1 and exponentially decayed by a small factor of 10 -6 . Finally, the learning rate is reduced in three steps from 0.1 to 0.01 and ultimately to 0.001. For the single-task transfer experiments and training the baseline models, training is halted and continued with a smaller learning rate if no increase in validation UAR has occurred for 50 epochs of training. While these parameters are applied for both adapter tuning and training from scratch on each of the corpora, full finetuning and tuning only the classifier head start training with a reduced learning rate of 0.01.\n\nWhen training on multiple datasets at once, a roundrobin approach is applied, sampling one batch of each dataset. One round is further considered as one step when defining the learning rate schedule. After a fixed number of 2 500 round-robin steps, the learning rate is set to the according value. After the second step, training continues for an additional 2 500 steps and then stops. After the shared model training, the adapters and classifier heads are further finetuned for each task in the same way as described above for the single-task transfer experiment. This additional finetuning step should help individual models which did not reach their global optimum at the very end of the shared model training procedure.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Cross-Corpus Strategies",
      "text": "For training the proposed deep learning models on the EMOSET corpus, two general directions are followed. First, a (mostly-)shared model can be trained jointly on all corpora in a multi-domain/task fashion. Second, the individual emotion classification corpora can be aggregated into a single large corpus by mapping the different available emotion categories into a shared label space.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Shared Model Multi-Domain Training",
      "text": "For training a shared model with multiple classifier heads on the EMOSET databases simultaneously, batches of samples from the individual datasets are sampled sequentially in a round-robin fashion, only updating parameters specific to this dataset or belonging to the shared model. In addition to the classifier heads, corpus specific residual adapter modules are introduced in parallel to the shared network's convolutions. In practice, this means that a training batch belonging to a specific EMOSET domain is used to update all of the shared model's parameters in addition to the respective domain specific adapters and classifier head.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Aggregated Corpus Training Via Arousal Valence Mapping",
      "text": "The second way of training in the multi-corpus SER setting considers ways of aggregating the individual corpora into a larger shared database. As all databases deal with emotion classification, a suitable approach is to map the annotated emotion categories into a shared label space for training. A fitting mapping is to use a dimensional approach and transform the multi-class problems into binary and three-way arousal/valence classification. As described in Section 2.1, a method for this mapping is to use a model, such as the circumplex of affect  [36] . In Table  2 , such a mapping has been performed for the emotions included over all of EMOSET's databases. In order to prevent the model from learning the class distributions of the EMOSET corpora, random subsampling is performed on a per dataset basis. For each corpus, the number of samples for each of the mapped classes, i. e. , the two arousal and three valence levels, is equal to the respective sample count of the minority class. This is applied",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation",
      "text": "The results for all baselines and each dataset in EMOSET are briefly analysed in Section 7.1. Afterwards, results of the transfer learning experiments with the residual adapters (in Section 7.2), are discussed.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Baseline Results",
      "text": "Three baseline systems are tested for their efficacy on each corpus of the EMOSET separately: i) the eGeMAPS SVM combination (cf. Section 7.1.1), ii) the DEEP SPECTRUM feature extraction with linear SVM (cf. Section 7.1.2), and iii) the ResNet architecture trained from scratch on the mel-spectrograms of each dataset (cf. Section 7.1.3).\n\nThe results of these methods can be found in Table  3 . From a high level view of these results, it can be seen that -among those baselines -there is no overall best approach to solving the SER classifications for every dataset. Rather, depending on the corpus, the best achieved result can be found in any of the models. Furthermore, results for some corpora fluctuate quite heavily, which becomes especially apparent by looking at the two presented types of results of the ResNet trained from scratch on each corpus. These two points are discussed in the following sections where appropriate.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Egemaps",
      "text": "As an expert-designed, handcrafted acoustic feature set specifically intended for paralinguistic speech analysis tasks, eGeMAPS is a competitive baseline for many of the included corpora in EMOSET. While it achieves the best results on the test partition of 12 databases, the margins by which it does differ. Large increases over the other two baselines can be found on ABC, CVE, and GEMEP with a delta of around 10 percentage points compared to the second best approach. Especially GEMEP with its 17 annotated classes seems to benefit from using a small higher-level feature set in combination with an SVM classifier. Slight increases from the other two baselines occur for EmotiW 2014, FAU Aibo, IEMOCAP, MELD, SIMIS, and SmartKom, while performance for the EA datasets (EA-ACT, EA-BMW, and EA-WSJ), SUSAS, and Turkish Emo is on par with the other baseline method. Surprisingly, DEEP SPECTRUM and the ResNet baseline both achieve considerably better results particularly on the test partition of EMO-DB. This might be caused by the optimisation procedure of the baselines utilising SVMs as classifiers. The complexity parameter is optimised from a fixed logarithmic set of values based on the performance on the development partition, which might not always be the best value to choose for performing the fit on the combined training and development partitions for evaluation on test.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Deep Spectrum",
      "text": "With a few exceptions, combining DEEP SPECTRUM features with a linear SVM classifier leads to results comparable to the eGeMAPS baseline method. Negative examples can be found with ABC, DES, and GEMEP, where the DEEP SPECTRUM system falls behind both other baselines. On GVEESS, on the other hand, it achieves the best test set result of 27.9 % compared to the runner-up with 24.7 %. For the EA datasets, EU-EV, and EmoFilm it matches performance with eGeMAPS. In the case of DEMoS, the system lags behind the ResNet trained from scratch considerably but improves on the eGeMAPS baseline. This might be explained by the larger size of the DEEP SPECTRUM features which can provide more discriminative features for the SVM classifier given enough training samples. When looking at BES and MES -two datasets that only differ in the recorded subjects and their spoken language -it can be seen that the system has problems consistently handling small datasets with a few number of speakers: In the case of BES, DEEP SPECTRUM achieves the best result on the test set of all the investigated baselines, while it is considerably less performant on MES. The nature of these datasets also has an impact on the other approaches, but is overall more pronounced here. All results can be found alongside the other two baselines in Table  3 . In the other two baseline approaches, the usage of a linear SVM has an advantage over the neural network approach, in that the combined training and validation data can be used to perform a final fit of the classifier after having found the optimal training parameters. In the ResNet experiments, however, the validation data is used to measure the model's performance and generalisation capabilities during training. Furthermore, with the help of the validation data learning rate adjustments are defined and the training is stopped early before performing the final evaluation on the held-out test set. Thus, the samples in the validation partition have no immediate impact on the weights of the trained model which might limit the generalisation capabilities especially for datasets containing only a few number of samples or speakers.\n\nAs the validation and testing splits contain different sets of speakers, cases exist where validation and testing performance diverge heavily. For example, for DES, the model achieves a test set UAR of 43.3 % against the overall best development performance of 34.7 %, but at the end of training, this discrepancy increases to 52.6 % on test, and only a mere 21.9 % on development which is near chance-level. A very similar picture is observed for EA-ACT, where development and test performance diverge to 12.9 % against 50.0 %. The performance on these datasets is further hindered by the fact that they only contain a small number of training and validation samples. Also, a few of the datasets can be identified as being challenging for the ResNet model in general, such as EU-EV, which is a corpus with a very large number of annotated emotion classes in three different languages. In addition, the training and evaluation setup chosen for EMOSET once more increases the difficulty by partitioning based on language. In the end, UARs of around 10 % are achieved by the model if only the corpus itself is used for training which is in line with the other baselines. For EmotiW 2014, the challenge lies in its multi-modal nature. The corpus addi-tionally contains visual content, which is immensely helpful in the identification and discrimination of emotions through the analysis of facial characteristics. As could already be seen in the baseline using eGeMAPS features and a linear SVM classifier, information extracted from only the audio content does not lead to favourable results for this corpus. On both SIMIS and SmartKom, the ResNet achieves only very weak results which are slightly above chancelevel. Here, DEEP SPECTRUM and eGeMAPS perform better. For eNTERFACE on the other hand, this approach substantially outperforms the DEEP SPECTRUM system reaching a test set UAR of over 80 %. Furthermore, for this dataset, the UARs at the best and final model checkpoint are consistent, i. e. , the model converged to an optimum. Especially weak performance can be found on CVE, with the ResNet trained from scratch falling behind the other two baselines by more than 20 % UAR when compared to eGeMAPS, and 15 % against DEEP SPECTRUM on both development and test partitions. Having only 4 speakers in total, this is another dataset where the danger of overfitting to the training data is especially high for a deep learning model that is trained from the raw audio content. Here, the eGeMAPS and DEEP SPECTRUM have the advantage of utilising abstracted feature representations in the form of an expert-designed, hand-crafted audio descriptor set and high-level image representations learnt from the task of object recognition. The strongest ResNet result is achieved for DEMoS with a test set UAR of 73.8 % which is around 30 % above the other baseline methods. This can be explained by the partitioning of the dataset for EMOSET which has separated non-prototypical from prototypical emotion portrayals in a speaker dependent way. As the ResNet approach learns most directly from the raw input data, fine-grained emotionally discriminative features for each speaker can be learnt from the low-level spectrogram representation. Adding layers of abstraction to the feature representation in the eGeMAPS and DEEP SPECTRUM baselines hides away this information.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Parallel Residual Adapters",
      "text": "In the case of the parallel residual adapter models trained on EMOSET, experiments and their evaluation are proceeded in a slightly different way. In a first set of experiments, denoted as \"single-task transfer\", four tasks out of EMOSET were chosen as base tasks for pre-training the deep learning architectures while for \"multi-task transfer\", all EMOSET corpora are used to pre-train a shared model. In both cases, afterwards, a transfer experiment is run for each EMOSET task by training only the adapter modules on the individual tasks data. Finally, the performance of the transfer learning approach is evaluated by comparing the achieved test set UAR to that of a model with same architecture trained from scratch on the specific task, and to more traditional transfer approaches, i. e. , full finetuning and classifier head tuning.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Single-Task Transfer",
      "text": "The EMOSET corpora DEMoS, FAU Aibo, GEMEP, and IEMOCAP are chosen as base pre-training tasks based on a number of qualitative and quantitative characteristics: With DEMoS, IEMOCAP, and GEMEP, three acted SER corpora are included which contain either a large amount of training samples (DEMoS, and IEMOCAP) or annotated classes (GEMEP).\n\nFor brevity, we leave out the detailed results of these transfer experiments and only summarise our findings. When comparing against a ResNet that is trained on the target corpus from scratch, adapter tuning from a model trained on a single source corpus achieved mixed results.\n\nWhile for some, especially smaller tasks, such as BES or Speech Emotion Database of the Institute of Automation of the Chinese Academy of Sciences (CASIA) increases in UAR could be observed, in most cases, performance on both development and test partitions stayed roughly the same. A noteworthy negative example was eNTERFACE, for which the performance drops noticeably compared to training a full model on only the target data. A model trained exclusively on this dataset is able to achieve a UAR of 81 % while the best result for the transfer experiments was more than 5 percentage points below that at 75 % UAR. On the other hand, SmartKom, a large but difficult corpus of natural emotional speech, seemed to benefit from pre-training and adapter transfer in all cases. Nevertheless, these results were encouraging when observed from another perspective. They showed that the features learnt by the ResNet model from Mel-spectrograms of one SER corpus can be used reasonably well for a large range of SER tasks by simply introducing a small amount of additional parameters -the adapter modules.\n\nIn addition to comparisons against training models on each task from scratch, the residual adapters approach should be related to more traditional finetuning strategies. The pre-trained models can be taken as feature extractors and only the last classification layers are re-trained for each task. This method tunes an even smaller amount of parameters for each corpus, further decreasing the risk of overfitting but having decreased learning capabilities.\n\nA comparison between adapter and classifier head tuning is made in Figure  7a . Apart from very few exceptions, it can be seen that adapter tuning beats the feature-extraction transfer approach in terms of UAR both on the development and test partitions. Notable outliers can be found with CASIA and CVE hinting at possible overfitting with the adapters approach. Moreover, for datasets with a large number of classes, e. g. , GVEESS or EU-EV performance on test can vary greatly from run to run, leading to classifier head tuning sometimes outperforming the adapter tuning.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multi-Task Transfer",
      "text": "The second set of experiments using the residual adapters train a shared base network for all EMOSET datasets while only the adapter modules and final classification layers are specific to each dataset. In this respect, every task has influence on the weights of the shared feature extraction base while still containing task specific parameters to account for inter-corpus variance. As sharing the 2D attention layer between different corpora was found to have only a minor impact on performance in the initial single-task transfer experiments (cf. Section 7.2.1), for multi-domain training, this module is further contained in the shared base model. Here, the performance differences between models trained from scratch and adapted from a pre-trained shared     [108] . Statistically significant (p < 0.05) differences in the proportion of errors are marked in Table  4 . It should be noted that, due to the highly unbalanced nature of some included databases, a performance increase as measured by the test does not necessarily correspond to a higher achieved test set UAR. In the following, whenever differences in the results are described as significant, they are so at p < 0.05.\n\nThe results for multi-task transfer experiments with a ResNet architecture are visualised in Figure  7b . Four different settings are analysed: First, training on all corpora in a multi-domain setting as described in Section 6.1, then, the other three settings are given by training on both the aggregated arousal and valence mapped data, either together (A+V) or separately (A and V). Apart from a couple of outliers, model performance for all of EMOSET's corpora either increases or stays the same compared to training a full ResNet model for every dataset when using the adapter transfer approach. Two negative examples are DES and EA-ACT, where performance on the development partition only ever slightly increases above chance level. Contrary to this, the performance on the test partition shows an increase for these two datasets that is significant in the case of using the valence pre-trained model. As it is already evident in the baseline results, this behaviour is most likely due to the corpora only containing a small number of samples and DES's validation and test partition only containing one speaker each, leading to diverging results. Again, CASIA and CVE seem to benefit from the transfer learning approach, leading to increases in both development and test set UARs from their near chance level performance when training a full ResNet model on their corpus data alone. For the particularly popular baseline SER corpus EMO-DB, performance is increased on both the development and test by around 10 %, the same is true for the Turkish emotional speech database. Both of these increases are further statistically significant measured by a McNemar's test.\n\nFor the choice of training data, training on all of EMOSET in a round-robin fashion seems to lead to the best results on both development and test partitions. However, it seems to be closely followed by training the model on the aggregated arousal data alone -suggesting the features learnt from discriminating arousal across corpora can be effectively tuned for various SER tasks with the help of residual adapter modules. As arousal is generally easier to detect from audio recordings of speech than valence, which is more effectively conveyed and perceived from visual information, such as gestures and mimic, training on the three class valence problem might have been too difficult for the network, thus not leading to emotionally salient feature representations. Further, when combining arousal and valence aggregated corpora for training, the individual strengths of pre-training on either corpus do not seem to be complementary.\n\nAs evident from Table  4 , the multi-task transfer experiments lead to increased results for 21 of the 26 databases included in EMOSET. For 10 corpora, some of these increases are further statistically significant. Only for 2 databases -eNTERFACE and MES -results are always significantly worse. For eNTERFACE this can be explained by the strong performance achieved by a model trained on the corpus from scratch which also beats all of the other considered baselines (cf. Table  3 ).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion And Outlook",
      "text": "In this manuscript, we presented a novel deep learning based, multi-corpus SER framework -EMONET -that makes use of state-of-the-art deep transfer learning approaches. We publicly release the framework including a flexible command line interface on GitHub, such that interested researchers can use it in their own work for a variety of multi-corpus speech and audio analysis tasks. EMONET was investigated and evaluated for the task of multi-corpus SER on a collection of 26 existing SER corpora. EMONET adapts the residual adapter approach for multidomain visual recognition was to the task of SER from melspectrograms.\n\nNeeding only a small portion of additional parameters for each database, it allowed for effective multi-domain training on EMOSET, leading to favourable results when compared to training models from scratch or adapting only the classifier head to each corpus. When all of EMOSET is utilised for training, either in a multi-domain fashion or by aggregating the corpora by mapping the included categories to arousal and valence classes, test set performance increases could be achieved for 21 of the 26 corpora. For ten databases, these improvements were statistically significant while on the other hand there are only two datasets (eNTERFACE and MES) that seem to always be negatively affected by the approach. Compared to fully finetuning a pre-trained model (which requires around ten times the number of trainable parameters), the adapter approach often came out on top.\n\nThe results with utilising the residual adapter model for transfer and multi-domain learning in SER motivates further research and exploration. One limitation of the work presented herein is that the base architecture of the ResNet has not been extensively optimised for a speech recognition task. Here, different configurations and variations, e. g. , with the number of filters, depth and width of the network, should be evaluated. Moreover, for purposes of constraining computational and time requirements in favour of exploring a wider range of transfer learning settings, the model was kept quite small. Increasing the model size could further improve performance but would require adding a larger amount of training data. This training data could come from large scale audio recognition databases that are not immediately related to SER, such as AudioSet  [47]  or the large scale speaker recognition dataset VoxCeleb  [109] . Having found an optimised model architecture for training, improvements could further be made by experimenting with the degree of influence each EMOSET corpus has on the shared model weights during training. This could for example be investigated by adjusting the probability of sampling a batch from a specific dataset compared to the default round-robin strategy utilised in this manuscript. For the different problem of multi-lingual large-scale ASR, residual adapters and probabilistic sampling have been explored in combination with an RNN architecture trained on Mel-spectrogram input  [110] . As RNNs are a popular choice for SER  [53] ,  [57] , evaluating the residual adapter approach with these networks in a multi-corpus training setting should be considered. Furthermore, both CNNs and RNNs could be modified with adapter modules and then trained simultaneously, combining their high level feature representations. For single-corpus SER without adapter modules this has been done in  [105]  with state-of-the-art results. Moreover, so far only SER corpora with categorical labels have been considered. Using a multi-domain learning model based on residual adapters, adding databases that are labelled with the dimensional approach and pose regression problems would be possible to further increase the size of the training data. Finally, transferring knowledge between different domains of paralinguistic speech recognition, e. g. , the detection of deception from speech, with the help of the adapter approach can be investigated.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the DEEP SPECTRUM feature extraction system. Audio segments are ﬁrst converted to a suitable image",
      "page": 3
    },
    {
      "caption": "Figure 1: The features extracted by the DEEP SPECTRUM system",
      "page": 3
    },
    {
      "caption": "Figure 2: Boxplot of sample durations for each EMOSET corpus. Boxes show the inner-quartile range (IQR) and the whiskers extend",
      "page": 4
    },
    {
      "caption": "Figure 3: shows a histogram of",
      "page": 5
    },
    {
      "caption": "Figure 2: which shows a boxplot for each of the 26",
      "page": 6
    },
    {
      "caption": "Figure 3: Histogram of sample durations in EMOSET. Most of the",
      "page": 6
    },
    {
      "caption": "Figure 4: Sample Mel spectrogram images created from speech recordings of IEMOCAP for each of its four base emotion categories.",
      "page": 8
    },
    {
      "caption": "Figure 5: Architecture of the base ResNet model used in the experiments for multi-corpus SER. Three convolutional stacks extract",
      "page": 8
    },
    {
      "caption": "Figure 5: gives a visual overview of the",
      "page": 8
    },
    {
      "caption": "Figure 6: Depiction of a residual adapter module. The adapter is a",
      "page": 9
    },
    {
      "caption": "Figure 6: ). Further, all BN layers contain parameters",
      "page": 9
    },
    {
      "caption": "Figure 7: a. Apart from very few exceptions, it",
      "page": 12
    },
    {
      "caption": "Figure 7: b. Four differ-",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "+0.0 dB +0.0 dB +0.0 dB +0.0 dB\n4.096 4.096 -20.0 dB 4.096 -20.0 dB 4.096 -20.0 dB -20.0 dB\n]zHk[ -40.0 dB ]zHk[ ]zHk[ ]zHk[\n2.048 2.048 2.048 -40.0 dB 2.048 -40.0 dB -40.0 dB\nycneuqerF ycneuqerF ycneuqerF ycneuqerF\n-60.0 dB\n1.024 1.024 -60.0 dB 1.024 1.024 -60.0 dB -60.0 dB\n-80.0 dB\n0.512 0.512 -80.0 dB 0.512 0.512 -80.0 dB -80.0 dB\n-100.0 dB\n-100.0 dB\n0.0 0.0 -120.0 dB 0.0 0.0 -100.0 dB -100.0 dB\n0 2 4 6 0 1 2 0 1 2 3 0 1.2 2.4 3.6 4.8": "Time [s] Time [s] Time [s] Time [s]\n:SampleMelspectrogramimagescreatedfromspeechrecordingsofIEMOCAPforeachofitsfourbaseemotioncatego\nlefttoright:angry,happy,neutral,andsad.\nmel spectograms\nconv 3x3 batch\n#f = 32 norm\nResidual Stack (nf= 64) Residual Stack (nf= 128) Residual Stack (nf= 256)\n2D batch"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.096\n]zHk[\n2.048\nycneuqerF\n1.024\n0.512\n0.0\n0 2 4": "",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n-40.0 dB\n2.048\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0 -120.0 dB\n6 0": "",
          "Column_3": "",
          "Column_4": "+0.0 dB\n-20.0 dB\n-40.0 dB\n-60.0 dB\n-80.0 dB\n-100.0 dB",
          "Column_5": "",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0\n0": "4.0\n]zHk[\n2.0\nycneuqerF\n1.0\n0.5",
          "Column_7": "+0.0 dB\n-20.0 dB\n-40.0 dB\n-60.0 dB\n-80.0 dB\n-100.0 dB",
          "Column_8": "",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n1.024 -60.0 dB\n0.512 -80.0 dB\n0.0 -100.0 dB\n3 0 1.2": "4.0\n]zHk[\n2.0\nycneuqerF\n1.0\n0.5",
          "+0.0 dB\n-20.0 dB\n-40.0 dB\n-60.0 dB\n-80.0 dB\n-100.0 dB\n2.4 3.6 4.8": ""
        },
        {
          "4.096\n]zHk[\n2.048\nycneuqerF\n1.024\n0.512\n0.0\n0 2 4": "",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n-40.0 dB\n2.048\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0 -120.0 dB\n6 0": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "1 2",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0\n0": "",
          "Column_7": "",
          "Column_8": "1 2",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n1.024 -60.0 dB\n0.512 -80.0 dB\n0.0 -100.0 dB\n3 0 1.2": "",
          "+0.0 dB\n-20.0 dB\n-40.0 dB\n-60.0 dB\n-80.0 dB\n-100.0 dB\n2.4 3.6 4.8": ""
        },
        {
          "4.096\n]zHk[\n2.048\nycneuqerF\n1.024\n0.512\n0.0\n0 2 4": "Time [s]",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n-40.0 dB\n2.048\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0 -120.0 dB\n6 0": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Time [s]",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n-60.0 dB\n1.024\n-80.0 dB\n0.512\n-100.0 dB\n0.0\n0": "",
          "Column_7": "",
          "Column_8": "Time [s]",
          "+0.0 dB\n4.096 -20.0 dB\n]zHk[\n2.048 -40.0 dB\nycneuqerF\n1.024 -60.0 dB\n0.512 -80.0 dB\n0.0 -100.0 dB\n3 0 1.2": "",
          "+0.0 dB\n-20.0 dB\n-40.0 dB\n-60.0 dB\n-80.0 dB\n-100.0 dB\n2.4 3.6 4.8": "Time [s]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "56.5": "85.8",
          "53.4": "81.4",
          "50.1": "84.6",
          "52.7": "86.5"
        },
        {
          "56.5": "58.3",
          "53.4": "56.7",
          "50.1": "61.7",
          "52.7": "60.0"
        },
        {
          "56.5": "34.7",
          "53.4": "38.0",
          "50.1": "36.3",
          "52.7": "36.3"
        },
        {
          "56.5": "31.4",
          "53.4": "37.5",
          "50.1": "42.0",
          "52.7": "38.3"
        },
        {
          "56.5": "",
          "53.4": "83.2",
          "50.1": "83.0",
          "52.7": "84.5"
        },
        {
          "56.5": "42.3",
          "53.4": "30.0",
          "50.1": "24.0",
          "52.7": "35.8"
        },
        {
          "56.5": "34.3",
          "53.4": "30.0",
          "50.1": "34.3",
          "52.7": "30.0"
        },
        {
          "56.5": "62.6",
          "53.4": "67.1",
          "50.1": "70.3",
          "52.7": "65.3"
        },
        {
          "56.5": "100.0",
          "53.4": "100.0",
          "50.1": "100.0",
          "52.7": "100.0"
        },
        {
          "56.5": "68.3",
          "53.4": "62.6",
          "50.1": "70.5",
          "52.7": "61.5"
        },
        {
          "56.5": "70.7",
          "53.4": "54.1",
          "50.1": "51.9",
          "52.7": "55.9"
        },
        {
          "56.5": "13.4",
          "53.4": "15.2",
          "50.1": "11.1",
          "52.7": "11.5"
        },
        {
          "56.5": "46.6",
          "53.4": "45.5",
          "50.1": "46.0",
          "52.7": "43.4"
        },
        {
          "56.5": "28.3",
          "53.4": "30.5",
          "50.1": "33.8",
          "52.7": "35.5"
        },
        {
          "56.5": "52.1",
          "53.4": "",
          "50.1": "51.4",
          "52.7": "52.7"
        },
        {
          "56.5": "44.2",
          "53.4": "41.6",
          "50.1": "",
          "52.7": "42.0"
        },
        {
          "56.5": "36.5",
          "53.4": "33.7",
          "50.1": "35.6",
          "52.7": "29.8"
        },
        {
          "56.5": "54.0",
          "53.4": "52.5",
          "50.1": "52.6",
          "52.7": ""
        },
        {
          "56.5": "25.3",
          "53.4": "25.8",
          "50.1": "25.4",
          "52.7": "23.7"
        },
        {
          "56.5": "65.0",
          "53.4": "56.7",
          "50.1": "70.0",
          "52.7": "76.7"
        },
        {
          "56.5": "70.8",
          "53.4": "75.0",
          "50.1": "70.9",
          "52.7": "72.8"
        },
        {
          "56.5": "40.9",
          "53.4": "43.2",
          "50.1": "43.0",
          "52.7": "44.0"
        },
        {
          "56.5": "25.4",
          "53.4": "23.4",
          "50.1": "25.0",
          "52.7": "24.8"
        },
        {
          "56.5": "59.4",
          "53.4": "63.9",
          "50.1": "60.5",
          "52.7": "52.5"
        },
        {
          "56.5": "63.6",
          "53.4": "65.9",
          "50.1": "60.2",
          "52.7": "73.9"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "41.1": "71.2",
          "36.4": "69.6",
          "27": "65",
          ".3": ".2",
          "45.5": "72.2"
        },
        {
          "41.1": "61.7",
          "36.4": "56.7",
          "27": "60",
          ".3": ".0",
          "45.5": "48.3"
        },
        {
          "41.1": "29.7",
          "36.4": "25.3",
          "27": "36",
          ".3": ".3",
          "45.5": "31.0"
        },
        {
          "41.1": "36.0",
          "36.4": "22.2",
          "27": "38",
          ".3": ".1",
          "45.5": "27.6"
        },
        {
          "41.1": "",
          "36.4": "68.8",
          "27": "66",
          ".3": ".1",
          "45.5": "69.6"
        },
        {
          "41.1": "37.2",
          "36.4": "43.6",
          "27": "31",
          ".3": ".8",
          "45.5": "45.9"
        },
        {
          "41.1": "31.4",
          "36.4": "41.4",
          "27": "31",
          ".3": ".4",
          "45.5": "31.4"
        },
        {
          "41.1": "33.3",
          "36.4": "33.0",
          "27": "40",
          ".3": ".0",
          "45.5": "33.3"
        },
        {
          "41.1": "96.2",
          "36.4": "98.1",
          "27": "98",
          ".3": ".1",
          "45.5": "96.2"
        },
        {
          "41.1": "64.1",
          "36.4": "55.2",
          "27": "55",
          ".3": ".6",
          "45.5": "54.2"
        },
        {
          "41.1": "74.8",
          "36.4": "64.8",
          "27": "59",
          ".3": ".0",
          "45.5": "61.4e"
        },
        {
          "41.1": "10.1",
          "36.4": "9.8",
          "27": "6.",
          ".3": "8",
          "45.5": "ksat_r\n8.3"
        },
        {
          "41.1": "43.7",
          "36.4": "43.6",
          "27": "46",
          ".3": ".1",
          "45.5": "efsnart\n46.9"
        },
        {
          "41.1": "21.8",
          "36.4": "21.2",
          "27": "26",
          ".3": ".9",
          "45.5": "25.0"
        },
        {
          "41.1": "36.6",
          "36.4": "",
          "27": "35",
          ".3": ".0",
          "45.5": "36.0"
        },
        {
          "41.1": "28.7",
          "36.4": "24.5",
          "27": "",
          ".3": "",
          "45.5": "25.1"
        },
        {
          "41.1": "17.6",
          "36.4": "26.4",
          "27": "23",
          ".3": ".6",
          "45.5": "16.8"
        },
        {
          "41.1": "53.7",
          "36.4": "53.7",
          "27": "54",
          ".3": ".4",
          "45.5": ""
        },
        {
          "41.1": "23.1",
          "36.4": "19.2",
          "27": "22",
          ".3": ".4",
          "45.5": "20.9"
        },
        {
          "41.1": "68.3",
          "36.4": "58.3",
          "27": "65",
          ".3": ".0",
          "45.5": "68.3"
        },
        {
          "41.1": "42.4",
          "36.4": "43.4",
          "27": "39",
          ".3": ".9",
          "45.5": "43.7"
        },
        {
          "41.1": "28.1",
          "36.4": "29.4",
          "27": "30",
          ".3": ".0",
          "45.5": "26.3"
        },
        {
          "41.1": "25.7",
          "36.4": "31.9",
          "27": "24",
          ".3": ".5",
          "45.5": "27.9"
        },
        {
          "41.1": "54.3",
          "36.4": "52.3",
          "27": "56",
          ".3": ".9",
          "45.5": "55.1"
        },
        {
          "41.1": "54.5",
          "36.4": "54.5",
          "27": "54",
          ".3": ".5",
          "45.5": "61.4T"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "%": "",
          "53.8": "85.4",
          "58.0": "88.1",
          "56.8": "84.0",
          "54.9": "88.9"
        },
        {
          "%": "%",
          "53.8": "68.3",
          "58.0": "70.0",
          "56.8": "60.0",
          "54.9": "65.0"
        },
        {
          "%": "",
          "53.8": "36.0",
          "58.0": "41.0",
          "56.8": "38.3",
          "54.9": "35.7"
        },
        {
          "%": "",
          "53.8": "48.2",
          "58.0": "33.0",
          "56.8": "48.9",
          "54.9": "39.3"
        },
        {
          "%": "%",
          "53.8": "86.0",
          "58.0": "84.0",
          "56.8": "85.0",
          "54.9": "81.4"
        },
        {
          "%": "",
          "53.8": "22.4",
          "58.0": "34.4",
          "56.8": "30.3",
          "54.9": "29.6"
        },
        {
          "%": "",
          "53.8": "22.9",
          "58.0": "37.1",
          "56.8": "38.6",
          "54.9": "41.4"
        },
        {
          "%": "%",
          "53.8": "67.3",
          "58.0": "66.3",
          "56.8": "68.1",
          "54.9": "66.3"
        },
        {
          "%": "1",
          "53.8": "00.0",
          "58.0": "100.0",
          "56.8": "100.0",
          "54.9": "100.0"
        },
        {
          "%": "",
          "53.8": "73.2",
          "58.0": "61.1",
          "56.8": "74.7",
          "54.9": "64.0"
        },
        {
          "%": "",
          "53.8": "65.2",
          "58.0": "57.8",
          "56.8": "61.9",
          "54.9": "54.8"
        },
        {
          "%": "",
          "53.8": "14.9",
          "58.0": "11.7",
          "56.8": "13.0",
          "54.9": "10.0"
        },
        {
          "%": "",
          "53.8": "47.3",
          "58.0": "44.1",
          "56.8": "47.2",
          "54.9": "42.2"
        },
        {
          "%": "",
          "53.8": "28.7",
          "58.0": "29.3",
          "56.8": "29.8",
          "54.9": "33.2"
        },
        {
          "%": "",
          "53.8": "50.1",
          "58.0": "53.9",
          "56.8": "54.5",
          "54.9": "53.1"
        },
        {
          "%": "",
          "53.8": "50.7",
          "58.0": "43.3",
          "56.8": "44.5",
          "54.9": "41.8"
        },
        {
          "%": "",
          "53.8": "31.7",
          "58.0": "35.1",
          "56.8": "34.1",
          "54.9": "36.5"
        },
        {
          "%": "%",
          "53.8": "50.4",
          "58.0": "52.2",
          "56.8": "51.3",
          "54.9": "50.3"
        },
        {
          "%": "",
          "53.8": "24.7",
          "58.0": "25.6",
          "56.8": "25.8",
          "54.9": "24.6"
        },
        {
          "%": "",
          "53.8": "56.7",
          "58.0": "56.7",
          "56.8": "50.0",
          "54.9": "56.7"
        },
        {
          "%": "%",
          "53.8": "77.7",
          "58.0": "70.7",
          "56.8": "75.0",
          "54.9": "78.7"
        },
        {
          "%": "",
          "53.8": "42.7",
          "58.0": "44.6",
          "56.8": "41.7",
          "54.9": "45.1"
        },
        {
          "%": "",
          "53.8": "23.7",
          "58.0": "24.5",
          "56.8": "25.5",
          "54.9": "26.0"
        },
        {
          "%": "%",
          "53.8": "58.8",
          "58.0": "61.1",
          "56.8": "57.6",
          "54.9": "57.9"
        },
        {
          "%": "",
          "53.8": "65.9",
          "58.0": "75.0",
          "56.8": "69.3",
          "54.9": "63.6"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "38.4": "72.3",
          "35.8": "72.2",
          "37.5": "61.3",
          "34.7": "68.2"
        },
        {
          "38.4": "66.7",
          "35.8": "58.3",
          "37.5": "60.0",
          "34.7": "71.7"
        },
        {
          "38.4": "32.0",
          "35.8": "31.7",
          "37.5": "37.3",
          "34.7": "26.3"
        },
        {
          "38.4": "47.6",
          "35.8": "35.5",
          "37.5": "34.8",
          "34.7": "35.8"
        },
        {
          "38.4": "73.8",
          "35.8": "68.7",
          "37.5": "69.7",
          "34.7": "63.7"
        },
        {
          "38.4": "54.0",
          "35.8": "56.1",
          "37.5": "46.0",
          "34.7": "55.6"
        },
        {
          "38.4": "40.0",
          "35.8": "32.9",
          "37.5": "51.4",
          "34.7": "51.4"
        },
        {
          "38.4": "43.3",
          "35.8": "39.6",
          "37.5": "50.0",
          "34.7": "36.7"
        },
        {
          "38.4": "98.1",
          "35.8": "100.0",
          "37.5": "100.0",
          "34.7": "100.0"
        },
        {
          "38.4": "72.6",
          "35.8": "61.1",
          "37.5": "68.4",
          "34.7": "64.8"
        },
        {
          "38.4": "70.0",
          "35.8": "62.9",
          "37.5": "61.9",
          "34.7": "59.5"
        },
        {
          "38.4": "10.0",
          "35.8": "9.5",
          "37.5": "11.1",
          "34.7": "9.2"
        },
        {
          "38.4": "48.2",
          "35.8": "47.0",
          "37.5": "45.0",
          "34.7": "44.5"
        },
        {
          "38.4": "29.4",
          "35.8": "17.9",
          "37.5": "24.4",
          "34.7": "24.2"
        },
        {
          "38.4": "34.0",
          "35.8": "34.7",
          "37.5": "37.4",
          "34.7": "35.0"
        },
        {
          "38.4": "29.1",
          "35.8": "26.8",
          "37.5": "29.2",
          "34.7": "27.3"
        },
        {
          "38.4": "18.1",
          "35.8": "21.3",
          "37.5": "23.9",
          "34.7": "24.7"
        },
        {
          "38.4": "52.1",
          "35.8": "53.1",
          "37.5": "54.3",
          "34.7": "51.4"
        },
        {
          "38.4": "20.2",
          "35.8": "20.8",
          "37.5": "20.3",
          "34.7": "19.4"
        },
        {
          "38.4": "58.3",
          "35.8": "56.7",
          "37.5": "63.3",
          "34.7": "56.7"
        },
        {
          "38.4": "47.9",
          "35.8": "43.4",
          "37.5": "41.6",
          "34.7": "44.7"
        },
        {
          "38.4": "28.0",
          "35.8": "27.5",
          "37.5": "26.5",
          "34.7": "26.4"
        },
        {
          "38.4": "21.7",
          "35.8": "21.0",
          "37.5": "24.2",
          "34.7": "29.3"
        },
        {
          "38.4": "53.9",
          "35.8": "44.9",
          "37.5": "59.6",
          "34.7": "42.4"
        },
        {
          "38.4": "70.5",
          "35.8": "52.3",
          "37.5": "61.4",
          "34.7": "56.8"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Alexa, siri, cortana, and more: an introduction to voice assistants",
      "authors": [
        "M Hoy"
      ],
      "year": "2018",
      "venue": "Medical reference services quarterly"
    },
    {
      "citation_id": "2",
      "title": "Alexa vs. siri vs. cortana vs. google assistant: a comparison of speech-based natural user interfaces",
      "authors": [
        "L Quesada",
        "L Guerrero"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Applied Human Factors and Ergonomics"
    },
    {
      "citation_id": "3",
      "title": "Conversational ai: The science behind the alexa prize",
      "authors": [
        "A Ram",
        "R Prasad",
        "C Khatri",
        "A Venkatesh",
        "R Gabriel",
        "Q Liu",
        "J Nunn",
        "B Hedayatnia",
        "M Cheng",
        "A Nagar",
        "E King",
        "K Bland",
        "A Wartick",
        "Y Pan",
        "H Song",
        "S Jayadevan",
        "G Hwang",
        "A Pettigrue"
      ],
      "year": "2018",
      "venue": "Conversational ai: The science behind the alexa prize"
    },
    {
      "citation_id": "4",
      "title": "Common sense, the Turing test, and the quest for real AI",
      "authors": [
        "H Levesque"
      ],
      "year": "2017",
      "venue": "Common sense, the Turing test, and the quest for real AI"
    },
    {
      "citation_id": "5",
      "title": "Emotion in man and animal; its nature and relation to attitude and motive",
      "authors": [
        "P Young"
      ],
      "year": "1943",
      "venue": "Emotion in man and animal; its nature and relation to attitude and motive"
    },
    {
      "citation_id": "6",
      "title": "Psychology, henry holt & company",
      "authors": [
        "R Woodworth"
      ],
      "year": "1940",
      "venue": "Psychology, henry holt & company"
    },
    {
      "citation_id": "7",
      "title": "Motivation of behavior",
      "authors": [
        "P Young"
      ],
      "year": "1936",
      "venue": "Motivation of behavior"
    },
    {
      "citation_id": "8",
      "title": "Emotional intelligence",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Imagination, cognition and personality"
    },
    {
      "citation_id": "9",
      "title": "Emotional intelligence in everyday life",
      "authors": [
        "J Ciarrochi",
        "J Forgas",
        "J Mayer"
      ],
      "year": "2006",
      "venue": "Psychology Press"
    },
    {
      "citation_id": "10",
      "title": "Emotional intelligence meets traditional standards for an intelligence",
      "authors": [
        "J Mayer",
        "D Caruso",
        "P Salovey"
      ],
      "year": "1999",
      "venue": "Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Emotional intelligence as a standard intelligence",
      "authors": [
        "J Mayer",
        "P Salovey",
        "D Caruso",
        "G Sitarenios"
      ],
      "year": "2001",
      "venue": "Emotional intelligence as a standard intelligence"
    },
    {
      "citation_id": "12",
      "title": "Why emotions should be integrated into conversational agents",
      "authors": [
        "C Becker",
        "S Kopp",
        "I Wachsmuth"
      ],
      "year": "2007",
      "venue": "Conversational informatics: an engineering approach"
    },
    {
      "citation_id": "13",
      "title": "Emotion and personality in a conversational agent",
      "authors": [
        "G Ball",
        "J Breese"
      ],
      "year": "2000",
      "venue": "Emotion and personality in a conversational agent"
    },
    {
      "citation_id": "14",
      "title": "Exploiting iot technologies for enhancing health smart homes through patient identification and emotion recognition",
      "authors": [
        "L Mano",
        "L Nakamura",
        "P Gomes",
        "G Libralon",
        "R Meneguete",
        "P Geraldo Filho",
        "G Giancristofaro",
        "G Pessin",
        "B Krishnamachari"
      ],
      "year": "2016",
      "venue": "Computer Communications"
    },
    {
      "citation_id": "15",
      "title": "Activity and emotion recognition to support early diagnosis of psychiatric diseases",
      "authors": [
        "D Tacconi",
        "O Mayora",
        "P Lukowicz",
        "B Arnrich",
        "C Setz",
        "G Troster",
        "C Haring"
      ],
      "year": "2008",
      "venue": "Proceedings of the International Conference on Pervasive Computing Technologies for Healthcare"
    },
    {
      "citation_id": "16",
      "title": "Usage of emotion recognition in military health care",
      "authors": [
        "S Tokuno",
        "G Tsumatori",
        "S Shono",
        "E Takei",
        "T Yamamoto",
        "G Suzuki",
        "S Mituyoshi",
        "M Shimura"
      ],
      "year": "2011",
      "venue": "Proceedings of the Defense Science Research Conference and Expo (DSR)"
    },
    {
      "citation_id": "17",
      "title": "An investigation of depressed speech detection: Features and normalization",
      "authors": [
        "N Cummins",
        "J Epps",
        "M Breakspear",
        "R Goecke"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "18",
      "title": "Detection of clinical depression in adolescentsâ€™ speech during family interactions",
      "authors": [
        "L.-S Low",
        "N Maddage",
        "M Lech",
        "L Sheeber",
        "N Allen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "19",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "N Cummins",
        "S Scherer",
        "J Krajewski",
        "S Schnieder",
        "J Epps",
        "T Quatieri"
      ],
      "year": "2015",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Diagnosis of depression by behavioural signals: a multimodal approach",
      "authors": [
        "N Cummins",
        "J Joshi",
        "A Dhall",
        "V Sethu",
        "R Goecke",
        "J Epps"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "21",
      "title": "An investigation of emotional speech in depression classification",
      "authors": [
        "B Stasak",
        "J Epps",
        "N Cummins",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "Proceedings of the Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "23",
      "title": "Feature pooling of modulation spectrum features for improved speech emotion recognition in the wild",
      "authors": [
        "A Avila",
        "Z Momin",
        "J Santos",
        "D O'shaughnessy",
        "T Falk"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "25",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Efficient parametrization of multi-domain deep neural networks",
      "authors": [
        "S.-A Rebuffi",
        "H Bilen",
        "A Vedaldi"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "27",
      "title": "Learning multiple visual domains with residual adapters",
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Bag-of-deep-features: Noise-robust deep feature representations for audio analysis",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "S Pugachevskiy",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "29",
      "title": "Emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "1988",
      "venue": "Emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "30",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "31",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "32",
      "title": "Expression and the nature of emotion",
      "year": "1984",
      "venue": "Approaches to emotion"
    },
    {
      "citation_id": "33",
      "title": "The nature of emotion: Fundamental questions",
      "authors": [
        "P Ekman",
        "R Davidson"
      ],
      "year": "1994",
      "venue": "The nature of emotion: Fundamental questions"
    },
    {
      "citation_id": "34",
      "title": "The emotions",
      "authors": [
        "N Frijda"
      ],
      "year": "1986",
      "venue": "The emotions"
    },
    {
      "citation_id": "35",
      "title": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "36",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "37",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "39",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "S Ottl",
        "S Amiriparian",
        "M Gerczuk",
        "V Karas",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "40",
      "title": "Selecting training data for cross-corpus speech emotion recognition: Prototypicality vs. generalization",
      "authors": [
        "B Schuller",
        "Z Zhang",
        "F Weninger",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "Proceedings of the Afeka-AVIOS Speech Processing Conference"
    },
    {
      "citation_id": "41",
      "title": "Using multiple databases for training in emotion recognition: To unite or to vote",
      "year": "2011",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "42",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Z Zhang",
        "F Weninger",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "Proceedings of the Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "43",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wollmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Efficient and effective strategies for cross-corpus acoustic emotion recognition",
      "authors": [
        "H Kaya",
        "A Karpov"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "45",
      "title": "Deep speech: Scaling up end-to-end speech recognition",
      "authors": [
        "A Hannun",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "G Diamos",
        "E Elsen",
        "R Prenger",
        "S Satheesh",
        "S Sengupta",
        "A Coates"
      ],
      "year": "2014",
      "venue": "Deep speech: Scaling up end-to-end speech recognition",
      "arxiv": "arXiv:1412.5567"
    },
    {
      "citation_id": "46",
      "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "authors": [
        "D Amodei",
        "S Ananthanarayanan",
        "R Anubhai",
        "J Bai",
        "E Battenberg",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "Q Cheng",
        "G Chen"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "47",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "49",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "50",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "51",
      "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "52",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "54",
      "title": "Deep representation learning techniques for audio signal processing",
      "authors": [
        "S Amiriparian"
      ],
      "year": "2019",
      "venue": "Deep representation learning techniques for audio signal processing"
    },
    {
      "citation_id": "55",
      "title": "Recognition of echolalic autistic child vocalisations utilising convolutional recurrent neural networks",
      "authors": [
        "S Amiriparian",
        "A Baird",
        "S Julka",
        "A Alcorn",
        "S Ottl",
        "S Petrović",
        "E Ainger",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "56",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "arxiv": "arXiv:1706.00612"
    },
    {
      "citation_id": "57",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Attentional pooling for action recognition",
      "authors": [
        "R Girdhar",
        "D Ramanan"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "60",
      "title": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks",
      "authors": [
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "61",
      "title": "The interspeech 2018 computational paralinguistics challenge: Atypical & self-assessed affect, crying & heart beats",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "P Marschik",
        "H Baumeister",
        "F Dong",
        "S Hantke",
        "F Pokorny",
        "E.-M Rathner",
        "K Bartl-Pokorny"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "62",
      "title": "Attention-based sequence classification for affect detection",
      "authors": [
        "C Gorrostieta",
        "R Brutti",
        "K Taylor",
        "A Shapiro",
        "J Moran",
        "A Azarbayejani",
        "J Kane"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "63",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "arxiv": "arXiv:2005.08453"
    },
    {
      "citation_id": "64",
      "title": "Meta-learning for speech emotion recognition considering ambiguity of emotion labels",
      "authors": [
        "T Fujioka",
        "T Homma",
        "K Nagamatsu"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "65",
      "title": "Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism",
      "authors": [
        "Q Chu",
        "W Ouyang",
        "H Li",
        "X Wang",
        "B Liu",
        "N Yu"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Computer Vision"
    },
    {
      "citation_id": "66",
      "title": "Deep learning in paralinguistic recognition tasks: Are hand-crafted features still relevant",
      "authors": [
        "J Wagner",
        "D Schiller",
        "A Seiderer",
        "E André"
      ],
      "year": "2018",
      "venue": "Deep learning in paralinguistic recognition tasks: Are hand-crafted features still relevant"
    },
    {
      "citation_id": "67",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "68",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "70",
      "title": "An image-based deep spectrum feature representation for the recognition of emotional speech",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "G Hagerer",
        "A Batliner",
        "S Steidl",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "71",
      "title": "Multimodal bag-of-words for cross domains sentiment analysis",
      "authors": [
        "N Cummins",
        "S Amiriparian",
        "S Ottl",
        "M Gerczuk",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Sentiment analysis using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "N Cummins",
        "S Ottl",
        "M Gerczuk",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "73",
      "title": "Emotion and themes recognition in music utilising convolutional and recurrent neural networks",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "E Coutinho",
        "A Baird",
        "S Ottl",
        "M Milling",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "MediaEval Benchmarking Initiative for Multimedia Evaluation"
    },
    {
      "citation_id": "74",
      "title": "are you playing a shooter again?!\" deep representation learning for audio-based video game genre recognition",
      "authors": [
        "S Amiriparian",
        "N Cummins",
        "M Gerczuk",
        "S Pugachevskiy",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "75",
      "title": "Towards cross-modal pre-training and learning tempo-spatial characteristics for audio recognition with convolutional and recurrent neural networks",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "L Stappen",
        "A Baird",
        "L Koebe",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "76",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "77",
      "title": "Covariate shift and local learning by distribution matching",
      "authors": [
        "J Qui Ñonero-Candela",
        "M Sugiyama",
        "A Schwaighofer",
        "N Lawrence"
      ],
      "year": "2008",
      "venue": "Covariate shift and local learning by distribution matching"
    },
    {
      "citation_id": "78",
      "title": "Dataset shift in machine learning, chapter 8. covariate shift and local learning by distribution matching",
      "authors": [
        "A Gretton",
        "A Smola",
        "J Huang",
        "M Schmittfull",
        "K Borgwardt",
        "B Schoelkopf"
      ],
      "year": "2009",
      "venue": "Dataset shift in machine learning, chapter 8. covariate shift and local learning by distribution matching"
    },
    {
      "citation_id": "79",
      "title": "Dataset shift in machine learning",
      "authors": [
        "M Sugiyama",
        "N Lawrence",
        "A Schwaighofer"
      ],
      "year": "2017",
      "venue": "Dataset shift in machine learning"
    },
    {
      "citation_id": "80",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2014",
      "venue": "Unsupervised domain adaptation by backpropagation",
      "arxiv": "arXiv:1409.7495"
    },
    {
      "citation_id": "81",
      "title": "Supervised domain adaptation for emotion recognition from speech",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "82",
      "title": "On acoustic emotion recognition: compensating for covariate shift",
      "authors": [
        "A Hassan",
        "R Damper",
        "M Niranjan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "83",
      "title": "Audiovisual behavior modeling by combined feature spaces",
      "authors": [
        "B Schuller",
        "D Arsic",
        "G Rigoll",
        "M Wimmer",
        "B Radig"
      ],
      "year": "2007",
      "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "84",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "85",
      "title": "The selected speech emotion database of institute of automation chinese academy of sciences (casia)",
      "year": "2010",
      "venue": "The selected speech emotion database of institute of automation chinese academy of sciences (casia)"
    },
    {
      "citation_id": "86",
      "title": "Recognizing vocal emotions in mandarin chinese: A validated database of chinese vocal emotional stimuli",
      "authors": [
        "P Liu",
        "M Pell"
      ],
      "year": "2012",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "87",
      "title": "Demos: an italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "88",
      "title": "Design, recording and verification of a danish emotional speech database",
      "authors": [
        "I Engberg",
        "A Hansen",
        "O Andersen",
        "P Dalsgaard"
      ],
      "year": "1997",
      "venue": "Proceedings of the European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "89",
      "title": "Automatische emotionserkennung aus sprachlicher und manueller interaktion",
      "authors": [
        "B Schuller"
      ],
      "year": "2006",
      "venue": "Technische Universität M ünchen"
    },
    {
      "citation_id": "90",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of the European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "91",
      "title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "K Sikka",
        "T Gedeon"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "92",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings of the International Conference on Data Engineering Workshops (ICDEW)"
    },
    {
      "citation_id": "93",
      "title": "The eu-emotion voice database",
      "authors": [
        "A Lassalle",
        "D Pigat",
        "H O'reilly",
        "S Berggen",
        "S Fridenson-Hayo",
        "S Tal",
        "S Elfstr Öm",
        "A Råde",
        "O Golan",
        "S Baron-Cohen",
        "D Lundqvist"
      ],
      "year": "2019",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-018-1048-1"
    },
    {
      "citation_id": "94",
      "title": "Categorical vs dimensional perception of italian emotional speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "95",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the fau aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E Öth"
      ],
      "year": "2008",
      "venue": "Proceedings of a Satellite Workshop of the International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "96",
      "title": "Introducing the geneva multimodal emotion portrayal (gemep) corpus",
      "authors": [
        "T Bänziger",
        "K Scherer"
      ],
      "year": "2010",
      "venue": "Blueprint for affective computing: A sourcebook"
    },
    {
      "citation_id": "97",
      "title": "Introducing the geneva multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "T Bänziger",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "98",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "99",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "100",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "101",
      "title": "Speech in minimal invasive surgery -towards an affective language resource of real-life medical operations",
      "authors": [
        "B Schuller",
        "F Eyben",
        "S Can",
        "H Feussner"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect"
    },
    {
      "citation_id": "102",
      "title": "The smartkom multimodal corpus at bas",
      "authors": [
        "F Schiel",
        "S Steininger",
        "U Ürk"
      ],
      "year": "2002",
      "venue": "Proceedings of the International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "103",
      "title": "Getting started with susas: A speech under simulated and actual stress database",
      "authors": [
        "J Hansen",
        "S Bou-Ghazale"
      ],
      "year": "1997",
      "venue": "Proceedings of the European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "104",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "105",
      "title": "Exploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Y Zhao",
        "Z Zhang",
        "N Cummins",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "106",
      "title": "Deep spectrum feature representations for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Zhao",
        "Z Bao",
        "H Wang",
        "Z Zhang",
        "C Li"
      ],
      "year": "2018",
      "venue": "Proceedings of the Joint Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data"
    },
    {
      "citation_id": "107",
      "title": "Wide residual networks",
      "authors": [
        "S Zagoruyko",
        "N Komodakis"
      ],
      "year": "2016",
      "venue": "Wide residual networks",
      "arxiv": "arXiv:1605.07146"
    },
    {
      "citation_id": "108",
      "title": "Note on the sampling error of the difference between correlated proportions or percentages",
      "authors": [
        "Q Mcnemar"
      ],
      "year": "1947",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "109",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset",
      "arxiv": "arXiv:1706.08612"
    },
    {
      "citation_id": "110",
      "title": "Large-scale multilingual speech recognition with a streaming end-to-end model",
      "authors": [
        "A Kannan",
        "A Datta",
        "T Sainath",
        "E Weinstein",
        "B Ramabhadran",
        "Y Wu",
        "A Bapna",
        "Z Chen",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Large-scale multilingual speech recognition with a streaming end-to-end model",
      "arxiv": "arXiv:1909.05330"
    }
  ]
}