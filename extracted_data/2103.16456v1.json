{
  "paper_id": "2103.16456v1",
  "title": "Enhancing Segment-Based Speech Emotion Recognition By Deep Self-Learning",
  "published": "2021-03-30T16:02:31Z",
  "authors": [
    "Shuiyang Mao",
    "P. C. Ching",
    "Tan Lee"
  ],
  "keywords": [
    "Segment-based speech emotion recognition",
    "learning with noisy labels",
    "deep self-learning",
    "soft labeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite the widespread utilization of deep neural networks (DNNs) for speech emotion recognition (SER), they are severely restricted due to the paucity of labeled data for training. Recently, segment-based approaches for SER have been evolving, which train backbone networks on shorter segments instead of whole utterances, and thus naturally augments training examples without additional resources. However, one core challenge remains for segment-based approaches: most emotional corpora do not provide ground-truth labels at the segment level. To supervisely train a segment-based emotion model on such datasets, the most common way assigns each segment the corresponding utterance's emotion label. However, this practice typically introduces noisy (incorrect) labels as emotional information is not uniformly distributed across the whole utterance. On the other hand, DNNs have been shown to easily over-fit a dataset when being trained with noisy labels. To this end, this work proposes a simple and effective deep self-learning (DSL) framework, which comprises a procedure to progressively correct segment-level labels in an iterative learning manner. The DSL method produces dynamically-generated and soft emotion labels, leading to significant performance improvements. Experiments on three well-known emotional corpora demonstrate noticeable gains using the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition (SER) aims at decoding emo- tional content from speech signals. It has constituted an active topic in the research area of human-machine interaction (HCI). In particular, monitoring call center services, detection of lies, and medical diagnoses are often considered promising application scenarios of SER.\n\nPrior research in SER has primarily focused on the utterance-based approach, where the backbone model is performed on the whole utterance. One major problem in the state-ofthe-art utterance-based paradigm is the scarcity of the training data compared to model complexity. Popular emotional speech databases, such as the Emo-DB corpus  [1]  and the SAVEE database  [2] , contain recordings of approximately one-hour speech only; the number of utterance-level feature vectors may not be sufficient for reliably estimating model parameters of a complex classifier such as deep neural networks (DNNs). An alternative is the segment-based approach  [3] [4] [5] [6] [7] [8] [9] [10] [11] , in which the backbone model can be trained more reliably on a large number of individual segments, which naturally augments training examples without extra resources.\n\nOne major shortcoming remains for the segment-based approach: Most emotional speech databases do not provide labels at the segment level. To supervisely train an emotion segment model, the most common way is to assign the utterance-level label to each segment  [4, [8] [9] [10] . Such labeling practice, however, may introduce noisy (incorrect) labels, since emotional information are not uniformly distributed over all positions of an utterance  [4, 7, 12] .\n\nAnother source of label noise in the segment-based approach comes from the dataset itself. For instance, most acted emotional corpora do not well enough simulate emotions naturally and clearly  [13] . In other words, the intended emotions are often not properly expressed. This is evidenced by the relatively poor recognition performance of human subjects; in  [14] , the reported human recognition rate is as low as about 65%. Also, the problem of elicited and natural emotional corpora lies in the label annotation; their emotion labels are often annotated based on human annotators' perception of emotions. However, in many cases, one may not clearly distinguish one emotion from another, which may introduce errors in the annotation process. Based on the above discussion, we can see that the labels provided by the dataset itself inevitably contain label noise due to subjective expression (for acted emotional corpora) or subjective perception (for elicited or natural emotional corpora) of emotions. Consequently, the segments that merely inherit their labels from corresponding utterances may also inherit the same inferiority of subjective labeling of emotional speech.\n\nOn the other hand, it has been widely reported that label noise significantly degrades the generalization performance of complex models such as DNNs since they easily over-fit the label noise  [15] [16] [17] . Therefore, limiting the negative influence of label noise is of great practical importance. In computer vision, many efforts have been made to improve the robustness of DNNs trained on noisy labels  [18] [19] [20] [21] [22] [23] . However, similar efforts have not been attempted in the segment-based SER systems to our best knowledge. Herein, it is our goal to train a robust emotion segment model on the noisy labeled segments. Specifically, we introduce deep self-learning (DSL), a solution that uses a DNN model and the data to correct segmentlevel labels during iterative training. This will be detailed in Section IV.\n\nAnother critical issue in the SER task is that human emotions are inherently impure. When designing systems to recognize human emotions, the emotional impurity must be considered. However, most of the current methods rest on the consensus, e. g., one single hard label for an utterance (hard labeling). This labeling principle imposes specific challenges on SER related tasks, e. g., incomplete labeling. For instance, frustration can overlap with other emotion categories ranging from anger to neutral and sadness  [5, 24] . SER systems designed to output one hard label for each speech utterance (or segment) may perform poorly if the target expression cannot be well captured by a single emotion label  [6] .\n\nA natural solution to the above problem is to perform soft labeling, which characterizes expressions as complex mixtures of possible emotions, rather than one-hot hard assignments. This work explores the soft labeling approach in the DSL framework. Specifically, three soft labeling-based strategies are proposed to construct training targets during iterative learning, i. e., basic dynamic soft labeling (BDSL), weighted dynamic soft labeling (WDSL), and global soft labeling (GSL). For comparison, we also investigate a hard labeling-based method called dynamic hard labeling (DHL). In our experiments, the soft labeling-based methods outperform the hard labeling-based one overall. In particular, the WDSL method achieves the best performance.\n\nThe contributions of this work are summarized as follows:  (1)  We propose an iterative learning framework DSL to improve the performance of segment-based SER task. The segments that inherit their labels from the corresponding utterance are treated as the \"noisy dataset\", and the training of segment-based emotion model on it is framed as the \"learning with noisy labels\" problem.  (2)  We empirically explore and compare various deep convolutional neural network (DCNN) architectures as the backbone for the SER task. (3) We demonstrate the capability of a network to improve accuracy by training against labels generated by another network of the same architecture. (4) Extensive experiments on three popular emotional corpora are conducted, and the experimental results consistently validate the effectiveness of our methods. In particular, when EfficientNet-B0  [25]  is used as the backbone model, our method improves recognition rate on the CASIA corpus  [26]  from 95.17% to 96.82% (WA and UA), the Emo-DB database  [1]  from 83.36% to 92.90% (WA) and 82.54% to 93.02% (UA), and the SAVEE database  [2]  from 73.75% to 86.46% (WA) and 72.26% to 85.71% (UA), achieving new state-of-the-art performance on all three databases.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Data Augmentation in SER: The scarcity of training data for emotional speech seriously limits the generalization per-formance of current SER systems. Data augmentation appears to be the most common way to address this problem. For instance, Etienne et al. augmented emotional speech data through a combination of oversampling and vocal tract length perturbation (VTLP)  [27] .  [28] [29] [30]  employed generative adversarial networks (GANs) to generate training samples. Motivated by related works in computer vision, transfer learning has also been applied to SER tasks  [31, 32] . This work adopts a segment-based framework in which the backbone model is trained reliably on a large number of individual segments. This naturally augments the training vectors without extra resources or an accessorial network.\n\nLearning with Noisy Labels: When training data are corrupted by label noise, an obvious solution is to filter out the erroneous labels themselves. Indeed, many methods have been proposed to find and remove mislabeled instances, with different degrees of success  [18] . However, a robust mechanism to identify and remove erroneous labels from the label set remains lacking. The noise transition matrix has also been widely utilized to characterize the transition probability between the observed (noisy) label and the latent true label  [19] [20] [21] . However, the above methods follow a simple assumption that the transition probability is independent of individual instances, which often does not hold in real-world noisy datasets and thus limits their performance. Furthermore, various noise-robust loss functions  [22, 23]  have been explored to combat label noise. However, most of these functions either have an assumption similar to the above transition matrixbased approaches or require extra clean samples, limiting their applications in practice. In contrast, the DSL framework adopted in this work does not rely on any assumption about the distribution of label noise, making it feasible for large-scale real-world scenarios. It also does not require additional clean samples or extra supervision, providing a simple and effective \"learning with noisy labels\" scheme.\n\nSoft Labeling in SER: Soft labeling approaches have recently received increased focus for modeling emotional ambiguity and impurity. For example, Lotfian et al. proposed a novel probabilistic approach for soft labeling of emotional speech  [33] . Ando et al. developed a DNN-based model trained with soft emotion targets as ground truth to better capture emotional ambiguity  [34] . Kim et al. utilized soft labeling and cross-entropy to compare emotion label distributions made by humans and machines  [35] . All of the above works performed soft labeling on the whole utterance. In contrast, this work aims at the individual segments, of which the emotional expressions might be more ambiguous than that of the longer utterances.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Segment-Based Ser",
      "text": "The segment-based approach for SER has existed in the research community for some years  [4] [5] [6] [7] [8] [9] [10] . Empirical comparisons between segment-based processing and traditional utterance-based approach in prior studies have demonstrated the superiority of the former  [4, [7] [8] [9] [10] . The key idea of the segment-based approach for SER is first to obtain classification results of the emotional state at segment level. Then, these segment-level decisions are aggregated to form utterance-level representations for utterance-level classification.\n\nHowever, since detailed annotation of the speech utterance frequently constitutes an ambiguous and expensive task, most emotional speech databases do not provide segment-level labels; instead, we only have labels for the whole utterance. Consequently, the segment-based method must address the problem of how to infer an emotion segment model without access to a training set of labeled segments. To circumvent this problem, a common practice is to follow the most straightforward approach, i. e., each segment inherits the label of corresponding utterance  [8] [9] [10] . A segment-based emotion model is then trained on the resulting dataset. The trained model aims to output a probability prediction q over all emotional states for each segment:\n\nwhere x i represents a specific segment; and K denotes the number of emotion categories. Figure  1  presents an example of segment-level emotion predictions across the audio file \"Happy liuchanhg 382.wav\" in the CASIA corpus, where a VGG network  [36]  was used as the backbone model. It can be observed that:  (1)  The probability distribution of each segment changes over the whole utterance. (  2 ) Although most segments convey information that conforms to the corresponding utterance, some segments are more related to other classes, introducing label noise and impeding model generalization performance. If we can correct those noisy (incorrect) labels -a topic that has not yet been touched in the segment-based SER to our best knowledge, system performance might be improved.\n\nInspired by recent successes of self-learning in dealing with noisy labeled images in computer vision  [17, 37, 38] , here, we propose an iterative learning scheme with DCNN as the backbone model, which we call deep self-learning (DSL) throughout this paper. We treat those individual segments that inherit their labels from corresponding utterances as the noisy data, and their labels as noisy labels. We then formulate the training of the emotion segment model as a \"learning with noisy labels\" problem, in which the model parameters and labels are jointly optimized. The following section specifies our approach.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Our Approach",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "In a supervised K-class classification problem setting, we have a dataset of N samples: D = {X, Y} = {(x 1 , y 1 ), . . . , (x N , y N )}, and y i ∈ {0, 1} K is the onehot vector representation of the class label corresponding to the input feature x i . Generally, the optimization problem is formulated as follows:\n\nwhere θ denotes the model parameters to be learned; and L represents a loss objective function, such as the cross entropy. Eq. (  2 ) works well on clean labels. However, when Y contains noise (erroneous labels), the solution of Eq. (  2 ) would be suboptimal, limiting the generalization performance of the derived models.\n\nIn this work, we first assign each segment the emotion label of the corresponding utterance. This labeling procedure results in a dataset with noisy labels. To attain the latent true label Ŷ from the initial set of segments with noisy labels and thus train a robust emotion segment model, we propose deep selflearning (DSL), an effective framework to jointly optimize the model parameters and labels in an iterative learning manner. Our optimization problem can be formulated as follows:\n\nwhere the network parameters and the class labels are updated alternatively. The details are described in the following subsections. 2) Iterative Self-Learning: The first backbone DCNN C θ0 is trained against the original noisy label. It takes individual segments, e. g., x i as input and produces corresponding label predictions q(θ 0 , x i ). The second backbone DCNN C θ1 is trained over the same input, e. g., x i , but uses updated labels g(q(θ 0 , x i )), where g(.) represents the update rule of labels to construct training targets for training the next backbone DCNN. Details of the update rule of labels will be described in the next subsection. Once C θ1 is trained, we can similarly use the updated labels g(q(θ 1 , x i )) to train a subsequent network C θ2 , etc. Intuitively, as the backbone networks improve over iterations of re-training, the labels are progressively corrected.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Dsl",
      "text": "We train the first network C θ0 using conventional crossentropy loss. We train each of the subsequent network C θt for t ≥ 1 by minimizing the KL-divergence between its output q(θ t , x i ) and the updated labels g(q(θ t-1 , x i )) from the backbone DCNN C θt-1 for t ≥ 1 in the previous iteration. Our loss function is formulated by:\n\nwhere n is the mini-batch size. The second term in Eq. (  4 ) is constant with respect to C θt . We can thus remove it and minimize the cross-entropy loss instead:\n\n3) Updating Labels: In this work, four update rules of labels are investigated, of which three are soft labeling-based, namely, basic dynamic soft labeling (BDSL), weighted dynamic soft labeling (WDSL), and global soft labeling (GSL). The remaining one is hard labeling-based, i. e., dynamic hard labeling (DHL). The details are outlined as follows.\n\n• Basic Dynamic Soft Labeling (BDSL). For the BDSL method, we directly use the model predictions, e. g., q(θ t-1 , x i ), to construct the supervision signal for training the next model C θt as follows:\n\nThe resulting loss function is:\n\n• Weighted Dynamic Soft Labeling (WDSL). The WDSL method uses a convex combination of original noisy labels, e. g., y i , and the model predictions to update training labels as follows:\n\nwhere α ∈ [0, 1] is a coefficient balancing the two terms.\n\nThe corresponding loss function for the WDSL method is:\n\n• Global Soft Labeling (GSL). To generate a global soft label for a certain speech segment x i , we first pass all segments within the corresponding utterance U to the current trained backbone network, and the global soft label is then computed by averaging the network outputs across the whole utterance as follows:\n\nwhere k denotes the k th emotion category.\n\nThe corresponding loss function is:\n\n• Dynamic Hard Labeling (DHL). For the DHL method, the one-hot hard label is assigned to each speech segment by choosing the most-likely category from the corresponding network output as follows:\n\nwhere k ( ) denotes the k ( )th emotion category. The corresponding loss function can be expressed as:\n\nC. Utterance-level Emotion Classification Algorithm 1 illustrates the overall process of training a robust backbone DCNN on noisy labeled speech segments using the proposed DSL framework. Segment-level predictions produced by the latest backbone network are used for constructing feature vectors for utterance-level classification. Specifically, the utterance-level representations are computed from the statistics of the segment-level predictions:\n\nwhere p k (x i ) is the probability of the k th emotion for a specific segment x i ; and U denotes the set of all segments from a certain utterance. h k 1 , h k 2 , h k 3 , and f k 4-6 are the arithmetic mean, percentile 1, percentile 99, quartiles 1-3 of segmentlevel probabilities of the k th emotion across an utterance, respectively. In particular, percentile 1 and percentile 99 serve as a robust substitute for the minimum value and maximum value, respectively. The remaining two features, h k 7-8 , correspond to the percentage of segments which have higher probabilities than a given threshold for the k th emotion. h k 7-8 are not sensitive to the threshold β, and we herein heuristically set β equal to 0.2 for h k 7 and 0.3 for h k 8 . This step results in a feature vector with a dimension of 8 × K for each utterance. With this collection of utterance-level feature vectors, we can train a second relatively simple classifier, i. e., random forest (RF)  [39] , to perform the utterance-level classification.\n\nV. EMOTIONAL CORPORA Three different emotion corpora are used to evaluate the proposed method, namely, a Chinese emotion corpus (CASIA)  [26] , a German emotion corpus (Emo-DB)  [1]  and an English emotional database (SAVEE)  [2] . They are summarized in Table  I . For each dataset, all of the emotion categories are selected for experiments.\n\nSpecifically, the CASIA corpus contains 9, 600 speech utterances that were produced by four subjects (two males and two females) to stimulate six different emotions, i. e., anger, fear, happiness, neutral, sadness, and surprise. Only the 7, 200 utterances corresponding to 300 linguistically neutral sentences with the same statements are involved in our experiments. The sample rate is 16 kHz.\n\nThe Berlin Emo-DB German Corpus (Emo-DB) contains 535 emotional speech utterances covering seven different emotions deliberately displayed by ten German actors (five males and five females). The number of spoken utterances for these seven emotions is not equally distributed: 126 anger, 81 boredom, 46 disgust, 69 fear, 71 happiness, 79 neutral, and 62 sadness. Audio files were recorded at 16 kHz.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Algorithm 1 Iterative Self-Learning Pseudocode",
      "text": "Input: speech segments with labels inherited from corresponding utterances (dataset with noisy labels) for t = 1 : num iteration do update backbone DCNN parameter θ t by SGD update labels by Eq. (  6 ) (BDSL) or Eq. (  8 ) (WDSL) or Eq. (  10 ) (GSL) or Eq. (  12 ) (DHL) end for Output: a robust backbone DCNN to make more corrected segment-level predictions, which will be used for constructing utterance-level representations\n\nThe Surrey audio-visual expressed emotion database (SAVEE) is a collection of read speech produced by four male British-English speakers (researchers and postgraduate students from the University of Surrey, age from 27 to 31 years). Seven different emotions are elicited: anger, disgust, fear, happiness, neutral, sadness, and surprise. The neutral emotion was uttered in 30 phonetically-balanced sentences, and each of the remaining emotions was expressed in 15 sentences. Sound files were sampled at 44.1 kHz.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Acoustic Features",
      "text": "In regards to the acoustic features, we follow the recent success of applying DCNN directly to (Mel) spectrograms  [9, 10, 40, 41] . Empirical comparisons between automatic features learned from spectrograms and standard humanengineered features in our prior work  [10]  have demonstrated the superiority of the former.\n\nSpecifically, our backbone DCNN works on the 64-bin log Mel filterbanks of individual segments. To calculate this spectrogram features for the segments, we first resample the speech signals in the SAVEE database to 16kHz using the Librosa framework  [42] , such that all audio files of the three emotional corpora have the same sampling rate. Then, a sequence of overlapping Hanning windows is applied to the speech signals, resulting in frames with window shift of 10 ms, and window size of 25 ms. Subsequently, for each frame, the STFT is computed with an FFT length of 512 points. Finally, we compute the logarithmic power of 64 Melfrequency filterbanks over a frequency range from 0.125 kHz\n\nThe resulting frame-level log Mel filterbanks are then concatenated to form a 2-dim time-frequency representation of the segment. In this work, the segment size is set to 32 frames, i. e., the total length of a segment is 10 ms × 32 + (25 -10) ms = 335 ms. For the CASIA corpus, the segment hop length is set to 30 ms, while it is set to 10 ms for the remaining two database. In this way, we collected 418, 722 segments for the CASIA corpus, 131, 053 segments for the Emo-DB database, and 51, 027 segments for the SAVEE database, for training the backbone network, respectively. We also attempted other segment sizes ranging from 215 ms to 415 ms with the same hop length and achieved similar performance for utterancelevel classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Backbone Networks",
      "text": "Deep convolutional neural networks (DCNN) have proven very effective in image classification and show promise for speech signals. In this work, we have investigated various DCNN architectures as the backbone network to generate segment-level emotion predictions. Specifically, we examined VGG19  [36] , DenseNet22  [43] , MobilenetV2  [44] , and EfficientNet-B0  [25] . For each of the DCNNs, the architecture of the convolutional layers is based on the configurations in the original paper. A slight adjustment is made to the neuron number in the last softmax layer in order to make it suitable for our tasks. Note that we did not put much effort into optimization of the network architectures since our major concern here is the DSL framework.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Viii. Experiments",
      "text": "We evaluate the proposed method on the three mentioned emotion corpora. We first explore the effect of BDSL, which updates the training targets without combining the initial noisy label. We then evaluate the WDSL method, which remains the initial noisy label as a part of supervision. Subsequently, we evaluate the performance of the GSL and HDL methods, where we also gain some insight into the source of the improvements obtained using the BDSL and WDSL methods. Finally, we present some ablation studies and analyses to investigate the effect of the weight coefficient α in the WDSL method. The WDSL method with α equal to 0.2 achieves the best results across the three emotion corpora.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Setup",
      "text": "For the DCNN training, the ADAM  [45]  optimizer with default setting in Tensorflow  [46]  is used. The initial learning rate is set to 0.001, and an exponential decay scheme with a rate of 0.8 every two epochs is applied. The batch size is set to 128. Early stopping strategy with patience of 3 epochs is applied to mitigate an overfitting problem. Maximum number of training epochs is set to 20 for the CASIA corpus, 12 for the Emo-DB corpus, and eight for the SAVEE database.\n\nThe segment-level predictions are generated using 10-fold stratified cross-validation to ensure that the segment-level predictions are out-of-sample. Furthermore, the fold split is done at the utterance level and not at the segment level. The utterance-level classification is performed using a random forest (RF) with the default setting in the open-source Scikitlearn toolkit  [47] , where another 10-fold stratified crossvalidation is performed. The results are presented in terms of weighted accuracy (WA) and unweighted accuracy (UA). Note that the WA and UA are the same for the CASIA corpus, as the CASIA corpus is perfectly balanced regarding the emotion class distribution.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Results Of Dsl-Bdsl",
      "text": "We first investigate the effect of applying DSL with basic dynamic soft labeling (DSL-BDSL), in which we use the soft labels (model predictions) solely as the supervision signal for training next model, without combining the initial noisy label. Table  II  shows the experimental results for DSL-BDSL on various DCNN architectures. In each sub-table, the superscript of each model stands for the iteration number of self-learning; each row represents a randomly-initialized model trained with soft labels generated by the model one row above it. For instance, VGG19 2 is trained with soft labels generated by VGG19 1 , and VGG19 1 is trained with class probabilities predicted by VGG19 0 . The first model VGG19 0 is trained with the original predefined noisy labels.\n\nAs can be observed from Table  II :  (1)  The DSL-BDSL method consistently improved the SER accuracy on all three emotional corpora across all four DCNN architectures, demonstrating the universality of the proposed method. (2) The best performance was achieved after one single round of re-training process. Subsequently, the performance diminished significantly.  (3)  The best performance varied with the backbone DCNN architectures. In particular, the recognition rate on the Emo-DB corpus of VGG19 (83.74% for WA and 83.96% for UA) and DenseNet22 (81.68% for WA and 81.41% for UA) were noticeably lower than that of MobileNetV2 (89.53% for WA and 89.21% for UA) and EfficientNet-B0 (91.78% for WA; 91.37% for UA). The latter two architectures use the mobile inverted bottleneck  [25, 44, 48]  as their main building block, in which the lightweight design of depthwise separable convolutions  [25, 44, 48]  might offer an advantage in further mitigating the data scarcity problem. Moreover, the overal superiority of EfficientNet-B0 over MobileNetV2 can be attributed to that the former was developed with a greedy neural architecture search (NAS) algorithm  [25, 49]  and thus better performance can be expected. Consequently, EfficientNet-B0 will be used in the remaining experiments. Figure  3  shows an example of segment-level label predictions across the audio file \"Happy liuchanhg 382.wav\" in the CASIA corpus, where the DSL-BDSL method was applied, and EfficientNet-B0 was used as the backbone network. It is obvious that the DSL-BDSL method encourages the predicted class probabilities to be near a uniform distribution, i. e., after the second iteration of re-training, each output dimension of the backbone network was already fairly close to 17%, i. e., the chances of a random hit for the CASIA corpus of six categories. We argue that this is because, for the DSL-BDSL method, the model minimizes Eq. (  7 ) solely. As the self-learning process continues, we gradually lost useful information in the original noisy one-hot label, which encourages sharp 1-of-K (one-hot) code predictions. Finally, we arrived at a trivial global optimal solution for Eq. (  7 ), where a backbone model always makes predictions to be near a uniform distribution.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Results Of Dsl-Hdl And Dsl-Gsl",
      "text": "We posit that the benefits of using DSL-BDSL are from two aspects: (1) Each segment is dynamically re-labeled with a more appropriate label; and (2) soft labeling is introduced. To observe the improvement from dynamic labeling alone, we perform label refinement with hard dynamic labeling (HDL) in the DSL framework, which is called DSL-HDL. Specifically, we pass each segment to the trained backbone model, and the one-hot label is assigned by choosing the most-likely category from the model output. Figure  4  shows the segment-level label predictions across the audio file \"Happy liuchanhg 382.wav\" in the CASIA corpus, with the DSL-HDL method applied and EfficientNet-B0 as the backbone. It can be observed that, as the self-training proceeds, the emotion prediction contour across the utterance oscillated in an increasingly strong manner.\n\nTo assess the improvement from soft labeling solely, we investigate global soft labeling (GSL) in the DSL framework, which is called DSL-GSL. To attain the soft global label for a given segment, we pass all segments within the same utterance to the trained backbone model, and the soft global label is calculated by averaging the model outputs over the whole utterance. Figure  5  presents the segment-level label predictions across the audio file \"Happy liuchanhg 382.wav\" in the CASIA corpus, with the DSL-GSL method applied and EfficientNet-B0 as the backbone. Similar to the DSL-BDSL method, after two iterations of self-learning, the model in the case of using the DSL-GSL method also arrives at a state where it always makes uniformly distributed predictions.\n\nTable  III  summarizes the results. Both DSL-HDL and DSL-GSL method consistently boost the recognition accuracy. When they are combined (i. e., the DSL-BDSL method), an additional improvement can be observed, indicating that they address different issues in labels.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Results Of Dsl-Wdsl",
      "text": "In the previous DSL-BDSL method, as self-learning proceeded to the second iteration, we got stuck in a trivial solution for Eq.  (7) , where the segment-level model always made predictions to be near a uniform distribution. In this section, we tackle this issue by keeping the original noisy label as a part of the supervision. Eq. (  9 ) shows the resulting loss function, where the second term (introduced by the original noisy label) is equivalent to the conventional softmax regression, which avoids the trivial global minima of Eq. (  7 ) by encouraging 1-of-K one-hot code predictions, as well as adding a strong bias towards the corresponding utterance-level emotion class (as it should be). Note that there is a weight coefficient α that balances the two terms in Eq. (  9 ). In the very beginning, we heuristically set α equal to 0.5. An ablation study on the influence of different α will be presented in the following section. Figure  6  shows the segment-level label predictions across the audio file \"Happy liuchanhg 382.wav\" in the CASIA corpus, with the DSL-WDSL method applied and EfficientNet-B0 as the backbone. It can be observed that the uniformly distributed predictions that were encountered in the case of using DSL-BDSL (see Figure  3 ) disappeared, indicating the promising potential of combining both the soft labels and original noisy labels in the DSL scheme.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Ablation Study On The Weight Factor Α",
      "text": "The weight factor α in Eq. (  9 ) is crucial for the DSL-WDSL method. If α is too large, the model concentrates on the initial noisy labels, and it disturbs the training. On the other hand, if α is too small, the training procedure discards the useful information in the initial noisy labels, leading to the trivial global optima as encountered in the previous DSL-BDSL method. In this section, we study the influence of different α. Specifically, α is constant ranging from 0.0 to 0.9.  In addition, we also investigated the deterministic annealing process as used in  [50] , by which α is slowly decreased (i. e., 0.9 → 0.7 • • • → 0.1), but the improvement in our experiment was somewhat limited. Table  IV  presents the results, where EfficientNet-B0 was applied as the backbone. We find that when the DSL method was not applied, which corresponds to α = 1 in the last row, the performance was relatively limited. On the other hand, the model trained using the generated label solely (i. e., the DSL-BDSL method that corresponds to α = 0 in Table  IV ) also performed sub-optimally. When α = 0.2, the model jointly trained using the original noisy label and the generated soft label achieved the best performance across all three databases.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "F. Discussion",
      "text": "Table V summarizes the results on the three mentioned emotional corpora, respectively, where EfficientNet-B0 was used as the backbone. The following can be seen: (1) Our baseline system achieved recognition rate of 95.17% for the CASIA corpus, which already surpasses the state-of-the-art performance in the literature, demonstrating the effectiveness of the segment-based approach. On the other hand, our baseline system did not show any superiority on the two smaller datasets, i. e., the Emo-DB corpus and the SAVEE database. The main reason for this is that, in the segmentation process, the segment hop length was only set to 10 ms for the Emo-DB corpus and the SAVEE database, whereas it was set to 30 ms for the CASIA corpus. Indeed, a smaller hop length (equivalent to larger overlapping) yields more segments for a given utterance, but it also results in more severe homogeneity among segments for training. This trade-off is more difficult for smaller datasets.  DSL-HDL consistently outperformed the static-based DSL-GSL method, indicating that labels should be defined at the segment-level rather than being identical across the whole utterance, which partly justified our motivation for this work.\n\n(4) The DSL-WDSL method substantially augmented the performance of the DSL-BDSL method, verifying that original noisy labels are useful in the training procedure. To our best knowledge, the DSL-WDSL method established new stateof-the-art performance on all three emotional corpora. (5) Figures  7 8 9 show the utterance-level recognition accuracy for different classes across the three emotional corpora, respectively. The proposed DSL framework improved classification accuracy for most emotion classes.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Ix. Conclusions",
      "text": "In this paper, we address shortcomings commonly found in segment-based speech emotion recognition (SER). We treat the original segment-level labels that merely inherited from corresponding utterances as noisy (incorrect) labels and formulate the corresponding emotion segment modeling as the \"learning with noisy labels\" problem. We propose a deep self-learning (DSL) framework to progressively update the labels for network re-training to improve the emotion segment model's robustness. Also, considering the impurity nature of emotional speech, the soft labeling approach is investigated within the DSL framework, which characterizes the underlying mixture of emotions by representing each segment label with an emotion class distribution. We achieve a noticeable gain in emotion recognition performance across a broad range of network architectures, including VGG19, DenseNet22, MobileNetV2, and EfficientNet-B0. Our experimental evaluation shows substantial improvement in the state-of-the-art accuracy on three well-known emotional corpora, respectively. In the future, we will explore combining self-learning with contrastive learning further to boost the generalization performance of the emotion segment model. Optimization of the network architectures will also be addressed.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of segment-level label predictions across the audio ﬁle",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of our system. The red block shows the proposed DSL framework, in which we train the backbone DCNN model and correct labels for each",
      "page": 3
    },
    {
      "caption": "Figure 1: presents an example of segment-level emotion",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates a schematic diagram of the",
      "page": 4
    },
    {
      "caption": "Figure 3: shows an example of segment-level label predic-",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the segment-level label",
      "page": 7
    },
    {
      "caption": "Figure 5: presents the segment-level label",
      "page": 7
    },
    {
      "caption": "Figure 6: shows the segment-level label",
      "page": 7
    },
    {
      "caption": "Figure 3: ) disappeared,",
      "page": 7
    },
    {
      "caption": "Figure 3: An example of segment-level label predictions across the audio ﬁle",
      "page": 8
    },
    {
      "caption": "Figure 4: An example of segment-level label predictions across the audio ﬁle",
      "page": 8
    },
    {
      "caption": "Figure 5: An example of segment-level label predictions across the audio ﬁle",
      "page": 8
    },
    {
      "caption": "Figure 6: An example of segment-level label predictions across the audio ﬁle",
      "page": 8
    },
    {
      "caption": "Figure 7: Utterance-level accuracy [%] for each class in the CASIA corpus obtained using: (1) original noisy segment labels, without DSL method applied",
      "page": 9
    },
    {
      "caption": "Figure 8: Utterance-level accuracy [%] for each class in the Emo-DB corpus obtained using: (1) original noisy segment labels, without DSL method applied",
      "page": 9
    },
    {
      "caption": "Figure 9: Utterance-level accuracy [%] for each class in the SAVEE database obtained using: (1) original noisy segment labels, without DSL method applied",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear": "happin\nneutral\nsadnes\nsurpris",
          "Column_2": "ess\ns\ne"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "WA": "70.63",
          "UA": "82.36",
          "Column_7": ""
        },
        {
          "Column_1": "VGG191",
          "WA": "71.88",
          "UA": "83.96",
          "Column_7": "70.00"
        },
        {
          "Column_1": "",
          "WA": "26.04",
          "UA": "67.92",
          "Column_7": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "WA": "63.75",
          "UA": "79.99",
          "Column_7": ""
        },
        {
          "Column_1": "DenseNet221",
          "WA": "64.79",
          "UA": "81.41",
          "Column_7": "61.31"
        },
        {
          "Column_1": "",
          "WA": "43.13",
          "UA": "67.18",
          "Column_7": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "CASIA": "WA",
          "Column_3": "UA",
          "Emo-DB": "WA",
          "Column_5": "UA",
          "Column_6": "WA",
          "Column_7": ""
        },
        {
          "Column_1": "",
          "CASIA": "95.17",
          "Column_3": "95.17",
          "Emo-DB": "83.36",
          "Column_5": "82.54",
          "Column_6": "73.75",
          "Column_7": ""
        },
        {
          "Column_1": "EfficientNet-B01",
          "CASIA": "95.61",
          "Column_3": "95.61",
          "Emo-DB": "91.78",
          "Column_5": "91.37",
          "Column_6": "84.79",
          "Column_7": "83.93"
        },
        {
          "Column_1": "",
          "CASIA": "92.85",
          "Column_3": "92.85",
          "Emo-DB": "85.42",
          "Column_5": "85.15",
          "Column_6": "80.63",
          "Column_7": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "WA": "72.92",
          "UA": "81.05",
          "Column_7": ""
        },
        {
          "Column_1": "MobileNetV21",
          "WA": "73.29",
          "UA": "89.21",
          "Column_7": "71.88"
        },
        {
          "Column_1": "",
          "WA": "26.67",
          "UA": "65.51",
          "Column_7": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "WA": "73.75\n76.25\n81.25",
          "UA": "82.54\n84.15\n90.85",
          "Column_7": ""
        },
        {
          "Column_1": "DSL-BDSL",
          "WA": "84.79",
          "UA": "91.37",
          "Column_7": "83.93"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear\nhappiness": "neutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear\nhappiness": "neutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear": "happiness\nneutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear": "happiness\nneutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear": "happiness\nneutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "anger\nfear": "happiness\nneutral\nsadness\nsurprise"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "WDSL( =0.2)\nGSL\nHDL",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "an",
          "Column_7": "g",
          "Column_8": "er",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "f",
          "Column_16": "ea",
          "Column_17": "",
          "Column_18": "r",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "ha",
          "Column_24": "",
          "Column_25": "p",
          "Column_26": "pin",
          "Column_27": "",
          "Column_28": "e",
          "Column_29": "",
          "Column_30": "ss\nE",
          "Column_31": "",
          "Column_32": "m",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "ot",
          "Column_36": "ne\nio",
          "Column_37": "ut\nn",
          "Column_38": "",
          "Column_39": "ral\ncl",
          "Column_40": "",
          "Column_41": "a",
          "Column_42": "ss",
          "Column_43": "",
          "Column_44": "s",
          "Column_45": "a",
          "Column_46": "dn",
          "Column_47": "",
          "Column_48": "ess",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "sur",
          "Column_55": "",
          "Column_56": "pr",
          "Baseline\nBDSL": "",
          "Column_58": "ise",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "ac\nrre\norr",
          "Column_5": "",
          "Column_6": "cu\ncte\nec",
          "Column_7": "ra\nd\nted",
          "Column_8": "cy\nby\nb",
          "Column_9": "[%\nth\nyt",
          "Column_10": "] for\neDS\nheD",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "e\nL-\nSL",
          "Column_14": "",
          "Column_15": "ach\nBD\n-G",
          "Column_16": "c\nSL\nSL",
          "Column_17": "",
          "Column_18": "las\nm\nm",
          "Column_19": "s in\neth\neth",
          "Column_20": "",
          "Column_21": "th\nod(\nod",
          "Column_22": "",
          "Column_23": "e C\nBD\n(GS",
          "Column_24": "",
          "Column_25": "AS\nS\nL)",
          "Column_26": "IA\nL);\n;(",
          "Column_27": "",
          "Column_28": "c\n(3\n5)",
          "Column_29": "",
          "Column_30": "orp\n)la\nlab",
          "Column_31": "",
          "Column_32": "us o\nbels\nelsc",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "bta\nco\nor",
          "Column_36": "in\nrre\nrec",
          "Column_37": "ed\ncte\nted",
          "Column_38": "",
          "Column_39": "usi\ndb\nby",
          "Column_40": "",
          "Column_41": "ng\ny\nt",
          "Column_42": ": (1)\ntheD\nheD",
          "Column_43": "",
          "Column_44": "o\nS\nSL",
          "Column_45": "rig\nL-\n-H",
          "Column_46": "ina\nWD\nD",
          "Column_47": "",
          "Column_48": "l n\nSL\nLm",
          "Column_49": "",
          "Column_50": "ois\nm\net",
          "Column_51": "y se\netho\nhod",
          "Column_52": "",
          "Column_53": "gm\nd,\n(H",
          "Column_54": "en\nwh\nDL",
          "Column_55": "",
          "Column_56": "t l\ner\n),",
          "Baseline\nBDSL": "",
          "Column_58": "abe\neth\nres",
          "Column_59": "",
          "Column_60": "ls,\ne\npe",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "100\n)%(\n95 ycarucca\n90\n85 noitacifissalC\n80\n75\n70",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "Baseline\nBDSL\nWDSL( =0.2)\nGSL\nHDL",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "Baseline\nBDSL",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "",
          "Column_62": "Baseline\nBDSL",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "",
          "Column_62": "WDSL( =0.2)\nGSL",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "HDL",
          "Column_62": "HDL",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "a",
          "Column_6": "ng",
          "Column_7": "er",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "b",
          "Column_12": "o",
          "Column_13": "r",
          "Column_14": "e",
          "Column_15": "do",
          "Column_16": "m",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "dis",
          "Column_22": "g",
          "Column_23": "",
          "Column_24": "us",
          "Column_25": "t",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "f\nE",
          "Column_32": "ea\nm",
          "Column_33": "",
          "Column_34": "r",
          "Column_35": "ot",
          "Column_36": "io",
          "Column_37": "",
          "Column_38": "h\nn",
          "Column_39": "a",
          "Column_40": "p\ncl",
          "Column_41": "pi\na",
          "Column_42": "ne\nss",
          "Column_43": "ss",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "n",
          "Column_48": "e",
          "Column_49": "u",
          "Column_50": "tra",
          "Column_51": "l",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "s",
          "Column_55": "a",
          "Column_56": "d",
          "Baseline\nBDSL": "",
          "Column_58": "n",
          "Column_59": "es",
          "Column_60": "s",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "l\no\nc",
          "Column_4": "r\no",
          "Column_5": "ac\nre\nrr",
          "Column_6": "cu\ncte\nec",
          "Column_7": "rac\nd\nted",
          "Column_8": "y\nby\nb",
          "Column_9": "[%\nth\nyt",
          "Column_10": "]\ne\nh",
          "Column_11": "fo\nD\ne",
          "Column_12": "r\nS\nD",
          "Column_13": "L\nS",
          "Column_14": "ea\n-\nL",
          "Column_15": "ch\nBD\n-G",
          "Column_16": "cl\nSL\nSL",
          "Column_17": "",
          "Column_18": "ass\nm\nm",
          "Column_19": "e\ne",
          "Column_20": "in\nth\nth",
          "Column_21": "th\nod\nod",
          "Column_22": "e\n(",
          "Column_23": "B\n(",
          "Column_24": "E\nD\nGS",
          "Column_25": "mo\nS\nL)",
          "Column_26": "-D\nL);\n;(",
          "Column_27": "",
          "Column_28": "B\n(\n5",
          "Column_29": "3\n)",
          "Column_30": "c\n)\nl",
          "Column_31": "or\nla\nab",
          "Column_32": "pus\nbe\nels",
          "Column_33": "",
          "Column_34": "o\nls\nc",
          "Column_35": "bt\nco\nor",
          "Column_36": "ain\nrre\nrec",
          "Column_37": "e\nc\nte",
          "Column_38": "d\nte\nd",
          "Column_39": "d",
          "Column_40": "us\nb\nby",
          "Column_41": "in\ny\nt",
          "Column_42": "g:\nthe\nhe",
          "Column_43": "(1\nD\nD",
          "Column_44": ") o\nS\nSL",
          "Column_45": "ri\nL-\n-H",
          "Column_46": "gi\nW\nD",
          "Column_47": "n\nD",
          "Column_48": "al\nL",
          "Column_49": "n\nSL\nm",
          "Column_50": "oi\nm\net",
          "Column_51": "sy\net\nho",
          "Column_52": "se\nho\nd",
          "Column_53": "g\nd,\n(H",
          "Column_54": "me\nw\nD",
          "Column_55": "n\nh\nL",
          "Column_56": "t\ne\n)",
          "Baseline\nBDSL": "l\nr\n,",
          "Column_58": "a\ne\nr",
          "Column_59": "be\nth\nes",
          "Column_60": "ls,\ne\npe",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "100\n)%(\nycarucca\n90\n80\nnoitacifissalC\n70\n60\n50",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "Baseline\nBDSL\nWDSL( =0.2)\nGSL\nHDL",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "Baseline\nBDSL",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "",
          "Column_30": "",
          "Column_31": "",
          "Column_32": "",
          "Column_33": "",
          "Column_34": "",
          "Column_35": "",
          "Column_36": "",
          "Column_37": "",
          "Column_38": "",
          "Column_39": "",
          "Column_40": "",
          "Column_41": "",
          "Column_42": "",
          "Column_43": "",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "",
          "Column_47": "",
          "Column_48": "",
          "Column_49": "",
          "Column_50": "",
          "Column_51": "",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "",
          "Column_55": "",
          "Column_56": "",
          "Baseline\nBDSL": "",
          "Column_58": "",
          "Column_59": "",
          "Column_60": "",
          "Column_61": "WDSL( =0.2)\nGSL\nHDL",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "a",
          "Column_6": "ng",
          "Column_7": "er",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "di",
          "Column_13": "s",
          "Column_14": "g",
          "Column_15": "us",
          "Column_16": "t",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "f",
          "Column_22": "e",
          "Column_23": "a",
          "Column_24": "r",
          "Column_25": "",
          "Column_26": "",
          "Column_27": "",
          "Column_28": "",
          "Column_29": "h",
          "Column_30": "a",
          "Column_31": "p\nE",
          "Column_32": "pi\nm",
          "Column_33": "",
          "Column_34": "ne",
          "Column_35": "ss\not",
          "Column_36": "io",
          "Column_37": "",
          "Column_38": "n",
          "Column_39": "",
          "Column_40": "ne\ncl",
          "Column_41": "u\na",
          "Column_42": "tra\nss",
          "Column_43": "l",
          "Column_44": "",
          "Column_45": "",
          "Column_46": "s",
          "Column_47": "a",
          "Column_48": "d",
          "Column_49": "n",
          "Column_50": "es",
          "Column_51": "s",
          "Column_52": "",
          "Column_53": "",
          "Column_54": "s",
          "Column_55": "u",
          "Column_56": "r",
          "Baseline\nBDSL": "p",
          "Column_58": "r",
          "Column_59": "is",
          "Column_60": "e",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "l\no\nc",
          "Column_4": "r\no",
          "Column_5": "ac\nre\nrr",
          "Column_6": "cu\ncte\nec",
          "Column_7": "rac\nd\nted",
          "Column_8": "y\nby\nb",
          "Column_9": "[%\nth\nyt",
          "Column_10": "",
          "Column_11": "fo\nD\ne",
          "Column_12": "r\nS\nD",
          "Column_13": "L\nS",
          "Column_14": "ea\n-\nL",
          "Column_15": "ch\nBD\n-G",
          "Column_16": "cl\nSL\nSL",
          "Column_17": "",
          "Column_18": "ass\nm\nm",
          "Column_19": "in\neth\neth",
          "Column_20": "in\nth\nth",
          "Column_21": "th\nod\nod",
          "Column_22": "e\n(",
          "Column_23": "B\n(",
          "Column_24": "SA\nD\nGS",
          "Column_25": "V\nS\nL)",
          "Column_26": "EE\nL);\n;(",
          "Column_27": "",
          "Column_28": "da\n(3\n5)",
          "Column_29": "a\n3\n)",
          "Column_30": "t\n)\nl",
          "Column_31": "ab\nla\nab",
          "Column_32": "as\nbe\nels",
          "Column_33": "",
          "Column_34": "e\nls\nc",
          "Column_35": "obt\nco\nor",
          "Column_36": "ai\nrre\nrec",
          "Column_37": "ned\ncte\nted",
          "Column_38": "d\nte\nd",
          "Column_39": "d",
          "Column_40": "us\nb\nby",
          "Column_41": "in\ny\nt",
          "Column_42": "g:\nthe\nhe",
          "Column_43": "(1\nD\nD",
          "Column_44": ")o\nS\nSL",
          "Column_45": "ri\nL-\n-H",
          "Column_46": "gi\nW\nD",
          "Column_47": "n\nD",
          "Column_48": "a\nL",
          "Column_49": "l\nSL\nm",
          "Column_50": "noi\nm\net",
          "Column_51": "sy\net\nho",
          "Column_52": "",
          "Column_53": "eg\nd,\n(H",
          "Column_54": "me\nw\nD",
          "Column_55": "h\nL",
          "Column_56": "nt\ne\n)",
          "Baseline\nBDSL": "r\n,",
          "Column_58": "la\ne\nr",
          "Column_59": "be\nth\nes",
          "Column_60": "ls,\ne\npe",
          "Column_61": "",
          "Column_62": "",
          "Column_63": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WA": "65.00\n75.60\n76.30\n76.40\n76.20\n−",
          "UA": "−\n−\n87.90\n−\n87.88\n88.25"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "2",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "3",
      "title": "Segment-based approach to the recognition of emotions in speech",
      "authors": [
        "M Shami",
        "M Kamel"
      ],
      "year": "2005",
      "venue": "Proc. ICME"
    },
    {
      "citation_id": "4",
      "title": "Timing levels in segment-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "5",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "E Mower",
        "A Metallinou",
        "C.-C Lee",
        "A Kazemzadeh",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "6",
      "title": "A framework for automatic human emotion classification using emotion profiles",
      "authors": [
        "E Mower",
        "M Mataric",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Sentence level emotion recognition based on decisions from subsentence segments",
      "authors": [
        "J Jeon",
        "R Xia",
        "Y Liu"
      ],
      "year": "2011",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "10",
      "title": "Deep learning of segment-level feature representation with multiple instance learning for utterance-level speech emotion recognition",
      "authors": [
        "S Mao",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "11",
      "title": "Emotion profile refinery for speech emotion classification",
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "13",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "15",
      "title": "A study of the effect of different types of noise on the precision of supervised learning techniques",
      "authors": [
        "D Nettleton",
        "A Orriols-Puig",
        "A Fornells"
      ],
      "year": "2010",
      "venue": "Artificial intelligence review"
    },
    {
      "citation_id": "16",
      "title": "Learning from massive noisy labeled data for image classification",
      "authors": [
        "T Xiao",
        "T Xia",
        "Y Yang",
        "C Huang",
        "X Wang"
      ],
      "year": "2015",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "17",
      "title": "Deep self-learning from noisy labels",
      "authors": [
        "J Han",
        "P Luo",
        "X Wang"
      ],
      "year": "2019",
      "venue": "Proc. ICCV"
    },
    {
      "citation_id": "18",
      "title": "Classification in the presence of label noise: a survey",
      "authors": [
        "B Frénay",
        "M Verleysen"
      ],
      "year": "2013",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "19",
      "title": "Training convolutional networks with noisy labels",
      "authors": [
        "S Sukhbaatar",
        "J Bruna",
        "M Paluri",
        "L Bourdev",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "Training convolutional networks with noisy labels",
      "arxiv": "arXiv:1406.2080"
    },
    {
      "citation_id": "20",
      "title": "Making deep neural networks robust to label noise: A loss correction approach",
      "authors": [
        "G Patrini",
        "A Rozza",
        "A Krishna Menon",
        "R Nock",
        "L Qu"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "21",
      "title": "Using trusted data to train deep networks on labels corrupted by severe noise",
      "authors": [
        "D Hendrycks",
        "M Mazeika",
        "D Wilson",
        "K Gimpel"
      ],
      "year": "2018",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "22",
      "title": "Generalized cross entropy loss for training deep neural networks with noisy labels",
      "authors": [
        "Z Zhang",
        "M Sabuncu"
      ],
      "year": "2018",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "23",
      "title": "Learning from noisy large-scale datasets with minimal supervision",
      "authors": [
        "A Veit",
        "N Alldrin",
        "G Chechik",
        "I Krasin",
        "A Gupta",
        "S Belongie"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "26",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2005",
      "venue": "Proc. the 4th Workshop on Blizzard Challenge"
    },
    {
      "citation_id": "27",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "28",
      "title": "Data augmentation in emotion classification using generative adversarial networks",
      "authors": [
        "X Zhu",
        "Y Liu",
        "Z Qin",
        "J Li"
      ],
      "year": "2017",
      "venue": "Data augmentation in emotion classification using generative adversarial networks",
      "arxiv": "arXiv:1711.00648"
    },
    {
      "citation_id": "29",
      "title": "Adversarial data augmentation network for speech emotion recognition",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2019",
      "venue": "Proc. APSIPA"
    },
    {
      "citation_id": "30",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "31",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. ACII 2013"
    },
    {
      "citation_id": "32",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "33",
      "title": "Formulating emotion per-ception as a probabilistic model with application to categorical emotion classification",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "34",
      "title": "Soft-target training with ambiguous emotional utterances for dnn-based speech emotion classification",
      "authors": [
        "A Ando",
        "S Kobashikawa",
        "H Kamiyama",
        "R Masumura",
        "Y Ijima",
        "Y Aono"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "35",
      "title": "Human-like emotion recognition: Multi-label learning from noisy labeled audio-visual expressive speech",
      "authors": [
        "Y Kim",
        "J Kim"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "36",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "37",
      "title": "Label refinery: Improving imagenet classification through label progression",
      "authors": [
        "H Bagherinezhad",
        "M Horton",
        "M Rastegari",
        "A Farhadi"
      ],
      "year": "2018",
      "venue": "Label refinery: Improving imagenet classification through label progression",
      "arxiv": "arXiv:1805.02641"
    },
    {
      "citation_id": "38",
      "title": "Joint optimization framework for learning with noisy labels",
      "authors": [
        "D Tanaka",
        "D Ikami",
        "T Yamasaki",
        "K Aizawa"
      ],
      "year": "2018",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "39",
      "title": "Classification and regression by randomforest",
      "authors": [
        "A Liaw",
        "M Wiener"
      ],
      "year": "2002",
      "venue": "R news"
    },
    {
      "citation_id": "40",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "42",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "the 14th Python in Science Conference"
    },
    {
      "citation_id": "43",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "44",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "45",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "46",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard"
      ],
      "year": "2016",
      "venue": "Proc. OSDI"
    },
    {
      "citation_id": "47",
      "title": "Scikit-learn: Machine learning in python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "48",
      "title": "Mnasnet: Platform-aware neural architecture search for mobile",
      "authors": [
        "M Tan",
        "B Chen",
        "R Pang",
        "V Vasudevan",
        "M Sandler",
        "A Howard",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "49",
      "title": "Neural architecture search with reinforcement learning",
      "authors": [
        "B Zoph",
        "Q Le"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "50",
      "title": "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks",
      "authors": [
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Workshop on Challenges in Representation Learning, ICML"
    },
    {
      "citation_id": "51",
      "title": "From simulated speech to natural speech, what are the robust features for emotion recognition",
      "authors": [
        "Y Li",
        "L Chao",
        "Y Liu",
        "W Bao",
        "J Tao"
      ],
      "year": "2015",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "52",
      "title": "Weighted spectral features based on local hu moments for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen",
        "J Wang"
      ],
      "year": "2015",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "53",
      "title": "Ensemble softmax regression model for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "54",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z Liu",
        "Q Xie",
        "M Wu",
        "W Cao",
        "Y Mei",
        "J Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "55",
      "title": "An effective discriminative learning approach for emotion-specific features using deep neural networks",
      "authors": [
        "S Mao",
        "P Ching"
      ],
      "year": "2018",
      "venue": "Proc. ICONIP"
    },
    {
      "citation_id": "56",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    }
  ]
}