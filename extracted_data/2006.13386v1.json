{
  "paper_id": "2006.13386v1",
  "title": "Gender And Emotion Recognition From Implicit User Behavior Signals",
  "published": "2020-06-23T23:41:13Z",
  "authors": [
    "Maneesh Bilalpur",
    "Seyed Mostafa Kia",
    "Mohan Kankanhalli",
    "Ramanathan Subramanian"
  ],
  "keywords": [
    "Gender and Emotion Recognition",
    "Emotional Face Perception",
    "Implicit User Behavior",
    "Electroencephalography",
    "Eye Gaze Tracking",
    "Unoccluded vs occluded faces"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work explores the utility of implicit behavioral cues, namely, Electroencephalogram (EEG) signals and eye movements for gender recognition (GR) and emotion recognition (ER) from psychophysical behavior. Specifically, the examined cues are acquired via low-cost, off-the-shelf sensors. 28 users (14 male) recognized emotions from unoccluded (no mask ) and partially occluded (eye or mouth masked) emotive faces; their EEG responses contained gender-specific differences, while their eye movements were characteristic of the perceived facial emotions. Experimental results reveal that (a) reliable GR and ER is achievable with EEG and eye features, (b) differential cognitive processing of negative emotions is observed for females and (c) eye gaze-based gender differences manifest under partial face occlusion, as typified by the eye and mouth mask conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "G Ender human-computer interaction (HCI) [1] and Af- fective HCI  [2]  have evolved as critical HCI sub-fields, as it is critical for computers to appreciate and adapt to the user's gender and emotional state. Inferring users' soft biometrics such as gender and emotion would benefit interactive and gaming systems in terms of a) visual and interface design  [3] ,  [4] , (b) game and product recommendation (via ads)  [5] ,  [6] , and (c) provision of appropriate motivation and feedback for optimizing user experience  [7] . Gender recognition (GR) and emotion recognition (ER) systems primarily work with facial  [8] ,  [9]  or speech  [10] ,  [11]  cues which are biometrics encoding a person's identity. Also, they can be recorded without the user's knowledge, posing grave privacy concerns  [12] .\n\nThis work examines GR and ER from implicit user behavioral signals, in the form of EEG brain signals and eye movements. Implicit behavioral signals are inconspicuous to the outside world, and cannot be recorded without express user cooperation making them privacy compliant  [13] . Also, behavioral signals such as EEG and eye movements are primarily anonymous as little is known regarding their uniqueness to a person's identity  [14] .\n\nSpecifically, we attempt GR and ER using signals captured by commercial, off-the-shelf devices which are minimally intrusive, affordable, and popularly used in gaming as input or feedback modalities  [15] ,  [16] . The Emotiv EEG wireless headset consists of 14 dry (plus two reference) electrodes having a configuration as shown in Fig.  1 . While being lightweight, wearable and easy-to-use, neuro-analysis\n\n• Maneesh Bilalpur is with the School of Computing and Information, Univ.\n\nPittsburgh, USA. (E-mail: mab623@pitt.edu) • Seyed Mostafa Kia is with the Donders Institute at Radboud University, Nijmegen, The Netherlands. (E-mail: s.kia@donders.ru.nl) • Mohan Kankanhalli is with School of Computing at National University of Singapore, Singapore. (E-mail: mohan@comp.nus.edu.sg) • Ramanathan Subramanian is with the Indian Institute of Technology, Ropar. (E-mail: s.ramanathan@iitrpr.ac.in)\n\nFig.  1 : Emotiv Epoc+ electrode configuration: The headset comprises 14 sensing plus two reference electrodes.\n\nwith Emotiv can be challenging due to relatively poor signal quality. Likewise, EyeTribe is a low-cost eye-tracker whose suitability for research has been endorsed  [17] . We show how relevant gender and emotion-specific information is captured by these low-cost devices via examination of event-related potential (ERP) and eye fixation patterns, and also through recognition experiments.\n\nWe set out to discover gender differences in human visual perception by designing a facial emotion recognition (FER) experiment. Males and females respond differently to affective information  [7] ,  [18] ,  [19] ,  [20] , and user eye movements are also characteristic of the perceived facial emotion  [21] ,  [22] ,  [23] ,  [24] ,  [25] , enabling stimulus ER. Our study performed with 28 viewers (14 males) confirms that women achieve superior FER, mirroring prior findings. Hypothesizing that enhanced female emotional sensitivity should reflect via their implicit behavior, we examined EEG and eye-gaze patterns to find that (1) Stronger ERPs are observed for females while processing negative facial emotions, (2) Female eye-gaze is more focused on the eyes for the purpose of FER, and (3) Emotion and informationspecific gender differences manifest starkly, enabling better GR under particular stimulus conditions.\n\nBuilding on preliminary results  [20] ,  [26] , our work makes the following research contributions: (a) While prior works have identified gender differences in emotional behavior, this is one of the first works to expressly perform GR and ER from implicit behavioral signals; (b) Apart from recognition experiments, we show that the employed devices capture meaningful perceptual information, as typified by gender and emotion-specific event related potentials (ERPs), and the extent of fixation over the eyes; (c) We demonstrate a significant performance improvement in GR with deep learning and boosting methods over traditional approaches presented in  [20] ,  [26] ; (d) The use of minimally intrusive, off-the-shelf and low-cost devices affirms the ecological validity of our study, and the utility of our experimental design for large-scale user profiling.\n\nHereon, Section 2 reviews related work. Section 3 describes our study. Section 4 examines explicit user responses, which are correlated with implicit behaviors in Section 5, followed by GR and ER experiments. Section 6 summarizes and concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Many works perform ER with implicit behavioral signals such as eye movements, EEG, Electromyogram (EMG), Galvanic Skin Response (GSR)  [27] ,  [28] ,  [29] ,  [30] ,  [31]  etc. However, very few works estimate soft biometrics such as gender, cognitive load and personality traits with such signals  [32] ,  [33] ,  [34] . Also, while some works isolate emotion and gender differences in eye movement and EEG responses to emotional faces  [24] ,  [25] ,  [35] ,  [36] , these differential features are never utilized for gender prediction. To position our work with respect to the literature, this section reviews related work on (a) user-centered ER, and (b) gender differences in emotional face processing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "User-Centered Er",
      "text": "Emotions evoked by multimedia stimuli are predicted via content-centered or user-centered methods. Contentcentered methods attempt to find emotional multimodal features  [37] ,  [38] ,  [39] ,  [40] , while user-centered methods monitor user behavioral cues (eye movements, EEG signals, etc.) to deduce the evoked emotion. As emotions are subjective, many user-centered approaches predict emotions by examining both explicit and implicit user behavioral cues. Conspicuous facial cues are studied to detect multimedia highlights in  [9] , while physiological measurements are utilized to model emotions induced by music and movie scenes in  [29] ,  [30] . EEG and eye movements are two popular modalities employed for ER, and many works have used a combination of both  [20] ,  [27] ,  [28]  or either signal exclusively  [21] ,  [24] ,  [29] ,  [31] ,  [36] ,  [41] ,  [42] .\n\nValence (positive vs. negative emotion) recognition from eye-gaze features is achieved in  [21] . ER from EEG and pupillary responses is discussed in  [27] . Deep unsupervised ER from raw EEG data is proposed in  [41] , and its effectiveness is shown to be comparable to designed features. Differential entropy EEG features are extracted to train an integrated deep belief network plus hidden Markov model for ER in  [42] . A differential auto-encoder that learns shared representations from EEG and eye-based features is proposed for valence recognition in  [28] . Almost all of these works employ lab-grade eye-trackers and EEG sensors which are bulky and intrusive, and therefore preclude naturalistic user behavior.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Gender Differences In Emotion Recognition",
      "text": "As facial emotions denote critical non-verbal communication cues in social interactions, many psychology studies have studied human FER. Certain facial features encode emotions better than others; the eyes, nose and mouth are the most attractive facial regions  [43] ,  [44] . Visual attention is localized around the eyes for mildly emotive faces, but the nose and mouth attract substantial eye fixations in highly emotive faces  [45] . An eye tracking study  [25]  notes that distinct eye fixation patterns emerge for different facial emotions. The mouth is the most informative for the joy and disgust emotions, whereas eyes mainly encode information relating to sadness, fear, anger and shame. A similar study  [46]  notes more fixations on the upper face half for anger as compared to disgust, while no differences are observed on lower face half for the two emotions. However, humans may find it difficult to distinguish between similar facial emotions-examples are the high overlap rate between the fear-surprise and anger-disgust emotion pairs  [44] ,  [47] .\n\nMultiple works have discovered gender differences during facial emotion processing. Females are generally better at FER irrespective of age  [48] . Other FER studies  [18] ,  [19] ,  [49]  also note that females recognize facial emotions more accurately than males, even under partial information. Some evidence also points to females achieving faster FER than males  [50] ,  [51] . Gender differences in gaze patterns and neural activations have been found while viewing emotional faces; female tendency to fixate on the eyes positively correlates with their ER capabilities, while men tend to look at the mouth for emotional cues  [48] ,  [50] . Likewise, EEG Event-related potentials (ERPs) reveal that negative facial emotions are processed differently and rapidly by women, and do not necessarily entail selective attention towards emotional cues  [35] ,  [52] .\n\nAn exhaustive review of GR methodologies is presented in  [32] , and the authors evaluate GR methods using metrics like universality, distinctiveness, permanence and collectability. While crediting bio-signals like EEG and Electrocardiography (ECG) for their accuracy and trustworthiness, authors also highlight the invasiveness of bio-sensors. The sensors used in this work are minimally intrusive, enabling naturalistic user experience, while also recording meaningful emotion and gender-related information. Among user-centric GR works, EEG and speech features are proposed for age and gender recognition in  [53] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Analysis Of Related Work",
      "text": "Close examination of the literature reveals (1) Many works achieve ER from user-centered cues, both conspicuous and latent, and a handful have discovered gender differences in gaze patterns and neural activations; nevertheless, very few works expressly predict gender from implicit user cues. Differently, we employ implicit signals for GR and ER, and achieve reliable gender and valence detection (AUC > 0.9);\n\n(2) Our GR/ER features are acquired from low cost, off-theshelf sensors, which record inferior user signals even while enabling natural user behavior. We nevertheless show how these sensors capture meaningful information via the analysis of ERPs and fixation distribution patterns; (3) Different to prior works which either analyze explicit or implicit user responses to discover gender differences, or do not expressly isolate bio-signal patterns; in contrast, we show multiple similarities among explicit and implicit user behaviors to validate our findings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Materials And Methods",
      "text": "Our study objective was to examine user behavior while viewing unoccluded/partly occluded emotional faces, and predict user gender therefrom. We hypothesized that gender differences would be captured via EEG and eye-gaze patterns. Also, eye movements are known to be characteristic of the perceived facial emotion  [25] , which enables inference of the stimulus emotion; we limit ourselves to predicting the stimulus valence, i.e., whether the face presented to the viewer exhibits a positive or negative emotion?\n\nWe designed a study to examine gender differences in visual emotional face processing with a) fully visible faces, and (b) faces with the eye and mouth regions occluded via a rectangular mask (Fig.  2 ). Salience of the occluded features towards conveying facial emotions is well known  [25] ,  [45] . Specifically, we considered emotional faces corresponding to four conditions: exhibiting high intensity (HI) and low intensity (LI) emotions, and additionally, high intensity emotions upon occluding the eye (eye-mask) or mouth (mouth-mask) regions (Fig.  2 ). Since we hypothesized that emotion perception under facial occlusion would be considerably difficult (cf. Table  2 ), we did not study masked and mildly emotive faces.\n\nParticipants: 28 students from different nationalities (14 male, age 26.1 ± 7.3 and 14 female, age 25.5 ± 6), with normal or corrected vision, took part in our study. All users provided informed consent, and were presented a token fee for participation as directed by the ethics committee.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stimuli:",
      "text": "We used emotional faces of 24 models (12 male, 12 female) from the Radboud Faces Database (RaFD)  [54] . RaFD includes facial emotions of 49 models rated for clarity, genuineness and intensity, and the 24 models were chosen such that their Ekman facial emotions (Anger, Disgust, Fear, Happy, Sad and Surprise) were roughly matched based on these ratings. We then morphed the emotive faces from neutral (0% intensity) to maximum (100% intensity) to generate intermediate morphs in steps of 5%. Derived morphs with 55-100% intensity were used as HI emotions, and 25-50% were used as LI emotions. Eye and mouthmasked faces were automatically generated upon locating facial landmarks via Openface  [55]  over the HI morphs. The eye mask covered the eyes and nasion, while the mouth mask covered the mouth and the nose ridge. All stimuli were resized to 361×451 pixels, encompassing a visual angle of 9.1 • and 11.4 • about x and y at 60cm screen distance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Protocol:",
      "text": "The experimental protocol is outlined in Fig.  2 , and involved the presentation of unmasked and masked faces to viewers over two separate sessions, with a break inbetween to avoid fatigue. We chose one face per model and emotion, resulting in 144 face images (1 morph/emotion × 6 emotions × 24 models). In the first session (no-mask condition), these faces were shown in random order and were again re-presented randomly with an eye or mouth mask in the second session. We ensured a 50% split of the HI and LI morphs in the first session, and eye/mouth-masked faces in the second.\n\nDuring each trial, an emotional face was displayed for 4s preceded by a fixation cross for 500 ms. The viewer then had a maximum of 30s to make one out of seven choices concerning the facial emotion (six Ekman emotions plus neutral) via a radio button. Neutral faces were only utilized for morphing purposes and not used in the experiment. Viewers' EEG signals were acquired via the 14-channel Emotiv Epoc+ device, and eye movements were recorded with the Eyetribe tracker during the trials. The face-viewing experiment was split into 4 segments to facilitate sensor recalibration and minimize data recording errors, and took about 90 minutes to complete.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "User Responses",
      "text": "We first compare male and female sensitivity to emotions based on explicitly observed user response times (RTs) and recognition rates (RRs), and will then proceed to examine their implicit eye movement and EEG responses. Our experimental design involved four stimulus types (HI, LI, eye and mouth mask), and two user types (male and female), resulting in 4 × 2 factor conditions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Response Time (Rt)",
      "text": "Overall user RTs for the HI, LI, eye mask and mouth mask conditions were respectively found to be 1.44 ± 0.24, 1.52 ± 0.05, 1.17 ± 0.12 and 1.25 ± 0.09 seconds, implying that FER was fairly instantaneous, and viewer responses were surprisingly faster with masked faces. Fine-grained comparison of male (m) and female (f ) RTs across stimulus types (Fig.  3 ) revealed that females (µ RT = 1.40 ± 0.10s) were generally faster than males (µ RT = 1.60 ± 0.10s) at recognizing HI emotions. There was no significant difference in RTs for LI emotions. Female alacrity nevertheless decreased for masked faces, with males responding marginally faster for eye masked (µ RT (m) = 1.13 ± 0.11s vs µ RT (f ) = 1.21 ± 0.13s), and both genders responding with similar speed for mouth masked faces (µ RT (m) = 1.24 ± 0.10s vs µ RT (f ) = 1.25 ± 0.09s).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Recognition Rates",
      "text": "While females recognized facial emotions marginally faster, we examined if they also achieved superior FER. Overall, RRs for unoccluded HI emotions (µ RR = 77.6) were expectedly higher than for eye-masked (µ RR = 59.7), mouthmasked (µ RR = 63.5) and LI emotions (µ RR = 49.1). Happy faces were recognized most accurately in all four conditions. Specifically focusing on gender differences (Fig.  4 ), females recognized facial emotions more accurately than males and this was particularly true for negative (A, D, F, S) emotions;",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Analyzing Implicit Responses",
      "text": "As females achieved quicker and superior FER for negative emotions, we hypothesized that these behavioral differences should also reflect via implicit eye gaze and EEG patterns. We first describe the EEG and eye-movement descriptors employed for analyses, before discussing (stimulus) emotion and (user) gender recognition results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg Preprocessing",
      "text": "We extracted EEG epochs for each trial (4.5s of stimulusplus-fixation viewing time at 128 Hz sampling rate), and the 64 leading pre-stimulus samples were used to remove DC offset. This was followed by (a) EEG band-limiting to within 0. Finally, a 7168 dimensional (14 channel ×4s×128 Hz) EEG feature vector was generated and fed to different classifiers for GR and ER (Section 5.3).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Event Related Potentials",
      "text": "Event Related Potentials are time-locked neural responses related to sensory and cognitive events, and denote the EEG response averaged over multiple users and trials. As examples, P300 and N100, N400 are exemplar ERPs which are typically noted around 300, 100 and 400 ms post stimulus onset. ERPs occurring within 100 ms post stimulus onset are stimulus-related (exogenous), while later ERPs are cognitionrelated (endogenous). We examined the leading 128 EEG epoch samples (one second of data) for ERP patterns relating to emotion and gender.\n\nPrior works have observed ERP-based gender differences from lab-grade sensor recordings  [35] ,  [52] ,  [56] . Specifically,  [52]  notes enhanced negative ERPs for females in response to negative valence stimuli. However, capturing ERPs with commercial devices is challenging due to their low signal-to-noise ratio  [57] . Figs.  5  and 6  present the P300, visual N100 and N400 ERP components in the occipital O1 and/or O2 electrodes (see Fig.  1  for sensor positions) corresponding to various face morphs. Note that the occipital lobe is the visual processing center in the brain, as it contains the primary visual cortex.\n\nComparing O1/O2 male and female ERPs for positive (H, Su) and negative (A, D, F, Sa) emotions, no significant differences can be observed between male positive and negative ERP peaks for HI or LI faces (columns 3,4 in Fig.  5 ). However, we observe stronger N100 and P300 peaks in the negative female ERPs for both HI and LI faces (columns 1,2). Also, a stronger female N400 peak can be noted for HI faces consistent with prior findings  [52] . Contrastingly, lower male N100 and P300 latencies are observed for positive HI emotions, with the pattern being more obvious at O2. Likewise, lower male N400 latencies can be generally noted at O2 for positive emotions. The positive vs negative ERP difference for females is narrower for LI faces, revealing the difficulty in identifying mild LI emotions. That LI faces produce weaker ERPs at O1 and O2 than HI faces further supports this observation.\n\nFig.  6  shows female ERPs observed in the occipital O2 electrode for the HI and eye mask conditions. Clearly, one can note enhanced N100 and P300 ERP components for negative HI emotions (Fig.  6(left) ). This effect is attenuated in the eye mask (Fig.  6 (right)) and mouth mask cases, although one can note stronger N400 amplitudes for D and F with eye mask. This ERP pattern is invisible for males, confirming thir gender-specificity. Overall, ERP patterns affirm that gender differences in emotional face processing can be reliably isolated with the low-cost Emotiv device.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Eye-Tracking Analysis",
      "text": "Gender differences in gaze patterns during emotional face processing have been noted by prior works  [25] ,  [48] . We used the low-cost Eyetribe device with 30 Hz sampling to record eye movements. Raw gaze data output by the tracker were processed to compute fixations (prolonged gazing at scene regions to assimilate visual information) and saccades (transition from one fixation to another) via the EyeMMV toolbox  [58] . Upon extracting fixations and saccades, we extracted features employed for valence recognition in  [21]  to compute an 825-dimensional feature vector for our analyses.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fixation Analysis",
      "text": "To study gender differences in fixating patterns, we computed the distribution of fixation duration (FD) over six facial regions, namely, eyes, nose, mouth, cheeks, forehead and chin. Fig.  7  presents the males and female FD distribution over these facial regions for various conditions. For both genders, the time spent examining the eyes, nose and mouth accounted for over 80% of the total FD, with eyes (≈45%) and nose (≈30%) attracting maximum attention as observed in  [25] ,  [36] ,  [45] . Relatively similar FD distributions were noted for both genders with HI and LI morphs (Fig.  7 a, b ). Fig.  7(c, d ) present FD distributions in the eye and mouth mask conditions. An independent t-test revealed a significant difference (p<0.05) between male and female FDs for the eye region in the mouth mask condition; prior works  [23]  have observed that females primarily look at the eyes for emotional cues, which is mirrored by longer FDs around the eyes in the HI and mouth mask conditions. Fig.  7(c ) shows that when eye information is unavailable, females tend to focus on the mouth and nasal regions for FER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "Having noticed gender-specific patterns in user responses, EEG ERPs and eye movements, we attempted binary emotion recognition (ER) and gender recognition (GR) employing EEG features, eye-based features and their combination for the various conditions. Recognition was attempted only on trials where viewers correctly recognized the presented facial emotion. We considered (i) EEG features, (ii) eyebased features, (iii) concatenation of the two (early fusion or EF), and (iv) probabilistic fusion of the EEG and eyebased classifier outputs (late fusion or LF) for our analyses. The W est technique  [59]  was used to fuse the EEG and eyebased outputs, and denotes maximum possible recognition performance as optimal weights maximizing the test AUC metric were determined via a 2D grid search.\n\nWe considered the area under ROC curve (AUC) plotting true vs false positive rates, as the performance metric for benchmarking. AUC is suitable for evaluating classifier performance on unbalanced data, and a random classifier will achieve an AUC of 0.5. As we attempted recognition with few training data, we report ER/GR results over five repetitions of 10-fold cross validation (CV) (i.e., total of 50 runs). CV is typically used to overcome the overfitting problem, and train generalizable classifiers on small datasets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Classification Approaches",
      "text": "As baselines, we considered the Naive-Bayes (NB), linear SVM (LSVM) and radial-basis SVM (RSVM) classifiers. NB is a generative classifier that estimates the test label based on the maximum a-posteriori criterion, p(C | X), assuming classconditional feature independence. C and X respectively denote test class-label and feature vector. LSVM and RSVM denote the linear and radial basis kernel versions of the",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cnn For Eeg-Based Gr & Er",
      "text": "Deep learning frameworks have recently become popular due to their ability to automatically learn optimal task features from raw data, thereby obviating the need for data cleaning and feature extraction. However, unlike in image or video processing, temporal dynamics of the human brain are largely unclear; designing relevant features is therefore hard. We hypothesized that CNNs would encode EEG patterns efficiently, and also be robust to artifacts. These factors inspired us to feed raw EEG data to a CNN, without any data processing (as in Sec. 5.1); preprocessed EEG data was nevertheless fed to the above baseline classifiers.\n\nWe adopted a 3-layer Convolutional Neural Network (CNN)  [60]  to learn a robust EEG representation for gender and valence recognition. Three convolution layers together with rectified linear unit (ReLU) activation, and average pooling layers are stacked to learn EEG descriptors (Fig.  8 ). The convolutions employed are 1-dimensional along time. Batch normalization  [61]  is used after the third CNN layer to minimize covariate shift and accelerate training. To prevent overfitting, we used dropout after the fully connected layer with 128 neurons. A softmax over two output neurons was used for classification.\n\nThe number of kernels increase with network depth (cf. Fig  8 ) analogous to the VGG architecture  [62] . We optimized the network for categorical cross-entropy loss using stochas-   1  were either adopted from  [60]  or upon cross-validation.\n\nThe CNN was traditionally trained for GR with He normal initializers (normally distributed weights). Conversely, we adopted two-stage training for ER. In the first stage, the model was pre-trained with EEG data acquired over all four experimental conditions (Fig 2 ); the second stage involved fine-tuning with data for a specific condition. The objective here was to extract valence-related features irrespective of stimulus type in the first stage, and fine-tune them to learn condition-specific descriptors. Experiments were designed using Keras  [63]  with Tensorflow back-end on a 16 GB CPU machine and an Intel i5 processor.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Adaboost For Er From Eye-Gaze",
      "text": "We employed the Adaboost classifier for ER from eyegaze features. AdaBoost is an ensemble classifier popularly used for face detection  [64]  and FER  [65] . Adaboost combines a number of weak classifiers (decision stumps), each marginally better than random, to cumulatively achieve optimal classification. The class label is based on the weighted output of each weak classifier. Our features (Sec. 5.1) and classifier design were inspired by  [65] , who capture local dependencies via histograms of gaze measures like fixations, saccades and saliency. We employed SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss  function)  [66]  for training. Similar to  [65] , local Gaussian and Gabor features were extracted at multiple scales to generate composite features for AdaBoost.\n\nWe only obtained sparse gaze features in our study as viewer gaze was (a) localized to face regions of emotional importance and (b) recorded via a low-cost and low sampling rate device. We firstly performed feature selection to prune features relevant for ER. Feature selection was based on sequential addition (or removal) for optimal performance  [21]  using sequential forward (SFS) or backward (SBS) selection. User data compiled for this study along with the CNN and Adaboost models employed for GR and ER respectively are available at https://github.com/ bmaneesh/emotion-xavier.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Recognition",
      "text": "We modeled stimulus ER as a binary classification problem, where the objective is to categorize positive (H, Su) and negative (A, D, F, Sa) valence stimuli employing gaze and EEGbased cues. ER results with (CNN-based) EEG, (Adaboostbased) eye gaze features, and late fusion of two modalities, with data acquired for different conditions characterized by user-gender (M/F) and stimulus type (HI/LI/eyemask/mouth-mask) are shown in Fig.  9 . Evidently, gaze features perform significantly better than EEG. Gaze features achieve near-ceiling valence recognition barring the case where female users view LI emotions. Optimal performance is noted with male data for HI emotions (AUC = 0.99) similar to  [67] ; ER from female eye-gaze data on HI emotions (AUC = 0.98) is also high.\n\nOn the other hand, ER results with EEG largely produced near-chance performance, with only male and female data for the mouth-masked condition being exceptions. These results reveal that the raw EEG features are not optimal for ER. Given the vast difference in ER performance with the gaze and EEG modalities, late fusion results are only occasionally superior to unimodal ones. Fusion slightly outperforms unimodal methods for the eye mask condition for both males (0.98 vs. 0.99) and females (0.97 vs. 0.98), and noticeably with males for mouth mask faces (0.91 vs. 0.94).\n\nOverall, while recognizing the stimulus facial emotion was not our primary objective, the extracted gaze features are nevertheless highly effective for valence recognition (VR). Adaboost employing gaze features significantly outperforms the CNN trained with EEG data. The ability of eye movements to characterize emotional differences is unsurprising. Distinctive eye movement patterns have been observed while scanning scenes and faces with different emotions  [21] ,  [24] ,  [25] . Eye-based features are found to achieve 52.5% VR accuracy in  [21] , where emotions are induced by presenting diverse emotional scenes, while our study is specific to emotive faces. Prior neural studies  [29] ,  [56]  which perform emotion recognition with lab-grade sensors achieve around 60% VR. Superior performance of eye-based features can be attributed to the fact that the saccade and saliency-based statistics can capture discrepancies in gazing patterns for positive and negative facial emotions  [21] ,  [24] ; in contrast, neural features have only been moderately effective at decoding emotions conveyed by audio-visual music and movie stimuli  [29] ,  [30] ,  [56] , which can presumably elicit emotions more effectively than plain imagery.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Gender Recognition",
      "text": "GR involves labeling the test EEG or eye-gaze sample as arising from a male/female user. Table  2  presents GR achieved with baseline classifiers, while Table  3  shows AUC scores obtained with the EEG-based CNN, and the Adaboost ensemble fed with eye-gaze features. Both tables present GR results when user data corresponding to All emotional faces, and data for each of the six Ekman emotions (Emotion-wise) are employed for model training. Also, the best EF and LF results along with the relevant classifier are specified for the All condition. Cumulatively, Tables  2  and 3  clearly convey that the CNN and Adaboost frameworks considerably outperform baseline classifiers.\n\nFocusing on Table  2 , one can clearly note that the AUC scores with HI emotions are typically higher (cf. columns 1,2); also, AUC metrics achieved in the mouth-mask condition are typically higher than with eye-mask (columns 3,4). These findings from implicit user behavior mirror explicit recognition results in Fig.  4 . Specifically, significant differences can be noted with EEG-based results for the above conditions, while differences with eye-gaze are inconspicuous. These results cumulatively convey that (a) analyses of explicit and implicit user behavior affirm similar trends relating to visual FER; (b) gender differences are better encoded while perceiving high-intensity emotions, and how eye cues are interpreted for emotion inference, and (c) EEG signals better encode gender differences in visual emotion processing than gaze patterns.\n\nExamining fusion results, LF is generally superior to EF when all emotions are considered, while EF achieves superior performance when emotion-specific user data are utilized for GR. We attempted GR from emotion-specific data to follow up on findings in Sec. 4, conveying females to be more sensitive to negative emotions. In Table  2 , eyegaze based GR performance improves significantly when emotion-specific data are considered (peak AUC of 0.68 for fear under eye-mask, vs peak AUC of 0.53 under mouthmask for all emotions), while EEG results remain stable (peak AUC of 0.71 for HI anger vs peak AUC of 0.71 for all HI emotions). Optimal GR results (in bold) are mostly achieved for the A, D, F and Sa emotions implying that gender differences best manifest for negative valence.\n\nTable  3  largely replicates the trends noted from Table 2, with higher AUC scores achieved with the CNN and Adaboost classifiers. The one notable difference is that while the optimal NB baseline performs poorly with gaze features but much better with EEG in Table  2 , the Adaboost framework outperforms the EEG-based CNN in the eye and mouth-mask conditions; EEG nevertheless encodes gender differences better while processing HI and LI emotions. These results imply that (a) eye movements in pursuit of FER, especially under partial face occlusion, are distinctive of gender, and (b) the discriminative Adaboost framework is able to better learn distinctive eye movement patterns as compared to the generative NB classifier. Late fusion results denoting the combination of decisions made by the CNN and Adaboost reveal that when one modality is considerably more potent than the other (EEG >> Eye for HI and LI, while Eye > EEG for mask), LF does not necessarily outperform constituent modalities.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Spatio-Temporal Eeg Analysis",
      "text": "We also examined spatio-temporal characteristics of the EEG signal to examine if (i) gender differences in visual emotion processing are effectively captured by certain electrodes, and are attributable to specific (functional) brain lobes and (ii) any time window(s) were critical for GR over 4s of visual processing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Spatial:",
      "text": "We evaluated the ability of each EEG channel (cf.  Fig 1)  to capture gender-discriminative information by feeding the deep network (Fig.  8 ) with single-channel EEG input. Consistent with prior findings  [20] ,  [67] ,  [68] , we noted optimal GR with the frontal and occipital brain lobes. Single channel GR followed a trend similar to Table  3 , with optimal GR achieved for HI emotions, and worst GR performance noted with mouth-mask data. A symmetricity was noted among the optimal EEG channels, namely, AF3 and AF4, and F3 and F4. These results mirror observations relating to the existence of brain hemispheres  [69]  for ER.\n\nTemporal: As isolation of gender-specific ERPs is possible from 1s time-windows (Sec. 5.1.1), we considered four nonoverlapping 1s windows W1-W4 spanning 4s of stimulus viewing. Fig.  10  presents GR AUCs achieved over W1-W4 from emotion-specific EEG data. Plots confirm that reliable, above-chance GR is achieved over each of W1-W4 across the different viewing conditions. Highest GR performance is noted for HI morphs and lowest GR for eye mask faces, consistent with Table  3 . While temporal analyses revealed no significant GR differences across windows, optimal GR was generally achieved for W1 and W2 over all morph conditions suggesting a primacy effect. Masked conditions result in higher GR performance variance, perhaps due to the initial difficulty in FER owing to facial occlusions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion & Conclusion",
      "text": "A critical requirement of today's ubiquitous computing devices is to sense user intention, emotion and cognition from multimodal cues, and devise effective interactions to    it would also be beneficial for AC systems to predict soft biometrics such as user age and gender for effective interaction  [70] . There is also a strong need to develop sensing mechanisms that respect user privacy concerns  [71] , and eye-movements and EEG signals represent privacy preserving implicit behaviors which enable inference of user traits even as the user identity remains hidden. The primary objective of this study is to explore the utility of eye movements and EEG for user gender prediction. We designed a study where these implicit user behaviors were recorded as 28 users (14 male) recognized facial emotions; since eye movements are known to be characteristic of the perceived facial emotion  [25] ,  [45] , our study design additionally enabled recognition of the facial (stimulus) valence. Crucially, our study employs lightweight and inexpensive sensors for inference, as against bulky and intrusive lab sensors which are typically used in user-centered analyses but significantly constrain user behavior. Also, to examine user efficacy for FER under occlusions, we presented both unoccluded (high and low-intensity), and occluded (with the eye or mouth region masked) emotive faces to users.\n\nThe fact that gender differences exist in visual emotional processing is demonstrated at multiple levels via our experiments. In terms of explicit user responses, female users are found to quickly and accurately perform FER on unoccluded faces, especially for negative valence emotions (Section 4). Likewise, females also achieve higher recognition of negative emotions with mouth-masked faces. Subsequent examination of implicit cues revealed interesting correlations; examination of eye fixation distributions showed greater female fixation around the eyes than males irrespective of stimulus type, and fixations on eyes were significantly longer in the mouth-mask condition. This observation is consistent with prior studies  [23] , and the proposition that females primarily look at the eyes for emotional cues.\n\nAnalysis of EEG ERPs also conveyed interesting patterns. While processing unoccluded faces, stronger N100 and P300 peaks are noted in female ERPs for negative emotions, as well as a stronger N400 peak for strongly negative faces. This suggests a differential cognitive processing of negative vs positive emotions by females, which results in their enhanced sensitivity towards negative emotions. Likewise, lower male N400 latencies are generally noted at the O2 electrode for positive emotions. Stronger N100 and P300 female ERPs are also noted for negative mouth-masked emotions, suggesting that negative emotions are processed rapidly by women, and do not necessarily entail selective attention to emotional cues.\n\nThat implicit and differential behaviors can be isolated using data acquired via low-cost sensors affirms that our experimental design can effectively capture emotion and gender-specific information. Greater female sensitivity to negative emotions can be attributed to several factors like social structure, environment and evolution, way of living and social stereotypes  [72] . While we did not seek to expressly elicit emotions through facial imagery, facial expressions are known to induce emotions in the viewer  [73] , and examination of users' emotional behavior enables prediction of both the user gender and stimulus valence.\n\nValence recognition experiments employing eye-gaze features (processed by an Adaboost ensemble) and EEG features (input to a 3-layer CNN performing 1D convolutions) revealed the following. Eye-gaze patterns on HI emotional faces were highly characteristic of facial valence for both male and female users, resulting in AUC scores ≥ 0.98. Contrastingly, EEG produced largely near-chance performance, implying that the EEG features are sub-optimal for ER. Gulf in the efficacy of eye-gaze and EEG features meant that late fusion of the classifier outputs was hardly beneficial. That human eye movements are highly characteristic of stimulus valence is unsurprising, with prior studies  [21]  achieving better-than-chance accuracy with gaze features compiled for diverse scenes; on the contrary, our study is specific to emotional faces.\n\nGender recognition results are summarized as follows. Table  2  presenting GR results achieved with the baseline NB, LSVM and RSVM classifiers affirms that consistent with the recognition rate statistics and ERP analyses, EEG-based gender differences best manifest for HI emotional faces, and are least observable for LI emotional faces. Emotionspecific analyses affirm that peak GR AUC scores are mostly achieved with negative valence data. Also, the NB classifier achieves much superior results with EEG as compared to eye-gaze features. On the other hand, Table  3  shows that much superior GR results are achievable with the CNN and Adaboost classifiers respectively fed with the EEG and eyegaze data. Interestingly, very high AUC scores are achieved with eye-gaze features compiled for the mask conditions (this trend is unobservable for the HI and LI conditions), implying that eye movements in pursuit of FER under par-tial face occlusion are highly gender-specific. The substantial disparity in EEG and eye-based results across the different conditions results in the fusion of the two modalities is hardly beneficial.\n\nWhile the presented study involves only a small pool of (N=28) users, the observed GR and ER results are nevertheless highly promising and demonstrate the utility of implicit behaviors for privacy preserving user profiling. The ever-increasing and commonplace availability of sensors employed in this work also opens up the possibility of conducting large-scale (crowdsourced) user-centric studies. Our larger endeavor is to predict soft biometrics (age, gender, emotional and cognitive state) of users via implicit and multimodal behavioral cues to empower gaming, advertising, augmented and virtual reality applications for behavioral change, and mental health monitoring systems for disorders like Alexithymia. Future work will focus on the modeling of shared behaviors (joint embedding of EEG and eye-gaze features) for efficient user-trait prediction employing techniques such as multi-task learning, and prototyping real-life profiling applications. Investigating the optimality of predesigned features (e.g., power-spectral density for EEG) or learned feature descriptors represents another interesting line of work.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotiv Epoc+ electrode conﬁguration: The headset",
      "page": 1
    },
    {
      "caption": "Figure 2: ). Salience of the occluded features",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Since we hypothesized",
      "page": 3
    },
    {
      "caption": "Figure 3: ) revealed that females",
      "page": 3
    },
    {
      "caption": "Figure 4: ), females",
      "page": 3
    },
    {
      "caption": "Figure 2: Experimental Protocol: Viewers were required to recognize the facial emotion from either an unmasked face (Session",
      "page": 4
    },
    {
      "caption": "Figure 3: Emotion-wise RTs of females and males in the",
      "page": 4
    },
    {
      "caption": "Figure 4: Emotion-wise RRs in various conditions.",
      "page": 4
    },
    {
      "caption": "Figure 1: for sensor positions) cor-",
      "page": 5
    },
    {
      "caption": "Figure 6: shows female ERPs observed in the occipital O2",
      "page": 5
    },
    {
      "caption": "Figure 6: (left)). This effect is attenuated in the",
      "page": 5
    },
    {
      "caption": "Figure 6: (right)) and mouth mask cases, although one",
      "page": 5
    },
    {
      "caption": "Figure 7: presents the males and female FD distribution",
      "page": 5
    },
    {
      "caption": "Figure 5: ERPs for HI morphs (top) and LI morphs (bottom): (left to right) O1 and O2 ERPs for females and males. y-axis",
      "page": 6
    },
    {
      "caption": "Figure 6: Female ERPs in the (left) HI and (right) eye mask",
      "page": 6
    },
    {
      "caption": "Figure 8: ) analogous to the VGG architecture [62]. We optimized",
      "page": 6
    },
    {
      "caption": "Figure 2: ); the second stage involved",
      "page": 6
    },
    {
      "caption": "Figure 7: (a–d) Fixation duration distributions for males and females in the HI, LI, eye and mouth-masked conditions.",
      "page": 7
    },
    {
      "caption": "Figure 8: CNN architecture showing various layers in the model and parameters.",
      "page": 7
    },
    {
      "caption": "Figure 9: Evidently, gaze",
      "page": 7
    },
    {
      "caption": "Figure 9: Valence recognition results for different conditions.",
      "page": 7
    },
    {
      "caption": "Figure 4: Speciﬁcally, signiﬁcant differ-",
      "page": 8
    },
    {
      "caption": "Figure 1: ) to capture gender-discriminative information",
      "page": 8
    },
    {
      "caption": "Figure 8: ) with single-channel",
      "page": 8
    },
    {
      "caption": "Figure 10: presents GR AUCs achieved over W1–W4",
      "page": 8
    },
    {
      "caption": "Figure 10: (a–d) Temporal EEG analyses: GR for HI, LI, Eye and Mouth mask conditions over temporal windows (W1–W4).",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Spatial performance evaluation. Parentheses de- optimize users’ individual and social behaviors. Affective",
      "data": [
        {
          "AUC": "All",
          "HI\nLI\nEyemask\nMouthmask": "0.714 ± 0.002\n0.600 ± 0.005\n0.690 ± 0.03\n0.654 ± 0.04\n0.493 ± 0.013\n0.481 ± 0.017\n0.471 ± 0.06\n0.525 ± 0.05\n0.522 ± 0.035\n0.524 ± 0.022\n0.520 ± 0.07\n0.520 ± 0.06\n0.549 ± 0.022\n0.523 ± 0.035\n0.540 ± 0.05\n0.610 ± 0.07"
        },
        {
          "AUC": "Emotionwise",
          "HI\nLI\nEyemask\nMouthmask": "0.708 ± 0.064\n0.580 ±0.074\n0.610 ± 0.16\n0.672 ± 0.07\n0.673 ± 0.055\n0.696 ± 0.062\n0.592 ± 0.06\n0.650 ± 0.14\n0.643 ± 0.059\n0.596 ± 0.089\n0.605 ± 0.14\n0.564 ± 0.12\n0.696 ± 0.047\n0.668 ± 0.046\n0.586 ± 0.07\n0.624 ± 0.08\n0.674 ± 0.048\n0.634 ± 0.064\n0.652 ± 0.08\n0.590 ± 0.08\n0.692 ± 0.048\n0.636 ± 0.071\n0.633 ± 0.08\n0.650 ± 0.09"
        },
        {
          "AUC": "",
          "HI\nLI\nEyemask\nMouthmask": "0.601 ± 0.021\n0.565 ± 0.031\n0.470 ± 0.20\n0.590 ± 0.16\n0.577 ± 0.011\n0.632 ± 0.009\n0.480 ± 0.14\n0.521 ± 0.27\n0.595 ± 0.015\n0.535 ± 0.029\n0.680 ± 0.25\n0.580 ± 0.17\n0.560 ± 0.021\n0.538 ± 0.017\n0.555 ± 0.13\n0.540 ± 0.12\n0.539 ± 0.015\n0.605 ± 0.030\n0.445 ± 0.15\n0.455 ± 0.13\n0.555 ± 0.008\n0.555 ± 0.018\n0.624 ± 0.13\n0.494 ± 0.16"
        },
        {
          "AUC": "",
          "HI\nLI\nEyemask\nMouthmask": "0.555 ± 0.021\n0.581 ± 0.037\n0.320 ± 0.25\n0.390 ± 0.18\n0.535 ± 0.024\n0.622 ± 0.025\n0.521 ± 0.18\n0.500 ± 0.24\n0.618 ± 0.011\n0.619 ± 0.041\n0.700 ± 0.22\n0.522 ± 0.18\n0.575 ± 0.017\n0.597 ± 0.009\n0.575 ± 0.13\n0.524 ± 0.14\n0.540 ± 0.021\n0.598 ± 0.024\n0.532 ± 0.16\n0.502 ± 0.14\n0.579 ± 0.013\n0.574 ± 0.014\n0.433 ± 0.13\n0.515 ± 0.16"
        },
        {
          "AUC": "",
          "HI\nLI\nEyemask\nMouthmask": "0.543 ± 0.062\n0.571 ± 0.081\n0.570 ± 0.15\n0.590 ± 0.09\n0.542 ± 0.044\n0.597 ± 0.093\n0.590 ± 0.10\n0.570 ± 0.14\n0.519 ± 0.029\n0.597 ± 0.170\n0.645 ± 0.16\n0.691 ± 0.12\n0.526 ± 0.031\n0.508 ±0.017\n0.564 ± 0.09\n0.552 ± 0.10\n0.573 ± 0.076\n0.584 ± 0.102\n0.513 ± 0.04\n0.580 ± 0.11\n0.562 ± 0.073\n0.568 ± 0.125\n0.581 ± 0.08\n0.583 ± 0.08"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: Spatial performance evaluation. Parentheses de- optimize users’ individual and social behaviors. Affective",
      "data": [
        {
          "AUC": "All",
          "HI\nLI\neye mask\nmouth mask": "0.931 ± 0.011\n0.882 ± 0.020\n0.892 ± 0.038\n0.886 ± 0.023\n0.521 ± 0.016\n0.538 ± 0.023\n0.969 ± 0.015\n0.966 ± 0.014\n0.920 ± 0.015\n0.850 ± 0.045\n0.974 ± 0.017\n0.946 ± 0.037"
        },
        {
          "AUC": "Emotionwise",
          "HI\nLI\neye mask\nmouth mask": "0.757 ± 0.113\n0.719 ± 0.086\n0.626 ± 0.165\n0.676 ± 0.157\n0.758 ± 0.047\n0.644 ± 0.080\n0.594 ± 0.161\n0.673 ± 0.108\n0.738 ± 0.083\n0.618 ± 0.107\n0.542 ± 0.128\n0.585 ± 0.199\n0.790 ± 0.049\n0.701 ± 0.054\n0.714 ± 0.082\n0.751 ± 0.051\n0.739 ± 0.089\n0.737 ± 0.078\n0.689 ± 0.097\n0.731 ± 0.079\n0.720 ± 0.076\n0.636 ± 0.042\n0.649 ± 0.098\n0.652 ± 0.067"
        },
        {
          "AUC": "",
          "HI\nLI\neye mask\nmouth mask": "0.651 ± 0.064\n0.651 ± 0.079\n0.840 ± 0.160\n0.957 ± 0.038\n0.591 ± 0.038\n0.600 ± 0.068\n0.950 ± 0.045\n0.910 ± 0.108\n0.569 ± 0.078\n0.528 ± 0.157\n0.950 ± 0.067\n0.881 ± 0.075\n0.574 ± 0.090\n0.529 ± 0.050\n0.931 ± 0.049\n0.929 ± 0.051\n0.587 ± 0.087\n0.576 ± 0.068\n0.959 ± 0.051\n0.958 ± 0.041\n0.530 ± 0.093\n0.546 ± 0.052\n0.938 ± 0.067\n0.921 ± 0.107"
        },
        {
          "AUC": "",
          "HI\nLI\neye mask\nmouth mask": "0.720 ± 0.082\n0.651 ± 0.113\n0.835 ± 0.167\n0.860 ± 0.132\n0.676 ± 0.083\n0.557 ± 0.153\n0.939 ± 0.070\n0.967 ± 0.105\n0.675 ± 0.140\n0.470 ± 0.164\n0.887 ± 0.139\n0.869 ± 0.097\n0.770 ± 0.050\n0.683 ± 0.088\n0.897 ± 0.042\n0.926 ± 0.060\n0.729 ± 0.062\n0.555 ± 0.107\n0.939 ± 0.062\n0.843 ± 0.133\n0.723 ± 0.090\n0.604 ± 0.105\n0.826 ± 0.142\n0.943 ± 0.058"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gender HCI: What about the software?",
      "authors": [
        "S Wiedenbeck",
        "V Grigoreanu",
        "L Beckwith",
        "M Burnett"
      ],
      "year": "2006",
      "venue": "Computer"
    },
    {
      "citation_id": "2",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "The interaction between gender, age, and multimedia interface design",
      "authors": [
        "D Passig",
        "H Levin"
      ],
      "year": "2001",
      "venue": "Education and Info Tech"
    },
    {
      "citation_id": "4",
      "title": "Women take a wider view",
      "authors": [
        "M Czerwinski",
        "D Tan",
        "G Robertson"
      ],
      "year": "2002",
      "venue": "CHI"
    },
    {
      "citation_id": "5",
      "title": "Gender and gaze gesture recognition for human-computer interaction",
      "authors": [
        "W Zhang",
        "M Smith",
        "L Smith",
        "A Farooq"
      ],
      "year": "2016",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "6",
      "title": "Gender and player characteristics in video game play of preadolescents",
      "authors": [
        "B Homer",
        "E Hayward",
        "J Frye",
        "J Plass"
      ],
      "year": "2012",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "7",
      "title": "Gender and personality trait measures impact degree of affect change in a hedonic computing paradigm",
      "authors": [
        "J Schwark",
        "I Dolgov",
        "D Hor",
        "W Graves"
      ],
      "year": "2013",
      "venue": "Int'l Journal Human-Computer Interaction"
    },
    {
      "citation_id": "8",
      "title": "Vision-based human gender recognition: A survey",
      "authors": [
        "C Ng",
        "Y Tay",
        "B.-M Goi"
      ],
      "year": "2012",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Looking at the viewer: analysing facial activity to detect personal highlights of multimedia contents",
      "authors": [
        "H Joho",
        "J Staiano",
        "N Sebe",
        "J Jose"
      ],
      "year": "2011",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "10",
      "title": "Automatic speaker age and gender recognition using acoustic and prosodic level information fusion",
      "authors": [
        "M Li",
        "K Han",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Comp. Speech & Language"
    },
    {
      "citation_id": "11",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Speech & Audio Process"
    },
    {
      "citation_id": "12",
      "title": "Biometric security poses huge privacy risks",
      "year": "2013",
      "venue": "Biometric security poses huge privacy risks"
    },
    {
      "citation_id": "13",
      "title": "Brain waves for automatic biometricbased user recognition",
      "authors": [
        "P Campisi",
        "La Rocca"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Info Forensics & Security"
    },
    {
      "citation_id": "14",
      "title": "On the effectiveness of EEG signals as a source of biometric information",
      "authors": [
        "S Yang",
        "F Deravi"
      ],
      "year": "2012",
      "venue": "EST"
    },
    {
      "citation_id": "15",
      "title": "Designing a brain-computer interface controlled video-game using consumer grade EEG hardware",
      "authors": [
        "M Van Vliet",
        "A Robben",
        "N Chumerin",
        "N Manyakov",
        "A Combaz",
        "M Hulle"
      ],
      "year": "2012",
      "venue": "BRC"
    },
    {
      "citation_id": "16",
      "title": "Mind-an EEG neurofeedback multitasking game",
      "authors": [
        "W Lim",
        "O Sourina",
        "L Wang"
      ],
      "year": "2015",
      "venue": "Cyberworlds"
    },
    {
      "citation_id": "17",
      "title": "Is the low-cost eyetribe eye tracker any good for research?",
      "authors": [
        "E Dalmaijer"
      ],
      "year": "2014",
      "venue": "PeerJ PrePrints, Tech. Rep"
    },
    {
      "citation_id": "18",
      "title": "Sex differences in the perception of affective facial expressions: Do men really lack emotional sensitivity?",
      "authors": [
        "B Montagne",
        "R Kessels",
        "E Frigerio",
        "E De Haan",
        "D Perrett"
      ],
      "year": "2005",
      "venue": "Cognitive Processing"
    },
    {
      "citation_id": "19",
      "title": "Gender differences in judgments of multiple emotions from facial expressions",
      "authors": [
        "J Hall",
        "D Matsumoto"
      ],
      "year": "2004",
      "venue": "Emotion"
    },
    {
      "citation_id": "20",
      "title": "Discovering gender differences in facial emotion recognition via implicit behavioral cues",
      "authors": [
        "M Bilalpur",
        "S Kia",
        "T.-S Chua",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "Discovering gender differences in facial emotion recognition via implicit behavioral cues",
      "arxiv": "arXiv:1708.08729"
    },
    {
      "citation_id": "21",
      "title": "Predicting the valence of a scene from observers eye movements",
      "authors": [
        "H Tavakoli",
        "A Atyabi",
        "A Rantanen",
        "S Laukka",
        "S Nefti-Meziani",
        "J Heikkil"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "22",
      "title": "Visual scanning in the recognition of facial affect: Is there an observer sex difference?",
      "authors": [
        "S Vassallo",
        "S Cooper",
        "J Douglas"
      ],
      "year": "2009",
      "venue": "Journal of Vision"
    },
    {
      "citation_id": "23",
      "title": "Identification of emotional facial expressions: Effects of expression, intensity, and sex on eye gaze",
      "authors": [
        "L Wells",
        "S Gillespie",
        "P Rotshtein"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "24",
      "title": "Emotion modulates eye movement patterns and subsequent memory for the gist and details of movie scenes",
      "authors": [
        "R Subramanian",
        "D Shankar",
        "N Sebe",
        "D Melcher"
      ],
      "year": "2014",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "25",
      "title": "Eye movements during emotion recognition in faces",
      "authors": [
        "M Schurgin",
        "J Nelson",
        "S Iida",
        "H Ohira",
        "J Chiao",
        "S Franconeri"
      ],
      "year": "2014",
      "venue": "Journal of Vision"
    },
    {
      "citation_id": "26",
      "title": "Gender and emotion recognition with implicit user signals",
      "authors": [
        "M Bilalpur",
        "S Kia",
        "M Chawla",
        "T.-S Chua",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "Int'l Conference on Multimodal Interaction"
    },
    {
      "citation_id": "27",
      "title": "Multimodal emotion recognition using EEG and eye tracking data",
      "authors": [
        "W.-L Zheng",
        "B.-N Dong",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "EMBC"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion recognition using multimodal deep learning",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Multimodal emotion recognition using multimodal deep learning",
      "arxiv": "arXiv:1602.08225"
    },
    {
      "citation_id": "29",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Ühl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdan",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "ASCERTAIN: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Human gender classification: A review",
      "authors": [
        "Y Wu",
        "Y Zhuang",
        "X Long",
        "F Lin",
        "W Xu"
      ],
      "year": "2015",
      "venue": "Human gender classification: A review",
      "arxiv": "arXiv:1507.05122"
    },
    {
      "citation_id": "33",
      "title": "Eeg-based evaluation of cognitive workload induced by acoustic parameters for data sonification",
      "authors": [
        "M Bilalpur",
        "M Kankanhalli",
        "S Winkler",
        "R Subramanian"
      ],
      "year": "2018",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "34",
      "title": "Eye movements during everyday behavior predict personality traits",
      "authors": [
        "S Hoppe",
        "T Loetscher",
        "S Morey",
        "A Bulling"
      ],
      "year": "2018",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "35",
      "title": "Processing of masked and unmasked emotional faces under different attentional conditions: an Electrophysiological investigation",
      "authors": [
        "M Zotto",
        "A Pegna"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "36",
      "title": "Making computers look the way we look: exploiting visual attention for image understanding",
      "authors": [
        "H Katti",
        "R Subramanian",
        "M Kankanhalli",
        "N Sebe",
        "T.-S Chua",
        "K Ramakrishnan"
      ],
      "year": "2010",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Affective video content representation and modeling",
      "authors": [
        "A Hanjalic",
        "L.-Q Xu"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "38",
      "title": "Affective understanding in film",
      "authors": [
        "H Wang",
        "L.-F Cheong"
      ],
      "year": "2006",
      "venue": "IEEE Trans. Circ. Syst. Video Tech"
    },
    {
      "citation_id": "39",
      "title": "A probabilistic approach to people-centric photo selection and sequencing",
      "authors": [
        "V Vonikakis",
        "R Subramanian",
        "J Arnfred",
        "S Winkler"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Affect recognition in ads with application to computational advertising",
      "authors": [
        "A Shukla",
        "S Gullapuram",
        "H Katti",
        "K Yadati",
        "M Kankanhalli",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "Affect recognition in ads with application to computational advertising",
      "arxiv": "arXiv:1709.01683"
    },
    {
      "citation_id": "41",
      "title": "Eeg based emotion identification using unsupervised deep feature learning",
      "authors": [
        "X Li",
        "P Zhang",
        "D Song",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2015",
      "venue": "Workshop Neuro-Phys. Methods in IR"
    },
    {
      "citation_id": "42",
      "title": "EEG-based emotion classification using deep belief networks",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "Y Peng",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "ICME"
    },
    {
      "citation_id": "43",
      "title": "Eye movement strategies involved in face perception",
      "authors": [
        "G Walker-Smith",
        "A Gale",
        "J Findlay"
      ],
      "year": "1977",
      "venue": "Perception"
    },
    {
      "citation_id": "44",
      "title": "Transmitting and decoding facial expressions",
      "authors": [
        "M Smith",
        "G Cottrell",
        "F Gosselin",
        "P Schyns"
      ],
      "year": "2005",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "45",
      "title": "Can computers learn from humans to see better?: Inferring scene semantics from viewers' eye movements",
      "authors": [
        "R Subramanian",
        "V Yanulevskaya",
        "N Sebe"
      ],
      "year": "2011",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Angry, disgusted, or afraid?",
      "authors": [
        "H Aviezer",
        "R Hassin",
        "J Ryan",
        "C Grady",
        "J Susskind",
        "A Anderson",
        "M Moscovitch",
        "S Bentin"
      ],
      "year": "2008",
      "venue": "Psych. Science"
    },
    {
      "citation_id": "47",
      "title": "Categorical perception of facial expressions",
      "authors": [
        "N Etcoff",
        "J Magee"
      ],
      "year": "1992",
      "venue": "Cognition"
    },
    {
      "citation_id": "48",
      "title": "What's good for the goose is not good for the gander: age and gender differences in scanning emotion faces",
      "authors": [
        "S Sullivan",
        "A Campbell",
        "S Hutton",
        "T Ruffman"
      ],
      "year": "2015",
      "venue": "Journals of Gerontology, Series B: Psychological Sciences and Social Sciences"
    },
    {
      "citation_id": "49",
      "title": "Emotion recognition: the role of facial movement and the relative importance of upper and lower areas of the face",
      "authors": [
        "J Bassili"
      ],
      "year": "1979",
      "venue": "Journal Pers. Social Psych"
    },
    {
      "citation_id": "50",
      "title": "Sex differences in scanning faces: Does attention to the eyes explain female superiority in facial expression recognition?",
      "authors": [
        "J Hall",
        "S Hutton",
        "M Morgan"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "51",
      "title": "Sex, sexual orientation, and identification of positive and negative facial affect",
      "authors": [
        "Q Rahman",
        "G Wilson",
        "S Abrahams"
      ],
      "year": "2004",
      "venue": "Brain and Cognition"
    },
    {
      "citation_id": "52",
      "title": "Are females more responsive to emotional stimuli? a neurophysiological study across arousal and valence dimensions",
      "authors": [
        "C Lithari",
        "C Frantzidis",
        "C Papadelis",
        "A Vivas",
        "M Klados",
        "C Kourtidou-Papadeli",
        "C Pappas",
        "A Ioannides",
        "P Bamidis"
      ],
      "year": "2010",
      "venue": "Brain Topography"
    },
    {
      "citation_id": "53",
      "title": "Age and gender classification using eeg paralinguistic features",
      "authors": [
        "P Nguyen",
        "D Tran",
        "X Huang",
        "W Ma"
      ],
      "year": "2013",
      "venue": "NER"
    },
    {
      "citation_id": "54",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "55",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Openface: an open source facial behavior analysis toolkit"
    },
    {
      "citation_id": "56",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "C Muhl",
        "B Allison",
        "A Nijholt",
        "G Chanel"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "57",
      "title": "Error related negativity in observing interactive tasks",
      "authors": [
        "C Vi",
        "I Jamil",
        "D Coyle",
        "S Subramanian"
      ],
      "year": "2014",
      "venue": "SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "58",
      "title": "Eyemmv toolbox: An eye movement post-analysis tool based on a two-step spatial dispersion threshold for fixation identification",
      "authors": [
        "V Krassanakis",
        "V Filippakopoulou",
        "B Nakos"
      ],
      "year": "2014",
      "venue": "Journal of Eye Movement Research"
    },
    {
      "citation_id": "59",
      "title": "Fusion of facial expressions and EEG for implicit affective tagging",
      "authors": [
        "S Koelstra",
        "I Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "60",
      "title": "Deep learning for automatic stereotypical motor movement detection using wearable sensors in autism spectrum disorders",
      "authors": [
        "N Rad",
        "S Kia",
        "C Zarbo",
        "T Van Laarhoven",
        "G Jurman",
        "P Venuti",
        "E Marchiori",
        "C Furlanello"
      ],
      "year": "2018",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Batch Normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch Normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "citation_id": "62",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "63",
      "title": "Keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "64",
      "title": "Robust real-time face detection",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2004",
      "venue": "Int. J. Comput. Vision",
      "doi": "10.1023/B:VISI.0000013087.49260.fb"
    },
    {
      "citation_id": "65",
      "title": "Feature selection using AdaBoost for face expression recognition",
      "authors": [
        "P Silapachote",
        "D Karuppiah",
        "A Hanson"
      ],
      "year": "2005",
      "venue": "Feature selection using AdaBoost for face expression recognition"
    },
    {
      "citation_id": "66",
      "title": "Multi-class adaboost",
      "authors": [
        "T Hastie",
        "S Rosset",
        "J Zhu",
        "H Zou"
      ],
      "year": "2009",
      "venue": "Statistics and its Interface"
    },
    {
      "citation_id": "67",
      "title": "Gender and emotion recognition with implicit user signals",
      "authors": [
        "M Bilalpur",
        "S Kia",
        "M Chawla",
        "T.-S Chua",
        "R Subramanian"
      ],
      "year": "2017",
      "venue": "Gender and emotion recognition with implicit user signals"
    },
    {
      "citation_id": "68",
      "title": "A developmental examination of gender differences in brain engagement during evaluation of threat",
      "authors": [
        "E Mcclure",
        "C Monk",
        "E Nelson",
        "E Zarahn",
        "E Leibenluft",
        "R Bilder",
        "D Charney",
        "M Ernst",
        "D Pine"
      ],
      "year": "2004",
      "venue": "Biological psychiatry"
    },
    {
      "citation_id": "69",
      "title": "Cerebral hemodynamics during discrimination of prosodic and semantic emotion in speech studied by transcranial doppler ultrasonography",
      "authors": [
        "G Vingerhoets",
        "C Berckmoes",
        "N Stroobant"
      ],
      "year": "2003",
      "venue": "Neuropsychology"
    },
    {
      "citation_id": "70",
      "title": "Affective computing and the impact of gender and age",
      "authors": [
        "S Rukavina",
        "S Gruss",
        "H Hoffmann",
        "J.-W Tan",
        "S Walter",
        "H Traue"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "71",
      "title": "Affective sensors, privacy, and ethical contracts",
      "authors": [
        "C Reynolds",
        "R Picard"
      ],
      "year": "2004",
      "venue": "CHI 04 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "72",
      "title": "Gender differences in emotional response: Inconsistency between experience and expressivity",
      "authors": [
        "Y Deng",
        "L Chang",
        "M Yang",
        "M Huo",
        "R Zhou"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "73",
      "title": "Experimental methods for inducing basic emotions: A qualitative review",
      "authors": [
        "E Siedlecka",
        "T Denson"
      ],
      "year": "2019",
      "venue": "Emotion Review"
    }
  ]
}