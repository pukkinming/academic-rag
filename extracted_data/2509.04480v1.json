{
  "paper_id": "2509.04480v1",
  "title": "Discrete Prompt Tuning Via Recursive Utilization Of Black-Box Multimodal Large Language Model For Personalized Visual Emotion Recognition",
  "published": "2025-08-30T23:47:17Z",
  "authors": [
    "Ryo Takahashi",
    "Naoki Saito",
    "Keisuke Maeda",
    "Takahiro Ogawa",
    "Miki Haseyama"
  ],
  "keywords": [
    "Visual emotion recognition",
    "multimodal large language model",
    "discrete prompt tuning."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Visual Emotion Recognition (VER) is an important research topic due to its wide range of applications, including opinion mining and advertisement design. Extending this capability to recognize emotions at the individual level further broadens its potential applications. Recently, Multimodal Large Language Models (MLLMs) have attracted increasing attention and demonstrated performance comparable to that of conventional VER methods. However, MLLMs are trained on large and diverse datasets containing general opinions, which causes them to favor majority viewpoints and familiar patterns. This tendency limits their performance in a personalized VER, which is crucial for practical and real-world applications, and indicates a key area for improvement. To address this limitation, the proposed method employs discrete prompt tuning inspired by the process of humans' prompt engineering to adapt the VER task to each individual. Our method selects the best natural language representation from the generated prompts and uses it to update the prompt for the realization of accurate personalized VER. INDEX TERMS Visual emotion recognition, multimodal large language model, discrete prompt tuning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "R ECOGNIZING human emotions is crucial for comput- ers to respond appropriately to users' questions and requests. In particular, research on human-induced emotion recognition by visual stimuli is typically addressed as Visual Emotion Recognition (VER)  [1] -  [4] . VER is expected to have various applications, including advertisement  [5] , education  [6] , mental health  [7] , image retrieval  [8] ,  [9] , and product recommendation  [10] . Although VER is a challenging research task because it deals with ambiguous information such as emotions, various methods have been proposed because of their vast applicability  [11] -  [13] .\n\nRecently, Multimodal Large Language Models (MLLMs) pretrained with numerous images and texts have demonstrated high performance on various tasks  [14] ,  [15] . MLLMs are easy to use and can be applied to various tasks, which have been the subject of many studies. MLLMs can understand the relationship between different modalities, and various responses can be obtained by changing the prompt. In the VER, many MLLM-based VER methods have been proposed, achieving high performance  [16] -  [21] .\n\nEmotional responses elicited by visual stimuli vary across individuals. For example, an image of a roller coaster may invoke feelings of ''excitement,'' in some viewers but \"fear\" in others. To apply VER technology to practical, real-world services, personalization of VER systems is essential. However, existing MLLM-based VER methods face challenges in generating recognition outcomes that reflect individual variability in emotional elicitation. This is because MLLMs are trained on a large number of texts and images, which contain extensive general information and opinions; thus, they tend to learn majority opinions and common patterns.  [22] -  [24] .\n\nFine-tuning is generally considered necessary to adapt an MLLM to a specific task. However, high-accuracy MLLMs tend to have numerous parameters, which poses the challenge of requiring substantial computational resources for finetuning. To address this issue, prompt tuning is a potential approach for adapting MLLMs to specific tasks such as personalized VER. Unlike fine-tuning, prompt tuning does not modify the parameters of the MLLM. Instead, it adjusts the input prompts using techniques such as gradient-based optimization or soft prompt tuning so that the MLLM can perform effectively on specific tasks. Gradient-based prompt tuning  [25]  is typically applied to white-box models, because these models allow access to model parameters and loss gradients. However, recent MLLMs with superior reasoning capabilities, such as Gemini 1  and GPT-4 omni (GPT-4o)  2   2  , are frequently deployed as black-box models, where access to parameters and loss gradients is restricted. This is primarily due to commercial, ethical, legal, security, and other concerns surrounding the release of model internals. In prompt-based learning, an extensively used technique is ''soft prompts''  [26] -  [28] . Soft prompts are learnable parameters represented as continuous vector embeddings that are prepended to the input of a pretrained language model. They are effective for improving model performance. However, they are numerical and lack explicit semantics, making it difficult for humans to understand their meaning, particularly when interpreting decisions involving ambiguous emotions. In addition, training with soft prompts requires access to gradient information. This poses a challenge in scenarios where users cannot access or modify the model internals, such as when interacting with models through restricted application programming interfaces or black-box systems. To address these issues, researchers have proposed ''discrete prompt tuning'' approaches  [29] -  [31] . Discrete prompts comprise natural language tokens, which makes them more interpretable for humans and allows them to be used without relying on gradients. Therefore, there is a potential to adapt MLLMs to VER tasks by employing discrete prompt tuning, while maintaining interpretability and the adaptability of black-box models.\n\nIn this paper, we propose a personalized VER method that introduces discrete prompt tuning via recursive utilization of a black-box MLLM. We employ an MLLM capable of processing both images and text to perform the VER task, and use an LLM for prompt generation and refinement. Specifically, the proposed method recognizes VER by inputting the target image and a discrete prompt into an MLLM. The discrete prompt is obtained via a prompt tuning approach that incorporates iterative generation and evaluation of discrete prompts inspired by the human prompt engineering process. In the prompt generation phase, the LLM generates multiple prompts in natural language suitable for VER. In the prompt evaluation phase, each generated prompt is input into the MLLM alongside training images for which the evoked emotions are known in advance. The recognition results for these images are then obtained, and the accuracy of these results is used as the performance score for each prompt. The proposed method then feeds the LLM with several high-and lowperforming prompts and accuracy scores, which are obtained when these prompts are used to perform the VER task. Based on the information provided to the LLM, it generates new prompts that are expected to improve VER performance. The self-correction capability of the LLM enables the discovery of better prompts. This process of prompt generation and evaluation is repeated to obtain prompts that are likely to enhance the accuracy of VER by the MLLM. The proposed method leverages discrete prompt tuning to derive an optimal prompt for the VER for each user, which is expected to elicit knowledge related to VER in an MLLM according to the user's characteristics. The discrete prompt tuning in the proposed method does not require changing the parameters of the MLLM and accessing its loss gradients; thus, it can be applied to both white-box and black-box MLLMs. This makes the proposed method less restrictive in its application than previously proposed MLLM-based VER methods. Finally, the proposed method determines the final results through a majority vote. The MLLM processes the target images using several optimal prompts generated via discrete prompt tuning, and the most frequent label among these outputs is selected.\n\nIn summary, the main contributions of this paper are as follows.\n\n• We introduce a discrete prompt tuning approach for a black-box MLLM inspired by humans' prompt engineering to VER. • We use a discrete prompt tuning approach to improve personalized VER performance. • We achieve accurate VER by introducing a simple merging process of the VER results obtained from the MLLM by inputting several optimized prompts.\n\nThis paper is organized as follows. Section II describes related studies on VER and prompt-based learning. Section III explains the proposed method for personalized VER via discrete prompt tuning. In addition, experimental results are reported in Section IV. Finally, Section V concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "In this section, we explain related studies. The proposed method focuses on a VER task and a prompt-based learning approach for an MLLM. This section is organized as follows. Subsection II-A introduces previous VER studies. Subsection II-B explains prompt-based learning approaches for MLLMs and their limitations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Ver",
      "text": "With the advancement of deep learning techniques, VER has significantly progressed. In particular, Convolutional Neural Network (CNN)-based approaches have emerged and significantly improved in recognition performance  [32] ,  [33] . For example, Chen et al.  [34]  proposed DeepSentiBank, which classifies a visual sentiment via a deep CNN. This method automatically extracted adjective-noun pairs and was trained on approximately one million images. DeepSentiBank has several limitations, such as noisy and weak labels and domain adaptation challenges. You et al.  [35]  introduced PCNN, which combines progressive training and transfer learning to improve performance to deal with the limitations of DeepSen-tiBank. In addition, She et al.  [36]  proposed WSCNet, a weakly supervised coupled network that integrates local regions evoking emotions with global image features. Focusing on the hierarchical structure of emotions, Xu et al.  [2]  presented MDAN, a multilevel dependent attention network built on CNNs, achieving fine-grained emotion classification via a hierarchy-aware learning strategy.\n\nWith the recent advancements in multimodal foundation models, various approaches leveraging these models have been proposed for VER, achieving recognition accuracies that exceed those of conventional methods based on CNNs and similar architectures  [37] -  [40] . Among various foundation models, CLIP  [41] , which learns from large-scale image-text pairs to align visual and linguistic representations in a common embedding space, has become a common backbone. For instance, Deng et al.  [42]  proposed a CLIP-based VER method that dynamically composes diverse prompts according to image content and emotion category, significantly outperforming fixed-prompt VER baselines. Then, Deng et al.  [37]  proposed a generation approach of knowledgerich prompts by incorporating emotion words and entitylevel information extracted from images. This approach enables more sophisticated affective understanding by leveraging linguistic structures beyond conventional label-based supervision. In addition, MLLM-based VER methods such as GPT-4o and Gemini have been proposed, and these methods have demonstrated high recognition performance even in zero-shot settings for VER tasks  [17] ,  [18] .\n\nHowever, MLLM-based VER methods cannot consider the individual differences in emotional elicitation for the realization of personalized VER. This is because MLLMs are trained on large and diverse datasets containing general opinions, causing them to favor majority viewpoints and familiar patterns. To overcome this limitation, a promising strategy is to adjust the parameters of MLLMs to enable specialization in personalized VER for each target user, i.e., a fine-tuning approach. However, fine-tuning these parameters requires a large number of training samples, and it is challenging to collect sufficient data specific to a target user's VER. Therefore, achieving effective fine-tuning for personalized VER remains difficult. Our approach focuses on prompt tuning, which aims to improve VER performance by optimizing the instructions provided to the MLLM, rather than fine-tuning the model parameters. Unlike conventional fine-tuning, prompt tuning can achieve task-specific performance even with a limited amount of training data, making it a promising approach for realizing personalized VER using MLLMs. The following subsection explains related studies on prompt tuning of MLLMs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Prompt Tuning Of Mllms",
      "text": "In recent years, many researchers have proposed prompt tuning methods as efficient alternatives to full fine-tuning, improving task performance while substantially reducing computational cost. A notable gradient-based approach is P-Tuning, proposed by Liu et al.  [25] , which uses continuous trainable embeddings instead of discrete manually crafted prompts and optimizes them via backpropagation. Similarly, Lester et al.  [28]  introduced prompt tuning, which freezes the language model parameters and optimizes only a small set of task-specific soft prompts. To further leverage gradientbased optimization for vision-language tasks, models such as InstructBLIP  [43]  and EmoVIT  [3]  have been proposed to align prompts with supervised instruction datasets.\n\nHowever, the emergence of powerful black-box models, which do not reveal internal parameters or gradients, has made gradient-based tuning approaches impractical in many real-world scenarios. In response, researchers have developed gradient-free prompt optimization techniques that function effectively in black-box settings. For example, Pryzant et al.  [31]  proposed ProTeGi, a method in which the language model generates natural language feedback to identify prompt weaknesses, thereby guiding its iterative refinement without relying on gradient signals. Similarly, Cheng et al.  [44]  introduced black-box prompt optimization, which trains a sequence-to-sequence model to rewrite prompts by learning from examples of both effective and ineffective responses. More recently, Liu et al.  [29]  proposed a dialoguebased framework that employs language models as interactive prompt engineers.\n\nWe introduce a prompt tuning approach applicable to black-box models for VER. The proposed method enables personalized VER by obtaining optimal prompts tailored to each individual.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Personalized Ver Via Discrete Prompt Tuning",
      "text": "In this section, we describe the proposed method. The proposed method inputs images and user-specific discrete prompts optimized for emotion recognition into an MLLM. Our method performs discrete prompt tuning based on the literature  [29]  inspired by the typical workflow of humans' prompt engineering process. We recognize the personalized emotion from the target image using the MLLM, inputting the optimized prompt. Figure  1  shows an overview of the proposed method. The precise processes of the proposed method are shown in Algorithm 1. The prompt tuning approach in the proposed method comprises the following iterations: (A) creating initial prompts for VER; (B) evaluating the recognition performance of prompts; (C) creating new modified prompts based on the prompt performance evaluation results obtained by (B), and then, repeatedly performing (B) and (C); (D) selecting the prompt with the highest recognition performance as the final discrete prompt. The proposed method can generate discrete prompts that are suitable for the VER of each user using the above iterative workflow. In addition, using multiple suitable prompts, the proposed method performs VER on the target image several times and obtains multiple recognition results. Then, through majority voting from these results, one emotion label is selected as the final recognition result. Consequently, stable and highly accurate VER becomes feasible using the proposed method.\n\nThis section is organized as follows. Subsection III-A ex-FIGURE  1 : Overview of the proposed method. The proposed method comprises four processes: initial prompt generation, evaluation of recognition performance, modified prompt generation, and recognition by an MLLM. The proposed method iterates these processes and obtains the final recognition result.\n\nplains the generation approach of initial prompts. Subsection III-B describes the recognition performance evaluation of the generated prompts. Subsection III-C presents the modification scheme of the generated prompts for personalized VER. Subsection III-D explains the recognition approach using the final discrete prompts.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Initial Prompt Generation",
      "text": "In this subsection, we explain the generation approach of initial prompts. The proposed method constructs initial prompts to obtain suitable discrete prompts for personalized VER using an MLLM. Specifically, an LLM is used to generate the initial prompt set\n\nbeing the number of initial prompts) as follows:\n\nwhere LLM(•) denotes the LLM used for generating initial prompts and t init denotes the natural language sentence given to the LLM as a prompt to generate the initial prompts for VER. The proposed method uses the sentence t init shown in TABLE 1. To explore diverse instructional strategies, the LLM is guided to generate various prompts. Based on   Get new prompts P mod ← LLM(t mod (P pos , P neg )) these initial prompts, the proposed method generates suitable prompts for personalized VER through the following sequential steps.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Evaluation Of Recognition Performance",
      "text": "In this subsection, we describe the VER performance evaluation of the generated prompts. The proposed method evaluates the recognition performance of the generated prompts by measuring the recognition accuracy as a basis for generating prompts. Let S L = {x l , y l } (l = 1, 2, . . . , L; L being the number of training images) be the l-th training image for personalized VER, where x l denotes the l-th training image.\n\nIn addition, y l denotes the emotion label evoked when the target user sees the image x l . The proposed method calculates the recognition accuracy ACC(p) defined in the following equation for VER using an MLLM with a prompt p.\n\nTABLE 1: Sentence t init using the generation of initial prompts in the proposed method.\n\nPlease provide me with N diverse prompts that are suitable for input into the MLLM. The prompts should be diverse, such as detailed and straightforward, and should give the LLM a role. The prompts should make the LLM classify the emotions that people evoke when they see the image. The emotion label should prompt the LLM to choose one of the following emotions: {Emotion labels} .\n\nHere are my requirements:\n\n-Please only reply with the template.\n\n-Each template should start with '-' in a separate line.\n\n-Ensure that the output is in a clear and consistent format.\n\nTABLE 2: Prompt sentence t mod (P pos , P neg ) using the modification of prompts in the proposed method.\n\nI have two lists of templates: one with good templates and the other with bad templates. Based on the characteristics that make a template good or bad, please provide T better templates. Here is the list of good templates with their accuracies: Top-k: [prompts with top-k accuracy] Here is the list of bad templates with their accuracies: Worst-k: [prompts with worst-k accuracy] Here are my requirements:\n\n-Please only reply with the template.\n\n-Each template should start with '-' in a separate line.\n\n-Ensure that the output is in a clear and consistent format.\n\nwhere r train l (p) denotes the recognition result of the l-th training image obtained using the MLLM with the prompt p. Based on the recognition accuracy, the proposed method modifies the prompts to achieve high-performance VER, as discussed in the next subsection.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Modified Prompt Generation",
      "text": "This subsection describes the procedure for generating modified prompts based on the recognition accuracy obtained in the previous subsection. The proposed method generates modified discrete prompts that achieve high-performance personalized VER using an MLLM. First, the proposed method uses top-k and bottom-k prompts to improve recognition accuracy. Let P pos and P neg be the sets of appropriate and inappropriate prompts, separately selected from the previously generated set of prompts P rank . The proposed method generates a new set of modified prompts P mod by inputting the prompt t mod (P pos , P neg ) to an LLM as follows:\n\nwhere t mod (P pos , P neg ) denotes the prompt with P pos and P neg . Specifically, the proposed method uses the natural language prompt t mod (P pos , P neg ) in TABLE  2 . Then, the proposed method obtains T modified prompts, which are added to the set P rank . Thus, the proposed method constructs modified prompts that facilitate more personalized VER using an MLLM. The modification information includes both appropriate and inappropriate prompts. The LLM refines prompt representations by increasing and decreasing their similarities to appropriate and inappropriate examples, respectively. These implicit gradients indicate the direction toward the optimal prompt for the user, thereby enabling the efficient generation of prompts that suit each user.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Personalized Ver By Mllm",
      "text": "In this subsection, we explain the recognition approach using an MLLM inputting the optimal prompts. The proposed method iteratively generates prompts and performs VER using them to identify optimal prompts. Specifically, the proposed method performs I 1 iterations of evaluating and modifying discrete prompts, and ultimately obtains the prompts with the highest accuracy using the training images. In addition, after repeating the acquisition of prompts via evaluation and modification processes, these processes are repeated I 2 times using the same initial prompt by assigning P init to P rank . Then, these processes are repeated I 3 times by generating new initial prompts P init . Note that the generated prompts are denoted as P all = {p all 1 , p all 2 , • • • , p all M } (M denoting the total number of prompts generated in all previous iterations). The proposed method obtains the optimal prompt p opt h (h = 1, 2, . . . , H ; H denoting the number of used prompts for the target) for the final VER task as follows:\n\nwhere the prompt p opt h is used to obtain the recognition result. In the proposed method, the recognition result r test h (h ∈ {1, 2, . . . , H }) for the target image I target using an MLLM is as follows:\n\nThe proposed method selects the optimal prompt p opt h+1 once again among the prompts P all excluding the prompt used P used , and performs VER. Multiple recognition results {r test 1 , r test 2 , • • • , r test H } are obtained by repeating this process. The emotion label obtained through the majority voting of these recognition results becomes the final recognition result. Consequently, the proposed method enables accurate and stable personalized VER.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Experimental Settings",
      "text": "In this experiment, we verify the effectiveness of the proposed method for personalized VER. This experiment evaluated the performance of the proposed method by classifying images based on the emotion labels assigned to each user. To evaluate personalized VER performance, we used the Affective Explanations (Affection) dataset  [45] , which comprises images and emotion labels elicited from each viewer. In the Affection dataset, each image was assigned eight types of emotion labels based on Mikel's wheel from the psychological model  [46] : amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. This experiment used images labeled with emotion labels from 15 users in the Affection dataset. These users were selected because they provided multiple images for each emotion label; individuals with either a small number of responses or an extremely large number of responses were excluded. Note that the average number of images used per user was 864, and the total number of used images was 12,964. In practical use cases, it is challenging to obtain numerous image-emotion label sets from a specific user for VER because of the considerable effort required  [47] ,  [48] . Therefore, this experiment assumed that emotion recognition would be performed using a few imageemotion label pairs as training data. Specifically, we used 30% of each user's image-emotion label pairs as training data and the remaining 70% as test data.\n\nIn this experiment, we employed the Large Language and Vision Assistant v1.6-Mistral-7B (LLaVA-v1.6-Mistral-7B)  [49]  as the MLLM to recognize emotions and GPT-4o as the LLM to facilitate discrete prompt tuning. In each iteration of the proposed method, this experiment used six initial prompts (N = 6) and generated five modified prompts (T = 5). In addition, the number of prompts used for each of the top and worst inputs for k was three. Furthermore, I 1 = 20, I 2 = 2, I 3 = 3, and H = 5.\n\nTo evaluate the effectiveness of the proposed method, its recognition performance was compared with that of three baseline methods (CMs 1-3) and four ablation study cases (CMs 4-7). CM1 is a CNN-based emotion recognition method  [36] , and CM2 employs Transformer-based recognition  [50] . A recognition method by an MLLM inputting a prompt proposed in the previous study  [18]  is used as CM3, and CM4 is a recognition method that uses an initial prompt. CM5 is a recognition method using a prompt modified in one iteration, i.e., I 2 = I 3 = 1, and CM6 is a recognition method using discrete prompts tuned to another user who is not the target user. Additionally, CM7 is a recognition method that uses a single modified prompt without majority voting.\n\nThree types of evaluation indices were used to evaluate VER performance: recognition accuracy, Emotion Confusion Confidence (ECC), and Emotional Misclassification Confidence (EMC). ECC and EMC are metrics proposed in  [51]  for evaluating the performance of an emotion recognition method, utilizing the concept of emotional distance to mitigate misrecognition based on Mikel's wheel from psychological theory. The three evaluation indices are defined as follows:\n\nwhere N test and N correct denote the number of test images and correctly recognized images, respectively, labels denote a set of the target emotion labels and S αβ represents the number of images recognized from the correct emotion label α to the label β. In addition, W αβ denotes the emotional distance, which is defined as follows:\n\nwhere dist(α, β) denotes the number of steps on Mikel's wheel, and C represents a constant that scales the importance of polarity, and it is set to four to ensure that thedistance between emotions of the same polarity is always smaller than that of emotions with different polarities following the method established in  [51] . C e (p) denotes a set of target emotion labels with the same polarity. Note that VER task using an MLLM occasionally outputs non-targeted labels, such as ''happy'' and ''sad,'' which are considered incorrect results in this experiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Quantitative Experimental Results",
      "text": "TABLE  3  shows the mean and standard deviation of the accuracy, ECC, and EMC values calculated from the VER results performed for each user using the proposed and comparative methods. These results demonstrate that the proposed method achieves the highest VER performance compared with the comparative methods. The proposed method is also effective in scenarios in which the MLLM is a black box model and does not allow for soft prompt injection. Compared with CM1 and CM2, both the baseline methods using MLLM and the proposed method outperform the CNN-and Transformerbased methods in terms of VER performance. These results confirm that the proposed method can realize a highperformance VER compared with methods based on conventional deep learning architectures. The linguistic capabilities of MLLMs are assumed to contribute to the recognition of not only objects in images but also the relationships between objects and people, thereby enhancing VER.\n\nCompared with CMs 3-7, the proposed method, which applied prompt tuning to the MLLM, achieved the highest VER performance in terms of accuracy and ECC. These results demonstrate the importance of tailoring prompts to the MLLM. In contrast, CM3 achieved the highest EMC score compared with the proposed method. The EMC metric evaluates the plausibility of the predicted labels for images that were not correctly classified. Some of these images may have features that make it easy to infer emotional labels. This may Compared with CM4, the discrete prompt tuning approach of the proposed method improves the performance by generating suitable prompts for personalized VER. Figure  2  shows the transition of the recognition accuracy of the training samples by modifying the prompts. This transition confirms that the performance improvement in the proposed method is not simply due to the selection of high-accuracy prompts from a set of randomly generated candidates but rather is attributable to the introduction of discrete prompt tuning. As the number of iterations increases, the generated prompts evolve to incorporate features that are more suitable for each user, thereby improving accuracy. Because the proposed method outperforms CM5, we can conclude that the iterative process of repeating the initial prompts achieves high-performance recognition. This is because the proposed method generates various prompts that are not fixed to a single pattern. Furthermore, compared with CM6, the proposed method generates suitable prompts for each user's emotion recognition. Therefore, we can confirm that the prompts generated by the proposed method are personalized, rather than generic for VER users in general. Finally, a comparison of the emotion recognition results of the proposed method and CM7 verifies that the majority voting approach effectively obtains the final recognition results. Using the majority voting scheme, the proposed method obtains the final recognition results by considering various prompts. Consequently, the influence of prediction errors and outliers by multiple prompts is averaged out, enabling the proposed method to derive stable and robust recognition results. The proposed approach ultimately results in improved recognition accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Qualitative Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Table 4",
      "text": "shows the example prompts with the highest recognition accuracy in the training samples for different numbers of iterations. prompt (i) is a simple command that only performs a VER task. In contrast, prompt (ii) indicates a specific role, and prompt (iii) follows a valid prompt representation for VER. This indicates that in the modification process, the LLM analyzes the valid parts of the prompts that are highly accurate and generates prompts suitable for the user. Finally, the use of prompt (iv) in the proposed method results in a more unique role for the model. From the recognition accuracy obtained using prompt (iv), we can confirm that this unique text representation enables the MLLM to extract emotional knowledge tailored to the user.\n\nFigure  3  shows examples of images, ground truth emotion labels, and recognition results obtained by the proposed method and ablation study cases: CMs 4-7. In image 1, compared with CM4 using the initial prompt, the proposed and other comparative methods recognized detailed information, such as facial expressions in person. A reason for this is that methods other than CM4 modify the prompts to recognize more advanced information for VER. In image 2, only the proposed method classified the image correctly. Specifically, compared with CM5, which included a majority voting approach and discrete prompt tuning, a more accurate emotion recognition was achieved using the proposed method. Images 3 and 4 show different emotion labels classified by different users. Many of the comparative methods are not well personalized to the user and uniformly classify images. CM4 outputs an emotion other than the target emotion for user 5 because the prompt fails to convey the task correctly. In contrast, the proposed method selects emotion labels tailored to each user. These results confirm that the prompt used in the proposed method enables advanced consideration of the same object based on various elements of the image, not only the object. Therefore, these examples demonstrate that discrete prompt tuning can adapt MLLM to personalized VER.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Limitations",
      "text": "The proposed method still has several limitations. Figure  4  shows the confusion matrices for the proposed method and CM4. Although the accuracy improves for most emotion labels, the performance for anger does not exhibit a noticeable improvement. As observed from the predicted emotion labels where the ground truth is anger, the results are extensively scattered across different categories. One possible reason is the class imbalance problem, because the number of anger images in the dataset is relatively small, resulting in biased learning and insufficient generalization. In future studies, it is necessary to design a model that can better leverage the knowledge of MLLMs to address class imbalance and achieve robust performance even with limited training samples.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study proposes a personalized VER method based on the discrete prompt tuning of an MLLM. The proposed method enables the generation of user-specific discrete prompts that can be seamlessly applied to various black-box MLLMs.\n\nThe experimental results demonstrate that these personalized prompts significantly improve VER performance. Notably, because the proposed method does not require access to model parameters or gradient information, it allows for the flexible integration of state-of-the-art, high-performance MLLMs without the need for additional fine-tuning. This approach not only enhances the adaptability and scalability of personalized VER systems but also opens up new possibilities for leveraging rapidly advancing MLLM technologies in diverse real-world applications.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an overview of the pro-",
      "page": 3
    },
    {
      "caption": "Figure 1: Overview of the proposed method. The proposed",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the",
      "page": 7
    },
    {
      "caption": "Figure 2: Transition of recognition accuracy of training",
      "page": 7
    },
    {
      "caption": "Figure 3: shows examples of images, ground truth emo-",
      "page": 7
    },
    {
      "caption": "Figure 3: Examples of target images and their VER results by proposed and comparative methods.",
      "page": 8
    },
    {
      "caption": "Figure 4: shows the confusion matrices for the proposed method and",
      "page": 8
    },
    {
      "caption": "Figure 4: Confusion matrices of VER results obtained from proposed method and CM4.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: To explore diverse instructional strategies,",
      "page": 4
    },
    {
      "caption": "Table 1: Sentence tinit using the generation of initial",
      "page": 5
    },
    {
      "caption": "Table 2: Prompt sentence tmod(Ppos, Pneg) using the modi-",
      "page": 5
    },
    {
      "caption": "Table 2: Then, the proposed",
      "page": 5
    },
    {
      "caption": "Table 3: shows the mean and standard deviation of the accu-",
      "page": 6
    },
    {
      "caption": "Table 3: Accuracy, ECC, and EMC of VER results obtained using proposed and comparative methods.",
      "page": 7
    },
    {
      "caption": "Table 4: shows the example prompts with the highest recog-",
      "page": 7
    },
    {
      "caption": "Table 4: Optimal prompts for each iteration by proposed method.",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Dependency exploitation: a unified cnn-rnn approach for visual emotion recognition",
      "authors": [
        "X Zhu",
        "L Li",
        "W Zhang",
        "T Rao",
        "M Xu",
        "Q Huang",
        "D Xu"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "MDAN: Multi-level dependent attention network for visual emotion analysis",
      "authors": [
        "L Xu",
        "Z Wang",
        "B Wu",
        "S Lui"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "H Xie",
        "C.-J Peng",
        "Y.-W Tseng",
        "H.-J Chen",
        "C.-F Hsu",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Learning class prototypes for visual emotion recognition",
      "authors": [
        "J Zhu",
        "S Zhao",
        "J Jiang",
        "Z Xu",
        "W Tang",
        "H Yao"
      ],
      "year": "2025",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "How to capture the heart? reviewing 20 years of emotion measurement in advertising",
      "authors": [
        "K Poels",
        "S Dewitte"
      ],
      "year": "2006",
      "venue": "Journal of Advertising Research"
    },
    {
      "citation_id": "6",
      "title": "From mining affective states to mining facial keypoint data: The quest towards personalized feedback,'' in International Design Engineering Technical Conferences and Computers and Information in Engineering Conference",
      "authors": [
        "C Lopez",
        "C Tucker"
      ],
      "year": "2017",
      "venue": "From mining affective states to mining facial keypoint data: The quest towards personalized feedback,'' in International Design Engineering Technical Conferences and Computers and Information in Engineering Conference"
    },
    {
      "citation_id": "7",
      "title": "What twitter profile and posted images reveal about depression and anxiety",
      "authors": [
        "S Guntuku",
        "D Preotiuc-Pietro",
        "J Eichstaedt",
        "L Ungar"
      ],
      "year": "2019",
      "venue": "Proceedings of the international AAAI conference on web and social media"
    },
    {
      "citation_id": "8",
      "title": "Deep multimodal learning for affective analysis and retrieval",
      "authors": [
        "L Pang",
        "S Zhu",
        "C.-W Ngo"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Who likes what and, why?'insights into modeling users' personality based on image 'likes",
      "authors": [
        "S Guntuku",
        "J Zhou",
        "S Roy",
        "W Lin",
        "I Tsang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "An intelligent recommendation system using gaze and emotion detection",
      "authors": [
        "S Jaiswal",
        "S Virmani",
        "V Sethi",
        "K De",
        "P Roy"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "11",
      "title": "Zero-shot vi-sual sentiment prediction via cross-domain knowledge distillation",
      "authors": [
        "Y Moroto",
        "Y Ye",
        "K Maeda",
        "T Ogawa",
        "M Haseyama"
      ],
      "year": "2023",
      "venue": "IEEE Open Journal of Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Building Emotional Machines: Recognizing image emotions through deep neural networks",
      "authors": [
        "H.-R Kim",
        "Y.-S Kim",
        "S Kim",
        "I.-K Lee"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "A hierarchical CNN-RNN approach for visual emotion classification",
      "authors": [
        "L Li",
        "X Zhu",
        "Y Hao",
        "S Wang",
        "X Gao",
        "Q Huang"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications"
    },
    {
      "citation_id": "14",
      "title": "Can GPT-4v (ision) serve medical applications? case studies on GPT-4v for multimodal medical diagnosis",
      "authors": [
        "C Wu",
        "J Lei",
        "Q Zheng",
        "W Zhao",
        "W Lin",
        "X Zhang",
        "X Zhou",
        "Z Zhao",
        "Y Zhang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Can GPT-4v (ision) serve medical applications? case studies on GPT-4v for multimodal medical diagnosis",
      "arxiv": "arXiv:2310.09909"
    },
    {
      "citation_id": "15",
      "title": "MM-Narrator: Narrating long-form videos with multimodal in-context learning",
      "authors": [
        "C Zhang",
        "K Lin",
        "Z Yang",
        "J Wang",
        "L Li",
        "C.-C Lin",
        "Z Liu",
        "L Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Disturbing image detection using LMMelicited emotion embeddings",
      "authors": [
        "M Tzelepi",
        "V Mezaris"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE International Conference on Image Processing Challenges and Workshops"
    },
    {
      "citation_id": "17",
      "title": "Vision-enabled large language and deep learning models for image-based emotion recognition",
      "authors": [
        "M Nadeem",
        "S Sohail",
        "L Javed",
        "F Anwer",
        "A Saudagar",
        "K Muhammad"
      ],
      "year": "2024",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "18",
      "title": "GPT-4v with Emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "19",
      "title": "'M3D: Advancing 3D medical image analysis with multi-modal large language models",
      "authors": [
        "F Bai",
        "Y Du",
        "T Huang",
        "-H Meng",
        "B Zhao"
      ],
      "year": "2024",
      "venue": "'M3D: Advancing 3D medical image analysis with multi-modal large language models",
      "arxiv": "arXiv:2404.00578"
    },
    {
      "citation_id": "20",
      "title": "MATHVERSE: Does your multi-modal LLM truly see the diagrams in visual math problems?",
      "authors": [
        "R Zhang",
        "D Jiang",
        "Y Zhang",
        "H Lin",
        "Z Guo",
        "P Qiu",
        "A Zhou",
        "P Lu",
        "K.-W Chang",
        "Y Qiao",
        "P Gao",
        "H Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "GPT as psychologist? preliminary evaluations for GPT-4v on visual affective computing",
      "authors": [
        "H Lu",
        "X Niu",
        "J Wang",
        "Y Wang",
        "Q Hu",
        "J Tang",
        "Y Zhang",
        "K Yuan",
        "B Huang",
        "Z Yu",
        "D He",
        "S Deng",
        "H Chen",
        "Y Chen",
        "S Shan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "22",
      "title": "Large language models as general pattern machines",
      "authors": [
        "S Mirchandani",
        "F Xia",
        "P Florence",
        "B Ichter",
        "D Driess",
        "M Arenas",
        "K Rao",
        "D Sadigh",
        "A Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of The Conference on Robot Learning"
    },
    {
      "citation_id": "23",
      "title": "Formality is favored: Unraveling the learning preferences of large language models on data with conflicting knowledge",
      "authors": [
        "J Li",
        "Y Cao",
        "S Huang",
        "J Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "The woman worked as a babysitter: On biases in language generation",
      "authors": [
        "E Sheng",
        "K.-W Chang",
        "P Natarajan",
        "N Peng"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "27",
      "title": "PPT: Pre-trained prompt tuning for few-shot learning",
      "authors": [
        "Y Gu",
        "X Han",
        "Z Liu",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "The power of scale for parameterefficient prompt tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Language models as black-box optimizers for vision-language models",
      "authors": [
        "S Liu",
        "S Yu",
        "Z Lin",
        "D Pathak",
        "D Ramanan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Blackbox prompt learning for pre-trained language models",
      "authors": [
        "S Diao",
        "Z Huang",
        "R Xu",
        "X Li",
        "Y Lin",
        "X Zhou",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "Blackbox prompt learning for pre-trained language models",
      "arxiv": "arXiv:2201.08531"
    },
    {
      "citation_id": "31",
      "title": "Automatic prompt optimization with ''gradient descent'' and beam search",
      "authors": [
        "R Pryzant",
        "D Iter",
        "J Li",
        "Y Lee",
        "C Zhu",
        "M Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks",
      "authors": [
        "T Chen",
        "D Borth",
        "T Darrell",
        "S.-F Chang"
      ],
      "year": "2014",
      "venue": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks",
      "arxiv": "arXiv:1410.8586"
    },
    {
      "citation_id": "35",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "WSCNet: Weakly supervised coupled networks for visual sentiment classification and detection",
      "authors": [
        "D She",
        "J Yang",
        "M.-M Cheng",
        "Y.-K Lai",
        "P Rosin",
        "L Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Simple but powerful, a language-supervised method for image emotion classification",
      "authors": [
        "S Deng",
        "L Wu",
        "G Shi",
        "L Xing",
        "W Hu",
        "H Zhang",
        "Y Xiang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "On the use of vision-language models for visual sentiment analysis: a study on clip",
      "authors": [
        "C Bustos",
        "C Civit",
        "B Du",
        "A Solé-Ribalta",
        "A Lapedriza"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "39",
      "title": "Enhancing emotion reasoning for image multi-emotion prediction",
      "authors": [
        "B Wang",
        "G Tu",
        "B Liang",
        "Z Bai",
        "M Yang",
        "X Zeng",
        "L Yao",
        "R Xu"
      ],
      "year": "2025",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Learning emotional prompt features with multiple views for visual emotion analysis",
      "authors": [
        "Q Xu",
        "Y Wei",
        "S Yuan",
        "J Wu",
        "L Wang",
        "C Wu"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "41",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "42",
      "title": "Learning to compose diversified prompts for image emotion classification",
      "authors": [
        "S Deng",
        "L Wu",
        "G Shi",
        "L Xing",
        "M Jian",
        "Y Xiang",
        "R Dong"
      ],
      "year": "2024",
      "venue": "Computational Visual Media"
    },
    {
      "citation_id": "43",
      "title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Black-box prompt optimization: Aligning large language models without model training",
      "authors": [
        "J Cheng",
        "X Liu",
        "K Zheng",
        "P Ke",
        "H Wang",
        "Y Dong",
        "J Tang",
        "M Huang"
      ],
      "year": "2024",
      "venue": "Black-box prompt optimization: Aligning large language models without model training",
      "arxiv": "arXiv:2311.04155"
    },
    {
      "citation_id": "45",
      "title": "Affection: Learning affective explanations for real-world visual data",
      "authors": [
        "P Achlioptas",
        "M Ovsjanikov",
        "L Guibas",
        "S Tulyakov"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "47",
      "title": "Reliability of crowdsourcing as a method for collecting emotions labels on pictures",
      "authors": [
        "O Korovina",
        "M Baez",
        "F Casati"
      ],
      "year": "2019",
      "venue": "BMC research notes"
    },
    {
      "citation_id": "48",
      "title": "A survey of deep learning for group-level emotion recognition",
      "authors": [
        "X Huang",
        "J Xu",
        "W Zheng",
        "Q Mao",
        "A Dhall"
      ],
      "year": "2024",
      "venue": "A survey of deep learning for group-level emotion recognition",
      "arxiv": "arXiv:2408.15276"
    },
    {
      "citation_id": "49",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "51",
      "title": "To err like human: Affective biasinspired measures for visual emotion recognition evaluation",
      "authors": [
        "C Zhao",
        "J Shi",
        "L Nie",
        "J Yang"
      ],
      "year": "2024",
      "venue": "Proceeding of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "where he is currently pursuing his M.S. degree with the Graduate School of Information Science and Technology, Hokkaido University. His research interests include multimodal large language models and their applications",
      "authors": [
        "Ryo Takahashi"
      ],
      "year": "2016",
      "venue": "Graduate Student Member, IEEE) received his B.S. degree in electronics and information engineering from Doshisha University"
    }
  ]
}