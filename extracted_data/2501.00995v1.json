{
  "paper_id": "2501.00995v1",
  "title": "Is It Still Fair? Investigating Gender Fairness In Cross-Corpus Speech Emotion Recognition",
  "published": "2025-01-02T01:21:34Z",
  "authors": [
    "Shreya G. Upadhyay",
    "Woan-Shiuan Chien",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "fairness",
    "crosscorpus",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is a vital component in various everyday applications. Cross-corpus SER models are increasingly recognized for their ability to generalize performance. However, concerns arise regarding fairness across demographics in diverse corpora. Existing fairness research often focuses solely on corpus-specific fairness, neglecting its generalizability in cross-corpus scenarios. Our study focuses on this underexplored area, examining the gender fairness generalizability in cross-corpus SER scenarios. We emphasize that the performance of cross-corpus SER models and their fairness are two distinct considerations. Moreover, we propose the approach of a combined fairness adaptation mechanism to enhance gender fairness in the SER transfer learning tasks by addressing both source and target genders. Our findings bring one of the first insights into the generalizability of gender fairness in crosscorpus SER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) has made significant strides by reshaping the landscape of human-computer interaction and enabling emotion-aware applications  [1] -  [3] . Having more generalized SER models has led to growing interest in cross-corpus SER modeling, which aims at developing models capable of generalizing across diverse corpora and linguistic contexts. This entails many strategies to compensate features, domains, or label mismatches, using techniques such as transfer, semi-supervised, and few-shot learning  [4] -  [7] . While these models have shown remarkable recognition performance on the target corpus but what about the fairness in performance?\n\nIn recent years, there has been an increasing emphasis on the trustworthiness of intelligent models, particularly concerning aspects such as privacy, fairness, safety, etc. Specifically, The notion of fairness has gained substantial attention and become a focal point of extensive discussions. Numerous studies within the SER domain have recognized the issue of fairness, highlighting the model exhibits biases with sensitive attributes. To address these fairness concerns, various techniques have been proposed, including the use of adversarial networks, reweighing schemes, and the development of loss functions designed to mitigate biases  [8] -  [11] . Some studies have specifically aimed at neutralizing different attributes, such as gender or age, within SER approaches  [10] ,  [12] . Most of these efforts have been focused on single-dataset (source-only) scenarios. However, the unfairness can manifest across multiple levels including sample, data distribution, modality, labeling, and domain-related aspects  [9] ,  [10] ,  [12] . The effectiveness of the model's fairness performance when deployed on another corpus has received limited, if any, exploration in the context of cross-domain SER scenarios. Emotions, with their intricate nature and cultural influences, manifest a diverse spectrum of expressions and interpretations across different domains and subjects. Hence, it becomes uncertain whether a SER model excelling in one emotional domain will demonstrate similar performance in another, particularly when accounting for cultural-linguistic variations.\n\nHowever, a SER model tailored to a specific source corpus may introduce unfairness related to sensitive attributes like gender or age in the target corpus, amplifying some emotions while neglecting others. Cross-corpus SER models, designed for generalization across domains or cultural contexts  [13] ,  [14] , must also ensure fairness across diverse demographics within these domains. This study emphasizes the dual considerations of performance and fairness in cross-corpus SER models. We investigate the generalizability of fairness in crosslingual SER models, employing a few state-of-the-art transfer learning and fairness techniques from the literature. Gender is specifically examined as the protected attribute of interest. Our study evaluates whether a cross-lingual SER model, which demonstrates fairness within its source corpus by showing no gender sensitivity, maintains this fairness when applied to the target corpus. This study introduces a novel perspective in cross-corpus SER, underscoring the importance of integrating fairness considerations alongside performance when deploying SER systems across corpora.\n\nIn this study, for cross-corpus SER fairness investigations, we utilize two large naturalistic speech corpora, MSP-Podcast  [15]  and BIIC-Podcast  [16] . Our experimental results reveal two insights, first despite exhibiting source-specific fairness with various transfer learning approaches, cross-corpus SER generalization can introduce gender biases for the target corpus which questions the fairness of the SER model. Second, we propose a simple fairness mechanism of combined fairness adaptation that integrates fairness towards the source and the target corpus while modeling a cross-corpus SER. Our proposed cross-corpus setting based Combined Fairness Adaptation (CFA) idea achieves significantly better gender fairness (GF) compared to the considered baselines of this study. The preliminary findings of this study demonstrate the effectiveness and necessity of research in this direction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Fairness Generalizability Analyses",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Naturalistic Corpora",
      "text": "The MSP-Podcast (MSP-P)  [15]  corpus contains 166 hours of emotional American English speech (v1.10), sourced from audio-sharing websites. This resource is valuable for SER research due to its extensive size and emotionally balanced dialogues from different individuals. It includes annotations for primary and secondary emotions and emotional attributes. We select 49,018 samples from this corpus, evenly divided between 24,466 male and 24,552 female subject samples.\n\nThe BIIC-Podcast (BIIC-P)  [16]  corpus is a SER database (v1.0) in Taiwanese Mandarin. It contains 157 hours of speech samples from podcasts and follows a data collection methodology similar to the MSP-P corpus. The annotations cover primary and secondary emotional categories, as well as three emotional attributes. Here, we utilize 18,706 samples of the data with 9,654 male and 9,326 female subject samples.\n\nThis work considers four primary emotion categories (Neutral, Happiness, Anger, and Sadness) samples for binary SER tasks with predefined train-valid-test sets of the corpora. For a detailed analysis, we analyze each emotion individually rather than as a 4-category SER task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Experimental Settings And Evaluation Metrics",
      "text": "In our experiments, we use MSP-P as the emotion-labeled source corpus and BIIC-P as the target corpus. We employ popular wav2vec2.0  [20]  feature vectors along with different transfer learning and fairness considering architectures from the literature to investigate fairness generalizability on the target corpus in cross-corpus SER models. Optimization is performed using the Adam optimizer with a learning rate of 0.0001 and a decay factor of 0.001, while training involves back-propagation with binary cross-entropy loss. Training proceeds for a maximum of 50 epochs with a batch size of 64 and includes early stopping. Evaluation of SER model performance is based on the Unweighted Average Recall (UAR). To measure fairness, we utilize established metrics: Statistical Parity (∆SP ) and Equal Odds (∆EO). This ranges from -1 to 1, where values closer to 0 indicate better fairness. These metrics are commonly used in fairness studies  [9] ,  [21] . ∆SP ensures the same proportion of positive outcomes across groups, regardless of their true characteristics (e.g., gender, race). ∆EO requires the model to have equal true positive and false positive rates for different groups, ensuring fair performance. By analysing both we are making more consistent investigation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Fairness Generalizability Evaluations",
      "text": "To examine the generalizability of gender fairness (GF) in cross-corpus SER models, we perform cross-test fairness assessments using state-of-the-art (SOTA) methods in two stages. First, we evaluate several cross-corpus techniques using effective SOTA transfer learning (TL) methods to assess GF generalizability. Second, we apply fairness approaches from the literature to the best-performing TL model to assess GF generalizability after incorporating a source-specific fairness approach. Table I summarizes SER performances (UAR) and fairness evaluations (∆SP and ∆EO) across four primary emotions for male and female speakers. Results are averaged from ten experiment repetitions. To assess whether the crosscorpus model that demonstrates GF on the source also maintains GF on the target corpus (beyond just performance), Table  I  presents the results for both the MSP-P and BIIC-P test sets for the respective cross-corpus model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gf Generalizability With Various Tl Approaches:",
      "text": "To analyze GF generalizability using TL approaches, we consider three SOTA methods: Few-shot (FS)  [5] , which leverages knowledge from source corpora and adapts the model to the target domain, GAN-based (GAN)  [17] , which employs adversarial training, and phonetically-anchored (PA)  [7] , which utilizes learning in a shared phonetic space for SER models. Table I highlights the gender-specific performance of these TL models, revealing disparities between males and females over the target BIIC-P corpus. For instance, in the Anger category, UARs for males are 57.74%, 63.72%, and 58.01%, and for females, 70.42%, 47.86%, and 71.79% for FS, GAN, and PA models, respectively. This performance discrepancy is also evident in the source MSP-P test set. Fairness metrics in Table  I  show gender unfairness across emotional classes over both corpora test sets. For example, in Anger, the ∆SP values are 0.380 and 0.534 for PA models for MSP-P and BIIC-P, respectively. This behavior can be seen in other emotion categories as well. The results highlight the challenges in GF generalizability for cross-corpus SER TL models, emphasizing the need to address fairness in such scenarios distinctly.\n\nCross-Corpus GF Generalizability with Fairness Techniques: Given our emphasis on GF in cross-corpus SER settings, we consider two state-of-the-art fairness methods from the literature: Fairway  [18] , Reweigh  [19] . These fairness methodologies are source-specific. However, existing literature lacks fairness techniques specifically designed for cross-corpus settings. Therefore, we proceed to test these existing fairness techniques in conjunction with our best-performing TL method (PA) from the previous section. Table  II  shows the results of PA-FairW (PA +Fairway) and PA-ReW (PA + Reweigh). Upon analyzing the performance of these models over MSP-P and BIIC-P test sets, we observe that although there is a decrease in the ∆SP and ∆EO values for MSP-P (indicating improvement), but no significant reduction is observed for BIIC-P. For instance, for Anger, for MSP-P, (PA-ReW) yields ∆SP and ∆EO values of 0.159 and 0.168, respectively, while on BIIC-P set, the ∆SP and ∆EO values are still high with 0.321 and 0.416, respectively (shows no improvement). Similar behavior is observed in other emotion categories as well. These findings indicate that even if the model exhibits better source-specific GF, these approaches fail to generalize GF across cross-evaluations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Fair Cross-Corpus Ser",
      "text": "Building on insights from Section II, cross-corpus SER models face challenges with GF generalizability in target corpora. To address this, we propose a strategy (Fig.  1 ) for the GF generalized cross-corpus SER. The architecture has two key blocks: an emotion classification (EC) block and a combined fairness adaptation (CFA) block. For testing our idea, in the EC block, we use a basic SER architecture; processing wav2vec2.0  [20]  features with a transformer and four fully-connected layers for binary emotion classification. The CFA block adds an auxiliary adversarial network for gender classification (GC) with a reverse gradient layer which transforms EC features into a gender-neutral state by minimizing GC's ability to predict gender from both corpora. Other experimental settings and evaluation metrics match those in Section II-B.\n\nThe training process unfolds in stages. Initially, we train the primary SER model on the source corpus, focusing on capturing gender-neutral emotional features while disregarding gender-specific information. Subsequently, we create minibatches that combine data from both the MSP-P and BIIC-P for gender-neutral adversarial training. The EC aims to generate gender-neutral embeddings, while the GC aims to make accurate gender predictions. This training process uses a binary cross-entropy loss function for classifying gender, penalizing the classifier for making gender predictions based on shared feature representations. In each training iteration, we update both the primary model's weights and the gender classifier's parameters. The inserted reverse gradient layer in learning modifies the gradients backpropagated from the GC branch to make the shared feature representations more gender-neutral. Equation  1 shows the binary classification loss used for EC and GC tasks. where z is the intermediate embeddings produced by the primary EC model. y be the true labels for emotions (0 for target, 1 for other) for EC and the gender of speakers (0 for male, 1 for female) for GC task. D represents the EC or GC, P (y|z) be the predicted target attribute probability distribution given z, and θ D be the parameters. To enforce a similarity between source and target genders, we use a contrastive loss that encourages samples with the same gender to be close in the feature space and those with different genders to be distant from each other. The goal is to make the learned representations of gender-related features similar between the source and target domains. The Equation 2 is used to integrate a source and target similarity loss.\n\nwhere D is the Euclidean distance between the embeddings of samples x 1 and x 2 . m is the margin and here is fixed to 1. This loss encourages the model to minimize the distance between samples with the same gender (y = 0) and maximize the distance between samples with different genders (y = 1). Equation 3 illustrates the total loss for the PA-CFA model, which combines the EC loss and the GC loss.\n\nwhere L emo and L GC are the binary emotion classification and gender classification loss. L GSim is the loss for the gender similarity. α and β are set to 0.5. Here, during training, target gender labels are needed, but not during inference.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiment And Analyses",
      "text": "Table  II  shows gender-specific performance and fairness metrics for Base-CFA with a basic SER model and PA-CFA, combined with the PA TL model. Table  II  demonstrates that on cross-evaluation (on BIIC-P test set), our proposed CFA baseline Base-CFA method outperforms PA-ReW in terms of GF generalizability across most emotion categories. For example, for Anger, Base-CFA achieves balanced genderspecific performance (M:69.28%, F:71.56%) compared to PA-ReW (M:52.98%, F:45.37%). In terms of fairness metrics, CFA shows a significant reduction in both ∆SP and ∆EO across all emotions compared to the PA-ReW. For instance, the SP value for Anger decreases by 0.107, reflecting improved  GF. Additionally, PA-CFA, which combines phonetic-based constraints with CFA, captures language-specific information in cross-lingual tasks, exceeds the performance of PA-ReW.\n\nFor example, for Anger,∆SP drops from 0.363 to 0.260, a consistent trend across both fairness metrics. We validate our approach by training a model without the gender similarity loss (L GSim ), called PA-Adv. Table  II  shows that PA-CFA still achieves better fairness. For example, in Anger, PA-Adv has a ∆SP of 0.322, while PA-CFA achieves 0.260, highlighting the effectiveness of L GSim in improving fairness by aligning gender features across corpora. For reference, Table  III  presents the overall UAR performance of the CFA model. From Table  III , we can see that PA-CFA shows a slight decrease in performance compared to PA, but it remains competitive over all emotion categories. We believe this minor performance drop is a worthwhile trade-off for improved GF. For reference, the overall UAR represents recall across all classes, irrespective of gender. In Table  III , UAR values are shown separately for males and females, so the overall UAR shown in Table  III  is not an average of gender-specific UARs.\n\nAnalysis: We utilize t-SNE plots to explore gender attribute feature spaces in PA-CFA and PA-ReW, shown in Figure  2 , aiming to understand why PA-CFA performs better. In Figure  2a , comparing Anger in Figure  2a  to Figure  2b , we see distinct male-female clusters in 2a and a mixed distribution in 2b. This indicates coexisting male and female features from different corpora, across all emotions studied. It underscores the importance of integrating CFA and questions the effectiveness of source-specific fairness in cross-corpus SER.\n\nWe also analyze embeddings from both the PA-ReW and proposed PA-CFA models to assess gender-related information across the MSP-P and BIIC-P contexts. Figure  3  presents the gender detection (GD) accuracy, with MSP-P on the left and BIIC-P on the right. PA-ReW exhibits varying GD accuracy: lower for MSP-P and higher for BIIC-P, suggesting residual gender information specific to BIIC-P despite lower MSP-P performance. Conversely, PA-CFA shows improved  gender neutralization, with similar GD accuracy across both corpora (e.g., for Anger, 36% MSP-P, 35% BIIC-P in Figure  3b ). In PA-CFA, there are improvements in emotions like Neutral and Anger for MSP-P GD accuracy compared to PA-ReW, reflecting compensation in MSP-P performance due to enhanced gender fairness across MSP-P and BIIC-P corpora.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "This research addresses overlooked fairness concerns in cross-corpus SER, particularly focusing on gender neutrality. Our findings reveal that cross-corpus SER models, while fair within their source corpus, introduce biases when generalized across different corpora. We propose an initial approach, combined fairness adaptation (CFA), to enhance gender neutrality across both source and target corpora in emotion transfer tasks. Initial experiments demonstrate the efficacy of our approach in creating gender-fair cross-corpus SER systems. Future research will refine our fairness mechanism through feature-side analysis to pinpoint specific areas where fairness issues arise in cross-corpus SER settings.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) for the GF",
      "page": 3
    },
    {
      "caption": "Figure 1: Proposed (CFA) SER approach using gender-neutral adaptation mech-",
      "page": 3
    },
    {
      "caption": "Figure 2: a, comparing Anger in Figure 2a to Figure 2b, we see distinct",
      "page": 4
    },
    {
      "caption": "Figure 2: t-SNE plot for Anger features.",
      "page": 4
    },
    {
      "caption": "Figure 3: Gender detection (GD) accuracy plot using PA-ReW and PA-CFA",
      "page": 4
    },
    {
      "caption": "Figure 3: b). In PA-CFA, there are improvements in emotions like",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FS [5]": "GAN [17]",
          "67.08\n69.66\n58.44\n48.33": "61.47\n67.67\n58.33\n44.44",
          "66.69\n70.00\n51.93\n64.47": "57.23\n62.73\n64.39\n47.57",
          "74.22\n70.94\n57.74\n70.42": "69.50\n70.30\n63.72\n47.86",
          "70.54\n65.58\n51.69\n62.22": "59.45\n59.92\n46.08\n66.04",
          "0.305\n0.215\n0.528\n0.625": "0.348\n0.302\n0.570\n0.667",
          "0.385\n0.355\n0.512\n0.624": "0.386\n0.304\n0.559\n0.632",
          "0.343\n0.335\n0.558\n0.599": "0.306\n0.329\n0.436\n0.465",
          "0.339\n0.423\n0.503\n0.435": "0.304\n0.396\n0.417\n0.512"
        },
        {
          "FS [5]": "PA [7]",
          "67.08\n69.66\n58.44\n48.33": "75.22\n73.16\n65.11\n57.99",
          "66.69\n70.00\n51.93\n64.47": "64.01\n68.85\n63.58\n57.75",
          "74.22\n70.94\n57.74\n70.42": "65.41\n68.29\n58.01\n71.79",
          "70.54\n65.58\n51.69\n62.22": "60.36\n65.06\n63.57\n52.51",
          "0.305\n0.215\n0.528\n0.625": "0.352\n0.341\n0.541\n0.515",
          "0.385\n0.355\n0.512\n0.624": "0.375\n0.351\n0.540\n0.565",
          "0.343\n0.335\n0.558\n0.599": "0.380\n0.384\n0.534\n0.459",
          "0.339\n0.423\n0.503\n0.435": "0.356\n0.322\n0.567\n0.549"
        },
        {
          "FS [5]": "PA-FairW [18]",
          "67.08\n69.66\n58.44\n48.33": "73.17\n79.42",
          "66.69\n70.00\n51.93\n64.47": "74.37\n75.73",
          "74.22\n70.94\n57.74\n70.42": "69.88\n60.12",
          "70.54\n65.58\n51.69\n62.22": "74.02\n63.88",
          "0.305\n0.215\n0.528\n0.625": "0.133\n0.253",
          "0.385\n0.355\n0.512\n0.624": "0.195\n0.205",
          "0.343\n0.335\n0.558\n0.599": "0.279\n0.242",
          "0.339\n0.423\n0.503\n0.435": "0.253\n0.221"
        },
        {
          "FS [5]": "",
          "67.08\n69.66\n58.44\n48.33": "69.43\n57.37",
          "66.69\n70.00\n51.93\n64.47": "59.72\n69.42",
          "74.22\n70.94\n57.74\n70.42": "48.23\n64.09",
          "70.54\n65.58\n51.69\n62.22": "65.33\n52.26",
          "0.305\n0.215\n0.528\n0.625": "0.346\n0.476",
          "0.385\n0.355\n0.512\n0.624": "0.462\n0.498",
          "0.343\n0.335\n0.558\n0.599": "0.335\n0.428",
          "0.339\n0.423\n0.503\n0.435": "0.412\n0.511"
        },
        {
          "FS [5]": "PA-ReW [19]",
          "67.08\n69.66\n58.44\n48.33": "73.25\n74.05\n67.25\n59.05",
          "66.69\n70.00\n51.93\n64.47": "73.91\n70.43\n58.91\n69.43",
          "74.22\n70.94\n57.74\n70.42": "59.93\n63.39\n49.93\n58.39",
          "70.54\n65.58\n51.69\n62.22": "65.63\n68.01\n54.63\n65.01",
          "0.305\n0.215\n0.528\n0.625": "0.121\n0.256\n0.320\n0.401",
          "0.385\n0.355\n0.512\n0.624": "0.135\n0.194\n0.411\n0.512",
          "0.343\n0.335\n0.558\n0.599": "0.159\n0.168\n0.321\n0.416",
          "0.339\n0.423\n0.503\n0.435": "0.277\n0.224\n0.404\n0.415"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PA-ReW": "Base-CFA",
          "72.63\n75.96\n59.03\n68.44": "73.25\n70.74\n67.60\n64.88",
          "68.82\n67.63\n59.31\n64.57": "75.30\n71.02\n63.05\n62.40",
          "68.42\n70.05\n63.67\n68.14\n52.98\n45.37\n61.81\n53.03": "75.33\n76.32\n71.18\n73.70\n69.28\n71.56\n64.94\n60.86",
          "0.161\n0.159\n0.391\n0.401": "0.256\n0.264\n0.335*\n0.294**",
          "0.198\n0.257\n0.412\n0.469": "0.190\n0.164\n0.212**\n0.263**",
          "0.111\n0.203\n0.363\n0.423": "0.130\n0.153\n0.256**\n0.205**",
          "0.154\n0.248\n0.398\n0.402": "0.132\n0.133\n0.193**\n0.275**"
        },
        {
          "PA-ReW": "PA-CFA",
          "72.63\n75.96\n59.03\n68.44": "74.44\n71.33\n68.24\n65.75",
          "68.82\n67.63\n59.31\n64.57": "76.93\n70.47\n61.55\n64.83",
          "68.42\n70.05\n63.67\n68.14\n52.98\n45.37\n61.81\n53.03": "79.74\n82.42\n70.69\n74.32\n68.15\n70.14\n65.36\n62.35",
          "0.161\n0.159\n0.391\n0.401": "0.211\n0.231\n0.205**\n0.211**",
          "0.198\n0.257\n0.412\n0.469": "0.119\n0.216\n0.223 **\n0.206**",
          "0.111\n0.203\n0.363\n0.423": "0.107\n0.195\n0.260**\n0.287**",
          "0.154\n0.248\n0.398\n0.402": "0.106\n0.201\n0.236**\n0.241**"
        },
        {
          "PA-ReW": "PA-Adv",
          "72.63\n75.96\n59.03\n68.44": "74.63\n71.91\n75.04\n71.06\n62.89\n66.05\n63.94\n66.38",
          "68.82\n67.63\n59.31\n64.57": "",
          "68.42\n70.05\n63.67\n68.14\n52.98\n45.37\n61.81\n53.03": "72.61\n76.08\n69.99\n72.35\n70.18\n68.84\n61.23\n64.21",
          "0.161\n0.159\n0.391\n0.401": "0.295\n0.301\n0.281\n0.345",
          "0.198\n0.257\n0.412\n0.469": "0.21\n0.234\n0.273\n0.357",
          "0.111\n0.203\n0.363\n0.423": "0.192\n0.205\n0.322\n0.306",
          "0.154\n0.248\n0.398\n0.402": "0.184\n0.217\n0.251\n0.365"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real-life emotion-related states detection in call centers: a cross-corpora study",
      "authors": [
        "L Devillers",
        "C Vaudable",
        "C Chastagnol"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "2",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "U Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS'17"
    },
    {
      "citation_id": "3",
      "title": "Using emotion to gain rapport in a spoken dialog system",
      "authors": [
        "J Acosta"
      ],
      "year": "2009",
      "venue": "Using emotion to gain rapport in a spoken dialog system"
    },
    {
      "citation_id": "4",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "5",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "6",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Phonetic anchor-based transfer learning to facilitate unsupervised cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "B.-H Su",
        "W.-C Lin",
        "W.-S Chien",
        "Y.-T Wu",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Fair transfer learning with missing protected attributes",
      "authors": [
        "A Coston",
        "K Ramamurthy",
        "D Wei",
        "K Varshney",
        "S Speakman",
        "Z Mustahsan",
        "S Chakraborty"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "9",
      "title": "Bias and fairness on multimodal emotion detection algorithms",
      "authors": [
        "M Schmitz",
        "R Ahmed",
        "J Cao"
      ],
      "year": "2022",
      "venue": "Bias and fairness on multimodal emotion detection algorithms",
      "arxiv": "arXiv:2205.08383"
    },
    {
      "citation_id": "10",
      "title": "Achieving fair speech emotion recognition via perceptual fairness",
      "authors": [
        "W.-S Chien",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Fairgrad: Fairness aware gradient descent",
      "authors": [
        "G Maheshwari",
        "M Perrot"
      ],
      "year": "2023",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "Gender de-biasing in speech emotion recognition",
      "authors": [
        "C Gorrostieta",
        "R Lotfian",
        "K Taylor",
        "R Brutti",
        "J Kane"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "13",
      "title": "The emotion probe: On the universality of cross-linguistic and cross-gender speech emotion recognition via machine learning",
      "authors": [
        "G Costantini",
        "E Parada-Cabaleiro",
        "D Casali",
        "V Cesarini"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Crosscorpus training strategy for speech emotion recognition using selfsupervised representations",
      "authors": [
        "M Pastor",
        "D Ribas",
        "A Ortega",
        "A Miguel",
        "E Lleida"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "15",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "2023 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Fairway: a way to build fair ml software",
      "authors": [
        "J Chakraborty",
        "S Majumder",
        "Z Yu",
        "T Menzies"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering"
    },
    {
      "citation_id": "19",
      "title": "Data preprocessing techniques for classification without discrimination",
      "authors": [
        "F Kamiran",
        "T Calders"
      ],
      "year": "2012",
      "venue": "Knowledge and information systems"
    },
    {
      "citation_id": "20",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "Ai fairness 360: an extensible toolkit for detecting",
      "authors": [
        "R Bellamy",
        "K Dey",
        "M Hind",
        "S Hoffman",
        "S Houde",
        "K Kannan",
        "P Lohia",
        "J Martino",
        "S Mehta",
        "A Mojsilovic"
      ],
      "year": "2018",
      "venue": "Understanding, and Mitigating Unwanted Algorithmic Bias"
    }
  ]
}