{
  "paper_id": "2210.16642v1",
  "title": "Unifying The Discrete And Continuous Emotion Labels For Speech Emotion Recognition",
  "published": "2022-10-29T16:12:31Z",
  "authors": [
    "Roshan Sharma",
    "Hira Dhamyal",
    "Bhiksha Raj",
    "Rita Singh"
  ],
  "keywords": [
    "speech emotion recognition",
    "discrete and continuous labels",
    "multi-task",
    "hierarchical multi-task"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Traditionally, in paralinguistic analysis for emotion detection from speech, emotions have been identified with discrete or dimensional (continuous-valued) labels. Accordingly, models that have been proposed for emotion detection use one or the other of these label types. However, psychologists like Russell and Plutchik have proposed theories and models that unite these views, maintaining that these representations have shared and complementary information. This paper is an attempt to validate these viewpoints computationally. To this end, we propose a model to jointly predict continuous and discrete emotional attributes and show how the relationship between these can be utilized to improve the robustness and performance of emotion recognition tasks. Our approach comprises multi-task and hierarchical multi-task learning frameworks that jointly model the relationships between continuous-valued and discrete emotion labels. Experimental results on two widely used datasets (IEMOCAP and MSPPodcast) for speech-based emotion recognition show that our model results in statistically significant improvements in performance over strong baselines with non-unified approaches. We also demonstrate that using one type of label (discrete or continuousvalued) for training improves recognition performance in tasks that use the other type of label. Experimental results and reasoning for this approach (called mis-matched training approach) are also presented.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a complex entity which can be modelled in different ways. When defined as a motor response to stimuli, it can be viewed as belonging to a discrete set; whereas when considered as a subjective feeling, it can be expressed as a continuous vector in multiple dimensions  [1] . Though these two definitions of emotion appear to be different, they have shared and complementary information. Psychological models like Plutchik's wheel of emotions  [2]  or Russell's \"arousal-valence\" space  [3]  represent discrete emotions on and within a circle on a continuous \"arousal-valence\" plane, demonstrating that the values of \"arousal\" and \"valence\" are correlated to the perception of discrete emotion.\n\nSpeech Emotion Recognition(SER), which refers to the task of identifying the emotional state of the speaker using speech as input, can be used to predict both discrete or continuous emotions. The most commonly used continuous representation for this task comprises three attributes -Valence (V), Arousal (A), and Dominance(D) (from Russell  [4] ). Among these three dimensions, Va-*These authors have equal contribution. lence represents the pleasantness of the emotion, Arousal denotes the intensity of it, and Dominance represents the degree of control over a social situation. Similarly, a number of discrete emotions have been identified -happy, sad, angry etc. Translating between these two forms of emotion representations has been of interest  [5, 6, 7, 8] , both with audio and textual input. However conflicting results have been found: while some studies  [6]  find that there is little correlation between continuous values and discrete labels, others  [8]  find correlations like 'anger has higher arousal and lower valence'. Based on the notion that discrete and continuous emotion attributes are likely related, in this paper, we propose to examine such dependency relationships.\n\nIn order to investigate the relationship between continuous and discrete attributes, we develop neural network models that jointly predict continuous and discrete emotion labels. The motivation behind this is two-fold: (a) holistic models for automatic emotion recognition must be able to capture both generic and specific notions of emotion contained in the continuous and discrete emotion labels, and (b) since discrete and continuous emotion labels may be related to each other, robust and generalizable machine learning models can be built by leveraging this dependency. We propose a multi-task learning framework where continuous and discrete emotion labels are predicted together, but independently of each other. Next, we consider the possibility that knowledge of the discrete emotion might help predict more accurately the continuous emotion and vice versa and hence introduce hierarchical multi-task models that model such a relationship to jointly predict discrete and continuous emotion.\n\nWe further demonstrate that these continuous and discrete labels need not necessarily be manually annotated within the same corpus to be improve recognition performance. Of the prevailing annotated datasets for emotion recognition, very few are annotated for both continuous and discrete attributes such as MSPPodcast  [9] , and IEMOCAP  [10]  while others like MELD  [11]  are annotated for discrete attributes only. We demonstrate that even if the model is trained on continuous emotion labels from MSPPodcast and discrete labels from IEMOCAP, the proposed Hierarchical Multi-Task approach improves performance and generalizability. This implies that emotion recognition datasets can be trained jointly on multiple corpora with different labels. In summary, this paper makes the following contributions:\n\n1. We propose a multi-task learning framework that jointly predicts the continuous and discrete labels from speech 2. We extend this framework to model hierarchical dependencies, where knowledge of discrete attributes aids continuous prediction and vice versa. 3. We demonstrate that the proposed approach can be used when the mis-matched labels (continuous and discrete) are drawn from different datasets. These hidden representations are the input to a decoder that either predicts continuous emotion ĉ, discrete emotion ŷ, or both. The decoder comprises a temporal self-attentive pooling layer, followed by a Multi-layer Perceptron (MLP) that extracts task specific embeddings, i.e., ED for discrete prediction and EC for continuous prediction. These embeddings are then passed through the final classification layer that maps to 3 dimensions for continuous prediction (corresponding to V,A,D), or 5 dimensions for discrete prediction (corresponding to the number of discrete emotion classes). The continuous and discrete baseline models use a similar neural network architecture, except for the number of output neurons.\n\nEach of the models we describe below, baseline models that perform either continuous or discrete prediction, and multi-task or hierarchical multi-task models that predict both discrete and continuous emotion, share the same encoder structure but with slightly different decoder architectures.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Baseline Models Baseline Discrete Model",
      "text": "To independently predict the discrete attributes from input X, our encoder-decoder model termed as Baseline D uses a decoder with discrete self-attentive pooling, MLP which generates the discrete embedding ED, and a discrete classification layer that produces the discrete label ŷ as its output. The model is shown in Fig.  1",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "(Top Left).",
      "text": "Given the true discrete label y, the model is optimized using the multi-class cross entropy criterion. For a dataset with A utterances, the cross-entropy loss is computed as: This model is optimized using the Concordance Correlation Coefficient (CCC) Loss. Given the prediction ĉ and the ground truth c, the CCC loss is defined as shown in Equation  2 .\n\nwhere s cĉ , s 2 c , s 2 ĉ , c, c represent the covariance between the ground truth and prediction, variance of the ground truth, variance of the prediction, mean of the groundtruth and mean of the prediction respectively.The resulting loss for continuous emotion recognition can be written as the sum of the CCC losses for valence, arousal and dominance prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Task Model",
      "text": "Since continuous and discrete representations carry information about the same emotion, we are interested in understanding if jointly predicting continuous and discrete attributes improves model performance. To do this, we use a multi-task architecture to predict both the discrete and continuous emotion attributes simultaneously. Since we seek to find the commonality between the discrete and continuous representations, in the multi-task architecture, we elect to utilize a shared speech encoder that transforms the input speech into hidden representations H that contains the information that pertains to both the continuous and discrete emotion attributes,i.e., the shared information.\n\nThe Multi-task C,D model predicts both discrete label ŷ and continuous vector ĉ = [ĉ V al , ĉAro, ĉDom] for every utterance.\n\nThe architecture of the model is shown in Fig.  1  (left). The model uses a common encoder, but within the decoder-two parallel branches are used to learn task specific pooling and MLP parameters. The discrete branch (left) generates a discrete embedding ED, which is then used to predict the discrete emotion ŷ. Similarly, the continuous branch predicts a continuous embedding EC , which is used to predict the continuous emotion ĉ. The model is optimized with the total loss as shown in Equation  (3) .\n\nwhere the values of α and β are set to 1 empirically.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hierarchical Multi-Task Model",
      "text": "Multi-task modelling described in the previous section seeks to utilize the shared information between discrete and dimensional attributes to predict them jointly. However, it doesn't assume any direct dependency between the discrete and continuous emotion attributes. Based on the hypothesis that knowledge of discrete emotion attributes would help improve continuous attribute predictions and vice-versa, we develop hierarchical multi-task models.\n\nIn this model, the continuous and discrete emotion prediction branches in the decoder are used to generate the continuous and discrete embeddings EC and ED respectively, as in the multi-task formulation. However, here, the predicted continuous emotion, i.e., ĉ is not computed solely based on EC , but also on ED. In other words, the discrete emotion embedding is used in conjunction with the continuous embeddding to predict the continuous emotion. We term this our Hierarchical D-C model since the discrete embedding is used as auxiliary input to predict the continuous emotion. Similarly, one can define a Hierarchical C-D model where the continuous emotion embedding is used as auxiliary input to predict the discrete emotion.\n\nThe Hierarchical D-C model is shown in Fig.  1  (right). To perform continuous emotion prediction with the help of the discrete predictions, discrete emotion embedding ED, (which is used to predict the discrete emotion ŷ) is concatenated with the continuous embedding EC . This concatenated embedding [EDEC ] is then used to predict the continuous emotion, ĉ. Similarly, in the Hierarchical C-D model, in order to utilize continuous attributes to predict discrete attributes, EC is concatenated with ED and used to predict ŷ.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "Our experiments are conducted on the MSPPodcast  [12]  and IEMO-CAP  [13] . MSP Podcast is the largest human labelled emotion dataset with 29,965 training examples and 10,013 test utterances. It is comprised of segments of speech from podcasts, which means that the speech is expressive and the acoustic environment less constrained by background noise and other interference. The MSP-Podcast data has labels for three continuous dimensions -valence, arousal and dominance, and for multiple discrete emotions -of which we use five : neutral, angry, happy, sad, and disgust. In this work, we report our results on the balanced test1 evaluation set with 30 male and 30 female speakers. The second dataset we use is IEMOCAP which is a 20 hour dataset, with annotations on acted emotion. It comprises of 10 different speakers, 5 male and 5 female. It is labelled for three continuous dimensions -valence, arousal and dominance, and for 9 discrete emotions out of which we utilise the ones that overlap with our MSPPodcast set, i.e., neutral, angry, happy, sad, and disgust.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Hyperparameters",
      "text": "All our models are built using the ESPNet  [14]  toolkit, and will be publicly released to encourage further research. We use HUBERTlarge  [15]  embeddings as the input features and a 4 layer conformer encoder with 64 hidden units for our models. We freeze the HUBERT-large frontend, and train the conformer and pooling decoder parameters. We employ separate self-attentive pooling  [16]  layers for continuous and discrete emotion prediction so the model can learn which frames to focus on in order to make emotion predictions. Our decoders consist of linear projections that map from the encoded output dimension of 768 to 64, 32 and consequently the output sizes of 3 for the continuous prediction and 5 for the discrete prediction. We use a ReLU activation to ensure that the predicted continuous attributes are strictly positive and use LeakyReLU elsewhere. We use a dropout of 0.2 in the decoder. Our models are trained with the Adam optimizer, and a peak learning rate of 1e-3 for 15,000 warmup steps.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Discrete emotion recognition is evaluated using F1 or accuracy. For continuous prediction, we compute the Concordance Correlation Coefficient for each of the attributes arousal, valence and dominance, and consequently their mean. Equation (  4 ) shows how the CCC is computed given prediction ĉ, and ground truth c, where s cĉ , s 2 c , s 2 ĉ , c, c represent the covariance between the ground truth and prediction, variance of the ground truth, variance of the prediction, mean of the groundtruth and mean of the prediction respectively.   1  shows our experimental results on the IEMOCAP dataset, where all results are computed using standard 5-fold cross validation  [17] . We observe that multi-task training improves CCC by 0.02 over the continuous baseline, however, it does not outperform the discrete baseline. We contend that this is because both of these predictions are being generated independent of the other. The proposed hierarchical models outperform the baselines and multi-task model on discrete and continuous emotion prediction. Specifically the hierarchical model that uses discrete attributes to aid the prediction of continuous attributes, i.e., Hierarchical D-C outperforms the multi-task model and the continuous baselines on CCCgaining 0.087 absolute CCC. This is perhaps because knowledge of the discrete emotion aids in the prediction of dominance and arousal. We also observe an absolute 0.8% improvement on accuracy in Hierarchical D-C. The hierarchical model that uses continuous emotions to help predict discrete attributes, i.e., Hierarchical C-D outperforms the other 3 models on accuracy. This reaffirms our hypothesis that knowledge of discrete and continuous emotion attributes can be used to improve performance on continuous and discrete emotion prediction respectively. We note that the Hierarchical C-D model improves the recognition of discrete emotion attributes compared to the multi-task model and the continuous baseline.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results On Msppodcast",
      "text": "Table  3  reports the results of experiments on MSPPodcast. The continuous and discrete emotion prediction baselines obtain comparable scores to state-of-the-art approaches  [18] . Table  2 . Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and MSPPodcast datasets respectively. CCC is reported on IEMOCAP Session05 and on the test1 evaluation sets for MSPPodcast respectively. F1 is reported on the test1 evaluation set for MSPPodcast and unweighted accuracy on Session05 for IEMOCAP (we use this because these are standard metrics reported for these datasets). Multi-task training to jointly predict continuous and discrete attributes improves F1 score over the discrete baseline while slightly degrading continuous emotion prediction performance. As with the IEMOCAP dataset, the proposed models outperform the baselines and multi-task prediction models. Specifically, the hierarchical D-C model improves CCC on continuous attribute prediction over the multi-task and baseline continuous models. Hierarchical multi-task learning helps learn useful intermediate representations  [19] . Therefore in the hierarchical D-C model, the continuous prediction task helps learn better discrete emotion representations, thereby improving discrete prediction as well with respect to the baseline. The Hierarchical C-D model also improves prediction of discrete emotion over the other 3 models. This model also outperforms the baseline and multi-task models on continuous emotion prediction, with gains arising from improved prediction of arousal and dominance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Id Iemocap Train",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Combining Iemocap And Msppodcast",
      "text": "In this section, we attempt to use discrete and continuous labels from different datasets and analyse gains when training models on multiple datasets with matched (e.g. discrete-discrete) and mis-matched (e.g. continuous-discrete) labels. We perform all experiments using a subset of the MSPPodcast data chosen randomly such that the resulting size is the same as that of the IEMOCAP training data. This is done in order to be able to make fair comparisons on transferrability of representations.   2 , we can conclude that when the MSPPodcast continuous labels are used alone, the best performance is achieved.\n\nIn conclusion, we observe that using mis-matched labels from the MSPPodcast data for training improves performance on discrete and continuous emotion prediction for IEMOCAP. We also note that though discrete labels from IEMOCAP are transferrable and improve performance on MSPPodcast, the continuous labels from IEMOCAP do not seem to provide gains on MSPPodcast.\n\nTherefore MSPPodcast discrete and continuous representations help improve performance on the mismatched label of IEMOCAP, while such transferred representations from IEMOCAP do not significantly impact MSPPodcast predictions. We believe this is in part because of the difference in the nature of annotations across the IEMOCAP and MSPPodcast datasets. For example, from our analysis we observe that anger is assigned a lower arousal and higher valence than neutral in IEMOCAP while in MSPPodcast, anger is assigned a higher arousal and lower valence than neutral. Furthermore, the inter-annotator agreement for continuous values in MSPPodcast are much lower in the IEMOCAP, which makes it challenging to obtain gains in MSPPodcast continuous predictions using transferred representations from IEMOCAP.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "Emotion Recognition remains a challenging task. Different datasets are labelled with varying annotation labels, making it challenging to train large scale emotion models utilising all the datasets. In this paper, we introduce hierarchical multi-task learning models that predict discrete or continuous labels by using continuous or discrete labels respectively.\n\nWith our method, we obtain absolute improvements of 1.2 % Accuracy and 4.3 points F-1 for discrete prediction on IEMOCAP and MSPPodcast respectively. On continuous labels, we improve 0.09 CCC for IEMOCAP, and 0.025 CCC in MSPPodcast. Furthermore, we also combine IEMOCAP and MSPPodcast with mismatched emotion annotations, and show that mis-matched labels from MSPPodcast help performance on IEMOCAP.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ﬁgure shows model architectures used in this work. The",
      "page": 2
    },
    {
      "caption": "Figure 1: (left). The",
      "page": 2
    },
    {
      "caption": "Figure 1: (right). To",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Carnegie Mellon University": "lence represents the pleasantness of the emotion, Arousal denotes the"
        },
        {
          "Carnegie Mellon University": "intensity of it, and Dominance represents the degree of control over a"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "social situation. Similarly, a number of discrete emotions have been"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "identiﬁed – happy,\nsad, angry etc.\nTranslating between these two"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "forms of emotion representations has been of\ninterest\n[5, 6, 7, 8],"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "both with audio and textual\ninput. However conﬂicting results have"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "been found: while some studies[6] ﬁnd that there is little correlation"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "between continuous values and discrete labels, others [8] ﬁnd corre-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "lations like ‘anger has higher arousal and lower valence’. Based on"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "the notion that discrete and continuous emotion attributes are likely"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "related, in this paper, we propose to examine such dependency rela-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "tionships."
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "In order to investigate the relationship between continuous and"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "discrete attributes, we develop neural network models that\njointly"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "predict\ncontinuous\nand discrete\nemotion labels.\nThe motivation"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "behind this is two-fold:\n(a) holistic models for automatic emotion"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "recognition must be able to capture both generic and speciﬁc notions"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "of emotion contained in the continuous and discrete emotion labels,"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "and (b) since discrete and continuous emotion labels may be related"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "to each other, robust and generalizable machine learning models can"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "be built by leveraging this dependency. We propose a multi-task"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "learning framework where continuous and discrete emotion labels"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "are predicted together, but\nindependently of each other. Next, we"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "consider the possibility that knowledge of the discrete emotion might"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "help predict more accurately the continuous emotion and vice versa"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "and hence introduce hierarchical multi-task models that model such"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "a relationship to jointly predict discrete and continuous emotion."
        },
        {
          "Carnegie Mellon University": "We further demonstrate that\nthese continuous and discrete la-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "bels need not necessarily be manually annotated within the same"
        },
        {
          "Carnegie Mellon University": "corpus to be improve recognition performance. Of\nthe prevailing"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "annotated datasets for emotion recognition, very few are annotated"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "for both continuous and discrete attributes such as MSPPodcast [9],"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "and IEMOCAP[10] while others like MELD[11] are annotated for"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "discrete attributes only. We demonstrate that even if\nthe model\nis"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "trained on continuous emotion labels from MSPPodcast and discrete"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "labels\nfrom IEMOCAP,\nthe proposed Hierarchical Multi-Task ap-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "proach improves performance and generalizability. This implies that"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "emotion recognition datasets can be trained jointly on multiple cor-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "pora with different labels. In summary, this paper makes the follow-"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "ing contributions:"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "1. We propose a multi-task learning framework that jointly predicts"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "the continuous and discrete labels from speech"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "2. We extend this framework to model hierarchical dependencies,"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "where knowledge of discrete attributes aids continuous prediction"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "and vice versa."
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "3. We demonstrate that\nthe proposed approach can be used when"
        },
        {
          "Carnegie Mellon University": ""
        },
        {
          "Carnegie Mellon University": "the mis-matched labels (continuous and discrete) are drawn from"
        },
        {
          "Carnegie Mellon University": "different datasets."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "lefT)."
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "Given the true discrete label y, the model is optimized using the"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "multi-class cross entropy criterion. For a dataset with A utterances,"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "the cross-entropy loss is computed as:"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "A(cid:88) i\nK(cid:88) c\n(1)\nyi,c log(ˆyi,c)\nLdisc = −"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "=1\n=1"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "Baseline Continuous Model"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "To predict the continuous attribute labels independently from the in-"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "put speech, our encoder-decoder model, called Baseline C uses a"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "continuous emotion pooling layer and MLP to predict\nthe continu-"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "ous emotion ˆc = [ˆcV al, ˆcAro, ˆcDom]."
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "The variables ˆcV al, ˆcAro, and ˆcDom correspond to the valence,"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "arousal and dominance predictions respectively."
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "This model is optimized using the Concordance Correlation Co-"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "efﬁcient (CCC) Loss. Given the prediction ˆc and the ground truth c,"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "the CCC loss is deﬁned as shown in Equation 2."
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "2scˆc"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "(2)\nLccc = 1 −"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "s2\nc + s2\nc + (¯c − ¯ˆc)2"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "the covariance between the ground\nwhere scˆc, s2\nc, s2\nc, ¯c, ¯ˆc represent"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "truth and prediction, variance of\nthe ground truth, variance of\nthe"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "prediction, mean of the groundtruth and mean of the prediction re-"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "spectively.The resulting loss for continuous emotion recognition can"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": ""
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "be written as the sum of\nthe CCC losses for valence, arousal and"
        },
        {
          "discrete label ˆy as its output.\nThe model\nis shown in Fig.\n1 (top": "dominance prediction."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Emotion Recognition Results on IEMOCAP using 5-",
      "data": [
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "all,Valence,Activation and Dominance"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "emotions, and Unweighted accuracy is reported for discrete emotion"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "CCC"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "0.580"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "-"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "0.603"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "0.667"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": "0.648"
        },
        {
          "fold cross-validation: Concordance Correlation Coefﬁcient- over-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Emotion Recognition Results on IEMOCAP using 5-",
      "data": [
        {
          "direct dependency between the discrete and continuous emotion at-": "tributes. Based on the hypothesis that knowledge of discrete emo-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "for 15,000 warmup steps."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "tion attributes would help improve continuous attribute predictions",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "and vice-versa, we develop hierarchical multi-task models.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "3.3. Evaluation Metrics"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "In this model,\nthe continuous and discrete emotion prediction",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Discrete\nemotion recognition is\nevaluated using F1 or\naccuracy."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "branches in the decoder are used to generate the continuous and dis-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "For\ncontinuous prediction, we\ncompute\nthe Concordance Corre-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "crete embeddings EC and ED respectively, as in the multi-task for-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "lation Coefﬁcient\nfor\neach of\nthe\nattributes\narousal,\nvalence\nand"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "mulation. However, here, the predicted continuous emotion, i.e., ˆc is",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "dominance,\nand\nconsequently\ntheir mean.\nEquation\n(4)\nshows"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "In other words,\nnot computed solely based on EC , but also on ED.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "how the CCC is computed given prediction ˆc, and ground truth c,"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "the discrete emotion embedding is used in conjunction with the con-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the covariance between the ground\nwhere scˆc, s2\nc, s2\nc, ¯c, ¯ˆc represent"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "tinuous embeddding to predict the continuous emotion. We term this",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "truth and prediction, variance of\nthe ground truth, variance of\nthe"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "our Hierarchical D-C model since the discrete embedding is used",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "prediction, mean of\nthe groundtruth and mean of\nthe prediction"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "as auxiliary input\nto predict\nthe continuous emotion. Similarly, one",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "respectively."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "can deﬁne a Hierarchical C-D model where the continuous emotion",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "embedding is used as auxiliary input to predict the discrete emotion.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "2scˆc"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "The Hierarchical D-C model\nis shown in Fig.\n1 (right).\nTo",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "CCC =\n(4)"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "s2\nc + s2\nc + (¯c − ¯ˆc)2"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "perform continuous emotion prediction with the help of the discrete",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "predictions, discrete emotion embedding ED, (which is used to pre-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "dict the discrete emotion ˆy) is concatenated with the continuous em-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Table 1.\nEmotion Recognition Results on IEMOCAP using 5-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "bedding EC . This concatenated embedding [EDEC ] is then used to",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "fold cross-validation: Concordance Correlation Coefﬁcient- over-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "predict the continuous emotion, ˆc. Similarly, in the Hierarchical C-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "all,Valence,Activation and Dominance\nis\nreported for\ncontinuous"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "D model,\nin order to utilize continuous attributes to predict discrete",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "emotions, and Unweighted accuracy is reported for discrete emotion"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "attributes, EC is concatenated with ED and used to predict ˆy.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "prediction"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Model Description\nCCC\nCCC-V\nCCC-A\nCCC-D\nAcc"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "3. EXPERIMENTS",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Baseline C\n0.580\n0.548\n0.606\n0.566\n-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "3.1. Data",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Baseline D\n-\n-\n-\n-\n0.737"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Multi-task C,D\n0.603\n0.571\n0.669\n0.567\n0.723"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "Our experiments are conducted on the MSPPodcast[12] and IEMO-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "0.667\n0.660\n0.717\n0.625\nHierarchical D-C\n0.744"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "CAP [13].\nMSP Podcast\nis\nthe\nlargest human labelled emotion",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "0.749\nHierarchical C-D\n0.648\n0.651\n0.694\n0.599"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "dataset with 29,965 training examples and 10,013 test utterances.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "It\nis comprised of segments of speech from podcasts, which means",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "that\nthe\nspeech is\nexpressive\nand the\nacoustic\nenvironment\nless",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "3.4. Results on IEMOCAP"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "constrained by background noise and other interference. The MSP-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "Podcast data has labels for\nthree continuous dimensions - valence,",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Table 1 shows our experimental\nresults on the IEMOCAP dataset,"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "arousal\nand dominance,\nand for multiple discrete\nemotions\n- of",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "where all\nresults are computed using standard 5-fold cross valida-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "which we use ﬁve :\nneutral,\nangry, happy,\nsad,\nand disgust.\nIn",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "tion [17]. We observe that multi-task training improves CCC by"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "this work, we report our\nresults on the balanced test1 evaluation",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "0.02 over the continuous baseline, however,\nit does not outperform"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "set with 30 male and 30 female speakers.\nThe second dataset we",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the discrete baseline. We contend that\nthis is because both of these"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "use is IEMOCAP which is a 20 hour dataset, with annotations on",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "predictions are being generated independent of the other."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "acted emotion.\nIt comprises of 10 different speakers, 5 male and",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "The proposed hierarchical models outperform the baselines and"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "5 female.\nIt\nis labelled for\nthree continuous dimensions - valence,",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "multi-task model on discrete\nand continuous emotion prediction."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "arousal and dominance, and for 9 discrete emotions out of which we",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Speciﬁcally the hierarchical model that uses discrete attributes to aid"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "utilise the ones that overlap with our MSPPodcast set,\ni.e., neutral,",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the prediction of continuous attributes, i.e., Hierarchical D-C outper-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "angry, happy, sad, and disgust.",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "forms the multi-task model and the continuous baselines on CCC -"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "gaining 0.087 absolute CCC. This is perhaps because knowledge of"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "3.2. Model Hyperparameters",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the discrete emotion aids in the prediction of dominance and arousal."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "We also observe an absolute 0.8% improvement on accuracy in Hier-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "All our models are built using the ESPNet[14] toolkit, and will be",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "archical D-C. The hierarchical model that uses continuous emotions"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "publicly released to encourage further research. We use HUBERT-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "to help predict discrete attributes, i.e., Hierarchical C-D outperforms"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "large\n[15]\nembeddings\nas\nthe\ninput\nfeatures\nand a 4 layer\ncon-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the other 3 models on accuracy. This reafﬁrms our hypothesis that"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "former encoder with 64 hidden units for our models. We freeze the",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "knowledge of discrete\nand continuous\nemotion attributes\ncan be"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "HUBERT-large frontend,\nand train the\nconformer\nand pooling",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "used to improve performance on continuous and discrete emotion"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "decoder parameters. We employ separate self-attentive pooling[16]",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "prediction respectively. We note that\nthe Hierarchical C-D model"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "layers for continuous and discrete emotion prediction so the model",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "improves the recognition of discrete emotion attributes compared to"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "can learn which frames to focus on in order to make emotion pre-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "the multi-task model and the continuous baseline."
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "dictions. Our decoders consist of linear projections that map from",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "the encoded output dimension of 768 to 64, 32 and consequently the",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "3.5. Results on MSPPodcast"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "output sizes of 3 for the continuous prediction and 5 for the discrete",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": ""
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "prediction. We use a ReLU activation to ensure that\nthe predicted",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "Table 3 reports the results of experiments on MSPPodcast. The con-"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "continuous attributes are strictly positive and use LeakyReLU else-",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "tinuous and discrete emotion prediction baselines obtain comparable"
        },
        {
          "direct dependency between the discrete and continuous emotion at-": "where. We use a dropout of 0.2 in the decoder. Our models are",
          "trained with the Adam optimizer, and a peak learning rate of 1e-3": "scores to state-of-the-art approaches[18]."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: , we observe that the best CCC on IEMOCAP is achieved",
      "data": [
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "cient(CCC) - overall,Valence,Activation and Dominance is reported"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "on the test1 evaluation set. For discrete emotions, unweighted F1 is"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "reported."
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Model Description"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Baseline C"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Baseline D"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Multi-task C,D"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Hierarchical D-C"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": "Hierarchical C-D"
        },
        {
          "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: , we observe that the best CCC on IEMOCAP is achieved",
      "data": [
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "MSPPodcast datasets respectively. CCC is reported on IEMOCAP Session05 and on the test1 evaluation sets for MSPPodcast respectively."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "F1 is reported on the test1 evaluation set for MSPPodcast and unweighted accuracy on Session05 for IEMOCAP (we use this because these"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "are standard metrics reported for these datasets)."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "IEMOCAP Train\nID"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "1\n-"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "2\nCont."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "3\n-"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "4\nDisc."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "5\nCont."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "6\nDisc."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "7\nCont."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "8\nDisc."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Table 3. Results on MSPPodcast: Concordance Correlation Coefﬁ-"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "cient(CCC) - overall,Valence,Activation and Dominance is reported"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "on the test1 evaluation set. For discrete emotions, unweighted F1 is"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "reported."
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Model Description\nCCC"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Baseline C\n0.593"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Baseline D\n-"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Multi-task C,D\n0.587"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "0.617"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Hierarchical D-C"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": "Hierarchical C-D\n0.605"
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        },
        {
          "Table 2. Results on MSPPodcast and IEMOCAP: The second two columns represent the attributes used in training from the IEMOCAP and": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-"
        },
        {
          "5. REFERENCES": "[1]\nJ.C. Borod, The Neuropsychology of Emotion, Affective Sci-",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "plin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya"
        },
        {
          "5. REFERENCES": "ence. Oxford University Press, USA, 2000.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Renduchintala,\nand Tsubasa Ochiai,\n“ESPnet:\nEnd-to-end"
        },
        {
          "5. REFERENCES": "[2] Robert Plutchik,\n“A general psychoevolutionary theory of",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "speech processing toolkit,”\nin Proceedings of\nInterspeech,"
        },
        {
          "5. REFERENCES": "emotion,” Theories of emotion, vol. 1, pp. 3–31, 1980.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "2018, pp. 2207–2211."
        },
        {
          "5. REFERENCES": "[3]\nJ.A. Russell, “A circumplex model of affect,” Journal of per-",
          "[14]": "[15] Wei-Ning Hsu,",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Benjamin Bolte,\nYao-Hung Hubert\nTsai,"
        },
        {
          "5. REFERENCES": "sonality and social psychology, vol. 39, no. 6, pp. 1161–1178,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman"
        },
        {
          "5. REFERENCES": "1980.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Mohamed,\n“Hubert:\nSelf-supervised speech representation"
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "IEEE/ACM\nlearning by masked prediction of hidden units,”"
        },
        {
          "5. REFERENCES": "[4]\nJames A Russell,\n“Culture and the categorization of emo-",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Transactions on Audio, Speech, and Language Processing, vol."
        },
        {
          "5. REFERENCES": "tions.,” Psychological bulletin, vol. 110, no. 3, pp. 426, 1991.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "29, pp. 3451–3460, 2021."
        },
        {
          "5. REFERENCES": "[5] Henrik Kessler, Alexander Festini, Harald C Traue, Suzanne",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "[16]",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Pooyan Safari, Miquel\nIndia,\nand Javier Hernando,\n“Self-"
        },
        {
          "5. REFERENCES": "Filipic, Michael Weber,\nand Holger Hoffmann,\n“Simplex–",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "attention encoding and pooling for speaker recognition,” Proc."
        },
        {
          "5. REFERENCES": "Affective Com-\nsimulation of personal emotion experience,”",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Interspeech 2020, pp. 941–945, 2020."
        },
        {
          "5. REFERENCES": "puting, pp. 255–270, 2008.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "[17] Edmilson Morais,",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Ron Hoory, Weizhong\nZhu,\nItai Gat,"
        },
        {
          "5. REFERENCES": "[6] Holger Hoffmann, Andreas\nScheck,\nTimo\nSchuster,\nStef-",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Matheus Damasceno, and Hagai Aronowitz, “Speech emotion"
        },
        {
          "5. REFERENCES": "fen Walter, Kerstin Limbrecht, Harald C Traue, and Henrik",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "recognition using self-supervised features,” in ICASSP 2022 -"
        },
        {
          "5. REFERENCES": "Kessler,\n“Mapping discrete\nemotions\ninto the dimensional",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "2022 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "5. REFERENCES": "space: An empirical approach,”\nin 2012 IEEE International",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Signal Processing (ICASSP), 2022, pp. 6922–6926."
        },
        {
          "5. REFERENCES": "Conference on Systems, Man, and Cybernetics (SMC). IEEE,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "2012, pp. 3316–3320.",
          "[14]": "[18]",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Sundararajan Srinivasan, Zhaocheng Huang, and Katrin Kirch-"
        },
        {
          "5. REFERENCES": "[7]\nSven Buechel and Udo Hahn, “A ﬂexible mapping scheme for",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "hoff,\n“Representation learning through cross-modal\ncondi-"
        },
        {
          "5. REFERENCES": "discrete and dimensional emotion representations: Evidence",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "tional teacher-student training for speech emotion recognition,”"
        },
        {
          "5. REFERENCES": "from textual stimuli,” in CogSci 2017—Proceedings of the 39th",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "in ICASSP 2022 - 2022 IEEE International Conference on"
        },
        {
          "5. REFERENCES": "Annual Meeting of\nthe Cognitive Science Society, 2017, pp.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Acoustics, Speech and Signal Processing (ICASSP), 2022, pp."
        },
        {
          "5. REFERENCES": "180–185.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "6442–6446."
        },
        {
          "5. REFERENCES": "[8] Mari´an Trnka, Sakhia Darjaa, Marian Ritomsk`y, R´obert Sabo,",
          "[14]": "[19] Ramon Sanabria and Florian Metze,",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "“Hierarchical multitask"
        },
        {
          "5. REFERENCES": "Milan Rusko, Meilin Schaper, and Tim H Stelkens-Kobsch,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "learning with ctc,” in 2018 IEEE Spoken Language Technology"
        },
        {
          "5. REFERENCES": "“Mapping discrete\nemotions\nin the dimensional\nspace:\nan",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": "Workshop (SLT), 2018, pp. 485–490."
        },
        {
          "5. REFERENCES": "acoustic approach,”\nElectronics, vol. 10, no. 23, pp. 2950,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "2021.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "[9] Reza Lotﬁan and Carlos Busso,\n“Building naturalistic emo-",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "tionally balanced speech corpus by retrieving emotional speech",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "IEEE Transactions on Af-\nfrom existing podcast recordings,”",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "fective Computing, vol. 10, no. 4, pp. 471–483, 2017.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "[10] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Chang, Sungbok Lee, and Shrikanth S. Narayanan, “Iemocap:",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Lan-\ninteractive emotional dyadic motion capture database.,”",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "guage Resources and Evaluation, vol. 42, no. 4, pp. 335–359,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "2008.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "[11]\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Gautam Naik, Erik Cambria, and Rada Mihalcea,\n“MELD:",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "A multimodal multi-party dataset\nfor emotion recognition in",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "the 57th Annual Meeting of\nconversations,”\nin Proceedings of",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "the Association for Computational Linguistics, Florence, Italy,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "July 2019, pp. 527–536, Association for Computational Lin-",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "guistics.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "[12] R. Lotﬁan and C. Busso,\n“Building naturalistic emotionally",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "balanced speech corpus by retrieving emotional speech from",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "IEEE Transactions on Affective\nexisting podcast recordings,”",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Computing, vol. 10, no. 4, pp. 471–483, October-December",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "2019.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "[13] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "Lan-\nInteractive emotional dyadic motion capture database,”",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        },
        {
          "5. REFERENCES": "2008.",
          "[14]": "",
          "Shinji Watanabe,\nTakaaki Hori,\nShigeki Karita,\nTomoki": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The Neuropsychology of Emotion, Affective Science",
      "authors": [
        "J Borod"
      ],
      "year": "2000",
      "venue": "The Neuropsychology of Emotion, Affective Science"
    },
    {
      "citation_id": "3",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "4",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "5",
      "title": "Culture and the categorization of emotions",
      "authors": [
        "Russell James"
      ],
      "year": "1991",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "6",
      "title": "Simplexsimulation of personal emotion experience",
      "authors": [
        "Henrik Kessler",
        "Alexander Festini",
        "Harald Traue",
        "Suzanne Filipic",
        "Michael Weber",
        "Holger Hoffmann"
      ],
      "year": "2008",
      "venue": "Simplexsimulation of personal emotion experience"
    },
    {
      "citation_id": "7",
      "title": "Mapping discrete emotions into the dimensional space: An empirical approach",
      "authors": [
        "Holger Hoffmann",
        "Andreas Scheck",
        "Timo Schuster",
        "Steffen Walter",
        "Kerstin Limbrecht",
        "Harald Traue",
        "Henrik Kessler"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "8",
      "title": "A flexible mapping scheme for discrete and dimensional emotion representations: Evidence from textual stimuli",
      "authors": [
        "Sven Buechel",
        "Udo Hahn"
      ],
      "year": "2017",
      "venue": "CogSci 2017-Proceedings of the 39th Annual Meeting of the Cognitive Science Society"
    },
    {
      "citation_id": "9",
      "title": "Mapping discrete emotions in the dimensional space: an acoustic approach",
      "authors": [
        "Marián Trnka",
        "Sakhia Darjaa",
        "Marian Ritomskỳ",
        "Róbert Sabo",
        "Milan Rusko",
        "Meilin Schaper",
        "Tim Stelkens-Kobsch"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "10",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "12",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "ESPnet: End-to-end speech processing toolkit",
      "authors": [
        "Shinji Watanabe",
        "Takaaki Hori",
        "Shigeki Karita",
        "Tomoki Hayashi",
        "Jiro Nishitoba",
        "Yuya Unno",
        "Nelson Enrique",
        "Yalta Soplin",
        "Jahn Heymann",
        "Matthew Wiesner",
        "Nanxin Chen",
        "Adithya Renduchintala",
        "Tsubasa Ochiai"
      ],
      "year": "2018",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Selfattention encoding and pooling for speaker recognition",
      "authors": [
        "Pooyan Safari",
        "Miquel India",
        "Javier Hernando"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "Sundararajan Srinivasan",
        "Zhaocheng Huang",
        "Katrin Kirchhoff"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Hierarchical multitask learning with ctc",
      "authors": [
        "Ramon Sanabria",
        "Florian Metze"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    }
  ]
}