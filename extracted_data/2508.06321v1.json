{
  "paper_id": "2508.06321v1",
  "title": "Emoaugnet: A Signal-Augmented Hybrid Cnn-Lstm Framework For Speech Emotion Recognition",
  "published": "2025-08-06T16:28:27Z",
  "authors": [
    "Durjoy Chandra Paul",
    "Gaurob Saha",
    "Md Amjad Hossain"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing emotional signals in speech has a significant impact on enhancing the effectiveness of human-computer interaction (HCI). This study introduces EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term Memory (LSTM) layers with one-dimensional Convolutional Neural Networks (1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and variety of the features that are taken from speech signals have a significant impact on how well SER systems perform. A comprehensive speech data augmentation strategy was used to combine both traditional methods, such as noise addition, pitch shifting, and time stretching, with a novel combination-based augmentation pipeline to enhance generalization and reduce overfitting. Each audio sample was transformed into a high-dimensional feature vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient (MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a weighted accuracy of 95.78% and unweighted accuracy of 92.52% on the IEMOCAP dataset and, with ELU activation, has a weighted accuracy of 96.75% and unweighted accuracy of 91.28%. On the RAVDESS dataset, we get a weighted accuracy of 94.53% and 94.98% unweighted accuracy for ReLU activation and 93.72% weighted accuracy and 94.64% unweighted accuracy for ELU activation. These results highlight EmoAugNet's effectiveness in improving the robustness and performance of SER systems through integated data augmentation and hybrid modeling.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion detection refers to the task of identifying a person's emotional state, such as anger, fear, neutrality, happiness, disgust, or sadness  [1] . Speech Emotion Recognition (SER), which records and interprets speech, is essential for enhancing human-computer interaction  [2] . Several applications-like intelligent robotics, audio monitoring, law enforcement, smart home control, and content recommendation systems-depend on recognizing the user's emotions through speech  [3] . In the past few years, deep learning has investigated tremendous advances in speech recognition  [4] -  [7] . However, compared to general speech recognition, there are fewer large-scale datasets available for speech emotion recognition, which makes data augmentation an important research topic to evaluate its effect on SER performance  [8] . In the field of SER, EmoAugNet focused on addressing the data shortage problem through integated data augmentation and enhancing the robustness and performance of SER systems by hybrid modeling. We conducted our experiments on two widely used emotional speech datasets: RAVDESS  [9]  and IEMOCAP  [10]  to evaluate the efficiency of our proposed model. Key contributions of this research include:\n\n• Introduction of a hybrid deep learning framework for efficient SER that combines LSTM networks with onedimensional Convolutional Neural Networks (1D-CNN). • Development of a comprehensive combination-based speech data augmentation pipeline consisting of noise injection with pitch shifting as well as both slow and fast time stretching and signal shifting address the training data shortage problem. • Investigation into the impact of activation functions on SER performance, with observations highlighting their varying effectiveness across different datasets. To extract features, we utilized ZCR, RMSE, and MFCC to measure temporal characteristics, energy distribution and spectral patterns of speech signals. To train the proposed model, each original and augmented sample was converted into a high-dimensional feature vector. For the purpose of this study, we focused on classifying seven distinct emotions that are consistently present across both datasets: neutral, surprise, disgust, fear, sad, happy, and angry.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "In the existing literature, a significant number of researchers have developed speech-based emotion recognition systems using CNNs, RNNs, DNNs, and LSTMs  [11] . In addition to these standalone approaches, some studies have investigated the effectiveness of combining multiple architectures to capture both spatial and temporal features more comprehensively  [12]    [13] .  Etienne et al. (2018)  suggested a hybrid neural model that integrates convolutional layers to extract spectral features and LSTM layers to capture temporal relationships in speech signals. The study used the IEMOCAP dataset and 80% accuracy on IEMOCAP and RAVDESS, respectively  [15] . The majority of the researchers believe that continuous speech characteristics, such as energy and pitch, aid in conveying the feelings that underlies what is being said  [16] -  [18] . Speech emotion recognition has made extensive use of continuous speech features. For instance, Banse et al. investigated vocal across 14 emotion categories  [19] . Fundamental frequency (F0), energy levels, articulation rate, and spectral properties were among the attributes they concentrated on. Anusha Koduru et al. extracted a comprehensive set of speech features-including MFCC, pitch, energy, ZCR, and DWT -to capture emotional characteristics. Their approach achieved up to 85% accuracy on the RAVDESS dataset using DTree  [20] . For data augmentation in Speech Emotion Recognition (SER), Atmaja et al. (2022) applied several techniques, including glottal source extraction, speech cleaning, impulse response, and noise addition, all combined with the original IEMOCAP dataset. The highest unweighted average recall (UAR) of 75.87% was obtained when the original data was augmented using a combination of speech cleaning, impulse response, and noise addition  [8] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Data Augmentation",
      "text": "Both RAVDESS and IEMOCAP are relatively small datasets, which poses a challenge when training neural network models. To address this limitation, data augmentation techniques are commonly employed to compensate for the limited data. In this work, the augmentation process began with the addition of Gaussian noise to the original waveform. The noise was scaled using a constant factor of 0.035 to keep the balance between distortion and realism. It was then multiplied by a random number between 0 and 1 and by the peak amplitude of the signal.\n\n• Pitch shifting was applied using a pitch factor randomly selected from the range [-1, 1], allowing both upward and downward shifts of the original frequency components, thus simulating natural variations in speaker tone. • Time stretching was also implemented to simulate changes in speech tempo: the audio was either slowed down using a stretch rate of 0.9 or sped up using a rate of 1.1, as commonly recommended in prior works  [21] ,  [22] . • Additionally, random temporal shifting was performed by rolling the waveform forward or backward by up to ±5000 samples, introducing timing variability similar to misalignments or speech onset variations in real-world recordings.\n\nThese augmentations were also combined in different ways, such as adding noise to audio that was already pitch-shifted or time-stretched. After applying these augmentations and their combinations, a total of 10 audio samples were created from each original audio file, ( Fig.  1 ).  layers to extract both short-term acoustic patterns and longterm temporal dependencies from speech. After each audio augmentation, features such as the ZCR, RMSE, and MFCC were extracted. These features were computed frame-by-frame to represent the signal's energy, frequency content, and spectral shape, which are essential for distinguishing different emotions ( Fig.  2 ). ZCR helps to capture the noise and frequency characteristics of speech, RMSE reflects signal energy, and MFCC encodes spectral features relevant to emotional tone. These features were stacked sequentially to maintain the temporal structure of the speech signal.\n\nTo learn localized features from these sequences, a series of 1D convolutional layers was used. Smaller kernel sizes (5 and 3) with a stride of 1 allow the model to focus on fine-grained changes in the signal, such as pitch and energy variations. In terms of dimensionality reduction, a max-pooling layer was employed following each CNN layer, with a stride of 2, to retain the most important features of the data. Batch normalization was added after each convolution to stabilize learning and dropout was applied to prevent overfitting.\n\nAs part of architectural experimentation, different activation functions were applied within the convolutional blocks to observe their impact on the model performance. It was found that our model achieved higher accuracy on the RAVDESS dataset when ReLU activation functions were used in the convolutional layers, likely due to the dataset's controlled and high-quality recordings. Conversely, we got improved results on the IEMOCAP dataset when ELU activation functions were employed, the credit can be attributed to ELU's capacity to better handle more expressive, spontaneous, and noisy speech samples.\n\nAfter the convolutional blocks, the input sequence's temporal progression of emotional patterns is captured by two LSTM layers. This helps the model retain and use the information from earlier and later parts of the audio signal. The output is then passed through a series of fully connected (FC) layers with ELU activation functions, which show consistent improvement (around 2-4%)in recognition accuracy compared to the ReLU activation functions' recognition accuracy when tested across multiple datasets. These FC layers help to refine the learned representations before the final softmax layer, which performs multi-class classification into seven emotion categories( Fig.  3 ).  ELU activation function, while replacing ELU with ReLU slightly improved the performance to a weighted accuracy of 94.53% and unweighted accuracy of 94.98%. On the IEMOCAP dataset, the model attained a weighted accuracy of 96.75% and unweighted accuracy of 91.28% with ELU, whereas using ReLU resulted in 95.78% weighted and 92.52% unweighted accuracy. We compared the performance of our Conv1D-LSTM model with a number of cutting-edge methods from the literature in order to further confirm its efficacy ( Table  II ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Experimental Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We present a hybrid Conv1D-LSTM architecture for SER in this work, enhanced by a novel data augmentation pipeline combining noise injection, pitch shifting, time stretching, and temporal shifting. Our approach leverages ZCR, RMSE, and MFCC features to capture spectral and temporal emotion cues, achieving state-of-the-art performance on both IEMOCAP and RAVDESS datasets. By integrating convolutional layers for local feature extraction and LSTM layers for sequential modeling, along with regularization techniques such as batch normalization and dropout, our architecture effectively addresses the challenges of variability in speech signals. In the future, our goal will be to explore multilingual and cross-lingual emotion recognition to assess the adaptability of the model across languages. Furthermore, our objective is to enhance the robustness and practical utility of this model by incorporating more diverse and comprehensive datasets.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data Augmentation Pipeline",
      "page": 2
    },
    {
      "caption": "Figure 2: Feature Extraction",
      "page": 2
    },
    {
      "caption": "Figure 3: Architecture of Conv1D-LSTM model",
      "page": 3
    },
    {
      "caption": "Figure 2: ). ZCR helps to capture the noise and frequency",
      "page": 3
    },
    {
      "caption": "Figure 4: Comparison of confusion matrices on the IEMOCAP",
      "page": 4
    },
    {
      "caption": "Figure 5: Comparison of confusion matrices on the RAVDESS",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer (type)": "Conv1D\nMaxPooling1D\nBatchNormalization\nDropout\nConv1D\nBatchNormalization\nMaxPooling1D\nDropout\nConv1D\nBatchNormalization\nMaxPooling1D\nLSTM\nDropout\nLSTM\nDropout\nDense\nBatchNormalization\nDense\nBatchNormalization\nDense\nBatchNormalization\nDropout\nDense",
          "Output Shape": "(None, 2376, 256)\n(None, 1188, 256)\n(None, 1188, 256)\n(None, 1188, 256)\n(None, 1188, 512)\n(None, 1188, 512)\n(None, 594, 512)\n(None, 594, 512)\n(None, 594, 256)\n(None, 594, 256)\n(None, 297, 256)\n(None, 297, 128)\n(None, 297, 128)\n(None, 128)\n(None, 128)\n(None, 128)\n(None, 128)\n(None, 64)\n(None, 64)\n(None, 32)\n(None, 32)\n(None, 32)\n(None, 7)",
          "Parameters": "1,536\n0\n1,024\n0\n393,728\n2,048\n0\n0\n393,472\n1,024\n0\n197,120\n0\n131,584\n0\n16,512\n512\n8,256\n256\n2,080\n128\n0\n231"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP\nIEMOCAP\nIEMOCAP\nIEMOCAP\nIEMOCAP\nRAVDESS\nRAVDESS\nRAVDESS\nRAVDESS",
          "Method": "CNN–LSTM [23]\nDST [24]\nMFGCN [25]\nProposed (with ReLU)\nProposed (with ELU)\nCNN–LSTM [23]\nMFGCN [25]\nProposed (with ReLU)\nProposed (with ELU)",
          "Classes": "6\n4\n6\n7\n7\n4\n4\n7\n7",
          "Weighted Accuracy": "51.3%\n71.2%\n65.5%\n95.78%\n96.75%\n63.2%\n85.7%\n94.53%\n93.72%",
          "Unweighted Accuracy": "46.5%\n72.9%\n65.8%\n92.52%\n91.28%\n60.7%\n85.1%\n94.98%\n94.64%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion detection using audio data samples",
      "authors": [
        "A Mande",
        "S Dani",
        "S Telang"
      ],
      "year": "2019",
      "venue": "International journal of advanced research in computer science"
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "An ensemble 1d-cnn-lstm-gru model with data augmentation for speech emotion recognition",
      "authors": [
        "M Ahmed",
        "S Islam",
        "A Islam",
        "S Shatabda"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "4",
      "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "authors": [
        "D Amodei",
        "S Ananthanarayanan",
        "R Anubhai",
        "J Bai",
        "E Battenberg",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "Q Cheng",
        "G Chen"
      ],
      "year": "2016",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "5",
      "title": "Improving english conversational telephone speech recognition",
      "authors": [
        "I Medennikov",
        "A Prudnikov",
        "A Zatvornitskiy"
      ],
      "year": "2016",
      "venue": "Improving english conversational telephone speech recognition"
    },
    {
      "citation_id": "6",
      "title": "The ibm 2015 english conversational telephone speech recognition system",
      "authors": [
        "G Saon",
        "H.-K Kuo",
        "S Rennie",
        "M Picheny"
      ],
      "year": "2015",
      "venue": "The ibm 2015 english conversational telephone speech recognition system",
      "arxiv": "arXiv:1505.05899"
    },
    {
      "citation_id": "7",
      "title": "based speech recognition with gated convnets",
      "authors": [
        "V Liptchinsky",
        "G Synnaeve",
        "R Collobert"
      ],
      "year": "2017",
      "venue": "based speech recognition with gated convnets",
      "arxiv": "arXiv:1712.09444"
    },
    {
      "citation_id": "8",
      "title": "Effects of data augmentations on speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "9",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "10",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "13",
      "title": "Eera-asr: An energy-efficient reconfigurable architecture for automatic speech recognition with hybrid dnn and approximate computing",
      "authors": [
        "B Liu",
        "H Qin",
        "Y Gong",
        "W Ge",
        "M Xia",
        "L Shi"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Cnn+ lstm architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "15",
      "title": "Clstm: Deep feature-based speech emotion recognition using the hierarchical convlstm network",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "17",
      "title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection"
    },
    {
      "citation_id": "18",
      "title": "Emotions, speech and the asr framework",
      "authors": [
        "L Bosch"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "20",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "21",
      "title": "Vocal tract length perturbation (vtlp) improves speech recognition",
      "authors": [
        "N Jaitly",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "Proc. ICML workshop on deep learning for audio, speech and language"
    },
    {
      "citation_id": "22",
      "title": "Data augmentation for deep neural network acoustic modeling",
      "authors": [
        "X Cui",
        "V Goel",
        "B Kingsbury"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Dst: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Mfgcn: Multimodal fusion graph convolutional network for speech emotion recognition",
      "authors": [
        "X Qi",
        "Y Wen",
        "P Zhang",
        "H Huang"
      ],
      "year": "2025",
      "venue": "Neurocomputing"
    }
  ]
}