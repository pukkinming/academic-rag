{
  "paper_id": "2410.11522v2",
  "title": "Leveraging Llm Embeddings For Cross Dataset Label Alignment And Zero Shot Music Emotion Prediction",
  "published": "2024-10-15T11:48:31Z",
  "authors": [
    "Renhang Liu",
    "Abhinaba Roy",
    "Dorien Herremans"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we present a novel method for music emotion recognition that leverages Large Language Model (LLM) embeddings for label alignment across multiple datasets and zero-shot prediction on novel categories. First, we compute LLM embeddings for emotion labels and apply non-parametric clustering to group similar labels, across multiple datasets containing disjoint labels. We use these cluster centers to map music features (MERT) to the LLM embedding space. To further enhance the model, we introduce an alignment regularization that enables dissociation of MERT embeddings from different clusters. This further enhances the model's ability to better adaptation to unseen datasets. We demonstrate the effectiveness of our approach by performing zero-shot inference on a new dataset, showcasing its ability to generalize to unseen labels without additional training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The task of automatic music emotion recognition has been a long-standing challenge in the field of music information retrieval  (Yang & Chen, 2012; Kim et al., 2010; Kang & Herremans, 2024) . Accurately predicting the emotional impact of music has numerous valuable applications, ranging from enhancing music streaming recommendations to providing more effective tools for music therapists. By understanding the emotional responses evoked by music, we can improve the user experience of music listening platforms, tailoring recommendations to individual preferences and emotional needs. Furthermore, this knowledge can benefit music therapists, enabling them to select and apply music more effectively in their treatments, ultimately leading to better outcomes for their patients  (Agres et al., 2021) .\n\nThe predominant approach in this field has been to model emotions using Russell's two dimensional valence-arousal space  (Russell, 1980) . However, this representation fails to be interpreted by human  (Eerola & Vuoskoski, 2011) . to address this limitation, researchers have explored the use of comprehensive categorical emotional models, such as the Geneva Emotional Music Scale  (Aljanaki et al., 2014) , which encompasses a broad range of discrete emotional attributes. More recent datasets  (Bogdanov et al., 2019; Turnbull et al., 2007)  introduce new label categories, sometimes in the form of free tags, that aim to capture the multifaceted nature of human emotions. Nonetheless, a key challenge in this area is the lack of a unified emotional taxonomy. Different datasets often employ disparate and often incompatible sets of emotional labels, posing a significant obstacle. The heterogeneity of emotion label taxonomies across datasets hinders the ability to effectively compare and combine findings across studies. This impedes our comprehensive understanding of the nuanced emotional responses to music. The challenge of aligning these disparate emotion label taxonomies limits our capacity to develop more comprehensive and robust music emotion recognition models. Traditionally, the majority of studies have focused on training and testing with a single dataset. However, recent advances in large language models have resulted in powerful general-purpose text encoders  (Reimers, 2019)  that can be leveraged to effectively align emotions across diverse datasets. The ability to align emotion labels across disparate datasets is a crucial step towards developing more comprehensive and robust music emotion recognition models that can generalise to a wider range of emotional experiences.\n\nIn this work, we present a novel methodology that leverages the powerful representational capabilities of Large Language Model embeddings to enable effective cross-dataset label alignment and facilitate zero-shot inference on new datasets with previously unseen emotion labels. The core of our approach involves computing LLM embeddings for the emotion labels across multiple datasets, and then applying non-parametric clustering to group semantically similar labels together. These cluster centres serve as anchor points that allow us to map the MERT features  (Li et al., 2023)  extracted from music wav files into a common label embedding space, effectively aligning the disparate emotion taxonomies present across datasets. To further enhance the model's performance and ensure it captures the underlying semantic relationships between emotions beyond training data, we introduce an alignment regularization. This regularization encourages music features from differ clusters in the label embedding space to dissociate from each other. This, in turn help the model to generalize better to unseen data and labels. To evaluate the efficacy of our proposed approach, we conduct extensive experiments on three benchmark datasets for music emotion recognition. The results demonstrate substantial improvements in the model's ability to perform zero-shot prediction on a new dataset containing previously unseen emotion labels, showcasing the strong generalization capabilities of our method. This is a crucial step towards developing more robust and comprehensive music emotion recognition models that can be applied to a wider range of emotional experiences beyond the singular datasets used during training.\n\nTo sum up, the key contributions of this work are:\n\n• This work is the first to leverage the capabilities of Large Language Model embeddings to enable robust cross-dataset label alignment for the domain of music emotion recognition. By computing LLM embeddings for emotion labels across datasets and applying non-parametric clustering, we are able to establish a common embedding space that allows us to align disparate emotion taxonomies. • We introduce an alignment regularization that further enhances the model's ability to dissociate music features from distinct emotions. • We demonstrate the ability of our approach to perform zero-shot inference on new datasets with previously unseen emotion labels. This showcases the model's strong generalization capabilities and its potential to be applied to a wider range of emotional experiences.\n\nThe rest of the paper is organized as follows. Related works in music emotion recognition and crossdataset transfer learning are discussed in Section 2. We describe our proposed label alignment and emotion prediction framework in detail in Section 3. Section 4 elaborates on our experimental setup, followed by a comprehensive discussion of the results. Finally we conclude the paper and outline future research directions in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The connection between music and emotions has long been a subject of study, dating back to  (Leonard, 1956)  and  (Hevner, 1935) , who explored how different musical elements evoke specific emotional responses. Over the decades, various frameworks have been proposed to represent emotions in music, ranging from categorical models (e.g., happy, sad, angry) to continuous dimensions like valence and arousal. Valence-arousal models  (Russell, 1980)  and more sophisticated systems such as the Geneva Emotional Music Scale  (Zentner et al., 2008)  have become prominent in recent studies of music emotion recognition (MER).\n\nDespite these advancements, current models often struggle to achieve robust generalization, particularly across diverse datasets. Existing approaches have attempted to improve performance by incorporating advanced encoders like MERT  (Li et al., 2023) , leveraging multi-modal data (e.g., lyrics, MIDI, or video), or introducing personalized models that adapt to listener-specific responses  (Chua et al., 2022; Koh et al., 2022; Sams & Zahra, 2023) . However, these methods primarily explore single dataset approach, i.e., training and testing done on a single dataset. Compared to other fields, such as image recognition or natural language processing, datasets for MER are considerably smaller and more fragmented, making it difficult to develop models that generalize across genres and emotional taxonomies. Datasets such as MTG-Jamendo  (Bogdanov et al., 2019) , and smaller, domain-specific collections like CAL500  (Turnbull et al., 2007)  and Emotify  (Aljanaki et al., 2016)  all use different emotion representation models, focus on distinct musical genres, and are thus often incompatible. This discrepancy in representation and dataset size presents a major challenge to building robust models that can generalize across various emotion taxonomies. Addressing this limitation requires innovative solutions that can overcome the dataset heterogeneity. One promising direction is zero-shot learning, which has shown success in fields such as computer vision  (Xian et al., 2018)  and natural language processing (Brown, 2020). Zero-shot learning models generalize to novel classes or tasks without requiring direct training data for every label. In the context of music emotion recognition, zero-shot learning enables models to predict emotions in datasets with unseen labels or emotion taxonomies, potentially overcoming the generalization bottleneck caused by small, disjoint datasets. Our work builds on these ideas by introducing a novel approach for emotion prediction across music datasets with disjoint label sets. By leveraging large language models (LLMs) to align emotion labels through semantic embeddings and clustering, we bridge the gap between datasets, enabling cross-dataset generalization and zero-shot performance. The next section provides details of the building blocks of our approach.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Present Work",
      "text": "Our approach leverages the power of Large Language Model (LLM) embeddings to align emotion labels across multiple music emotion datasets and to enable zero-shot inference on previously unseen emotion labels. An overview is shown in Figure  1 . This section describes the key components of our methodology, including the label embedding procedure, non-parametric clustering, and the mapping of MERT features (i.e., encoded audio)  (Li et al., 2023)  to the label embedding space.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Label Embedding",
      "text": "For each dataset, we obtain an embedding for each emotion label using a pre-trained LLM. Let L d = {l 1 , l 2 , . . . , l n d } represent the set of emotion labels in dataset d, where n d is the number of labels in dataset d. We compute an embedding for each label l i ∈ L d using the LLM, which produces a vector representation e li ∈ R m , where m is the dimension of the LLM's embedding space:\n\nThis embedding process is performed independently for each dataset. The advantage of using LLM embeddings is that the pre-trained model captures rich semantic relationships between words (emotion labels), which allows for natural grouping of labels even when they differ slightly across datasets. For example, the labels \"happy\" and \"joyful\" are likely to have similar embeddings despite being distinct.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Clustering Of Emotion Embeddings",
      "text": "Once we have the LLM embeddings for the emotion labels from multiple datasets, the next step is to group semantically similar labels. Instead of relying on a predefined number of clusters, we utilize the Mean Shift clustering algorithm  (Cheng, 1995) , a non-parametric clustering technique that automatically determines the number of clusters based on the density of points in the embedding space.\n\nLet E = {e li } N i=1 represent the set of all emotion label embeddings, where N is the total number of unique labels across all datasets. The Mean Shift algorithm identifies clusters by shifting each point towards the mode (the densest region of points) iteratively.\n\nThe algorithm converges when the embedding positions no longer change significantly, resulting in a set of cluster centers C = {c 1 , c 2 , . . . , c K }.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mapping Mert Features To The Label Embedding Space",
      "text": "Features extracted with the Music undERstanding model with large-scale self-supervised Training (MERT)  (Li et al., 2023)  have shown to achieve state-of-the-art performance in various music understanding tasks. MERT features can be seen as a highly expressive and generalizable feature representation for music. We leverage these MERT features as the input representation for music emotion recognition. These features, however, do not directly correspond to the semantic meaning of emotion labels. To bridge this gap, we propose a mapping from the MERT feature space to the LLM emotion embedding space. We employ an attention-based mechanism to map the MERT features into the label embedding space wherein the emotion label embeddings reside. The model ingests MERT features extracted from music and outputs corresponding embeddings that are aligned with the LLM-generated emotion label embeddings. Let x i ∈ R d be the MERT feature matrix for the i-th music sample, where d is the feature dimension. The model uses self-attention mechanisms to process this input, applying two encoder layers and projecting it into the label embedding space. The model can be mathematically described as follows:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input Projection",
      "text": "We select the 3rd, 6th, 9th, and 12th layers from MERT and concatenate them to linearly project to a vector of dimension d model :\n\nThe projected vector z i is passed through multiple self-attention layers, where each layer applies multi-head attention followed by a feedforward neural network and layer normalization. The output of the l-th attention block is:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Output Projection",
      "text": "The final output from the last attention block is linearly projected to the output size m, where m is the dimension of the shared LLM embedding space:\n\nrepresents the projected embedding of the MERT features into the shared emotion label embedding space.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Alignment Loss",
      "text": "To facilitate alignment, we employ a triplet loss based alignment loss that ensures the model learns to bring MERT embeddings closer to the LLM embeddings of their true emotion labels, while pushing them apart from embeddings of incorrect emotion categories. In this formulation:\n\n• The anchor f θ (x i ) represents the MERT feature of the audio sample x i passed through the model. • The positive e LLM (y i ) is the LLM embedding of the true label y i for the sample.\n\n• The negative e LLM (y k ) is the LLM embedding of an incorrect or different emotion label y k (where\n\nThe triplet loss is defined based on cosine similarity:\n\nWhere:\n\n• cos(a, b) is the cosine similarity between vectors a and b,\n\n• margin is a hyperparameter that defines the minimum desired separation between positive and negative pairs, • f θ (x i ) is the model output for the MERT feature x i , • e LLM (y i ) and e LLM (y k ) are the LLM embeddings for the true and incorrect labels, respectively.\n\nThis loss function encourages the model to map MERT features closer to their corresponding emotion label embeddings in the label embedding space while ensuring that they are distinct from embeddings of incorrect labels.Ultimately, this mapping procedure allows us to project music representations into the same semantic space as the emotion labels, facilitating more meaningful and interpretable emotion recognition. By aligning the music features and emotion labels in a label embedding space, we can better capture the nuanced relationships between musical characteristics and emotional responses.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Alignment Regularization",
      "text": "To further improve the alignment between the MERT feature representations and the emotion label embeddings, we introduce an alignment regularization term. This regularization encourages the model to map MERT features with semantically similar emotion labels to nearby positions in the label embedding space. By minimizing the distance between the embeddings of MERT features corresponding to the same or closely related emotion labels, the model is incentivised to position these representations in close proximity within the label embedding space. The alignment regularization is formulated as follows:\n\nLet C k represent the set of samples in cluster k, and x i ∈ C k l , x j ∈ C km be two MERT feature vectors whose corresponding emotion labels belong to the different clusters, i.e., C k l ̸ = C km . The alignment regularization term maximizes the distance between their embeddings f θ (x i ) and f θ (x j ) in the label embedding space. We define the alignment regularization loss as:\n\nWhere:\n\n• f θ (x i ) and f θ (x j ) are the model outputs for the MERT features x i and x j ,\n\n• D represents the cosine distance,\n\n• The sum is taken over all\n\nK is the number of such pairs present in the dataset.\n\nBy minimizing L align , the model encourages MERT features from different labels to dissociate further with each other. The final objective function combines the alignment loss and the regularization term, with a hyperparameter λ controlling the trade-off between these two components:\n\nThe network is trained to minimize L and we posit this regularization helps the model to generalise to unseen emotion labels and enables it to perform zero shot classification on new datasets. To test this, we set up the following zero shot evaluation scenario described in the next subsection.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Zero Shot Classification",
      "text": "We perform zero-shot inference on a new dataset containing disjoint set of labels, some of which can be previously unseen. Given a new emotion label l new , we compute its embedding e lnew using the LLM:\n\nTo predict the top k emotions for a given music sample, we project the MERT features of the sample into the LLM embedding space using the trained network f θ . The predicted emotions ŷ is given by the following equation:\n\nWhere (D) is the cosine distance, k is number of top predictions to select. In our experiments we fix k to 2/3/4 depending on dataset.\n\nIn the next section, we detail our empirical setup and demonstrate the effectiveness of our approach.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "To demonstrate the efficacy of our approach, we evaluate on three distinct emotion recognition datasets. The first subsection provides a concise overview of the datasets. This is then followed by the experimental setup, and finally, we present the results accompanied by a discussion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "For the purpose of evaluating our music emotion recognition approach, we have selected three prominent datasets in this research domain: the MTG-Jamendo Dataset  (Bogdanov et al., 2019) , the Computer Audition Lab 500 dataset  (Turnbull et al., 2007) , and the the Emotify Dataset  (Aljanaki et al., 2016) .\n\nThe MTG-Jamendo dataset is an openly available resource for the task of automatic music tagging. It was constructed using music content hosted on the Jamendo platform, which is licensed under Creative Commons, and incorporating tags provided by the content contributors. This dataset encompasses a vast collection of over 55,000 full-length audio recordings annotated with 56 relevant emotion tags.\n\nThe Computer Audition Lab 500 (CAL500) dataset is a widely utilised dataset in music emotion recognition research. It consists of 500 popular Western songs, each annotated with a standard set of 17 emotion labels by at least 3 human annotators.\n\nLastly, the Emotify dataset is another prominent open dataset for music emotion recognition. It contains 400 music excerpts annotated with 9 emotional categories of the Geneva Emotional Music Scale model, obtained through a crowdsourcing game.\n\nBoth the CAL500 and Emotify datasets feature annotations from multiple users. For the CAL500 dataset, we consider a label as true if its average score is greater than 3 on a scale of 1 to 5. In case none of the emotion score is greater than 3, we select the highest scored emotion. For the Emotify dataset, we fix the selection process by choosing the top 3 rated labels as the true labels. These processed labels are made available online to allow future benchmarking with our work 1  . It is worth noting that the MTG-Jamendo dataset already incorporates multiple emotion tags, and thus all our datasets are inherently multi-label in nature.   (Reimers, 2019) , given the multi-label nature of all three datasets. Specifically, we use the all-MiniLM-L6-v2 model 3  , which has been fine-tuned on 1 billion sentence pairs from a pre-trained MiniLM-L6-H384-uncased model. We leverage the Hugging Face implementation for this approach. The MERT features for each 10s fragment are averaged and fed into a shallow two-layered attention network to project these MERT features to the LLM embedding space. We implement Mean Shift Clustering  (Fukunaga & Hostetler, 1975)  using the scikit-learn library. The k parameter to obtain the actual labels ŷ is set to the average number of labels per instance in the dataset: 2 for Jamendo MTG, 4 for CAL500, and 3 for Emotify. For all the training processes, we ran for 100 epochs with a batch size of 256, utilising the AdamW optimiser. The base learning rate is fixed to 2e - 4  with a ReduceLROnPlateau scheduler monitoring the validation macro F1 score and a minimum learning rate threshold of 1.6e -7 . All models are trained on 4 NVIDIA Tesla V100 DGXS 32 GB GPUs, and the training and evaluation code is implemented using PyTorch and available online 4 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "Our experimental evaluation comprises three distinct phases, each designed to systematically assess the efficacy of our approach for cross-dataset generalization and zero-shot inference. We propose two baselines and finally, an alignment regularization approach:\n\nBaseline 1 -Single Dataset Training: The first baseline experiment assesses the model's performance when trained solely on a single dataset. The evaluation is on the test set of the same dataset to gauge in-domain performance, as well as the test sets of the other two datasets to measure cross-dataset generalization. This baseline shows how well the models can generalise across diverse datasets without any external label alignment or regularization.\n\nBaseline 2 -Clustering of LLM Embeddings: In the second baseline experiment, we exploit alignment of emotion labels by clustering the label embeddings generated by a Large Language Model across two datasets (see Section 3.2). These clusters capture semantically akin emotions.\n\nThe model is then trained on this aligned label space, wherein the target emotion is inferred from the cluster centroids of the LLM embeddings instead of individual labels. This approach leverages the presence of common emotion clusters, thereby enhancing the model's capacity for generalisation across datasets. In this case, the model is trained on two datasets conjointly and evaluated on the test sets of all three datasets.\n\nAlignment Regularization: We build upon the second baseline by incorporating alignment regularization as described in Section 3.4. This regularization technique encourages MERT feature embeddings of semantically similar emotion labels to be mapped to proximate positions within the shared embedding space. This phase aims to refine the model's capacity for generalization to unseen labels and assess the model's zero-shot performance on new dataset with disjoint sets of labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "This section provides a detailed analysis of our results. The evaluation is structured to assess the model's capacity for generalisation from single-dataset training, the enhancement achieved through label clustering utilising LLM embeddings, and the final improvement attained via alignment regularization.\n\nBaseline 1: The first set of experiments evaluates the model's performance when trained solely on a single dataset. Table  2  presents the results, showcasing strong in-domain performance with macro F1 scores of 0.078, 0.380 and 0.448 for MTG-Jamendo, CAL500 and Emotify respectively. The large difference in F1 scores between datasets is partly due to a different number of classes: 56, 17, and 9 respectively. The cross-dataset generalization remains a significant challenge, with performance drops of over 50 percent when evaluating on the test sets of other datasets. For example, MTG-Jamendo performance drops from 0.078 to 0.0202 when the training dataset switches to CAL500.\n\nThe results show that the model's performance declines when evaluated on unseen datasets. This highlights the challenge of training a model solely on a limited set of labels and attempting direct cross-dataset inference without any form of alignment or adaptation. Alignment regularization: The final baseline incorporates alignment regularization to further enhance the model's cross-dataset and zero-shot capabilities. By regularizing the MERT feature embeddings of dissimilar examples based on their label embeddings, the model is encouraged to increase the separation between examples of dissimilar emotion labels. This aims to refine the model's ability to distinguish between distinct emotion categories, even when encountering previously unseen labels. Table  4  presents the detailed results for this phase. We observe that the cross-dataset performance is reduced when compared to Baseline 2. For example, Emotify macro F1 score reduces from 0.5037 to 0.4551 when trained on CAL500 and Emotify combination (row 2, column 3 in Table  3  vs Table  4 ). This is because the regularization term accentuates the dissociation of dissimilar emotion labels during training, which in turn diminishes the model's interpolation capacity between known labels. However, the model's zero-shot performance on the new dataset improves significantly, with the F1 score increasing by 15-20% compared to the previous baselines. Specifically, the zero-shot F1 score on the Emotify dataset increased to 0.4022 from 0.3411 in Baseline 2. Detailed comparisons across different splits are shown in Table  5 . These results demonstrate that the alignment regularisation substantially enhances the model's ability to generalise in zero-shot scenarios, where it must effectively handle completely novel emotion labels.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Trained On",
      "text": "",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our approach.",
      "page": 3
    },
    {
      "caption": "Figure 1: This section describes the key components of our",
      "page": 3
    },
    {
      "caption": "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the",
      "page": 8
    },
    {
      "caption": "Figure 2: plots all the labels from all datasets. A quick inspection of clusters",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "In this work, we present a novel method for music emotion recognition that lever-"
        },
        {
          "ABSTRACT": "ages Large Language Model (LLM) embeddings for label alignment across multi-"
        },
        {
          "ABSTRACT": "ple datasets and zero-shot prediction on novel categories. First, we compute LLM"
        },
        {
          "ABSTRACT": "embeddings for emotion labels and apply non-parametric clustering to group sim-"
        },
        {
          "ABSTRACT": "ilar labels, across multiple datasets containing disjoint labels. We use these cluster"
        },
        {
          "ABSTRACT": "centers to map music features (MERT) to the LLM embedding space. To further"
        },
        {
          "ABSTRACT": "enhance the model, we introduce an alignment regularization that enables disso-"
        },
        {
          "ABSTRACT": "ciation of MERT embeddings from different clusters. This further enhances the"
        },
        {
          "ABSTRACT": "model’s ability to better adaptation to unseen datasets. We demonstrate the ef-"
        },
        {
          "ABSTRACT": "fectiveness of our approach by performing zero-shot"
        },
        {
          "ABSTRACT": "showcasing its ability to generalize to unseen labels without additional training."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "our approach involves computing LLM embeddings for the emotion labels across multiple datasets,"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "and then applying non-parametric clustering to group semantically similar\nlabels together. These"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "cluster centres serve as anchor points that allow us to map the MERT features (Li et al., 2023)"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "extracted from music wav files into a common label embedding space, effectively aligning the dis-"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "parate emotion taxonomies present across datasets. To further enhance the model’s performance and"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "ensure it captures the underlying semantic relationships between emotions beyond training data, we"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "introduce an alignment\nregularization. This regularization encourages music features from differ"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "clusters in the label embedding space to dissociate from each other. This,\nin turn help the model"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "to generalize better to unseen data and labels. To evaluate the efficacy of our proposed approach,"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "we conduct extensive experiments on three benchmark datasets for music emotion recognition. The"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "results demonstrate substantial improvements in the model’s ability to perform zero-shot prediction"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "on a new dataset containing previously unseen emotion labels, showcasing the strong generalization"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "capabilities of our method. This is a crucial step towards developing more robust and comprehen-"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "sive music emotion recognition models that can be applied to a wider range of emotional experiences"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "beyond the singular datasets used during training."
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "To sum up, the key contributions of this work are:"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "• This work is the first\nto leverage the capabilities of Large Language Model embeddings"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "to enable robust cross-dataset\nlabel alignment for the domain of music emotion recogni-"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "tion.\nBy computing LLM embeddings for emotion labels across datasets and applying"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "non-parametric clustering, we are able to establish a common embedding space that allows"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "us to align disparate emotion taxonomies."
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "• We introduce an alignment regularization that further enhances the model’s ability to dis-"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "sociate music features from distinct emotions."
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "• We demonstrate the ability of our approach to perform zero-shot inference on new datasets"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "with previously unseen emotion labels. This showcases the model’s strong generalization"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "capabilities and its potential to be applied to a wider range of emotional experiences."
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "The rest of the paper is organized as follows. Related works in music emotion recognition and cross-"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "dataset\ntransfer learning are discussed in Section 2. We describe our proposed label alignment and"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "emotion prediction framework in detail in Section 3. Section 4 elaborates on our experimental setup,"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "followed by a comprehensive discussion of the results. Finally we conclude the paper and outline"
        },
        {
          "facilitate zero-shot\ninference on new datasets with previously unseen emotion labels. The core of": "future research directions in Section 5."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Overview of our approach.": "limitation requires innovative solutions that can overcome the dataset heterogeneity. One promising"
        },
        {
          "Figure 1: Overview of our approach.": "direction is zero-shot\nlearning, which has shown success in fields such as computer vision (Xian"
        },
        {
          "Figure 1: Overview of our approach.": "et al., 2018) and natural language processing (Brown, 2020). Zero-shot learning models generalize"
        },
        {
          "Figure 1: Overview of our approach.": "to novel classes or\ntasks without\nrequiring direct\ntraining data for every label.\nIn the context of"
        },
        {
          "Figure 1: Overview of our approach.": "music emotion recognition, zero-shot\nlearning enables models to predict emotions in datasets with"
        },
        {
          "Figure 1: Overview of our approach.": "unseen labels or emotion taxonomies, potentially overcoming the generalization bottleneck caused"
        },
        {
          "Figure 1: Overview of our approach.": "by small, disjoint datasets. Our work builds on these ideas by introducing a novel approach for emo-"
        },
        {
          "Figure 1: Overview of our approach.": "tion prediction across music datasets with disjoint\nlabel sets. By leveraging large language models"
        },
        {
          "Figure 1: Overview of our approach.": "(LLMs) to align emotion labels through semantic embeddings and clustering, we bridge the gap be-"
        },
        {
          "Figure 1: Overview of our approach.": "tween datasets, enabling cross-dataset generalization and zero-shot performance. The next section"
        },
        {
          "Figure 1: Overview of our approach.": "provides details of the building blocks of our approach."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "Once we have the LLM embeddings for\nthe emotion labels from multiple datasets,\nthe next step"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "is to group semantically similar labels.\nInstead of relying on a predefined number of clusters, we"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "utilize the Mean Shift clustering algorithm (Cheng, 1995), a non-parametric clustering technique"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "that automatically determines the number of clusters based on the density of points in the embedding"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "space."
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "Let E = {eli}N\ni=1 represent the set of all emotion label embeddings, where N is the total number of"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "unique labels across all datasets. The Mean Shift algorithm identifies clusters by shifting each point"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "towards the mode (the densest region of points) iteratively."
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "The algorithm converges when the embedding positions no longer change significantly, resulting in"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "a set of cluster centers C = {c1, c2, . . . , cK}."
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "3.3\nMAPPING MERT FEATURES TO THE LABEL EMBEDDING SPACE"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "Features extracted with the Music undERstanding model with large-scale self-supervised Training"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "(MERT) (Li et al., 2023) have shown to achieve state-of-the-art performance in various music un-"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "derstanding tasks. MERT features can be seen as a highly expressive and generalizable feature"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "representation for music. We leverage these MERT features as the input representation for music"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "emotion recognition. These features, however, do not directly correspond to the semantic mean-"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "ing of emotion labels. To bridge this gap, we propose a mapping from the MERT feature space to"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "the LLM emotion embedding space. We employ an attention-based mechanism to map the MERT"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "features into the label embedding space wherein the emotion label embeddings reside. The model"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "ingests MERT features extracted from music and outputs corresponding embeddings that are aligned"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "with the LLM-generated emotion label embeddings. Let xi ∈ Rd be the MERT feature matrix for"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "the i-th music sample, where d is the feature dimension. The model uses self-attention mechanisms"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "to process this input, applying two encoder layers and projecting it into the label embedding space."
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "The model can be mathematically described as follows:"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "3.3.1\nINPUT PROJECTION"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "We select the 3rd, 6th, 9th,\nand 12th layers from MERT and concatenate them to linearly"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "project to a vector of dimension dmodel:"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "(cid:16)\n(cid:17)"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": ", x(6)\n, x(9)\n, x(12)\n)\n,\nConcat(x(3)\nzi ∈ Rdmodel\nzi = Linearproj"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "i\ni\ni\ni"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "3.3.2\nSELF-ATTENTION"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "is passed through multiple self-attention layers, where each layer applies\nThe projected vector zi"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "multi-head attention followed by a feedforward neural network and layer normalization. The output"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "of the l-th attention block is:"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "(cid:17)\n(cid:17)\n(cid:16)\n(cid:16)"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": ",\nl = 1, 2, . . . , L\n+ a(l)\na(l+1)\na(l)\n= LayerNorm\nSelfAttention"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "i\ni\ni"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "where a(0)\n= zi.\ni"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "3.3.3\nOUTPUT PROJECTION"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "The final output from the last attention block is linearly projected to the output size m, where m is"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "the dimension of the shared LLM embedding space:"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": ")\nfθ(xi) = Linearoutput(a(L)"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "Thus, fθ(xi) ∈ Rm represents the projected embedding of the MERT features into the shared emo-"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "tion label embedding space."
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "3.3.4\nALIGNMENT LOSS"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "To facilitate alignment, we employ a triplet loss based alignment loss that ensures the model learns to"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "bring MERT embeddings closer to the LLM embeddings of their true emotion labels, while pushing"
        },
        {
          "3.2\nCLUSTERING OF EMOTION EMBEDDINGS": "them apart from embeddings of incorrect emotion categories. In this formulation:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "model."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "• The positive eLLM(yi) is the LLM embedding of the true label yi for the sample."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "• The negative eLLM(yk) is the LLM embedding of an incorrect or different emotion label"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "yk (where yk ̸= yi)."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "The triplet loss is defined based on cosine similarity:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "1 N\nN(cid:88) i\nLalign =\nmax (0, cos (fθ(xi), eLLM(yi)) − cos (fθ(xi), eLLM(yk)) + margin)"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "=1"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "Where:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "•\ncos(a, b) is the cosine similarity between vectors a and b,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "• margin is a hyperparameter that defines the minimum desired separation between positive"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "and negative pairs,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "•\nfθ(xi) is the model output for the MERT feature xi,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "• eLLM(yi) and eLLM(yk) are the LLM embeddings for the true and incorrect labels, respec-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "tively."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "This loss function encourages the model to map MERT features closer to their corresponding emo-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "tion label embeddings in the label embedding space while ensuring that\nthey are distinct from em-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "beddings of incorrect\nlabels.Ultimately,\nthis mapping procedure allows us to project music repre-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "sentations into the same semantic space as the emotion labels,\nfacilitating more meaningful and"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "interpretable emotion recognition. By aligning the music features and emotion labels in a label em-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "bedding space, we can better capture the nuanced relationships between musical characteristics and"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "emotional responses."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "3.4\nALIGNMENT REGULARIZATION"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "To further\nimprove the alignment between the MERT feature representations and the emotion la-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "bel embeddings, we introduce an alignment regularization term. This regularization encourages the"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "model\nto map MERT features with semantically similar emotion labels to nearby positions in the"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "label embedding space. By minimizing the distance between the embeddings of MERT features cor-"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "responding to the same or closely related emotion labels, the model is incentivised to position these"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "representations in close proximity within the label embedding space. The alignment regularization"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "is formulated as follows:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "Let Ck represent\nthe set of samples in cluster k, and xi ∈ Ckl , xj ∈ Ckm be two MERT feature"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "vectors whose corresponding emotion labels belong to the different clusters,\n̸= Ckm. The\ni.e., Ckl"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "alignment regularization term maximizes the distance between their embeddings fθ(xi) and fθ(xj)"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "in the label embedding space. We define the alignment regularization loss as:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "(cid:88)"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "1 K\n1 − D(fθ(xi), fθ(xj))\nLreg ="
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "(xi,xj )"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "Where:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "•\nfθ(xi) and fθ(xj) are the model outputs for the MERT features xi and xj,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "• D represents the cosine distance,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "̸= Ckm . K is the number of\n• The sum is taken over all xi ∈ Ckl , xj ∈ Ckm such that Ckl"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "such pairs present in the dataset."
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "the model encourages MERT features from different labels to dissociate fur-\nBy minimizing Lalign,"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "ther with each other. The final objective function combines the alignment loss and the regularization"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "term, with a hyperparameter λ controlling the trade-off between these two components:"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "L = Lalign + λLreg"
        },
        {
          "• The anchor fθ(xi) represents the MERT feature of the audio sample xi passed through the": "5"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "to unseen emotion labels and enables it to perform zero shot classification on new datasets. To test"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "this, we set up the following zero shot evaluation scenario described in the next subsection."
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "3.5\nZERO SHOT CLASSIFICATION"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "We perform zero-shot\ninference on a new dataset containing disjoint set of labels, some of which"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "can be previously unseen. Given a new emotion label\nlnew, we compute its embedding elnew using"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "the LLM:"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "elnew = LLM(lnew)"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "To predict the top k emotions for a given music sample, we project the MERT features of the sample"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "into the LLM embedding space using the trained network fθ. The predicted emotions ˆy is given by"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "the following equation:"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "y = arg mink\nD(fθ(x), yi)"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "i"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "Where (D) is the cosine distance, k is number of top predictions to select.\nIn our experiments we"
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "fix k to 2/3/4 depending on dataset."
        },
        {
          "The network is trained to minimize L and we posit this regularization helps the model to generalise": "In the next section, we detail our empirical setup and demonstrate the effectiveness of our approach."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Brief overview of the datasets": "4.2\nEXPERIMENTAL SETUP"
        },
        {
          "Table 1: Brief overview of the datasets": "For our experimental setup, we employ the standard train-test split as recommended by the original"
        },
        {
          "Table 1: Brief overview of the datasets": "authors of the datasets. For the MERT feature extraction, we split the songs in 10 second segments"
        },
        {
          "Table 1: Brief overview of the datasets": "and we utilise the widely-used MERT-v1-95M model available on Hugging Face2. Regarding the"
        },
        {
          "Table 1: Brief overview of the datasets": "LLM embeddings, we select Sentence Transformers (Reimers, 2019), given the multi-label nature"
        },
        {
          "Table 1: Brief overview of the datasets": "of all three datasets. Specifically, we use the all-MiniLM-L6-v2 model3, which has been fine-tuned"
        },
        {
          "Table 1: Brief overview of the datasets": "on 1 billion sentence pairs from a pre-trained MiniLM-L6-H384-uncased model. We leverage the"
        },
        {
          "Table 1: Brief overview of the datasets": "Hugging Face implementation for\nthis approach.\nThe MERT features for each 10s fragment are"
        },
        {
          "Table 1: Brief overview of the datasets": "averaged and fed into a shallow two-layered attention network to project these MERT features to the"
        },
        {
          "Table 1: Brief overview of the datasets": "LLM embedding space. We implement Mean Shift Clustering (Fukunaga & Hostetler, 1975) using"
        },
        {
          "Table 1: Brief overview of the datasets": "the scikit-learn library. The k parameter to obtain the actual\nlabels ˆy is set\nto the average number"
        },
        {
          "Table 1: Brief overview of the datasets": "of labels per instance in the dataset: 2 for Jamendo MTG, 4 for CAL500, and 3 for Emotify. For"
        },
        {
          "Table 1: Brief overview of the datasets": "all\nthe training processes, we ran for 100 epochs with a batch size of 256, utilising the AdamW"
        },
        {
          "Table 1: Brief overview of the datasets": "optimiser. The base learning rate is fixed to 2e−4 with a ReduceLROnPlateau scheduler monitoring"
        },
        {
          "Table 1: Brief overview of the datasets": "the validation macro F1 score and a minimum learning rate threshold of 1.6e−7. All models are"
        },
        {
          "Table 1: Brief overview of the datasets": "trained on 4 NVIDIA Tesla V100 DGXS 32 GB GPUs, and the training and evaluation code is"
        },
        {
          "Table 1: Brief overview of the datasets": "implemented using PyTorch and available online4."
        },
        {
          "Table 1: Brief overview of the datasets": "4.3\nBASELINES"
        },
        {
          "Table 1: Brief overview of the datasets": "Our experimental evaluation comprises three distinct phases, each designed to systematically assess"
        },
        {
          "Table 1: Brief overview of the datasets": "the efficacy of our approach for cross-dataset generalization and zero-shot\ninference. We propose"
        },
        {
          "Table 1: Brief overview of the datasets": "two baselines and finally, an alignment regularization approach:"
        },
        {
          "Table 1: Brief overview of the datasets": "Baseline 1 - Single Dataset Training:\nThe first baseline experiment assesses\nthe model’s per-"
        },
        {
          "Table 1: Brief overview of the datasets": "formance when trained solely on a single dataset.\nThe evaluation is on the test set of\nthe same"
        },
        {
          "Table 1: Brief overview of the datasets": "dataset to gauge in-domain performance, as well as the test sets of the other two datasets to measure"
        },
        {
          "Table 1: Brief overview of the datasets": "cross-dataset generalization. This baseline shows how well the models can generalise across diverse"
        },
        {
          "Table 1: Brief overview of the datasets": "datasets without any external label alignment or regularization."
        },
        {
          "Table 1: Brief overview of the datasets": "Baseline 2 - Clustering of LLM Embeddings:\nIn the second baseline experiment, we exploit"
        },
        {
          "Table 1: Brief overview of the datasets": "alignment of emotion labels by clustering the label embeddings generated by a Large Language"
        },
        {
          "Table 1: Brief overview of the datasets": "Model across two datasets (see Section 3.2). These clusters capture semantically akin emotions."
        },
        {
          "Table 1: Brief overview of the datasets": "The model is then trained on this aligned label space, wherein the target emotion is inferred from the"
        },
        {
          "Table 1: Brief overview of the datasets": "cluster centroids of the LLM embeddings instead of individual\nlabels. This approach leverages the"
        },
        {
          "Table 1: Brief overview of the datasets": "presence of common emotion clusters,\nthereby enhancing the model’s capacity for generalisation"
        },
        {
          "Table 1: Brief overview of the datasets": "across datasets.\nIn this case,\nthe model\nis trained on two datasets conjointly and evaluated on the"
        },
        {
          "Table 1: Brief overview of the datasets": "test sets of all three datasets."
        },
        {
          "Table 1: Brief overview of the datasets": "Alignment Regularization: We build upon the second baseline by incorporating alignment\nreg-"
        },
        {
          "Table 1: Brief overview of the datasets": "ularization as described in Section 3.4.\nThis regularization technique encourages MERT feature"
        },
        {
          "Table 1: Brief overview of the datasets": "embeddings of semantically similar emotion labels to be mapped to proximate positions within the"
        },
        {
          "Table 1: Brief overview of the datasets": "shared embedding space. This phase aims to refine the model’s capacity for generalization to unseen"
        },
        {
          "Table 1: Brief overview of the datasets": "labels and assess the model’s zero-shot performance on new dataset with disjoint sets of labels."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "same cluster. Labels (nodes) from the same cluster are shown connected by an edge. Spatial distance"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "here is irrelevent."
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "4.4\nRESULTS"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "This section provides a detailed analysis of our results. The evaluation is structured to assess the"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "model’s capacity for generalisation from single-dataset training, the enhancement achieved through"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "label clustering utilising LLM embeddings, and the final improvement attained via alignment regu-"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "larization."
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "Baseline 1: The first set of experiments evaluates the model’s performance when trained solely on a"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "single dataset. Table 2 presents the results, showcasing strong in-domain performance with macro F1"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "scores of 0.078, 0.380 and 0.448 for MTG-Jamendo, CAL500 and Emotify respectively. The large"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "difference in F1 scores between datasets is partly due to a different number of classes: 56, 17, and"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "9 respectively. The cross-dataset generalization remains a significant challenge, with performance"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "drops of over 50 percent when evaluating on the test sets of other datasets.\nFor example, MTG-"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "Jamendo performance drops from 0.078 to 0.0202 when the training dataset switches to CAL500."
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "The results show that\nthe model’s performance declines when evaluated on unseen datasets. This"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "highlights the challenge of training a model solely on a limited set of labels and attempting direct"
        },
        {
          "Figure 2: Graph of labels taken from CAL500 and Emotify. Nodes with same color belong to the": "cross-dataset inference without any form of alignment or adaptation."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: presents the detailed results for this phase. We observe that the cross-dataset",
      "data": [
        {
          "Table 4: Macro F1 scores for alignment regularization.": "Train-MTG+CAL"
        },
        {
          "Table 4: Macro F1 scores for alignment regularization.": "Test-EMO"
        },
        {
          "Table 4: Macro F1 scores for alignment regularization.": "0.315"
        },
        {
          "Table 4: Macro F1 scores for alignment regularization.": "0.346"
        },
        {
          "Table 4: Macro F1 scores for alignment regularization.": "0.400 (λ = 2.5)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Comparison of positive and negative formulation of alignment regularization. Train only",
      "data": [
        {
          "such positive pairs.": "(cid:88)"
        },
        {
          "such positive pairs.": "1 K\nD(fθ(xi), fθ(xj))\nLreg’ ="
        },
        {
          "such positive pairs.": "′"
        },
        {
          "such positive pairs.": "(xi,xj )"
        },
        {
          "such positive pairs.": "We carry out small scale experiment using segment level evaluation. By training on a combination"
        },
        {
          "such positive pairs.": "of Jamendo-MTG + CAL500, we observe that\nthe formulation in Section 3.4,\ni.e., negative pair-"
        },
        {
          "such positive pairs.": "based regularization, outperforms positive pair-based formulation by a considerable margin. Results"
        },
        {
          "such positive pairs.": "in Table 6. Based on this, we decided to use negative pair based formulation for all our experiment."
        },
        {
          "such positive pairs.": "Tested on"
        },
        {
          "such positive pairs.": "MTG-Jamendo\nCAL500\nEmotify"
        },
        {
          "such positive pairs.": "Regulizer"
        },
        {
          "such positive pairs.": "Positive\n0.0402\n0.206\n0.246"
        },
        {
          "such positive pairs.": "0.0761\n0.320\n0.402\nNegative"
        },
        {
          "such positive pairs.": "Table 6: Comparison of positive and negative formulation of alignment regularization. Train only"
        },
        {
          "such positive pairs.": "on MTG-Jamendo+CAL500."
        },
        {
          "such positive pairs.": "Discussion: Our findings show that combining the Jamendo-MTG dataset with others consistently"
        },
        {
          "such positive pairs.": "enhanced Jamendo-MTG’s performance in Baseline 2. However,\nthis improvement was not ob-"
        },
        {
          "such positive pairs.": "served across the other datasets. For instance, when integrating Emotify with Jamendo-MTG, Emo-"
        },
        {
          "such positive pairs.": "tify’s performance in Baseline 2 declined in comparison to Baseline 1 (0.448 vs 0.419). This can"
        },
        {
          "such positive pairs.": "be attributed to Jamendo being a more extensive dataset with greater diversity in data and emotion"
        },
        {
          "such positive pairs.": "labels, while Emotify is smaller and focuses on a specific kind of music.\nIncorporating Jamendo-"
        },
        {
          "such positive pairs.": "MTG diluted Emotify’s performance by introducing music of genres that was less representative"
        },
        {
          "such positive pairs.": "of Emotify’s concentrated content consisting of a specific genre. Conversely, adding Emotify to"
        },
        {
          "such positive pairs.": "Jamendo-MTG introduced focused data that enhanced Jamendo-MTG’s performance on that genre."
        },
        {
          "such positive pairs.": "When merging datasets, we should also be mindful of variations in music emotion datasets and the"
        },
        {
          "such positive pairs.": "resulting challenges as outlined by Kang & Herremans (2024). For instance, the added dataset may"
        },
        {
          "such positive pairs.": "contain noisier data, due to cloud annotation instead of expert annotation.\nIt may contain different"
        },
        {
          "such positive pairs.": "genres of music or fragments of different length. There may also be many labels per instance, versus"
        },
        {
          "such positive pairs.": "only one. Our proposed framework can handle data of all these variations, however, the user should"
        },
        {
          "such positive pairs.": "be mindful to focus on high-quality data with similar distribution to the desired target music, in order"
        },
        {
          "such positive pairs.": "achieve a quality improvement in the model."
        },
        {
          "such positive pairs.": "Our\nresults suggest\nthat when aiming to improve a dataset’s performance,\nit\nis advantageous to"
        },
        {
          "such positive pairs.": "augment it with another dataset that may be smaller but contains music of the same character present"
        },
        {
          "such positive pairs.": "in the original dataset.\nIn such cases, Baseline 2 serves as an effective strategy to enhance the"
        },
        {
          "such positive pairs.": "performance of the larger, more diverse dataset."
        },
        {
          "such positive pairs.": "Furthermore,\nfor\ntasks\nrequiring zero-shot\nlearning, alignment\nregularization proves\nto be more"
        },
        {
          "such positive pairs.": "effective. However, when all data and labels are available, Baseline 2 should be preferred, as it tends"
        },
        {
          "such positive pairs.": "to yield better results, particularly for larger datasets like Jamendo-MTG, as described earlier."
        },
        {
          "such positive pairs.": "5\nCONCLUSION"
        },
        {
          "such positive pairs.": "In this paper, we introduce a novel approach to music emotion recognition by integrating Large"
        },
        {
          "such positive pairs.": "Language Model (LLM) embeddings to harmonize label spaces across diverse datasets, and enable"
        },
        {
          "such positive pairs.": "zero-shot learning for new emotion categories. Our method not only effectively clusters and aligns"
        },
        {
          "such positive pairs.": "emotion labels through LLM embeddings but also employs a novel alignment\nregularization, en-"
        },
        {
          "such positive pairs.": "hancing the semantic coherence between music features and emotion labels across disjoint datasets."
        },
        {
          "such positive pairs.": "The experimental results show zero shot performance of 0.402, 0248, 0.262 in terms of macro F1-"
        },
        {
          "such positive pairs.": "score for Emotify, MTG-Jamendo, and CAL500 respectively, setting a new benchmark in the field"
        },
        {
          "such positive pairs.": "of music emotion recognition. This approach opens up possibilities for broader applications in af-"
        },
        {
          "such positive pairs.": "fective computing where emotional nuances can be universally understood and processed across"
        },
        {
          "such positive pairs.": "contextual bounds."
        },
        {
          "such positive pairs.": "10"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "Kat R Agres, Rebecca S Schaefer, Anja Volk, Susan van Hooren, Andre Holzapfel, Simone"
        },
        {
          "REFERENCES": "Dalla Bella, Meinard M¨uller, Martina De Witte, Dorien Herremans, Rafael Ramirez Melendez,"
        },
        {
          "REFERENCES": "et al. Music, computing, and health: a roadmap for the current and future roles of music technol-"
        },
        {
          "REFERENCES": "ogy for health care and well-being. Music & Science, 4:2059204321997709, 2021."
        },
        {
          "REFERENCES": "Anna Aljanaki, Frans Wiering, Remco Veltkamp, et al. Computational modeling of induced emo-"
        },
        {
          "REFERENCES": "the 15th Conference of\nthe International Society for Music\ntion using gems.\nIn Proceedings of"
        },
        {
          "REFERENCES": "Information Retrieval (ISMIR 2014), pp. 373–378, 2014."
        },
        {
          "REFERENCES": "Anna Aljanaki, Frans Wiering, and Remco C Veltkamp. Studying emotion induced by music through"
        },
        {
          "REFERENCES": "a crowdsourcing game.\nInformation Processing & Management, 52(1):115–128, 2016."
        },
        {
          "REFERENCES": "Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The mtg-jamendo"
        },
        {
          "REFERENCES": "dataset for automatic music tagging.\nICML, 2019."
        },
        {
          "REFERENCES": "Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020."
        },
        {
          "REFERENCES": "IEEE transactions on pattern analysis\nYizong Cheng. Mean shift, mode seeking, and clustering."
        },
        {
          "REFERENCES": "and machine intelligence, 17(8):790–799, 1995."
        },
        {
          "REFERENCES": "Phoebe Chua, Dimos Makris, Dorien Herremans, Gemma Roig, and Kat Agres.\nPredicting emo-"
        },
        {
          "REFERENCES": "tion from music videos: exploring the relative contribution of visual and auditory information to"
        },
        {
          "REFERENCES": "affective responses. arXiv:2202.10453, 2022."
        },
        {
          "REFERENCES": "Tuomas Eerola and Jonna K Vuoskoski. A comparison of the discrete and dimensional models of"
        },
        {
          "REFERENCES": "emotion in music. Psychology of Music, 39(1):18–49, 2011."
        },
        {
          "REFERENCES": "Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function, with"
        },
        {
          "REFERENCES": "applications in pattern recognition. IEEE Transactions on information theory, 21(1):32–40, 1975."
        },
        {
          "REFERENCES": "Kate Hevner. The affective character of the major and minor modes in music. The American Journal"
        },
        {
          "REFERENCES": "of Psychology, 47(1):103–118, 1935."
        },
        {
          "REFERENCES": "Jaeyong Kang and Dorien Herremans. Are we there yet? a brief survey of music emotion prediction"
        },
        {
          "REFERENCES": "datasets, models and outstanding challenges. arXiv preprint arXiv:2406.08809, 2024."
        },
        {
          "REFERENCES": "Youngmoo E Kim, Erik M Schmidt, Raymond Migneco, Brandon G Morton, Patrick Richardson,"
        },
        {
          "REFERENCES": "Jeffrey Scott, Jacquelin A Speck, and Douglas Turnbull. Music emotion recognition: A state of"
        },
        {
          "REFERENCES": "the art review.\nIn Proc. ismir, volume 86, pp. 937–952, 2010."
        },
        {
          "REFERENCES": "En Yan Koh, Kin Wai Cheuk, Kwan Yee Heung, Kat R Agres, and Dorien Herremans. Merp:\na"
        },
        {
          "REFERENCES": "music dataset with emotion ratings and raters’ profile information. Sensors, 23(1):382, 2022."
        },
        {
          "REFERENCES": "Meyer Leonard. Emotion and meaning in music. Chicago: University of Chicago, 1956."
        },
        {
          "REFERENCES": "Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao,"
        },
        {
          "REFERENCES": "Chenghua Lin, Anton Ragni, Emmanouil Benetos, et al. Mert: Acoustic music understanding"
        },
        {
          "REFERENCES": "model with large-scale self-supervised training. arXiv preprint arXiv:2306.00107, 2023."
        },
        {
          "REFERENCES": "arXiv preprint\nN Reimers.\nSentence-bert:\nSentence embeddings using siamese bert-networks."
        },
        {
          "REFERENCES": "arXiv:1908.10084, 2019."
        },
        {
          "REFERENCES": "James A Russell. A circumplex model of affect.\nJournal of personality and social psychology, 39"
        },
        {
          "REFERENCES": "(6):1161, 1980."
        },
        {
          "REFERENCES": "Andrew Steven Sams and Amalia Zahra. Multimodal music emotion recognition in indonesian songs"
        },
        {
          "REFERENCES": "based on cnn-lstm, xlnet transformers. Bulletin of Electrical Engineering and Informatics, 12(1):"
        },
        {
          "REFERENCES": "355–364, 2023."
        },
        {
          "REFERENCES": "Douglas Turnbull, Luke Barrington, David Torres, and Gert Lanckriet. Towards musical query-by-"
        },
        {
          "REFERENCES": "semantic-description using the cal500 data set.\nIn Proc. of the 30th annual Int. ACM SIGIR Conf."
        },
        {
          "REFERENCES": "on Research and development in information retrieval, pp. 439–446, 2007."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": "zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recog-"
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": ""
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": "ACM"
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": ""
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": ""
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": ""
        },
        {
          "Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Music, computing, and health: a roadmap for the current and future roles of music technology for health care and well-being",
      "authors": [
        "Rebecca Kat R Agres",
        "Anja Schaefer",
        "Susan Volk",
        "Andre Van Hooren",
        "Simone Holzapfel",
        "Meinard Dalla Bella",
        "Martina Müller",
        "Dorien Witte",
        "Rafael Herremans",
        "Melendez"
      ],
      "year": "2021",
      "venue": "Music & Science"
    },
    {
      "citation_id": "2",
      "title": "Computational modeling of induced emotion using gems",
      "authors": [
        "Anna Aljanaki",
        "Frans Wiering",
        "Remco Veltkamp"
      ],
      "year": "2014",
      "venue": "Proceedings of the 15th Conference of the International Society for Music Information Retrieval (ISMIR 2014)"
    },
    {
      "citation_id": "3",
      "title": "Studying emotion induced by music through a crowdsourcing game",
      "authors": [
        "Anna Aljanaki",
        "Frans Wiering",
        "Remco Veltkamp"
      ],
      "year": "2016",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "4",
      "title": "The mtg-jamendo dataset for automatic music tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "5",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom B Brown"
      ],
      "year": "2020",
      "venue": "Language models are few-shot learners",
      "arxiv": "arXiv:2005.14165"
    },
    {
      "citation_id": "6",
      "title": "Mean shift, mode seeking, and clustering",
      "authors": [
        "Yizong Cheng"
      ],
      "year": "1995",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "7",
      "title": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "authors": [
        "Phoebe Chua",
        "Dimos Makris",
        "Dorien Herremans",
        "Gemma Roig",
        "Kat Agres"
      ],
      "year": "2022",
      "venue": "Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses",
      "arxiv": "arXiv:2202.10453"
    },
    {
      "citation_id": "8",
      "title": "A comparison of the discrete and dimensional models of emotion in music",
      "authors": [
        "Tuomas Eerola",
        "Jonna Vuoskoski"
      ],
      "year": "2011",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "9",
      "title": "The estimation of the gradient of a density function, with applications in pattern recognition",
      "authors": [
        "Keinosuke Fukunaga",
        "Larry Hostetler"
      ],
      "year": "1975",
      "venue": "IEEE Transactions on information theory"
    },
    {
      "citation_id": "10",
      "title": "The affective character of the major and minor modes in music",
      "authors": [
        "Kate Hevner"
      ],
      "year": "1935",
      "venue": "The American Journal of Psychology"
    },
    {
      "citation_id": "11",
      "title": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "authors": [
        "Jaeyong Kang",
        "Dorien Herremans"
      ],
      "year": "2024",
      "venue": "Are we there yet? a brief survey of music emotion prediction datasets, models and outstanding challenges",
      "arxiv": "arXiv:2406.08809"
    },
    {
      "citation_id": "12",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Erik Youngmoo E Kim",
        "Raymond Schmidt",
        "Migneco",
        "Patrick Brandon G Morton",
        "Jeffrey Richardson",
        "Jacquelin Scott",
        "Douglas Speck",
        "Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ismir"
    },
    {
      "citation_id": "13",
      "title": "Merp: a music dataset with emotion ratings and raters' profile information",
      "authors": [
        "En Yan Koh",
        "Kin Cheuk",
        "Kwan Yee Heung",
        "Kat Agres",
        "Dorien Herremans"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Emotion and meaning in music",
      "authors": [
        "Meyer Leonard"
      ],
      "year": "1956",
      "venue": "Emotion and meaning in music"
    },
    {
      "citation_id": "15",
      "title": "Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos"
      ],
      "year": "2023",
      "venue": "Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "16",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Reimers"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "17",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "18",
      "title": "Multimodal music emotion recognition in indonesian songs based on cnn-lstm, xlnet transformers",
      "authors": [
        "Steven Sams",
        "Amalia Zahra"
      ],
      "year": "2023",
      "venue": "Bulletin of Electrical Engineering and Informatics"
    },
    {
      "citation_id": "19",
      "title": "Towards musical query-bysemantic-description using the cal500 data set",
      "authors": [
        "Douglas Turnbull",
        "Luke Barrington",
        "David Torres",
        "Gert Lanckriet"
      ],
      "year": "2007",
      "venue": "Proc. of the 30th annual Int. ACM SIGIR Conf. on Research and development in information retrieval"
    }
  ]
}