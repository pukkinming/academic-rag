{
  "paper_id": "2002.03566v1",
  "title": "Emotion Recognition Using Speaker Cues",
  "published": "2020-02-04T08:20:30Z",
  "authors": [
    "Ismail Shahin"
  ],
  "keywords": [
    "\"Emirati-accented speech database",
    "emotion recognition",
    "hidden Markov model",
    "speaker identification",
    "twostage approach\""
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This research aims at identifying the unknown emotion using speaker cues. In this study, we identify the unknown emotion using a two-stage framework. The first stage focuses on identifying the speaker who uttered the unknown emotion, while the next stage focuses on identifying the unknown emotion uttered by the recognized speaker in the prior stage. This proposed framework has been evaluated on an Arabic Emirati-accented speech database uttered by fifteen speakers per gender. \"Mel-Frequency Cepstral Coefficients (MFCCs) have been used as the extracted features and Hidden Markov Model (HMM)\" has been utilized as the classifier in this work. Our findings demonstrate that emotion recognition accuracy based on the two-stage framework is greater than that based on the one-stage approach and the \"state-of-the-art classifiers and models such as Gaussian Mixture Model (GMM), Support Vector Machine (SVM), and Vector Quantization (VQ)\". The average emotion recognition accuracy based on the two-stage approach is 67.5%, while the accuracy reaches to 61.4%, 63.3%, 64.5%, and 61.5%, based on the one-stage approach, GMM, SVM, and VQ, respectively. The achieved results based on the two-stage framework are very close to those attained in subjective assessment by human listeners.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is made up of two elements: emotion identification and emotion verification. In emotion identification, a speech sample from the unknown emotion is analyzed and competed with models of known emotions. The unknown emotion is recognized as the emotion whose model best fits the input speech signal. In emotion verification, the aim is to decide whether an emotion corresponds to a certain known emotion or to some other unknown emotions  [1] .\n\nEmotion recognition can be utilized in many various applications in \"telecommunications, human robotic interfaces, smart call centers, and intelligent spoken tutoring systems. In telecommunications, emotion recognition emerges in assessing the emotion of a caller for telephone response facilities. Recognizing emotions can also be exploited in human robotic interfaces where robots can be trained to interact with humans and recognize human emotions. Further applications can be observed in smart call centers where probable problems happening from poor communications can be sensed by emotion recognition. Emotion recognition can be used in intelligent verbal tutoring to sense and alter to the emotions of students when students ran into a tedious situation throughout tutoring gatherings\"  [2] ,  [3] ,  [4] .\n\nEmotion recognition typically performs in one of two input forms, \"text-dependent form or text-independent form. In the text-dependent form, emotions supply utterances of the alike text for both training and testing assessments. In the text-independent form, emotions are not limited to give specific texts in evaluation trials\".\n\nIn this work, a two-stage architecture has been proposed, applied, and tested to identify the unknown emotion. The first stage identifies the unknown speaker followed by the second stage that identifies the unknown emotion which is uttered by the recognized speaker in the first stage. This paper is organized as follows: Section II covers the literature review of emotion recognition. Section III explains the used dataset and extraction of features. Section IV details the proposed framework of emotion recognition and the experiments. Section V discusses the experiments and the achieved results. Finally, section VI gives conclusion of this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "In the last three decades, emotion recognition becomes a hot topic in the research community. Emotion recognition accuracy is still imperfect. There are many studies that focus on enhancing emotion recognition accuracy using English and Arabic Corpora  [1, [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] .\n\nEmotion recognition has been investigated in many studies  [1, [5] [6] [7] [8] [9] [10] [11] . Yogesh et.al  [5]  introduced \"a novel particle swarm optimization aided biogeography-based method for feature selection. They did their experiments utilizing Berlin Emotional Speech corpus (BES), Surrey Audio-Visual Expressed Emotion corpus (SAVEE), and Speech Under Simulated and Actual Stress (SUSAS) corpus\". Shahin shed the light in one of his prior work  [6]  on studying and improving \"text-independent and speakerindependent talking condition identification in stressful and emotional environments\" (completely two separate environments) based on three independent and distinct classifiers: \"Hidden Markov Models (HMMs), Second-Order Circular Hidden Markov Models (CHMM2s), and Suprasegmental Hidden Markov Models (SPHMMs)\". His study demonstrated that \"SPHMMs lead HMMs and CHMM2s for emotion recognition in the two environments\"  [6] . Shahin and Ba-Hutair  [7]  investigated in one of their investigations on how to enhance \"text-independent and speakerindependent talking condition recognition in each of stressful and emotional environments based on Second-Order Circular Suprasegmental Hidden Markov Models (CSPHMM2s) as classifiers\". In addition, one of the main targets of their research was to distinguish \"stressful talking environment from emotional talking environment based on CSPHMM2s. They arrived at a judgement that talking recognition in stressful and emotional environments based on CSPHMM2s is a leader to that based on HMMs, CHMM2s, and SPHMMs\". In one of his previous work  [8] , Shahin utilized emotions to recognize the unknown speakers. He proposed a novel approach to recognize speakers from their emotions based on HMMs. In another study by Shahin  [9] , he proposed, implemented, and assessed \"speaker-dependent and text-dependent speaking style authentication (verification) systems that accept or reject the identity claim of a speaking style based on SPHMMs\". His findings, based on SPHMMs, demonstrated that the \"average speaking style authentication performance is: 99%, 37%, 85%, 60%, 61%, 59%, 41%, 61%, and 57% belonging, respectively, to the speaking styles: neutral, shouted, slow, loud, soft, fast, angry, happy, and fearful\". To improve \"speaker verification accuracy in emotional talking environments, Shahin  [10]  introduced a two-stage approach that employs the emotion of speaker cues (text-independent and emotion-dependent speaker verification problem) based on both HMMs and SPHMMs as classifiers\". His architecture is made up of \"two sequential stages that combine and integrate emotion recognizer followed by a speaker recognizer into one recognizer\". His approach has been tested on two different and independent emotional speech corpora: his captured corpus and \"Emotional Prosody Speech and Transcripts (EPST)\" corpus. The results of his study showed that his framework gave better results with a significant improvement over prior work and other approaches such as \"emotion-independent speaker verification approach and emotion-dependent speaker verification approach based entirely on HMMs\". In one more research by Shahin and Bou Nassif  [11] , they targeted to improve emotion recognition accuracy based on \"Third-Order Hidden Markov Models (HMM3s) as a classifier\". Their work has been evaluated on EPST dataset. \"The extracted features of EPST database are Mel-Frequency Cepstral Coefficients (MFCCs)\". Their findings yielded an average emotion recognition accuracy of 71.8%. Shahin  [1]  spotlighted on recognizing the unknown emotion based on the \"Third-Order Circular Suprasegmental Hidden Markov Model (CSPHMM3) as a classifier\". His research has been assessed on EPST dataset. The extracted features of EPST database are the MFCCs. His results gave average emotion recognition accuracy of 77.8% based on the CSPHMM3. The results of his study showed that \"CSPHMM3 is superior to HMM3, Gaussian Mixture Model (GMM), Support Vector Machine (SVM), and Vector Quantization (VQ) by 6.0%, 4.9%, 3.5%, and 5.4%, respectively, for emotion recognition\"  [1] .\n\nThere are not many studies that focus on emotion recognition using Arabic speech databases  [12] [13] [14] . Klaylat et. al  [12]  introduced a two-phase model to enhance emotion recognition system. Their system recognizes three different emotions: happy, angry, and surprised utilizing an Arabic speech database. They implemented thirty five classification models and a Sequential Minimal Optimization (SMO) classifier in their work. Their results yielded 95.52% as an emotion recognition accuracy  [12] . El Gohary et. al  [13]  concerned with emotion detection in Arabic text. Their study is based on a moderate sized Arabic emotion lexicon used to annotate Arabic children stories for six distinct emotions: anger, fear, joy, surprise, sadness, and disgust. They attained 65% as an emotion detection accuracy in Arabic text. Shahin et. al  [14]  focused on \"recognizing text-independent and speaker-independent emotions using Arabic Emiratiaccented speech dataset based on a proposed hybrid classifier called cascaded Gaussian Mixture Model and Deep Neural Network, GMM-DNN, (GMM followed by DNN). Six diverse emotions have been utilized in their study. These emotions are: neutrality, happiness, sadness, disgust, anger, and fear. They achieved 83.97% as an average emotion recognition accuracy using the novel GMM-DNN classifier\"  [14] .\n\nIn this research, we propose, implement, and assess a text-independent two-stage emotion recognizer based on Hidden Markov Model (HMM) as a classifier in each stage. The first stage is nothing but a speaker recognizer followed by an emotion recognizer. Our work has been tested on our captured Arabic Emirati-accented speech database that is made up of fifteen Emirati speakers per gender talking in six different emotions using MFCCs as the extracted features.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Speech Dataset And Extraction Of Features",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Speech Dataset",
      "text": "In this work, our proposed approach has been evaluated on an \"Arabic Emirati-accented speech dataset captured from fifteen local Emirati speakers per gender. Speakers utter eight Emirati sentences that are regularly used in the UAE society. Each sentence is uttered by every speaker nine times under each of neutral, happy, sad, disgust, angry, and fear emotions. Table  1  exhibits the database used in this research where the right column shows the utterances in Emirati dialect while the left column exhibits the English translation of these sentences. This database was picked up in two separated and diverse sessions: training session and testing session. The dataset was recorded in an uncontaminated environment in the College of Communication, University of Sharjah, United Arab Emirates by a set of skilled engineering students. The database was recorded by a speech acquisition board using a 16-bit linear coding A/D converter and sampled at a sampling rate of 44.6 kHz\".",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Extraction Of Features",
      "text": "Static Mel-Frequency Cepstral Coefficients (static MFCCs) and delta MFCCs define the phonetic content of speech signals in our corpus. Such coefficients have been mainly used in many studies of emotion recognition and speaker recognition  [7] ,  [10] ,  [14] ,  [15] ,  [16] ,  [17] ,  [18] . The observation vector in HMM has been expressed by the MFCC feature analysis. A 32-dimension MFCC (16 static MFCCs and 16 delta MFCCs) feature analysis forms the observation vectors in a 6-state HMM model with a continuous mixture observation density.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Proposed Two-Stage Emotion Recogntion Approach And The Experiments",
      "text": "Our proposed two-stage emotion recognizer is given in Fig.  1 . This recognizer is comprised of two cascaded recognizers:\n\nFirst Stage: Speaker Recognizer Based on our proposed approach, the first stage is to identify the unknown speaker who produced the utterance in the unknown emotion (speaker identification problem). In this stage, there are n probabilities that are computed based on HMM and the highest probability is chosen as the recognized speaker as shown in the following formula,  (1)  where, \"S * is the index of the recognized speaker, O is the observation sequence of the unknown speaker who uttered the utterance in an unknown emotion, and is the probability of the observation sequence O of the unknown speaker given the sth HMM speaker model\".\n\nIn this stage, the \"sth HMM speaker model has been derived in the training session for every speaker talking in neutral condition using all the first four sentences of the database with a repetition of nine utterances/sentence. The overall number of utterances used to build every HMM speaker model in this session is 36 (4 sentences × 9 utterances/sentence)\".",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Second Stage: Emotion Recognizer",
      "text": "The second stage of our proposed framework is to identify the unknown emotion given that the unknown speaker was recognized in the first stage (speaker-specific emotion identification problem). In this stage, m probabilities per speaker are calculated based on HMM and the maximum probability is chosen as the identified emotion for the recognized speaker as given in the following formula,\n\nwhere, \"E * is the index of the identified emotion, is the probability of the observation sequence e that corresponds to the unknown emotion given the eth HMM emotion model and the identified speaker\".\n\nThe \"eth HMM emotion model for every speaker has been obtained using nine utterances per sentence (the first four sentences of the corpus). The total number of utterances utilized to get every speaker-dependent HMM emotion model is 36 (4 sentences × 9 utterances/sentence)\".\n\nIn the \"test or identification session (completely independent from the training session), each one of the thirty speakers used nine utterances per sentence (last four sentences of the dataset) under each emotion including the neutral state. The whole number of utterances utilized in this session is 6,480 (30 speakers × 4 sentences × 9 utterances/sentence × 6 emotions)\".",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "In this work, a two-stage approach has been proposed, applied, and tested to enhance emotion recognition performance using our captured Arabic Emirati-accented dataset. MFCC has been used as the extracted features and HMM has been used as the adopted classifier. We evaluated our proposed architecture using six different emotions spoken by fifteen speakers per gender. The six emotions are: \"neutral, happy, sad, disgust, angry, and fear\".\n\nTable  II  shows emotion recognition accuracy using the one-stage framework (identifying the unknown emotion directly without identifying the unknown speaker). This table yields average emotion recognition accuracy of 61.4%. It is evident from this table that the highest emotion identification accuracy happens when speakers speak neutrally, while the least accuracy takes place when speakers speak angrily.\n\nBased on the two-stage approach, the average speaker identification accuracy in the first stage is 72.8%. The emotion identification accuracy in the second stage of our proposed approach is given in Table  III . This table gives average emotion identification accuracy of 67.5%. This table apparently demonstrates that the maximum accuracy occurs when speakers talk neutrally, whereas the minimum accuracy happens when speakers communicate angrily.\n\nTo conform whether \"emotion recognition accuracy differences (emotion recognition accuracy based on the twostage approach and that based on the one-stage approach) are actual or just come from statistical differences, a statistical significance test has been performed. The statistical significance test has been done based on the Student's t Distribution test\" as given by,  (3)  where \"\n\nis the mean of the first sample (model x) of size n, is the mean of the second sample (model y) of equal size, and SD pooled is the pooled standard deviation of the two samples (models x and y)\" given as,  (4)  where \"SD model x is an estimate of the standard deviation of the average of the first sample (model x) of size n and SD model y is an estimate of the standard deviation of the average of the second sample (model y) of same size\".\n\nThe \"computed t value between the two-stage architecture and the one-stage framework is calculated based on Table  II  and Table  III . The calculated value is t two-stage, one- stage = 1.798. This calculated value is greater than the tabulated critical value t 0.05 = 1.645 at 0.05 significant level\". Hence, it is evident that the two-stage approach yields higher emotion recognition accuracy than the one-stage approach.\n\nEmotion recognition accuracy based on the two-stage framework has been compared with that based on the \"stateof-the-art classifiers and models such as Gaussian Mixture Model (GMM)  [19] , Support Vector Machine (SVM)  [20] , and Vector Quantization (VQ)\"  [21] . The average emotion recognition accuracy based on \"GMM, SVM, and VQ\" is 63.3%, 64.5%, and 61.5%, respectively. It is apparent from this experiment that the two-stage approach gives greater emotion recognition accuracy than each one of these three classifiers and models.\n\nAn \"informal subjective assessment for emotion recognition using our collected corpus has been carried out using 10 human non-professional listeners. In this assessment, a total of 540 utterances (30 speakers × 6 emotions × 3 repetitions) have been involved. These listeners are asked two questions. The first one is to recognize the speaker and the second question is to recognize the unknown emotion given that the speaker was already recognized. Based on this assessment, the average speaker recognition accuracy is 70.4% and the average emotion recognition accuracy is 65.4%. This average emotion recognition accuracy is very similar to the achieved average based on the two-stage architecture (67.5%)\".",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vi. Concluding Remarks",
      "text": "In this work, a two-stage emotion recognizer that is composed of a speaker recognizer followed by an emotion recognizer has been introduced, applied, and assessed on an \"Arabic Emirati-accented speech dataset\". Some concluding remarks can be drawn in this study. First, the two-stage emotion recognizer gives better accuracy than the one-stage emotion recognizer. Therefore, speaker cues help enhancing emotion recognition accuracy. Second, the two-stage emotion recognizer leads the \"state-of-the-art classifiers and models such as GMM, SVM, and VQ\". Third, the highest emotion recognition accuracy occurs when speakers speak neutrally. Finally, the least emotion recognition accuracy takes place when speakers talk angrily. This work has some limitations. Firstly, our dataset is limited to six emotions only. Secondly, the attained emotion recognition accuracy based on the two-stage framework is imperfect. This is because the accuracy of the two-stage approach is the resultant of two accuracies: a) Speaker recognition accuracy which is nonideal. b) Emotion recognition accuracy which is imperfect.\n\nOur coming plan is to use \"Deep Neural Network (DNN)\" to obtain better accuracy  [22] . Furthermore, our strategy is to analyze Emirati-accented emotion recognition in biased talking environments  [23] ,  [24] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This recognizer is comprised of two cascaded",
      "page": 2
    },
    {
      "caption": "Figure 1: Block diagram of the overall proposed two-stage emotion recognizer",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Neutral",
          "Emotion Recognition Accuracy (%)": "84.2"
        },
        {
          "Emotion": "Happy",
          "Emotion Recognition Accuracy (%)": "63.4"
        },
        {
          "Emotion": "Sad",
          "Emotion Recognition Accuracy (%)": "61.5"
        },
        {
          "Emotion": "Disgust",
          "Emotion Recognition Accuracy (%)": "55.9"
        },
        {
          "Emotion": "Angry",
          "Emotion Recognition Accuracy (%)": "40.2"
        },
        {
          "Emotion": "Fear",
          "Emotion Recognition Accuracy (%)": "63.2"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Neutral",
          "Emotion Recognition Accuracy (%)": "90.4"
        },
        {
          "Emotion": "Happy",
          "Emotion Recognition Accuracy (%)": "70.1"
        },
        {
          "Emotion": "Sad",
          "Emotion Recognition Accuracy (%)": "66.7"
        },
        {
          "Emotion": "Disgust",
          "Emotion Recognition Accuracy (%)": "61.8"
        },
        {
          "Emotion": "Angry",
          "Emotion Recognition Accuracy (%)": "48.6"
        },
        {
          "Emotion": "Fear",
          "Emotion Recognition Accuracy (%)": "67.6"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition based on third-order circular suprasegmental hidden Markov model",
      "authors": [
        "I Shahin"
      ],
      "year": "2019",
      "venue": "The 2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in speech signal: experimental study, development, and application",
      "authors": [
        "V Petrushin"
      ],
      "year": "2000",
      "venue": "Proceedings of International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Collias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "N Fragopanagos",
        "J Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks, special issue"
    },
    {
      "citation_id": "5",
      "title": "A new hybrid PSO assisted biogeographybased optimization for emotion and stress recognition from speech signal",
      "authors": [
        "C Yogesh",
        "M Hariharan",
        "R Ngadiran",
        "A Adom",
        "S Yaacob",
        "C Berkai",
        "K Polat"
      ],
      "year": "2017",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "6",
      "title": "Studying and enhancing talking condition recognition in stressful and emotional talking environments based on HMMs, CHMM2s and SPHMMs",
      "authors": [
        "I Shahin"
      ],
      "year": "2012",
      "venue": "Journal on Multimodal User Interfaces",
      "doi": "10.1007/s12193-011-0082-4"
    },
    {
      "citation_id": "7",
      "title": "Talking condition recognition in stressful and emotional talking environments based on CSPHMM2s",
      "authors": [
        "I Shahin",
        "Mohammed Nasser Ba-Hutair"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-014-9251-7"
    },
    {
      "citation_id": "8",
      "title": "Using emotions to identify speakers",
      "authors": [
        "I Shahin"
      ],
      "year": "2008",
      "venue": "The 5th International Workshop on Signal Processing and its Applications"
    },
    {
      "citation_id": "9",
      "title": "Speaking style authentication using suprasegmental hidden Markov models",
      "authors": [
        "I Shahin"
      ],
      "year": "2008",
      "venue": "University of Sharjah Journal of Pure and Applied Sciences"
    },
    {
      "citation_id": "10",
      "title": "Employing emotion cues to verify speakers in emotional talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2016",
      "venue": "Journal of Intelligent Systems, Special Issue on Intelligent Healthcare Systems",
      "doi": "10.1515/jisys-2014-0118"
    },
    {
      "citation_id": "11",
      "title": "Utilizing third-order hidden Markov models for emotional talking condition recognition",
      "authors": [
        "I Shahin",
        "Ali Bou"
      ],
      "year": "2018",
      "venue": "14th IEEE International Conference on Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Enhancement of an Arabic speech emotion recognition system",
      "authors": [
        "S Klaylat",
        "Z Osman",
        "L Hamandi",
        "R Zantout"
      ],
      "year": "2018",
      "venue": "International Journal of Applied Engineering Research"
    },
    {
      "citation_id": "13",
      "title": "A Computational approach for analyzing and detecting emotions in Arabic text",
      "authors": [
        "A El-Gohary",
        "T Sultan",
        "M Hana",
        "M Dosoky"
      ],
      "year": "2013",
      "venue": "International Journal of Engineering Research and Applications (IJERA)"
    },
    {
      "citation_id": "14",
      "title": "Emotion Recognition using Hybrid Gaussian Mixture Model and Deep Neural Network",
      "authors": [
        "I Shahin",
        "Ali Bou Nassif",
        "Shibani Hamsa"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2901352"
    },
    {
      "citation_id": "15",
      "title": "Employing second-order circular suprasegmental hidden Markov models to enhance speaker identification performance in shouted talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2010",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing",
      "doi": "10.1155/2010/862138"
    },
    {
      "citation_id": "16",
      "title": "Speaker identification in a shouted talking environment based on novel third-order circular suprasegmental hidden Markov models",
      "authors": [
        "I Shahin"
      ],
      "year": "2016",
      "venue": "Circuits, Systems and Signal Processing",
      "doi": "10.1007/s00034-015-0220-4"
    },
    {
      "citation_id": "17",
      "title": "Employing both gender and emotion cues to enhance speaker identification performance in emotional talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2013",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-013-9188-2"
    },
    {
      "citation_id": "18",
      "title": "Ensemble methods for spoken emotion recognition in call-centres",
      "authors": [
        "D Morrison",
        "R Wang",
        "L Silva"
      ],
      "year": "2007",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "19",
      "title": "Speaker identification and verification using Gaussian mixture speaker models",
      "authors": [
        "D Reynolds"
      ],
      "year": "1995",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "20",
      "title": "Support vector machines for speaker verification and identification",
      "authors": [
        "V Wan",
        "W Campbell"
      ],
      "year": "2000",
      "venue": "Neural Networks for Signal Processing X, Proceedings of the 2000 IEEE Signal Processing Workshop"
    },
    {
      "citation_id": "21",
      "title": "An overview of text-independent speaker recognition: From features to supervectors",
      "authors": [
        "T Kinnunen",
        "H Li"
      ],
      "year": "2010",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "22",
      "title": "Speech recognition using deep neural networks: a Systematic Review",
      "authors": [
        "A Nassif",
        "I Shahin",
        "I Attili",
        "M Azzeh",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2896880"
    },
    {
      "citation_id": "23",
      "title": "Speaker identification investigation and analysis in unbiased and biased emotional talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-012-9156-2"
    },
    {
      "citation_id": "24",
      "title": "Analysis and investigation of emotion identification in biased emotional talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2011",
      "venue": "IET Signal Processing",
      "doi": "10.1049/iet-spr.2010.0059"
    },
    {
      "citation_id": "25",
      "title": "EMIRATI DATASET AND ITS ENGLISH VERSION",
      "authors": [
        "I Table"
      ],
      "venue": "EMIRATI DATASET AND ITS ENGLISH VERSION"
    }
  ]
}