{
  "paper_id": "2010.14535v4",
  "title": "Neural Architecture Search Of Spd Manifold Networks",
  "published": "2020-10-27T18:08:57Z",
  "authors": [
    "Rhea Sanjay Sukthanker",
    "Zhiwu Huang",
    "Suryansh Kumar",
    "Erik Goron Endsjo",
    "Yan Wu",
    "Luc Van Gool"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a new neural architecture search (NAS) problem of Symmetric Positive Definite (SPD) manifold networks, aiming to automate the design of SPD neural architectures. To address this problem, we first introduce a geometrically rich and diverse SPD neural architecture search space for an efficient SPD cell design. Further, we model our new NAS problem with a one-shot training process of a single supernet. Based on the supernet modeling, we exploit a differentiable NAS algorithm on our relaxed continuous search space for SPD neural architecture search. Statistical evaluation of our method on drone, action, and emotion recognition tasks mostly provides better results than the state-of-the-art SPD networks and traditional NAS algorithms. Empirical results show that our algorithm excels in discovering better performing SPD network design and provides models that are more than three times lighter than searched by the state-of-the-art NAS algorithms.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Designing an efficient neural network architecture for a given application generally requires a significant amount of time, effort, and domain expertise. To mitigate this issue, there have been emerging a number of neural architecture search (NAS) algorithms to automate the design process of neural architectures  [Zoph and Le, 2016; Liu et al., 2017; Liu et al., 2018a; Liu et al., 2018b; Real et al., 2019] . However, researchers have barely proposed NAS algorithms to optimize those neural network architecture designs that deal with non-Euclidean data representations and the corresponding set of operations -to the best of our knowledge.\n\nIt is well-known that Symmetric Positive Definite (SPD) manifold-valued data representation has shown overwhelming accomplishments in many real-world applications such as magnetic resonance imaging analysis  [Pennec et al., 2006] , pedestrian detection  [Tuzel et al., 2008] , human action recognition  [Huang and Van Gool, 2017] , hand gesture recognition  [Nguyen et al., 2019] , etc. Also, in applications like diffusion tensor imaging of the brain, drone imaging, samples are collected directly as SPD's. As a result, neural network usage based on Euclidean data representation becomes inefficient for those applications. Consequently, this has led to the development of SPD neural networks (SPDNets)  [Huang and Van Gool, 2017]  to improve these research areas further. However, the SPDNets are handcrafted, and therefore, the operations or the parameters defined for these networks generally change as per the application. This motivates us to propose a new NAS problem of SPD manifold networks. A solution to this problem can reduce unwanted efforts in SPDNet architecture design. Compared to the traditional NAS problem, our NAS problem requires searching for a new basic neural computation cell modeled by a specific directed acyclic graph (DAG), where each node indicates a latent SPD representation, and each edge corresponds to a SPD candidate operation.\n\nTo solve the suggested NAS problem, we exploit a new supernet search strategy that models the architecture search problem as a one-shot training process of a supernet comprised of a mixture of SPD neural architectures. The supernet modeling enables a differential architecture search on a continuous relaxation of SPD neural architecture search space. We evaluate the proposed NAS method on three benchmark datasets, showing the automatically searched SPD neural architectures perform better than the state-of-the-art handcrafted SPDNets for radar, facial emotion, and skeletal action recognition. In summary, our work makes the following contributions: • We introduce a brand-new NAS problem of SPD manifold networks that opens up a novel research problem at the intersection of automated machine learning and SPD manifold learning. For this problem, we exploit a new search space and a new supernet modeling, both of which respect the particular Riemannian geometry of SPD manifold networks. • We propose a sparsemax-based NAS technique to optimize sparsely mixed architecture candidates during the search stage. This reduces the discrepancy between the search on mixed candidates and the training on one single candidate.\n\nTo optimize the new NAS objective, we exploit a new bilevel algorithm with manifold-and convex-based updates. • Evaluation on three benchmark datasets shows that our searched SPD neural architectures can outperform handcrafted SPDNets  [Huang and Van Gool, 2017; Brooks et al., 2019; Chakraborty et al., 2020]  and the state-of-the-art NAS methods  [Liu et al., 2018b; Chu et al., 2020] . Our searched architecture is more than three times lighter than those searched using traditional NAS algorithms.\n\nAs our work is directed towards solving a new NAS problem, we confine our discussion to the work that has greatly influenced our method i.e., one-shot NAS methods and SPD networks. There are mainly two types of one-shot NAS methods based on the architecture modeling  [Elsken et al., 2018]  (i) parameterized architecture  [Liu et al., 2018b; Zheng et al., 2019; Wu et al., 2019; Chu et al., 2020] , and (ii) sampled architecture  [Deb et al., 2002; Chu et al., 2019] . In this paper, we adhere to the parametric modeling due to its promising results on conventional neural architectures. A majority of the previous work on NAS with continuous search space fine-tunes the explicit feature of specific architectures  [Saxena and Verbeek, 2016; Veniat and Denoyer, 2018; Ahmed and Torresani, 2017; Shin et al., 2018] . On the contrary,  [Liu et al., 2018b; Liang et al., 2019; Zhou et al., 2019; Zhang et al., 2020; Wu et al., 2021; Chu et al., 2020]  provides architectural diversity for NAS with highly competitive performances.\n\nThe other part of our work focuses on SPD network architectures. There exist algorithms to develop handcrafted SPDNets like  [Huang and Van Gool, 2017; Brooks et al., 2019; Chakraborty et al., 2020] . To automate the process of SPD network design, in this work, we choose the most promising approaches from the field of NAS  [Liu et al., 2018b]  and SPD networks  [Huang and Van Gool, 2017] ).\n\nNext, we summarize some of the essential notions of Riemannian geometry of SPD manifolds, followed by introducing some basic SPDNet operations and layers. As some of the introduced operations and layers have been well-studied by the existing literature, we apply them directly to define our SPD neural architectures' search space. Representation and Operation. We denote n×n real SPD as X ∈ S n ++ . A real SPD matrix X ∈ S n ++ satisfies the property that for any non-zero z ∈ R n , z T Xz > 0. We denote T X M as the tangent space of the manifold M at X ∈ S n ++ and log corresponds to matrix logarithm.\n\nLet X 1 , X 2 be any two points on the SPD manifold then the distance between them is given by δ\n\nThere are other efficient methods to compute distance between two points on the SPD manifold, however, their discussion is beyond the scope of our work  [Gao et al., 2019; Dong et al., 2017b] . Other property of the Riemannian manifold of our interest is local diffeomorphism of geodesics which is a one-to-one mapping from the point on the tangent space of the manifold to the manifold  [Pennec, 2020; Lackenby, 2020] . To define such notions, let X ∈ S n ++ be the base point and, Y ∈ T X S n ++ , then Y ∈ T X S n ++ is associated to a point on the SPD manifold  [Pennec, 2020]  by the map\n\n(1) Likewise,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "++",
      "text": "(2) is defined as its inverse map. 1) Basic operations of SPD Network. Operations such as mean centralization, normalization, and adding bias to a batch of data are inherent performance booster for neural networks. Accordingly, works like  [Brooks et al., 2019; Chakraborty, 2020]  use the notion of such operations for the manifold-valued data to define analogous operations on manifolds. Below we introduce them following  [Brooks et al., 2019]  work.\n\n• Batch mean, centering and bias: Given a batch of N SPD matrices {Xi} N i=1 , we can compute its Riemannian barycenter (B) as:\n\n(3)\n\nIt is sometimes referred to as Fréchet mean  [Moakher, 2005; Bhatia and Holbrook, 2006] . This definition can be extended to compute the weighted Riemannian Barycenter also known as weighted Fréchet Mean (wFM) 1\n\n(4) Eq:(4) can be estimated using Karcher flow  [Karcher, 1977; Bonnabel, 2013; Brooks et al., 2019]  or recursive geodesic mean  [Cheng et al., 2016; Chakraborty et al., 2020] .\n\n2) Basic layers of SPD Network. Analogous to standard convolutional networks (ConvNets),  [Huang and Van Gool, 2017; Brooks et al., 2019; Chakraborty et al., 2020]  designed SPD layers to perform operations that respect SPD manifold constraints. Assuming X k-1 ∈ S n ++ be the input SPD matix to the k th layer, the SPD layers are defined as follows:\n\n• BiMap layer: This layer corresponds to a dense layer for SPD data. It reduces the dimension of a input SPD matrix via a transformation matrix\n\nTo ensure X k to be SPD, W k is commonly required to be of full row-rank through an orthogonality constraint on it.\n\n• Batch normalization layer: To perform batch normalization after each BiMap layer, we first compute the Riemannian barycenter of one batch of SPD samples followed by a running mean update step, which is Riemannian weighted average between the batch mean and the current running mean, with the weights (1θ) and (θ) respectively. With the mean, we centralize and add a bias to each SPD sample in the batch using Eq:(  5 ), where P is the notation used for parallel transport and I is the identity matrix:\n\nCentering the B :\n\n(5)\n\n• ReEig layer: The ReEig layer is analogous to ReLU layers presented in the classical ConvNets. It aims to introduce non-linearity to SPD networks. The ReEig for the k th layer is defined as:\n\n, I is the identity matrix, and > 0 is a rectification threshold value. U k-1 , Σ k-1 are the orthonormal matrix and singular-value matrix respectively, and obtained via matrix factorization of X k-1 . • LogEig layer: To map the manifold representation of SPDs to a flat space so that a Euclidean operation can be performed, LogEig layer is introduced. The LogEig layer is defined as:\n\nThe LogEig layer is used with fully connected layers to solve tasks with SPD representation.\n\n• ExpEig layer: The ExpEig layer is designed to map the corresponding SPD representation from a flat space back to an SPD manifold space. It is defined as\n\n• Weighted Riemannian pooling layer: It uses wFM to perform the weighted pooling on SPD samples. To compute wFM, we follow  [Brooks et al., 2019]  to use Karcher flow  [Karcher, 1977]  due to its simplicity and wide usage.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "Firstly, we introduce a new definition of the computation cell for our task. In contrast to traditional NAS computation cell design  [Liu et al., 2018b; Chu et al., 2020] , our computational cell (called SPD cell), incorporates the notion of SPD manifold geometry while performing any SPD operations. Similar to the basic NAS cell design, our SPD cell can either be a normal cell that returns SPD feature maps of the same width and height or, a reduction cell in which the SPD feature maps are reduced by a certain factor in width and height. Secondly, solving our new NAS problem will require an appropriate and diverse SPD search space that can help NAS method to optimize for an effective SPD cell, which can then be stacked and trained to build an efficient SPD neural network architecture.\n\nConcretely, an SPD cell is modeled by a directed asyclic graph (DAG) which is composed of nodes and edges. In our DAG each node indicates an latent representation of the SPD manifold-valued data i.e., an intermediate SPD feature map, and each edge corresponds to a valid candidate operation on the SPD manifold (see Fig.  1(a) ). Each edge of a SPD cell is associated with a set of candidate SPD manifold operations (O M ) that transforms the SPD-valued latent representation from the source node (say X (i) M ) to the target node (say X (j) M ). We define the intermediate transformation between the nodes in our SPD cell as:\n\nwhere M denotes the geodesic distance. This transformation result corresponds to the unweighted Fréchet mean of the operations based on the predecessors, such that the mixture of all operations still reside on SPD manifolds. Note that our definition of SPD cell ensures that each computational graph preserves the appropriate geometric structure of the SPD manifold. Equipped with the notion of SPD cell and its intermediate transformation, we are ready to propose our search space ( §3.1) followed by the solution to our new NAS problem ( §3.2) and its results ( §4).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Search Space",
      "text": "Our search space consists of a set of valid SPD network operations. It includes some existing SPD operations, e.g., BiMap, Batch Normalization, ReEig, LogEig, ExpEig, and weighted Riemannian pooling layers, all of which are introduced in Sec.2. Though existing works have well explored those individual operations (e.g., BiMap, LogEig, ExpEig), their different combinations are still understudied and are essential to enrich our search space. To enhance the search space, following traditional NAS methods  [Liu et al., 2018b; Gong et al., 2019] , we apply the SPD batch normalization to every SPD convolution operation (i.e., BiMap), and design three variants of convolution blocks including the one without activation (i.e., ReEig), the one using post-activation and the one using pre-activation (see Table  1 ). In addition, we introduce five new operations analogous to  [Liu et al., 2018b]  to enrich the search space in the context of SPD networks. These are, skip normal, none normal, average pooling, max pooling, and skip reduced. The effect of such diverse operation choices has not been fully explored for SPD networks. All the candidate operations are illustrated in Table (1), and their definitions are as follows:\n\n(a) Skip normal: It preserves the input representation and is similar to regular skip connections. (b) None normal: It corresponds to the operation that returns identity as the output i.e, the notion of zero in the SPD space. (c) Max pooling: Given a set of SPD matrices, the max pooling operation first projects these samples to a flat space via a LogEig operation, where a standard max pooling operation is performed. Finally, an ExpEig operation is used to map the samples back to the SPD manifold. (d) Average pooling: Similar to Max pooling, the average pooling operation first projects the samples to the flat space using a LogEig operation, where a standard average pooling is employed. To map the sample back to the SPD manifold, an ExpEig operation is used. (e) Skip reduced: It is similar to 'skip normal' but in contrast, it decomposes the input into small matrices to reduces the inter-dependency between channels. Our definition of the reduce operation is in line with the work of  [Liu et al., 2018b] . • LogEig layer: To map the manifold representation of SPDs to a flat space so that a Euclidean operation can be performed, LogEig layer is introduced. The LogEig layer is defined as:\n\nThe LogEig layer is used with fully connected layers to solve tasks with SPD representation.\n\n• ExpEig layer: The ExpEig layer is designed to map the corresponding SPD representation from a flat space back to an SPD manifold space. It is defined as\n\n• Weighted Riemannian pooling layer: It uses wFM to perform the weighted pooling on SPD samples. To compute wFM, we follow  [Brooks et al., 2019]  to use Karcher flow  [Karcher, 1977]  due to its simplicity and wide usage.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "Firstly, we introduce a new definition of the computation cell for our task. In contrast to traditional NAS computation cell design  [Liu et al., 2018b; Chu et al., 2020] , our computational cell (called SPD cell), incorporates the notion of SPD manifold geometry while performing any SPD operations. Similar to the basic NAS cell design, our SPD cell can either be a normal cell that returns SPD feature maps of the same width and height or, a reduction cell in which the SPD feature maps are reduced by a certain factor in width and height. Secondly, solving our new NAS problem will require an appropriate and diverse SPD search space that can help NAS method to optimize for an effective SPD cell, which can then be stacked and trained to build an efficient SPD neural network architecture.\n\nConcretely, an SPD cell is modeled by a directed asyclic graph (DAG) which is composed of nodes and edges. In our DAG each node indicates an latent representation of the SPD manifold-valued data i.e., an intermediate SPD feature map, and each edge corresponds to a valid candidate operation on the SPD manifold (see Fig.  1(a) ). Each edge of a SPD cell is associated with a set of candidate SPD manifold operations (O M ) that transforms the SPD-valued latent representation from the source node (say\n\nWe define the intermediate transformation between the nodes in our SPD cell as:\n\nwhere δ M denotes the geodesic distance. This transformation result corresponds to the unweighted Fréchet mean of the operations based on the predecessors, such that the mixture of all operations still reside on SPD manifolds. Note that our definition of SPD cell ensures that each computational graph preserves the appropriate geometric structure of the SPD manifold. Equipped with the notion of SPD cell and its intermediate transformation, we are ready to propose our search space ( §3.1) followed by the solution to our new NAS problem ( §3.2) and its results ( §4).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Search Space",
      "text": "Our search space consists of a set of valid SPD network operations. It includes some existing SPD operations, e.g., BiMap, Batch Normalization, ReEig, LogEig, ExpEig, and weighted Riemannian pooling layers, all of which are introduced in Sec.2. Though existing works have well explored those individual operations (e.g., BiMap, LogEig, ExpEig), their different combinations are still understudied and are essential to enrich our search space. To enhance the search space, following traditional NAS methods  [Liu et al., 2018b; Gong et al., 2019] , we apply the SPD batch normalization to every SPD convolution operation (i.e., BiMap), and design three variants of convolution blocks including the one without activation (i.e., ReEig), the one using post-activation and the one using pre-activation (see Table  1 ). In addition, we introduce five new operations analogous to  [Liu et al., 2018b]  to enrich the search space in the context of SPD networks. These are, skip normal, none normal, average pooling, max pooling, and skip reduced. The effect of such diverse operation choices has not been fully explored for SPD networks. All the candidate operations are illustrated in Table (1), and their definitions are as follows:\n\n(a) Skip normal: It preserves the input representation and is similar to regular skip connections. (b) None normal: It corresponds to the operation that returns identity as the output i.e, the notion of zero in the SPD space. (c) Max pooling: Given a set of SPD matrices, the max pooling operation first projects these samples to a flat space via a LogEig operation, where a standard max pooling operation is performed. Finally, an ExpEig operation is used to map the samples back to the SPD manifold. (d) Average pooling: Similar to Max pooling, the average pooling operation first projects the samples to the flat space using a LogEig operation, where a standard average pooling is employed. To map the sample back to the SPD manifold, an ExpEig operation is used. (e) Skip reduced: It is similar to 'skip normal' but in contrast, it decomposes the input into small matrices to reduces the inter-dependency between channels. Our definition of the reduce operation is in line with the work of  [Liu et al., 2018b] . The newly introduced operations allow us to generate a more diverse discrete search space. As presented in Table (2), the randomly selected architecture (generally consisting of the newly introduced SPD operations) shows some improvement over the handcrafted SPDNets, which only contain conventional SPD operations. This establishes the effectiveness of the introduced rich search space. For more details about the proposed SPD operations, please refer to our Appendix.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Supernet Search",
      "text": "To solve the new NAS problem, one of the most promising methodologies is supernet modeling. In general, the supernet method models the architecture search problem as a one-shot training process of a single supernet that consists of all candidate architectures. Based on the supernet modeling, we search for the optimal SPD neural architecture by parameterizing the design of the supernet architectures, which is based on the continuous relaxation of the SPD neural architecture representation. Such an approach allows for an efficient search of architecture using the gradient descent approach. Next, we introduce our supernet search method, followed by a solution to our proposed bi-level optimization problem. Fig.  1 (b) and Fig.  1(c ) illustrate an overview of our proposed method.\n\nTo search for an optimal SPD architecture parameterized by α, we optimize the over parameterized supernet. In essence, it stacks the basic computation cells with the parameterized candidate operations from our search space in a one-shot search manner. The contribution of specific subnets to the supernet helps in deriving the optimal architecture from the supernet. Since the proposed operation search space is discrete in nature, we relax the explicit choice of an operation to make the search space continuous. To do so, we use wFM over all possible candidate operations. Mathematically,\n\nsubject to:\n\nwhere O k M is the k th candidate operation between nodes, X µ is the intermediate SPD manifold mean (Eq.4) and, N e denotes number of edges. We can compute wFM solution either using Karcher flow  [Karcher, 1977]  or recursive geodesic mean  [Chakraborty et al., 2020]  algorithm. Nonetheless, we adhere to Karcher flow algorithm as it is widely used to calculate wFM. To impose the explicit convex constraint on α, we project the solution onto the probability simplex as minimize α α -α 2 2 ; subject to:\n\nEq:(  8 ) enforces the explicit constraint on the weights to supply α for our task and can easily be added as a convex layer in the framework  [Agrawal et al., 2019]  From Eq:(7-8), the mixing of operations between nodes is determined by the weighted combination of alpha's (α k ) and the set of operations (O k M ). This relaxation makes the search space continuous and therefore, architecture search can be achieved by learning a set of alpha (α = {α k , ∀ k ∈ N e }). To achieve our goal, we simultaneously learn the contributions (i.e., the architecture parameterization) α of all the candidate operations and their corresponding network weights (w). Consequently, for a given w, we optimize α and vice-versa resulting in the following bi-level optimization problem. min.\n\nwhere the lower-level optimization (E L train ) corresponds to the optimal weight variable learned for a given α i.e., w opt (α) using a training loss. The upper-level optimization (E U val ) solves for the variable α given the optimal w using a validation loss. This bi-level search method gives an optimal mixture of multiple small architectures. To derive each node in the discrete architecture, we maintain top-k operations i.e., the k th highest weight among all the candidate operations associated with all the previous nodes. Bi-level Optimization. The bi-level optimization problem proposed in Eq:(  9 ) is difficult to solve. On one hand, the α can be interpreted as hyper-parameter but it's not a scalar and hence, harder to optimize. On the other hand, the lower optimization is computationally expensive. Following  [Liu et al., 2018b ] work, we approximate w opt (α) in the upperoptimization problem to skip inner-optimization as follows:\n\n) Here, η is the learning rate and ∇ is the gradient operator. Note that the gradient based optimization for w must follow the geometry of SPD manifolds to update the structured connection weight, and its corresponding SPD matrix data. Applying the chain rule to Eq:(10) gives first term\n\nwhere, w = Ψ r wη ∇w E L train (w, α) denotes the weight update on the SPD manifold for the forward model. ∇w , Ψ r symbolizes the Riemannian gradient and the retraction operator respectively. The second term in the Eq:(11) involves second order differentials with very high computational complexity, hence, using the finite approximation method the second term of Eq:(11) reduces to:\n\nwhere, w ± = Ψ r (w ± δ ∇ wE U val ( w, α)) and δ is a small number set to 0.01/ ∇ wE U val ( w, α) 2 . Though the optimization follows the suggested structure in  [Liu et al., 2018b] , there are some key differences. Firstly, the updates on the manifold-valued kernel weights are constrained on manifolds, which ensures that the feature maps at every intermediate layer are SPDs. For concrete derivations on back-propagation for SPD network layers, refer to  [Huang and Van Gool, 2017]  work. Secondly, the update on the aggregation weights of the involved SPD operations needs to satisfy an additional strict convex constraint, which is enforced as a part of the optimization problem Eq:(8). The pseudo code of our method is outlined in Algorithm 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "To keep the experimental evaluation consistent with the existing SPD networks  [Huang and Van Gool, 2017      et al., 2020]  to first apply a single wFM layer with kernel size 5, stride 3, and 8 channels, followed by three temporal wFM layers of kernel size 3 and stride 2, with the channels being 1, 4, 8 respectively. We closely follow the official implementation of ManifoldNet for the wFM layers and adapt the code to our specific task. Since SPDNet, SPDNetBN, and our SPDNetNAS require a single channel SPD matrix as input, we use the final 512-dimensional vector extracted from the convolutional network, project it using a dense layer to a 100-dimensional feature vector and compute a 100 × 100 temporal covariance matrix. To study our algorithm's transferability, we evaluate its searched architecture on RADAR and HDM05. Also, we evaluate DARTS and FairDARTS directly on the video frames of AFEW. Table (3) reports the evaluations results. As we can observe, the transferred architectures can handle the new dataset quite convincingly, and their test accuracies are better than those of the state-of-the-art SPDNets and Euclidean NAS algorithms. In Appendix ?? , we present the results of these competing methods and our searched models on the raw SPD features of the AFEW dataset. d) Comparison under similar model complexities. We compare the statistical performance of our method against the other competing methods under similar model sizes.  degradation in the performance accuracy -mainly because the network starts overfitting rapidly. The performance degradation is far more severe for the HDM05 dataset with SPDNet (1.047MB) performing 0.7619% and SPDNetBN (1.082MB) performing 1.45% and hence, is not reported in Table (  4 ). That further indicates the ability of SPDNetNAS to generalize better and avoid overfitting despite the larger model size. e) Ablation study. Lastly, we conducted an ablation study to realize the effect of probability simplex constraint (sparsemax) on our suggested Fréchet mixture of SPD operations. Although in Fig.  2 (a) we show better probability weight distribution with sparsemax, Table (5) shows that it performs better empirically as well on both RADAR and HDM05 compared to the softmax and sigmoid cases. Therefore, SPD architectures derived using the sparsemax is observed to be better.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Direction",
      "text": "We presented a neural architecture search problem of SPD manifold networks. To address it, we proposed a new differentiable NAS algorithm that consists of sparsemax-based Fréchet relaxation of search space and manifold update-based bilevel optimization. Evaluation on several benchmark datasets showed the clear superiority of the proposed NAS method over handcrafted SPD networks and Euclidean NAS algorithms. As a future work, it would be interesting to generalize our NAS algorithm to cope with other manifold valued data (e.g.,  [Huang et al., 2017; Huang et al., 2018; Chakraborty et al., 2018; Kumar et al., 2018; Zhen et al., 2019; Kumar, 2019; Kumar et al., 2020] ) and manifold poolings (e.g.,  [Wang et al., 2017; Engin et al., 2018; Wang et al., 2019] ), which are generally valuable for visual recognition, structure from motion, medical imaging, radar imaging, forensics, appearance tracking to name a few.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Technical Appendix",
      "text": "In the appendix, we will introduce some more details about the proposed SPD operations and the suggested optimization. Finally, we present additional experimental analysis for our proposed method.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Detailed Description Of Our Proposed Spd Operations",
      "text": "In this section, we describe some of the major operations defined in the main paper from an intuitive point of view. We particularly focus on some of the new operations that are defined for the input SPDs, i.e., the Weighted Riemannian Pooling, the Average/Max Pooling, the Skip Reduced operation and the Mixture of Operations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A.1 Weighted Riemannian Pooling",
      "text": "Figure  3  provides an intuition behind the Weighted Riemannian Pooling operation. Here, w 11, w 21, etc., corresponds to the set of normalized weights for each channel (shown as two blue channels). The next channel -shown in orange, is then computed as weighted Fréchet mean over these two input channels. This procedure is repeated to achieve the desired number of output channels (here two), and finally all the output channels are concatenated. The weights are learnt as a part of the optimization procedure ensuring the explicit convex constraint is imposed.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.2 Average And Max Pooling",
      "text": "In Figure  4  we show our average and max pooling operations. We first perform a LogEig map on the SPD matrices to project them to the Euclidean space. Next, we perform average and max pooling on these Euclidean matrices similar to classical convolutional neural networks. We further perform an ExpEig map to project the Euclidean matrices back on the SPD manifold. The diagram shown in Figure  4  is inspired by  [Huang and Van Gool, 2017]",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3 Skip Reduced",
      "text": "Following  [Liu et al., 2018b] , we defined an analogous of Skip operation on a single channel for the reduced cell (Figure  5 ). We start by using a BiMap layer -equivalent to Conv in  [Liu et al., 2018b] , to map the input channel to an SPD whose space dimension is half of the input dimension. We further perform an SVD decomposition on the two SPDs followed by concatenating the Us, Vs and Ds obtained from SVD to block diagonal matrices. Finally, we compute the output by multiplying the block diagonal U, V and D computed before.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Reassemble Output",
      "text": "Output=U_out * D_out * V_out\n\nFigure  5 : Skip Reduced:Maps input to two smaller matrices using BiMaps, followed by SVD decomposition on them and then computes the output using a block diagonal form of U's D's and V's",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4 Mixed Operation On Spds",
      "text": "In Figure  6  we provide an intuition of the mixed operation we have proposed in the main paper. We consider a very simple base case of three nodes, two input nodes (1 and 2) and one output node (node 3). The goal is to compute the output node 3 from input nodes 1 and 2. We perform a candidate set of operations on the input node, which correspond to edges between the nodes (here two for simplicity). Each operation has a weight α i j where i corresponds to the node index and j is the candidate operation identifier. In Figure  6  below i and j ∈ {1, 2} and α 1 = {α 1 1 , α 1 2 } , α 2 = {α 2 1 , α 2 2 } . α's are optimized as a part of the bi-level optimization procedure proposed in the main paper. Using these alpha's, we perform a channel-wise weighted Fréchet mean (wFM) as depicted in the figure below. This effectively corresponds to a mixture of the candidate operations. Note that the alpha's corresponding to all channels of a single operation are assumed to be the same.\n\nOnce the weighted Fréchet means have been computed for nodes 1 and 2, we perform a channel-wise concatenation on the outputs of the two nodes, effectively doubling the number of channels in node 3.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B Suggested Optimization Details",
      "text": "As described in the paper we suggest to replace the standard softmax constraint commonly used in differentiable architecture search with the sparsemax constraint. In addition to ensuring that it outputs valid probabilities (summing to 1), sparsemax is capable of producing sparse distribution, i.e., preserving only a few dominant contributions. This further helps in discovery of a more efficient and optimal architecture. Figure  7  shows the implementation of the differentiable convex layer for our suggested sparsemax optimization. We use a convex optimization layer to explicitly ensure that the w i > 0 and i w i = 1 constraints for a valid Fréchet mean on the SPD manifold are enforced.\n\nUnder review as a conference paper at ICLR 2021 node (node 3). The goal is to compute the output node 3 from input nodes 1 and 2. We perform a candidate set of operations on the input node, which correspond to edges between the nodes (here two for simplicity). Each operation has a weight ↵i j where i corresponds to the node index and j is the candidate operation identifier. In Figure  11  below i and j 2 {1, 2} and ↵1 = {↵1 1, ↵1 2} , ↵2 = {↵2 1, ↵2 2} . ↵'s are optimized as a part of the bi-level optimization procedure proposed in the main paper. Using these alpha's, we perform a channel-wise weighted Fréchet mean (wFM) as depicted in the figure below. This effectively corresponds to a mixture of the candidate operations. Note that the alpha's corresponding to all channels of a single operation are assumed to be the same.\n\nOnce the weighted Fréchet means have been computed for nodes 1 and 2, we perform a channel-wise concatenation on the outputs of the two nodes, effectively doubling the number of channels in node 3.  Unlike  [Huang and Van Gool, 2017]  work on the SPD network, where multiple transformation matrices are applied at multiple layers to reduce the dimension of the input data, our reduction cell presented in the main paper is one step. For example: For HDM05 dataset  [Müller et al., 2007] , the author's of SPDNet  [Huang and Van Gool, 2017]  apply 93 × 70, 70 × 50, 50 × 30, transformation matrices to reduce the dimension of the input matrix, on the contrary, we reduce the dimension in one step from 93 to 30 which is inline with  [Brooks et al., 2019]  work.\n\nTo study the behaviour of our method under multiple dimesionality reduction pipeline on HDM05, we use the preprocessing layers to perform dimensionality reduction. To be precise, we consider a preprocessing step to reduce the dimension from 93 to 70 to 50 and then, a reduction cell that reduced the dimension from 50 to 24. This modification has the advantage that it reduces the search time from 3 CPU days to 2.5 CPU days, and in addition, provides a performance gain (see Table  (6     et al., 2007] . We added one extra intermediate node (N = 6) to the cell design.\n\nWe observe that we converge towards an architecture design that is very much similar in terms of operations (see Figure  9 ). The evaluation results shown in",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C.3 Effect Of Adding Multiple Cells",
      "text": "In our paper we stack 1 normal cell over 1 reduction cell for all the experiments. For more extensive analysis of the proposed method, we conducted training experiments by stacking multiple cells which is in-line with the experiments conducted by  [Liu et al., 2018b] . We then transfer the optimized architectures from the singe cell search directly to the multi-cell architectures for training. Hence, the search time for all our experiments is same as for a single cell search i.e. 3 CPU days.\n\nResults for this experiment are provided in Table  8 . The first row in the table shows the performance for single cell model, while the second and third rows show the performance with multi-cell stacking. Remarkably, by stacking multiple cells our proposed SPDNetNAS outperforms  SPDNetBN [Brooks et al., 2019]  by a large margin (about 8%, i.e., about 12% for the relative improvement).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C.4 Afew Performance Comparison On Raw Spd Features",
      "text": "In addition to the evaluation on CNN features in the major paper, we also use the raw SPD features (extracted from gray video frames) from  [Huang and Van Gool, 2017 ; Brooks et al., 2019] to compare the competing methods. To be specific, each frame is normalized to 20 × 20 and then represent each video using a 400 × 400 covariance matrix  [Wang et al., 2012; Huang and Van Gool, 2017] . Table  9  summarizes the results.\n\nAs we can see, the transferred architecture can handle the new dataset quite convincingly. The test accuracy is comparable to the best SPD network method for RADAR model transfer.\n\nFor HDM05 model transfer, the test accuracy is much better than the existing SPD networks.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) A SPD cell structure composed of 4 SPD nodes, 2 input node and 1 output node. Initially the edges are unknown (b) Mixture of",
      "page": 3
    },
    {
      "caption": "Figure 2: (a),2(b) and §4).",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) Distribution of edge weights for operation selection using softmax, sigmoid, and sparsemax on the Fr´echet mixture of SPD",
      "page": 5
    },
    {
      "caption": "Figure 2: (b). The rich search space of our algorithm allows",
      "page": 6
    },
    {
      "caption": "Figure 2: (b) shows the ﬁnal cell architec-",
      "page": 6
    },
    {
      "caption": "Figure 2: (a) we show better probability weight distri-",
      "page": 7
    },
    {
      "caption": "Figure 3: provides an intuition behind the Weighted Rieman-",
      "page": 9
    },
    {
      "caption": "Figure 3: Weighted Riemannian Pooling: Performs multiple weighted",
      "page": 9
    },
    {
      "caption": "Figure 4: we show our average and max pooling operations.",
      "page": 9
    },
    {
      "caption": "Figure 4: is inspired by [Huang",
      "page": 9
    },
    {
      "caption": "Figure 4: Avg/Max Pooling: Maps the SPD matrix to Euclidean",
      "page": 9
    },
    {
      "caption": "Figure 5: Skip Reduced:Maps input to two smaller matrices using",
      "page": 9
    },
    {
      "caption": "Figure 6: we provide an intuition of the mixed operation",
      "page": 9
    },
    {
      "caption": "Figure 6: below i and",
      "page": 9
    },
    {
      "caption": "Figure 6: Detailed overview of mixed operations. We simplify the",
      "page": 10
    },
    {
      "caption": "Figure 7: shows the implementation of the differentiable con-",
      "page": 10
    },
    {
      "caption": "Figure 7: Function to Solve the sparsemax constraint optimization",
      "page": 10
    },
    {
      "caption": "Figure 8: (a)-(b) Normal cell and Reduction cell for multiple dimensionality reduction respectively",
      "page": 11
    },
    {
      "caption": "Figure 9: (a)-(b) Optimal Normal cell and Reduction cell with 6 nodes on the HDM05 dataset",
      "page": 11
    },
    {
      "caption": "Figure 10: (a) and Figure 10(b) show the cell architecture ob-",
      "page": 11
    },
    {
      "caption": "Figure 11: (a) shows the validation curve which almost saturates",
      "page": 11
    },
    {
      "caption": "Figure 10: (a), (b) Derived architecture by using softmax and sigmoid on the Fr´echet mixture of SPD operations. These are the normal cell and",
      "page": 12
    },
    {
      "caption": "Figure 11: (a) Validation accuracy of our method in comparison",
      "page": 12
    },
    {
      "caption": "Figure 11: (b)) clealy show our superiority of SPDNetNAS algorithm",
      "page": 12
    },
    {
      "caption": "Figure 12: (a) Loss function curve showing the values over 200",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Operation": "BiMap 0",
          "Deﬁnition": "{BiMap, Batch Normalization}",
          "Operation\nDeﬁnition": "WeightedReimannPooling normal\n{wFM on SPD multiple times}"
        },
        {
          "Operation": "BiMap 1",
          "Deﬁnition": "{BiMap,Batch Normalization, ReEig}",
          "Operation\nDeﬁnition": "AveragePooling reduced\n{LogEig, AveragePooling, ExpEig}"
        },
        {
          "Operation": "BiMap 2",
          "Deﬁnition": "{ReEig, BiMap, Batch Normalization}",
          "Operation\nDeﬁnition": "MaxPooling reduced\n{LogEig, MaxPooling, ExpEig}"
        },
        {
          "Operation": "Skip normal",
          "Deﬁnition": "{Output same as input}",
          "Operation\nDeﬁnition": "Skip reduced = {Cin = BiMap(Xin), [Uin, Din, ∼] = svd(Cin); in = 1, 2},\nCout = UbDbUT\nb , where, Ub = diag(U1, U2) and Db = diag(D1, D2)"
        },
        {
          "Operation": "None normal",
          "Deﬁnition": "{Return identity matrix}",
          "Operation\nDeﬁnition": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Differentiable convex optimization layers",
      "authors": [
        "A Agrawal",
        "B Amos",
        "S Barratt",
        "S Boyd",
        "S Diamond",
        "J Kolter",
        "L Ahmed",
        "Torresani"
      ],
      "year": "2017",
      "venue": "NeurIPS",
      "arxiv": "arXiv:1709.09582"
    },
    {
      "citation_id": "2",
      "title": "Riemannian geometry and matrix geometric means",
      "authors": [
        "Holbrook Bhatia",
        "R Bhatia",
        "J Holbrook"
      ],
      "year": "2006",
      "venue": "Linear algebra and its applications"
    },
    {
      "citation_id": "3",
      "title": "A statistical recurrent model on the manifold of symmetric positive definite matrices",
      "authors": [
        "; Bonnabel",
        "Bonnabel",
        "Brooks"
      ],
      "year": "2013",
      "venue": "Manifoldnet: A deep neural network for manifold-valued data with applications. T-PAMI",
      "arxiv": "arXiv:2003.13869"
    },
    {
      "citation_id": "4",
      "title": "Micro-doppler effect in radar: phenomenon, model, and simulation study. T-AES",
      "year": "2006",
      "venue": "Micro-doppler effect in radar: phenomenon, model, and simulation study. T-AES"
    },
    {
      "citation_id": "5",
      "title": "Recursive computation of the fréchet mean on nonpositively curved riemannian manifolds with applications",
      "authors": [
        "Cheng"
      ],
      "year": "2016",
      "venue": "Scarletnas: Bridging the gap between scalability and fairness in neural architecture search",
      "arxiv": "arXiv:1908.06022"
    },
    {
      "citation_id": "6",
      "title": "A fast and elitist multiobjective genetic algorithm: Nsga-ii",
      "year": "2002",
      "venue": "A fast and elitist multiobjective genetic algorithm: Nsga-ii"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition in the wild challenge 2014: Baseline, data and protocol",
      "authors": [
        "Dhall"
      ],
      "year": "2014",
      "venue": "ICMI"
    },
    {
      "citation_id": "8",
      "title": "Deep manifold learning of symmetric positive definite matrices with application to face recognition",
      "authors": [
        "Dong"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "9",
      "title": "Deepkspd: Learning kernel-matrix-based spd representation for fine-grained image recognition",
      "authors": [
        "Elsken"
      ],
      "year": "2018",
      "venue": "Neural architecture search: A survey",
      "arxiv": "arXiv:1808.05377"
    },
    {
      "citation_id": "10",
      "title": "Accelerating the svd two stage bidiagonal reduction and divide and conquer using gpus",
      "authors": [
        "Gates"
      ],
      "year": "2017",
      "venue": "Dimensionality reduction on spd manifolds: The emergence of geometry-aware methods. T-PAMI"
    },
    {
      "citation_id": "11",
      "title": "Deep learning on Lie groups for skeletonbased action recognition",
      "authors": [
        "Huang"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "12",
      "title": "Building deep networks on Grassmann manifolds",
      "authors": [
        "Huang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Riemannian center of mass and mollifier smoothing",
      "authors": [
        "; Karcher",
        "Karcher"
      ],
      "year": "1977",
      "venue": "Communications on pure and applied mathematics"
    },
    {
      "citation_id": "14",
      "title": "Scalable dense non-rigid structure-from-motion: A grassmannian perspective",
      "authors": [
        "Kumar"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "Dense non-rigid structure from motion: A manifold viewpoint",
      "authors": [
        "Kumar"
      ],
      "year": "2020",
      "venue": "Dense non-rigid structure from motion: A manifold viewpoint",
      "arxiv": "arXiv:2006.09197"
    },
    {
      "citation_id": "16",
      "title": "Jumping manifolds: Geometry aware dense non-rigid from motion",
      "authors": [
        "; Kumar",
        "Kumar"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "Marc Lackenby",
        "Lackenby"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "Darts+: Improved differentiable architecture search with early stopping",
      "authors": [
        "Liang"
      ],
      "year": "2019",
      "venue": "Darts+: Improved differentiable architecture search with early stopping",
      "arxiv": "arXiv:1909.06035"
    },
    {
      "citation_id": "19",
      "title": "Hierarchical representations for efficient architecture search",
      "authors": [
        "Liu"
      ],
      "year": "2017",
      "venue": "Hierarchical representations for efficient architecture search",
      "arxiv": "arXiv:1711.00436"
    },
    {
      "citation_id": "20",
      "title": "Progressive neural architecture search",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "21",
      "title": "From softmax to sparsemax: A sparse model of attention and multi-label classification",
      "authors": [
        "Liu"
      ],
      "year": "2016",
      "venue": "ICLR"
    },
    {
      "citation_id": "22",
      "title": "A differential geometric approach to the geometric mean of symmetric positivedefinite matrices",
      "authors": [
        "; Moakher",
        "Moakher",
        "Müller"
      ],
      "year": "2005",
      "venue": "SIAM"
    },
    {
      "citation_id": "23",
      "title": "A neural network based on spd manifold learning for skeleton-based hand gesture recognition",
      "authors": [
        "Nguyen"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "24",
      "title": "Regularized evolution for image classifier architecture search",
      "authors": [
        "Pennec"
      ],
      "year": "2006",
      "venue": "Riemannian Geometric Statistics in Medical Image Analysis"
    },
    {
      "citation_id": "25",
      "title": "Differentiable neural network architecture search",
      "authors": [
        "Verbeek Saxena",
        "S Saxena",
        "J Verbeek",
        "Shin"
      ],
      "year": "2008",
      "venue": "Pedestrian detection via classification on riemannian manifolds. T-PAMI"
    },
    {
      "citation_id": "26",
      "title": "Learning time/memory-efficient deep architectures with budgeted super networks",
      "authors": [
        "Denoyer Veniat",
        "T Veniat",
        "L Denoyer"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "G2denet: Global gaussian distribution embedding network and its application to visual recognition",
      "authors": [
        "Wang"
      ],
      "year": "2012",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search",
      "authors": [
        "Wang"
      ],
      "year": "2019",
      "venue": "Deep cnns meet global covariance pooling: Better representation and generalization",
      "arxiv": "arXiv:1904.06836"
    },
    {
      "citation_id": "29",
      "title": "You only search once: Single shot neural architecture search via direct sparse optimization",
      "authors": [
        "Zhang"
      ],
      "year": "2020",
      "venue": "You only search once: Single shot neural architecture search via direct sparse optimization"
    },
    {
      "citation_id": "30",
      "title": "Multinomial distribution learning for effective neural architecture search",
      "authors": [
        "Zhen"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "31",
      "title": "Neural architecture search with reinforcement learning",
      "authors": [
        "Le Zoph",
        "B Zoph",
        "Q Le"
      ],
      "year": "2016",
      "venue": "Neural architecture search with reinforcement learning",
      "arxiv": "arXiv:1611.01578"
    }
  ]
}