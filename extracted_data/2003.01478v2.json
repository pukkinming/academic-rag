{
  "paper_id": "2003.01478v2",
  "title": "Multi-Task Learning With Auxiliary Speaker Identification For Conversational Emotion Recognition",
  "published": "2020-03-03T12:25:03Z",
  "authors": [
    "Jingye Li",
    "Meishan Zhang",
    "Donghong Ji",
    "Yijiang Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conversational emotion recognition (CER) has attracted increasing interests in the natural language processing (NLP) community. Different from the vanilla emotion recognition, effective speakersensitive utterance representation is one major challenge for CER. In this paper, we exploit speaker identification (SI) as an auxiliary task to enhance the utterance representation in conversations. By this method, we can learn better speaker-aware contextual representations from the additional SI corpus. Experiments on two benchmark datasets demonstrate that the proposed architecture is highly effective for CER, obtaining new state-of-the-art results on two datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition has been one hot topic in natural language processing (NLP) which aims to detect emotions in texts  [Wen and Wan, 2014; Li et al., 2015] . Recently, emotion recognition in conversions has been received increasing attentions  [Majumder et al., 2019; Zhong et al., 2019; Ghosal et al., 2019] . Given a sequence of utterances by multiple speakers, conversational emotion recognition (CER) aims to recognize emotion for each utterance. CER is a typical sequence labeling problem, and end-to-end neural sequence labeling models have achieved state-of-the-art performance  [Poria et al., 2017; Jiao et al., 2019] .\n\nIntuitively, speaker information can be greatly helpful for CER. For example, the last utterance of a same speaker could be severed as an important clue for the current utterance. Thus, how to effectively represent the speaker-sensitive utterances in conversions is critical for CER models. Previous studies, e.g.,  ConGCN [Zhang et al., 2019]  and  Dia-logueGCN [Ghosal et al., 2019] , build graphical structures over the input utterance sequences by speaker information and then exploit graph neural networks to model the dependencies, leading to better performance for CER.\n\nThe above models just adopt speakers as indicators to build connections over sequential utterances, using only CER training corpus to learn speaker-aware representations, which Figure  1 : Illustration of multi-task learning of our method. For CER, the emotion of every utterance in conversation is predicted. For SI, several pairs of utterances will be selected for binary classification, where 1 denotes the same speaker and 0 otherwise. could be insufficient for speaker exploration. In most cases, we can have much larger scale corpora of raw conversions, where no emotion information is annotated. These corpora could be potentially useful to learn speaker-aware contextual representations, since the utterances as well as the corresponding speaker identities are offered jointly in them. Speaker identification (SI) could be one good alternative for this purpose. As shown in Figure  1 , we can learn speakeraware contextual representations through the raw conversions by judging whether two utterances are from the same user.\n\nIn this work, we propose to use SI as one auxiliary task in order to obtain better speaker-aware contextual representations of the conversational utterances. We exploit a multi-task learning (MTL) framework to achieve our final goal to enhance CER. For CER and SI, we use the same network structure for utterance encoding, but with different set of model parameters. We adopt BERT as the basic representation to make our baseline strong, and hierarchical bidirectional gated recurrent neural networks (Bi-GRU) are exploited at the utterance-level and conversation-level to enhance the contextual representations. Further, we unite the two tasks with two bridging network structures by using an attention network  [Bahdanau et al., 2014]  and a gate mechanism  [Wu et al., 2019]  at the utterance-level and conversation-level for full mutual interaction, respectively .\n\nWe conduct experiments on two benchmark datasets to verify our framework, which are respectively EmoryNLP and MELD by name, both are sourced from the TV show of Friends. The results show that our baseline system is highly strong, achieving better performance than previous state-ofthe-arts. Our final model can lead to significant improvements on the two datasets both, which demonstrates the ef-fectiveness of our proposed method. In addition, we conduct extensive analysis work to examine the model in depth, for better understanding the advantages of our model. All codes and experimental settings will be released publicly available on https://github.com/ThdLee/CER for research purpose under Apache License 2.0.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition in conversational texts is generally treated as a sequence labelling problem in the literature. Traditional approaches often use lexicon-based and acoustic features to detect emotions  [Forbes-Riley and Litman, 2004; Devillers and Vidrascu, 2006] . Recently, deep learning based recurrent neural networks (RNN) and transformer can bring state-of-the-art performance  [Poria et al., 2017; Tzirakis et al., 2017; Zhong et al., 2019] , which is able to capture contextual utterance representations effectively. Our baseline CER model follow these settings, adopting a sophisticated RNN for contextualized representation learning.\n\nSeveral studies attempt to integrate speaker information in the conversational utterances, as it can affect the final CER performance much  [Hazarika et al., 2018b; Hazarika et al., 2018a] .  Majumder et al. [2019]  propose a recurrent model to detect emotion by tracking party state and global state dynamically. Graph convolutional networks (GCN) have been demonstrated stronger in modeling context-sensitive and speaker-sensitive dependence in conversation  [Zhang et al., 2019; Ghosal et al., 2019] . In this work, we propose an improved approach to better utilize speaker information by multi-task learning with a closely-related auxiliary task.\n\nMulti-task learning aims to model multiple relevant tasks simultaneously, which is capable of exploiting potential shared features across tasks effectively. There have been a number of successful studies in the NLP community  [Liu et al., 2017; Ma et al., 2018; Xiao et al., 2018; Pentyala et al., 2019; Wu et al., 2019] .  Zhou et al. [2019]  exploit language modeling as an auxiliary task to assist question generation task by multi-task learning, which motivates our work greatly. Similar to the language modeling task, speaker identification has rich training corpus, which can be collected automatically, and we exploit it to assist CER mainly.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Suppose we have a conversation with N consecutive utterances\n\nEach utterance u i is uttered by one speaker p S(ui) , where the function S maps the index of the utterance into its corresponding speaker. The objective of CER is to predict the emotion label of each utterance, and the objective the auxiliary task SI is to classify whether two given utterances u p , u q in a conversation are from the same speaker. In this section, we will introduce our specific-task model and multi-task learning framework in detail. Figure  2  shows the architecture of our baseline model, which exploits attention-based hierarchical network as encoder. Figure  3  depicts the multi-task learning framework of our proposed model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "W3",
      "text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" J q a p 7 p i I c W W 5 u p 2\n\n+ F q 0 5 J 5 s 5 h j 9 w P n 8 A D o C N p Q = = < / l a t e x i t > w 4 < l a t e x i t s h a 1 _ b a s e 6 4 = \" Z 4 V s f m\n\n+ F q 0 5 J 5 s 5 h j 9 w P n 8 A E A S N p g = = < / l a t e x i t > w5 < l a t e x i t s h a 1 _ b a s e 6 4 = \" + 8 r m s z E C I 9 E V 3 G o x P s 4 y 4 O 5 9 c p 8 = \" >\n\nFigure  2 : The overall architecture of our individual models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conversational Emotion Recognition",
      "text": "For the baseline CER model, first a hierarchy encoder is built on the input conversion utterance sequence, resulting in one feature vector for each utterance, then a standard emotion classification is performed based on the encoder sequence.\n\nOur hierarchy encoder consists of two components: individual utterance encoder and contextual utterance encoder, where the individual utterance encoder is an attention-based network with Bi-GRU, as shown by the right part of Figure  2 , and the contextual utterance encoder is a Bi-GRU over the output sequence of individual utterance encoder, aiming to capture the conversation-level context. 1  Individual Utterance Encoder The input of our model is a sequence of utterances\n\n, where L(i) is the length of u i . We exploit BERT as the basic encoder because it has achieved state-of-the-art performances for a number of NLP tasks  [Devlin et al., 2019] , obtaining the word-level output for u i :\n\nFurther, we adopt a single-layer Bi-GRU to further enhance the contextualized word representation at the utterance level, which can be formulated as:\n\nwhere h i, * denotes the Bi-GRU output for word w i, * .\n\nThe goal of individual utterance encoder is to derive a single feature vector for each utterance based on the covered words. Next, we need to aggregate the word-level outputs into a single vector for the utterance. We exploit an attentionbased aggregation to accomplish the goal. Formally, the utterance representation is defined as follows:\n\nwhere W a , b a and v a are model parameters, indicates vector transposition, u e i is the vector representation for utterance\n\nGate Gate\n\nR X e n N h 5 c d 6 d j 0 V r z s l m j u E P n M 8 f A x e P W g = = < / l a t e x i t > w i,4\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" 7 x B r s s   u i . Intuitively, α i,j is the importance of word w i,i in u i , and the weighted sum is adopted for the utterance representation. Contextual Utterance Encoder After the individual utterance we obtain a sequence of utterance-level representations {u 1 , • • • , u N }. These representations are solely sourced from their internal words. In order to encode utterance-level contextualized information, we build a second Bi-GRU as follows:\n\nwhere {f 1 , • • • , f N } is the final utterance presentations for prediction, which can capture surrounding contextual information in conversations.\n\nOutput Layer When the final contextualized utterance feature representation {f 1 , • • • , f N } is ready, we can calculate the output probability of each candidate emotion labels by a linear transformation followed with a softmax operation:\n\nwhere W CER is one model parameter, and p CER i denotes the output distribution of emotion labels for utterance u i . Training We optimize the CER model by minimizing the cross-entropy between the predicted emotion distribution and the true distribution. For a single conversation, the obejctive function is defined as follows:\n\nwhere N and K are the number of utterances in a conversation and emotion labels, respectively, y CER i is the one-hot vector of the ground truth emotion for utterance u i , and p CER i is the predicted distribution.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speaker Identification",
      "text": "As discussed before, speaker information plays an important role in CER. Here we use the same hierarchical network as CER to encoder utterances for SI. Similarly, we obtain the first stage utterance representations {u 1 , • • • , u N } by individual utterance encoder, and then achieve the second stage\n\nThe goal of SI is to determine whether two selected utterances are from the same speaker, which is a binary classification problem. To reach this goal, we randomly sample T pairs of utterances for classification. Note that we do not extract all utterance pairs in a conversion for a balance with CER. In addition, we do not really recognize the utterance speakers, as the classification category could be extremely large in some real scenarios, which makes the training very difficult. Classification Given a sampled pair of utterance representations f i , f j from a single conversation, we adopt four sources of features for SI classification: (1)f i , (2) f j , (3) f i -f j , and (4) f i f j ( denotes element-wise multiplication). We concatenate them, apply a nonlinear MLP layer to reach the final feature vector f SI i,j , and then perform binary classification based on it. The overall process can be formalized as follows:\n\nwhere W f , W SI and b f are model parameters, ⊕ denotes vector concatenation, and p SI i,j is a two-dimensional vector with one dim indicates the probability of the same speaker and the other dim denotes the probability of different speakers.\n\nTraining For SI, we also adopt the cross-entropy loss between the ground truth and the predicted distribution as the training objective:\n\nwhere y SI i,j is a two-dimensional vector for the ground-truth answer, and T is one hyperparameter. We randomly sample T times for the utterance pair (u i , u j ).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "The two above models for CER and SI have the same encoder network structure, which brings convenience in unite them under a multi-task learning framework. In this work, we keep the task-specific encoders, and exploit two bridging network structures for mutual interaction of the hierarchical encoders, where one network is designed for the utterancelevel individual utterance encoder, and the other is targeted to the conversation-level contextual utterance encoder.\n\nIndividual Utterance Encoder For the utterance-level individual utterance encoder, we exploit a shared-private structure to connect the two tasks. Concretely, as shown by the right part of Figure  3 , we add a shared Bi-GRU module to unite the two tasks. Given the BERT outputs\n\n), we compute shared hidden vectors by Bi-GRU first:\n\nand then design a gated mechanism to dynamically incorporate shared feature h sh i,j into task-specific features. The network is mostly inspired by  Wu et al. [2019] . The updated contextual word representations are computed as follows:\n\nwhere g i,j is a gate to control the portion of information flowing from the shared Bi-GRU layer, σ is a sigmoid function.\n\nFor the SI part, we only need to substitute h i,j into h i,j , and ĥi,j changes to ĥ i,j correspondingly. Finally, we use ĥi,j and ĥ i,j as the individual utterance encoder outputs for CER and SI, respectively.\n\nContextual Utterance Encoder For the union of contextual utterance encoder, we adopt a cross attention mechanism to augment the utterance representation from the task of the apart side. Taken the target CER model as an example, we obtain one kind of extra features from the SI contextual utterance encoder. Concretely, assuming that the contextual utterance representations of CER and SI are {f\n\n, respectively, then we compute the additional feature for f i by the following equations:\n\nwhere W c is one model parameter. The attention mechanism is mainly motivated by  Bahdanau et al. [2014]  for feature selection from the apart side. When the SI model is the target task, the mechanism is just performed at an opposite direction, and f i is obtained. Finally, we use { f1 , • • • , fN } and { f 1 , • • • , f N } instead for CER and SI decoding.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Task Training",
      "text": "For the multi-task learning of the two tasks, we simply sum the losses of the two individual tasks together as the joint objective:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our model on two benchmark datasets, MELD and EmoryNLP, following previous work  [Zhong et al., 2019] . Table  1  shows the corpus statistics.\n\nMELD  [Poria et al., 2019]  This is a multimodal dataset collected from TV show of Friends. There are seven emotion categories for each utterance, including anger, sadness, disgust, surprise, fear, joy and neutral.\n\nEmoryNLP  [Zahiri and Choi, 2018]  This dataset is also collected from TV show scripts of Friends. The difference lies in the type of emotion labels, which includes neutral, joyful, peaceful, powerful, scared, mad and sad.\n\nIn particular, we collect a large corpus for SI training, which is the entire scripts of the TV show of Friends, a superset of MELD and EmoryNLP.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Settings",
      "text": "We adopt Adam as the optimizer with the batch size of 4 to train our models, where the learning rates to fine-tune BERT and the other parameters are 1e-6 and 2.5e-4, respectively. Dropout rate with 0.5 is applied to avoid overfitting. The dimension sizes of the hidden states of all the BiGRUs is set to 200 on EmoryNLP and 150 on MELD. For evaluation, we exploit the standard weighted macro-F1 score as the major metric to measure all models, following Zhong et al.  [2019] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models",
      "text": "For a comprehensive evaluation, we compare our model with the following baselines as well: CNN  [Kim, 2014]  A convolutional neural network for utterance-level classification without using contextual information at the conversation level. c-LSTM  [Poria et al., 2017]  A hierarchical classification model based on LSTM-RNN model, where contextual utterance-level features are adopted.  DialogueRNN [Majumder et al., 2019]  A sophisticated RNN-based model based on three GRUs, which are used to model speakers, global contexts and historical emotions.  KET [Zhong et al., 2019]  The state-of-the-art model in the literature which exploits external commonsense knowledge to enhance the contextual utterance representation.  DialogueGCN [Ghosal et al., 2019]  A GCN-based model aiming to better representing inter-speaker dependence, where GRU is used as the basic feature composition modules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Developmental Results",
      "text": "We conduct experiments on the developmental datasets of EmoryNLP and MELD to examine our proposed models.\n\nThe Influence of T T represents the number of utterance pairs sampled from a conversation for SI, aiming to leverage the combined loss of the two tasks. We investigate the influence of T by ranging it from 1 to 5. Figure  4  shows the results. As shown, we can obtain the best results when T = 3 on EmoryNLP and T = 2 on MELD, respectively, demonstrating the importance of sampling. In addition, the optimum T of different datasets may vary, which could be possibly due to the different averaged conversation length.\n\nThe Influence of BERT Fine-Tuning The utilizing of BERT should be carefully studied. When BERT parameters are frozen, we can save the resource cost greatly, for example, the memory of GPU. However, it may lead to significant performance decrease. Here we study the gap between BERT fine-tuning and frozen. Figure  5    on EmoryNLP and MELD, respectively, demonstrating that fine-tuning is a necessary for CER.\n\nAblation Study To comprehensively study the effectiveness of the two bridging neural network structures at the different levels for mutual interaction, we conduct ablation experiments in detail. The results are offered in Table  2 . We can see that both the two networks can bring improved performance for CER. By excluding the bridging network at IUE, our final model falls by 0.19% on EmoryNLP and 0.50% on MELD, respectively. Similarly, the model shows 0.45% and 0.80% declines on the two datasets by eliminating the bridging network at CUE, respectively. When both network structures are removed, drops by 0.72% and 1.43% points on the two datasets are resulted, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Final Results",
      "text": "Table  3  shows the performance of various models on the test sections of the two datasets, respectively. We report the performance of our baseline model with three kinds of word representations, namely the pretrained glove word embeddings  [Pennington et al., 2014]    EmoryNLP and MELD datasets are 34.39% and 59.40%, respectively. Our BERT-based baseline gives F-scores of 34.76% and 61.31%, respectively, which are both higher than the previous state-of-the-arts.\n\nAccording to the results, our models with MTL can achieve better performance on two datasets as compared to their corresponding baselines. On the two datasets, the F1-score improvements based on the pretrained golve embeddings are 1.95% and 1.02%, respectively. When the contextualized ELMO representations are exploited, the improvements over the baseline are 1.30% and 0.76%, respectively. For our baseline based on BERT, the final MTL enhanced model leads to F1 increases of 1.16% and 0.59% on the two datasets, respectively. All the improvements by MTL are significant (p-value below 0.0001 by using pair-wised t-test). Interestingly, we can also find that the improvements become smaller as the baseline becomes stronger.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Analysis",
      "text": "For comprehensive understanding of our proposed models, we visualize the attention matrices by a case study, which is selected from the MELD test dataset. Figure  6  shows the example, where both salient words of individual utterance encoder and the key utterances of cross attention neural structure both are offered.\n\nFirst, we examine the difference in salient words for individual utterance encoder. As shown, the final model treats gimme it, she, Rachel and Pheebs as strong clues for CER, which are speaker related, while misses the punctuation such as comma and period, which are mostly objective. The observation indicates that the shared Bi-GRU module can help to identify speaker-related words such as speaker names and attributes, highlighting them for further feature representation, and meanwhile can effectively exclude the unimportant ob-jective words. In addition, the comparison further demonstrates the importance of the speaker information because of the better performance of our final model.\n\nAt the conversation-level, cross attention mechanism is used to identify closely-related utterances for a given utterance. Here we show the indexes of the related utterances to study the learned information by MTL with SI. As shown by the rightmost part of Figure  6 , we can see that the cross attention mechanism can help to associate the utterances with the same speaker and the next utterances of the targeted speakers for a specific utterance. For example, for the 9th utterance, the speaker is Phoebe, and the targeted speaker is Joey. By MTL, the model connects the 7th and 11th utterances from the same speaker, and meanwhile connects the 6th utterance from the target speaker. Intuitively, these utterances could be potential evidences to recognize the current emotion, which is demonstrated by our final model.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed a multi-task learning network for CER with the assistance of SI, aiming to better capture speaker-related information, which has been demonstrated important for CER. We exploited a strong baseline with BERT as backend, and then presented two neural network structures to bridge the two tasks for mutual interaction. We conducted on two benchmark datasets to verify the effectiveness of the proposed method. Results showed that our baseline is very strong, achieving the best performance compared with the previous state-of-the-art. Further, the MTL based method can boost the performance significantly, leading to a new state-of-the-art in the literature. Detailed experiments showed that our suggested components for MTL are both important. In addition, we analyzed the proposed model in depth for comprehensive understanding.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of multi-task learning of our method. For CER,",
      "page": 1
    },
    {
      "caption": "Figure 1: , we can learn speaker-",
      "page": 1
    },
    {
      "caption": "Figure 2: shows the architecture of our",
      "page": 2
    },
    {
      "caption": "Figure 3: depicts the multi-task learning",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall architecture of our individual models.",
      "page": 2
    },
    {
      "caption": "Figure 2: , and the contextual utterance encoder is a Bi-GRU over the",
      "page": 2
    },
    {
      "caption": "Figure 3: Illustration of our proposed model for CER and SI.",
      "page": 3
    },
    {
      "caption": "Figure 3: , we add a shared Bi-GRU mod-",
      "page": 4
    },
    {
      "caption": "Figure 4: Developmental experimental results of our model on",
      "page": 5
    },
    {
      "caption": "Figure 5: The inﬂuence of BERT Fine-Tuning.",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the compari-",
      "page": 5
    },
    {
      "caption": "Figure 6: Visualization of the attentions, where the thresholds are 0.1 and 0.2 for words and utterances by their attention values, respectively.",
      "page": 6
    },
    {
      "caption": "Figure 6: shows the",
      "page": 6
    },
    {
      "caption": "Figure 6: , we can see that the cross atten-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#conversations": "train/val/test",
          "#utterances": "train/val/test"
        },
        {
          "#conversations": "659/89/79",
          "#utterances": "7551/954/984"
        },
        {
          "#conversations": "1028/114/280",
          "#utterances": "9989/1109/2610"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Ablated performance on EmoryNLP and MELD, where",
      "data": [
        {
          "Column_1": "(a\nFi\nCN\nwhich\neaker\nDeve\nnduc\nNLP\nfluen\nampl\nmbin\nof T\nAss\noryN\ngthe\nffere\ndiffer\nnflue\nshou\nzen,\ne me\nrfor\nfine-\nults,\nvesti\npara",
          "Column_2": ")\ngur\n[Z\nal\n-se\nlo\nt e\nan\nc\ned\ned\nho\nLP\nim\nnt\nen\nnc\nld\nwe\nm\nma\ntu\nw\nga\nme",
          "Column_3": "Em\ne5\nha\nso\nnsi\npm\nxp\nd\neof\nfro\nlos\nby\nwn\na\npor\ndat\ntav\ne\nbe\nca\nory\nnce\nnin\nher\nted\nter",
          "Column_4": "oryN\n:Th\nng\nexp\ntive\nent\nerim\nMEL\nT\nma\ns o\nrang\n,w\nnd T\ntan\naset\nera\nof B\ncar\nn s\nof\nde\ng an\nethe\n. As\ns,dr",
          "Column_5": "LP\nein\net a\nloit\nde\nal\nent\nD\nT\nco\nf th\ning\neca\n=\nceo\nsm\nged\nE\neful\nave\nGP\ncrea\nd f\nba\nsh\nops"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Ablated performance on EmoryNLP and MELD, where",
      "data": [
        {
          "Column_1": "BER\n] A\nmod\ne.\ndev\nneou\nntst\nnfo\nsks.\n1 to\ntheb\nELD\nng.I\nwhic\nation\n-Tun\nd. W\nurce\never,\ne we\nigure\nodel\ncan\n2%",
          "Column_2": "(\nT\nm\nel\nel\nr\nhe\nrS\nW\n5.\nes\n,\nna\nh\nle\nin\nh\nco\nit\nst\n5\nan\nse\nan",
          "Column_3": "b) M\nFine\nulti-\nthe\nopm\npro\nnu\nI,a\ne i\nFi\ntre\nres\nddi\ncou\nngt\ng\nen\nst\nma\nudy\nsh\ndo\neth\nd4",
          "Column_4": "EL\n-Tu\nmo\nco\nen\npos\nmb\nimi\nnve\ngur\nsul\npec\ntion\nldb\nh.\nTh\nBE\ngrea\ny l\nth\now\nur\natb\n%c",
          "Column_5": "D\nning\ndal\nntex\ntal d\nedm\nerof\nngt\nstig\ne 4\ntsw\ntivel\n,th\nepo\ne ut\nRT p\ntly,\nead\ne ga\ns the\nfinal\nyfr\nanb"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Gold Baseline Final\nPhoebe: Hey everybody , Rachel was so good Phoebe: Hey everybody , Rachel was so good\njoy joy joy Phoebe\ntoday . She didn t gossip at all . today . She didn t gossip at all .\njoy surprise Rachel: I didn t ! anger Rachel: I didn t ! Rachel\nRachel: Even when I found out umm , all Rachel: Even when I found out umm , all\nright , well let s just say I found something right , well let s just say I found something\nneutral neutral neutral Rachel\nout something about someone and let s just out something about someone and let s just\nsay she s gonna keep it . say she s gonna keep it .\njoy joy Joey: Hey , Pheebs ! Check check this out . joy Joey: Hey , Pheebs ! Check check this out . Joey\nsurprise joy Phoebe: Ooh , you nailed the Old Lady ! surprise Phoebe: Ooh , you nailed the Old Lady ! Phoebe\nJoey: Yeah listen so , I thought I was getting Joey: Yeah listen so , I thought I was getting\nneutral neutral better , so on my way home today I stopped neutral better , so on my way home today I stopped Joey\nby this guitar store and by this guitar store and\nanger neutral Phoebe: Did you , did you anger Phoebe: Did you , did you Phoebe\nneutral neutral Joey: No. neutral Joey: No. Joey\nPhoebe: Give me your hands . Strings . Phoebe: Give me your hands . Strings .\nanger joy Gimme it ! Pick. Do you want to learn to anger Gimme it ! Pick. Do you want to learn to Phoebe\nplay guitar ? play guitar ?\njoy joy Joey: Yes ! joy Joey: Yes ! Joey\nanger anger Phoebe: Then do not touch one ! ! anger Phoebe: Then do not touch one ! ! Phoebe": "",
          "Column_2": "Phoebe"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Kyunghyun Dzmitry Bahdanau",
        "Yoshua Cho",
        "Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "2",
      "title": "Laurence Devillers and Laurence Vidrascu. Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "Vidrascu Devillers"
      ],
      "year": "2006",
      "venue": "ICSLP"
    },
    {
      "citation_id": "3",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "4",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Kate Forbes-Riley",
        "Diane Litman",
        "; Ghosal"
      ],
      "year": "2004",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "5",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "6",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Jiao"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "Sentence-level emotion classification with label and context dependence",
      "authors": [
        "Yoon Kim",
        "; Li"
      ],
      "year": "2014",
      "venue": "ACL-IJCNLP"
    },
    {
      "citation_id": "8",
      "title": "Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classification",
      "authors": [
        "Liu"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "9",
      "title": "Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Ma"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "10",
      "title": "Multi-task networks with universe, group, and task feature learning",
      "authors": [
        "Christopher Socher",
        "Manning ; Pentyala"
      ],
      "year": "2014",
      "venue": "Deep contextualized word representations",
      "arxiv": "arXiv:1802.05365"
    },
    {
      "citation_id": "11",
      "title": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "12",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Tzirakis"
      ],
      "year": "2017",
      "venue": "J-STSP"
    },
    {
      "citation_id": "13",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Shiyang Wen",
        "Xiaojun Wan",
        "; Wu"
      ],
      "year": "2014",
      "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Multi-task learning with language modeling for question generation",
      "authors": [
        "Zhou"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    }
  ]
}