{
  "paper_id": "2410.22023v3",
  "title": "Multi-Modal Speech Emotion Recognition Via Feature Distribution Adaptation Network",
  "published": "2024-10-29T13:13:30Z",
  "authors": [
    "Shaokai Li",
    "Yixuan Ji",
    "Peng Song",
    "Haoqin Sun",
    "Wenming Zheng"
  ],
  "keywords": [
    "Transfer learning",
    "Multi-modal speech emotion recognition",
    "Feature distribution adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a novel deep inductive transfer learning framework, named feature distribution adaptation network, to tackle the challenging multi-modal speech emotion recognition problem. Our method aims to use deep transfer learning strategies to align visual and audio feature distributions to obtain consistent representation of emotion, thereby improving the performance of speech emotion recognition. In our model, the pre-trained ResNet-34 is utilized for feature extraction for facial expression images and acoustic Mel spectrograms, respectively. Then, the cross-attention mechanism is introduced to model the intrinsic similarity relationships of multi-modal features. Finally, the multi-modal feature distribution adaptation is performed efficiently with feed-forward network, which is extended using the local maximum mean discrepancy loss. Experiments are carried out on two benchmark datasets, and the results demonstrate that our model can achieve excellent performance compared with existing ones. Our code is available at https://github.com/shaokai1209/FDAN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims at mining human's emotional information from speech signals, and has attracted extensive attention due to its wide practical applications  [23, 26] . In the past dacades, many machine learning methods have been applied to SER, e.g., support vector machine (SVM), Gaussian mixture model (GMM), least square regression, subspace learning, and deep learning methods  [23, 9] . These previous works have achieved satisfactory results in the SER task, but the emotion expression ability of the single modal feature is limited, and more involved modalities would boost the recognition performance. In this way, the challenging task for SER is referred to as multi-modal SER.\n\nTo tackle the above challenging task, some methods have tried to introduce linguistic information to improve the results of SER  [14, 32, 29, 8] . In  [14] , Liu et al. use self-attention convolutional neural network (CNN) and self-attention long-short term memory (LSTM) to learn the multi-scale fusion features of text and speech, respectively, and then improves results of multi-modal SER through a decision fusion layer. In  [32] , Yang et al. develop a cross-modal transformer to explore the interactions between text and audio modalities to improve the results of SER. In  [29] ,  Wang et al.  propose a modality-sensitive multi-modal speech emotion recognition framework to deal with this challenging task. In  [8] ,  Khan et al.  propose a deep feature fusion technique using a multi-headed cross-attention mechanism. However, according to 7% -38% -55% criterion for emotion expression  [17] , the emotional expression of linguistic information is far inferior to that of facial expression information. Therefore, in this paper, we focus on facial expression information and acoustic information to complete the challenging multi-modal SER task  [7, 30] .\n\nOver the past decades, transfer learning has proven to be an effective strategy to improve the generalization ability of emotion classification  [19, 5, 26] . Theoretically, different modals can be regarded as different domains, so that the multimodal emotion recognition task can be solved by a transfer learning framework. Recently, many studies have also successfully introduced transfer learning into multi-modal emotion recognition frameworks  [31, 21, 16, 3] , which can effectively improve the emotion recognition performance.\n\nHowever, the existing multi-modal classification methods do not fully consider the discrepancy in feature distribution between different modalities. Thus, different from previous multi-modal SER frameworks, in this work, we introduce the deep transfer learning framework for multi-modal SER, aiming to improve the generalization ability and recognition performance of emotion model by aligning the multi-modal feature distributions. According to  [18] , due to both modal feature domains of our framework are guided by label information during the training process, so our work belongs to the category of inductive transfer learning.\n\nIn this paper, we propose a novel deep transfer learning framework called feature distribution adaptation network (FDAN) for multi-modal SER tasks (see Fig.  1 ). Specifically, first, the pre-trained ResNet-34  [6]  on ImageNet is selected as the feature extraction for visual and speech modalities, respectively. Second, the cross-attention mechanism is introduced to model the intrinsic similarity relationships of multi-modal features. Third, the multi-modal feature distribution adaptation is achieved efficiently with feed-forward network by extending it with local maximum mean discrepancy (LMMD) loss  [34] . It is worth mentioning that the proposed model focuses on narrowing the feature distribution among different modalities. Compared to the existing multi-modal SER methods, the proposed method does not need to perform association learning on different modal features of the same sample. To verify the effectiveness of the proposed framework, we conduct extensive experiments on two popular datasets, and the experimental results indicate that the proposed framework can effectively improve the results of multi-modal SER tasks. , where x vn and x an represent samples in the visual and acoustic domains respectively, y vn and y an correspond to their labels respectively, and n v and n a represent the numbers of samples. p v and p a represent feature distributions in the visual and acoustic domains. p (c) a and p (c) v represent the feature distribution of samples belonging to class c in the visual and acoustic domains. X v and X a represent the visual and acoustic feature matrices. We define the feature subspace of the visual modality in the i-th layer as Z i v ∈ R nv×dv and the feature subspace of the acoustic modality in the i-th layer as Z i a ∈ R n×da . The loss of our model is formulated as\n\nwhere\n\nis the mathematical expectation of the class, f (•) is the consistent representation of emotion, and α is a trade-off parameter to balance the crossentropy loss and the domain adaptation loss.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross-Attention Module",
      "text": "To obtain the feature correlation between the visual and acoustic modalities, the features of each modality are first projected into three feature subspaces, known as query, key, and value. The process can be computed as follows:\n\nwhere Q v , K v , V v ∈ R dv×nv are the query, key, and value of the visual modality, and\n\nare the projection matrices of them. n v and d v represent the number and the feature dimension of samples of the visual modality respectively. The above calculation process can also yield the query, key and value, i.e., Q a , K a , V a ∈ R da×na of the acoustic modality, and their associated projection matrices\n\nFollowing  [24] , we first cross calculate the dot product of the query and key for visual and acoustic to obtain the association between the coupled modalities. The results are then scaled and normalized by the softmax function to obtain attention weights. Then, we use the corresponding weights to aggregate the value items of each feature sequence. The calculation formulas are as follows:\n\nwhere ∆Z i v→a ∈ R na×dv and ∆Z i a→v ∈ R nv×da are the propagated information of visual-to-audio and audio-to-visual, respectively.\n\nWe further update the features of one modality based on the propagation information of the other modality, the update rules are formulated as follows:\n\nwhere LN (•) represents the layer normalization, and F F N represents the feedforward network. Finally, the intrinsic similarity relationships of the different modalities propagate to each other in the feature subspace Z i j , j ∈ {a, v}. In the case of Z i a , the details of the cross-attention module are shown in Fig.  2 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Feature Distribution Adaptation",
      "text": "In order to obtain more plentiful consistent representation of emotion, we hope to align the feature distributions of the different modalities within the same category. As a popular distance metric, maximum mean discrepancy (MMD) is widely used to measure the discrepancy of the probability distribution between feature domains, so as to improve the generalization ability of emotion classification models  [11]    [15] . Unlike traditional MMD, this paper follows  [34] , but uses true labels to guide MMD learning. While aligning the feature distribution of different modalities, the feature distribution of samples belonging to the same label is more compact. This process is called local maximum mean discrepancy (LMMD). The objective function of LMMD is written as follows:\n\nwhere p (c) a and p (c) v represent the feature distribution of samples belonging to class c in the visual and acoustic domains. H is the reproducing kernel Hilbert space (RKHS), and φ(•) represents its mapping function, which is usually implemented based on a kernel function. We use the weight w c to determine which category each sample belongs to, and Eq. (  5 ) can be further transformed into the following form:\n\nwhere w vnc and w amc represent the weight of x vn and x am belonging to class c, and m ∨ n ∈ n a ∨ n v . w jnc (j ∈ {a, v}) can be computed as follows:\n\nwhere y jnc is the c-th entry of vector y jn . Obviously, the labels y vn and y am are the one-hot vectors, which are used to compute w vnc and w amc in equation  (6) . Note that we cannot directly calculate the φ(•) in Eq. (  6 ). To align the feature distributions of the coupled modalities, the network will generate the activation after the cross-attention module in the i-th layers as {z i vn } nv n=1 and {z i an } na n=1 ,\n\ni ∈ l = {1, 2, ..., l}. Thus, Eq. (  6 ) can be reformulated as follows:\n\nwhere\n\nis the kernel function, in which •, • represents the inner product of two vectors. Finally, the feature distribution adaptation loss of our model is written as follows:\n\nwhere f (•) is the consistent representation of emotion, α is a trade-off parameter.\n\n3 Experiments",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "The following two datasets are used in our experiments.\n\n-SAVEE  [4] : It is an acted English audio-visual emotional dataset, which consists of 480 utterances from four male actors in seven emotional categories.\n\n-MELD  [20] : It is an acted English audio-visual-text emotional dataset, which consists of 13,708 utterances from 1,433 dialogues from TV-series F riends in seven emotional categories.\n\nNote that the samples of all emotional categories (anger, disgust, fear, happiness/joy, neutral, sadness, surprise) from the above-mentioned two datasets are used in our experiments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "In our experiments, following the experimental settings of our previous works  [10, 28] , the visual modality is taken as the source domain, the acoustic modality is taken as the target domain, all samples of the source domain and a part of the target domain are taken for training, and the remaining samples of the target domain are taken for testing. More details are shown in Fig.  3 . We extract the Mel spectrograms as the features of the acoustic signals, and the video frames are uniformly sampled in {V 1 , V 2 , V 3 } to obtain more facial expression samples.\n\nFor the division of training and test sets, we selected 8/10 speech samples of each emotional categories and all visual samples for training, and the remaining 2/10 speech samples for testing. In all experiments, the value of momentum is set to 0.99. We fine-tune the value of α in {10 -3 , 10 -4 , 10 -5 } and fine-tune the value of decay in {10 -3 , 10 -4 }. The batch size is 32, and 300 epochs are executed in the training. We use the weighted average recall (WAR), unweighted average recall (UAR), and weighted F1-score (w-F1) as the experimental evaluation metrics. Note that the computer used in our experiments has a GeForce RTX 2080 Ti GPU and a 40 GB RAM, and the software environment is PyTorch 1.10.0.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "To assess the performance of our model, we compare it with the following pretrained conventional networks, state-of-the-art multi-feature fusion and multimodal SER methods.\n\n-Pre-trained models  [22] : We fine-tune the pre-trained VGG-16, VGG-19, ResNet-18, ResNet-34, and ResNet-50 in the Torchvision library. -INCA  [25] : A feature selection method for SER, which uses iterative neighborhood component analysis to select discriminative features. -GM-TCNet  [33] : A multi-scale feature fusion method for SER, which uses multi-scale receptive field to obtain emotional causality representation. -SMIN  [12] : A multi-modal emotion recognition method, which combines a semi-supervised learning framework with multi-modal emotion recognition. -TRIN  [2] : A multi-modal SER method, which explores the underlying associations between different modal features under the sequential temporal guidance.   [12]  65.59 -64.50 TRIN  [2]  79.60 70.52 -SDT  [16]  67.55 -66.60 CADF  [8]  -72.30 -MAP  [13]  78 -SDT  [16] : A multi-modal emotion recognition method, which introduces transformer-based model with self-distillation to transfer knowledge. -CADF  [8] : A multi-modal SER method, which uses the multi-headed crossattention mechanism to fuse multi-modal features. -TF-Mix  [27] : A feature fusion method for SER, which amalgamates various feature extraction techniques to enhance emotion recognition. -MAP  [13] : A multi-modal emotion recognition method, which presents a multi-modal attentive prompt learning framework to improve emotion recognition in conversations.\n\nSince the experimental settings of above methods are similar to ours and most of them do not have publicly available source code, we present and compare the reported results from their publications.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Results",
      "text": "The experimental results are shown in Tables  1  and 2 . From the tables, we have the following observations:\n\nFirst, the performance of multi-modal and feature fusion methods in SER is significantly better than that of traditional residual networks. In addition, among VGG and ResNet networks, ResNet-34 achieves the best recognition performance, which also indicates that ResNet-34 is more suitable as the feature extractor of our model.\n\nSecond, from Table  1 , we can find that our model achieves better results than INCA, GM-TCNet, and TF-Mix on the SAVEE dataset. The reason is that INCA only considers the feature selection, while GM-TCNet and TF-Mix consider feature fusion but only focuses on the acoustic modality. Moreover, from Table  2 , we can find that our model achieves better performance compared to SMIN, TRIN, SDT, CADF, and MAP on the MELD dataset. The reason is that SMIN considers semi-supervised multi-modal feature learning and ignores some label information. Meanwhile, although TRIN, SDT, and CADF fully consider multimodal label information, they only focus on the relationship between different modal features of the same sample, ignoring the feature distribution between different modalities. In addition, the MAP method obtain good recognition results by introducing the prompt learning strategy to fine-tune the pre-trained language model, learning the weighted feature fusion across text, visual, and speech modalities, which outperforms the proposed method on the w-F1 metric. However, MAP did not consider the discrepancy in feature distributions between modalities, resulting in poor overall performance compared with the proposed method.\n\nThird, compared with the baseline models, our method achieves better recognition results on both the SAVEE and MELD datasets. This demonstrates the validity of our model. The reason is that, in our model, the transfer learning strategy is introduced to align the multi-modal feature distribution, which can obtain a consistent emotion representation to improve the results of SER.\n\nLast, it is worth mentioning that compared to the existing multi-modal SER methods, the proposed method does not need to perform association learning on different modal features of the same sample. Thus, our method can use crossdataset or multi-dataset facial expression features to guide the learning of speech emotional feature representation. The results of the multi-dataset multi-modal SER are shown in Table  3 . Specifically, the task SAVEE{S v , M v , S a } uses all visual samples from the SAVEE and MELD datasets and 8/10 speech samples from the SAVEE for training, and the remaining 2/10 speech samples for testing. The results show that FDAN performs well for the multi-dataset multi-modal SER tasks. Compared with the experimental settings of the single dataset (see Tables  1  and 2 ), the results of the multi-modal SER are significantly improved.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Confusion Matrices",
      "text": "Fig.  4  shows the confusion matrices of the proposed method on the MELD and SAVEE datasets. As can be seen from the figure, first, the proposed method has a recognition accuracy of more than 75% for most emotion categories. Second, the SU, HA and SA emotions are easier to be confused with other emotions, and NE has the best recognition results. Finally, we can find that the recognition results  of SU on MELD and SAVEE differ greatly, indicating that given an emotion category, the proposed model cannot well recognize the samples of each dataset.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "T-Sne Visualization",
      "text": "To better show the effectiveness of the proposed FDAN, we give the data visualization results using the t-SNE algorithm  [1] . Fig.  5  illustrates the t-SNE visualizations of visual and acoustic modal features obtained by FDAN on the SAVEE and MELD datasets. Since MELD is a large dataset, too many samples will destroy the visualization effects, we randomly select 20% of the samples for visualization.\n\nFrom the results in Fig.  5 , it can be seen that the proposed FDAN method effectively aligns the feature distributions of the visual and acoustic modalities and retains emotional discriminative information, resulting in a consistent emotional representation. These results sufficiently demonstrates the effectiveness of the proposed FDAN method.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "To analyze the effectiveness, we further give the ablation study of FDAN. By removing the following components, i.e., cross-attention module and feature distribution adaptation, we can obtain two special cases of FDAN, i.e., FDAN 1 and FDAN 2 . From the Table  4 , we have the following two observations. First, when the cross-attention mechanism is ignored, the recognition results of FDAN 1 decrease significantly. This result proves that the cross-attention module plays a positive role in our framework. Second, when setting the α = 0, the feature distribution adaptation is ignored, the recognition accuracy of FDAN 2 on all datasets decreases significantly. This proves that the feature distribution adaptation also plays a positive role in our framework.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a novel deep inductive transfer learning framework, called feature distribution adaptation network (FDAN), for multi-modal SER problem. To the best of our knowledge, FDAN could be the first attempt to utilize deep transfer learning framework for multi-modal SER tasks. To demonstrate the effectiveness of our framework, extensive experiments are carried out on two popular datasets, and the results demonstrate that the proposed model achieves better performance than previous models. In the future, we would investigate to modify the proposed framework to tackle more challenging problems, such as the cross-corpus multi-modal SER.",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Speciﬁcally, ﬁrst, the pre-trained ResNet-34 [6] on ImageNet is selected",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of FDAN. The blue and yellow parts represent the learning",
      "page": 3
    },
    {
      "caption": "Figure 2: The structure of the cross-attention module.",
      "page": 4
    },
    {
      "caption": "Figure 3: We extract the",
      "page": 6
    },
    {
      "caption": "Figure 3: The framework of the training and testing process of the FDAN model.",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the confusion matrices of the proposed method on the MELD and",
      "page": 9
    },
    {
      "caption": "Figure 4: Confusion matrices of our model. The horizontal axis represents the predicted",
      "page": 10
    },
    {
      "caption": "Figure 5: illustrates the t-SNE",
      "page": 10
    },
    {
      "caption": "Figure 5: , it can be seen that the proposed FDAN method",
      "page": 10
    },
    {
      "caption": "Figure 5: The t-SNE data visualization results. The + and ◦represent the visual and",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-modal Speech Emotion Recognition via": "Feature Distribution Adaptation Network"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Shaokai Li1, Yixuan Ji2, Peng Song1, Haoqin Sun3, and Wenming Zheng4"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "1 School of Computer and Control Engineering, Yantai University, Yantai 264005,"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "China"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "2 Business School, Xuzhou University of Technology, Xuzhou 221018, China"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "3 College of Computer Science, Nankai University, Tianjin 300350, China"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "4 Key Laboratory of Child Development and Learning Science of Ministry of"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Education, Southeast University, Nanjing 210096, China"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Abstract.\nIn this paper, we propose a novel deep inductive\ntransfer"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "learning framework, named feature distribution adaptation network, to"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "tackle the challenging multi-modal speech emotion recognition problem."
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Our method aims\nto use deep transfer\nlearning strategies\nto align vi-"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "sual and audio feature distributions to obtain consistent representation"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "of emotion, thereby improving the performance of speech emotion recog-"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "nition.\nIn our model,\nthe pre-trained ResNet-34 is utilized for\nfeature"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "extraction for\nfacial expression images and acoustic Mel\nspectrograms,"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "respectively. Then, the cross-attention mechanism is introduced to model"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "the\nintrinsic\nsimilarity\nrelationships\nof multi-modal\nfeatures. Finally,"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "the multi-modal\nfeature distribution adaptation is performed eﬃciently"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "with feed-forward network, which is extended using the local maximum"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "mean discrepancy loss. Experiments are carried out on two benchmark"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "datasets, and the results demonstrate that our model can achieve excel-"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "lent performance compared with existing ones. Our code is available at"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "https://github.com/shaokai1209/FDAN."
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Keywords: Transfer learning· Multi-modal speech emotion recognition·"
        },
        {
          "Multi-modal Speech Emotion Recognition via": "Feature distribution adaptation."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nS. Li et al.": "et al. use self-attention convolutional neural network (CNN) and self-attention"
        },
        {
          "2\nS. Li et al.": "long-short term memory (LSTM) to learn the multi-scale fusion features of text"
        },
        {
          "2\nS. Li et al.": "and speech, respectively, and then improves results of multi-modal SER through"
        },
        {
          "2\nS. Li et al.": "a decision fusion layer.\nIn [32], Yang et al. develop a cross-modal transformer"
        },
        {
          "2\nS. Li et al.": "to explore the interactions between text and audio modalities\nto improve the"
        },
        {
          "2\nS. Li et al.": "results of SER.\nIn [29], Wang et al. propose a modality-sensitive multi-modal"
        },
        {
          "2\nS. Li et al.": "speech emotion recognition framework to deal with this\nchallenging task.\nIn"
        },
        {
          "2\nS. Li et al.": "[8], Khan et al. propose a deep feature fusion technique using a multi-headed"
        },
        {
          "2\nS. Li et al.": "cross-attention mechanism. However, according to 7% − 38% − 55% criterion"
        },
        {
          "2\nS. Li et al.": "for emotion expression [17], the emotional expression of linguistic information is"
        },
        {
          "2\nS. Li et al.": "far inferior to that of\nfacial expression information. Therefore,\nin this paper, we"
        },
        {
          "2\nS. Li et al.": "focus on facial expression information and acoustic information to complete the"
        },
        {
          "2\nS. Li et al.": "challenging multi-modal SER task [7,30]."
        },
        {
          "2\nS. Li et al.": "Over the past decades, transfer learning has proven to be an eﬀective strategy"
        },
        {
          "2\nS. Li et al.": "to improve the generalization ability of emotion classiﬁcation [19,5,26]. Theoret-"
        },
        {
          "2\nS. Li et al.": "ically, diﬀerent modals can be regarded as diﬀerent domains, so that the multi-"
        },
        {
          "2\nS. Li et al.": "modal emotion recognition task can be solved by a transfer learning framework."
        },
        {
          "2\nS. Li et al.": "Recently, many studies have also successfully introduced transfer learning into"
        },
        {
          "2\nS. Li et al.": "multi-modal emotion recognition frameworks [31,21,16,3], which can eﬀectively"
        },
        {
          "2\nS. Li et al.": "improve the emotion recognition performance."
        },
        {
          "2\nS. Li et al.": "However, the existing multi-modal classiﬁcation methods do not\nfully con-"
        },
        {
          "2\nS. Li et al.": "sider the discrepancy in feature distribution between diﬀerent modalities. Thus,"
        },
        {
          "2\nS. Li et al.": "diﬀerent\nfrom previous multi-modal SER frameworks,\nin this work, we intro-"
        },
        {
          "2\nS. Li et al.": "duce the deep transfer learning framework for multi-modal SER, aiming to im-"
        },
        {
          "2\nS. Li et al.": "prove the generalization ability and recognition performance of emotion model"
        },
        {
          "2\nS. Li et al.": "by aligning the multi-modal feature distributions. According to [18], due to both"
        },
        {
          "2\nS. Li et al.": "modal feature domains of our framework are guided by label\ninformation during"
        },
        {
          "2\nS. Li et al.": "the training process, so our work belongs to the category of\ninductive transfer"
        },
        {
          "2\nS. Li et al.": "learning."
        },
        {
          "2\nS. Li et al.": "In this paper, we propose a novel deep transfer learning framework called fea-"
        },
        {
          "2\nS. Li et al.": "ture distribution adaptation network (FDAN)\nfor multi-modal SER tasks (see"
        },
        {
          "2\nS. Li et al.": "Fig. 1). Speciﬁcally, ﬁrst, the pre-trained ResNet-34 [6] on ImageNet is selected"
        },
        {
          "2\nS. Li et al.": "as the feature extraction for visual and speech modalities, respectively. Second,"
        },
        {
          "2\nS. Li et al.": "the cross-attention mechanism is introduced to model the intrinsic similarity re-"
        },
        {
          "2\nS. Li et al.": "lationships of multi-modal\nfeatures. Third, the multi-modal\nfeature distribution"
        },
        {
          "2\nS. Li et al.": "adaptation is achieved eﬃciently with feed-forward network by extending it with"
        },
        {
          "2\nS. Li et al.": "local maximum mean discrepancy (LMMD) loss [34]. It is worth mentioning that"
        },
        {
          "2\nS. Li et al.": "the proposed model focuses on narrowing the feature distribution among diﬀerent"
        },
        {
          "2\nS. Li et al.": "modalities. Compared to the existing multi-modal SER methods, the proposed"
        },
        {
          "2\nS. Li et al.": "method does not need to perform association learning on diﬀerent modal features"
        },
        {
          "2\nS. Li et al.": "of the same sample. To verify the eﬀectiveness of the proposed framework, we"
        },
        {
          "2\nS. Li et al.": "conduct extensive experiments on two popular datasets, and the experimental"
        },
        {
          "2\nS. Li et al.": "results indicate that the proposed framework can eﬀectively improve the results"
        },
        {
          "2\nS. Li et al.": "of multi-modal SER tasks."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "process of visual and acoustic\nfeatures\nrespectively. The purple part\nrepresents\nthe"
        },
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "cross-attention mechanism. Xv and Xa represent visual and acoustic features extracted"
        },
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "by the pre-trained ResNet-34 respectively. By minimizing the LMMD loss, the feature"
        },
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "distribution discrepancy between the coupled feature subspaces Zi\nv and Zi\na in the i-th"
        },
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "layer\nis\nreduced, where i ∈ l. Yv and Ya\nrepresent\nthe true labels\nfor\nthe visual and"
        },
        {
          "Fig. 1: The\nframework of FDAN. The blue and yellow parts\nrepresent\nthe\nlearning": "acoustic samples, and ˆYv and ˆYa represent the corresponding predicted labels."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "T\nT\nT"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "(2)\n,\n,\nVv = W V\nKv = W K\nQv = W Q\nv Z i\nv Z i\nv Z i"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "are\nthe projection matrices of\nand\n, W V\n∈ Rdv×dv\nthem. nv"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "the number and the\nfeature dimension of\nsamples of\nthe visual"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "of\nthe acoustic modality, and their\ni.e., Qa, Ka, Va ∈ Rda×na"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ", W Q\n∈ Rda×da. Note that da = dv.\na , W V\na"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "Following [24], we ﬁrst cross calculate the dot product of the query and key"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "∆Z i\ndv)V T\nv→a = sof tmax(QT\na Kv/"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "(3)"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "p"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "∆Z i\nda)V T\nv Ka/\na→v = sof tmax(QT"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "p"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "a→v ∈ Rnv×da are the propagated information"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "We further update the features of one modality based on the propagation"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": ""
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "Z i\nv))\nv + F F N (Z i\nv = LN (Z i\na→v),\nv + ∆Z i\nv = LN (Z i"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "(4)"
        },
        {
          "Fig. 2: The structure of the cross-attention module.": "Z i\na = LN (Z i\na + ∆Z i\nv→a),\na = LN (Z i\na + F F N (Z i\na))"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "where LN (·) represents the layer normalization, and F F N represents the feed-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "forward network."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "Finally, the intrinsic similarity relationships of the diﬀerent modalities prop-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "agate to each other in the feature subspace Z i\na, the\nj, j ∈ {a, v}. In the case of Z i"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "details of the cross-attention module are shown in Fig. 2."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "2.3\nFeature distribution adaptation"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "In order to obtain more plentiful consistent representation of emotion, we hope"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "to align the\nfeature distributions of\nthe diﬀerent modalities within the\nsame"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "category. As a popular distance metric, maximum mean discrepancy (MMD) is"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "widely used to measure the discrepancy of the probability distribution between"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "feature domains,\nso as\nto improve the generalization ability of emotion classi-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "ﬁcation models\n[11]\n[15]. Unlike traditional MMD,\nthis paper\nfollows [34], but"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "uses true labels to guide MMD learning. While aligning the feature distribution"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "of diﬀerent modalities, the feature distribution of samples belonging to the same"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "label\nis more compact. This process is called local maximum mean discrepancy"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n5": "(LMMD). The objective function of LMMD is written as follows:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "C\nnv\nnv"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "1 C\nwvmcwvnck(zi\ndi(pv, pa) =\nvm, zi\nvn)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "(cid:20)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "m=1\nn=1\nX\nX\nX"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "(8)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "na\nna\nnv\nna"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "+\nwamcwanck(zi\nwvmcwanck(zi\nam, zi\nan)−2\nvm, zi\nan)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "(cid:21)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "m=1\nn=1\nm=1\nn=1\nX\nX\nX\nX"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "function,\nin which h·, ·i represents\nwhere k(zv, za) = hφ(zv), φ(za)i is the kernel"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "the inner product of two vectors."
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "Finally,\nthe feature distribution adaptation loss of our model\nis written as"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "follows:"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "na\nnv"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "1 n\n1 n\nmin\nJ (f (xan), yan) +\nJ (f (xvn), yvn) + α\ndi(pv, pa)\n(9)"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "f\na\nv"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "n=1\nn=1\nX\nX\nX"
        },
        {
          "i ∈ l = {1, 2, ..., l}. Thus, Eq. (6) can be reformulated as follows:": "where f (·) is the consistent representation of emotion, α is a trade-oﬀ parameter."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "to 0.99. We ﬁne-tune the value of α in {10−3, 10−4, 10−5} and ﬁne-tune the value"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "of decay in {10−3, 10−4}. The batch size is 32, and 300 epochs are executed in the"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "training. We use the weighted average recall (WAR), unweighted average recall"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "(UAR), and weighted F1-score (w-F1) as the experimental evaluation metrics."
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "Note that\nthe computer used in our experiments has a GeForce RTX 2080 Ti"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "GPU and a 40 GB RAM, and the software environment is PyTorch 1.10.0."
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "3.3\nBaselines"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "To assess the performance of our model, we compare it with the following pre-"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "trained conventional networks, state-of-the-art multi-feature fusion and multi-"
        },
        {
          "Fig. 3: The framework of the training and testing process of the FDAN model.": "modal SER methods."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To assess the performance of our model, we compare it with the following pre-": "trained conventional networks, state-of-the-art multi-feature fusion and multi-"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "modal SER methods."
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "– Pre-trained models\n[22]: We ﬁne-tune\nthe pre-trained VGG-16, VGG-19,"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "ResNet-18, ResNet-34, and ResNet-50 in the Torchvision library."
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "– INCA [25]: A feature selection method for SER, which uses iterative neigh-"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "borhood component analysis to select discriminative features."
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "– GM-TCNet\n[33]: A multi-scale feature fusion method for SER, which uses"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "multi-scale receptive ﬁeld to obtain emotional causality representation."
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "– SMIN [12]: A multi-modal emotion recognition method, which combines a"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "semi-supervised learning framework with multi-modal emotion recognition."
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "– TRIN [2]: A multi-modal SER method, which explores\nthe underlying as-"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "sociations between diﬀerent modal\nfeatures under\nthe sequential\ntemporal"
        },
        {
          "To assess the performance of our model, we compare it with the following pre-": "guidance."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: The recognition results of different models on the SAVEEdataset.",
      "data": [
        {
          "Ours": "Table 2: The recognition results of diﬀerent models on the MELD dataset.",
          "86.66": "",
          "86.19": "",
          "86.96": ""
        },
        {
          "Ours": "Models",
          "86.66": "WAR(%)",
          "86.19": "UAR(%)",
          "86.96": "w-F1(%)"
        },
        {
          "Ours": "VGG-16",
          "86.66": "50.31",
          "86.19": "49.51",
          "86.96": "49.03"
        },
        {
          "Ours": "VGG-19",
          "86.66": "54.53",
          "86.19": "54.08",
          "86.96": "53.33"
        },
        {
          "Ours": "ResNet-18",
          "86.66": "58.68",
          "86.19": "59.80",
          "86.96": "58.41"
        },
        {
          "Ours": "ResNet-34",
          "86.66": "63.82",
          "86.19": "63.10",
          "86.96": "62.91"
        },
        {
          "Ours": "ResNet-50",
          "86.66": "61.02",
          "86.19": "60.31",
          "86.96": "60.79"
        },
        {
          "Ours": "SMIN [12]",
          "86.66": "65.59",
          "86.19": "—",
          "86.96": "64.50"
        },
        {
          "Ours": "TRIN [2]",
          "86.66": "79.60",
          "86.19": "70.52",
          "86.96": "—"
        },
        {
          "Ours": "SDT [16]",
          "86.66": "67.55",
          "86.19": "—",
          "86.96": "66.60"
        },
        {
          "Ours": "CADF [8]",
          "86.66": "—",
          "86.19": "72.30",
          "86.96": "—"
        },
        {
          "Ours": "MAP [13]",
          "86.66": "78.50",
          "86.19": "78.50",
          "86.96": "78.40"
        },
        {
          "Ours": "Ours",
          "86.66": "81.16",
          "86.19": "75.91",
          "86.96": "80.72"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: The recognition results of different models on the SAVEEdataset.",
      "data": [
        {
          "S. Li et al.": "Table 1: The recognition results of diﬀerent models on the SAVEE dataset."
        },
        {
          "S. Li et al.": "Models"
        },
        {
          "S. Li et al.": "VGG-16"
        },
        {
          "S. Li et al.": "VGG-19"
        },
        {
          "S. Li et al.": "ResNet-18"
        },
        {
          "S. Li et al.": "ResNet-34"
        },
        {
          "S. Li et al.": "ResNet-50"
        },
        {
          "S. Li et al.": "INCA [25]"
        },
        {
          "S. Li et al.": "GM-TCNet [33]"
        },
        {
          "S. Li et al.": "TF-Mix [27]"
        },
        {
          "S. Li et al.": "Ours"
        },
        {
          "S. Li et al.": ""
        },
        {
          "S. Li et al.": "Models"
        },
        {
          "S. Li et al.": "VGG-16"
        },
        {
          "S. Li et al.": "VGG-19"
        },
        {
          "S. Li et al.": "ResNet-18"
        },
        {
          "S. Li et al.": "ResNet-34"
        },
        {
          "S. Li et al.": "ResNet-50"
        },
        {
          "S. Li et al.": "SMIN [12]"
        },
        {
          "S. Li et al.": "TRIN [2]"
        },
        {
          "S. Li et al.": "SDT [16]"
        },
        {
          "S. Li et al.": "CADF [8]"
        },
        {
          "S. Li et al.": "MAP [13]"
        },
        {
          "S. Li et al.": "Ours"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Specifically, the task SAVEE{S ,M ,S } uses all",
      "data": [
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "First,\nthe performance of multi-modal and feature fusion methods\nin SER"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "is\nsigniﬁcantly better\nthan that of\ntraditional\nresidual networks.\nIn addition,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "among VGG and ResNet networks, ResNet-34 achieves the best recognition per-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "formance, which also indicates\nthat ResNet-34 is more suitable as\nthe feature"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "extractor of our model."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "Second, from Table 1, we can ﬁnd that our model achieves better results than"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "INCA, GM-TCNet, and TF-Mix on the SAVEE dataset. The reason is that INCA"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "only considers the feature selection, while GM-TCNet and TF-Mix consider fea-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "ture fusion but only focuses on the acoustic modality. Moreover,\nfrom Table 2,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "we\ncan ﬁnd that our model achieves better performance compared to SMIN,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "TRIN, SDT, CADF, and MAP on the MELD dataset. The reason is that SMIN"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "considers semi-supervised multi-modal\nfeature learning and ignores some label"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "information. Meanwhile, although TRIN, SDT, and CADF fully consider multi-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "modal\nlabel\ninformation, they only focus on the relationship between diﬀerent"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "modal\nfeatures of\nthe same sample,\nignoring the feature distribution between"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "diﬀerent modalities. In addition, the MAP method obtain good recognition re-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "sults by introducing the prompt\nlearning strategy to ﬁne-tune the pre-trained"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "language model,\nlearning the weighted feature\nfusion across\ntext, visual, and"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "speech modalities, which outperforms the proposed method on the w-F1 metric."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "However, MAP did not consider the discrepancy in feature distributions between"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "modalities,\nresulting in poor overall performance compared with the proposed"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "method."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "Third, compared with the baseline models, our method achieves better recog-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "nition results on both the SAVEE and MELD datasets. This demonstrates the"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "validity of our model. The reason is\nthat,\nin our model,\nthe transfer\nlearning"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "strategy is introduced to align the multi-modal\nfeature distribution, which can"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "obtain a consistent emotion representation to improve the results of SER."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "Last,\nit is worth mentioning that compared to the existing multi-modal SER"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "methods, the proposed method does not need to perform association learning on"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "diﬀerent modal\nfeatures of the same sample. Thus, our method can use cross-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "dataset or multi-dataset facial expression features to guide the learning of speech"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "emotional\nfeature representation. The results of the multi-dataset multi-modal"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "SER are shown in Table 3. Speciﬁcally,\nthe task SAVEE{Sv, Mv, Sa} uses all"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "visual samples from the SAVEE and MELD datasets and 8/10 speech samples"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "from the SAVEE for training, and the remaining 2/10 speech samples for testing."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "The results\nshow that FDAN performs well\nfor\nthe multi-dataset multi-modal"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "SER tasks. Compared with the experimental settings of the single dataset (see"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "Tables 1 and 2), the results of the multi-modal SER are signiﬁcantly improved."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "3.5\nConfusion matrices"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "Fig. 4 shows the confusion matrices of the proposed method on the MELD and"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "SAVEE datasets. As can be seen from the ﬁgure, ﬁrst, the proposed method has a"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "recognition accuracy of more than 75% for most emotion categories. Second, the"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "SU, HA and SA emotions are easier to be confused with other emotions, and NE"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n9": "has the best recognition results. Finally, we can ﬁnd that the recognition results"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: The recognition results of the proposed FDAN in multi-dataset multi-modal",
      "data": [
        {
          "10\nS. Li et al.": "Table 3: The recognition results of the proposed FDAN in multi-dataset multi-modal"
        },
        {
          "10\nS. Li et al.": "SER tasks."
        },
        {
          "10\nS. Li et al.": "Tasks\nWAR(%)\nUAR(%)\nw-F1(%)"
        },
        {
          "10\nS. Li et al.": "SAVEE{Sv, Mv, Sa}\n87.50\n87.14\n87.70"
        },
        {
          "10\nS. Li et al.": "MELD{Sv, Mv, Ma}\n81.78\n78.31\n81.01"
        },
        {
          "10\nS. Li et al.": "(a) SAVEE\n(b) MELD"
        },
        {
          "10\nS. Li et al.": "Fig. 4: Confusion matrices of our model. The horizontal axis represents the predicted"
        },
        {
          "10\nS. Li et al.": "label, and the vertical axis represents the true label (AN: anger, DI: disgust, FE:\nfear,"
        },
        {
          "10\nS. Li et al.": "HA: happiness, NE: neutral, SA: sadness, and SU: surprise)."
        },
        {
          "10\nS. Li et al.": "of SU on MELD and SAVEE diﬀer greatly,\nindicating that given an emotion"
        },
        {
          "10\nS. Li et al.": "category, the proposed model cannot well recognize the samples of each dataset."
        },
        {
          "10\nS. Li et al.": "3.6\nt-SNE visualization"
        },
        {
          "10\nS. Li et al.": "To better\nshow the eﬀectiveness of\nthe proposed FDAN, we give the data vi-"
        },
        {
          "10\nS. Li et al.": "sualization results using the t-SNE algorithm [1]. Fig. 5 illustrates\nthe t-SNE"
        },
        {
          "10\nS. Li et al.": "visualizations of visual and acoustic modal\nfeatures obtained by FDAN on the"
        },
        {
          "10\nS. Li et al.": "SAVEE and MELD datasets. Since MELD is a large dataset, too many samples"
        },
        {
          "10\nS. Li et al.": "will destroy the visualization eﬀects, we randomly select 20% of the samples for"
        },
        {
          "10\nS. Li et al.": "visualization."
        },
        {
          "10\nS. Li et al.": "From the results in Fig. 5,\nit can be seen that the proposed FDAN method"
        },
        {
          "10\nS. Li et al.": "eﬀectively aligns the feature distributions of the visual and acoustic modalities"
        },
        {
          "10\nS. Li et al.": "and retains emotional discriminative information, resulting in a consistent emo-"
        },
        {
          "10\nS. Li et al.": "tional representation. These results suﬃciently demonstrates the eﬀectiveness of"
        },
        {
          "10\nS. Li et al.": "the proposed FDAN method."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: , we have the following two observations. First, when",
      "data": [
        {
          "AN": "DI"
        },
        {
          "AN": "FE"
        },
        {
          "AN": "HA"
        },
        {
          "AN": "NE"
        },
        {
          "AN": "SA"
        },
        {
          "AN": "SU"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: , we have the following two observations. First, when",
      "data": [
        {
          "NE\nNE": "SA\nSA"
        },
        {
          "NE\nNE": "SU\nSU"
        },
        {
          "NE\nNE": "(a) SAVEE\n(b) MELD"
        },
        {
          "NE\nNE": "Fig. 5: The t-SNE data visualization results. The + and ◦ represent"
        },
        {
          "NE\nNE": "acoustic\nsamples,\nrespectively, and diﬀerent colors"
        },
        {
          "NE\nNE": "gories (AN: anger, DI: disgust, FE: fear, HA: happiness, NE: neutral, SA: sadness, and"
        },
        {
          "NE\nNE": "SU: surprise)."
        },
        {
          "NE\nNE": "Table 4: The recognition results (WAR/UAR) (%) of the proposed FDAN and its two"
        },
        {
          "NE\nNE": "special cases,\ni.e., FDAN1 and FDAN2."
        },
        {
          "NE\nNE": "Datasets\nFDAN1\nFDAN2\nFDAN"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "achieves better performance than previous models. In the future, we would inves-",
          "S. Li et al.": ""
        },
        {
          "12": "",
          "S. Li et al.": "tigate to modify the proposed framework to tackle more challenging problems,"
        },
        {
          "12": "such as the cross-corpus multi-modal SER.",
          "S. Li et al.": ""
        },
        {
          "12": "References",
          "S. Li et al.": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. Boureau, Y.L., Bach, F., LeCun, Y., Ponce, J.: Learning mid-level\nfeatures\nfor"
        },
        {
          "References": "recognition. In: 2010 IEEE Computer Society Conference on Computer Vision and"
        },
        {
          "References": "Pattern Recognition. pp. 2559–2566.\nIEEE (2010)"
        },
        {
          "References": "2. Dong, G.N., Pun, C.M., Zhang, Z.: Temporal relation inference network for mul-"
        },
        {
          "References": "timodal speech emotion recognition.\nIEEE Transactions on Circuits and Systems"
        },
        {
          "References": "for Video Technology 32(9), 6472–6485 (2022)"
        },
        {
          "References": "3. Ghorbanali, A., Sohrabi, M.K.: Capsule network-based deep ensemble\ntransfer"
        },
        {
          "References": "learning for multimodal sentiment analysis. Expert Systems with Applications 239,"
        },
        {
          "References": "122454 (2024)"
        },
        {
          "References": "4. Haq, S., Jackson, P.J., Edge, J.: Speaker-dependent audio-visual emotion recogni-"
        },
        {
          "References": "tion. In: AVSP. vol. 2009, pp. 53–58 (2009)"
        },
        {
          "References": "5. Hazarika, D., Poria, S., Zimmermann, R., Mihalcea, R.: Conversational\ntransfer"
        },
        {
          "References": "learning for emotion recognition. Information Fusion 65, 1–12 (2021)"
        },
        {
          "References": "6. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:"
        },
        {
          "References": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition."
        },
        {
          "References": "pp. 770–778 (2016)"
        },
        {
          "References": "7.\nIto, K., Fujioka, T., Sun, Q., Nagamatsu, K.: Audio-visual speech emotion recog-"
        },
        {
          "References": "nition by disentangling emotion and identity attributes. In: Interspeech. pp. 4493–"
        },
        {
          "References": "4497 (2021)"
        },
        {
          "References": "8. Khan, M., Gueaieb, W., El Saddik, A., Kwon, S.: Mser: Multimodal speech emotion"
        },
        {
          "References": "recognition using cross-attention with deep fusion. Expert Systems with Applica-"
        },
        {
          "References": "tions p. 122946 (2023)"
        },
        {
          "References": "9. Latif, S., Rana, R., Khalifa, S., Jurdak, R., Qadir, J., Schuller, B.: Survey of deep"
        },
        {
          "References": "representation learning for speech emotion recognition. IEEE Transactions on Af-"
        },
        {
          "References": "fective Computing 14(2), 1634–1654 (2023)"
        },
        {
          "References": "10. Li, S., Song, P., Ji, L., Jin, Y., Zheng, W.: A generalized subspace distribution"
        },
        {
          "References": "adaptation framework for\ncross-corpus\nspeech emotion recognition.\nIn:\nICASSP"
        },
        {
          "References": "2023-2023 IEEE International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "References": "cessing (ICASSP). pp. 1–5. IEEE (2023)"
        },
        {
          "References": "11. Li, S., Song, P., Zhang, W.: Transferable discriminant linear regression for cross-"
        },
        {
          "References": "corpus speech emotion recognition. Applied Acoustics 197, 108919 (2022)"
        },
        {
          "References": "12. Lian, Z., Liu, B., Tao, J.: Smin: Semi-supervised multi-modal\ninteraction network"
        },
        {
          "References": "for conversational emotion recognition. IEEE Transactions on Aﬀective Computing"
        },
        {
          "References": "(2022)"
        },
        {
          "References": "13. Liang, X., Tu, G., Du, J., Xu, R.: Multi-modal attentive prompt learning for few-"
        },
        {
          "References": "shot\nemotion recognition in conversations. Journal of Artiﬁcial\nIntelligence Re-"
        },
        {
          "References": "search 79, 825–863 (2024)"
        },
        {
          "References": "14. Liu, Y., Sun, H., Guan, W., Xia, Y., Zhao, Z.: Multi-modal speech emotion recog-"
        },
        {
          "References": "nition using self-attention mechanism and multi-scale\nfusion framework. Speech"
        },
        {
          "References": "Communication 139,\n1–9 (2022)"
        },
        {
          "References": "15. Lu, C., Zong, Y., Zheng, W., Li, Y., Tang, C., Schuller, B.W.: Domain invariant"
        },
        {
          "References": "feature learning for speaker-independent speech emotion recognition. IEEE/ACM"
        },
        {
          "References": "Transactions on Audio, Speech, and Language Processing 30, 2217–2230 (2022)"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "16. Ma, H., Wang, J., Lin, H., Zhang, B., Zhang, Y., Xu, B.: A transformer-based"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "model with self-distillation for multimodal emotion recognition in conversations."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "IEEE Transactions on Multimedia (2023)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "17. Mehrabian, A.: Communication without words.\nIn: Communication Theory, pp."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "193–200. Routledge (2017)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "18. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Transactions on Knowl-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "edge and Data Engineering 22(10), 1345–1359 (2009)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "19. Poria, S., Cambria, E., Bajpai, R., Hussain, A.: A review of aﬀective\ncomput-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "ing: From unimodal analysis to multimodal\nfusion. Information Fusion 37, 98–125"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "(2017)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "20. Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., Mihalcea, R.: Meld:"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "A multimodal multi-party dataset for emotion recognition in conversations. arXiv"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "preprint arXiv:1810.02508 (2018)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "21.\nSharma, A., Sharma, K., Kumar, A.: Real-time\nemotional health detection us-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "ing ﬁne-tuned transfer networks with multimodal\nfusion. Neural Computing and"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "Applications 35(31), 22935–22948 (2023)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "22.\nShwartz-Ziv, R., Goldblum, M., Souri, H., Kapoor, S., Zhu, C., LeCun, Y., Wilson,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "A.G.: Pre-train your loss: Easy bayesian transfer learning with informative priors."
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "Advances in Neural Information Processing Systems 35, 27706–27715 (2022)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "23.\nSingh, Y.B., Goel, S.: A systematic literature review of speech emotion recognition"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "approaches. Neurocomputing 492, 245–263 (2022)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "24.\nSun, L., Liu, B., Tao, J., Lian, Z.: Multimodal cross-and self-attention network for"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "speech emotion recognition. In: ICASSP 2021-2021 IEEE International Conference"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "on Acoustics, Speech and Signal Processing (ICASSP). pp. 4275–4279. IEEE (2021)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "25. Tuncer, T., Dogan, S., Acharya, U.R.: Automated accurate speech emotion recog-"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "nition system using twine shuﬄe pattern and iterative neighborhood component"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "analysis techniques. Knowledge-Based Systems 211, 106547 (2021)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "26. Vettoruzzo, A., Bouguelia, M.R., Vanschoren, J., Rognvaldsson, T., Santosh, K.:"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "Advances and challenges in meta-learning: A technical review. IEEE Transactions"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "on Pattern Analysis and Machine Intelligence (2024)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "27. Wang, M., Ma, H., Wang, Y., Sun, X.: Design of smart home system speech emotion"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "recognition model based on ensemble deep learning and feature\nfusion. Applied"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "Acoustics 218, 109886 (2024)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "28. Wang, R., Song, P., Li, S., Ji, L., Zheng, W.: Common latent embedding space for"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "cross-domain facial expression recognition.\nIEEE Transactions on Computational"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "Social Systems (2023)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "29. Wang, S., Ma, Y., Ding, Y.: Exploring complementary features\nin multi-modal"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "speech emotion recognition. In: ICASSP 2023-2023 IEEE International Conference"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "on Acoustics, Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "30. Wei, J., Hu, G., Yang, X., Tuan, L.A., Dong, Y.: Audio-visual domain adaptation"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "feature fusion for speech emotion recognition. In: Proc. Interspeech 2022. pp. 1988–"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "1992 (2022)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "31. Yang, D., Huang, S., Kuang, H., Du, Y., Zhang, L.: Disentangled representation"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "learning for multimodal\nemotion recognition.\nIn: Proceedings of\nthe 30th ACM"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "International Conference on Multimedia. pp. 1642–1651 (2022)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "32. Yang, D., Huang, S., Liu, Y., Zhang, L.: Contextual and cross-modal\ninteraction"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "for multi-modal\nspeech emotion recognition.\nIEEE Signal Processing Letters 29,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "2093–2097 (2022)"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "33. Ye, J.X., Wen, X.C., Wang, X.Z., Xu, Y., Luo, Y., Wu, C.L., Chen, L.Y., Liu,"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "K.H.: Gm-tcnet: Gated multi-scale temporal convolutional network using emotion"
        },
        {
          "Feature distribution Adaptation Network for Speech Emotion Recognition\n13": "causality for speech emotion recognition. Speech Communication 145, 21–35 (2022)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "34. Zhu, Y., Zhuang, F., Wang, J., Ke, G., Chen, J., Bian, J., Xiong, H., He, Q.:",
          "S. Li et al.": ""
        },
        {
          "14": "",
          "S. Li et al.": "Deep subdomain adaptation network for image classiﬁcation."
        },
        {
          "14": "",
          "S. Li et al.": "on Neural Networks and Learning Systems 32(4), 1713–1722 (2020)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning mid-level features for recognition",
      "authors": [
        "Y Boureau",
        "F Bach",
        "Y Lecun",
        "J Ponce"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Temporal relation inference network for multimodal speech emotion recognition",
      "authors": [
        "G Dong",
        "C Pun",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "3",
      "title": "Capsule network-based deep ensemble transfer learning for multimodal sentiment analysis",
      "authors": [
        "A Ghorbanali",
        "M Sohrabi"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "4",
      "title": "Speaker-dependent audio-visual emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson",
        "J Edge"
      ],
      "year": "2009",
      "venue": "AVSP"
    },
    {
      "citation_id": "5",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "6",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Audio-visual speech emotion recognition by disentangling emotion and identity attributes",
      "authors": [
        "K Ito",
        "T Fujioka",
        "Q Sun",
        "K Nagamatsu"
      ],
      "year": "2021",
      "venue": "Audio-visual speech emotion recognition by disentangling emotion and identity attributes"
    },
    {
      "citation_id": "8",
      "title": "Mser: Multimodal speech emotion recognition using cross-attention with deep fusion. Expert Systems with Applications p",
      "authors": [
        "M Khan",
        "W Gueaieb",
        "A El Saddik",
        "S Kwon"
      ],
      "year": "2023",
      "venue": "Mser: Multimodal speech emotion recognition using cross-attention with deep fusion. Expert Systems with Applications p"
    },
    {
      "citation_id": "9",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "A generalized subspace distribution adaptation framework for cross-corpus speech emotion recognition",
      "authors": [
        "S Li",
        "P Song",
        "L Ji",
        "Y Jin",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Transferable discriminant linear regression for crosscorpus speech emotion recognition",
      "authors": [
        "S Li",
        "P Song",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "12",
      "title": "Smin: Semi-supervised multi-modal interaction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Multi-modal attentive prompt learning for fewshot emotion recognition in conversations",
      "authors": [
        "X Liang",
        "G Tu",
        "J Du",
        "R Xu"
      ],
      "year": "2024",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "14",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "15",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication Theory"
    },
    {
      "citation_id": "18",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "19",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "21",
      "title": "Real-time emotional health detection using fine-tuned transfer networks with multimodal fusion",
      "authors": [
        "A Sharma",
        "K Sharma",
        "A Kumar"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "22",
      "title": "Pre-train your loss: Easy bayesian transfer learning with informative priors",
      "authors": [
        "R Shwartz-Ziv",
        "M Goldblum",
        "H Souri",
        "S Kapoor",
        "C Zhu",
        "Y Lecun",
        "A Wilson"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "24",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "26",
      "title": "Advances and challenges in meta-learning: A technical review",
      "authors": [
        "A Vettoruzzo",
        "M Bouguelia",
        "J Vanschoren",
        "T Rognvaldsson",
        "K Santosh"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Design of smart home system speech emotion recognition model based on ensemble deep learning and feature fusion",
      "authors": [
        "M Wang",
        "H Ma",
        "Y Wang",
        "X Sun"
      ],
      "year": "2024",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "28",
      "title": "Common latent embedding space for cross-domain facial expression recognition",
      "authors": [
        "R Wang",
        "P Song",
        "S Li",
        "L Ji",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "29",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Audio-visual domain adaptation feature fusion for speech emotion recognition",
      "authors": [
        "J Wei",
        "G Hu",
        "X Yang",
        "L Tuan",
        "Y Dong"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "H Kuang",
        "Y Du",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Contextual and cross-modal interaction for multi-modal speech emotion recognition",
      "authors": [
        "D Yang",
        "S Huang",
        "Y Liu",
        "L Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "33",
      "title": "Gm-tcnet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Wen",
        "X Wang",
        "Y Xu",
        "Y Luo",
        "C Wu",
        "L Chen",
        "K Liu"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "34",
      "title": "Deep subdomain adaptation network for image classification",
      "authors": [
        "Y Zhu",
        "F Zhuang",
        "J Wang",
        "G Ke",
        "J Chen",
        "J Bian",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    }
  ]
}