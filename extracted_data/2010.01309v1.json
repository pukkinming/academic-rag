{
  "paper_id": "2010.01309v1",
  "title": "Personality Trait Detection Using Bagged Svm Over Bert Word Embedding Ensembles",
  "published": "2020-10-03T09:25:51Z",
  "authors": [
    "Amirmohammad Kazameini",
    "Samin Fatehi",
    "Yash Mehta",
    "Sauleh Eetemadi",
    "Erik Cambria"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, the automatic prediction of personality traits has received increasing attention and has emerged as a hot topic within the field of affective computing. In this work, we present a novel deep learning-based approach for automated personality detection from text. We leverage state of the art advances in natural language understanding, namely the BERT language model to extract contextualized word embeddings from textual data for automated author personality detection. Our primary goal is to develop a computationally efficient, high performance personality prediction model which can be easily used by a large number of people without access to huge computation resources. Our extensive experiments with this ideology in mind, led us to develop a novel model which feeds contextualized embeddings along with psycholinguistic features to a Bagged-SVM classifier for personality trait prediction. Our model outperforms the previous state of the art by 1.04% and, at the same time is significantly more computationally efficient to train. We report our results on the famous gold standard Essays dataset for personality detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Related Work",
      "text": "An individual's personality has a great impact on their lives, affecting their life choices, well-being, health and even preferences and desires. Hence, there is a huge interest to develop models which can automatically identify an individual's personality with important practical applications such as in recommendation systems  (Yin et al., 2018) , job screening  (Liem et al., 2018) , social network analysis  (Maria Balmaceda et al., 2014) , etc. Our model makes binary predictions of the author's personality based on the famous Big-Five  (Digman, 1990)  personality measure, which are the following five traits: Extraversion (EXT), Neuroticism (NEU), Agreeableness (AGR), Conscientiousness (CON) and Openness (OPN).\n\nCommon author personality detection techniques usually involve extracting psycholinguistic features from text, such as Linguistic Inquiry and Word Count (LIWC)  (Pennebaker et al., 2001) , Mairesse features  (Mairesse et al., 2007) , and SenticNet  (Cambria et al., 2018) , which are then fed into traditional machine learning classifiers such as support vector machine (SVM)  (Hearst et al., 1998) , Naïve Bayes, etc. More recent work leverage deep learning and make use of pre-trained word embeddings like Word2Vec  (Mikolov et al., 2013)  and Glove  (Pennington et al., 2014) . Recently,  (Mehta et al., 2020)  reviewed the latest advances in deep learning-based automated personality detection from the viewpoint of different input modalities along with recent techniques for effective multimodal personality prediction.\n\nThe previous state of the art  (Majumder et al., 2017)  on the Essays dataset also make use of a deep learning based approach with a convolutional network on top of word embeddings extracted from Word2Vec. They also incorporate other inputs such as the Mairesse features, word count, average sentence length, etc. for their final prediction. Their model outperformed the previous best (Mohammad and Kiritchenko, 2015) by 0.55%, whereas our model outperforms  (Majumder et al., 2017 ) by 1.04% and at the same time being significantly more computationally efficient to train.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "Each of our inputs is an essay with a mean size of around 650 words. The maximum number of tokens BERT can process at a time is 512. Hence, to extract maximum information from the input, we break the essay into multiple chunks (sub-documents), with the maximum length of a chunk being 200 tokens. All these sub-documents of a particular essay are then annotated with the same personality label as that essay. We experimented with various methods of pre-processing the essays text before feeding it to the BERT tokenizer and use the best performing one.\n\nWe split the text into a sequence of sentences at the period and question mark characters and remove all characters other than ASCII letters, digits, quotations and exclamation marks. We expand all shortenings (e.g., \"you're\" becomes \"you are\") which increases the maximum length of a sub-document from 200 to 250 tokens. After this initial pre-processing step, the sub-documents are then fed into the pre-trained BERT BASE model. For each layer of BERT, we average the contextual token representations of that layer. Then, we concatenate the last four layer representations and concatenate this with the corresponding 84 Mairesse features for the essay. This is then considered as the feature vector for the document, which is of the dimension IR 3156 .\n\nIn the classification phase, we feed the document feature vector to a SVM which predicts a binary label corresponding to a particular personality trait. To enhance the performance further, we use ten SVM classifiers to perform the prediction in parallel like the bagging classifier  (Breiman, 1996) . The estimator trains on all the features on the total number of the sub-document stack with replacement and the final predicted model for a document is done by majority voting.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Evaluation",
      "text": "Our model achieves a 1.04% increase in performance in comparison to the previous state of the art along with being significantly more computationally efficient. To put it in perspective,  (Majumder et al., 2017)  train their model on an Intel Core i7-4720 HQ CPU with the optimal configuration and it takes about 50 hours to complete. Our fine-tuning model only takes about 7 minutes to train. Table  1  gives a comparison of our model, BB-SVM, with others. We modify various parts of the BB-SVM model and discuss their effect on performance in the following section. We compare the model performance with context-independent word embeddings such as Word2Vec.\n\nTable  2  shows a comparison of the results. Also, results reported by  (Devlin et al., 2018)  suggest that concatenating the last four layers of BERT gives the best representation for a word. The comparison of BB-SVM results with inputs of different BERT layers is shown in Figure  2 . In the classification phase, we experimented with a SVM and a multi-layer perceptron for making the final personality trait predictions. We found that using a SVM results in better performance. We also experiment with feeding sub-document features to DocBERT  (Adhikari et al., 2019) , followed by averaging sub-document predictions to obtain the document's prediction. However, this did not improve the results. Table  3  shows a comparison of the results. We train the model by applying Bagging (using ten simultaneous SVM classifiers) and in line with previous studies  (Kim et al., 2002) , Bagging improved the classification accuracy for the task of personality detection as well (table  4 ). We also tried 2 different ways to extract the document features. In the traditional approach, the document features are directly constructed from word features. A different approach is to first construct sentence features using the word features and then construct the document features from these sentence features. Table  5  shows a comparison of the results.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we presented a computationally efficient deep learning-based model which outperformed the state of the art on the famous stream of consciousness Essays dataset. We hope that our model can be useful for research teams which do not have access to large computational resources. We believe a promising direction of future research would be to make more interpretable deep learning models which can provide valuable insights into the main psychological features driving these predictions and in turn also help advance psychological studies. Currently, the availability of quality personality datasets is quite limited. If an individual's personality can predicted with a little more reliability, there is scope for integrating automated personality detection in almost all human-machine interaction agents such as voice assistants, robots, cars, etc.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of our deep learning-based Bagged-SVM model for automated personality detec-",
      "page": 2
    },
    {
      "caption": "Figure 2: Figure 2: Accuracy of BB-SVM model with dif-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: A comparison of average accuracy of all",
      "data": [
        {
          "Personality Traits": "EXT\nNEU\nAGR\nCON\nOPN"
        },
        {
          "Personality Traits": "51.72\n50.2\n53.10\n50.79\n51.52"
        },
        {
          "Personality Traits": "55.13\n58.9\n55.35\n55.28\n59.57"
        },
        {
          "Personality Traits": "56.71\n58.09\n57.33\n56.71\n61.13"
        },
        {
          "Personality Traits": "59.30\n59.39\n57.84\n62.09\n56.52"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: shows a comparison of the results. Also, results reported by (Devlin et al., 2018) suggest that",
      "data": [
        {
          "Model\nId": "DocBERT",
          "Word\nEmbedding": "BERT",
          "Sentence\nFeature\nExtraction": "-",
          "Document\nFeature\nExtraction": "-",
          "Classiﬁer": "MLP",
          "Average\nAccuracy": "57.11"
        },
        {
          "Model\nId": "M11",
          "Word\nEmbedding": "BERT\n(layer 11)",
          "Sentence\nFeature\nExtraction": "Mean",
          "Document\nFeature\nExtraction": "CNN+Max",
          "Classiﬁer": "MLP",
          "Average\nAccuracy": "57.42"
        },
        {
          "Model\nId": "M12",
          "Word\nEmbedding": "BERT\n(layer 11)",
          "Sentence\nFeature\nExtraction": "Mean",
          "Document\nFeature\nExtraction": "CNN+GRU",
          "Classiﬁer": "MLP",
          "Average\nAccuracy": "57.42"
        },
        {
          "Model\nId": "M3",
          "Word\nEmbedding": "BERT\n(layer 11)",
          "Sentence\nFeature\nExtraction": "-",
          "Document\nFeature\nExtraction": "Mean",
          "Classiﬁer": "SVM",
          "Average\nAccuracy": "58.49"
        },
        {
          "Model\nId": "M14",
          "Word\nEmbedding": "BERT\n(layer 11)",
          "Sentence\nFeature\nExtraction": "-",
          "Document\nFeature\nExtraction": "Mean",
          "Classiﬁer": "Bagging-SVM",
          "Average\nAccuracy": "58.51"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Docbert: Bert for document classification",
      "authors": [
        "Ashutosh Adhikari",
        "Achyudh Ram",
        "Raphael Tang",
        "Jimmy Lin"
      ],
      "year": "2019",
      "venue": "Docbert: Bert for document classification",
      "arxiv": "arXiv:1904.08398"
    },
    {
      "citation_id": "2",
      "title": "Bagging predictors",
      "authors": [
        "Leo Breiman"
      ],
      "year": "1996",
      "venue": "Machine learning"
    },
    {
      "citation_id": "3",
      "title": "Discovering conceptual primitives for sentiment analysis by means of context embeddings",
      "authors": [
        "Erik Cambria",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Kenneth Kwok"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "4",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "5",
      "title": "Personality structure: Emergence of the five-factor model",
      "authors": [
        "John M Digman"
      ],
      "year": "1990",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "6",
      "title": "Support vector machines. IEEE Intelligent Systems and their applications",
      "authors": [
        "Marti Hearst",
        "Susan Dumais",
        "Edgar Osuna",
        "John Platt",
        "Bernhard Scholkopf"
      ],
      "year": "1998",
      "venue": "Support vector machines. IEEE Intelligent Systems and their applications"
    },
    {
      "citation_id": "7",
      "title": "Support vector machine ensemble with bagging",
      "authors": [
        "Hyun-Chul Kim",
        "Shaoning Pang",
        "Hong-Mo Je",
        "Daijin Kim",
        "Sung-Yang Bang"
      ],
      "year": "2002",
      "venue": "International Workshop on Support Vector Machines"
    },
    {
      "citation_id": "8",
      "title": "Psychology meets machine learning: Interdisciplinary perspectives on algorithmic job candidate screening",
      "authors": [
        "Cynthia Cs Liem",
        "Markus Langer",
        "Andrew Demetriou",
        "Annemarie Hiemstra",
        "Achmadnoer Sukma Wicaksana",
        "Marise Ph Born",
        "Cornelius König"
      ],
      "year": "2018",
      "venue": "Explainable and Interpretable Models in Computer Vision and Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "Using linguistic cues for the automatic recognition of personality in conversation and text",
      "authors": [
        "Marilyn Franc ¸ois Mairesse",
        "Matthias Walker",
        "Roger Mehl",
        "Moore"
      ],
      "year": "2007",
      "venue": "Journal of artificial intelligence research"
    },
    {
      "citation_id": "10",
      "title": "Deep learning-based document modeling for personality detection from text",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "11",
      "title": "How do personality traits affect communication among users in online social networks?",
      "authors": [
        "Maria Jose",
        "Silvia Balmaceda",
        "Daniela Schiaffino",
        "Godoy"
      ],
      "year": "2014",
      "venue": "Online Information Review"
    },
    {
      "citation_id": "12",
      "title": "Recent trends in deep learning based personality detection",
      "authors": [
        "Yash Mehta",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "13",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "14",
      "title": "Using hashtags to capture fine emotion categories from tweets",
      "authors": [
        "M Saif",
        "Svetlana Mohammad",
        "Kiritchenko"
      ],
      "year": "2015",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Linguistic inquiry and word count: Liwc",
      "authors": [
        "Martha James W Pennebaker",
        "Roger Francis",
        "Booth"
      ],
      "year": "2001",
      "venue": "Linguistic inquiry and word count: Liwc"
    },
    {
      "citation_id": "16",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "17",
      "title": "A network-enhanced prediction method for automobile purchase classification using deep learning",
      "authors": [
        "Han Yin",
        "Yue Wang",
        "Qian Li",
        "Wei Xu",
        "Ying Yu",
        "Tao Zhang"
      ],
      "year": "2018",
      "venue": "PACIS"
    }
  ]
}