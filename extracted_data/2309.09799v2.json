{
  "paper_id": "2309.09799v2",
  "title": "Watch The Speakers: A Hybrid Continuous Attribution Network For Emotion Recognition In Conversation With Emotion Disentanglement",
  "published": "2023-09-18T14:18:16Z",
  "authors": [
    "Shanglin Lei",
    "Xiaoping Wang",
    "Guanting Dong",
    "Jiang Li",
    "Yingjian Liu"
  ],
  "keywords": [
    "Natural language processing",
    "emotion recognition in conversation",
    "context modeling",
    "dialogue relationship"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) has attracted widespread attention in the natural language processing field due to its enormous potential for practical applications. Existing ERC methods face challenges in achieving generalization to diverse scenarios due to insufficient modeling of context, ambiguous capture of dialogue relationships and overfitting in speaker modeling. In this work, we present a Hybrid Continuous Attributive Network (HCAN) to address these issues in the perspective of emotional continuation and emotional attribution. Specifically, HCAN adopts a hybrid recurrent and attentionbased module to model global emotion continuity. Then a novel Emotional Attribution Encoding (EAE) is proposed to model intra-and inter-emotional attribution for each utterance. Moreover, aiming to enhance the robustness of the model in speaker modeling and improve its performance in different scenarios, A comprehensive loss function emotional cognitive loss LEC is proposed to alleviate emotional drift and overcome the overfitting of the model to speaker modeling. Our model achieves state-ofthe-art performance on three datasets, demonstrating the superiority of our work. Another extensive comparative experiments and ablation studies on three benchmarks are conducted to provided evidence to support the efficacy of each module. Further exploration of generalization ability experiments shows the plugand-play nature of the EAE module in our method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition in Conversation (ERC) is a rapidly growing research field within Natural Language Processing (NLP) that focuses on identifying the emotions conveyed in each utterance of a conversation. Different from the single sentence's emotional classification in explicit sentiment analysis  [1] -  [4] , this task contains samples with vastly different conversation lengths, ambiguous emotional expressions, and complex conversational relationships. Fig.  1  illustrates an example of the conversation scenario, where the utterance to be predicted (the last utterance) is influenced by the historical utterances of that conversation. As expected, ERC task has attracted the attention of many researchers due to its potential",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speaker Utterance",
      "text": "Fig.  1 . A example for the conversation in the MELD dataset.\n\nPervious ERC methods generally formulate the task as a supervised learning task based on different architectures of neural networks. This places a significant demand on the model's ability to capture the context of each utterance and effectively utilize speaker information  [12] . Moreover, various modeling methods for context and speaker have significantly raised the baseline, but there are still two remaining challenges of ERC need to solve. (1) Insufficient modeling of context. Existing works on context modeling can be broadly categorized into two types: The recurrent based methods  [13] -  [16]  focus on establishing more natural context temporal correlation. However, these methods may struggle to capture the global emotional continuity in long conversations. Although attention-based methods  [17] -  [20]  aim to aggregate emotional features at multiple levels, they may not be as effective as temporal models in capturing emotional continuity between speakers over time. These methods adopt a single and redundant network architecture, which results in a lack of generalization in context modeling. (2) Ambiguous capture of dialogue relationships. Studies  [21] ,  [22]  provide evidence that generating emotional responses can effectively improve the performance of ERC models. It can be inferred that in reallife conversations, more direct conversational relationships often lead to more direct emotional transmission. Nonetheless, the ERC field still lacks of detailed modeling of the emotional influence within and between speakers in the perspective of dialogue relationship.  (3)  Overfitting in speaker modeling. In the ERC task, speakers often exhibit distinct characteristics in their emotional expressions due to differences in identity and personality. To better leverage fine-grained information, several studies have made significant contributions  [23] ,  [24] . Although intricate network designs have been developed from various perspectives, such as speaker psychological states, dialogue memory, and relative positional relationships, these approaches have yielded limited results. Specifically, The models have encountered overfitting issues in different dialogue scenarios, which has hindered their effectiveness. Therefore, these three limitations greatly hinder the application of ERC models in real-world scenarios, which is precisely what our work aims to address.\n\nWe have proposed HCAN to effectively address the aforementioned issues. To tackle the problem of insufficient context modeling, we propose Emotional Continuation Encoding (ECE) to extract more robust features in different conversation situations, which comprehensively utilizes both the recurrent units and the attention blocks. The Attribution Theory  [25]  proposes that a stimulus triggers perception, which leads individuals to consider the situation, and physiological reactions lead to cognitive interpretation of physiological changes, both of which together result in emotional expression. Drawing inspiration from the Attribution Theory and accurately capturing dialogue relationships, we present Emotional Attribution Encoding (EAE) based on IA-attention, which models the intra-attribution and inter-attribution of each sentence in an attribution perspective. Due to the diverse input perturbations in conversations  [26] ,  [27] , we also design emotional cognitive loss to effectively enhance the model's robustness and extend the applicability of the overall model. The Emotional Cognitive loss L EC is composed of cross-entropy L cross , KL divergence L KL for predicting and recognizing emotions, and Adversarial Emotion Disentanglement loss L adv  [28] . Among them, cross-entropy calculation serves as the main emotional loss, KL divergence can alleviate emotional drift, and Adversarial Emotion Disentanglement loss can mitigate the overfitting of the model to speaker modeling.\n\nOur contributions are three-fold:\n\n(1) By combining the recurrent and attention-based approaches, our proposed ECE module achieves strong robustness in global emotion continuity modeling across different datasets, particularly demonstrating outstanding performance on long conversation samples.\n\n(2) Consider capturing dialogue relationships in the perpective of Attribution Theory, we propose an original IA-attention to extract intra-attribution and inter-attribution features, which offers a more direct and accurate modeling of human emotional comprehension.\n\n(3) Our model achieves state-of-the-art performance on three datasets, demonstrating the superiority of our work. The proposed EAE module is a plugin module that exhibits strong generalization and effectiveness across different baselines.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Recognition Of Conversation",
      "text": "The significant advancement of deep learning has greatly promoted the improvement of baseline performance in ERC tasks. Recently, ERC models can be categorized into two types: recurrent-based methods and attention-based methods.\n\n1) Recurrent-Based Methods: Through the use of a sequential network structure, recurrent-based methods have the potential to offer a more precise and authentic representation of the emotional dynamics present in a conversation: Dia-logueRNN  [13]  is the first to utilize a recurrent neural network for monitoring both speaker states and global states in conversations. COSMIC is a conversational model that integrates commonsense knowledge to enhance its performance. This model injects commonsense knowledge into Gated Recurrent Units (GRU) to capture features related to the internal state, external state, and intent state. The performance of SKAIG is enhanced by integrating action information inferred from the preceding context and the intention suggested by the subsequent context. DialogueCRN  [29]  is designed with multiturn reasoning modules that extract and integrate emotional clues. These modules perform an iterative process of intuitive retrieval and conscious reasoning, which imitates the distinctive cognitive thinking of humans. With the goal of achieving a comprehensive understanding of the dialogue, CauAIN  [30]  first retrieves and enhances causal clues in the dialogue through an external knowledge base. Then, it models intra-and inter-speaker interactions using GRUs.\n\n2) Attention-Based Methods: To enable the extraction of emotional features at both coarse-grained and fine-grained levels, attention-based methods often employ a variety of encoders and decoders with different levels and structures. KET  [17]  extracts concepts related to non-pause words in neutral discourse from a knowledge base and enhances the semantic representation of vectors using a dynamic context graph attention mechanism. Finally, a hierarchical selfattention mechanism is utilized to model the dialogue level. By leveraging four distinct attention mechanisms, DialogXL  [19] utilizes the language model layers of XLNet to encode multi-turn dialogues that are arranged in a sliding window. By regarding the internal structure of dialogue as a directed acyclic graph to encode utterances, DAG-ERC offers a more intuitive approach to modeling the information flow between the distant conversation background and the nearby context. TODKAT, as proposed in  [31] , presents a language model (LM) that is enhanced with topics through an additional layer specialized in detecting them. This model also incorporates commonsense statements obtained from a knowledge base based on the dialogue context.",
      "page_start": 2,
      "page_end": 5
    },
    {
      "section_name": "B. Dialogue Relation Extraction",
      "text": "The task of Relationship Extraction (RE) aims to identify the relationships that exist between pairs of entities within a document. While in dialogue scenarios, the task of extracting dialogue relations becomes more challenging due to the ellipsis of expression, the fuzziness of semantic reference and the presence of long-distance context dependencies.\n\nDialogRE  [32]  introduced the first human-annotated dataset for dialogue relationship extraction (DiaRE), which aims to capture the relationships between two arguments that arise in predictive conversations. Building upon this dataset, Chen  [21]  proposed a DiaRE method based on a graphical attention network that constructs meaningful graphs connecting speakers, entities, entity types, and corpus nodes to model the relationships between critical speakers. Similarly, Sun  [22]  proposed an utterance-aware graph neural network (ERMC-DisGCN) for ERMC, which leverages a relational convolution to propagate contextual information and takes into account the self-speaker dependency of interlocutors.\n\nDespite the promising results achieved by the aforementioned methods, they have not been validated on the ERC dataset. Furthermore, unlike directly identifying the current emotional state based on DiaRE, our approach extracts dialogue relationships from an attributional perspective and adds an emotional prediction loss to the task, which better aligns with human thought processes and enhances the robustness of the model in different scenarios.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we present the details of how to approach conversation modeling from a continuation-attribution perspective. The overview of HCAN is shown in Fig.  2 , which is consist of Emotional Continuation Encoding and Emotional Attribution Encoding.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Task Statement",
      "text": "In the ERC task, the goal is to identify the emotion s i of each utterance u i in a conversation [u 1 , u 2 , ..., u N ] by analyzing the dialogic context and the related speaker information p i in speaker set {p i , . . . , p M }, where the emotion should be selected from a pre-defined emotional target set S and each utterance corresponds to one speaker in the set of speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Emotional Continuation Encoding",
      "text": "To mimic the natural conversational flow between speakers, the bidirectional LSTM is employed to encode the utterances' feature c i ∈ R du in a temporal sequence as follows:\n\nwhere h i ∈ R 2du is the hidden state of the LSTM. Noted that the feature at the utterance-level of u i is represented by c i ∈ R du , and it is obtained through the employment of the COSMIC method for extraction.\n\nTo avoid the vanishing of emotional continuity over long time spans, we utilized a multi-head attention module to aggregate the global information from the LSTM encoding result G l as follows:\n\nwhere\n\nThe use of residual connections + ensures that even in the worst-case scenario, the global emotional state degrades to a temporal emotional state, thereby enhancing the robustness of the model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Emotional Attribution Encoding",
      "text": "Emotional Attribution Encoding is the core of this article, consisting of the IA-attention module and Emotional Cognitive loss. The IA-attention module efficiently captures the dialogue relationship and establishes emotional influence from the perspective of attribution. The Emotional Cognitive loss effectively mitigates the overfitting of modeling on different datasets.\n\n1) IA-Attention: Inspired by the attribution theory of emotion, we examine the emotional influnence in dialogue realtionships in an attributional prespective. Specially, we model emotional influence as intra-attribution and inter-attribution.\n\nTo achieve this, we introduce IA-attention, which is inspired by several works about self-attention mechanism  [33] -  [35] . This method views each global utterance representation g i as a query, which is mapped to intra-attribution partial space Q a and inter-attribution partial space Q e to get two different query embeddings q ia , q ie . Meanwhile, the historical utterance [g 1 , . . . g i-1 ] are also projected to K and V partial space to obtain k i and v i . To summarize, for each utterance, we apply different attribution attention matrices to get the intraatribution weighted sum and inter-attribution weighted sum which are divided by each utterance's speaker p i . The specific formula is as follows:\n\nwhere W Qa , W Qe , W KIA , W VIA ∈ R 2du×4du are trainable parameters, q ia , q ie , k j , k i ∈ R 4du and Z is the normalized factor.\n\nTo enable a more realistic perception in the dialogic relationship, the Gaussian Self-attention Mechanism  [36]  is introduced to distinguish the varying effects of dialogic temporal location. Assuming that the emotional attribution of historical utterances to the current utterance follows a normal distribution, the encoding results of the IA-attention module will be assigned weights that obey a Gaussian distribution, which is calculated as follows:\n\nwhere vi ∈ R 4du ,ϕ is a Gaussian distribution, µ and σ are their corresponding learnable parameters, d i,j  [36]  stands for distance measuring the turn-taking interval between speakers.\n\n2) Emotional Cognitive Loss: The emotional overfitting of the ERC task mainly focuses on emotional drift and speaker modeling. Motivated by multi-task learning  [37] ,  [38] , our proposed Emotional Cognitive loss L EC is mainly composed of basic cross-entropy L cross , KL divergence L KL for predicting and recognizing emotions, and Adversarial Emotion Disentanglement loss L adv . Among them, cross-entropy calculation is the main emotional loss, KL divergence can alleviate emotional drift, and Adversarial Emotion Disentanglement loss can overcome the overfitting of the model to speaker modeling.\n\nCross-Entropy Loss L cross , the key elements of which are computed as follows:\n\nwhere L is the total number of conversations in the trainset, τ (i) is the number of utterances in the conversation, y i,k denotes the one-hot vector and ŷi,k denotes probability vector for candidate emotional class n of the i th utterance in l th sample. KL Divergence L KL are calculated as follows:\n\nwhere λ θ ∈ R 4du×2du and W D ∈ R 2du×|E| denotes the emotional state generation network. |E| is the number of emotion labels. By utilizing a shared weight matrix W D to map the predicted emotion D tmp and the recognized emotion D src , the model is able to generate more accurate emotional representations in the current emotional state and make more precise inferences based on historical utterances.\n\nAdversarial Emotion Disentanglement: loss L adv is proposed to further prevent the model from excessively focusing on the emotional information of a dialogue role, inspired by adversarial training methods  [26] ,  [28] ,  [39] -  [41] . To be more specific, given an input sentence, we obtain its hidden representations using LSTM. Next, the model classify them based on predicted probability distributions. Then, we obtain the classification cross-entropy loss L cross . However, existing methods often being influenced by a specific dialogue role, it is difficult to consider the overall semantic information of the whole conversation, Therefore, we apply the Fast Gradient Value (FGV) technique  [39] ,  [40]  to approximate the worstcase perturbation as a noise vector:\n\nHere, the gradient represents the first-order derivative of the loss function L cross , and e denotes the direction of rapid increase in the loss function. We perform normalization and use a small ϵ to ensure the approximation is reasonable. Then, we add the noise vector v noise and conduct a second forward pass, obtaining a new adversarial loss L ′ cross . Therefore, we obtain the adversarial disentanglement loss function as follow:\n\nThe overall training loss, namely L EC calculated as:\n\nwhere αandβ are hyperparameter mentioned in Implementation Details. As a result, the combined loss facilitates the model's learning of emotional continuity coding and emotional attribution coding, ultimately improving its overall performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments A. Dataset",
      "text": "We assess the performance of HCAN on three benchmark datasets which are IEMOCAP  [42] , MELD  [43]  and EmoryNLP  [44] .\n\nIEMOCAP is a dataset recorded as dyadic conversational video clips with eight speaker participating in the training set while two speaker in testing set.\n\nMELD dataset is a multimodal dataset that has been expanded from the EmotionLines dataset with seven emotional labels. MELD is obtained from the popular TV show Friends and comprises over 1400 dialogues and 13000 utterances.\n\nEmoryNLP is a textual dataset also collected from the TV series Friends. The dataset comprises utterances that are categorized into seven distinct emotional classes.\n\nIn this work, we only consider the emotional classes for the MELD and EmoryNLP datasets. Additionally, we maintain consistency with COSMIC in terms of the train/val/test splits. The details of datasets are presented in",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Baselines",
      "text": "For the baselines, we mainly select two groups of outstanding models to compare with our approach.\n\n1) Recurrent-Based Methods: DialogueRNN  [13]  dynamically models emotions by taking into account the current speaker, contextual content, and emotional state, with a focus on distinguishing between different speakers. COSMIC  [14]  is a conversational model that incorporates commonsense knowledge to improve its performance which injects commonsense knowledge into Gated Recurrent Units to capture the internal state, external state, and intent state' features. SKAIG  [45]  is improved by incorporating action information inferred from the preceding context and the intention suggested by the subsequent context. Additionally, it utilized a CSK method to represent the edges with knowledge, and introduced a graphics converter to handle them. DialogueCRN  [29]  designs multiturn reasoning modules to extract and integrate emotional clues which performs an iterative process of intuitive retrieval and conscious reasoning, mimicking the unique cognitive thinking of humans.\n\n2) Attention-Based Methods: KET  [17]  utilizes external commonsense knowledge through the use of hierarchical selfattention and context-aware graph attention. This approach allows for dynamic incorporation of knowledge into transformers, resulting in a knowledge-enriched model. DAG-ERC  [12]  regards the internal structure of dialogue as a directed acyclic graph to encode utterances, providing a more intuitive approach to model the information flow between the distant conversation background and the nearby context. TODKAT  [31]  proposes a language model (LM) augmented with topics, which includes an additional layer specialized in detecting topics, and incorporates commonsense statements obtained from a knowledge base based on the dialogue context. CoG-BART  [20]  presents a new method that employs a contrastive loss and a task for generating responses to ensure that distinct emotions are mutually exclusive.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Implementation Detail",
      "text": "Following COSMIC  [14] , we only utilize utterance-level text features that are fine-tuned using RoBERTa  [46]  to accomplish the ERC task. We conduct all HCAN experiments with a learning rate of 1e-4. The batch size is set to 32 and the dropout rate is kept at 0.2. The number of LSTM layers was set to 2, 1, and 1 on IEMOCAP, MELD, and EmoryNLP datasets, respectively. The number of heads in standard multihead attention and IA-attention are 8 and 4, respectively. The hyperparameter α is set as 0.1, 0.2, 0.2 for IEMOCAP, MELD, and EmoryNLP datasets while β is unified as 0.05. The results reported in our experiments are based on the average score of 5 random runs on the test set. A server with one NVIDIA A100(40G) GPU is used to conduct our experiments. The addtional reproduction experiments are aligned to the baselines strictly.  datasets. Furthermore, compared to the previous state-of-theart (SOTA) models on IEMOCAP, MELD, and EmoryNLP, HCAN outperforms them by 1.18%, 0.95%, and 0.63%, respectively. IEMOCAP is known for having longer multi-turn dialogues and a well-balanced distribution of emotions, which allows for a more comprehensive evaluation of model performance. Our significant improvement(1.18%) in performance on this dataset successfully demonstrates the model's ability to model long-distance emotional continuity and effectiveness in dyadic conversational scenario. MELD and EmoryNLP datasets consist of multiple dialogue roles and shorter conversations, which closely resemble real-life scenarios. Additionally, these datasets have highly imbalanced emotion categories. Our model's improvement on these datasets demonstrates its effectiveness in capturing complex dialogue relationships and interpersonal emotional dependencies, as well as its robustness in recognizing different emotions. It is worth noting that the previous SOTA models were achieved using different models for each dataset, as the sample characteristics of each dataset vary significantly. However, our method unifies the SOTA across these benchmarks, demonstrating the generalizability of our approach in different application scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Main Result",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Ablation Studies",
      "text": "As shown in TBALE IV, we conducted more detailed ablation experiments to quantify the contributions of the ECE module, EAE module, L KL , L sec to the performance. (1) For ECE module, the ablation experiments leads to a performance decrease of 2.75%, 0.60%and 0.32% on IEMOCAP, MELD and EmoryNLP respectively, demonstrating its generalization on different scenarios and especially effectiveness in long conversation.  (2)  For EAE module, the removal of EAE leads to a performance decrease of 0.67%, 1.65% and 1.99%on IEMOCAP, MELD and EmoryNLP respectively. The results elaborate the effectiveness of EAE and the importance of emotional attribution modeling based on dialogue relationship.\n\n(3) For KL loss, the removal of L KL causes a decrease in model performance by 0.93% on the EmoryNLP dataset. This suggests the effectiveness of KL in detecting emotional shifts, as this dataset often contains emotional shifting samples. Overall, the unique contributions of different modules jointly contribute to the generalization and effectiveness of HCAN.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. The Exploration Of Generality",
      "text": "Regarding the universality of the EAE module, as it has strong transferability, we conducted experiments by adding it to different models based on recurrent and attention-based methods shown in TBALE II. The results show that our EAE module can effectively improve the performance of models based on different architectures. Moreover, the performance improvement on IEMOCAP, a dataset with long dialogues, is stronger than that on MELD, which has shorter conversations. Meanwhile, we observe that the improvement in models based on recurrent methods(i.e. COSMIC) is greater than that in models based on attention mechanisms(i.e TODKAT). This is logical because our EAE module is implemented based on attention mechanisms, which are naturally superior to temporal structures in modeling various levels of emotional attribution. It is reasonable to assume that attention-based methods implicitly capture emotional attribution to some extent, while our method captures more comprehensive emotional attribution information, leading to performance improvement.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "G. The Robustness Of Speaker Modeling",
      "text": "By incorporating the ECE module to capture the conversational dynamics, our model has successfully captured rich speaker characteristics. Our approach to modeling speaker robustness is primarily reflected in the Adversarial Emotion Disentanglement loss. To quantify the contribution of this loss in mitigating speaker modeling overfitting, we conducted experiments similar to those in the EAE module's generalization study. TBALE V shows that removing the L adv module results in a certain degree of performance degradation for the HCAN model. Conversely, adding the +L adv module to other baselines leads to significant performance improvements. For the SKAIG and COSMIC models, which utilize a large amount of common sense knowledge to model speaker emotions, our loss function effectively prevents overfitting on the IEMOCAP and MELD datasets, while maintaining their performance improvements. However, for models that focus on modeling conversational dynamics, such as DAG-ERC, the effect of loss improvement is limited. This is because their modeling of conversational dynamics enhances the robustness of speaker modeling to some extent.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "H. Case Study",
      "text": "Fig.  3  shows a segment of a dyadic conversation. Intuitively, the anger expressed by speakerB in the n th sentence seems to have been mainly triggered by his own surprise towards \"kiss\" and the question posed by speakerA in the n-1 th sentence. Meanwhile, the perfunctory response from speakerA in the 2 rd sentence may have also contributed to some extent.It is evident that the distribution of predictive emotion aligns with that of identifying the emotion, and the attention weights further demonstrate model's ability to effectively capture the relationship in long-distance conversations.\n\nV. CONCLUSION Insufficient modeling of context and ambiguous capture of dialogue relationships have been persistent challenges in improving the performance of ERC models. In this work, Presents? It was only a trivial, little brooch.\n\n[HAPPY]\n\nI knew nothing of the sort. You took presents from him.\n\nWell, what of it?\n\nWhat about me?",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates an",
      "page": 1
    },
    {
      "caption": "Figure 1: A example for the conversation in the MELD dataset.",
      "page": 1
    },
    {
      "caption": "Figure 2: The overall architecture of HCAN consisting of two main components, namely Emotional Continuation Encoding and Emotional Attribution Encoding.",
      "page": 3
    },
    {
      "caption": "Figure 3: shows a segment of a dyadic conversation. Intuitively,",
      "page": 7
    },
    {
      "caption": "Figure 3: Dyadic conversation for case study.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "#Dialogue": "Train\nVal",
          "#Utterance": "Train\nVal"
        },
        {
          "Dataset": "IEMOCAP",
          "#Dialogue": "120",
          "#Utterance": "5810"
        },
        {
          "Dataset": "MELD",
          "#Dialogue": "1039\n114",
          "#Utterance": "9989\n1109"
        },
        {
          "Dataset": "EmoryNLP",
          "#Dialogue": "659\n89",
          "#Utterance": "7551\n954"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "# classes": "6",
          "Metric": "Weighted Avg. F1",
          "# Speakers": "2"
        },
        {
          "Dataset": "EmoryNLP",
          "# classes": "7",
          "Metric": "Weighted Avg. F1",
          "# Speakers": "2-3"
        },
        {
          "Dataset": "MELD",
          "# classes": "7",
          "Metric": "Weighted Avg. F1",
          "# Speakers": "2-3"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "W-Avg F1",
          "MELD": "W-Avg F1",
          "EmoryNLP": "W-Avg F1"
        },
        {
          "Models": "KET",
          "IEMOCAP": "61.33",
          "MELD": "58.18",
          "EmoryNLP": "34.39"
        },
        {
          "Models": "DialogueRNN†",
          "IEMOCAP": "62.75",
          "MELD": "-",
          "EmoryNLP": "-"
        },
        {
          "Models": "TODKAT†",
          "IEMOCAP": "63.75",
          "MELD": "65.27",
          "EmoryNLP": "38.59"
        },
        {
          "Models": "DialogueGCN",
          "IEMOCAP": "64.37",
          "MELD": "58.10",
          "EmoryNLP": "-"
        },
        {
          "Models": "COSMIC†",
          "IEMOCAP": "65.28",
          "MELD": "64.21",
          "EmoryNLP": "37.61"
        },
        {
          "Models": "DialogXL",
          "IEMOCAP": "66.2",
          "MELD": "62.41",
          "EmoryNLP": "34.73"
        },
        {
          "Models": "DialogueCRN",
          "IEMOCAP": "66.33",
          "MELD": "58.39",
          "EmoryNLP": "-"
        },
        {
          "Models": "SKAIG†",
          "IEMOCAP": "65.79",
          "MELD": "65.18",
          "EmoryNLP": "37.57"
        },
        {
          "Models": "DAG-ERC†",
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "EmoryNLP": "38.94"
        },
        {
          "Models": "COG-BART",
          "IEMOCAP": "66.18",
          "MELD": "64.81",
          "EmoryNLP": "39.04"
        },
        {
          "Models": "DialogueRNN†\n+EAE",
          "IEMOCAP": "64.85(↑ 2.10)",
          "MELD": "-",
          "EmoryNLP": "-"
        },
        {
          "Models": "COSMIC†\n+EAE",
          "IEMOCAP": "67.77(↑ 2.50)",
          "MELD": "65.73(↑ 1.52)",
          "EmoryNLP": "38.71(↑ 1.10)"
        },
        {
          "Models": "TODKAT†\n+EAE",
          "IEMOCAP": "64.98(↑ 1.23)",
          "MELD": "65.87(↑ 0.60)",
          "EmoryNLP": "38.92(↑ 0.33)"
        },
        {
          "Models": "SKAIG†\n+EAE",
          "IEMOCAP": "68.09(↑ 2.30)",
          "MELD": "65.68(↑ 0.50)",
          "EmoryNLP": "38.50(↑ 1.07)"
        },
        {
          "Models": "DAG-ERC†\n+EAE",
          "IEMOCAP": "68.80(↑ 0.77)",
          "MELD": "64.73(↑ 1.08)",
          "EmoryNLP": "39.45(↑ 0.51)"
        },
        {
          "Models": "HCAN(Ours)",
          "IEMOCAP": "69.21",
          "MELD": "66.24",
          "EmoryNLP": "39.67"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "W-Avg F1",
          "MELD": "W-Avg F1",
          "EmoryNLP": "W-Avg F1"
        },
        {
          "Models": "HCAN",
          "IEMOCAP": "69.21",
          "MELD": "66.24",
          "EmoryNLP": "39.67"
        },
        {
          "Models": "- w/o ECE",
          "IEMOCAP": "66.46(↓ 2.75)",
          "MELD": "65.73(↓ 0.60)",
          "EmoryNLP": "38.95(↓ 0.72)"
        },
        {
          "Models": "- w/o EAE",
          "IEMOCAP": "68.57(↓ 0.67)",
          "MELD": "64.59(↓ 1.65)",
          "EmoryNLP": "37.28(↓ 2.39)"
        },
        {
          "Models": "- w/o LKL",
          "IEMOCAP": "69.13(↓ 0.08)",
          "MELD": "65.97(↓ 0.27)",
          "EmoryNLP": "38.74(↓ 0.93)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "IEMOCAP": "W-Avg F1",
          "MELD": "W-Avg F1",
          "EmoryNLP": "W-Avg F1"
        },
        {
          "Models": "DialogueRNN†",
          "IEMOCAP": "62.75",
          "MELD": "-",
          "EmoryNLP": "-"
        },
        {
          "Models": "TODKAT†",
          "IEMOCAP": "63.75",
          "MELD": "65.27",
          "EmoryNLP": "38.59"
        },
        {
          "Models": "COSMIC†",
          "IEMOCAP": "65.28",
          "MELD": "64.21",
          "EmoryNLP": "37.61"
        },
        {
          "Models": "SKAIG†",
          "IEMOCAP": "65.79",
          "MELD": "65.18",
          "EmoryNLP": "37.57"
        },
        {
          "Models": "DAG-ERC†",
          "IEMOCAP": "68.03",
          "MELD": "63.65",
          "EmoryNLP": "38.94"
        },
        {
          "Models": "DialogueRNN†\n+Ladv",
          "IEMOCAP": "63.61",
          "MELD": "-",
          "EmoryNLP": "-"
        },
        {
          "Models": "COSMIC†\n+Ladv",
          "IEMOCAP": "66.54",
          "MELD": "65.28",
          "EmoryNLP": "38.60"
        },
        {
          "Models": "TODKAT†\n+Ladv",
          "IEMOCAP": "64.12",
          "MELD": "65.54",
          "EmoryNLP": "38.78"
        },
        {
          "Models": "SKAIG†\n+Ladv",
          "IEMOCAP": "67.28",
          "MELD": "65.46",
          "EmoryNLP": "38.39"
        },
        {
          "Models": "DAG-ERC†\n+Ladv",
          "IEMOCAP": "68.40",
          "MELD": "64.73",
          "EmoryNLP": "39.12"
        },
        {
          "Models": "HCAN(Ours)−Ladv",
          "IEMOCAP": "69.03",
          "MELD": "65.92",
          "EmoryNLP": "39.29"
        },
        {
          "Models": "HCAN(Ours)",
          "IEMOCAP": "69.21",
          "MELD": "66.24",
          "EmoryNLP": "39.67"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fine-grained sentiment classification using bert",
      "authors": [
        "M Munikar",
        "S Shakya",
        "A Shrestha"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence for Transforming Business and Society (AITB)"
    },
    {
      "citation_id": "2",
      "title": "Sentibert: A transferable transformer-based architecture for compositional sentiment semantics",
      "authors": [
        "D Yin",
        "T Meng",
        "K.-W Chang"
      ],
      "year": "2020",
      "venue": "Sentibert: A transferable transformer-based architecture for compositional sentiment semantics",
      "arxiv": "arXiv:2005.04114"
    },
    {
      "citation_id": "3",
      "title": "Deep learning for aspect-based sentiment analysis: a comparative review",
      "authors": [
        "H Do",
        "P Prasad",
        "A Maag",
        "A Alsadoon"
      ],
      "year": "2019",
      "venue": "Expert systems with applications"
    },
    {
      "citation_id": "4",
      "title": "Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence",
      "authors": [
        "C Sun",
        "L Huang",
        "X Qiu"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "5",
      "title": "Sentiment analysis is a big suitcase",
      "authors": [
        "E Cambria",
        "S Poria",
        "A Gelbukh",
        "M Thelwall"
      ],
      "year": "2017",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "6",
      "title": "Social media analysis and public opinion: The 2010 uk general election",
      "authors": [
        "N Anstead",
        "B O'loughlin"
      ],
      "year": "2015",
      "venue": "Journal of computer-mediated communication"
    },
    {
      "citation_id": "7",
      "title": "Human-robot interaction: status and challenges",
      "authors": [
        "T Sheridan"
      ],
      "year": "2016",
      "venue": "Human factors"
    },
    {
      "citation_id": "8",
      "title": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2018",
      "venue": "Towards empathetic open-domain conversation models: A new benchmark and dataset",
      "arxiv": "arXiv:1811.00207"
    },
    {
      "citation_id": "9",
      "title": "Exploiting domain-slot related keywords description for few-shot cross-domain dialogue state tracking",
      "authors": [
        "G Qixiang",
        "G Dong",
        "Y Mou",
        "L Wang",
        "C Zeng",
        "D Guo",
        "M Sun",
        "W Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Semi-supervised knowledgegrounded pre-training for task-oriented dialog systems",
      "authors": [
        "W Zeng",
        "K He",
        "Z Wang",
        "D Fu",
        "G Dong",
        "R Geng",
        "P Wang",
        "J Wang",
        "C Sun",
        "W Wu"
      ],
      "year": "2022",
      "venue": "Semi-supervised knowledgegrounded pre-training for task-oriented dialog systems",
      "arxiv": "arXiv:2210.08873"
    },
    {
      "citation_id": "11",
      "title": "Improving few-shot performance of dst model through multitask to better serve language-impaired people",
      "authors": [
        "M Sun",
        "Q Gao",
        "Y Mou",
        "G Dong",
        "R Liu",
        "W Guo"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "12",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "13",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "14",
      "title": "Cosmic: Commonsense knowledge emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "15",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "16",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "17",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    },
    {
      "citation_id": "18",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "19",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "21",
      "title": "Dialogue relation extraction with document-level heterogeneous graph attention networks",
      "authors": [
        "H Chen",
        "P Hong",
        "W Han",
        "N Majumder",
        "S Poria"
      ],
      "year": "2023",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "22",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Y Sun",
        "N Yu",
        "G Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "23",
      "title": "A multi-task semantic decomposition framework with task-specific pre-training for few-shot ner",
      "authors": [
        "G Dong",
        "Z Wang",
        "J Zhao",
        "G Zhao",
        "D Guo",
        "D Fu",
        "T Hui",
        "C Zeng",
        "K He",
        "X Li"
      ],
      "year": "2023",
      "venue": "A multi-task semantic decomposition framework with task-specific pre-training for few-shot ner",
      "arxiv": "arXiv:2308.14533"
    },
    {
      "citation_id": "24",
      "title": "Coach: A coarse-to-fine approach for cross-domain slot filling",
      "authors": [
        "Z Liu",
        "G Winata",
        "P Xu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Coach: A coarse-to-fine approach for cross-domain slot filling",
      "arxiv": "arXiv:2004.11727"
    },
    {
      "citation_id": "25",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "S Schachter",
        "J Singer"
      ],
      "year": "1962",
      "venue": "Psychological review"
    },
    {
      "citation_id": "26",
      "title": "Pssat: A perturbed semantic structure awareness transferring method for perturbation-robust slot filling",
      "authors": [
        "G Dong",
        "D Guo",
        "L Wang",
        "X Li",
        "Z Wang",
        "C Zeng",
        "K He",
        "J Zhao",
        "H Lei",
        "X Cui"
      ],
      "year": "2022",
      "venue": "Pssat: A perturbed semantic structure awareness transferring method for perturbation-robust slot filling",
      "arxiv": "arXiv:2208.11508"
    },
    {
      "citation_id": "27",
      "title": "Revisit out-of-vocabulary problem for slot filling: A unified contrastive framework with multi-level data augmentations",
      "authors": [
        "D Guo",
        "G Dong",
        "D Fu",
        "Y Wu",
        "C Zeng",
        "T Hui",
        "L Wang",
        "X Li",
        "Z Wang",
        "K He"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "A robust contrastive alignment method for multi-domain text classification",
      "authors": [
        "X Li",
        "H Lei",
        "L Wang",
        "G Dong",
        "J Zhao",
        "J Liu",
        "W Xu",
        "C Zhang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP (1)"
    },
    {
      "citation_id": "30",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "L Zhu",
        "G Pergola",
        "L Gui",
        "D Zhou",
        "Y He"
      ],
      "year": "2021",
      "venue": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "arxiv": "arXiv:2106.01071"
    },
    {
      "citation_id": "32",
      "title": "Dialogue-based relation extraction",
      "authors": [
        "D Yu",
        "K Sun",
        "C Cardie",
        "D Yu"
      ],
      "year": "2020",
      "venue": "Dialogue-based relation extraction",
      "arxiv": "arXiv:2004.08056"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for kbqa",
      "authors": [
        "G Dong",
        "R Li",
        "S Wang",
        "Y Zhang",
        "Y Xian",
        "W Xu"
      ],
      "year": "2023",
      "venue": "Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for kbqa",
      "arxiv": "arXiv:2308.14436"
    },
    {
      "citation_id": "35",
      "title": "Entity-level interaction via heterogeneous graph for multimodal named entity recognition",
      "authors": [
        "G Zhao",
        "G Dong",
        "Y Shi",
        "H Yan",
        "W Xu",
        "S Li"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "36",
      "title": "Gaussian transformer: a lightweight approach for natural language inference",
      "authors": [
        "M Guo",
        "Y Zhang",
        "T Liu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Generative zero-shot prompt learning for cross-domain slot filling with inverse prompting",
      "authors": [
        "X Li",
        "L Wang",
        "G Dong",
        "K He",
        "J Zhao",
        "H Lei",
        "J Liu",
        "W Xu"
      ],
      "year": "2023",
      "venue": "Generative zero-shot prompt learning for cross-domain slot filling with inverse prompting",
      "arxiv": "arXiv:2307.02830"
    },
    {
      "citation_id": "38",
      "title": "A prototypical semantic decoupling method via joint contrastive learning for few-shot named entity recognition",
      "authors": [
        "G Dong",
        "Z Wang",
        "L Wang",
        "D Guo",
        "D Fu",
        "Y Wu",
        "C Zeng",
        "X Li",
        "T Hui",
        "K He"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Explaining and harnessing adversarial examples",
      "authors": [
        "I Goodfellow",
        "J Shlens",
        "C Szegedy"
      ],
      "year": "2014",
      "venue": "Explaining and harnessing adversarial examples",
      "arxiv": "arXiv:1412.6572"
    },
    {
      "citation_id": "40",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2016",
      "venue": "Adversarial examples in the physical world",
      "arxiv": "arXiv:1607.02533"
    },
    {
      "citation_id": "41",
      "title": "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "authors": [
        "T Manzini",
        "Y Lim",
        "Y Tsvetkov",
        "A Black"
      ],
      "year": "2019",
      "venue": "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "arxiv": "arXiv:1904.04047"
    },
    {
      "citation_id": "42",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "43",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "44",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "45",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "46",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    }
  ]
}