{
  "paper_id": "2212.07088v1",
  "title": "Unsupervised Time-Aware Sampling Network With Deep Reinforcement Learning For Eeg-Based Emotion Recognition",
  "published": "2022-12-14T08:25:28Z",
  "authors": [
    "Yongtao Zhang",
    "Yue Pan",
    "Yulin Zhang",
    "Linling Li",
    "Li Zhang",
    "Gan Huang",
    "Zhen Liang",
    "Zhiguo Zhang"
  ],
  "keywords": [
    "Electroencephalography",
    "Affective Brain-Computer Interface",
    "Emotion Recognition",
    "Deep Reinforcement Learning",
    "Unsupervised Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing human emotions from complex, multivariate, and non-stationary electroencephalography (EEG) time series is essential in affective brain-computer interface. However, because continuous labeling of ever-changing emotional states is not feasible in practice, existing methods can only assign a fixed label to all EEG timepoints in a continuous emotionevoking trial, which overlooks the highly dynamic emotional states and highly non-stationary EEG signals. To solve the problems of high reliance on fixed labels and ignorance of time-changing information, in this paper we propose a timeaware sampling network (TAS-Net) using deep reinforcement learning (DRL) for unsupervised emotion recognition, which is able to detect key emotion fragments and disregard irrelevant and misleading parts. Specifically, we formulate the process of mining key emotion fragments from EEG time series as a Markov decision process and train a time-aware agent through DRL without label information. First, the time-aware agent takes deep features from a feature extractor as input and generates samplewise importance scores reflecting the emotion-related information each sample contains. Then, based on the obtained samplewise importance scores, our method preserves top-X continuous EEG fragments with relevant emotion and discards the rest. Finally, we treat these continuous fragments as key emotion fragments and feed them into a hypergraph decoding model for unsupervised clustering. Extensive experiments are conducted on three public datasets (SEED, DEAP, and MAHNOB-HCI) for emotion recognition using leave-one-subject-out cross-validation, and the results demonstrate the superiority of the proposed method against previous unsupervised emotion recognition methods. The proposed TAS-Net has great potential in achieving a more practical and accurate affective brain-computer interface in a dynamic and label-free circumstance. The source code is made available at https://github.com/infinite-tao/TAS-Net.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "H UMAN emotion is a complex and dynamic process involving different levels of processing and integration  [1] ,  [2] . In emotion-related studies, how to accurately describe * Corresponding author: Zhen Liang and Zhiguo  Zhang.  and further recognize human emotions has been a critical issue in the last decade. Recently, the developments in using electroencephalography (EEG) signals for emotion recognition have gained increasing attention from researchers with a various of backgrounds  [3] -  [6] , because EEG could provide a more direct and objective clue to understand and estimate emotional states. The current EEG-based emotion recognition studies are sample-based emotion recognition, which treats the EEG samples in a continuous data collection equally and assigns the fixed emotional label (trial-based groundtruth), without taking into account emotion dynamics. As a consequence, the highly viable emotional states and associated EEG signals are not explored comprehensively, which hinders our understanding of emotion dynamics and decreases the accuracy of emotion recognition. To solve these problems, a novel EEG-based emotion recognition approach is proposed in this paper to unsupervisedly and adaptively determine key emotion fragments from an EEG trial which would be then used for emotion recognition.\n\nCurrently, many existing EEG-based emotion recognition models have been built via supervised machine learning, which can roughly fall into two categories. (1) Traditional machine learning-based methods. Based on the commonly used handcrafted EEG features, such as power spectral density (PSD)  [7] , differential entropy (DE)  [8] , differential asymmetry (DASM)  [9] , rational asymmetry (RASM)  [10] , and differential caudality (DCAU)  [11] , machine learning-based classifiers are built for emotion recognition  [8] ,  [12] ,  [13] . For example, Alsolamy et al.  [7]  extracted the PSD features at different frequency bands and used a support vector machine (SVM) as a classifier to predict emotions during listening to Quran. To enhance emotion recognition performance, Atkinson et al.  [14]  introduced to use the minimum-Redundancy-Maximum-Relevance (mRMR) method to select the most emotion-related handcrafted features for modeling. However, due to the unstable and weak performance of the traditional machine-learning methods, more and more EEGbased emotion recognition models are developed by using  (2)  deep learning-based methods. EEG-based emotion recognition models with deep learning are able to automatically extract discriminant features and identify emotional states from EEG signals in an end-to-end manner  [11] ,  [15] -  [17] . For example, Li et al.  [18]  proposed a bi-hemispheres domain adversarial neural network (BiDANN) based on the asymmetry between the left and right hemispheres of the brain, which made the data representation easy for emotion recognition by separately mapping the EEG data of the left and right hemispheres into discriminant feature spaces. Song et al.  [5]  proposed a dynamical graph convolutional neural network (DGCNN) with five types of handcrafted features as input, which dynamically learns the intrinsic relationships between different EEG channels represented by an adjacency matrix. Zhong et al.  [19]  proposed an EEG-based regularized graph neural network (RGNN) for performance enhancement. Li et al.  [20]  proposed to extract discriminative spatial-temporal EEG features via a region-to-global feature learning process. Tao et al.  [21]  developed an attention-based convolutional recurrent neural network (ACRNN) to adaptive estimate the importance of temporal and spatial information in EEG signals using the channel attention mechanism and self-attention mechanism. Compared with traditional machine learning-based methods, deep learning-based methods have achieved great success in EEG-based emotion recognition due to their powerful data representation ability.\n\nMost of the existing deep learning methods for EEG-based emotion recognition require a large number of training samples with emotional labels. However, it is unrealistic to collect a large number of EEG signals from different participants and manually annotate each sample based on emotional labels. To increase the sample size, previous studies usually divided each EEG trial into samples with a fixed length of 1s and annotated these samples with the fixed emotional label (trial-based groundtruth), which leads to the failure of neural networks to learn the true data distribution  [11] ,  [17] ,  [22] . In our previous work  [23]  [24], we proposed unsupervised learning-based methods to eliminate the model's reliance on labels. In particular, Liang et al.  [24]  developed a hybrid deep convolutional recurrent generative adversarial network named EEGFuseNet, which automatically characterized spatial and temporal dynamics from EEG signals in a self-learning paradigm and realized a possible way for unsupervised EEG feature learning in the affective brain-computer interface applications. However, the emotion dynamics in a trial were still underestimated. In a continuous emotion-evoking process, the elicited emotions are in a state of flux and the simultaneously collected EEG signals should not be assigned the fixed emotional label.\n\nWith the rapid development of deep learning technology, deep reinforcement learning (DRL) has been applied in many real-world scenarios, such as games  [25] , robotics  [26] , natural language processing  [27] , visual understanding  [28] , and neural architecture search  [29] . Unlike the conventional machine learning method, DRL learns through the reward signals of actions. In other words, a reasonable reward function is essential for a specific task based on DRL. Mnih et al.  [30]  successfully approximated value function with a deep CNN, the proposed TAS-Net for unsupervised emotion recognition, respectively. In the proposed TAS-Net, the key emotion fragments (purple and light-purple parts) are firstly selected via a DRL-based time-aware sampling method, and then only the selected key emotion fragments are used for emotion recognition, as opposed to using all the samples in the baseline method.\n\nand enabled their agent to beat a human expert in several Atari games. Later on, researchers start to apply DRL algorithms to time-series processing such as EEG analysis  [31] ,  [32] . For example, Zhang et al.  [31]  introduced DRL to aggregate local spatio-temporal and global temporal information from EEG signals for movement intention detection. Li et al.  [32]  developed a neural architecture search framework by using DRL to automatically design network architectures and learn discriminative EEG features for emotion recognition. However, these methods also ignore the emotion dynamics in a trial. Inspired by the video summarization method presented in  [33]  which detects key video frames to largely enhance the model efficiency for video processing, we introduce a novel time-aware sampling method to automatically detect key emotion fragments from the time-series EEG signals in order to fully consider the emotion dynamics in a trial. In comparison to  [33] , we make further improvements based on the characteristics of emotion occurrence, including agent design, reward improvement, and detection strategy. In other words, for each trial, only the EEG data located at the selected key emotion fragments are retained for further unsupervised emotion recognition, and other less informative data are discarded. Here, we take \"EEGFuseNet + Unsupervised Clustering\" as the baseline method, and highlight the flowchart difference between the baseline method and the proposed method (TAS-Net) in Fig.  1  • We conduct comprehensive experiments on the emotion recognition task with the SEED, DEAP, and MAHNOB-HCI datasets. The proposed method not only achieves better performance than other unsupervised learning methods on the three public datasets, but is also comparable to some supervised learning methods under the same validation strategy. Additionally, compared to the baseline method, the performance using the proposed method is significantly improved.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Overview",
      "text": "The framework of the proposed TAS-Net is shown in Fig.  2 , in which three parts are included: (3) unsupervised clustering: realize unsupervised emotion recognition using EEG signals. Here, the collected raw EEG signals in one trial are first segmented into a number of samples with a fixed length of 1s and input to the deep feature extractor of the baseline method to extract samplebased deep EEG features. Second, the extracted sample-based deep EEG features are formed into trial-based representation and input to the time-aware agent (the first part in the timeaware sampling) to measure the contribution of each sample to the emotion dynamics in one trial. The output is a sequence of probabilities indicating how informative each sample is in referring to emotion dynamics in a trial. The EEG samples with high probabilities are then selected as the informative emotion samples in a trial. Considering that human emotion has \"short-term continuity\", we further extract top-X continuous fragments centered on the selected informative emotion samples through the key fragment selection (the second part in the time-aware sampling). Finally, the extracted informative emotional fragments are used for emotion recognition using the unsupervised clustering method. More details about each module in the proposed TAS-Net are presented below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Deep Feature Extraction",
      "text": "An unsupervised deep feature extraction network (EEG-FuseNet) was proposed  [24]  to efficiently extract reliable features from high-dimensional raw EEG signals. Therefore, we employ the EEGFuseNet as our feature extractor to characterize sample-based deep EEG features. Specifically, EEGFuseNet was a hybrid deep encoder-decoder network architecture, which integrated a convolutional neural network (CNN), recurrent neural network (RNN), and generative adversarial network (GAN) in a smart and efficient manner. In the CNN-RNN based deep encoder-decoder network, the spatial and temporal dynamics in a time-series EEG data were efficiently characterized to represent the complex relationships of brain signals at different brain locations and at different time points. Based on the extracted spatial information by CNN, RNN was then adopted to measure the temporal information by exploring the feature relationships at adjacent time points. To further benefit high-quality feature characterization, a discriminator was incorporated to enhance the generator (CNN-RNN based)'s performance. EEGFuseNet is a self-learning network without any reliance on labeled samples, which is suitable to solve unsupervised EEG processing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Time-Aware Sampling",
      "text": "In a continuous emotion-evoking process, not all the timepoint are equally important to the final elicited emotions. In other words, not all of the simultaneously collected EEG data in a trial should be treated equally using the fixed emotional label. Only a small percentage of EEG samples in a trial may contain significant and relevant emotion information, whereas the remaining data may represent less emotion information that could be discarded in emotion recognition. In this section, we present the proposed time-aware sampling method that adaptively highlights the most informative emotion fragments from each trial, without any need for label information.\n\nFollowing the deep feature extraction, the time-aware agent in the proposed time-aware sampling approach interprets the key moment detection task as a Markov decision process. During the training phase, the time-aware agent interacts with an environment that provides rewards and takes actions a t on whether a sample should be selected into the key moment set S by maximizing the total expected reward R. In the inference stage, we retain the top-X continuous fragments centered on key emotion fragments for unsupervised clustering. The state, agent, action, and reward of the Markov decision process are described below.\n\nState. The state s is the extracted deep features in the deep feature extraction part. Agent. Considering that the human emotion has not only \"short-term continuity\" but also \"long-term similarity\", the time-aware agent is designed to consist of two components: a local time-based GCN and a global time-based BiGRU topped with a FC layer. Given a sequence of trial-based deep features {s t } T t=1 calculated by the feature extractor, the timeaware agent first divides the EEG samples in a trial into M successive segments (m ∈ {1, ..., M }), which can be explained as follows:\n\nwhere L denotes the time length of a successive segment, and T is the length of one trial. Then, the local timebased GCN takes these successive segments as input and estimates the local temporal features in parallel. After that, we concatenate these segment-based local temporal features in the time dimension and input them into the global timebased BiGRU to predict an action probability score p t for each sample. The action probability score is defined as:\n\nwhere σ represents the sigmoid function, W is the trainable parameter of the FC layer, and h t = {h GCN t , h BiGRU t } denotes the time-aware agent produces hidden state for t-th sample.\n\nAction. The designed time-aware agent further takes actions to select the key emotion fragments according to the obtained action probability score, and the action α t is given as:\n\nwhere a t ∈ {0, 1} indicates whether t-th sample in a trial should be selected in the key moment set or not. Therefore, the key moment set S can be defined as:\n\nwhere y i represents the indices of the selected samples. Reward. To optimize the key moment selection of the timeaware agent and guarantee all the informative and discriminant points in a trial are covered, we introduce two reward functions to optimize the training process:\n\nwhere R rep and R sim denote the representativeness reward and the similarity reward, respectively. R rep measures how well the selected key moment set S represents the original trial data, given as\n\nwhere Y = {y i | a yi = 1, i = 1, 2, ..., |Y|}. Through maximizing R rep , the model tends to preserve the key emotion fragments that could represent the overall temporal information of the input trial data. On the other hand, to minimize the information redundancy on the selected key emotion fragments, we also introduce the similarity reward (R sim ) to estimate the similarities among those selected key moment set S, given as:\n\nt∈Y t ∈Y,t =t sim (s t , s t ) ,  (7)  where sim (•, •) is the cosine similarity of two vectors.\n\n1) Training with Policy Gradient: Algorithm 1 shows the training details of the proposed time-aware agent. The goal of the time-aware agent is to learn a policy function π θ with parameters θ by maximizing the expected reward:\n\nwhere p θ (a 1:T ) represents the probability distribution over possible action sequences, and R is calculated by Eq. (  5 ).\n\nAccording to the definition given above, the obtained action sequences could consist of different selection choices on the key emotion fragments. There are 2 actions for each segment and the exponential 2 T is computationally infeasible for the time-aware agent training. Thus, we employ the policy gradient method to efficiently compute the continuous action space.\n\nInspired from the REINFORCE algorithm proposed in  [34] , we compute the derivative of the expected reward J (θ) w.r.t. the parameters θ as:\n\nwhere a t is the action taken by the time-aware agent for t-th sample, and h t is the hidden state from the time-aware agent. As Eq. (  9 ) involves the expectation over high dimensional action sequences, which is difficult to compute directly. Here, we approximate the gradient by running the agent for N episodes on the same trial data and then taking the average gradient as follows:\n\nwhere R n is the reward computed at the n-th episode. Although the gradient in Eq. (  10 ) gives a direction for updating θ, it may contain high variance and lead to poor network convergence. To tackle this issue, we normalize the reward by subtracting the reward using a constant baseline b. The gradient becomes:\n\nwhere b is a moving average of rewards experienced so far for computational efficiency enhancement.\n\nOn the other hand, to avoid too many key moments selected for increasing the reward, we also introduce a regularization 3: Compute importance scores {pt} T t=1 through policy π θ 4: Sort importance scores in descending order 5: Select the top-X continuous fragments according to Eq. (  14 ) 6: Save the top-X continuous fragments for unsupervised clustering 7: return {pt} T t=1 term (L sampling ) to limit the selection percentage for the key moment set. The regularization is defined as:\n\nwhere ϑ is a scalar representing the percentage of key emotion fragments to be selected. Finally, we optimize the timeaware agent through a joint loss of the reward functions and regularization:\n\nwhere β denotes a balance factor of the two terms.\n\n2) Key fragment selection in inference: Algorithm 2 shows the key fragment selection part in the proposed TAS-Net method. Given a deep feature sequence {s t } T t=1 from a testing set, we feed it into the trained time-aware sampling model π to predict sample-level probabilities as importance scores. We first sort the sample-wise importance scores in descending order and select top-X points as the key moments. Considering the nature of \"short-term continuity\" in human emotions, we further use the selected key moments to form top-X emotion offsets which are centered around the selected key moments. The emotion offsets are defined as:\n\nwhere s cx is the key moment of f x , and s cx ∈ S. • and • represent the ceiling and floor operators, respectively. o l x and o r\n\nx denote two absolute offsets, which are formulated as:\n\nwhere õl\n\nx and õr x are relative offset coefficients, which are equal to the importance score p cx at c x -th key moment. l max ∈ Z ≥ 0 is the maximum absolute offset to the left, and r max ∈ Z ≥ 0 is the maximum absolute offset to the right. Finally, we obtain top-X key emotion fragments for unsupervised emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Unsupervised Clustering Model",
      "text": "For a fair comparison with the baseline method, we also employ the hypergraph structure  [35]  to perform unsupervised emotion recognition. Hypergraph is widely regarded as an effective method to describe complex hidden data structures, so it has natural advantages in decoding EEG signals and completing emotion classification. In a hypergraph, one hyperedge could connect more than two nodes and reveal more complex hidden structures than a pairwise connection in a simple graph (one edge can only connect two nodes). In the implementation, we formulate a hypergraph as G = (V, E, w), where V refers to vertices indicating the trial-based EEG representation obtained in the proposed time-aware sampling method and E is the constructed hyperedge information based on the V. The unsupervised emotion recognition is realized by calculating the hypergraph Laplacian of the constructed hypergraph (G) and solving it in an optimal eigenspace.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iii. Experimental Results",
      "text": "We conduct a comprehensive experiment to evaluate the effectiveness of the proposed TAS-Net in dealing with unsupervised emotion recognition using EEG signals. In this section, we would first describe the datasets used in our experiments, followed by an introduction to experiment settings and evaluation metrics. Then, we would report the experimental results. The promising results demonstrate the effectiveness of the proposed method.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We perform experiments on three publicly available EEG datasets, including SEED  [11] , DEAP  [36] , and MAHNOB-HCI  [37] . All these datasets have been widely used as benchmarks for evaluating EEG-based emotion recognition algorithms.\n\n• SEED. The SEED dataset contains EEG signals of  15  subjects, which were recorded at a 62-channel setting according to the international 10-20 system  [38]  when each subject watched 15 film clips. These film clips can evoke specific target emotions (positive, neutral, or negative), and each film clip is about 4 minutes long. • DEAP. The DEAP dataset contains EEG signals of 32 subjects, which were recorded at a 32-channel setting according to the international 10-20 system when each subject watched 40 music videos. These music videos were all 60 seconds in duration, and each was given corresponding subjective feedback on different emotional dimensions (valence, arousal, dominance, and liking).\n\nThe participants were asked to rate different emotion dimensions on a scale of 1 to 9 after watching each music video. To cross-compare with the baseline method and other methods, we use a fixed threshold of 5 for each emotion dimension to discretize the subjective feedback into two classes (low and high).\n\n• MAHNOB-HCI. The MAHNOB-HCI dataset contains EEG signals of 30 subjects, which were recorded at a 32-channel setting according to the international 10-20 system when each subject watched 20 video clips. These film clips were selected to evoke emotions and the subjective feedback was given using a score in the range of 1 to 9. In the model evaluation, a fixed threshold of 5 is used to discretize the subjective feedback into binaries for each emotion dimension (valence, arousal, dominance, and predictability).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Experiment Settings And Evaluation Metrics",
      "text": "The proposed TAS-Net is implemented in the PyTorch platform. Here, we set the L in Eq. (  1 ) to 16, the ϑ in Eq. (  12 ) to 0.5, the β in Eq. (  13 ) to 0.01, and the number of episodes N to 5. The dimension of the edge feature in the GCN is 32, and the dimension of the hidden state in the GRU cell is 256. The above hyper-parameters are set according to the relevant    4  (b). As a result, our method can be regarded as significantly better than the baseline method on the DEAP dataset. Also, compared with the other existing unsupervised EEG decoding method  [23] , our method also achieves significant performance gains on all emotion dimensions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Evaluation On Mahnob-Hci Dataset",
      "text": "Similarly, a binary classification task is conducted on the MAHNOB-HCI dataset to evaluate the emotion recognition performance of each emotion dimension. As shown in Table  III , our proposed method achieves a comparable performance compared to the other supervised methods, where the recognition accuracies (P acc ) of valence, arousal, dominance, and predictability are 62.51%, 64.69%, 69.84% and 75.11% and the corresponding F1-Score (P f ) values are 74.64%, 60.60%, 79.34%, and 84.10%. Compared with the baseline approach, our method improved the average accuracy of the above four affective dimensions by 3.08%, 4.24%, 4.11%, and 0.64%, respectively. The corresponding statistical difference analysis is reported in Fig.  4 (c) . It shows, compared with the baseline method, our method achieves significant performance improvement on two emotional dimensions (valence and dominance), while achieving similar performance on the other two emotional dimensions (arousal and predictability).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Iv. Discussions",
      "text": "To fully investigate the effectiveness and efficiency of the proposed TAS-Net, we conduct multiple sets of ablation experiments, including comparing different components, evaluating the defined rewards, evaluating the existing clustering methods with or without TAS-Net, and analyzing the parameter effect. Further, to quantitatively evaluate the key emotion fragment detection performance using TAS-Net, we introduce temporal intersection over union (tIoU)  [39]  to calculate the recall rate of samplings. In the performance comparison with different clustering methods, we add normalized mutual information (NMI) as another performance metric to check the corresponding clustering quality. Due to space limitations, all experimental analyses in this section are based on the SEED dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Ablation Study",
      "text": "We conduct ablation experiments to evaluate the contribution of each component in the proposed TAS-Net. The compared methods are: (i) M1: the agent is a FC layer, which produces the action probability. (ii) M2: the agent is a local time-based GCN topped with a FC layer; (iii) M3: the agent is a global time-based BiGRU topped with a FC layer  [33] . Fig.  5  improves the average accuracy P acc by 26.05%, 5.24%, and respectively. Therefore, we believe that it is effective to model the time-aware agent by considering that human emotion has not only \"short-term continuity\" but also \"longterm similarity\".",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Effectiveness Of Rewards",
      "text": "To explore the optimization effect of reward functions the proposed TAS-Net, we set different reward functions for comparative experiments. Here, R rep is to measure how well these selected key emotion fragments can represent the whole trial's emotional information. Considering the signals evoked by the same emotion should be similar, R sim is calculated to cater to the \"long-term similarity\" of emotions. R div is another commonly used reward function to measure the difference between the selected key emotion fragments in the feature space to evaluate the degree of diversity of the generated summary. Fig.  5 (b) shows the obtained unsupervised emotion recognition accuracies using different reward functions. R1, R2, R3, R4 and R5 represent R rep , R sim , R div , R rep + R div and R rep + R sim , respectively. It is found that the reward function of R sim achieves better performance than R div , which indicates that the similarity reward function is more suitable for the key emotion fragment detection task. Compared with the reward function R rep , when the reward function is composed of R rep and R div , the recognition performance decreases, while when the reward function is composed of R rep and R sim , the recognition performance increases, which proves that the same emotion has similarity in different time periods.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Performance Comparison Using Different Clustering Methods",
      "text": "The purpose of this study is to design a sampling model that can efficiently localize short-term continuous fragments with correlated emotions in EEG time series, so as to achieve better unsupervised emotion recognition performance. Theoretically, the sampling set can effectively improve the emotion recognition performance of different clustering methods. We employ six decoding models for this comparative experiment, including a simple graph based method, principal component analysis (PCA) and K-means clustering method (PCA+Kmeans), K-nearest neighbors (KNN) algorithm, robust continuous clustering method (RCC), directed graph based agglomerative algorithm (AGDL), and hypergraph algorithm. The simple graph completes unsupervised clustering by measuring pairwise relationships. K-means is one of the outstanding representatives of unsupervised clustering, and KNN is one of the simplest classification algorithms in supervised clustering. RCC is a fast, simple, and efficient high-dimensional clustering algorithm that achieves decoding of highly mixed clusters by optimizing a clear global objective. AGDL is a graphbased agglomerative algorithm, which quantifies the stability of clustering results by computing the product of average indegree and average outdegree. Table  4  reports the unsupervised recognition accuracies of different clustering methods without or with the time-aware sampling method. In the case of using the time-aware sampling method, significant performance improvement can be observed in all the above clustering methods, which shows the effectiveness of the proposed time sampling method in unsupervised based emotion recognition using EEG signals.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Parameter Analysis",
      "text": "We further quantitatively evaluate the effectiveness of the proposed TAS-Net under different parameters. There are two important hyper-parameters in our method: the number K of selected fragments and the maximum absolute offset to the left and right parameter l max &r max . The setting of K can affect the recall rate of sampling and the accuracy of emotion recognition, while l max &r max determines the maximum length of a selected fragment. Following  [39] , we set K to 10 and set l max &r max to 5, 6, 7, 8, 9, 10. As shown in Fig.  5 (c), our method achieves the best performance when l max &r max is 8, which means that the max length of a fragment with coherent stronger emotion is 16. According to the experimental results, we clearly find that our method is sensitive to this parameter when l max &r max is less than 8, while the results obtained by our method are relatively stable when l max &r max is greater than 8.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Performance On Emotion Localization",
      "text": "Recently, Zhang et al.  [39]  proposed a supervised learning based hierarchical self-attention network on SEED dataset, which incorporates emotion localization task for improving emotion recognition performance. Similarly, we also extend our proposed TAS-Net on emotion localization application under an unsupervised learning manner. In the emotion localization task, EEG signals corresponding to each video stimulus were segmented, and then twenty people were asked to score each fragment according to the corresponding emotion to obtain the ground truth labels. In addition, the recall is used as the evaluation metric for this task, and the threshold of tIoU (the overlap rate in the timeline between the ground truth fragments and the top-X selected fragments f x ) is set to 0.5. Note here that, for cross-comparison with  [39] , both subject-independent LOOCV and subject-dependent LOOCV are used for emotion localization in our method. Tables 5 and 6 report the recall rates of emotion localization obtained by our method using subject-independent LOOCV and subjectdependent LOOCV, respectively. Compared with the other two emotion localization methods that employ subject-dependent LOOCV, our method achieves the highest performance. When subject-independent LOOCV is used (which is more difficult than subject-dependent LOOCV due to the individual differences), our model's subject-independent LOOCV results are comparable with AsI-AEIR  [53] 's subject-dependent LOOCV results. In other words, our model is more effective and efficient in emotion localization under both subject-dependent LOOCV and subject-independent LOOCV. For a more intuitive comparison, we also visualize the emotion localization results when different methods are used. As shown in Fig.  6 , the ground truth labels with the localization results and tIoU values are reported, which shows the possibility to detect key emotion fragments even when label information is missing. Experiments on three challenging public datasets show that our TAS-Net achieves excellent performance. The performance in detecting key emotion fragments from the continuously collected EEG signals has been verified, and the superiority of emotion recognition has been demonstrated. Since the label information is not required by the proposed new method, TAS-Net is a fully unsupervised method that could be easily extended to other EEG-related applications and be utilized to automatically extract essential information from time-series EEG data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conflicts Of Interest",
      "text": "",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The ﬂowcharts of (a) the baseline method and (b)",
      "page": 2
    },
    {
      "caption": "Figure 1: TAS-Net attempts to detect key emotion fragments in an",
      "page": 3
    },
    {
      "caption": "Figure 2: The framework of the proposed TAS-Net, which includes deep feature extraction, time-aware sampling, and unsupervised",
      "page": 4
    },
    {
      "caption": "Figure 3: Two validation strategies for EEG-based emotion",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the details of data partitioning for",
      "page": 7
    },
    {
      "caption": "Figure 4: (a), which",
      "page": 7
    },
    {
      "caption": "Figure 4: (a) prove that the proposed method",
      "page": 7
    },
    {
      "caption": "Figure 4: The obtained p-values of the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.",
      "page": 8
    },
    {
      "caption": "Figure 4: (b). As a result, our method can be regarded as signiﬁcantly",
      "page": 8
    },
    {
      "caption": "Figure 4: (c). It shows, compared with the",
      "page": 8
    },
    {
      "caption": "Figure 5: (a) shows the obtained accuracies when different",
      "page": 8
    },
    {
      "caption": "Figure 5: Emotion recognition performance (%) under different methods and parameters comparison.",
      "page": 9
    },
    {
      "caption": "Figure 5: (b) shows the obtained unsupervised emotion",
      "page": 9
    },
    {
      "caption": "Figure 6: Visualization of selected key fragments in different videos. Ours1 and Ours2 represent the results obtained by our",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "Abstract—Recognizing human emotions from complex, multi-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "variate, and non-stationary electroencephalography (EEG)\ntime"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "series is essential\nin affective brain-computer interface. However,"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "because\ncontinuous\nlabeling\nof\never-changing\nemotional\nstates"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "is\nnot\nfeasible\nin\npractice,\nexisting methods\ncan\nonly\nassign"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "a ﬁxed label\nto\nall EEG timepoints\nin a\ncontinuous\nemotion-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "evoking\ntrial, which\noverlooks\nthe\nhighly\ndynamic\nemotional"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "states\nand\nhighly\nnon-stationary EEG signals. To\nsolve\nthe"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "problems\nof\nhigh\nreliance\non\nﬁxed\nlabels\nand\nignorance\nof"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "time-changing\ninformation,\nin this paper we propose\na\ntime-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "aware\nsampling\nnetwork\n(TAS-Net)\nusing\ndeep\nreinforcement"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "learning (DRL)\nfor unsupervised emotion recognition, which is"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "able\nto detect key emotion fragments and disregard irrelevant"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "and misleading parts. Speciﬁcally, we\nformulate\nthe process of"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "mining key emotion fragments from EEG time series as a Markov"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "decision\nprocess\nand\ntrain\na\ntime-aware\nagent\nthrough DRL"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "without label\ninformation. First, the time-aware agent takes deep"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "features from a feature extractor as input and generates sample-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "wise importance scores reﬂecting the emotion-related information"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "each\nsample\ncontains. Then,\nbased\non\nthe\nobtained\nsample-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "wise importance scores, our method preserves top-X continuous"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "EEG fragments with relevant\nemotion and discards\nthe\nrest."
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "Finally, we\ntreat\nthese\ncontinuous\nfragments\nas\nkey\nemotion"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "fragments and feed them into a hypergraph decoding model\nfor"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "unsupervised clustering. Extensive experiments are conducted on"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "three public datasets\n(SEED, DEAP,\nand MAHNOB-HCI)\nfor"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "emotion recognition using leave-one-subject-out cross-validation,"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "and\nthe\nresults\ndemonstrate\nthe\nsuperiority\nof\nthe\nproposed"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "method against previous unsupervised emotion recognition meth-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "ods. The proposed TAS-Net has great potential\nin achieving a"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "more practical and accurate affective brain-computer interface"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "in a dynamic and label-free\ncircumstance. The\nsource\ncode\nis"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "made available at https://github.com/inﬁnite-tao/TAS-Net."
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "Index\nTerms—Electroencephalography;\nAffective\nBrain-"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "Computer Interface; Emotion Recognition; Deep Reinforcement"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "Learning; Unsupervised Learning."
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "I.\nINTRODUCTION"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "is\na\ncomplex\nand\ndynamic\nprocess"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "H UMAN emotion"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "levels of processing and integration"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "[1], [2]. In emotion-related studies, how to accurately describe"
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": ""
        },
        {
          "¶lzhang@szu.edu.cn, (cid:107)huanggan@szu.edu.cn, ‡‡janezliang@szu.edu.cn, ††zhiguozhang@hit.edu.cn": "* Corresponding author: Zhen Liang and Zhiguo Zhang."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Feature Extraction"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Feature Extraction"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Time-Aware Sampling"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Unsupervised Clustering"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Unsupervised Clustering"
        },
        {
          "2": ""
        },
        {
          "2": "(a)  Baseline"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "(b) TAS-Net"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Fig.\n1: The ﬂowcharts\nof\n(a)\nthe\nbaseline method\nand\n(b)"
        },
        {
          "2": ""
        },
        {
          "2": "the proposed TAS-Net\nfor unsupervised emotion recognition,"
        },
        {
          "2": ""
        },
        {
          "2": "respectively.\nIn the proposed TAS-Net,\nthe key emotion frag-"
        },
        {
          "2": ""
        },
        {
          "2": "ments\n(purple and light-purple parts) are ﬁrstly selected via"
        },
        {
          "2": ""
        },
        {
          "2": "a DRL-based\ntime-aware\nsampling method,\nand\nthen\nonly"
        },
        {
          "2": ""
        },
        {
          "2": "the\nselected\nkey\nemotion\nfragments\nare\nused\nfor\nemotion"
        },
        {
          "2": ""
        },
        {
          "2": "recognition, as opposed to using all the samples in the baseline"
        },
        {
          "2": ""
        },
        {
          "2": "method."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "and enabled their agent to beat a human expert in several Atari"
        },
        {
          "2": "games. Later on,\nresearchers\nstart\nto apply DRL algorithms"
        },
        {
          "2": "to\ntime-series\nprocessing\nsuch\nas EEG analysis\n[31],\n[32]."
        },
        {
          "2": "For example, Zhang et al.\n[31]\nintroduced DRL to aggregate"
        },
        {
          "2": "local\nspatio-temporal\nand global\ntemporal\ninformation from"
        },
        {
          "2": "et\nal.\nEEG signals\nfor movement\nintention\ndetection.\nLi"
        },
        {
          "2": "[32]\ndeveloped\na\nneural\narchitecture\nsearch\nframework\nby"
        },
        {
          "2": "using DRL to automatically design network architectures and"
        },
        {
          "2": "learn\ndiscriminative EEG features\nfor\nemotion\nrecognition."
        },
        {
          "2": "However,\nthese methods also ignore the emotion dynamics in"
        },
        {
          "2": "a trial. Inspired by the video summarization method presented"
        },
        {
          "2": "in [33] which detects key video frames\nto largely enhance"
        },
        {
          "2": "the model\nefﬁciency\nfor\nvideo\nprocessing, we\nintroduce\na"
        },
        {
          "2": "novel time-aware sampling method to automatically detect key"
        },
        {
          "2": "emotion fragments from the time-series EEG signals in order"
        },
        {
          "2": "to fully consider\nthe\nemotion dynamics\nin a\ntrial.\nIn com-"
        },
        {
          "2": "parison to [33], we make further\nimprovements based on the"
        },
        {
          "2": "characteristics of emotion occurrence,\nincluding agent design,"
        },
        {
          "2": "reward improvement, and detection strategy.\nIn other words,"
        },
        {
          "2": "for each trial, only the EEG data located at\nthe selected key"
        },
        {
          "2": "emotion fragments are retained for further unsupervised emo-"
        },
        {
          "2": "tion recognition, and other less informative data are discarded."
        },
        {
          "2": "Here, we take ”EEGFuseNet + Unsupervised Clustering” as"
        },
        {
          "2": "the baseline method,\nand highlight\nthe ﬂowchart difference"
        },
        {
          "2": "between the baseline method and the proposed method (TAS-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "we\nemploy\nthe\nEEGFuseNet\nas\nour\nfeature\nextractor\nto"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "characterize\nsample-based\ndeep EEG features. Speciﬁcally,"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "EEGFuseNet was\na\nhybrid\ndeep\nencoder-decoder\nnetwork"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "architecture, which integrated a convolutional neural network"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "(CNN),\nrecurrent neural network (RNN), and generative ad-"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "versarial\nnetwork\n(GAN)\nin\na\nsmart\nand\nefﬁcient manner."
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "In the CNN-RNN based deep encoder-decoder network,\nthe"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "spatial and temporal dynamics in a time-series EEG data were"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "efﬁciently characterized to represent the complex relationships"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "of brain signals at different brain locations and at different time"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "points. Based on the extracted spatial\ninformation by CNN,"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "RNN was then adopted to measure the temporal\ninformation"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "by exploring the feature relationships at adjacent\ntime points."
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "To further beneﬁt high-quality feature characterization, a dis-"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "criminator was incorporated to enhance the generator\n(CNN-"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "RNN based)’s\nperformance. EEGFuseNet\nis\na\nself-learning"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "network without\nany reliance on labeled samples, which is"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "suitable to solve unsupervised EEG processing."
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "C. Time-Aware Sampling"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": ""
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "In a continuous emotion-evoking process, not all\nthe time-"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "point are equally important\nto the ﬁnal elicited emotions.\nIn"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "other words, not all of the simultaneously collected EEG data"
        },
        {
          "features\nfrom high-dimensional\nraw EEG signals. Therefore,": "in a trial should be treated equally using the ﬁxed emotional"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "emotion samples\nin a trial. Considering that human emotion"
        },
        {
          "3": "has ”short-term continuity”, we further extract\ntop-X contin-"
        },
        {
          "3": "uous fragments centered on the selected informative emotion"
        },
        {
          "3": "samples\nthrough the key fragment\nselection (the second part"
        },
        {
          "3": "in the time-aware sampling). Finally,\nthe extracted informative"
        },
        {
          "3": "emotional\nfragments are used for emotion recognition using"
        },
        {
          "3": "the unsupervised clustering method. More details about each"
        },
        {
          "3": "module in the proposed TAS-Net are presented below."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "B. Deep Feature Extraction"
        },
        {
          "3": ""
        },
        {
          "3": "An\nunsupervised\ndeep\nfeature\nextraction\nnetwork\n(EEG-"
        },
        {
          "3": ""
        },
        {
          "3": "FuseNet) was\nproposed\n[24]\nto\nefﬁciently\nextract\nreliable"
        },
        {
          "3": ""
        },
        {
          "3": "features\nfrom high-dimensional\nraw EEG signals. Therefore,"
        },
        {
          "3": ""
        },
        {
          "3": "we\nemploy\nthe\nEEGFuseNet\nas\nour\nfeature\nextractor\nto"
        },
        {
          "3": ""
        },
        {
          "3": "characterize\nsample-based\ndeep EEG features. Speciﬁcally,"
        },
        {
          "3": ""
        },
        {
          "3": "EEGFuseNet was\na\nhybrid\ndeep\nencoder-decoder\nnetwork"
        },
        {
          "3": ""
        },
        {
          "3": "architecture, which integrated a convolutional neural network"
        },
        {
          "3": ""
        },
        {
          "3": "(CNN),\nrecurrent neural network (RNN), and generative ad-"
        },
        {
          "3": ""
        },
        {
          "3": "versarial\nnetwork\n(GAN)\nin\na\nsmart\nand\nefﬁcient manner."
        },
        {
          "3": ""
        },
        {
          "3": "In the CNN-RNN based deep encoder-decoder network,\nthe"
        },
        {
          "3": ""
        },
        {
          "3": "spatial and temporal dynamics in a time-series EEG data were"
        },
        {
          "3": ""
        },
        {
          "3": "efﬁciently characterized to represent the complex relationships"
        },
        {
          "3": ""
        },
        {
          "3": "of brain signals at different brain locations and at different time"
        },
        {
          "3": ""
        },
        {
          "3": "points. Based on the extracted spatial\ninformation by CNN,"
        },
        {
          "3": ""
        },
        {
          "3": "RNN was then adopted to measure the temporal\ninformation"
        },
        {
          "3": ""
        },
        {
          "3": "by exploring the feature relationships at adjacent\ntime points."
        },
        {
          "3": ""
        },
        {
          "3": "To further beneﬁt high-quality feature characterization, a dis-"
        },
        {
          "3": ""
        },
        {
          "3": "criminator was incorporated to enhance the generator\n(CNN-"
        },
        {
          "3": ""
        },
        {
          "3": "RNN based)’s\nperformance. EEGFuseNet\nis\na\nself-learning"
        },
        {
          "3": ""
        },
        {
          "3": "network without\nany reliance on labeled samples, which is"
        },
        {
          "3": ""
        },
        {
          "3": "suitable to solve unsupervised EEG processing."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "C. Time-Aware Sampling"
        },
        {
          "3": ""
        },
        {
          "3": "In a continuous emotion-evoking process, not all\nthe time-"
        },
        {
          "3": "point are equally important\nto the ﬁnal elicited emotions.\nIn"
        },
        {
          "3": "other words, not all of the simultaneously collected EEG data"
        },
        {
          "3": "in a trial should be treated equally using the ﬁxed emotional"
        },
        {
          "3": "label. Only a small percentage of EEG samples in a trial may"
        },
        {
          "3": ""
        },
        {
          "3": "contain signiﬁcant and relevant emotion information, whereas"
        },
        {
          "3": ""
        },
        {
          "3": "the\nremaining data may represent\nless\nemotion information"
        },
        {
          "3": "that could be discarded in emotion recognition. In this section,"
        },
        {
          "3": "we\npresent\nthe\nproposed\ntime-aware\nsampling method\nthat"
        },
        {
          "3": "adaptively highlights the most\ninformative emotion fragments"
        },
        {
          "3": "from each trial, without any need for\nlabel\ninformation."
        },
        {
          "3": "Following the deep feature extraction,\nthe time-aware agent"
        },
        {
          "3": "in the proposed time-aware sampling approach interprets\nthe"
        },
        {
          "3": "key moment\ndetection\ntask\nas\na Markov\ndecision\nprocess."
        },
        {
          "3": "During the training phase,\nthe time-aware agent\ninteracts with"
        },
        {
          "3": "an environment\nthat provides rewards and takes actions at on"
        },
        {
          "3": "whether a sample should be selected into the key moment set"
        },
        {
          "3": "S by maximizing the total expected reward R. In the inference"
        },
        {
          "3": "stage, we retain the top-X continuous\nfragments centered on"
        },
        {
          "3": "key emotion fragments for unsupervised clustering. The state,"
        },
        {
          "3": "agent, action, and reward of\nthe Markov decision process are"
        },
        {
          "3": "described below."
        },
        {
          "3": "State. The state s is the extracted deep features in the deep"
        },
        {
          "3": "feature extraction part."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hypergraph Algorithm": "GCN"
        },
        {
          "Hypergraph Algorithm": "Time-aware sampling: time-aware agent\nUnsupervised clustering"
        },
        {
          "Hypergraph Algorithm": "Fig. 2: The framework of the proposed TAS-Net, which includes deep feature extraction, time-aware sampling, and unsupervised"
        },
        {
          "Hypergraph Algorithm": "concatenation; BiGRU: bidirectional gated recurrent unit; FC Layer:\nfully\nclustering. GCN: graph convolution network; C(cid:13):"
        },
        {
          "Hypergraph Algorithm": "connected layer; pt: action probability score; fk: the detected key emotion fragments. Here, the detected key emotion fragments"
        },
        {
          "Hypergraph Algorithm": "are composed of\nthe detected key moments\n(dark purple) and the emotion offsets\n(light purple) calculated according to the"
        },
        {
          "Hypergraph Algorithm": "nature of ”short-term continuity” in human emotions. The ﬁnal detected key emotion fragments are composed of both dark"
        },
        {
          "Hypergraph Algorithm": "and light purple parts."
        },
        {
          "Hypergraph Algorithm": "Agent. Considering that\nthe human emotion has not only\nAction. The designed time-aware agent further takes actions"
        },
        {
          "Hypergraph Algorithm": "”short-term continuity”\nbut\nalso\n”long-term similarity”,\nthe\nto select\nthe key emotion fragments according to the obtained"
        },
        {
          "Hypergraph Algorithm": "time-aware agent\nis designed to consist of\ntwo components:\nis given as:\naction probability score, and the action αt"
        },
        {
          "Hypergraph Algorithm": "a\nlocal\ntime-based GCN and\na\nglobal\ntime-based BiGRU"
        },
        {
          "Hypergraph Algorithm": "(3)\nat ∼ Bernoulli (pt) ,"
        },
        {
          "Hypergraph Algorithm": "topped with a FC layer. Given a sequence of\ntrial-based deep"
        },
        {
          "Hypergraph Algorithm": "the time-\nfeatures {st}T\nt=1 calculated by the feature extractor,"
        },
        {
          "Hypergraph Algorithm": "t-th sample\nin a\ntrial\nwhere at ∈ {0, 1} indicates whether"
        },
        {
          "Hypergraph Algorithm": "aware\nagent ﬁrst\ndivides\nthe EEG samples\nin\na\ntrial\ninto"
        },
        {
          "Hypergraph Algorithm": "should be selected in the key moment\nset or not. Therefore,"
        },
        {
          "Hypergraph Algorithm": "M successive\nsegments\n(m ∈\n{1, ..., M }), which\ncan\nbe"
        },
        {
          "Hypergraph Algorithm": "the key moment set S can be deﬁned as:"
        },
        {
          "Hypergraph Algorithm": "explained as follows:"
        },
        {
          "Hypergraph Algorithm": "(4)\nS = {syi\n| ayi = 1, i = 1, 2, ...},"
        },
        {
          "Hypergraph Algorithm": ": i = (m − 1) · L + 1, ..., min{T, m · L}},\n(1)\nCm = {si"
        },
        {
          "Hypergraph Algorithm": "represents the indices of\nthe selected samples.\nwhere yi"
        },
        {
          "Hypergraph Algorithm": "where L denotes\nthe\ntime\nlength\nof\na\nsuccessive\nsegment,"
        },
        {
          "Hypergraph Algorithm": "Reward. To optimize the key moment selection of the time-"
        },
        {
          "Hypergraph Algorithm": "T\nand\nis\nthe\nlength\nof\none\ntrial.\nThen,\nthe\nlocal\ntime-"
        },
        {
          "Hypergraph Algorithm": "aware agent and guarantee all the informative and discriminant"
        },
        {
          "Hypergraph Algorithm": "based GCN takes\nthese\nsuccessive\nsegments\nas\ninput\nand"
        },
        {
          "Hypergraph Algorithm": "points in a trial are covered, we introduce two reward functions"
        },
        {
          "Hypergraph Algorithm": "estimates\nthe\nlocal\ntemporal\nfeatures\nin parallel. After\nthat,"
        },
        {
          "Hypergraph Algorithm": "to optimize the training process:"
        },
        {
          "Hypergraph Algorithm": "we\nconcatenate these\nsegment-based local\ntemporal\nfeatures"
        },
        {
          "Hypergraph Algorithm": "in the time dimension and input\nthem into the global\ntime-"
        },
        {
          "Hypergraph Algorithm": "(5)\nR = Rrep + Rsim,"
        },
        {
          "Hypergraph Algorithm": "based BiGRU to predict\nfor\nan action probability score pt"
        },
        {
          "Hypergraph Algorithm": "each sample. The action probability score is deﬁned as:\nreward\nwhere Rrep\nand Rsim denote the representativeness"
        },
        {
          "Hypergraph Algorithm": "and the\nsimilarity reward,\nrespectively. Rrep measures how"
        },
        {
          "Hypergraph Algorithm": "(2)\npt = σ (W ht) ,"
        },
        {
          "Hypergraph Algorithm": "well\nthe\nselected key moment\nset S represents\nthe original"
        },
        {
          "Hypergraph Algorithm": "where σ represents\nthe sigmoid function, W is\nthe trainable\ntrial data, given as"
        },
        {
          "Hypergraph Algorithm": "= {hGCN\n, hBiGRU\n}\nparameter\nof\nthe\nFC layer,\nand\nht"
        },
        {
          "Hypergraph Algorithm": "t\nt\n(cid:32)\n(cid:33)"
        },
        {
          "Hypergraph Algorithm": "denotes\nthe time-aware agent produces hidden state for\nt-th"
        },
        {
          "Hypergraph Algorithm": "1 T\n−\n,\nmin\n(6)\nRrep = exp"
        },
        {
          "Hypergraph Algorithm": "T(cid:88) t\n(cid:107)st − st(cid:48) (cid:107)2"
        },
        {
          "Hypergraph Algorithm": "sample.\nt(cid:48) ∈Y"
        },
        {
          "Hypergraph Algorithm": "=1"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "Algorithm 1 Time-aware agent\ntraining"
        },
        {
          "5": ""
        },
        {
          "5": "Input: Deep feature sequence {st}T\nt=1 from training set"
        },
        {
          "5": "Output: Parameters θ of\nthe time-aware agent"
        },
        {
          "5": ""
        },
        {
          "5": "1: Initialize θ with small\nrandom values"
        },
        {
          "5": ""
        },
        {
          "5": "for i ← 1, 2, ..., ξ do\n2:"
        },
        {
          "5": "3:\nInput a deep feature sequence {st}T"
        },
        {
          "5": "t=1 from training set"
        },
        {
          "5": "4:\nCompute action probabilities {pt}T"
        },
        {
          "5": "t=1"
        },
        {
          "5": "for episode ← 1, 2, ..., N do\n5:"
        },
        {
          "5": ""
        },
        {
          "5": "6:\nSample {at}T\nt=1 from policy πθ"
        },
        {
          "5": ""
        },
        {
          "5": "7:\nCompute reward Rn"
        },
        {
          "5": "end for\n8:"
        },
        {
          "5": ""
        },
        {
          "5": "9:\nUpdate θ with gradient ∇θJ (θ) and ∇θLsampling"
        },
        {
          "5": ""
        },
        {
          "5": "10:\nUpdate the moving average of\nrewards b for\nthe corresponding"
        },
        {
          "5": ""
        },
        {
          "5": "deep feature sequence."
        },
        {
          "5": "11: end for"
        },
        {
          "5": ""
        },
        {
          "5": "12: return θ"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Algorithm 2 Time-aware agent\ntesting"
        },
        {
          "5": ""
        },
        {
          "5": "Input: Deep feature sequence {st}T\nt=1 from testing set"
        },
        {
          "5": ""
        },
        {
          "5": "Output: Action probabilities {pt}T\nt=1"
        },
        {
          "5": "1: Initialize θ from the trained time-aware agent"
        },
        {
          "5": "2:\nInput a deep feature sequence {st}T\nt=1"
        },
        {
          "5": "3: Compute importance scores {pt}T\nt=1 through policy πθ"
        },
        {
          "5": ""
        },
        {
          "5": "4: Sort\nimportance scores in descending order"
        },
        {
          "5": ""
        },
        {
          "5": "5: Select\nthe top-X continuous fragments according to Eq.\n(14)"
        },
        {
          "5": "6: Save the top-X continuous fragments for unsupervised clustering"
        },
        {
          "5": "7: return {pt}T\nt=1"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "to limit\nthe selection percentage for\nthe key\nterm (Lsampling)"
        },
        {
          "5": "moment set. The regularization is deﬁned as:"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "1 T\nT(cid:88) t\n(12)\nLsampling =||\npt − ϑ ||2,"
        },
        {
          "5": ""
        },
        {
          "5": "=1"
        },
        {
          "5": ""
        },
        {
          "5": "where ϑ is a scalar representing the percentage of key emotion"
        },
        {
          "5": "fragments\nto\nbe\nselected.\nFinally, we\noptimize\nthe\ntime-"
        },
        {
          "5": ""
        },
        {
          "5": "aware agent\nthrough a joint\nloss of\nthe reward functions and"
        },
        {
          "5": ""
        },
        {
          "5": "regularization:"
        },
        {
          "5": ""
        },
        {
          "5": "(13)\nLtotal = βLsampling − R,"
        },
        {
          "5": ""
        },
        {
          "5": "where β denotes a balance factor of\nthe two terms."
        },
        {
          "5": ""
        },
        {
          "5": "2) Key fragment selection in inference: Algorithm 2 shows"
        },
        {
          "5": ""
        },
        {
          "5": "the\nkey\nfragment\nselection\npart\nin\nthe\nproposed TAS-Net"
        },
        {
          "5": ""
        },
        {
          "5": "method. Given a deep feature sequence {st}T\nt=1 from a testing"
        },
        {
          "5": ""
        },
        {
          "5": "set, we\nfeed it\ninto the\ntrained time-aware\nsampling model"
        },
        {
          "5": "π to predict\nsample-level probabilities as\nimportance scores."
        },
        {
          "5": ""
        },
        {
          "5": "We ﬁrst sort\nthe sample-wise importance scores in descending"
        },
        {
          "5": ""
        },
        {
          "5": "order and select top-X points as the key moments. Considering"
        },
        {
          "5": ""
        },
        {
          "5": "the nature of ”short-term continuity” in human emotions, we"
        },
        {
          "5": ""
        },
        {
          "5": "further use the selected key moments to form top-X emotion"
        },
        {
          "5": ""
        },
        {
          "5": "offsets which are centered around the selected key moments."
        },
        {
          "5": ""
        },
        {
          "5": "The emotion offsets are deﬁned as:"
        },
        {
          "5": ""
        },
        {
          "5": "fx = {s(cid:100)ol\nx(cid:99)}, x ∈ {1, 2, ..., X},\nx(cid:101), s(cid:100)ol\nx(cid:101)+1, ..., scx , ..., s(cid:98)or"
        },
        {
          "5": "(14)"
        },
        {
          "5": "where scx\nis the key moment of fx, and scx ∈ S. (cid:100)·(cid:101) and (cid:98)·(cid:99)"
        },
        {
          "5": ""
        },
        {
          "5": "represent\nthe ceiling and ﬂoor operators,\nrespectively. ol\nx and"
        },
        {
          "5": ""
        },
        {
          "5": "or\nx denote two absolute offsets, which are formulated as:"
        },
        {
          "5": ""
        },
        {
          "5": "ol"
        },
        {
          "5": "(15)\nx = cx − ˜ol\nx · lmax,"
        },
        {
          "5": ""
        },
        {
          "5": "or\n(16)\nx = cx + ˜or\nx · rmax."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "subject-dependent"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "video 1\nvideo 1\nvideo 1"
        },
        {
          "6": ""
        },
        {
          "6": "video 2\nvideo 2\nvideo 2"
        },
        {
          "6": ""
        },
        {
          "6": "video v\nvideo v\nvideo v"
        },
        {
          "6": ""
        },
        {
          "6": "S1\nS2\nSj"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "subject-independent"
        },
        {
          "6": ""
        },
        {
          "6": "video 1\nvideo 1\nvideo 1"
        },
        {
          "6": ""
        },
        {
          "6": "video 2\nvideo 2\nvideo 2"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "video v\nvideo v\nvideo v"
        },
        {
          "6": ""
        },
        {
          "6": "S1\nS2\nSj"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "Fig.\n3:\nTwo\nvalidation\nstrategies\nfor\nEEG-based\nemotion"
        },
        {
          "6": "recognition. The subject-dependent validation strategy divides"
        },
        {
          "6": "data\nsets\n(purple\nrepresents\nthe\ntest\nset\nand the\nrest\nis\nthe"
        },
        {
          "6": "training\nset) within\nthe\nsubject, which\ncan\nbe\nsubdivided"
        },
        {
          "6": "into two categories: video-level K-fold cross-validation (CV)"
        },
        {
          "6": "and video-level\nleave-one-out cross-validation (LOOCV). The"
        },
        {
          "6": ""
        },
        {
          "6": "subject-independent validation strategy divides data sets (pur-"
        },
        {
          "6": ""
        },
        {
          "6": "ple represents the test set and the rest is the training set) among"
        },
        {
          "6": ""
        },
        {
          "6": "subjects, which can be subdivided into two categories: subject-"
        },
        {
          "6": ""
        },
        {
          "6": "level K-fold CV and subject-level LOOCV. v refers\nto the"
        },
        {
          "6": ""
        },
        {
          "6": "represents\nnumber of videos used in the experiment, and Sj"
        },
        {
          "6": ""
        },
        {
          "6": "j-th subject."
        },
        {
          "6": ""
        },
        {
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to the": ""
        },
        {
          "to the": "represents\nnumber of videos used in the experiment, and Sj"
        },
        {
          "to the": ""
        },
        {
          "to the": "j-th subject."
        },
        {
          "to the": ""
        },
        {
          "to the": ""
        },
        {
          "to the": "video. To cross-compare with the baseline method and"
        },
        {
          "to the": "other methods, we use\na ﬁxed threshold of 5 for\neach"
        },
        {
          "to the": "emotion dimension to discretize the subjective feedback"
        },
        {
          "to the": ""
        },
        {
          "to the": "into two classes (low and high)."
        },
        {
          "to the": ""
        },
        {
          "to the": "• MAHNOB-HCI. The MAHNOB-HCI dataset\ncontains"
        },
        {
          "to the": ""
        },
        {
          "to the": "EEG signals\nof\n30\nsubjects, which were\nrecorded\nat"
        },
        {
          "to the": ""
        },
        {
          "to the": "a 32-channel\nsetting according to the\ninternational 10-"
        },
        {
          "to the": ""
        },
        {
          "to the": "20 system when each subject watched 20 video clips."
        },
        {
          "to the": ""
        },
        {
          "to the": "These ﬁlm clips were\nselected to evoke\nemotions\nand"
        },
        {
          "to the": ""
        },
        {
          "to the": "the subjective feedback was given using a score in the"
        },
        {
          "to the": ""
        },
        {
          "to the": "range of 1 to 9. In the model evaluation, a ﬁxed threshold"
        },
        {
          "to the": ""
        },
        {
          "to the": "of 5 is used to discretize\nthe\nsubjective\nfeedback into"
        },
        {
          "to the": ""
        },
        {
          "to the": "binaries\nfor\neach emotion dimension (valence,\narousal,"
        },
        {
          "to the": ""
        },
        {
          "to the": "dominance, and predictability)."
        },
        {
          "to the": ""
        },
        {
          "to the": ""
        },
        {
          "to the": "B. Experiment Settings and Evaluation Metrics"
        },
        {
          "to the": ""
        },
        {
          "to the": "The\nproposed TAS-Net\nis\nimplemented\nin\nthe\nPyTorch"
        },
        {
          "to the": "platform. Here, we set\nthe L in Eq. (1) to 16,\nthe ϑ in Eq. (12)"
        },
        {
          "to the": "to 0.5,\nthe β in Eq.\n(13)\nto 0.01, and the number of episodes"
        },
        {
          "to the": "N to 5. The dimension of the edge feature in the GCN is 32,"
        },
        {
          "to the": "and the dimension of the hidden state in the GRU cell\nis 256."
        },
        {
          "to the": "The above hyper-parameters are set according to the relevant"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Emotion recognition performance": "dataset.",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "dataset."
        },
        {
          "TABLE I: Emotion recognition performance": "Methods\nClassiﬁcation Task\nPacc",
          "(%) on SEED": "Pf",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Valence\nArousal\nDominance\nLiking"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Methods"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Pacc\nPacc\nPacc\nPacc\nPf\nPf\nPf\nPf"
        },
        {
          "TABLE I: Emotion recognition performance": "Supervised\nSubject-Dependent",
          "(%) on SEED": "Video-level LOOCV",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Supervised\nSubject-Dependent\nK-fold CV"
        },
        {
          "TABLE I: Emotion recognition performance": "DBN [11]\n3-Class\n86.08",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "GSCCA [22]\n3-Class\n82.96",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Li et al.\n[40]\n58.40\n-\n64.20\n-\n65.80\n-\n66.90\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "BiDANN [18]\n3-Class\n92.38",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Chen et al.\n[41]\n67.89\n67.83\n69.09\n68.96\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "DGCNN [5]\n3-Class\n90.40",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Supervised\nSubject-Dependent\nVideo-level LOOCV"
        },
        {
          "TABLE I: Emotion recognition performance": "R2G-STNN [20]\n3-Class\n93.38",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "RGNN [19]\n3-Class\n94.24",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Koelstra et al.\n[36]\n57.60\n56.30\n62.00\n58.30\n-\n-\n55.40\n50.20"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "DT-CWPT [42]\n64.30\n-\n66.20\n-\n68.90\n-\n70.20\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "Supervised with Transfer Learning\nSubject-Independent",
          "(%) on SEED": "Subject-level LOOCV",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "EMD [43]\n69.10\n-\n71.99\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "TCA [15]\n3-Class\n63.64",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Supervised\nSubject-Independent\nK-fold CV"
        },
        {
          "TABLE I: Emotion recognition performance": "BiDANN [18]\n3-Class\n83.28",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "DGCNN [5]\n3-Class\n79.95",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Torres-Valencia et al.\n[44]\n58.75\n-\n55.00\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "R2G-STNN [20]\n3-Class\n84.16",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Atkinson and Campos [14]\n73.14\n-\n73.06\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "RGNN [19]\n3-Class\n85.30",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Liu et al.\n[45]\n69.90\n-\n71.20\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "JDA [17]\n3-Class\n88.28",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Supervised\nSubject-Independent\nSubject-level LOOCV"
        },
        {
          "TABLE I: Emotion recognition performance": "Supervised without Transfer Learning\nSubject-Independent",
          "(%) on SEED": "Subject-level LOOCV",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Shahnaz et al.\n[46]\n64.71\n74.94\n66.51\n76.68\n66.88\n76.67\n70.52\n81.94"
        },
        {
          "TABLE I: Emotion recognition performance": "JDA [17]\n(source domain only)\n3-Class\n58.23",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "DGCNN [5]\n59.29\n-\n61.10\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "ATDD-LSTM [47]\n69.06\n-\n72.97\n-\n-\n-\n-\n-"
        },
        {
          "TABLE I: Emotion recognition performance": "Unsupervised\nSubject-Independent",
          "(%) on SEED": "Subject-level LOOCV",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Unsupervised\nSubject-Dependent\nVideo-level LOOCV"
        },
        {
          "TABLE I: Emotion recognition performance": "2-Class\n80.83",
          "(%) on SEED": "82.03",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "Baseline [24]",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "3-Class\n59.06",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Liang et al.\n[23]\n56.25\n61.25\n62.34\n60.44\n64.22\n64.80\n66.09\n77.52"
        },
        {
          "TABLE I: Emotion recognition performance": "82.34\n2-Class",
          "(%) on SEED": "82.32",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "TAS-Net\n(Ours)",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Unsupervised\nSubject-Independent\nSubject-level LOOCV"
        },
        {
          "TABLE I: Emotion recognition performance": "63.10\n3-Class",
          "(%) on SEED": "-",
          "TABLE II: Emotion recognition performance (%) on DEAP": ""
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Liang et al.\n[23]\n54.30\n53.45\n55.55\n52.77\n57.03\n52.84\n58.91\n59.99"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "Baseline [24]\n56.44\n70.83\n58.55\n72.00\n61.71\n74.32\n65.89\n78.46"
        },
        {
          "TABLE I: Emotion recognition performance": "",
          "(%) on SEED": "",
          "TABLE II: Emotion recognition performance (%) on DEAP": "TAS-Net\n(Ours)\n57.84\n71.80\n60.51\n72.64\n63.17\n75.32\n67.32\n79.74"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": "Predictability"
        },
        {
          "TABLE\nIII:": "Methods",
          "Emotion": "Pacc",
          "recognition": "Pacc",
          "performance": "Pacc",
          "(%)": "Pacc",
          "on": "Pf"
        },
        {
          "TABLE\nIII:": "Supervised",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "Zhu et al.\n[48]",
          "Emotion": "55.72",
          "recognition": "60.23",
          "performance": "-",
          "(%)": "-",
          "on": "-"
        },
        {
          "TABLE\nIII:": "Supervised",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "Soleymani et al.",
          "Emotion": "57.00",
          "recognition": "52.40",
          "performance": "-",
          "(%)": "-",
          "on": "-"
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "Huang et al.\n[49]",
          "Emotion": "62.13",
          "recognition": "61.80",
          "performance": "-",
          "(%)": "-",
          "on": "-"
        },
        {
          "TABLE\nIII:": "LRFS [50]",
          "Emotion": "69.93",
          "recognition": "67.43",
          "performance": "-",
          "(%)": "-",
          "on": "-"
        },
        {
          "TABLE\nIII:": "Unsupervised",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "Baseline",
          "Emotion": "60.64",
          "recognition": "62.06",
          "performance": "67.08",
          "(%)": "74.63",
          "on": "83.61"
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        },
        {
          "TABLE\nIII:": "TAS-Net(Ours)",
          "Emotion": "62.51",
          "recognition": "64.69",
          "performance": "69.84",
          "(%)": "75.11",
          "on": "84.10"
        },
        {
          "TABLE\nIII:": "",
          "Emotion": "",
          "recognition": "",
          "performance": "",
          "(%)": "",
          "on": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "on",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "under"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "Sampling",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "NMI"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.1728"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.1896"
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.0658"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.0879"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.1139"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.4237"
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.0601"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.0869"
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.0481"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w/o",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.4381"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        },
        {
          "Fig. 4: The obtained p-values of": "w",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": "0.4135"
        },
        {
          "Fig. 4: The obtained p-values of": "",
          "the paired t-test between the proposed TAS-Net and the baseline method in terms of Pacc.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: reports the unsupervised",
      "data": [
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "improves\nthe average accuracy Pacc by 26.05%, 5.24%, and"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "21.32%, respectively. Therefore, we believe that\nit\nis effective"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "to model\nthe\ntime-aware\nagent\nby\nconsidering\nthat\nhuman"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "emotion has not only ”short-term continuity” but also ”long-"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "term similarity”."
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "B. Effectiveness of Rewards"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "To explore\nthe optimization effect of\nreward functions\nin"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "the proposed TAS-Net, we set different\nreward functions\nfor"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "comparative experiments. Here, Rrep is to measure how well"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "these selected key emotion fragments can represent\nthe whole"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "trial’s emotional\ninformation. Considering the signals evoked"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "by the same emotion should be similar, Rsim is calculated to"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "cater to the ”long-term similarity” of emotions. Rdiv is another"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "commonly\nused\nreward\nfunction\nto measure\nthe\ndifference"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "between the\nselected key emotion fragments\nin the\nfeature"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "space\nto\nevaluate\nthe\ndegree\nof\ndiversity\nof\nthe\ngenerated"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "summary. Fig. 5(b) shows the obtained unsupervised emotion"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "recognition accuracies using different\nreward functions. R1,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "R2, R3, R4 and R5 represent Rrep, Rsim, Rdiv, Rrep + Rdiv"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "respectively.\nIt\nis\nfound that\nthe\nreward\nand Rrep + Rsim,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "function\nbetter\nperformance\nof Rsim achieves\nthan Rdiv,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "which indicates\nthat\nthe\nsimilarity reward function is more"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "suitable\nfor\nthe key emotion fragment detection task. Com-"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "pared with the reward function Rrep, when the reward function"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "the recognition performance\nis composed of Rrep and Rdiv,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "decreases, while when the\nreward function is\ncomposed of"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "the recognition performance increases, which\nRrep and Rsim,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "proves that\nthe same emotion has similarity in different\ntime"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "periods."
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "C. Performance\nComparison\nUsing\nDifferent\nClustering"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "Methods"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": ""
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "The purpose of this study is to design a sampling model that"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "can efﬁciently localize short-term continuous\nfragments with"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "correlated emotions in EEG time series, so as to achieve better"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "unsupervised emotion recognition performance. Theoretically,"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "the sampling set can effectively improve the emotion recogni-"
        },
        {
          "Fig. 5: Emotion recognition performance (%) under different methods and parameters comparison.": "tion performance of different clustering methods. We employ"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "#1"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "0"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "0"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "53"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": ""
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "#1"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "0"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "0"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "33"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "100"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "67"
        },
        {
          "TABLE V: The recall (%) of emotion localization task on SEED dataset. Note that our method is based on subject-independent": "53"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "dependent validation strategy.",
          "three methods are based on subject-": ""
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "",
          "three methods are based on subject-": ""
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject\nID",
          "three methods are based on subject-": ""
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "",
          "three methods are based on subject-": "AsI-AEIR [53]"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject1",
          "three methods are based on subject-": "40"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject2",
          "three methods are based on subject-": "42"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject3",
          "three methods are based on subject-": "18"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject4",
          "three methods are based on subject-": "45"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject5",
          "three methods are based on subject-": "40"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject6",
          "three methods are based on subject-": "58"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject7",
          "three methods are based on subject-": "43"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject8",
          "three methods are based on subject-": "76"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject9",
          "three methods are based on subject-": "51"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject10",
          "three methods are based on subject-": "69"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject11",
          "three methods are based on subject-": "73"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject12",
          "three methods are based on subject-": "62"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject13",
          "three methods are based on subject-": "39"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject14",
          "three methods are based on subject-": "57"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject15",
          "three methods are based on subject-": "43"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "All Subjects",
          "three methods are based on subject-": "50"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "dependent validation strategy.",
          "three methods are based on subject-": ""
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "",
          "three methods are based on subject-": "Average"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject\nID",
          "three methods are based on subject-": ""
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "",
          "three methods are based on subject-": "AsI-AEIR [53]"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject1",
          "three methods are based on subject-": "40"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject2",
          "three methods are based on subject-": "42"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject3",
          "three methods are based on subject-": "18"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject4",
          "three methods are based on subject-": "45"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject5",
          "three methods are based on subject-": "40"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject6",
          "three methods are based on subject-": "58"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject7",
          "three methods are based on subject-": "43"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject8",
          "three methods are based on subject-": "76"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject9",
          "three methods are based on subject-": "51"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject10",
          "three methods are based on subject-": "69"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject11",
          "three methods are based on subject-": "73"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject12",
          "three methods are based on subject-": "62"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject13",
          "three methods are based on subject-": "39"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject14",
          "three methods are based on subject-": "57"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "Subject15",
          "three methods are based on subject-": "43"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "All Subjects",
          "three methods are based on subject-": "50"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "E. Performance on Emotion Localization",
          "three methods are based on subject-": "that employ subject-dependent"
        },
        {
          "TABLE VI: The recall (%) of emotion localization task on SEED dataset. Note that all": "",
          "three methods are based on subject-": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E. Performance on Emotion Localization": "",
          "emotion localization methods\nthat employ subject-dependent": "LOOCV, our method achieves the highest performance. When"
        },
        {
          "E. Performance on Emotion Localization": "Recently, Zhang et al.\n[39] proposed a supervised learning",
          "emotion localization methods\nthat employ subject-dependent": "subject-independent LOOCV is used (which is more difﬁcult"
        },
        {
          "E. Performance on Emotion Localization": "based\nhierarchical\nself-attention\nnetwork\non SEED dataset,",
          "emotion localization methods\nthat employ subject-dependent": "than subject-dependent LOOCV due to the individual differ-"
        },
        {
          "E. Performance on Emotion Localization": "which incorporates\nemotion localization task for\nimproving",
          "emotion localization methods\nthat employ subject-dependent": "ences), our model’s\nsubject-independent LOOCV results are"
        },
        {
          "E. Performance on Emotion Localization": "emotion recognition performance. Similarly, we\nalso extend",
          "emotion localization methods\nthat employ subject-dependent": "comparable with AsI-AEIR [53]’s subject-dependent LOOCV"
        },
        {
          "E. Performance on Emotion Localization": "our\nproposed TAS-Net\non\nemotion\nlocalization\napplication",
          "emotion localization methods\nthat employ subject-dependent": "results.\nIn\nother words,\nour model\nis more\neffective\nand"
        },
        {
          "E. Performance on Emotion Localization": "under an unsupervised learning manner. In the emotion local-",
          "emotion localization methods\nthat employ subject-dependent": "efﬁcient\nin emotion localization under both subject-dependent"
        },
        {
          "E. Performance on Emotion Localization": "ization task, EEG signals corresponding to each video stimulus",
          "emotion localization methods\nthat employ subject-dependent": "LOOCV and subject-independent LOOCV. For a more intu-"
        },
        {
          "E. Performance on Emotion Localization": "were segmented, and then twenty people were asked to score",
          "emotion localization methods\nthat employ subject-dependent": "itive comparison, we also visualize the emotion localization"
        },
        {
          "E. Performance on Emotion Localization": "each\nfragment\naccording\nto\nthe\ncorresponding\nemotion\nto",
          "emotion localization methods\nthat employ subject-dependent": "results when different methods are used. As shown in Fig. 6,"
        },
        {
          "E. Performance on Emotion Localization": "obtain the ground truth labels.\nIn addition,\nthe recall\nis used",
          "emotion localization methods\nthat employ subject-dependent": "the ground truth labels with the localization results and tIoU"
        },
        {
          "E. Performance on Emotion Localization": "as\nthe\nevaluation metric\nfor\nthis\ntask,\nand the\nthreshold of",
          "emotion localization methods\nthat employ subject-dependent": "values are reported, which shows the possibility to detect key"
        },
        {
          "E. Performance on Emotion Localization": "tIoU (the\noverlap\nrate\nin\nthe\ntimeline\nbetween\nthe\nground",
          "emotion localization methods\nthat employ subject-dependent": "emotion fragments even when label\ninformation is missing."
        },
        {
          "E. Performance on Emotion Localization": "is\nset\ntruth fragments and the top-X selected fragments fx)",
          "emotion localization methods\nthat employ subject-dependent": ""
        },
        {
          "E. Performance on Emotion Localization": "",
          "emotion localization methods\nthat employ subject-dependent": "V. CONCLUSION"
        },
        {
          "E. Performance on Emotion Localization": "to 0.5. Note here that,\nfor cross-comparison with [39], both",
          "emotion localization methods\nthat employ subject-dependent": ""
        },
        {
          "E. Performance on Emotion Localization": "subject-independent LOOCV and subject-dependent LOOCV",
          "emotion localization methods\nthat employ subject-dependent": "This paper develops an unsupervised time-aware sampling"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ground Truth": "AsI-AEIR[52]",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": "144s\nsad\n147s"
        },
        {
          "Ground Truth": "",
          "66s": "67s",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": ""
        },
        {
          "Ground Truth": "Zhang et al. [24]",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": "sad"
        },
        {
          "Ground Truth": "",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": ""
        },
        {
          "Ground Truth": "Ours1",
          "66s": "",
          "sad": "",
          "80s": "78s",
          "135s": "",
          "149s": "sad"
        },
        {
          "Ground Truth": "",
          "66s": "66s",
          "sad": "",
          "80s": "80s",
          "135s": "",
          "149s": ""
        },
        {
          "Ground Truth": "Ours2",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": "sad"
        },
        {
          "Ground Truth": "",
          "66s": "",
          "sad": "Case2",
          "80s": "",
          "135s": "",
          "149s": ""
        },
        {
          "Ground Truth": "Ground Truth",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "97s",
          "149s": ""
        },
        {
          "Ground Truth": "AsI-AEIR[52]",
          "66s": "",
          "sad": "",
          "80s": "74s",
          "135s": "",
          "149s": "sad"
        },
        {
          "Ground Truth": "Zhang et al. [24]",
          "66s": "",
          "sad": "sad",
          "80s": "",
          "135s": "96s",
          "149s": "110s"
        },
        {
          "Ground Truth": "Ours1",
          "66s": "",
          "sad": "",
          "80s": "",
          "135s": "",
          "149s": "sad"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "63s": "",
          "sad": "Case4",
          "77s": "",
          "tIoU=82%": "",
          "96s": "",
          "110s": "",
          "tIoU=76%": ""
        },
        {
          "63s": "120s",
          "sad": "sad",
          "77s": "",
          "tIoU=82%": "",
          "96s": "165s",
          "110s": "",
          "tIoU=76%": ""
        },
        {
          "63s": "118s",
          "sad": "172s",
          "77s": "",
          "tIoU=82%": "tIoU=63%",
          "96s": "",
          "110s": "",
          "tIoU=76%": "tIoU=47%"
        },
        {
          "63s": "121s",
          "sad": "sad",
          "77s": "",
          "tIoU=82%": "tIoU=87%",
          "96s": "",
          "110s": "177s",
          "tIoU=76%": "tIoU=67%"
        },
        {
          "63s": "",
          "sad": "sad",
          "77s": "",
          "tIoU=82%": "tIoU=33%",
          "96s": "162s",
          "110s": "176",
          "tIoU=76%": "tIoU=61%"
        },
        {
          "63s": "120s",
          "sad": "sad",
          "77s": "",
          "tIoU=82%": "tIoU=100%",
          "96s": "",
          "110s": "",
          "tIoU=76%": "tIoU=81%"
        },
        {
          "63s": "",
          "sad": "Case6",
          "77s": "",
          "tIoU=82%": "",
          "96s": "",
          "110s": "",
          "tIoU=76%": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Case5\nCase6"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Fig. 6: Visualization of\nselected key fragments\nin different videos. Ours1\nand Ours2\nrepresent\nthe results obtained by our"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "method based on subject-independent LOOCV and subject-dependent LOOCV,\nrespectively."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "REFERENCES\nﬂat\nimportance score for each sample-level feature in the EEG"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "time series, the proposed TAS-Net accomplishes more efﬁcient"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[1] A. S. Cowen and D. Keltner, “Self-report captures 27 distinct categories"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "key emotion fragment detection by seamlessly combining local"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "of emotion bridged by continuous gradients,” Proceedings of the national"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "time-based GCN and a global\ntime-based BiGRU. To the best\nacademy of sciences, vol. 114, no. 38, pp. E7900–E7909, 2017."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[2]\nT. Horikawa, A. S. Cowen, D. Keltner, and Y. Kamitani, “The neural"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "of our knowledge, we are the ﬁrst to utilize deep reinforcement"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "representation of visually evoked emotion is high-dimensional, categor-"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "learning to complete key emotion fragment detection in EEG"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "ical, and distributed across transmodal brain regions,” Iscience, vol. 23,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "signals\nfor more accurate unsupervised emotion recognition.\nno. 5, p. 101060, 2020."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[3] Y.-J. Huang, C.-Y. Wu, A. M.-K. Wong, and B.-S. Lin, “Novel active\nExperiments on three\nchallenging public datasets\nshow that"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "IEEE\ncomb-shaped dry electrode\nfor\neeg measurement\nin hairy site,”"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "our TAS-Net achieves excellent performance. The performance"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Transactions on Biomedical Engineering, vol. 62, no. 1, pp. 256–263,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "in\ndetecting\nkey\nemotion\nfragments\nfrom the\ncontinuously\n2014."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[4] Y.-J. Liu, M. Yu, G. Zhao,\nJ. Song, Y. Ge,\nand Y. Shi,\n“Real-time\ncollected EEG signals has been veriﬁed, and the superiority"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "IEEE\nmovie-induced discrete\nemotion recognition from eeg signals,”"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "of emotion recognition has been demonstrated. Since the label"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Transactions on Affective Computing, vol. 9, no. 4, pp. 550–562, 2017."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "information\nis\nnot\nrequired\nby\nthe\nproposed\nnew method,\n[5]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "dynamical graph convolutional neural networks,” IEEE Transactions on\nTAS-Net\nis a fully unsupervised method that could be easily"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "extended\nto\nother EEG-related\napplications\nand\nbe\nutilized"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[6] X. Li, Y. Zhang,\nP. Tiwari, D.\nSong, B. Hu, M. Yang, Z. Zhao,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "to automatically extract essential\ninformation from time-series"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "N. Kumar, and P. Marttinen, “Eeg based emotion recognition: A tutorial"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "and review,” ACM Computing Surveys (CSUR), 2022.\nEEG data."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[7] M. Alsolamy and A. Fattouh,\n“Emotion estimation from eeg signals"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "during listening to quran using psd features,” in 2016 7th International"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Conference on Computer Science and Information Technology (CSIT).\nVI. CONFLICTS OF INTEREST"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "IEEE, 2016, pp. 1–5."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "The authors declare that\nthey have no conﬂicts of\ninterest.\n[8] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "eeg-based emotion classiﬁcation,” in 2013 6th International IEEE/EMBS"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Conference on Neural Engineering (NER).\nIEEE, 2013, pp. 81–84."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "VII. ACKNOWLEDGMENT\n[9] R.-N. Duan, X.-W. Wang, and B.-L. Lu, “Eeg-based emotion recognition"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "in listening music by using support vector machine and linear dynamic"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "This work was\nsupported\nin\npart\nby\nthe National Nat-\nsystem,” in International Conference on Neural Information Processing."
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Springer, 2012, pp. 468–475.\nural\nScience\nFoundation\nof China\nunder Grant\n82272114,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[10] Y.-P. Lin, C.-H. Wang, T.-P. Jung, T.-L. Wu, S.-K. Jeng, J.-R. Duann, and"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "62276169, 61906122,\nand 62071310,\nin part by Shenzhen-"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "J.-H. Chen, “Eeg-based emotion recognition in music listening,” IEEE"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Hong Kong\nInstitute\nof\nBrain\nScience-Shenzhen\nFunda-"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "Transactions on Biomedical Engineering, vol. 57, no. 7, pp. 1798–1806,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "2010.\nmental\nResearch\nInstitutions\n(2022SHIBS0003),\nin\npart"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "[11] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "by\nShenzhen\nScience\nand\nTechnology Research\nand De-"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "channels for eeg-based emotion recognition with deep neural networks,”"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "velopment\nFund\nfor\nSustainable\nDevelopment\nProject"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "IEEE Transactions on autonomous mental development, vol. 7, no. 3,"
        },
        {
          "tIoU=100%\ntIoU=81%\nsad\n167s\nsad\n120s\n134s\nOurs2\n181s\nOurs2": "pp. 162–175, 2015.\n(No.KCXFZ20201221173613036)."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "[35] D. Zhou, J. Huang, and B. Sch¨olkopf, “Learning with hypergraphs: Clus-"
        },
        {
          "12": "information\ntering, classiﬁcation, and embedding,” Advances in neural"
        },
        {
          "12": "processing systems, vol. 19, 2006."
        },
        {
          "12": "[36]\nS. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,"
        },
        {
          "12": "T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;"
        },
        {
          "12": "using physiological signals,” IEEE transactions on affective computing,"
        },
        {
          "12": "vol. 3, no. 1, pp. 18–31, 2011."
        },
        {
          "12": "[37] M. Soleymani,\nJ. Lichtenauer, T. Pun, and M. Pantic, “A multimodal"
        },
        {
          "12": "database for affect recognition and implicit\ntagging,” IEEE transactions"
        },
        {
          "12": "on affective computing, vol. 3, no. 1, pp. 42–55, 2011."
        },
        {
          "12": "[38] N. Kwak and C.-H. Choi, “Input feature selection by mutual information"
        },
        {
          "12": "based on parzen window,” IEEE transactions on pattern analysis and"
        },
        {
          "12": "machine intelligence, vol. 24, no. 12, pp. 1667–1671, 2002."
        },
        {
          "12": "[39] Y. Zhang, H. Liu, D. Zhang, X. Chen, T. Qin,\nand Q. Zheng, “Eeg-"
        },
        {
          "12": "based emotion recognition with emotion localization via hierarchical"
        },
        {
          "12": "self-attention,” IEEE Transactions on Affective Computing, 2022."
        },
        {
          "12": "[40] X. Li, P. Zhang, D. Song, G. Yu, Y. Hou, and B. Hu, “Eeg based emotion"
        },
        {
          "12": ""
        },
        {
          "12": "identiﬁcation using unsupervised deep feature learning,” 2015."
        },
        {
          "12": ""
        },
        {
          "12": "[41]\nJ. Chen, B. Hu, P. Moore, X. Zhang, and X. Ma, “Electroencephalogram-"
        },
        {
          "12": ""
        },
        {
          "12": "based\nemotion\nassessment\nsystem using\nontology\nand\ndata mining"
        },
        {
          "12": ""
        },
        {
          "12": "techniques,” Applied Soft Computing, vol. 30, pp. 663–674, 2015."
        },
        {
          "12": ""
        },
        {
          "12": "[42] D. S. Naser and G. Saha, “Recognition of emotions induced by music"
        },
        {
          "12": ""
        },
        {
          "12": "videos using dt-cwpt,” in 2013 Indian Conference on Medical Informat-"
        },
        {
          "12": ""
        },
        {
          "12": "ics and Telemedicine (ICMIT).\nIEEE, 2013, pp. 53–57."
        },
        {
          "12": ""
        },
        {
          "12": "[43] N. Zhuang, Y. Zeng, L. Tong, C. Zhang, H. Zhang, and B. Yan, “Emotion"
        },
        {
          "12": ""
        },
        {
          "12": "recognition from eeg signals using multidimensional information in emd"
        },
        {
          "12": ""
        },
        {
          "12": "domain,” BioMed research international, vol. 2017, 2017."
        },
        {
          "12": ""
        },
        {
          "12": "[44] C. A. Torres-Valencia, H. F. Garcia-Arias, M. A. A. Lopez, and A. A."
        },
        {
          "12": ""
        },
        {
          "12": "Orozco-Guti´errez,\n“Comparative\nanalysis of physiological\nsignals\nand"
        },
        {
          "12": ""
        },
        {
          "12": "electroencephalogram (eeg)\nfor multimodal emotion recognition using"
        },
        {
          "12": ""
        },
        {
          "12": "generative models,” in 2014 XIX Symposium on Image, Signal Process-"
        },
        {
          "12": ""
        },
        {
          "12": "ing and Artiﬁcial Vision.\nIEEE, 2014, pp. 1–5."
        },
        {
          "12": ""
        },
        {
          "12": "[45]\nJ. Liu, H. Meng, A. Nandi,\nand M. Li,\n“Emotion\ndetection\nfrom"
        },
        {
          "12": ""
        },
        {
          "12": "2016\n12th\ninternational\nconference\non\nnatural\neeg\nrecordings,”\nin"
        },
        {
          "12": ""
        },
        {
          "12": "computation,\nfuzzy\nsystems\nand\nknowledge\ndiscovery\n(ICNC-FSKD)."
        },
        {
          "12": ""
        },
        {
          "12": "IEEE, 2016, pp. 1722–1727."
        },
        {
          "12": ""
        },
        {
          "12": "[46] C. Shahnaz, S. S. Hasan et al., “Emotion recognition based on wavelet"
        },
        {
          "12": ""
        },
        {
          "12": "analysis of empirical mode decomposed eeg signals responsive to music"
        },
        {
          "12": ""
        },
        {
          "12": "videos,” in 2016 IEEE Region 10 Conference (TENCON).\nIEEE, 2016,"
        },
        {
          "12": ""
        },
        {
          "12": "pp. 424–427."
        },
        {
          "12": ""
        },
        {
          "12": "[47] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-J. Liu,"
        },
        {
          "12": ""
        },
        {
          "12": "and H. Wang, “An efﬁcient\nlstm network for emotion recognition from"
        },
        {
          "12": ""
        },
        {
          "12": "multichannel eeg signals,” IEEE Transactions on Affective Computing,"
        },
        {
          "12": ""
        },
        {
          "12": "2020."
        },
        {
          "12": ""
        },
        {
          "12": "[48] Y. Zhu, S. Wang, and Q. Ji, “Emotion recognition from users’ eeg signals"
        },
        {
          "12": ""
        },
        {
          "12": "with the help of stimulus videos,” in 2014 IEEE international conference"
        },
        {
          "12": ""
        },
        {
          "12": "on multimedia and expo (ICME).\nIEEE, 2014, pp. 1–6."
        },
        {
          "12": ""
        },
        {
          "12": "[49] X. Huang, J. Kortelainen, G. Zhao, X. Li, A. Moilanen, T. Sepp¨anen, and"
        },
        {
          "12": ""
        },
        {
          "12": "M. Pietik¨ainen, “Multi-modal emotion analysis from facial expressions"
        },
        {
          "12": ""
        },
        {
          "12": "and electroencephalogram,” Computer Vision and Image Understanding,"
        },
        {
          "12": ""
        },
        {
          "12": "vol. 147, pp. 114–124, 2016."
        },
        {
          "12": ""
        },
        {
          "12": "[50]\nZ. Yin, L. Liu,\nJ. Chen, B. Zhao,\nand Y. Wang,\n“Locally\nrobust"
        },
        {
          "12": ""
        },
        {
          "12": "eeg feature selection for\nindividual-independent emotion recognition,”"
        },
        {
          "12": ""
        },
        {
          "12": "Expert Systems with Applications, vol. 162, p. 113768, 2020."
        },
        {
          "12": ""
        },
        {
          "12": "[51]\nS. A. Shah and V. Koltun, “Robust continuous clustering,” Proceedings"
        },
        {
          "12": ""
        },
        {
          "12": "of\nthe National Academy of Sciences, vol. 114, no. 37, pp. 9814–9819,"
        },
        {
          "12": ""
        },
        {
          "12": "2017."
        },
        {
          "12": ""
        },
        {
          "12": "[52] W. Zhang, X. Wang, D. Zhao,\nand X. Tang,\n“Graph degree\nlinkage:"
        },
        {
          "12": ""
        },
        {
          "12": "Agglomerative clustering on a directed graph,” in European conference"
        },
        {
          "12": ""
        },
        {
          "12": "on computer vision.\nSpringer, 2012, pp. 428–441."
        },
        {
          "12": "[53]\nP. C.\nPetrantonakis\nand L.\nJ. Hadjileontiadis,\n“Adaptive\nemotional"
        },
        {
          "12": "information retrieval\nfrom eeg signals\nin the time-frequency domain,”"
        },
        {
          "12": "IEEE Transactions on Signal Processing, vol. 60, no. 5, pp. 2604–2616,"
        },
        {
          "12": "2012."
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "2",
      "title": "The neural representation of visually evoked emotion is high-dimensional, categorical, and distributed across transmodal brain regions",
      "authors": [
        "T Horikawa",
        "A Cowen",
        "D Keltner",
        "Y Kamitani"
      ],
      "year": "2020",
      "venue": "Iscience"
    },
    {
      "citation_id": "3",
      "title": "Novel active comb-shaped dry electrode for eeg measurement in hairy site",
      "authors": [
        "Y.-J Huang",
        "C.-Y Wu",
        "-K Wong",
        "B.-S Lin"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "4",
      "title": "Real-time movie-induced discrete emotion recognition from eeg signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "7",
      "title": "Emotion estimation from eeg signals during listening to quran using psd features",
      "authors": [
        "M Alsolamy",
        "A Fattouh"
      ],
      "year": "2016",
      "venue": "2016 7th International Conference on Computer Science and Information Technology"
    },
    {
      "citation_id": "8",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "9",
      "title": "Eeg-based emotion recognition in listening music by using support vector machine and linear dynamic system",
      "authors": [
        "R.-N Duan",
        "X.-W Wang",
        "B.-L Lu"
      ],
      "year": "2012",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "10",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "11",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "12",
      "title": "fractal-based valence level recognition in Transactions on computational science XVIII",
      "authors": [
        "Y Liu",
        "O Sourina"
      ],
      "year": "2013",
      "venue": "fractal-based valence level recognition in Transactions on computational science XVIII"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition using recurrence plot analysis and k nearest neighbor classifier",
      "authors": [
        "F Bahari",
        "A Janghorbani"
      ],
      "year": "2013",
      "venue": "2013 20th Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "14",
      "title": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "16",
      "title": "3dcann: a spatio-temporal convolution attention neural network for eeg emotion recognition",
      "authors": [
        "S Liu",
        "X Wang",
        "L Zhao",
        "B Li",
        "W Hu",
        "J Yu",
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "17",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "18",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "19",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "C Wang",
        "Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "23",
      "title": "An unsupervised eeg decoding system for human emotion recognition",
      "authors": [
        "Z Liang",
        "S Oba",
        "S Ishii"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "Eegfusenet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional eeg with an application to emotion recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "25",
      "title": "Playing fps games with deep reinforcement learning",
      "authors": [
        "G Lample",
        "D Chaplot"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Where does alphago go: From church-turing thesis to alphago thesis and beyond",
      "authors": [
        "F.-Y Wang",
        "J Zhang",
        "X Zheng",
        "X Wang",
        "Y Yuan",
        "X Dai",
        "J Zhang",
        "L Yang"
      ],
      "year": "2016",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "27",
      "title": "Deep reinforcement learning with a natural language action space",
      "authors": [
        "J He",
        "J Chen",
        "X He",
        "J Gao",
        "L Li",
        "L Deng",
        "M Ostendorf"
      ],
      "year": "2015",
      "venue": "Deep reinforcement learning with a natural language action space",
      "arxiv": "arXiv:1511.04636"
    },
    {
      "citation_id": "28",
      "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
      "authors": [
        "D Yarats",
        "I Kostrikov",
        "R Fergus"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "Neural architecture search with reinforcement learning",
      "authors": [
        "B Zoph",
        "Q Le"
      ],
      "year": "2016",
      "venue": "Neural architecture search with reinforcement learning",
      "arxiv": "arXiv:1611.01578"
    },
    {
      "citation_id": "30",
      "title": "Playing atari with deep reinforcement learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Graves",
        "I Antonoglou",
        "D Wierstra",
        "M Riedmiller"
      ],
      "year": "2013",
      "venue": "Playing atari with deep reinforcement learning",
      "arxiv": "arXiv:1312.5602"
    },
    {
      "citation_id": "31",
      "title": "Fuzzy integral optimization with deep q-network for eeg-based intention recognition",
      "authors": [
        "D Zhang",
        "L Yao",
        "S Wang",
        "K Chen",
        "Z Yang",
        "B Benatallah"
      ],
      "year": "2018",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "32",
      "title": "Eeg-based emotion recognition via neural architecture search",
      "authors": [
        "C Li",
        "Z Zhang",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Deep reinforcement learning for unsupervised video summarization with diversity-representativeness reward",
      "authors": [
        "K Zhou",
        "Y Qiao",
        "T Xiang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "authors": [
        "R Williams"
      ],
      "year": "1992",
      "venue": "Machine learning"
    },
    {
      "citation_id": "35",
      "title": "Learning with hypergraphs: Clustering, classification, and embedding",
      "authors": [
        "D Zhou",
        "J Huang",
        "B Schölkopf"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "36",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "38",
      "title": "Input feature selection by mutual information based on parzen window",
      "authors": [
        "N Kwak",
        "C.-H Choi"
      ],
      "year": "2002",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "39",
      "title": "Eegbased emotion recognition with emotion localization via hierarchical self-attention",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Zhang",
        "X Chen",
        "T Qin",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Eeg based emotion identification using unsupervised deep feature learning",
      "authors": [
        "X Li",
        "P Zhang",
        "D Song",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2015",
      "venue": "Eeg based emotion identification using unsupervised deep feature learning"
    },
    {
      "citation_id": "41",
      "title": "Electroencephalogrambased emotion assessment system using ontology and data mining techniques",
      "authors": [
        "J Chen",
        "B Hu",
        "P Moore",
        "X Zhang",
        "X Ma"
      ],
      "year": "2015",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "42",
      "title": "Recognition of emotions induced by music videos using dt-cwpt",
      "authors": [
        "D Naser",
        "G Saha"
      ],
      "year": "2013",
      "venue": "2013 Indian Conference on Medical Informatics and Telemedicine (ICMIT)"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition from eeg signals using multidimensional information in emd domain",
      "authors": [
        "N Zhuang",
        "Y Zeng",
        "L Tong",
        "C Zhang",
        "H Zhang",
        "B Yan"
      ],
      "year": "2017",
      "venue": "BioMed research international"
    },
    {
      "citation_id": "44",
      "title": "Comparative analysis of physiological signals and electroencephalogram (eeg) for multimodal emotion recognition using generative models",
      "authors": [
        "C Torres-Valencia",
        "H Garcia-Arias",
        "M Lopez",
        "A Orozco-Gutiérrez"
      ],
      "year": "2014",
      "venue": "2014 XIX Symposium on Image, Signal Processing and Artificial Vision"
    },
    {
      "citation_id": "45",
      "title": "Emotion detection from eeg recordings",
      "authors": [
        "J Liu",
        "H Meng",
        "A Nandi",
        "M Li"
      ],
      "year": "2016",
      "venue": "2016 12th international conference on natural computation, fuzzy systems and knowledge discovery"
    },
    {
      "citation_id": "46",
      "title": "Emotion recognition based on wavelet analysis of empirical mode decomposed eeg signals responsive to music videos",
      "authors": [
        "C Shahnaz",
        "S Hasan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Region 10 Conference (TENCON)"
    },
    {
      "citation_id": "47",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition from users' eeg signals with the help of stimulus videos",
      "authors": [
        "Y Zhu",
        "S Wang",
        "Q Ji"
      ],
      "year": "2014",
      "venue": "2014 IEEE international conference on multimedia and expo (ICME)"
    },
    {
      "citation_id": "49",
      "title": "Multi-modal emotion analysis from facial expressions and electroencephalogram",
      "authors": [
        "X Huang",
        "J Kortelainen",
        "G Zhao",
        "X Li",
        "A Moilanen",
        "T Seppänen",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "50",
      "title": "Locally robust eeg feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "51",
      "title": "Robust continuous clustering",
      "authors": [
        "S Shah",
        "V Koltun"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "52",
      "title": "Graph degree linkage: Agglomerative clustering on a directed graph",
      "authors": [
        "W Zhang",
        "X Wang",
        "D Zhao",
        "X Tang"
      ],
      "year": "2012",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "53",
      "title": "Adaptive emotional information retrieval from eeg signals in the time-frequency domain",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Signal Processing"
    }
  ]
}