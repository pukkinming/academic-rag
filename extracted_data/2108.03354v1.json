{
  "paper_id": "2108.03354v1",
  "title": "Hetemotionnet: Two-Stream Heterogeneous Graph Recurrent Neural Network For Multi-Modal Emotion Recognition",
  "published": "2021-08-07T03:03:52Z",
  "authors": [
    "Ziyu Jia",
    "Youfang Lin",
    "Jing Wang",
    "Zhiyang Feng",
    "Xiangheng Xie",
    "Caijie Chen"
  ],
  "keywords": [
    "Multi-modal emotion recognition",
    "Graph recurrent neural network",
    "Affective computing",
    "Heterogeneous graph"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The research on human emotion under multimedia stimulation based on physiological signals is an emerging field, and important progress has been achieved for emotion recognition based on multimodal signals. However, it is challenging to make full use of the complementarity among spatial-spectral-temporal domain features for emotion recognition, as well as model the heterogeneity and correlation among multi-modal signals. In this paper, we propose a novel two-stream heterogeneous graph recurrent neural network, named HetEmotionNet, fusing multi-modal physiological signals for emotion recognition. Specifically, HetEmotionNet consists of the spatial-temporal stream and the spatial-spectral stream, which can fuse spatial-spectral-temporal domain features in a unified framework. Each stream is composed of the graph transformer network for modeling the heterogeneity, the graph convolutional network for modeling the correlation, and the gated recurrent unit for capturing the temporal domain or spectral domain dependency. Extensive experiments on two real-world datasets demonstrate that our proposed model achieves better performance than state-of-theart baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is a mental and physiological state which results from many senses and thoughts  [10] . In recent years, emotion plays an increasingly important role in multiple areas, such as disease detection, human-computer interaction (HCI), and virtual reality. To collect data for emotion recognition, researchers often utilize multiple multimedia materials to stimulate participants and induce emotion. For the reason that videos, audio, and so on have the advantage of easy collection, there is a mass of studies using these data. Though they achieve certain performance, these data cannot guarantee to reflect a real emotional state since human can disguise their facial expression, sound, and so on. Physiological signals cannot be disguised, so they can reflect human emotion objectively. Therefore, physiological signals are more suitable for emotion recognition.\n\nTo achieve better performance for emotion recognition, some methods are proposed which use physiological signals and achieve  The graph of EEG signals. The graph method preserves the functional connectivity among the electrodes to the greatest extent and does not introduce noise.\n\nstate-of-the-art performance  [12, 34, 45] . However, several challenges still exist for emotion recognition using physiological signals.\n\nC1: How to utilize the complementarity among spatialspectral-temporal domain information efficiently. Temporal domain information and spectral domain information in the spatial domain of physiological signals usually have different activation degree. For example, Figure  1  indicates the differences between the temporal domain and the spectral domain in the spatial domain of electroencephalography (EEG) signals for different emotional states. To be specific, in the spatial-temporal domain, the activation degree of the temporal domain information directly reflects brain activity. The high degree of activation is usually related to positive emotion and the low degree of activation is often related to negative emotion. In the spatial-spectral domain, the activation degree of the ğ›¾ band is often high in negative emotions and low in positive emotions  [35] . Considering these phenomena, most studies design various deep learning methods to extract spatial-temporal domain information or spatial-spectral domain information for emotion recognition. These methods can be mainly divided into two categories. One concentrates on the spatial-temporal domain  [6, 25, 40, 47] , and the other one concentrates on the spatial-spectral domain  [49, 51, 56, 57] . Specifically, the methods focusing on spatial-temporal domain information often only take the raw signals or other statistical features as the input of the network. The methods focusing on the spatialspectral domain often capture the correlation of the spectral domain features such as power spectral density (PSD) and differential entropy (DE) in the physiological signals. Though these two categories of methods achieve relatively satisfactory performance, they ignore complementarity among spatial-spectral-temporal domain information. SST-EmotionNet is proposed to extract the spatialspectral-temporal domain information in the image-like maps  [12] . However, areas of brain that are not covered by biosensors also produce bioelectrical signals. The voltage of these signals is usually not 0, so artificial zero-filling may introduce noise, as shown in Figure  2 (a). Besides, in Figure  2 (b), the graph-based method can reflect the topological relationship of the brain  [17, 48] . These shortcomings limit the performance of classification to a certain extent. Therefore, how to effectively extract spatial-spectral-temporal domain information is challenging work.   [23, 26] . However, most current methods only model the heterogeneity or correlation among different modalities separately. As for the heterogeneity, existing works use different feature extractors, such as CNN, to extract features of different modalities and capture the heterogeneity in multi-modal data  [27, 29, 34, 36] . However, these methods ignore the correlation among different modalities because the signals of different modalities are extracted separately. As for the correlation, existing works usually combine the signals of different modalities into a new data representation and feed it to a deep neural network to capture the correlation  [30, 38] , such as group sparse canonical correlation analysis (GSCCA)  [55] . However, these methods ignore the heterogeneity among different modalities because the features of all modalities are extracted by the same feature extractor. Therefore, how to model the heterogeneity and correlation among multi-modal signals simultaneously becomes a challenge.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Eeg Ecg",
      "text": "To address these challenges, we propose a two-stream heterogeneous graph recurrent neural network named HetEmotionNet, which is composed of the spatial-temporal stream and spatialspectral stream and takes heterogeneous graph sequences as input. A stream consists of a graph transformer network (GTN), a graph convolutional network (GCN), and a gated recurrent unit (GRU). The physiological signals are constructed into heterogeneous graph sequences as the input of our model. Several heterogeneous graphs are stacked to form a heterogeneous graph sequence. A heterogeneous graph consists of nodes and edges. A node represents a channel of signals whose type depends on the modality of the channel. An edge represents the connection between two nodes. To sum up, the main contributions of our work are summarized as follows:\n\nâ€¢ We propose a novel graph-based two-stream structure composed of the spatial-temporal stream and the spatial-spectral stream which can simultaneously fuse spatial-spectral-temporal domain features of physiological signals in a unified deep neural network framework.\n\nâ€¢ Each stream consists of a GTN for modeling the heterogeneity, a GCN for modeling the correlation, and a GRU for capturing the temporal or spectral dependency.\n\nâ€¢ Extensive experiments are conducted on two benchmark datasets to evaluate the performance of the proposed model. The results indicate the proposed model outperforms all the state-of-the-art models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "In recent years, time series analysis has attracted the attention of many researchers  [13, 14] . As a typical time series, physiological signals are used in many fields, such as sleep staging  [4, 11, 15, 17, 18] , motor imagery  [16, 24, 61] , seizure detection  [31, 32] , and emotion recognition  [12, 41, 57] , etc. The physiological signals have been applied in emotion recognition widely because they can accurately reflect real emotions. Musha et al. use EEG signals to recognize emotions for the first time  [37] . In the early stage of research, researchers adopted traditional machine learning models like SVM to extract features of EEG signals to recognize emotions  [28] . However, those methods are often limited by prior knowledge. Deep learning methods  [33, 41, 44, 45, 57, 59 ] are proposed to make up for the shortcomings of traditional machine learning methods. They have shown outstanding results in natural language processing  [1]  and computer vision  [42]  as well as in affective computing.\n\nMulti-domain Features Extraction. In building deep learning models for emotion recognition, researchers often extract spatialtemporal domain features or spatial-spectral domain features to train models. For instance, for spatial-temporal domain features extraction, Zhang et al. propose a spatial-temporal recurrent neural network combining the spatial RNN layer and the temporal RNN layer to capture the spatial and temporal features, respectively  [54] .  Yang     [60] . Further, Jia et al. present SST-EmotionNet, extracting spatial features, spectral features, and temporal features simultaneously, and achieving higher performance for emotion recognition. However, image-like maps introduce noise into the network  [12] .\n\nMulti-modal Physiological Signals. Most methods model either heterogeneity  [29, 36]  or correlation  [38]     [30] .\n\nOverall, existing emotion recognition methods have achieved high accuracy. However, they still ignore the fusion of multi-domain features, multi-modal heterogeneity, and correlation. To solve the limitations, we propose HetEmotionNet, which models the complementarity among spatial-spectral-temporal domain features, the heterogeneity, and the correlation of the signals simultaneously.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preliminaries",
      "text": "Definition 1. Heterogeneous Graph. We define ğº = (ğ‘‰ , ğ¸) as a graph, where ğ‘‰ denotes the node set and ğ¸ denotes the edge set. The network schema is defined as T ğº = (A, R), where A is the node type set and R is the edge type set. We define ğœ™ : ğ‘‰ â†’ A as the node type mapping and ğœ“ : ğ¸ â†’ R as the edge type mapping. Therefore, a network can be defined as ğº = (ğ‘‰ , ğ¸, ğœ™,ğœ“ ). For a heterogeneous network ğº = (ğ‘‰ , ğ¸, ğœ™,ğœ“ ), |A| + |R| > 2.\n\nDefinition 2. Heterogeneous Emotional Network. The heterogeneous emotional network    Each node corresponds to a channel, whose node type depends on the modality of channel. Graph structure represents the relationship among channels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Heterogeneous Graph Sequence Construction",
      "text": "For each sample, we construct a heterogeneous spatial-temporal graph sequence and a heterogeneous spatial-spectral graph sequence separately as shown in Figure  5 . These heterogeneous graph sequences are used to describe the spatial distribution of the temporal domain and spectral domain information of multi-modal signals.\n\nThe heterogeneous graph sequence is made up of several heterogeneous graphs. The general heterogeneous graph is usually defined as ğº = (ğ‘‹ ğ¹ , ğ´), where ğ‘‹ ğ¹ denotes the graph node features, ğ´ denotes the graph adjacency matrix which describes the relationship of channels. Besides, the node type depends on the modality of the channel.\n\nFigure  5  presents the process of constructing the heterogeneous graph sequence. Firstly, we build the relationship of the channels as 1\n\nâ—‹ in Figure  5 . Specifically, we calculate the correlation degree among different channels in the sample to determine the relationship of the channels using mutual information  [22] . Formally, given a channel pair (ğ‘¢, ğ‘£), their correlation ğ‘ ğ‘¢,ğ‘£ can be formulated as follows:\n\nwhere 1 â‰¤ ğ‘¢ â‰¤ ğ‘ , 1 â‰¤ ğ‘£ â‰¤ ğ‘ , ğ‘‹ ğ‘‡ ğ‘¢ represents the signals of channel ğ‘¢, ğ‘‹ ğ‘‡ ğ‘£ represents the signals of channel ğ‘£, ğ‘‡ is the number of timesteps, and ğ‘ is the number of channels. After calculating the correlation degree for all the channel pairs, we obtain the adjacency matrix ğ´ = (ğ‘ 1,1 , . . . , ğ‘ ğ‘¢,ğ‘£ , . . . , ğ‘ ğ‘ ,ğ‘ ) âˆˆ R ğ‘ Ã—ğ‘ of the spatialtemporal graphs and the spatial-spectral graphs.\n\nTo construct the heterogeneous spatial-temporal graph sequence, we compute the temporal node features ğ‘‹ ğ¹ ğ‘¡ = (ğ‘‹ ğ¹ 1 ğ‘¡ , ğ‘‹ ğ¹ 2 ğ‘¡ , . . . , ğ‘‹ ğ¹ ğ‘ ğ‘¡ ) âˆˆ R ğ‘ , where ğ‘‹ ğ¹ğ‘– ğ‘¡ denotes the amplitude (ğœ‡ğ‘‰ ) on the ğ‘– ğ‘¡â„ channel in the ğ‘¡ ğ‘¡â„ timestep from the multi-modal signals, 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ . Then, we combine the temporal domain feature vector ğ‘‹ ğ¹ ğ‘¡ and the adjacency matrix ğ´ to form a heterogeneous spatial-temporal graph ğº ğ‘¡ = (ğ‘‹ ğ¹ ğ‘¡ , ğ´) as 2 â—‹. Therefore, we obtain ğ‘‡ heterogeneous spatialtemporal graphs from the sample. These heterogeneous spatialtemporal graphs are stacked into a heterogeneous spatial-temporal graph sequence ğº ğ‘‡ = (ğº 1 , ğº 2 , . . . , ğº ğ‘‡ -1 , ğº ğ‘‡ ) as 3 â—‹. To construct the heterogeneous spatial-spectral graph sequence, similar to the process of generating heterogeneous spatial-temporal graph sequence, we extract the DE features from the ğµ âˆˆ ğ‘ + frequency bands in the sample as 4\n\nâ—‹. These features extracted from different channels and the correlation among channels are converted to heterogeneous spatial-spectral graphs. Specifically, these feature vectors extracted from ğ‘ channels in the ğ‘ ğ‘¡â„ frequency band ğ‘‹ ğ¹ ğ‘ = (ğ‘‹ ğ¹ 1 ğ‘ , ğ‘‹ ğ¹ 2 ğ‘ , . . . , ğ‘‹ ğ¹ ğ‘ ğ‘ ) âˆˆ R ğ‘ and the adjacency matrix ğ´ of the sample are composed of a heterogeneous spatial-spectral graph ğº ğ‘ = (ğ‘‹ ğ¹ ğ‘ , ğ´) as 5 â—‹. Therefore, we obtain ğµ heterogeneous spatial-spectral graphs in ğµ frequency bands for each sample. Then, these heterogeneous spatial-spectral graphs are stacked into a heterogeneous spatial-spectral sequence ğº ğµ = (ğº ğœƒ , ğº ğ›¼ , ğº ğ›½ , ğº ğ›¾ ) as 6 â—‹.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Heterogeneous Graph Recurrent Neural Network",
      "text": "HetEmotionNet consists of graph transformer network and graph recurrent neural network. Graph recurrent neural network is composed of graph convolutional network and gated recurrent unit. Graph Transformer Network. The Graph Transformer Network (GTN)  [52]  is used to model the heterogeneity of multi-modal signals by automatically extracting the meta-paths from the adjacency matrix set A. Specifically, A is generated from the heterogeneous adjacency matrix obtained in Section 4.1. The heterogeneous adjacency matrix is divided into homogeneous adjacency matrix set A = {A 1 , A 2 , . . . , A ğ‘ ğ‘ } according to edge types, where A ğ‘– is homogeneous adjacency matrix, 1 â‰¤ ğ‘– â‰¤ ğ‘ ğ‘ , and ğ‘ ğ‘ is the number of adjacency matrices which is decided by the number of edge types.\n\nGTN consists of several GT-layers. Each GT-layer aims to learn a two-level meta-path, which represents the second-order neighbor relationship in the graph by learning two graph structures from A. GT-layer has two steps: the first step is designed to softly construct two graph structures, which are defined as:\n\nwhere ğœ™ denotes the 1 Ã— 1 convolution, ğ‘Š ğœ™1 âˆˆ R 1Ã—1Ã—ğ¾ and ğ‘Š ğœ™2 âˆˆ R 1Ã—1Ã—ğ¾ are the parameters of ğœ™. ğ‘„ 1 and ğ‘„ 2 are the graph structures constructed by the 1 Ã— 1 convolution. The second step is designed to generate meta-paths by matrix multiplication of ğ‘„ 1 and ğ‘„ 2 . Formally, ğ» (ğ‘™) = ğ‘„ 1 ğ‘„ 2 , where ğ» (ğ‘™)  represents the meta-paths generated by ğ‘™ ğ‘¡â„ GT-layer. For calculation stability, ğ» (ğ‘™) is updated as ğ» (ğ‘™) = ğ· -1 ğ‘„ 1 ğ‘„ 2 , where ğ· denotes the degree matrices of A.\n\nStacking several GT-layers in GTN aims to learn a high-level meta-path that is a useful relationship of multi-modal signals. The ğ‘™ ğ‘¡â„ GT-layer in GTN updates the graph structure and multiplies the graph structure with the one constructed by (ğ‘™ -1) ğ‘¡â„ GT-layer. Formally, ğ» (ğ‘™) = ğ‘„ ğ‘™ ğ» (ğ‘™-1) , where ğ‘„ ğ‘™ denotes the graph structure learned by ğ‘™ ğ‘¡â„ GT-layer, ğ» (ğ‘™) and ğ» (ğ‘™-1) represent the meta-paths learned by (ğ‘™ -1) ğ‘¡â„ and ğ‘™ ğ‘¡â„ GT-layers, respectively. To consider multiple relationship of multi-modal signals, which are decided by the number of meta-paths simultaneously, multi-channel 1 Ã— 1 convolution is applied. The graph structure generated by multichannel 1 Ã— 1 convolution is defined as:\n\nwhere Î¦ is the multi-channel 1 Ã— 1 convolution, ğ‘Š Î¦ âˆˆ R 1Ã—ğ¶ ğ´ Ã—ğ¾ is learnable parameter, ğ¶ ğ´ is the number of channels, and ğ‘„ â€² ğ‘™ is the graph structure generated from ğ‘™ ğ‘¡â„ GT-layer by this convolution. To concurrently learn various lengths of meta-paths, the identity matrix is added into the result of each GT-layer. Finally, the graph sequence is updated into ğ¶ ğ´ sequences according to the meta-paths learned from GTN.\n\nGraph Recurrent Neural Network. Graph recurrent neural network has two chief components, namely graph convolutional network and gated recurrent unit. These two components are used to extract both spatial domain and temporal/spectral domain features from graph sequences. Specifically, GCN is used to capture the spatial domain features by aggregating information from neighbor nodes. In addition, GRU is applied to extract temporal/spectral domain features from the graph sequence obtained after GCN.\n\nGraph Convolutional Network. GCN captures the correlation among the nodes in the graph. The correlation is valuable to recognize different emotions. For example, there exists much functional connectivity among different brain regions. Capturing the spatial domain relationship among these functional connectivity contributes to recognizing different emotions.\n\nTo capture the correlation, the spectral graph theory extends the convolution operation from image-like data to graph structure data  [3] . The graph structure is represented by its Laplacian matrix in the spectral graph analysis. The Laplacian matrix and its eigenvalues reflect the property of the graph. The Laplacian matrix is defined as ğ¿ = ğ· -ğ´, where ğ¿ âˆˆ R ğ‘ Ã—ğ‘ is the Laplacian matrix, ğ´ âˆˆ R ğ‘ Ã—ğ‘ is the adjacency matrix, ğ· âˆˆ R ğ‘ Ã—ğ‘ is the degree matrix of the graph, and ğ‘ is the vertex number. The eigenvalues matrix is obtained by decomposing the Laplacian matrix with ğ¿ = ğ‘ˆ Î›ğ‘ˆ ğ‘‡ , where ğ‘ˆ is the Fourier basis and Î› âˆˆ R ğ‘ Ã—ğ‘ is a diagonal matrix. The node feature is defined as ğ‘¥ ğ‘– âˆˆ R ğ‘ for the graph ğº ğ‘– in graph sequence ğº ğ‘  . Then, the graph Fourier transform and inverse Fourier transform are defined as xğ‘– = ğ‘ˆ ğ‘‡ ğ‘¥ ğ‘– and ğ‘¥ ğ‘– = ğ‘ˆ xğ‘– , respectively. Because the graph convolution operation is equal to the product of these signals which have been transformed into the spectral domain  [3] , the graph convolution of signals ğ‘¥ ğ‘– on graph ğº ğ‘– is defined as:\n\nwhere ğ‘” ğœƒ denotes a graph convolution kernel and â˜… ğº ğ‘– denotes a graph convolution operation on graph ğº ğ‘– . In addition, to simplify the calculation of Laplace matrix, the ğ¾ order Chebyshev Polynomials are adopted, which is defined as:\n\nwhere Î› = 2 ğœ† ğ‘šğ‘ğ‘¥ Î› -ğ¼ ğ‘ , ğ¼ ğ‘ is the identity matrix, and ğœ† ğ‘šğ‘ğ‘¥ is the maximum eigenvalues of Î›. The ğœƒ âˆˆ ğ‘… ğ¾ is a vector of polynomial coefficients. The Chebyshev polynomials are defined as ğ‘‡ ğ‘˜ (ğ‘¥) = 2ğ‘¥ğ‘‡ ğ‘˜-1 (ğ‘¥) -ğ‘‡ ğ‘˜-2 (ğ‘¥) recursively, where ğ‘‡ 0 (ğ‘¥) = 1 and ğ‘‡ 1 (ğ‘¥) = ğ‘¥.\n\nWith the ğ¾ order Chebyshev Polynomials, the message from 0 to ğ¾ ğ‘¡â„ neighbours are aggregated into the center node. For the fast calculation  [20] , the first-order polynomials are used and ğœ† ğ‘šğ‘ğ‘¥ = 2, ğ‘˜ = 1 are set. Then, the graph convolution is further simplified as:\n\nwhere ğ· is the degree matrix, ğ´ is the adjacency matrix, and ğœƒ is the parameter. ğ¼ ğ‘ + ğ· -1 2 ğ´ğ· -1 2 is changed into D-1 2 Ãƒ D-1 2 to prevent numerical instability, gradient explosion and gradient disappearance, where D = ğ‘— Ãƒğ‘– ğ‘— and Ãƒ = ğ´ + ğ¼ ğ‘ . The graph convolution of signals ğ‘¥ ğ‘– on graph ğº ğ‘– in graph sequence is defined as:\n\nwhere ğ‘¥ ğ‘– and ğ» ğºğ¶ğ‘ ğ‘– are the node features before and after the graph convolution, respectively. ğ‘Š is the weight vector, ğœ is the Rectified Linear Unit (ReLU) activation function, and ğ‘ is a bias.\n\nThe graph convolution operation is applied to the graph sequence. Formally, for graph sequence ğº ğ‘  = {ğº 1 , ğº 2 , . . . , ğº ğ‘›ğ‘  } âˆˆ R ğ‘ Ã—ğ‘›ğ‘  and each graph ğº ğ‘– in ğº ğ‘  , the graph convolution operation is applied on ğº ğ‘– . After the graph convolution operation is performed on all graphs in ğº ğ‘  , the results are stacked together to construct a new graph sequence ğ» ğ‘  = {ğ» 1 , ğ» 2 , . . . , ğ» ğ‘›ğ‘  } âˆˆ R ğ‘ Ã—ğ‘›ğ‘  . To decrease the number of parameters, all graph convolution operations share the same parameters.\n\nSince the meta-paths learned by GTN update the graph sequence obtained in Section 4.2 into ğ¶ ğ´ graph sequences, the graph convolution operation is performed on each of the graph sequences. Then, the weighted sum operation is adopted to concatenate the graph sequences together. The weighted sum operation on graph sequences ğº ğºğ‘‡ ğ‘ ğ‘  is defined as:\n\nwhere ğ›¿ ğ‘  denotes the graph convolution operation on the graph sequence, ğ‘¤ ğ‘– is the weight for the weighted sum operation, and\n\nis the ğ‘– ğ‘¡â„ graph sequence in ğº ğºğ‘‡ ğ‘ ğ‘  . Gated Recurrent Unit. After the graph convolutional network, GRU is applied to capture the dependency among different timesteps or frequency bands. Gated recurrent unit consists of the reset gate and the update gate  [5] , which is defined as:\n\nwhere the parameters and the operator are defined as follows, âŠ™ : the product of two matrices on each element.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "ğ» ğºğ¶ğ‘ ğ‘–",
      "text": ": the ğ‘– ğ‘¡â„ graph of the graph sequence, which is the input of the ğ‘– ğ‘¡â„ GRU unit.\n\nğ» ğºğ‘…ğ‘ˆ ğ‘–-1 : the output of previous GRU unit, which is the input of the ğ‘– ğ‘¡â„ GRU unit. It contains the information of the previous graphs.\n\nğ‘… ğ‘– : the output of the reset gate from ğ» ğºğ¶ğ‘ ğ‘– and ğ» ğºğ‘…ğ‘ˆ ğ‘–-1 . Hğºğ¶ğ‘ ğ‘– : the output of tanh activation controlled by the reset gate ğ‘… ğ‘– , which determines the forgotten degree of the previous state ğ» ğºğ‘…ğ‘ˆ ğ‘–-1 .\n\nğ‘ ğ‘– : the output of the update gate, which determines how many elements of ğ» ğºğ‘…ğ‘ˆ ğ‘–-1 and its state are updated in the ğ‘– ğ‘¡â„ GRU unit. ğ» ğºğ‘…ğ‘ˆ ğ‘– : the output of the ğ‘– ğ‘¡â„ GRU unit. ğ‘Š ğ‘Ÿ , ğ‘‰ ğ‘… , ğ‘Š â„ , ğ‘‰ â„ , ğ‘Š ğ‘§ , and ğ‘‰ ğ‘§ are the learnable parameters of the GRU unit.\n\nEach graph ğ» ğºğ¶ğ‘ ğ‘ ğ‘¡ in ğ» ğºğ¶ğ‘ ğ‘  is fed into the ğ‘¡ ğ‘¡â„ GRU unit to capture the features among the graphs. ğ» ğºğ‘…ğ‘ˆ ğ‘  is obtained by concatenating all outputs of the GRU units together.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fusion And Classification",
      "text": "The heterogeneous graph recurrent neural network is used to extract the spatial-temporal domain and spatial-spectral domain features of multi-modal data in the two streams, respectively. After extraction, the features obtained from the spatial-temporal and the spatial-spectral streams are fused, which are defined as:\n\nwhere âˆ¥ denotes the concatenate operation, ğ» ğºğ‘…ğ‘ˆ 1 and ğ» ğºğ‘…ğ‘ˆ 2 denote the the features extracted by spatial-temporal and spatialspectral stream, and ğ‘Œ denotes the classification result of our model. ğ‘Š 1 , ğ‘ 1 , ğ‘Š 2 , ğ‘ 2 , ğ‘Š 3 , ğ‘ 3 , ğ‘Š 4 , and ğ‘ 4 denote the learnable parameters of the classification layer. In this paper, the cross entropy  [7]  is applied as loss function.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 5.1 Datasets",
      "text": "We conduct experiments on two datasets that contain the multimodal physiological signals: DEAP  [21]  and the MAHNOB-HCI  [43]  datasets, which evoke emotion by multimedia materials.\n\nDEAP dataset records the data generated by 32 participants under multimedia stimulation. Each participant needs to undergo 40 trials and watches a 1-minute music video during each trial. Their physiological signals are recorded while they are watching music videos, and data of each trial includes 3s pre-trial signals and 60s trial signals. The dataset contains 32-channel EEG signals and 8-channel Peripheral Physiological Signals (PPS). The peripheral physiological signals include EOG, EMG, GSR, BVP, respiration, and temperature. The music videos are rated on valence, arousal, and other emotional dimensions from 1 to 9 by all participants.\n\nMAHNOB-HCI dataset records the data generated by 27 participants under multimedia stimulation. Each participant watches 20 video clips, while physiological data of 20 trials is recorded. The length of these video clips is between 34.9s and 117s (mean is 81.4s, standard deviation is 22.5s). The dataset contains 32-channel EEG signals and 6-channel PPS. The peripheral physiological signals include ECG, GSR, respiration, and temperature. Participants are required to score each video clip from 1 to 9 on the emotional dimensions of valence, arousal, etc.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models And Settings",
      "text": "The baseline models include MLP  [39] , SVM  [2] , GCN  [20] , DGCNN  [44] , MM-ResLSTM  [34] , ACRNN  [45] , and SST-EmotionNet  [12] . In order to make a fair comparison with the baseline methods, we perform the same data processing experimental settings for all methods. Specifically, for the DEAP dataset, trials are applied the baseline reduction according to  [50] . Then, a 1s non-overlapping window  [46]  is employed to divide each trial into several samples. For each sample, the Fourier transform with Hanning is utilized to extract the DE features  [8, 58] . We compute DE features in four frequency bands for each channel. Due to the different attributes of different modal signals, we have adopted different frequency band division strategies for different modal signals. The specific division strategy is detailed in Table  S .2 in the Supplementary Material. For the MAHNOB-HCI dataset, similar to the processing of the DEAP dataset, we employ the baseline reduction to each trial and extract the DE features in four frequency bands.\n\nOur model is implemented with Pytorch framework and trained on NVIDIA 2080 Ti. The code has been released in GitHub 1  . The threshold is set as 5 to divide valence and arousal into two categories, respectively  [21, 45] . Valence measures the pleasantness of emotion and arousal measures the intensity of emotion. We evaluate all methods using 10-fold cross-validation  [9] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results Analysis And Comparison",
      "text": "We compare our model with other baseline models on the DEAP dataset and the MAHNOB-HCI dataset, respectively.\n\nTable  1  presents the average accuracy and standard deviation of these models for emotion recognition on the DEAP dataset. The",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "To verify the effectiveness of our model, ablation experiments are performed on the DEAP dataset. There are three kinds of ablation experiments: 1) Ablation experiments on the effectiveness of graph transformer network, graph convolutional network, and gated recurrent unit. 2) Ablation experiments on the effectiveness of fusing different modalities. 3) Ablation experiments on two-stream structure to verify whether utilizing spatial-spectral-temporal domain features simultaneously is useful.\n\nAblation on different components. To verify the effectiveness of different components in our model, we design three variants, which are described as follows:\n\nâ€¢ HetEmotionNet-noGRU: To verify the effects of GRU on capturing temporal or spectral dependency, this variant removes GRU from HetEmotionNet.\n\nâ€¢ HetEmotionNet-noGTN: To verify the effectiveness of GTN on modeling the heterogeneity of multi-modal data, this variant removes GTN from HetEmotionNet.\n\nâ€¢ HetEmotionNet-noGCN: To verify the effectiveness of GCN on capturing the correlation, we remove GCN from HetEmotionNet.\n\nFigure  6  presents that removing different components from Het-EmotionNet reduces the performance. HetEmotionNet outperforms HetEmotionNet-noGTN, which indicates that GTN is effective to model the heterogeneity of multi-modal data. HetEmotionNet has a better performance than HetEmotionNet-noGCN, demonstrating that GCN is effective to capture the correlation among channels. By comparing HetEmotionNet and HetEmotionNet-noGRU, it presents that HetEmotionNet-noGRU performs worse than HetEmotionNet, which indicates that GRU is important to capture the temporal or spectral dependency and improves the performance of model.\n\nAblation on fusing data of different modalities. To verify the effects of using multi-modal data and modeling the heterogeneity of multi-modal data, we conduct ablation experiments and design three variants of HetEmotionNet:\n\nâ€¢ HetEmotionNet-noEEG: It removes EEG signals from the multimodal data to verify the effects of only using PPS on the model's performance. In addition, we remove GTN from this variant because GTN is used to model the heterogeneity of multi-modal data while this variant only uses PPS.\n\nâ€¢ HetEmotionNet-noPPS: It removes PPS from the multi-modal data to verify the influences of only using EEG signals on the model's performance. We remove the GTN for the same reason of HetEmotionNet-noEEG. However, it removes GTN to verify that modeling the heterogeneity of multi-modal data is important.\n\nAs Figure  7  illustrates, HetEmotionNet-noPPS performs better than HetEmotionNet-noEEG. This result indicates that the effects of only using EEG signals is better than only using PPS, because EEG signals are the main physiological signals used in emotion recognition  [53] . Besides, HetEmotionNet-noGTN outperforms HetEmotionNet-noPPS and HetEmotionNet-noEEG, which demonstrates that fusing data of different modalities further improves the performance. However, HetEmotionNet-noGTN does not model the heterogeneity of multi-modal data. Therefore, HetE-motionNet is designed to model the heterogeneity of multi-modal data and reaches the best results.\n\nAblation on two-stream structure. To verify the effectiveness of integrating two-stream, we design two variants as follows:\n\nâ€¢ HetEmotionNet-noST (Spatial-Temporal stream): This variant removes the spatial-temporal stream to verify the effects of utilizing the spatial-temporal domain features of physiological signals.\n\nâ€¢ HetEmotionNet-noSS (Spatial-Spectral stream): This variant removes the spatial-spectral stream to verify the effects of using the spatial-spectral domain features of physiological signals.\n\nAs Figure  8  illustrates, HetEmotionNet-noSS performs better than HetEmotionNet-noST, which demonstrates that extracting features in spatial and temporal domain is more effective than extracting features in spatial and spectral domain. Besides, HetEmotionNet has a better performance than two variants, which manifests that our model is able to fuse the spatial-spectral-temporal domain features simultaneously and improve the classification accuracy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel two-stream heterogeneous graph recurrent neural network using multi-modal physiological signals for emotion recognition. The proposed HetEmotionNet is based on the two-stream structure, which can extract spatial-spectraltemporal domain features from multi-modal signals simultaneously. Moreover, each stream consists of GTN for modeling the heterogeneity, GCN for modeling the correlation, and GRU for capturing the temporal domain or spectral domain dependency. Experiments on the DEAP and the MAHNOB-HCI datasets indicate that our proposed model achieves state-of-the-art performance on DEAP and MAHNOB-HCI. Besides, the proposed model is a general framework for multi-modal physiological signals.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The complementarity among spatial-spectral-",
      "page": 2
    },
    {
      "caption": "Figure 2: Two different EEG representations. (a) The 2D map",
      "page": 2
    },
    {
      "caption": "Figure 1: indicates the differences between the",
      "page": 2
    },
    {
      "caption": "Figure 2: (a). Besides, in Figure 2(b), the graph-based method can reflect the",
      "page": 2
    },
    {
      "caption": "Figure 3: The heterogeneity of the multi-modal physiologi-",
      "page": 2
    },
    {
      "caption": "Figure 3: The correlation includes intra-modality correlation and",
      "page": 2
    },
    {
      "caption": "Figure 2: (b). The cross-modality",
      "page": 2
    },
    {
      "caption": "Figure 4: The whole schematic process for multi-modal emotion recognition. We obtain the multi-modal signals from the",
      "page": 4
    },
    {
      "caption": "Figure 4: illustrates the overall structure of our model. We propose a",
      "page": 4
    },
    {
      "caption": "Figure 5: The schematic process of heterogeneous graph se-",
      "page": 4
    },
    {
      "caption": "Figure 5: These heterogeneous graph",
      "page": 4
    },
    {
      "caption": "Figure 5: presents the process of constructing the heterogeneous",
      "page": 5
    },
    {
      "caption": "Figure 5: Specifically, we calculate the correlation degree",
      "page": 5
    },
    {
      "caption": "Figure 6: Ablation studies on different components.",
      "page": 8
    },
    {
      "caption": "Figure 7: Ablation studies on fusing different modalities.",
      "page": 8
    },
    {
      "caption": "Figure 6: presents that removing different components from Het-",
      "page": 8
    },
    {
      "caption": "Figure 8: Ablation studies on two-stream structure.",
      "page": 8
    },
    {
      "caption": "Figure 7: illustrates, HetEmotionNet-noPPS performs bet-",
      "page": 8
    },
    {
      "caption": "Figure 8: illustrates, HetEmotionNet-noSS performs better",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": ""
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Ziyu Jia"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "ziyujia@bjtu.edu.cn"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "School of Computer and Information"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Technology, Beijing Jiaotong"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "University & Beijing Key Laboratory"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "of Traffic Data Analysis and Mining"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Beijing, China"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": ""
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": ""
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Zhiyang Feng"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "zhiyangfeng@bjtu.edu.cn"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "School of Computer and Information"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Technology, Beijing Jiaotong"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "University"
        },
        {
          "HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent": "Beijing, China"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University\nUniversity": "Beijing, China\nBeijing, China",
          "University": "Beijing, China"
        },
        {
          "University\nUniversity": "ABSTRACT",
          "University": "CCS CONCEPTS"
        },
        {
          "University\nUniversity": "The research on human emotion under multimedia stimulation",
          "University": "â€¢ Computing methodologies â†’ Neural networks; â€¢ Information"
        },
        {
          "University\nUniversity": "based on physiological signals is an emerging field, and important",
          "University": "systems â†’ Multimedia information systems; â€¢ Human-centered"
        },
        {
          "University\nUniversity": "progress has been achieved for emotion recognition based on multi-",
          "University": "computing â†’ HCI design and evaluation methods."
        },
        {
          "University\nUniversity": "modal signals. However, it is challenging to make full use of the",
          "University": ""
        },
        {
          "University\nUniversity": "complementarity among spatial-spectral-temporal domain features",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "KEYWORDS"
        },
        {
          "University\nUniversity": "for emotion recognition, as well as model the heterogeneity and",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "Multi-modal emotion recognition; Graph recurrent neural network;"
        },
        {
          "University\nUniversity": "correlation among multi-modal signals. In this paper, we propose a",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "Affective computing; Heterogeneous graph"
        },
        {
          "University\nUniversity": "novel two-stream heterogeneous graph recurrent neural network,",
          "University": ""
        },
        {
          "University\nUniversity": "named HetEmotionNet, fusing multi-modal physiological signals",
          "University": "ACM Reference Format:"
        },
        {
          "University\nUniversity": "for emotion recognition. Specifically, HetEmotionNet consists of",
          "University": "Ziyu Jia, Youfang Lin, Jing Wang, Zhiyang Feng, Xiangheng Xie, and Caijie"
        },
        {
          "University\nUniversity": "the spatial-temporal stream and the spatial-spectral stream, which",
          "University": "Chen. 2021. HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent"
        },
        {
          "University\nUniversity": "",
          "University": "Neural Network for Multi-modal Emotion Recognition. In Proceedings of"
        },
        {
          "University\nUniversity": "can fuse spatial-spectral-temporal domain features in a unified",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "the 29th ACM International Conference on Multimedia (MM â€™21), October"
        },
        {
          "University\nUniversity": "framework. Each stream is composed of\nthe graph transformer",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA, 11 pages."
        },
        {
          "University\nUniversity": "network for modeling the heterogeneity, the graph convolutional",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "https://doi.org/10.1145/3474085.3475583"
        },
        {
          "University\nUniversity": "network for modeling the correlation, and the gated recurrent unit",
          "University": ""
        },
        {
          "University\nUniversity": "for capturing the temporal domain or spectral domain dependency.",
          "University": ""
        },
        {
          "University\nUniversity": "Extensive experiments on two real-world datasets demonstrate that",
          "University": ""
        },
        {
          "University\nUniversity": "",
          "University": "1\nINTRODUCTION"
        },
        {
          "University\nUniversity": "our proposed model achieves better performance than state-of-the-",
          "University": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "High": "Active\nActive"
        },
        {
          "High": "Arousal\nArousal"
        },
        {
          "High": ""
        },
        {
          "High": "â€¦\nâ€¦"
        },
        {
          "High": "Valence\nNegative\nPositive\nValence\nPositive\nNegative"
        },
        {
          "High": ""
        },
        {
          "High": ""
        },
        {
          "High": "t"
        },
        {
          "High": "â€¦\nâ€¦\nÎ¸Î±\nt-1"
        },
        {
          "High": "Î²\nÎ³\nPassive\nPassive\n2\n1"
        },
        {
          "High": "Spatial-Temporal Domain \nSpatial-Spectral Domain"
        },
        {
          "High": "Low"
        },
        {
          "High": "Figure\n1:\nThe\ncomplementarity\namong\nspatial-spectral-"
        },
        {
          "High": "temporal domain features. The brain topographic maps indi-"
        },
        {
          "High": "cate that there are differences of activation degree between"
        },
        {
          "High": "temporal domain information and spectral domain infor-"
        },
        {
          "High": "mation on the spatial domain for different emotional states"
        },
        {
          "High": "which are categorized by valence and arousal. These differ-"
        },
        {
          "High": "ences in spatial-spectral-temporal domain information are"
        },
        {
          "High": "complementary for emotion recognition."
        },
        {
          "High": ""
        },
        {
          "High": "FP2\nFP1\nFPZ"
        },
        {
          "High": "FP2\nFP1"
        },
        {
          "High": "AF4\nAF3"
        },
        {
          "High": "AF3\nAF4"
        },
        {
          "High": "F8\nF7\nF5\nF3\nF1\nFZ\nF2\nF4\nF6\nF8\nF7"
        },
        {
          "High": "F4\nF3"
        },
        {
          "High": "FZ\nFT7\nFC5\nFC3\nFC1\nFCZ\nFC2\nFC4\nFC6\nFT8\nFC5\nFC6"
        },
        {
          "High": "FC1\nFC2"
        },
        {
          "High": "T7\nC5\nC3\nC1\nCZ\nC2\nC4\nC6\nT8\nC4\nC3\nCZ"
        },
        {
          "High": "T8\nT7"
        },
        {
          "High": "CP5\nCP6\nTP7\nCP5\nCP3\nCP1 CPZ\nCP2\nCP4\nCP6\nTP8"
        },
        {
          "High": "CP1\nCP2"
        },
        {
          "High": "P3\nP8\nP7\nPZ P4"
        },
        {
          "High": "P7\nP5\nP3\nP1\nPZ\nP2\nP4\nP6\nP8"
        },
        {
          "High": ""
        },
        {
          "High": "PO3\nPO4\nPO7 PO5 PO3 POZ\nPO4 PO6 PO8"
        },
        {
          "High": "O1\nO2\nCB1\nO1\nOZ\nO2\nCB2"
        },
        {
          "High": "OZ"
        },
        {
          "High": "(a)\n(b)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features of all modalities are extracted by the same feature extractor.": "Therefore, how to model the heterogeneity and correlation among",
          "et al. propose a 3D EEG representation, which combines features": "from different\nfrequency bands while retaining spatial\ninforma-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "multi-modal signals simultaneously becomes a challenge.",
          "et al. propose a 3D EEG representation, which combines features": "tion among channels [49]. Zheng et al. utilize Short-Time Fourier"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "To address these challenges, we propose a two-stream hetero-",
          "et al. propose a 3D EEG representation, which combines features": "Transform with a non-overlapped Hanning window to extract fea-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "geneous graph recurrent neural network named HetEmotionNet,",
          "et al. propose a 3D EEG representation, which combines features": "tures from multi-channel EEG signals and calculate DE features for"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "which is composed of\nthe spatial-temporal stream and spatial-",
          "et al. propose a 3D EEG representation, which combines features": "each frequency band to predict the positive and negative emotional"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "spectral stream and takes heterogeneous graph sequences as input.",
          "et al. propose a 3D EEG representation, which combines features": "states [60]. Further, Jia et al. present SST-EmotionNet, extracting"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "A stream consists of a graph transformer network (GTN), a graph",
          "et al. propose a 3D EEG representation, which combines features": "spatial features, spectral features, and temporal features simultane-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "convolutional network (GCN), and a gated recurrent unit (GRU).",
          "et al. propose a 3D EEG representation, which combines features": "ously, and achieving higher performance for emotion recognition."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "The physiological signals are constructed into heterogeneous graph",
          "et al. propose a 3D EEG representation, which combines features": "However, image-like maps introduce noise into the network [12]."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "sequences as the input of our model. Several heterogeneous graphs",
          "et al. propose a 3D EEG representation, which combines features": "Multi-modal Physiological Signals. Most methods model ei-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "are stacked to form a heterogeneous graph sequence. A hetero-",
          "et al. propose a 3D EEG representation, which combines features": "ther heterogeneity [29, 36] or correlation [38] to fuse multi-modal"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "geneous graph consists of nodes and edges. A node represents a",
          "et al. propose a 3D EEG representation, which combines features": "signals. For instance, for the heterogeneity of multi-modal data,"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "channel of signals whose type depends on the modality of the chan-",
          "et al. propose a 3D EEG representation, which combines features": "Lin et al. extract the features of EEG signals and Peripheral Physio-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "nel. An edge represents the connection between two nodes. To sum",
          "et al. propose a 3D EEG representation, which combines features": "logical Signals (PPS) separately, and then concatenate these features"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "up, the main contributions of our work are summarized as follows:",
          "et al. propose a 3D EEG representation, which combines features": "to model\nthe heterogeneity of multi-modal signals for emotion"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "â€¢ We propose a novel graph-based two-stream structure com-",
          "et al. propose a 3D EEG representation, which combines features": "recognition [27]. Ma et al. design a multi-modal residual LSTM"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "posed of the spatial-temporal stream and the spatial-spectral stream",
          "et al. propose a 3D EEG representation, which combines features": "network (MMResLSTM) for emotion recognition. The signals of"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "which can simultaneously fuse spatial-spectral-temporal domain",
          "et al. propose a 3D EEG representation, which combines features": "different modalities are fed into different LSTM branches to extract"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "features of physiological signals in a unified deep neural network",
          "et al. propose a 3D EEG representation, which combines features": "multi-modal features [34]. For the correlation of multi-modal"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "framework.",
          "et al. propose a 3D EEG representation, which combines features": "data, Zhang et al. adopt GSCCA to learn the correlation by extract-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "â€¢ Each stream consists of a GTN for modeling the heterogeneity,",
          "et al. propose a 3D EEG representation, which combines features": "ing the group structure information between EEG signals and eye"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "a GCN for modeling the correlation, and a GRU for capturing the",
          "et al. propose a 3D EEG representation, which combines features": "movement features [55]. Liu et al. fuse features of EEG signals and"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "temporal or spectral dependency.",
          "et al. propose a 3D EEG representation, which combines features": "features of other modalities with bimodal deep autoencoders and"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "â€¢ Extensive experiments are conducted on two benchmark datasets",
          "et al. propose a 3D EEG representation, which combines features": "then obtain the share representation [30]."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "to evaluate the performance of the proposed model. The results",
          "et al. propose a 3D EEG representation, which combines features": "Overall, existing emotion recognition methods have achieved"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "indicate the proposed model outperforms all the state-of-the-art",
          "et al. propose a 3D EEG representation, which combines features": "high accuracy. However, they still ignore the fusion of multi-domain"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "models.",
          "et al. propose a 3D EEG representation, which combines features": "features, multi-modal heterogeneity, and correlation. To solve the"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "limitations, we propose HetEmotionNet, which models the comple-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "mentarity among spatial-spectral-temporal domain features, the"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "2\nRELATED WORK",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "heterogeneity, and the correlation of the signals simultaneously."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "In recent years, time series analysis has attracted the attention of",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "many researchers [13, 14]. As a typical time series, physiological sig-",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "3\nPRELIMINARIES"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "nals are used in many fields, such as sleep staging [4, 11, 15, 17, 18],",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "Definition 1. Heterogeneous Graph. We define ğº = (ğ‘‰ , ğ¸) as"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "motor imagery [16, 24, 61], seizure detection [31, 32], and emotion",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "a graph, where ğ‘‰ denotes the node set and ğ¸ denotes the edge set."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "recognition [12, 41, 57], etc. The physiological signals have been",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "The network schema is defined as Tğº = (A, R), where A is the"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "applied in emotion recognition widely because they can accurately",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "node type set and R is the edge type set. We define ğœ™ : ğ‘‰ â†’ A as"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "reflect real emotions. Musha et al. use EEG signals to recognize",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "the node type mapping and ğœ“ : ğ¸ â†’ R as the edge type mapping."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "emotions for the first time [37]. In the early stage of research, re-",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "Therefore, a network can be defined as ğº =\n(ğ‘‰ , ğ¸, ğœ™,ğœ“ ). For a"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "searchers adopted traditional machine learning models like SVM",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "heterogeneous network ğº = (ğ‘‰ , ğ¸, ğœ™,ğœ“ ),\n|A| + |R| > 2."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "to extract features of EEG signals to recognize emotions [28]. How-",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "Definition 2. Heterogeneous Emotional Network. The het-"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "ever, those methods are often limited by prior knowledge. Deep",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "erogeneous emotional network ğºğ¸ğ‘… = (ğ‘‰ â€², ğ¸â€², ğœ™ â€²,ğœ“ â€²\n) is constructed"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "learning methods [33, 41, 44, 45, 57, 59] are proposed to make up for",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "based on multi-modal signals. We define ğ‘‰ â€²\n= (ğ‘£1, ğ‘£2, . . . , ğ‘£ğ‘ ) as"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "the shortcomings of traditional machine learning methods. They",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "the node set, where the node ğ‘£ğ‘– denotes a channel of physiological"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "have shown outstanding results in natural language processing [1]",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "",
          "et al. propose a 3D EEG representation, which combines features": "signals. The edge represents a connection relationship between two"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "and computer vision [42] as well as in affective computing.",
          "et al. propose a 3D EEG representation, which combines features": ""
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "Multi-domain Features Extraction. In building deep learning",
          "et al. propose a 3D EEG representation, which combines features": "different channels. ğ¸â€²\n= (ğ‘’1, ğ‘’2, . . . , ğ‘’ğ‘›) is defined as the edge set,"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "models for emotion recognition, researchers often extract spatial-",
          "et al. propose a 3D EEG representation, which combines features": "where ğ‘› is the number of edges. The number of node types is equal"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "temporal domain features or spatial-spectral domain features to",
          "et al. propose a 3D EEG representation, which combines features": "to the number of modalities. When the number of modalities is ğ´â€²\n,"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "train models. For\ninstance,\nfor spatial-temporal domain fea-",
          "et al. propose a 3D EEG representation, which combines features": "(ğ´â€²\n= ğ´â€²\nthe number of edge types is ğ‘…â€²\n+ 1)/2. After the node type"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "tures extraction, Zhang et al. propose a spatial-temporal recurrent",
          "et al. propose a 3D EEG representation, which combines features": "â†’ ğ´â€²\nâ†’ ğ‘…â€²\nmapping ğœ™ â€²\n: ğ‘‰ â€²\nand the edge type mapping ğœ“ â€²\n: ğ¸â€²"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "neural network combining the spatial RNN layer and the temporal",
          "et al. propose a 3D EEG representation, which combines features": "are defined, we construct the heterogeneous emotional network"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "RNN layer to capture the spatial and temporal\nfeatures, respec-",
          "et al. propose a 3D EEG representation, which combines features": "ğºğ¸ğ‘… = (ğ‘‰ â€², ğ¸â€², ğœ™ â€²,ğœ“ â€²\n).\nIn addition,\nthe heterogeneous emotional"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "tively [54]. Yang et al. propose a parallel convolutional recurrent",
          "et al. propose a 3D EEG representation, which combines features": "network can be defined as ğºğ¸ğ‘… = (ğ‘‹ ğ¹ , ğ´), where ğ‘‹ ğ¹ denotes the"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "neural network, which utilizes CNN to extract spatial features from",
          "et al. propose a 3D EEG representation, which combines features": "features of each node and ğ´ denotes the adjacency matrix of ğºğ¸ğ‘…."
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "data frames and RNN to extract temporal features from EEG sig-",
          "et al. propose a 3D EEG representation, which combines features": "Definition 3. Heterogeneous Graph Sequence. We define"
        },
        {
          "features of all modalities are extracted by the same feature extractor.": "nals [50]. For spatial-spectral domain features extraction, Yang",
          "et al. propose a 3D EEG representation, which combines features": "ğºğ‘  = (ğº1, ğº2, . . . , ğºğ‘›) as a heterogeneous graph sequence, where ğ‘›"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": "ECG"
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": "ECG"
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": "ECG"
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        },
        {
          "ECG": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EOG": "GSR"
        },
        {
          "EOG": "SAD"
        },
        {
          "EOG": "Multimedia Stimulation\nGÎ¸"
        },
        {
          "EOG": "Figure 4: The whole schematic process for multi-modal emotion recognition. We obtain the multi-modal signals from the"
        },
        {
          "EOG": "subjects and construct the graph sequence representation for the HetEmotionNet. HetEmotionNet consists of spatial-temporal"
        },
        {
          "EOG": "stream and spatial-spectral stream. The two streams have the same structure. Each stream is composed of graph transformer"
        },
        {
          "EOG": "network (GTN), graph convolutional network (GCN), and gated recurrent unit (GRU)."
        },
        {
          "EOG": ""
        },
        {
          "EOG": "is the number of heterogeneous graphs. For instance, we construct"
        },
        {
          "EOG": "âˆˆ\n=\nthe spatial-temporal graph sequence ğºğ‘‡\n(ğº1, ğº2, . . . , ğºğ‘‡ )"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "Rğ‘ Ã—ğ‘‡ , where ğ‘‡ denotes the number of the timesteps in the sam-"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "ple. Analogously, we construct the spatial-spectral graph sequence"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "âˆˆ Rğ‘ Ã—ğµ, where ğµ is the number of\nfre-\n= (ğº1, ğº2, . . . , ğºğµ)"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "quency bands."
        },
        {
          "EOG": ""
        },
        {
          "EOG": ""
        },
        {
          "EOG": "Problem Statement. Our\nresearch goal\nis to learn a mapping"
        },
        {
          "EOG": "function between graph sequences and emotional states. Given"
        },
        {
          "EOG": "spatial-temporal graph sequence ğºğ‘‡ and spatial-spectral graph se-"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "quence ğºğµ,\nthe emotion recognition problem can be defined as"
        },
        {
          "EOG": "ğ‘Œ = ğ¹ (Gğ‘‡ , Gğµ), where ğ‘Œ denotes the emotion classification and ğ¹"
        },
        {
          "EOG": "denotes the mapping function."
        },
        {
          "EOG": ""
        },
        {
          "EOG": ""
        },
        {
          "EOG": ""
        },
        {
          "EOG": "4\nMETHODOLOGY"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "Figure 4 illustrates the overall structure of our model. We propose a"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "HetEmotionNet based on the constructed graph sequence from the"
        },
        {
          "EOG": "multi-modal signals. HetEmotionNet consists of a spatial-temporal"
        },
        {
          "EOG": "stream and a spatial-spectral stream, and these two streams have"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "the similar structure. Each stream of HetEmotionNet is composed"
        },
        {
          "EOG": ""
        },
        {
          "EOG": "of graph transformer network, graph convolutional network, and"
        },
        {
          "EOG": "gated recurrent unit. We summarize five core ideas of HetEmo-"
        },
        {
          "EOG": "tionNet: 1) Design a heterogeneous graph-based spatial-spectral-"
        },
        {
          "EOG": "temporal representation for multi-modal emotion recognition. 2) In-"
        },
        {
          "EOG": "tegrate the graph-based spatial-temporal stream and spatial-spectral"
        },
        {
          "EOG": "stream in a unified network structure to extract spatial-spectral-"
        },
        {
          "EOG": "temporal domain features simultaneously. 3) Adopt graph trans-"
        },
        {
          "EOG": "former network to model the heterogeneity of multi-modal signals."
        },
        {
          "EOG": "4) Apply graph convolutional network to capture the correlation"
        },
        {
          "EOG": "among different channels. 5) Employ the gated recurrent unit to cap-"
        },
        {
          "EOG": "ture temporal domain or spectral domain dependency, respectively."
        },
        {
          "EOG": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 5 presents the process of constructing the heterogeneous": "graph sequence. Firstly, we build the relationship of the channels",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "a two-level meta-path, which represents the second-order neighbor"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "as\nâ—‹ in Figure 5. Specifically, we calculate the correlation degree",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "relationship in the graph by learning two graph structures from A."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "among different channels in the sample to determine the relation-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "GT-layer has two steps: the first step is designed to softly construct"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ship of the channels using mutual information [22]. Formally, given",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "two graph structures, which are defined as:"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "a channel pair (ğ‘¢, ğ‘£), their correlation ğ‘ğ‘¢,ğ‘£ can be formulated as",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "(2)\nğ‘„1 = ğœ™ (A, ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘Šğœ™1)),"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "follows:",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "(3)\nğ‘„2 = ğœ™ (A, ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘Šğœ™2)),"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘ (ğ‘š, ğ‘›)\nâˆ‘ï¸\nâˆ‘ï¸",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘ğ‘¢,ğ‘£ = ğ¼ (ğ‘‹ğ‘‡",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": ",\nğ‘ (ğ‘š, ğ‘›) log\nğ‘¢ ; ğ‘‹ğ‘‡",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "(1)\nğ‘£ ) =",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "where ğœ™ denotes the 1 Ã— 1 convolution, ğ‘Šğœ™1 âˆˆ R1Ã—1Ã—ğ¾ and ğ‘Šğœ™2 âˆˆ"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘ (ğ‘š)ğ‘ (ğ‘›)",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘š âˆˆğ‘‹ğ‘‡\nğ‘› âˆˆğ‘‹ğ‘‡",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "R1Ã—1Ã—ğ¾ are the parameters of ğœ™. ğ‘„1 and ğ‘„2 are the graph structures"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "constructed by the 1 Ã— 1 convolution."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "where 1 â‰¤ ğ‘¢ â‰¤ ğ‘ , 1 â‰¤ ğ‘£ â‰¤ ğ‘ , ğ‘‹ğ‘‡\nrepresents the signals of chan-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "The second step is designed to generate meta-paths by matrix"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "nel ğ‘¢, ğ‘‹ğ‘‡\nrepresents the signals of channel ğ‘£, ğ‘‡ is the number of",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "multiplication of ğ‘„1 and ğ‘„2. Formally, ğ» (ğ‘™) = ğ‘„1ğ‘„2, where ğ» (ğ‘™)"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "timesteps, and ğ‘ is the number of channels. After calculating the",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "represents the meta-paths generated by ğ‘™ğ‘¡â„ GT-layer. For calculation"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "correlation degree for all\nthe channel pairs, we obtain the adja-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "stability, ğ» (ğ‘™)\nis updated as ğ» (ğ‘™) = ğ·âˆ’1ğ‘„1ğ‘„2, where ğ· denotes the"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "cency matrix ğ´ = (ğ‘1,1, . . . , ğ‘ğ‘¢,ğ‘£, . . . , ğ‘ğ‘ ,ğ‘ ) âˆˆ Rğ‘ Ã—ğ‘ of the spatial-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "degree matrices of A."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "temporal graphs and the spatial-spectral graphs.",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "Stacking several GT-layers in GTN aims to learn a high-level"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "To construct the heterogeneous spatial-temporal graph sequence,",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "meta-path that is a useful relationship of multi-modal signals. The"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": ", ğ‘‹ ğ¹ 2\n, . . . , ğ‘‹ ğ¹ ğ‘\n) âˆˆ\n= (ğ‘‹ ğ¹ 1\nwe compute the temporal node features ğ‘‹ ğ¹",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘¡\nğ‘¡",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "ğ‘™ğ‘¡â„ GT-layer in GTN updates the graph structure and multiplies"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "Rğ‘ , where ğ‘‹ ğ¹ğ‘–\ndenotes the amplitude (ğœ‡ğ‘‰ ) on the ğ‘–ğ‘¡â„ channel in",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "the graph structure with the one constructed by (ğ‘™ âˆ’ 1)ğ‘¡â„ GT-layer."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "the ğ‘¡ğ‘¡â„ timestep from the multi-modal signals, 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ . Then,",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "Formally, ğ» (ğ‘™) = ğ‘„ğ‘™ ğ» (ğ‘™âˆ’1) , where ğ‘„ğ‘™ denotes the graph structure"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "we combine the temporal domain feature vector ğ‘‹ ğ¹\nand the adja-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "learned by ğ‘™ğ‘¡â„ GT-layer, ğ» (ğ‘™) and ğ» (ğ‘™âˆ’1)\nrepresent the meta-paths"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "cency matrix ğ´ to form a heterogeneous spatial-temporal graph",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "learned by (ğ‘™ âˆ’ 1)ğ‘¡â„ and ğ‘™ğ‘¡â„ GT-layers, respectively. To consider"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğºğ‘¡ = (ğ‘‹ ğ¹\n, ğ´) as\nâ—‹. Therefore, we obtain ğ‘‡ heterogeneous spatial-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "multiple relationship of multi-modal signals, which are decided"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "temporal graphs from the sample. These heterogeneous spatial-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "by the number of meta-paths simultaneously, multi-channel 1 Ã— 1"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "temporal graphs are stacked into a heterogeneous spatial-temporal",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "convolution is applied. The graph structure generated by multi-"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "â—‹.\ngraph sequence ğºğ‘‡ = (ğº1, ğº2, . . . , ğºğ‘‡ âˆ’1, ğºğ‘‡ ) as",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "channel 1 Ã— 1 convolution is defined as:"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "To construct the heterogeneous spatial-spectral graph sequence,",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "ğ‘„ â€²"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "similar to the process of generating heterogeneous spatial-temporal",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "(4)\nğ‘™ = Î¦(A, ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘ŠÎ¦)),"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "graph sequence, we extract the DE features from the ğµ âˆˆ ğ‘ + fre-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "where Î¦ is the multi-channel 1 Ã— 1 convolution, ğ‘ŠÎ¦ âˆˆ R1Ã—ğ¶ğ´Ã—ğ¾ is"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "quency bands in the sample as\nâ—‹. These features extracted from",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "learnable parameter, ğ¶ğ´ is the number of channels, and ğ‘„ â€²\nis the\nğ‘™"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "different channels and the correlation among channels are con-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "graph structure generated from ğ‘™ğ‘¡â„ GT-layer by this convolution."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "verted to heterogeneous spatial-spectral graphs. Specifically, these",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "To concurrently learn various lengths of meta-paths, the identity"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "feature vectors extracted from ğ‘ channels in the ğ‘ğ‘¡â„ frequency",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "matrix is added into the result of each GT-layer. Finally, the graph"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": ", ğ‘‹ ğ¹ 2\n, . . . , ğ‘‹ ğ¹ ğ‘\n= (ğ‘‹ ğ¹ 1\nband ğ‘‹ ğ¹\n) âˆˆ Rğ‘ and the adjacency matrix",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ‘\nğ‘\nğ‘\nğ‘",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "sequence is updated into ğ¶ğ´ sequences according to the meta-paths"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ğ´ of the sample are composed of a heterogeneous spatial-spectral",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "learned from GTN."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "graph ğºğ‘ = (ğ‘‹ ğ¹\n, ğ´) as\nâ—‹. Therefore, we obtain ğµ heterogeneous\nğ‘",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "Graph Recurrent Neural Network. Graph recurrent neural"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "spatial-spectral graphs in ğµ frequency bands for each sample. Then,",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "network has two chief components, namely graph convolutional"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "these heterogeneous spatial-spectral graphs are stacked into a het-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "network and gated recurrent unit. These two components are used"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "erogeneous spatial-spectral sequence ğºğµ = (ğºğœƒ , ğºğ›¼, ğºğ›½, ğºğ›¾ ) as\nâ—‹.",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "to extract both spatial domain and temporal/spectral domain fea-"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "tures from graph sequences. Specifically, GCN is used to capture"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "4.2\nHeterogeneous Graph Recurrent Neural",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "the spatial domain features by aggregating information from neigh-"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "Network",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": ""
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "bor nodes. In addition, GRU is applied to extract temporal/spectral"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "HetEmotionNet consists of graph transformer network and graph",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "domain features from the graph sequence obtained after GCN."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "recurrent neural network. Graph recurrent neural network is com-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "Graph Convolutional Network. GCN captures the correlation"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "posed of graph convolutional network and gated recurrent unit.",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "among the nodes in the graph. The correlation is valuable to rec-"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "Graph Transformer Network. The Graph Transformer Net-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "ognize different emotions. For example, there exists much func-"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "work (GTN) [52] is used to model the heterogeneity of multi-modal",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "tional connectivity among different brain regions. Capturing the"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "signals by automatically extracting the meta-paths from the adja-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "spatial domain relationship among these functional connectivity"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "cency matrix set A. Specifically, A is generated from the heteroge-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "contributes to recognizing different emotions."
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "neous adjacency matrix obtained in Section 4.1. The heterogeneous",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "To capture the correlation, the spectral graph theory extends the"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "adjacency matrix is divided into homogeneous adjacency matrix",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "convolution operation from image-like data to graph structure data"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "is\nset A = {A1, A2, . . . , Ağ‘ğ‘ } according to edge types, where Ağ‘–",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "[3]. The graph structure is represented by its Laplacian matrix in the"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "homogeneous adjacency matrix, 1 â‰¤ ğ‘– â‰¤ ğ‘ğ‘, and ğ‘ğ‘ is the num-",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "spectral graph analysis. The Laplacian matrix and its eigenvalues"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "ber of adjacency matrices which is decided by the number of edge",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "reflect the property of the graph. The Laplacian matrix is defined as"
        },
        {
          "Figure 5 presents the process of constructing the heterogeneous": "types.",
          "GTN consists of several GT-layers. Each GT-layer aims to learn": "ğ¿ = ğ· âˆ’ ğ´, where ğ¿ âˆˆ Rğ‘ Ã—ğ‘ is the Laplacian matrix, ğ´ âˆˆ Rğ‘ Ã—ğ‘ is"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "and ğ‘ is the vertex number. The eigenvalues matrix is obtained",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ¶ğ´"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "by decomposing the Laplacian matrix with ğ¿ = ğ‘ˆ Î›ğ‘ˆ ğ‘‡ , where ğ‘ˆ",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ¶ğ‘\n),\nğ‘¤ğ‘–ğ›¿ğ‘  (ğºğºğ‘‡ ğ‘\n=\n(9)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "âˆ‘ï¸ ğ‘–\nğ‘ "
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "is the Fourier basis and Î› âˆˆ Rğ‘ Ã—ğ‘ is a diagonal matrix. The node",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘ ğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "feature is defined as ğ‘¥ğ‘– âˆˆ Rğ‘ for the graph ğºğ‘–\nin graph sequence ğºğ‘  .",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "where ğ›¿ğ‘  denotes the graph convolution operation on the graph"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "Then, the graph Fourier transform and inverse Fourier transform",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "sequence, ğ‘¤ğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "are defined as\nğ‘¥ğ‘– = ğ‘ˆ ğ‘‡ ğ‘¥ğ‘– and ğ‘¥ğ‘– = ğ‘ˆ Ë†ğ‘¥ğ‘– , respectively. Because the",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "is the weight for the weighted sum operation, and"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğºğºğ‘‡ ğ‘\nis the ğ‘–ğ‘¡â„ graph sequence in ğºğºğ‘‡ ğ‘\n."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "graph convolution operation is equal to the product of these signals",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘ ğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "which have been transformed into the spectral domain [3],\nthe",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "Gated Recurrent Unit. After the graph convolutional network,"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "graph convolution of signals ğ‘¥ğ‘– on graph ğºğ‘–\nis defined as:",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "GRU is applied to capture the dependency among different timesteps"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "or frequency bands. Gated recurrent unit consists of the reset gate"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "(5)\nğ‘”ğœƒ â˜…ğºğ‘– ğ‘¥ğ‘– = ğ‘ˆ ğ‘”ğœƒğ‘ˆ ğ‘‡ ğ‘¥ğ‘–,",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "and the update gate [5], which is defined as:"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "(cid:16)\n(cid:17)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "where ğ‘”ğœƒ denotes a graph convolution kernel and â˜…ğºğ‘– denotes a",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘Šğ‘Ÿ ğ»ğºğ¶ğ‘\n+ ğ‘‰ğ‘Ÿ ğ»ğºğ‘…ğ‘ˆ\n+ ğ‘ğ‘Ÿ ,\nğ‘…ğ‘– = ğœ\n(10)\nğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "graph convolution operation on graph ğºğ‘– . In addition, to simplify",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "(cid:16)\n(cid:16)\n(cid:17)\n(cid:17)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "the calculation of Laplace matrix, the ğ¾ order Chebyshev Polyno-",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ¶ğ‘\n,\nğ‘…ğ‘– âŠ™ ğ»ğºğ‘…ğ‘ˆ\nğ‘Šâ„ğ»ğºğ¶ğ‘\n+ ğ‘‰â„\n+ ğ‘â„\n= tanh\n(11)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–\nğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "mials are adopted, which is defined as:",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "(cid:16)\n(cid:17)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘Šğ‘§ğ»ğºğ¶ğ‘\n+ ğ‘‰ğ‘§ğ»ğºğ‘…ğ‘ˆ\n+ ğ‘ğ‘§,\nğ‘ğ‘– = ğœ\n(12)\nğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğ¾âˆ‘ï¸ ğ‘˜\nğ‘”ğœƒ (Î›) â‰ˆ\nğœƒğ‘˜ğ‘‡ğ‘˜ ( ËœÎ›),\n(6)",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "=0",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ‘…ğ‘ˆ\n,\n+ ğ‘ğ‘– âŠ™ ğ»ğºğ‘…ğ‘ˆ\n= (1 âˆ’ ğ‘ğ‘– ) âŠ™ Ëœğ»ğºğ¶ğ‘\n(13)"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–\nğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "2",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "where the parameters and the operator are defined as follows,"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "where ËœÎ› =\nÎ› âˆ’ ğ¼ğ‘ , ğ¼ğ‘ is the identity matrix, and ğœ†ğ‘šğ‘ğ‘¥ is the",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğœ†ğ‘šğ‘ğ‘¥",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "âŠ™ : the product of two matrices on each element."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "maximum eigenvalues of Î›. The ğœƒ âˆˆ ğ‘…ğ¾ is a vector of polynomial",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ¶ğ‘\n: the ğ‘–ğ‘¡â„ graph of the graph sequence, which is the input"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "coefficients. The Chebyshev polynomials are defined as ğ‘‡ğ‘˜ (ğ‘¥) =",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "of the ğ‘–ğ‘¡â„ GRU unit."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "2ğ‘¥ğ‘‡ğ‘˜âˆ’1 (ğ‘¥) âˆ’ ğ‘‡ğ‘˜âˆ’2 (ğ‘¥) recursively, where ğ‘‡0 (ğ‘¥) = 1 and ğ‘‡1 (ğ‘¥) = ğ‘¥.",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ‘…ğ‘ˆ\n: the output of previous GRU unit, which is the input of"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "With the ğ¾ order Chebyshev Polynomials, the message from 0 to",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğ¾ğ‘¡â„ neighbours are aggregated into the center node. For the fast",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "the ğ‘–ğ‘¡â„ GRU unit. It contains the information of the previous graphs."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘…ğ‘–\n: the output of the reset gate from ğ»ğºğ¶ğ‘\nand ğ»ğºğ‘…ğ‘ˆ"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "calculation [20], the first-order polynomials are used and ğœ†ğ‘šğ‘ğ‘¥ = 2,",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ".\nğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ¶ğ‘"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğ‘˜ = 1 are set. Then, the graph convolution is further simplified as:",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ": the output of tanh activation controlled by the reset gate\nğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘…ğ‘– , which determines the forgotten degree of the previous state"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "(cid:17)\n(cid:16)",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": ""
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "2 ğ´ğ·âˆ’ 1",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ‘…ğ‘ˆ"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğ‘¥ğ‘–,\nğ¼ğ‘ + ğ·âˆ’ 1\n(7)\nğ‘”ğœƒ â˜…ğºğ‘– ğ‘¥ğ‘– â‰ˆ ğœƒ",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘ğ‘–\n: the output of the update gate, which determines how many"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "where ğ· is the degree matrix, ğ´ is the adjacency matrix, and ğœƒ is the",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "elements of ğ»ğºğ‘…ğ‘ˆ\nand its state are updated in the ğ‘–ğ‘¡â„ GRU unit."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–âˆ’1"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ğ´ Ëœğ·âˆ’ 1\n2 ğ´ğ·âˆ’ 1",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ»ğºğ‘…ğ‘ˆ\n: the output of the ğ‘–ğ‘¡â„ GRU unit."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "parameter. ğ¼ğ‘ + ğ·âˆ’ 1\nis changed into Ëœğ·âˆ’ 1\nto prevent",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘–"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "numerical instability, gradient explosion and gradient disappear-",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "ğ‘Šğ‘Ÿ , ğ‘‰ğ‘…, ğ‘Šâ„ , ğ‘‰â„, ğ‘Šğ‘§, and ğ‘‰ğ‘§ are the learnable parameters of the"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "ance, where\nğ´ğ‘– ğ‘— and\nğ´ = ğ´ + ğ¼ğ‘ . The graph convolution of\nğ· = (cid:205)ğ‘—",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "GRU unit."
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "signals ğ‘¥ğ‘– on graph ğºğ‘–\nin graph sequence is defined as:",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "Each graph ğ»ğºğ¶ğ‘\nin ğ»ğºğ¶ğ‘\nis fed into the ğ‘¡ğ‘¡â„ GRU unit\nto"
        },
        {
          "the adjacency matrix, ğ· âˆˆ Rğ‘ Ã—ğ‘ is the degree matrix of the graph,": "",
          "sequences ğºğºğ‘‡ ğ‘\nis defined as:": "capture the features among the graphs. ğ»ğºğ‘…ğ‘ˆ\nis obtained by con-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "Model\nValence (%)\nArousal (%)"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "MLP [39]\n74.31Â±4.56\n76.23Â±5.12"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "SVM [2]\n83.14Â±3.66\n84.50Â±4.43"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "GCN [20]\n89.17Â±2.90\n90.33Â±3.59"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "DGCNN [44]\n90.44Â±3.01\n91.70Â±3.46"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "MM-ResLSTM [34]\n92.30Â±1.55\n92.87Â±2.11"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "ACRNN [45]\n93.72Â±3.21\n93.38Â±3.73"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "SST-EmotionNet [12]\n95.54Â±2.54\n95.97Â±2.86"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "97.66Â±1.54\n97.30Â±1.65\nHetEmotionNet"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "Table 2: The performance on the MAHNOB-HCI dataset."
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "Model\nValence (%)\nArousal (%)"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "MLP [39]\n72.84Â±6.88\n73.61Â±10.75"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "SVM [2]\n77.31Â±6.77\n77.16Â±9.14"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "GCN [20]\n81.31Â±6.09\n83.43Â±7.50"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "DGCNN [44]\n86.21Â±9.35\n86.07Â±10.53"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "MM-ResLSTM [34]\n89.46Â±8.64\n89.66Â±8.09"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "ACRNN [45]\n88.10Â±5.49\n89.90Â±5.96"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "SST-EmotionNet [12]\n90.06Â±4.80\n88.37Â±7.19"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "93.95Â±3.38\n93.90Â±3.04\nHetEmotionNet"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "results indicate our model achieves the best performance on the"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "DEAP dataset. Due to the advantages of automatic feature extrac-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "tion, most deep learning models have achieved better performance"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "than traditional methods (MLP and SVM). GCN extracts the spa-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "tial domain features from signals based on a fixed graph, while"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "DGCNN can adaptively learn the intrinsic relationship of different"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "channels by optimizing the adjacency matrix. Consequently, the"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "performance of DGCNN is more accurate than GCN by around"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "1%. MM-ResLSTM uses deep LSTM with residual network to cap-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "ture the temporal information in the multi-modal signals (EEG and"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "PPS). Hence, MM-ResLSTM can extract more abundant features"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "and perform better than GCN and DGCNN. Compared with MM-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "ResLSTM, ACRNN can capture both temporal domain and spatial"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "domain information and achieve a better accuracy of 93.72%, 93.38%"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "for valence and arousal. Further, SST-EmotionNet can extract the"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "spatial-spectral-temporal domain information from EEG signals"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "by 3D CNN, with the accuracy of 95.54% and 95.97%. Although"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "SST-EmotionNet achieves relatively excellent performance, it does"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "not utilize complementarity of multi-modal signals efficiently. Our"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "proposed graph-based model can explore spatial-spectral-temporal"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "domain information from multi-modal signals efficiently. More-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "over, our model takes the heterogeneous information of different"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "modalities into consideration. Therefore, our model can adequately"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "learn comprehensive information for emotion recognition, achiev-"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "ing the highest accuracy on the DEAP dataset. Meanwhile, our"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "model achieves the lowest standard deviation, which indicates the"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "stability of our model\nis also satisfied. Besides, Table 2 presents"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "the performance of all models on the MAHNOB-HCI dataset. Our"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "model achieves the accuracy of 93.95% and 93.90% for valence and"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "arousal, respectively. The performance of other baseline methods"
        },
        {
          "Table 1: The performance on the DEAP dataset.": ""
        },
        {
          "Table 1: The performance on the DEAP dataset.": "is between 72.84% and 90.06%. The results indicate that our model"
        },
        {
          "Table 1: The performance on the DEAP dataset.": "performs best on the MAHNOB-HCI dataset."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.85\n0.85": "accuracy\nF1-score\naccuracy\nF1-score"
        },
        {
          "0.85\n0.85": "b) arousal\na) valence"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "Figure 7: Ablation studies on fusing different modalities."
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "5.4\nAblation Studies"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "To verify the effectiveness of our model, ablation experiments are"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "performed on the DEAP dataset. There are three kinds of ablation"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "experiments: 1) Ablation experiments on the effectiveness of graph"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "transformer network, graph convolutional network, and gated re-"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "current unit. 2) Ablation experiments on the effectiveness of fusing"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "different modalities. 3) Ablation experiments on two-stream struc-"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "ture to verify whether utilizing spatial-spectral-temporal domain"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "features simultaneously is useful."
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "Ablation on different components. To verify the effective-"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "ness of different components in our model, we design three variants,"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "which are described as follows:"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "â€¢ HetEmotionNet-noGRU: To verify the effects of GRU on cap-"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "turing temporal or spectral dependency, this variant removes GRU"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "from HetEmotionNet."
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "â€¢ HetEmotionNet-noGTN: To verify the effectiveness of GTN"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "on modeling the heterogeneity of multi-modal data, this variant"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "removes GTN from HetEmotionNet."
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "â€¢ HetEmotionNet-noGCN: To verify the effectiveness of GCN on"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "capturing the correlation, we remove GCN from HetEmotionNet."
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "Figure 6 presents that removing different components from Het-"
        },
        {
          "0.85\n0.85": "EmotionNet reduces the performance. HetEmotionNet outperforms"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "HetEmotionNet-noGTN, which indicates that GTN is effective to"
        },
        {
          "0.85\n0.85": "model the heterogeneity of multi-modal data. HetEmotionNet has"
        },
        {
          "0.85\n0.85": "a better performance than HetEmotionNet-noGCN, demonstrating"
        },
        {
          "0.85\n0.85": "that GCN is effective to capture the correlation among channels. By"
        },
        {
          "0.85\n0.85": "comparing HetEmotionNet and HetEmotionNet-noGRU, it presents"
        },
        {
          "0.85\n0.85": "that HetEmotionNet-noGRU performs worse than HetEmotionNet,"
        },
        {
          "0.85\n0.85": "which indicates that GRU is important to capture the temporal or"
        },
        {
          "0.85\n0.85": "spectral dependency and improves the performance of model."
        },
        {
          "0.85\n0.85": "Ablation on fusing data of different modalities. To verify"
        },
        {
          "0.85\n0.85": "the effects of using multi-modal data and modeling the hetero-"
        },
        {
          "0.85\n0.85": "geneity of multi-modal data, we conduct ablation experiments and"
        },
        {
          "0.85\n0.85": "design three variants of HetEmotionNet:"
        },
        {
          "0.85\n0.85": "â€¢ HetEmotionNet-noEEG: It removes EEG signals from the multi-"
        },
        {
          "0.85\n0.85": "modal data to verify the effects of only using PPS on the modelâ€™s"
        },
        {
          "0.85\n0.85": ""
        },
        {
          "0.85\n0.85": "performance. In addition, we remove GTN from this variant because"
        },
        {
          "0.85\n0.85": "GTN is used to model the heterogeneity of multi-modal data while"
        },
        {
          "0.85\n0.85": "this variant only uses PPS."
        },
        {
          "0.85\n0.85": "â€¢ HetEmotionNet-noPPS: It removes PPS from the multi-modal"
        },
        {
          "0.85\n0.85": "data to verify the influences of only using EEG signals on the"
        },
        {
          "0.85\n0.85": "modelâ€™s performance. We remove the GTN for the same reason of"
        },
        {
          "0.85\n0.85": "HetEmotionNet-noEEG."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.99\n0.99": "0.975\n0.975",
          "1\n1": "0.95\n0.95"
        },
        {
          "0.99\n0.99": "noGRU\nnoGRU",
          "1\n1": "noST\nnoST"
        },
        {
          "0.99\n0.99": "0.96\n0.96",
          "1\n1": "0.9\n0.9"
        },
        {
          "0.99\n0.99": "noGTN\nnoGTN\n0.945\n0.945",
          "1\n1": "0.85\n0.85"
        },
        {
          "0.99\n0.99": "",
          "1\n1": "noSS\nnoSS"
        },
        {
          "0.99\n0.99": "0.93\n0.93",
          "1\n1": "0.8\n0.8"
        },
        {
          "0.99\n0.99": "noGCN\nnoGCN",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "0.915\n0.915",
          "1\n1": "our model\n0.75\n0.75\nour model"
        },
        {
          "0.99\n0.99": "our model\n0.9",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "0.9\nour model",
          "1\n1": "0.7\n0.7"
        },
        {
          "0.99\n0.99": "accuracy\nF1-score\naccuracy\nF1-score",
          "1\n1": "accuracy\nF1-score\naccuracy\nF1-score"
        },
        {
          "0.99\n0.99": "a) valence\nb) arousal",
          "1\n1": "b) arousal\na) valence"
        },
        {
          "0.99\n0.99": "Figure 6: Ablation studies on different components.",
          "1\n1": "Figure 8: Ablation studies on two-stream structure."
        },
        {
          "0.99\n0.99": "1\n1",
          "1\n1": "â€¢ HetEmotionNet-noGTN:\nIt uses both EEG signals and PPS."
        },
        {
          "0.99\n0.99": "noEEG\nnoEEG\n0.97\n0.97",
          "1\n1": "However, it removes GTN to verify that modeling the heterogeneity"
        },
        {
          "0.99\n0.99": "0.94\n0.94\nnoPPS\nnoPPS",
          "1\n1": "of multi-modal data is important."
        },
        {
          "0.99\n0.99": "0.91\n0.91\nnoGTN\nnoGTN",
          "1\n1": "As Figure 7 illustrates, HetEmotionNet-noPPS performs bet-"
        },
        {
          "0.99\n0.99": "0.88\n0.88",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "our model\nour model",
          "1\n1": "ter\nthan HetEmotionNet-noEEG. This\nresult\nindicates\nthat\nthe"
        },
        {
          "0.99\n0.99": "0.85\n0.85",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "accuracy\nF1-score\naccuracy\nF1-score",
          "1\n1": "effects of only using EEG signals is better than only using PPS,"
        },
        {
          "0.99\n0.99": "b) arousal\na) valence",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "because EEG signals are the main physiological signals used in"
        },
        {
          "0.99\n0.99": "Figure 7: Ablation studies on fusing different modalities.",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "emotion recognition [53]. Besides, HetEmotionNet-noGTN outper-"
        },
        {
          "0.99\n0.99": "5.4\nAblation Studies",
          "1\n1": "forms HetEmotionNet-noPPS and HetEmotionNet-noEEG, which"
        },
        {
          "0.99\n0.99": "",
          "1\n1": "demonstrates that fusing data of different modalities further im-"
        },
        {
          "0.99\n0.99": "To verify the effectiveness of our model, ablation experiments are",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "proves the performance. However, HetEmotionNet-noGTN does"
        },
        {
          "0.99\n0.99": "performed on the DEAP dataset. There are three kinds of ablation",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "not model the heterogeneity of multi-modal data. Therefore, HetE-"
        },
        {
          "0.99\n0.99": "experiments: 1) Ablation experiments on the effectiveness of graph",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "motionNet is designed to model the heterogeneity of multi-modal"
        },
        {
          "0.99\n0.99": "transformer network, graph convolutional network, and gated re-",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "data and reaches the best results."
        },
        {
          "0.99\n0.99": "current unit. 2) Ablation experiments on the effectiveness of fusing",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "Ablation on two-stream structure. To verify the effectiveness"
        },
        {
          "0.99\n0.99": "different modalities. 3) Ablation experiments on two-stream struc-",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "of integrating two-stream, we design two variants as follows:"
        },
        {
          "0.99\n0.99": "ture to verify whether utilizing spatial-spectral-temporal domain",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "â€¢ HetEmotionNet-noST (Spatial-Temporal stream): This variant"
        },
        {
          "0.99\n0.99": "features simultaneously is useful.",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "removes the spatial-temporal stream to verify the effects of utilizing"
        },
        {
          "0.99\n0.99": "Ablation on different components. To verify the effective-",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "the spatial-temporal domain features of physiological signals."
        },
        {
          "0.99\n0.99": "ness of different components in our model, we design three variants,",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "â€¢ HetEmotionNet-noSS (Spatial-Spectral stream): This variant"
        },
        {
          "0.99\n0.99": "which are described as follows:",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "removes the spatial-spectral stream to verify the effects of using"
        },
        {
          "0.99\n0.99": "â€¢ HetEmotionNet-noGRU: To verify the effects of GRU on cap-",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "the spatial-spectral domain features of physiological signals."
        },
        {
          "0.99\n0.99": "turing temporal or spectral dependency, this variant removes GRU",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "As Figure 8 illustrates, HetEmotionNet-noSS performs better"
        },
        {
          "0.99\n0.99": "from HetEmotionNet.",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "than HetEmotionNet-noST, which demonstrates that extracting fea-"
        },
        {
          "0.99\n0.99": "â€¢ HetEmotionNet-noGTN: To verify the effectiveness of GTN",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "tures in spatial and temporal domain is more effective than extract-"
        },
        {
          "0.99\n0.99": "on modeling the heterogeneity of multi-modal data, this variant",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "ing features in spatial and spectral domain. Besides, HetEmotionNet"
        },
        {
          "0.99\n0.99": "removes GTN from HetEmotionNet.",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "has a better performance than two variants, which manifests that"
        },
        {
          "0.99\n0.99": "â€¢ HetEmotionNet-noGCN: To verify the effectiveness of GCN on",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "our model is able to fuse the spatial-spectral-temporal domain fea-"
        },
        {
          "0.99\n0.99": "capturing the correlation, we remove GCN from HetEmotionNet.",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "tures simultaneously and improve the classification accuracy."
        },
        {
          "0.99\n0.99": "Figure 6 presents that removing different components from Het-",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "EmotionNet reduces the performance. HetEmotionNet outperforms",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "6\nCONCLUSION"
        },
        {
          "0.99\n0.99": "HetEmotionNet-noGTN, which indicates that GTN is effective to",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "model the heterogeneity of multi-modal data. HetEmotionNet has",
          "1\n1": "In this paper, we propose a novel two-stream heterogeneous graph"
        },
        {
          "0.99\n0.99": "a better performance than HetEmotionNet-noGCN, demonstrating",
          "1\n1": "recurrent neural network using multi-modal physiological signals"
        },
        {
          "0.99\n0.99": "that GCN is effective to capture the correlation among channels. By",
          "1\n1": "for emotion recognition. The proposed HetEmotionNet is based"
        },
        {
          "0.99\n0.99": "comparing HetEmotionNet and HetEmotionNet-noGRU, it presents",
          "1\n1": "on the two-stream structure, which can extract spatial-spectral-"
        },
        {
          "0.99\n0.99": "that HetEmotionNet-noGRU performs worse than HetEmotionNet,",
          "1\n1": "temporal domain features from multi-modal signals simultaneously."
        },
        {
          "0.99\n0.99": "which indicates that GRU is important to capture the temporal or",
          "1\n1": "Moreover, each stream consists of GTN for modeling the hetero-"
        },
        {
          "0.99\n0.99": "spectral dependency and improves the performance of model.",
          "1\n1": "geneity, GCN for modeling the correlation, and GRU for capturing"
        },
        {
          "0.99\n0.99": "Ablation on fusing data of different modalities. To verify",
          "1\n1": "the temporal domain or spectral domain dependency. Experiments"
        },
        {
          "0.99\n0.99": "the effects of using multi-modal data and modeling the hetero-",
          "1\n1": "on the DEAP and the MAHNOB-HCI datasets indicate that our"
        },
        {
          "0.99\n0.99": "geneity of multi-modal data, we conduct ablation experiments and",
          "1\n1": "proposed model achieves state-of-the-art performance on DEAP"
        },
        {
          "0.99\n0.99": "design three variants of HetEmotionNet:",
          "1\n1": "and MAHNOB-HCI. Besides, the proposed model is a general frame-"
        },
        {
          "0.99\n0.99": "â€¢ HetEmotionNet-noEEG: It removes EEG signals from the multi-",
          "1\n1": "work for multi-modal physiological signals."
        },
        {
          "0.99\n0.99": "modal data to verify the effects of only using PPS on the modelâ€™s",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "",
          "1\n1": "ACKNOWLEDGMENTS"
        },
        {
          "0.99\n0.99": "performance. In addition, we remove GTN from this variant because",
          "1\n1": ""
        },
        {
          "0.99\n0.99": "GTN is used to model the heterogeneity of multi-modal data while",
          "1\n1": "Financial supports by National Natural Science Foundation of China"
        },
        {
          "0.99\n0.99": "this variant only uses PPS.",
          "1\n1": "(61603029), the Fundamental Research Funds for the Central Univer-"
        },
        {
          "0.99\n0.99": "â€¢ HetEmotionNet-noPPS: It removes PPS from the multi-modal",
          "1\n1": "sities (2020YJS025), and the China Scholarship Council (202007090056)"
        },
        {
          "0.99\n0.99": "data to verify the influences of only using EEG signals on the",
          "1\n1": "are gratefully acknowledge. We are grateful for supporting from"
        },
        {
          "0.99\n0.99": "modelâ€™s performance. We remove the GTN for the same reason of",
          "1\n1": "Swarma-Kaifeng Workshop which is sponsored by Swarma Club"
        },
        {
          "0.99\n0.99": "HetEmotionNet-noEEG.",
          "1\n1": "and Kaifeng Foundation."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "page 032005. IOP Publishing, 2020."
        },
        {
          "REFERENCES": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine trans-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[26] Antje Lichtenstein, Astrid Oehme, Stefan Kupschick, and Thomas JÃ¼rgensohn."
        },
        {
          "REFERENCES": "lation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Comparing two emotion models for deriving affective states from physiological"
        },
        {
          "REFERENCES": "2014.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "data.\nIn Affect and emotion in human-computer interaction, pages 35â€“50. Springer,"
        },
        {
          "REFERENCES": "[2] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. A training algorithm",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "2008."
        },
        {
          "REFERENCES": "for optimal margin classifiers.\nIn Proceedings of the fifth annual workshop on",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[27] Wenqian Lin, Chao Li, and Shouqian Sun. Deep convolutional neural network"
        },
        {
          "REFERENCES": "Computational learning theory, pages 144â€“152, 1992.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "for emotion recognition using eeg and peripheral physiological signal.\nIn Inter-"
        },
        {
          "REFERENCES": "[3]\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral net-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "national Conference on Image and Graphics, pages 385â€“394. Springer, 2017."
        },
        {
          "REFERENCES": "works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[28] Yi-Lin Lin and Gang Wei. Speech emotion recognition based on hmm and svm."
        },
        {
          "REFERENCES": "2013.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "In 2005 international conference on machine learning and cybernetics, volume 8,"
        },
        {
          "REFERENCES": "[4] Xiyang Cai, Ziyu Jia, Minfang Tang, and Gaoxing Zheng. Brainsleepnet: Learning",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "pages 4898â€“4901. IEEE, 2005."
        },
        {
          "REFERENCES": "multivariate eeg representation for automatic sleep staging.\nIn 2020 IEEE Inter-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Jie-Lin Qiu, Wei-Long Zheng, and Bao-Liang Lu. Multimodal emo-\n[29] Wei Liu,"
        },
        {
          "REFERENCES": "national Conference on Bioinformatics and Biomedicine (BIBM), pages 976â€“979.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "tion recognition using deep canonical correlation analysis.\narXiv preprint"
        },
        {
          "REFERENCES": "IEEE, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "arXiv:1908.05349, 2019."
        },
        {
          "REFERENCES": "[5]\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empir-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[30] Wei Liu, Wei-Long Zheng, and Bao-Liang Lu. Multimodal emotion recognition"
        },
        {
          "REFERENCES": "ical evaluation of gated recurrent neural networks on sequence modeling. arXiv",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "using multimodal deep learning. arXiv preprint arXiv:1602.08225, 2016."
        },
        {
          "REFERENCES": "preprint arXiv:1412.3555, 2014.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Yunxiao Liu, Youfang Lin, Ziyu Jia, Yan Ma, and Jing Wang. Representation based\n[31]"
        },
        {
          "REFERENCES": "[6] Muhammad Najam Dar, Muhammad Usman Akram, Sajid Gul Khawaja, and",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "on ordinal patterns for seizure detection in eeg signals. Computers in Biology and"
        },
        {
          "REFERENCES": "Amit N Pujari. Cnn and lstm-based emotion charting using physiological signals.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Medicine, 126:104033, 2020."
        },
        {
          "REFERENCES": "Sensors, 20(16):4551, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[32]\nYunxiao Liu, Youfang Lin, Ziyu Jia, Jing Wang, and Yan Ma. A new dissimilarity"
        },
        {
          "REFERENCES": "[7]\nPieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "measure based on ordinal pattern for analyzing physiological signals. Physica A:"
        },
        {
          "REFERENCES": "tutorial on the cross-entropy method. Annals of operations research, 134(1):19â€“67,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Statistical Mechanics and its Applications, 574:125997, 2021."
        },
        {
          "REFERENCES": "2005.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[33] Yifei Lu, Wei-Long Zheng, Binbin Li, and Bao-Liang Lu. Combining eye move-"
        },
        {
          "REFERENCES": "[8] Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu. Differential entropy feature for",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "ments and eeg to enhance emotion recognition.\nIn IJCAI, volume 15, pages"
        },
        {
          "REFERENCES": "eeg-based emotion classification.\nIn 2013 6th International IEEE/EMBS Conference",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "1170â€“1176. Citeseer, 2015."
        },
        {
          "REFERENCES": "on Neural Engineering (NER), pages 81â€“84. IEEE, 2013.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[34]\nJiaxin Ma, Hao Tang, Wei-Long Zheng, and Bao-Liang Lu. Emotion recogni-"
        },
        {
          "REFERENCES": "[9] Gene H Golub, Michael Heath, and Grace Wahba. Generalized cross-validation",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "tion using multimodal residual\nlstm network.\nIn Proceedings of the 27th ACM"
        },
        {
          "REFERENCES": "as a method for choosing a good ridge parameter. Technometrics, 21(2):215â€“223,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "International Conference on Multimedia, pages 176â€“183, 2019."
        },
        {
          "REFERENCES": "1979.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[35] Nicola Martini, Danilo Menicucci, Laura Sebastiani, Remo Bedini, Alessandro"
        },
        {
          "REFERENCES": "[10] William James. What is an Emotion? Simon and Schuster, 2013.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Pingitore, Nicola Vanello, Matteo Milanesi, Luigi Landini, and Angelo Gemignani."
        },
        {
          "REFERENCES": "[11] Ziyu Jia, Xiyang Cai, Gaoxing Zheng, Jing Wang, and Youfang Lin. Sleepprintnet:",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "The dynamics of eeg gamma responses to unpleasant visual stimuli: From local"
        },
        {
          "REFERENCES": "A multivariate multimodal neural network based on physiological time-series for",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "activity to functional connectivity. NeuroImage, 60(2):922â€“932, 2012."
        },
        {
          "REFERENCES": "automatic sleep staging.\nIEEE Transactions on Artificial Intelligence, 1(3):248â€“257,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[36] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh"
        },
        {
          "REFERENCES": "2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Manocha. M3er: Multiplicative multimodal emotion recognition using facial,"
        },
        {
          "REFERENCES": "[12] Ziyu Jia, Youfang Lin, Xiyang Cai, Haobin Chen, Haijun Gou, and Jing Wang.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "textual, and speech cues.\nIn Proceedings of the AAAI Conference on Artificial"
        },
        {
          "REFERENCES": "Sst-emotionnet: Spatial-spectral-temporal based attention 3d dense network for",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Intelligence, volume 34, pages 1359â€“1367, 2020."
        },
        {
          "REFERENCES": "eeg emotion recognition.\nIn Proceedings of the 28th ACM International Conference",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[37] Toshimitsu Musha, Yuniko Terasaki, Hasnine A Haque, and George A Ivamitsky."
        },
        {
          "REFERENCES": "on Multimedia, pages 2909â€“2917, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Feature extraction from eegs associated with emotions. Artificial Life and Robotics,"
        },
        {
          "REFERENCES": "[13] Ziyu Jia, Youfang Lin, Zehui Jiao, Yan Ma, and Jing Wang. Detecting causality in",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "1(1):15â€“19, 1997."
        },
        {
          "REFERENCES": "multivariate time series via non-uniform embedding. Entropy, 21(12):1233, 2019.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Joana Pinto, Ana Fred, and Hugo PlÃ¡cido da Silva. Biosignal-based multimodal\n[38]"
        },
        {
          "REFERENCES": "[14] Ziyu Jia, Youfang Lin, Yunxiao Liu, Zehui Jiao, and Jing Wang. Refined nonuni-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "emotion recognition in a valence-arousal affective framework applied to immer-"
        },
        {
          "REFERENCES": "form embedding for coupling detection in multivariate time series.\nPhysical",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "sive video visualization.\nIn 2019 41st Annual International Conference of the IEEE"
        },
        {
          "REFERENCES": "Review E, 101(6):062113, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Engineering in Medicine and Biology Society (EMBC), pages 3577â€“3583. IEEE, 2019."
        },
        {
          "REFERENCES": "[15] Ziyu Jia, Youfang Lin, Jing Wang, Xuehui Wang, Peiyi Xie, and Yingbin Zhang.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[39] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal"
        },
        {
          "REFERENCES": "Salientsleepnet: Multimodal salient wave detection network for sleep staging.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "representations by error propagation.\nTechnical report, California Univ San"
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2105.13864, 2021.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Diego La Jolla Inst for Cognitive Science, 1985."
        },
        {
          "REFERENCES": "[16] Ziyu Jia, Youfang Lin, Jing Wang, Kaixin Yang, Tianhang Liu, and Xinwang Zhang.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[40]\nElham S Salama, Reda A El-Khoribi, Mahmoud E Shoman, and Mohamed A Wahby"
        },
        {
          "REFERENCES": "Mmcnn: A multi-branch multi-scale convolutional neural network for motor",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Shalaby. Eeg-based emotion recognition using 3d convolutional neural networks."
        },
        {
          "REFERENCES": "imagery classification.\nIn Frank Hutter, Kristian Kersting, Jefrey Lijffijt, and",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Int. J. Adv. Comput. Sci. Appl, 9(8):329â€“337, 2018."
        },
        {
          "REFERENCES": "Isabel Valera, editors, Machine Learning and Knowledge Discovery in Databases,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[41] Annett Schirmer and Ralph Adolphs. Emotion perception from face, voice, and"
        },
        {
          "REFERENCES": "pages 736â€“751, Cham, 2021. Springer International Publishing.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "touch: comparisons and convergence. Trends in cognitive sciences, 21(3):216â€“228,"
        },
        {
          "REFERENCES": "[17] Ziyu Jia, Youfang Lin, Jing Wang, Ronghao Zhou, Xiaojun Ning, Yuanlai He, and",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "2017."
        },
        {
          "REFERENCES": "Yaoshuai Zhao. Graphsleepnet: Adaptive spatial-temporal graph convolutional",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[42] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for"
        },
        {
          "REFERENCES": "networks for sleep stage classification.\nIn IJCAI, pages 1324â€“1330, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014."
        },
        {
          "REFERENCES": "[18] Ziyu Jia, Youfang Lin, Hongjun Zhang, and Jing Wang. Sleep stage classification",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[43] Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun, and Maja Pantic. A"
        },
        {
          "REFERENCES": "model based ondeep convolutional neural network. Journal of ZheJiang University",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "multimodal database for affect recognition and implicit tagging.\nIEEE transactions"
        },
        {
          "REFERENCES": "(Engineering Science), 54(10):1899â€“1905, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "on affective computing, 3(1):42â€“55, 2011."
        },
        {
          "REFERENCES": "[19] Yingying Jiang, Wei Li, M Shamim Hossain, Min Chen, Abdulhameed Alelaiwi,",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[44] Tengfei Song, Wenming Zheng, Peng Song, and Zhen Cui. Eeg emotion recogni-"
        },
        {
          "REFERENCES": "and Muneer Al-Hammadi. A snapshot research and implementation of mul-",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "tion using dynamical graph convolutional neural networks.\nIEEE Transactions on"
        },
        {
          "REFERENCES": "timodal\ninformation fusion for data-driven emotion recognition.\nInformation",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Affective Computing, 2018."
        },
        {
          "REFERENCES": "Fusion, 53:209â€“221, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[45] Wei Tao, Chang Li, Rencheng Song, Juan Cheng, Yu Liu, Feng Wan, and Xun Chen."
        },
        {
          "REFERENCES": "[20] Thomas N Kipf and Max Welling.\nSemi-supervised classification with graph",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Eeg-based emotion recognition via channel-wise attention and self attention."
        },
        {
          "REFERENCES": "convolutional networks. arXiv preprint arXiv:1609.02907, 2016.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "IEEE Transactions on Affective Computing, 2020."
        },
        {
          "REFERENCES": "[21]\nSander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[46] Xiao-Wei Wang, Dan Nie, and Bao-Liang Lu. Emotional state classification from"
        },
        {
          "REFERENCES": "Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap:",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "eeg data using machine learning approach. Neurocomputing, 129:94â€“106, 2014."
        },
        {
          "REFERENCES": "A database for emotion analysis; using physiological signals.\nIEEE transactions",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[47] Zhiyuan Wen, Ruifeng Xu, and Jiachen Du. A novel convolutional neural net-"
        },
        {
          "REFERENCES": "on affective computing, 3(1):18â€“31, 2011.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "works for emotion recognition based on eeg signal.\nIn 2017 International Confer-"
        },
        {
          "REFERENCES": "[22] Alexander Kraskov, Harald StÃ¶gbauer, and Peter Grassberger. Estimating mutual",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "ence on Security, Pattern Analysis, and Cybernetics (SPAC), pages 672â€“677. IEEE,"
        },
        {
          "REFERENCES": "information. Physical review E, 69(6):066138, 2004.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "2017."
        },
        {
          "REFERENCES": "[23]\nSylvia D Kreibig. Autonomic nervous system activity in emotion: A review.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[48] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and"
        },
        {
          "REFERENCES": "Biological psychology, 84(3):394â€“421, 2010.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "S Yu Philip. A comprehensive survey on graph neural networks. IEEE transactions"
        },
        {
          "REFERENCES": "[24] Zhenqi Li, Jing Wang, Ziyu Jia, and Youfang Lin. Learning space-time-frequency",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "on neural networks and learning systems, 2020."
        },
        {
          "REFERENCES": "representation with two-stream attention based 3d network for motor imagery",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "[49] Yilong Yang, Qingfeng Wu, Yazhen Fu, and Xiaowei Chen. Continuous con-"
        },
        {
          "REFERENCES": "classification. In 2020 IEEE International Conference on Data Mining (ICDM), pages",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "volutional neural network with 3d input\nfor eeg-based emotion recognition."
        },
        {
          "REFERENCES": "1124â€“1129. IEEE, 2020.",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "In International Conference on Neural\nInformation Processing, pages 433â€“443."
        },
        {
          "REFERENCES": "[25]\nJinxiang Liao, Qinghua Zhong, Yongsheng Zhu, and Dongli Cai. Multimodal",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        },
        {
          "REFERENCES": "",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": "Springer, 2018."
        },
        {
          "REFERENCES": "physiological signal emotion recognition based on convolutional recurrent neural",
          "network.\nIn IOP Conference Series: Materials Science and Engineering, volume 782,": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "tion recognition from multi-channel eeg through parallel convolutional recurrent",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "modal emotion recognition model using physiological signals. arXiv preprint"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "neural network. In 2018 International Joint Conference on Neural Networks (IJCNN),",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "arXiv:1911.12918, 2019."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "pages 1â€“7. IEEE, 2018.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "[57] Wei-Long Zheng, Wei Liu, Yifei Lu, Bao-Liang Lu, and Andrzej Cichocki. Emo-"
        },
        {
          "[50]": "[51]",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Yu-Xuan Yang, Zhong-Ke Gao, Xin-Min Wang, Yan-Li Li, Jing-Wei Han, Norbert",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "tionmeter: A multimodal\nframework for recognizing human emotions.\nIEEE"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Marwan, and JÃ¼rgen Kurths. A recurrence quantification analysis-based channel-",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "transactions on cybernetics, 49(3):1110â€“1122, 2018."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "frequency convolutional neural network for emotion recognition from eeg. Chaos:",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "[58] Wei-Long Zheng and Bao-Liang Lu.\nInvestigating critical frequency bands and"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "An Interdisciplinary Journal of Nonlinear Science, 28(8):085724, 2018.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "channels for eeg-based emotion recognition with deep neural networks.\nIEEE"
        },
        {
          "[50]": "[52]",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "Transactions on Autonomous Mental Development, 7(3):162â€“175, 2015."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Graph transformer networks. arXiv preprint arXiv:1911.06455, 2019.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "[59] Wei-Long Zheng, Jia-Yi Zhu, and Bao-Liang Lu.\nIdentifying stable patterns over"
        },
        {
          "[50]": "[53]",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Jianhua Zhang, Zhong Yin, Peng Chen, and Stefano Nichele. Emotion recognition",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "time for emotion recognition from eeg.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "using multi-modal data and machine learning techniques: A tutorial and review.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "10(3):417â€“429, 2017."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Information Fusion, 59:103â€“126, 2020.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "[60] Wei-Long Zheng, Jia-Yi Zhu, Yong Peng, and Bao-Liang Lu. Eeg-based emotion"
        },
        {
          "[50]": "[54] Tong Zhang, Wenming Zheng, Zhen Cui, Yuan Zong, and Yang Li.",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Spatialâ€“",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "classification using deep belief networks.\nIn 2014 IEEE International Conference"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "temporal recurrent neural network for emotion recognition.\nIEEE transactions",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "on Multimedia and Expo (ICME), pages 1â€“6. IEEE, 2014."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "on cybernetics, 49(3):839â€“847, 2018.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "[61]\nJia Ziyu, Lin Youfang, Liu Tianhang, Yang Kaixin, Zhang Xinwang, and Wang Jing."
        },
        {
          "[50]": "[55] Xiaowei Zhang, Jing Pan, Jian Shen, Zia Ud Din, Junlei Li, Dawei Lu, Manxi Wu,",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "Motor imagery classification based on multiscale feature extraction and squeeze-"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "and Bin Hu.\nFusing of electroencephalogram and eye movement with group",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "excitation model. Journal of Computer Research and Development, 57(12):2481,"
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "sparse canonical correlation analysis for anxiety detection.\nIEEE Transactions on",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": "2020."
        },
        {
          "[50]": "",
          "Yilong Yang, Qingfeng Wu, Ming Qiu, Yingdong Wang, and Xiaowei Chen. Emo-": "Affective Computing, 2020.",
          "[56] Yuxuan Zhao, Xinyan Cao,\nJinlong Lin, Dunshan Yu, and Xixin Cao. Multi-": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table S.1: Notations and explanations": "Explanation"
        },
        {
          "Table S.1: Notations and explanations": "Node features"
        },
        {
          "Table S.1: Notations and explanations": "Adjacency matrix"
        },
        {
          "Table S.1: Notations and explanations": "Heterogeneous emotional network"
        },
        {
          "Table S.1: Notations and explanations": "Number of timesteps"
        },
        {
          "Table S.1: Notations and explanations": "Number of frequency bands"
        },
        {
          "Table S.1: Notations and explanations": "Spatial-temporal graph sequence"
        },
        {
          "Table S.1: Notations and explanations": "Spatial-spectral graph sequence"
        },
        {
          "Table S.1: Notations and explanations": "Signals of channel ğ‘¢"
        },
        {
          "Table S.1: Notations and explanations": ""
        },
        {
          "Table S.1: Notations and explanations": "Correlation between channel pair (ğ‘¢, ğ‘£)"
        },
        {
          "Table S.1: Notations and explanations": "Homogeneous adjacency matrix set"
        },
        {
          "Table S.1: Notations and explanations": "Graph struture generated in the ğ‘™ğ‘¡â„ GT-layer"
        },
        {
          "Table S.1: Notations and explanations": "Laplacian matrix"
        },
        {
          "Table S.1: Notations and explanations": "Degree matrix"
        },
        {
          "Table S.1: Notations and explanations": "Fourier basis"
        },
        {
          "Table S.1: Notations and explanations": "Graph convolution kernel"
        },
        {
          "Table S.1: Notations and explanations": "Order of Chebyshev Polynomials"
        },
        {
          "Table S.1: Notations and explanations": "Activation function"
        },
        {
          "Table S.1: Notations and explanations": "Number of graphs in a graph sequence"
        },
        {
          "Table S.1: Notations and explanations": "Number of channels"
        },
        {
          "Table S.1: Notations and explanations": "Output of the reset gate"
        },
        {
          "Table S.1: Notations and explanations": "Output of the update gate"
        },
        {
          "Table S.1: Notations and explanations": "Weight vector"
        },
        {
          "Table S.1: Notations and explanations": "Bias vector"
        },
        {
          "Table S.1: Notations and explanations": "1 Ã— 1 convolution"
        },
        {
          "Table S.1: Notations and explanations": ""
        },
        {
          "Table S.1: Notations and explanations": "Multi-channel 1 Ã— 1 convolution"
        },
        {
          "Table S.1: Notations and explanations": "Element-wise multiplication"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": ""
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "Modality"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "EEG"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "EOG"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "EMG"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "ECG"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "GSR"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "BVP"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "Respiration"
        },
        {
          "Table S.2: The division strategy of frequency bands for physiological signals in different modalities according to [14,34]. For": "Temperature"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "2",
      "title": "A training algorithm for optimal margin classifiers",
      "authors": [
        "Bernhard Boser",
        "Isabelle Guyon",
        "Vladimir Vapnik"
      ],
      "year": "1992",
      "venue": "Proceedings of the fifth annual workshop on Computational learning theory"
    },
    {
      "citation_id": "3",
      "title": "Spectral networks and locally connected networks on graphs",
      "authors": [
        "Joan Bruna",
        "Wojciech Zaremba",
        "Arthur Szlam",
        "Yann Lecun"
      ],
      "year": "2013",
      "venue": "Spectral networks and locally connected networks on graphs",
      "arxiv": "arXiv:1312.6203"
    },
    {
      "citation_id": "4",
      "title": "Brainsleepnet: Learning multivariate eeg representation for automatic sleep staging",
      "authors": [
        "Xiyang Cai",
        "Ziyu Jia",
        "Minfang Tang",
        "Gaoxing Zheng"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "5",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "6",
      "title": "Cnn and lstm-based emotion charting using physiological signals",
      "authors": [
        "Muhammad Najam Dar",
        "Muhammad Usman Akram",
        "Sajid Gul Khawaja",
        "Amit Pujari"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "7",
      "title": "A tutorial on the cross-entropy method",
      "authors": [
        "Pieter-Tjerk De Boer",
        "Dirk Kroese",
        "Shie Mannor",
        "Reuven Rubinstein"
      ],
      "year": "2005",
      "venue": "Annals of operations research"
    },
    {
      "citation_id": "8",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "9",
      "title": "Generalized cross-validation as a method for choosing a good ridge parameter",
      "authors": [
        "Gene Golub",
        "Michael Heath",
        "Grace Wahba"
      ],
      "year": "1979",
      "venue": "Technometrics"
    },
    {
      "citation_id": "10",
      "title": "What is an Emotion?",
      "authors": [
        "William James"
      ],
      "year": "2013",
      "venue": "What is an Emotion?"
    },
    {
      "citation_id": "11",
      "title": "Sleepprintnet: A multivariate multimodal neural network based on physiological time-series for automatic sleep staging",
      "authors": [
        "Ziyu Jia",
        "Xiyang Cai",
        "Gaoxing Zheng",
        "Jing Wang",
        "Youfang Lin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Sst-emotionnet: Spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Xiyang Cai",
        "Haobin Chen",
        "Haijun Gou",
        "Jing Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Detecting causality in multivariate time series via non-uniform embedding",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Zehui Jiao",
        "Yan Ma",
        "Jing Wang"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "14",
      "title": "Refined nonuniform embedding for coupling detection in multivariate time series",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Yunxiao Liu",
        "Zehui Jiao",
        "Jing Wang"
      ],
      "year": "2020",
      "venue": "Physical Review E"
    },
    {
      "citation_id": "15",
      "title": "Salientsleepnet: Multimodal salient wave detection network for sleep staging",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Xuehui Wang",
        "Peiyi Xie",
        "Yingbin Zhang"
      ],
      "year": "2021",
      "venue": "Salientsleepnet: Multimodal salient wave detection network for sleep staging",
      "arxiv": "arXiv:2105.13864"
    },
    {
      "citation_id": "16",
      "title": "Mmcnn: A multi-branch multi-scale convolutional neural network for motor imagery classification",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Kaixin Yang",
        "Tianhang Liu",
        "Xinwang Zhang"
      ],
      "year": "2021",
      "venue": "Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "17",
      "title": "Graphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Ronghao Zhou",
        "Xiaojun Ning",
        "Yuanlai He",
        "Yaoshuai Zhao"
      ],
      "year": "2020",
      "venue": "IJCAI"
    },
    {
      "citation_id": "18",
      "title": "Sleep stage classification model based ondeep convolutional neural network",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Hongjun Zhang",
        "Jing Wang"
      ],
      "year": "2020",
      "venue": "Journal of ZheJiang University (Engineering Science)"
    },
    {
      "citation_id": "19",
      "title": "A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition",
      "authors": [
        "Yingying Jiang",
        "Wei Li",
        "M Shamim Hossain",
        "Min Chen",
        "Abdulhameed Alelaiwi",
        "Muneer Al-Hammadi"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "21",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "22",
      "title": "Estimating mutual information",
      "authors": [
        "Alexander Kraskov",
        "Harald StÃ¶gbauer",
        "Peter Grassberger"
      ],
      "year": "2004",
      "venue": "Physical review E"
    },
    {
      "citation_id": "23",
      "title": "Autonomic nervous system activity in emotion: A review",
      "authors": [
        "D Sylvia",
        "Kreibig"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "24",
      "title": "Learning space-time-frequency representation with two-stream attention based 3d network for motor imagery classification",
      "authors": [
        "Zhenqi Li",
        "Jing Wang",
        "Ziyu Jia",
        "Youfang Lin"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "25",
      "title": "Multimodal physiological signal emotion recognition based on convolutional recurrent neural network",
      "authors": [
        "Jinxiang Liao",
        "Qinghua Zhong",
        "Yongsheng Zhu",
        "Dongli Cai"
      ],
      "year": "2020",
      "venue": "IOP Conference Series: Materials Science and Engineering"
    },
    {
      "citation_id": "26",
      "title": "Comparing two emotion models for deriving affective states from physiological data",
      "authors": [
        "Antje Lichtenstein",
        "Astrid Oehme",
        "Stefan Kupschick",
        "Thomas JÃ¼rgensohn"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "27",
      "title": "Deep convolutional neural network for emotion recognition using eeg and peripheral physiological signal",
      "authors": [
        "Wenqian Lin",
        "Chao Li",
        "Shouqian Sun"
      ],
      "year": "2017",
      "venue": "International Conference on Image and Graphics"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition based on hmm and svm",
      "authors": [
        "Yi-Lin Lin",
        "Gang Wei"
      ],
      "year": "2005",
      "venue": "2005 international conference on machine learning and cybernetics"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition using deep canonical correlation analysis",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition using deep canonical correlation analysis",
      "arxiv": "arXiv:1908.05349"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition using multimodal deep learning",
      "authors": [
        "Wei Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2016",
      "venue": "Multimodal emotion recognition using multimodal deep learning",
      "arxiv": "arXiv:1602.08225"
    },
    {
      "citation_id": "31",
      "title": "Representation based on ordinal patterns for seizure detection in eeg signals",
      "authors": [
        "Yunxiao Liu",
        "Youfang Lin",
        "Ziyu Jia",
        "Yan Ma",
        "Jing Wang"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "32",
      "title": "A new dissimilarity measure based on ordinal pattern for analyzing physiological signals",
      "authors": [
        "Yunxiao Liu",
        "Youfang Lin",
        "Ziyu Jia",
        "Jing Wang",
        "Yan Ma"
      ],
      "year": "2021",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "33",
      "title": "Combining eye movements and eeg to enhance emotion recognition",
      "authors": [
        "Yifei Lu",
        "Wei-Long Zheng",
        "Binbin Li",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IJCAI"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "Jiaxin Ma",
        "Hao Tang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "The dynamics of eeg gamma responses to unpleasant visual stimuli: From local activity to functional connectivity",
      "authors": [
        "Nicola Martini",
        "Danilo Menicucci",
        "Laura Sebastiani",
        "Remo Bedini",
        "Alessandro Pingitore",
        "Nicola Vanello",
        "Matteo Milanesi",
        "Luigi Landini",
        "Angelo Gemignani"
      ],
      "year": "2012",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "36",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Feature extraction from eegs associated with emotions",
      "authors": [
        "Toshimitsu Musha",
        "Yuniko Terasaki",
        "Hasnine Haque",
        "George Ivamitsky"
      ],
      "year": "1997",
      "venue": "Artificial Life and Robotics"
    },
    {
      "citation_id": "38",
      "title": "Biosignal-based multimodal emotion recognition in a valence-arousal affective framework applied to immersive video visualization",
      "authors": [
        "Joana Pinto",
        "Ana Fred",
        "Hugo PlÃ¡cido Da Silva"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "39",
      "title": "Learning internal representations by error propagation",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1985",
      "venue": "Learning internal representations by error propagation"
    },
    {
      "citation_id": "40",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "Reda Elham S Salama",
        "Mahmoud El-Khoribi",
        "Mohamed A Wahby Shoman",
        "Shalaby"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "41",
      "title": "Emotion perception from face, voice, and touch: comparisons and convergence",
      "authors": [
        "Annett Schirmer",
        "Ralph Adolphs"
      ],
      "year": "2017",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "42",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "43",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "44",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao",
        "Chang Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Emotional state classification from eeg data using machine learning approach",
      "authors": [
        "Xiao-Wei Wang",
        "Dan Nie",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "47",
      "title": "A novel convolutional neural networks for emotion recognition based on eeg signal",
      "authors": [
        "Zhiyuan Wen",
        "Ruifeng Xu",
        "Jiachen Du"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)"
    },
    {
      "citation_id": "48",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Fengwen Chen",
        "Guodong Long",
        "Chengqi Zhang",
        "S Yu"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "49",
      "title": "Continuous convolutional neural network with 3d input for eeg-based emotion recognition",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Yazhen Fu",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "50",
      "title": "Emotion recognition from multi-channel eeg through parallel convolutional recurrent neural network",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Ming Qiu",
        "Yingdong Wang",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "51",
      "title": "A recurrence quantification analysis-based channelfrequency convolutional neural network for emotion recognition from eeg",
      "authors": [
        "Yu-Xuan Yang",
        "Zhong-Ke Gao",
        "Xin-Min Wang",
        "Yan-Li Li",
        "Jing-Wei Han",
        "Norbert Marwan",
        "JÃ¼rgen Kurths"
      ],
      "year": "2018",
      "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science"
    },
    {
      "citation_id": "52",
      "title": "Graph transformer networks",
      "authors": [
        "Seongjun Yun",
        "Minbyul Jeong",
        "Raehyun Kim",
        "Jaewoo Kang",
        "Hyunwoo J Kim"
      ],
      "year": "2019",
      "venue": "Graph transformer networks",
      "arxiv": "arXiv:1911.06455"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "Jianhua Zhang",
        "Zhong Yin",
        "Peng Chen",
        "Stefano Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "54",
      "title": "Spatialtemporal recurrent neural network for emotion recognition",
      "authors": [
        "Tong Zhang",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Yang Li"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "55",
      "title": "Fusing of electroencephalogram and eye movement with group sparse canonical correlation analysis for anxiety detection",
      "authors": [
        "Xiaowei Zhang",
        "Jing Pan",
        "Jian Shen",
        "Zia Ud Din",
        "Junlei Li",
        "Dawei Lu",
        "Manxi Wu",
        "Bin Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Multimodal emotion recognition model using physiological signals",
      "authors": [
        "Yuxuan Zhao",
        "Xinyan Cao",
        "Jinlong Lin",
        "Dunshan Yu",
        "Xixin Cao"
      ],
      "year": "2019",
      "venue": "Multimodal emotion recognition model using physiological signals",
      "arxiv": "arXiv:1911.12918"
    },
    {
      "citation_id": "57",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "58",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "59",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Bao-Liang Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Eeg-based emotion classification using deep belief networks",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Yong Peng",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "61",
      "title": "Motor imagery classification based on multiscale feature extraction and squeezeexcitation model",
      "authors": [
        "Jia Ziyu",
        "Lin Youfang",
        "Liu Tianhang",
        "Yang Kaixin",
        "Zhang Xinwang",
        "Wang Jing"
      ],
      "year": "2020",
      "venue": "Journal of Computer Research and Development"
    }
  ]
}