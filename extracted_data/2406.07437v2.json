{
  "paper_id": "2406.07437v2",
  "title": "Graph-Based Multi-Feature Fusion Method For Speech Emotion Recognition",
  "published": "2024-06-11T16:45:34Z",
  "authors": [
    "Xueyu Liu",
    "Jie Lin",
    "Chao Wang"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Speech feature fusion",
    "Multi-dimensional Edge",
    "Graph representation Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Exploring proper way to conduct multi-speech feature fusion for cross-corpus speech emotion recognition is crucial as different speech features could provide complementary cues reflecting human emotion status. While most previous approaches only extract a single speech feature for emotion recognition, existing fusion methods such as concatenation, parallel connection, and splicing ignore heterogeneous patterns in the interaction between features and features, resulting in performance of existing systems. In this paper, we propose a novel graph-based fusion method to explicitly model the relationships between every pair of speech features. Specifically, we propose a multidimensional edge features learning strategy called Graph-based multi-Feature fusion method for speech emotion recognition. It represents each speech feature as a node and learns multi-dimensional edge features to explicitly describe the relationship between each feature-feature pair in the context of emotion recognition. This way, the learned multi-dimensional edge features encode speech feature-level information from both the vertex and edge dimensions. Our Approach consists of three modules: an Audio Feature Generation(AFG)module, an Audio-Feature Multi-dimensional Edge Feature(AMEF) module and a Speech Emotion Recognition (SER) module. The proposed methodology yielded satisfactory outcomes on the SEWA dataset. Furthermore, the method demonstrated enhanced performance compared to the baseline in the AVEC 2019 Workshop and Challenge. We used data from two cultures as our training and validation sets: two cultures containing German and Hungarian on the SEWA dataset, the CCC scores for German are improved by 17.28% for arousal and 7.93% for liking, and for Hungarian, the CCC scores are improved by 11.15% for arousal and 131.11% for valence. The outcomes of our methodology demonstrate a 13% improvement over alternative fusion techniques, including those employing one dimensional edge-based feature fusion approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is an easily accessible medium for humans to communicate their intentions and messages, which can be conveyed via ubiquitous devices such as microphones and phone. Consequently, many previous studies devoted to recognizing human emotions from their speeches  1  . Recent development in speech emotion recognition has greatly transformed consumer shopping, information gathering, and communication  2  . Practice feedback has been well applied commercially to the fields of human-computer interaction, healthcare, and marketing  3, 4  .\n\nA key challenge in speech emotion recognition is to learn discriminative emotional representations from raw speech signals. Early studies 5 used low-level handcrafted features to complete speech emotion recognition tasks, including Low level descriptors(LLDS) and High level statistics functions (HSFs, e.g. mean and maximum values of LLDs)  6  , Mel Frequency Cepstral Coefficients (MFCCs)  7  , Linear Predictive Coding (LPC)  8  , and Perceptual Linear Prediction (PLP)  9, 10  . Based on LLDs, researchers applied audio word bag (BoAW) to extract features in an unsupervised manner, and obtained more statistical information by aggregating multiple frame-level descriptive words from audio segments  11  . However, the above-mentioned hand-crafted features may fail to learn task-specific (e.g., emotionrelated) features from the given audio signals. As a result, with the emergence of deep learning model, more and more speech emotion recognition methods have been built on Deep Neural Network (DNNs), which can be optimized based on the target and thus can extract task-specific features from audios to make better predictions  12, 13, 14, 15  . Deep learning methods can extract hierarchical and discriminative feature representations through supervised learning, and the application of models such as CNN and LSTM has also been shown to further improve accuracy  6, 16, 17  . Recently, other artificial intelligence (AI) techniques, such as self-attention mechanisms, have also been proposed for SER tasks  18  .\n\nIn previous studies, researchers frequently predict emotions based on a single speech feature described above  19, 20, 21  . In the context of SER tasks, low-level hand-crafted features have been employed in small-sample data sets due to their transparent computational principles and straightforward structure. In contrast, deep learning methods are more suited to feature extraction tasks in massive data sets, as they are capable of automatically learning optimal feature representations and combining low-level features to form novel features  22, 23  . For speech emotion recognition, the use of a single type of feature in some cases leads to low recognition accuracy. First, when dealing with speech data from speakers with different backgrounds, due to the influence of contextual factors such as gender information, language, culture, etc., these problems lead to a single audio feature that cannot cover all the emotional information  24  ; second, when dealing with speech data with a large amplitude of emotional change, due to the influence of the speaker's emotion, the existence of changes in the speed of speech, as well as the difference in the energy between the various spectra, a single feature can only express speech's emo- Showing a unique association pattern encoded for each feature vector in the node, these features also determine the specific task topology of the graph, and additionally describe the use of multidimensional features for each edge (the relationship between a pair of AFs).\n\ntional information from one side to express the emotional information of speech, so a single type of feature has a performance bottleneck in speech emotion recognition  25, 26  .\n\nTo overcome the limitations of speech recognition based on a single speech feature, combining different speech features has proven to be an effective solution  27  . Specifically, the most common solution is to directly concatenate multiple different speech features  28  . For example, Zhou et al. combine emotion feature maps extracted from different spectral diagrams, thereby enhancing the accuracy of speech recognition  29  . Alternatively, some studies conduct decision-level fusion, which first make an emotion prediction from each feature and then average all predictions  30, 31  . However, both strategies fail to comprehensively model the inter-relation between different speech features, which may lead to not only the final combined feature contain redundant information but also ignore crucial relationship cues for the emotion recognition. Recently, Graph Neural Networks (GNNs) have attracted more attentions from researchers for speech emotion analysis  32  . However, the node features used in this study are all deep learning features from LSTM outputs, and as mentioned earlier, a single feature type can provide very limited useful information. Therefore, we propose the hypothesis that using different types of speech features as nodes, more useful information can be learned from neighboring nodes using an effective aggregation method  6  .\n\nIn this paper, we propose a novel graph-based fusion strategy that leverages the merits of both deep-learned and hand-crafted audio features for speech emotion recognition. Inspired by GRATIS  33  , we propose a novel method that learns a multi-dimensional edge feature to explicitly describe the relationship between each pair of to address the challenge of feature integration in cross-corpus speech emotion recognition. Compared to methods such as GIN, it has higher coupling with non-graphical heterogeneous data such as deep learning and hand-crafted audio features and can easily be combined with various deep learning backbone and GNN to predict different downstream tasks, facilitating subsequent speech emotion classification. Our approach consists of three modules: an Audio Feature Generation (AFG) module, an Audio-Feature Multi-dimensional Edge Feature (AMEF) module and a Speech Emotion Recognition (SER) module. Initially, it draws upon two conventional features, eGeMAPs and MFCCs, as well as two bag-of-words models, BoAW-e and BoAW-m, and the deep learning feature, Deep Spectrum. AFG module extracts five feature vectors representing these five speech features within an LSTM framework. In a subsequent phase, the AMEF module was used to construct a graph representation based on multidimensional edges, which is ultimately integrated into a combined feature. Finally, the combined feature is fed back into the model as an input. The SER module completes the emotion recognition of speech vectors through regression methods, resulting in the outcome. The main contribution of this work is summarized as follows:\n\n• To the best of our knowledge, we introduce the first approach to leverage multidimensional edge features for graph-based cross-corpus sentiment recognition tasks. By constructing learnable multi-dimensional edge features, we explicitly consider and model the relationships between complex and diverse audio features. • We present a method for fusing different types of speech features in a way that captures the information and relationships of different speech features. Through cross-attention, we effectively extract novel features and utilize them to guide the updating of multi-dimensional edge features. • We conducted experiments on SEWA dataset,an official cross-lingual speech dataset,significant improvements in the accuracy of speech emotion recognition have been achieved by integrating fusion methods into existing graph-based frameworks.\n\nGraph-based multi-Feature fusion method for speech emotion recognition 5\n\n2. Related Work",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Recognition Methods",
      "text": "The primary task of Speech Emotion Recognition (SER) is to extract and identify the emotional information contained in speech. Over the past several decades, the study of identifying human emotional states from speech through computer technology has been a popular field of research. The SER system's recognition process of speech emotions is divided into two steps. Firstly, speech features need to be extracted. These features carry contextual information and are important sources of information for computers to recognize emotions. In early research, the common features used in speech emotion recognition by academia included prosody features, spectral features, and timbre features  9  . The hand-crafted features include statistical functions such as mean values and extrema, as well as features such as Mel Frequency Cepstral Coefficients (MFCCs), Linear Predictive Coding (LPC), and Perceptual Linear Prediction (PLP)  9, 10  . On the basis of LLDs, researchers have used the Audio Word Bag (BoAW) to extract features in an unsupervised manner  11  .\n\nRecently, many studies have begun to use deep learning methods to extract Deep Spectrum representations from audio segments. Deep Spectrum representations are mainly due to the advantages of transfer learning, as they are formed by passing spectral maps through pre-trained image classification CNNs such as AlexNet or VGG  34  . Deep Spectrum features are also typical Deep Spectrum features, and have been proven effective for various general audio recognition tasks  35  . For example, the DeepSpectrumLite transfer learning framework is used for pre-training image convolutional neural networks (CNNs) for speech recognition  15  . Moreover, humans may have more than one tone when speaking, and the diversity of tones represents changes in emotion  36, 37  . Under such circumstances, the boundaries of emotional recognition become even more complex due to human factors. On the other hand, the perception of speech in a language depends on factors such as the speaker's cultural background, language, gender, and even the speaker's age, which makes speech emotion recognition more complex  38  . Based on the above description, the second step of speech emotion recognition is to classify the emotional content of the speech file through machine learning and other classification algorithms. In most cases, classifiers are divided into two main categories: traditional classifiers and deep learning classifiers. Traditional classifiers include SVM, HMM, GMM, KNN, decision trees, LDA, and maximum likelihood methods  39, 40, 41  . In contrast, there has been an increasing number of studies in recent years using deep learning classifiers such as CNN, DNN, RNN, DBN, LSTM, and DBM for speech emotion recognition. Notably, the emotional content in speech changes over time, so it is appropriate to use techniques that are effective for modeling time, such as stochastic HMMs or neural networks with recursive units, such as LSTM or GRU  42  .\n\nDespite notable progress in emotion recognition, cross-corpus speech emotion recognition remains a significant challenge due to cultural and linguistic disparities  43  . In the CES sub-project of the AVEC 2019 Workshop and Challenge, Fabien and colleagues utilized a 2-layer LSTM-RNN (64/32 units) as the time-dependent regressor for each representation of audio signals and employed SVMs for late-stage fusion of predictions  44  . Compared to the performance reported in the previous AVEC CES, arousal and valence scores for German improved by 7.25% and 8.25%, respectively, while those for Hungarian improved by 17.3% and 13.3%, respectively. However, the study still faces an issue: individual features such as the MFCCs, eGeMAPs, and deep learning DEEP SPECTRUM cannot achieve optimal results across multiple emotion categories due to varying information content within each feature  45  . This is particularly true since these features describe emotional information from different perspectives. Therefore, there is a need to find a method for deep fusion of features to generate new ones, aiming for better results across multiple emotion categories.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Graph Representation Learning And Graph Neural Networks",
      "text": "While deep learning effectively captures the hidden patterns in Euclidean data such as images, text, and videos, an increasing number of applications represent data in graphical form  46  . For instance, in e-commerce, graph-based learning systems can leverage relationships between users and products to make highly accurate recommendations  47  . Graph Neural Networks (GNNs) have garnered significant attention for their ability to handle non-Euclidean data  48  . Thanks to their unique advantages in relationship construction, GNNs can effectively represent multiple features of the same type as nodes in a graph, thereby capturing both similarities and differences between data points  49, 50, 51, 52  .\n\nThe purpose of graph representation learning is to convert nodes in a graph into low-dimensional vector representations through specific methods. This lowdimensional vector representation can preserve the node features, structural features, and semantic information of the original graph, represented by a set of vertices and edges  53, 54, 55  . However, most graphs are constructed by manually defining the vertices and the characteristics of each edge through predefined rules, which ignores the links related to the task and affects the performance of graph analysis 56 ,  57  . As an essential component of the graph, edge features can avoid overlooking crucial feature-relation clues by leveraging the relationships between vertices 58,?,  60  . In recent years, there have been studies using multi-dimensional edge features to describe specific relationships between vertices, such as using heuristic edge features to represent relationships  61  . However, these hand-crafted features still cannot capture specific task relationship information between vertices  62, 63  . This requires a graph representation learning framework to automatically generate task-specific topology and multi-dimensional edge features for different input modalities. Some studies have utilized multiple sub-graphs to learn rich node representations in graph-based networks, proposing two types of edge features and effectively combining them with GAT and GCN models for relationship extraction  64  . Prior to this, multi-dimensional edge features have been widely used in areas  65, 66, 67, 81  . Song et al. proposed a general graph representation learning framework (called GRATIS) that can generate strong graph representations with specific task topologies and multidimensional edge features for arbitrary inputs. Research results show that GRATIS not only greatly enhances pre-defined graphs but also learns strong graph representations of non-graph data, resulting in significant performance improvements across all tasks  33  . We leverage the GRATIS graph representation learning framework as an auxiliary task, fully utilizing information from various speech feature datasets. In this section, we propose a novel graph-based speech feature fusion method, where multi-dimensional edge features are learned to specifically describe the relationship between every pair of speech features.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Method",
      "text": "Overview: The overall architecture is illustrated in Figure  2 . First, our Audio Feature Generation (AFG) module extracts features from the given audio signal, including five hand-crafted features, namely eGeMAPs and MFCCs, two bag-of-words model features, BoAW-e, and BoAW-m, as well as deep learning features, Deep Spectrum. These five features encompass different speech emotional information. The feature extraction is accomplished through two layers of LSTM. Secondly, Due to the varying dimensions of diverse types of speech features, a 2-layer LSTM-RNN (64/32 units) can be employed to achieve a uniform feature vector dimensionality. Thirdly, the GRATIS framework generate graph representations with task-specific topology and multidimensional edge features, since speech features are non-graph data, the input speech features will be represented by a set of vectors and the GRATIS framework connects them into a matrix, by training to obtain the optimal topology. This vector, through edge feature information, contains the association between features, and through point feature information, represents the differences between features. Finally, the vector is fed back into the model as an input, and the end-to-end output is obtained through a fully connected layer.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Hand-Crafted And Deep Learning Speech Features Extraction",
      "text": "Identifying emotions from audio-visual signals usually relies on feature sets, which are extracted based on different aspects that help the model acquire knowledge. In the first module of Figure  2  (AFG), the study achieve the extraction of handcrafted speech features.\n\n• eGeMAPs features using the extended Geneva Minimum Acoustic Parameter Set (eGeMAPs). This set encompasses 88 metrics covering the acoustic dimensions, include spectral, cepstral, prosodic, and speech quality information. • MFCCs features have 13 coefficients, which have better robustness than LPCCs based on vocal tract models, and have been proven to have good recognition performance even when the signal-to-noise ratio decreases  68  . And the extraction of MFCCs was accomplished using the openSMILE2 toolkit 45 , 69 . • Bag-of-words-eGeMAPs (BoAW-e) features are based on the bag-of-words model features of handcrafted features, eGeMAPs LLDs are standardized in an online manner (zero mean, unit variance) before vector quantization. The entire cross-modal BoW (XBoW) processing is executed using the open-source toolkit openXBOW4 chain  72   In summary, a total of five different features can be obtained, the LLDs features contain acoustic features with a small number of dimensions, which can effectively reduce the computational complexity of the subsequent pattern recognition system; whereas the Deep Spectrum features contain more dimensions, which have high computational complexity when inputted to the recognition system but contain spatio-temporal features, such as temporal information, and each has its own advantages and disadvantages. Therefore, we expect to further improve their performance in speech emotion recognition scenarios through feature fusion.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Audio-Feature Temporal Modelling",
      "text": "Before feature fusion, we need to preprocess different types of features, The method comprises two stages. The first stage involves unifying the vector dimensions of each type of speech feature, represented as a graph comprising speech features with the same vector dimensions. This reduces the computational complexity of the subsequent speech emotion recognition model. Second, to consider the temporal sequence when processing the feature vectors, due to the time-series nature of the speech data, the LSTM is more time-sensitive than the ordinary RNN, and it can learn the patterns and features in the time-series data. Therefore, we choose a 2-layer LSTM-RNN to standardize the dimension of speech feature vectors. The 2-layer LSTM layers with different units can better capture the key information between sequential data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Audio Feature Fusion Method",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vertex Feature Extraction",
      "text": "Five different speech features (eGeMAPs, MFCCs, BoAW-M, BoAW-e and DS) are obtained from the input audio feature data,these five speech features are used as the vertex features with the µ. Firstly, The graph representation consists of five vertex features, each representing a speech feature: the hand-crafted features eGeMAPs and MFCCs, the two bag of words features BoAW-e and BoAW-m, and Deep Spectrum features. Each speech feature is processed by two LSTM layers in order to obtain a feature vector µ with a unified dimension of [N, K], the representative speech data has N frames and each feature contains K dimensions. Then, a set of vertex features for each frame can be represented as:\n\nwhere the vertex µ i represents the use of eGeMAPs features processed into feature vectors by two-layer LSTM; The vertex µ j represents the use of MFCCs features processed into feature vectors by two-layer LSTM; The vertex µ k represents the use of BoAW-M features processed into feature vectors by two-layer LSTM; The vertex µ n represents the use of BoAW-e features processed into feature vectors by twolayer LSTM; The vertex µ l represents the use of Deep Spectrum features processed into feature vectors by two-layer LSTM.\n\nThe vertex features are initially fed to a backbone, which may be any suitable machine learning model. In the case of graph data, this may be a GNN, whereas in the case of non-graph data, this may be a Transformer. In this study, we have chosen to use a BiGRU model as the backbone. The composition of the backbone is due to the fact that the BiGRU model introduces a bi-directional structure on top of the GRU model, which better captures the bi-directional dependencies of sequential data, such as speech features. In our implementation, we use GRU as the backbone, to directly extract the global contextual representationY ∈ R A×K×N from the input data, where A denotes the number of vertices. The global contextual representation Y is defined as:",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Graph Definition",
      "text": "We need to use the vertex features µ, the basic adjacency matrix A and edge features β to build its graph representation G A (µ, β). The number of vertex feature extractors is i, each of which consists of a fully connected layer (FC). The vertices set µ contains vectors describing five audio features (each has K dimensions). We treat each vector directly as a vertex, and the basic adjacency matrix concatenating all vectors as the global contextual representation Y . We define edge presence in the basic graph G A according to Rule, These can be formulated as:\n\nwhich is subject to:\n\nwhere e i,j represents the vertex µ i effect on its adjacent vertex µ j is controlled by a single value. Rule represents the basic edge representation between a pair of connected vertex i and vertex j is defined as 1.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Task-Specific Topology And Vertex Features(Ttf)",
      "text": "This section takes the global contextual representation Y , the basic adjacency matrix A and vertex features µ as the input, and generates the task-specific adjacency matrix topology and vertex features. Firstly, we consider the feature vectors as five node features and define the connectivity (vertex-to-vertex relationship) and the connectivity between a pair of nodes µ i and µ j by the similarity of the features(Sim(i, j) = µ T i µ j ). At the same time, we select the K nearest neighbors of each node as its neighbors, thus completing the definition of the graph topology. Then, a GCN layer is employed to jointly update all vertexs from the produced graph, where the i th node's taskspecific vertex feature μi is generated by µ i and its connected nodes as:\n\nwhere Relu is the nonlinear activation function, p and q denote functions of the GCN layer, a i,j represents the relationship between two vertices. After obtaining the optimal task-specific vertex feature representation, we then use this to calculate the vertex-to-vertex similarity to obtain the task-specific adjacency matrix topology Âi,j .\n\nÂi,j = μi μT j (6)",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Audio-Feature Multi-Dimensional Edge Feature Learning (Amef)",
      "text": "To further learn various cues that describe the relationship between speech features and specific tasks, each learned edge is a 1×N dimension, which assign a multi-dimensional to each learned edge feature êi,j ∈ R 1×N . This module is trained under the supervision of target graph analysis tasks. Due to the complex information dimension of speech features, including mathematical information and temporal information, we assume that the relationship between the connected multidimensional vertices μi and μj cannot be best described by one-dimensional edge features. In addition, this relationship clue is not only contained in their vertex features, but also reflected in the global context of the graph. The fusion method takes various speech features µ and global context representation Y as inputs and outputs a multi-dimensional edge feature vector. The Audio-feature Vertex-Context Relationship Modelling (AVCR) block takes vertex features μi and μj and the global contextual representation Y as input. It conducts cross attention between μi and Y as well as μj and Y . Before that, the global contextual representation Y goes through a reshape operation into Ŷ to keep the shape consistent with that of μi and μj . The vertex features μi and μj are independently used as queries to locate vertex-context relationship features F i,x and F j,x in Ŷ . Mathematically speaking, this process can be represented as:\n\nwith the cross-attention operation in AFR defined as:\n\nwhere S q ,S k ,S µ are learnable weight matrices for the encoding of the query, the key and the values, and f k is a scaling factor.\n\nThe Audio-feature Vertex-Vertex Relationship Modelling (AVVR) block further extracts the features. It also conducts the cross-attention (has the same form as Eq. 10 but independent weights) produces features F i,j,x and F j,i,x , where F i,j,x is generated by using F j,x as the query and F i,x as the key and value. Finally, we feed F i,j,x and F j,i,x to a fully-connected (FC) layer to obtain multi-dimensional edge feature vectors e i,j and e j,i , respectively. This process can be represented as:\n\nIn order to better integrate the multidimensional edge feature learning method for multiple speech features, The graph was constructed using the GCN model after the multidimensional edge features were obtained. These features were then combined with the previously obtained task-specific topology and vertex features. The taskspecific vertex features were used to create the task-specific topology, which was then used as input.The task-specific topology is unified with the multidimensional edge feature through multiplication. Prior to this, the task-specific topology is reshaped once, with the reshaped dimension matching that of the multidimensional edge features.\n\nê = e Â (  10 )\n\nwhere μ represents the set of all task-specific vertex features obtained in the TTF section, Â represents the task-specific topology between all vertices.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Feature Fusion And Emotion Recognition",
      "text": "Once the audio-feature relation graph G L = µ L , e L that consists of A node features and A × A multi-dimensional directed edge features is learned, we feed this into the GCN model to jointly recognise all audio data. The next step is the feature fusion, where graphs containing multiple classes of features are fed to through the employed GCN network, and then a final fusion vector is obtained. Specifically, in the reasoning phase, the graph consists of multiple types of speech features, using the number of nodes to determine the structure and size of the output layer, this leads to feature fusion. In initialising the weights, the number of nodes N um is used to initialise the weight matrix. In the step of batch normalisation layer operation, the layers normalised against node features, their dimension is set to N um and in the layers normalised against edge features, their dimension is set to N um × N um as the feature matrix of the edges is two dimensional. In the update step of the nodes, N um is used to normalise the aggregated edge features, i.e. divide by N um. This is because when calculating the update for each node, it is necessary to take into account how many edges point to that node. Compared to traditional feature fusion methods, our method first fuse node features by applying convolution operation on the graph structure, through which not only the features of the node itself are fused, but also the features of the neighbouring nodes are taken into account, and then the edge information is preserved by the method of multidimensional edge features. Secondly, the output layer of our method usually uses a fully connected layer whose output dimension is equal to N um, and then, the features of each node are mapped to different class probabilities. Finally, in contrast to approaches that perform fusion at the feature level and fusion at the decision level, our approach allows dynamic feature fusion at different levels of the graph structure, which provides more flexibility to the model. After completing the above updates, we obtained the final output of the AMEF module µ L , which is a N × N um feature vector, where N is the dimension of the audio data and N um represents the number of classifications set by the model. At the end of the model, we add a linear layer with the purpose of transforming N um in the output of the AMEF module into a feature vector with a value of 1.This step is the fusion of speech features, we fuse five features into a new feature vector and use the feature vector to complete the task of speech emotion recognition.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Training Scheme",
      "text": "The training set and validation set are divided according to cultural types, where the training set is divided into monolingual data sets and multilingual data sets. We use two types of data sets to complete the experiment, and input the fused feature vectors into the 2-layer LSTM-RNN. For time series prediction problems, it is equivalent to setting sequence length to 1. The prediction results are processed by squeeze dimensionality reduction, performing standardization of the prediction results r i and label values r j . Calculate the mean and variance of the output prediction values and label values. As shown in equations (  12 ) and (  13 ):\n\nwhere prediction results is r i and label values is r j , The N represents the number of data points. The correlation coefficient σ rirj is calculated as follows:\n\nThe loss function for training and validation is based on the consistency correlation coefficient (CCC) as shown below:\n\nwhere C represents a constant, and in this study, and C is taken as 2 when calculating CCC loss. The objective of this loss function is to update the model in order to identify the model with the highest CCC score.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiment",
      "text": "This section introduces the experimental details and results. Section 4.1 first presents the dataset. Then, Sections 4.2 and 4.3 introduce Evaluation Metrics and Implementation Details, respectively. Subsection 4.4 explains the advantages of our method compared to the baseline. Finally, subsection 4.5 shows ablation experiments and analyze the accuracy of speech emotion recognition with different fusion features and our fusion features.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Dataset",
      "text": "We conducted extensive experiments on a cross-cultural speech dataset: the SEWA dataset, which was officially authorized for access and use  80  . The SEWA database comprises 199 experimental sessions, involving 398 participants from six distinct cultures (UK, Germany, Hungary, Greece, Serbia, and China). We selected the SEWA data used in the AVEC 2019 Workshop and Challenge, encompassing the German and Hungarian cultures, the emotional dimensions arousal and valence, and liking, were independently assessed by several native speakers  44  .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "To ensure comparable results, we used a training and validation set consistent with Ringeval et al  44  . These datasets were taken from the SEWA dataset, and the training set contained both German and Hungarian cultures, and for training and validation we used three combinations of data: data from the German culture only, data from the Hungarian culture only, and data from both the German and Hungarian cultures for training. cultures, as well as adding new cultures to the test set: China and the UK, in order to evaluate the potential of our approach in cross-cultural speech emotion recognition tasks. In all experiments, we evaluated our model and baseline using a widely adopted evaluation protocol. The consistent correlation coefficient (CCC) was used as the evaluation metric. as the evaluation metric.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Implementation Details",
      "text": "The operating system we used in the experimental environment configuration is Ubuntu 18.04, the development language is python3.9.0, the framework is Py-torch1.11.0+CUDA 11.1, the CPU is AMD EPYC 7742 64-Core Processor, and the GPU is NVIDIA A100-SXM4-40GB. The experimental part is divided into three parts: feature extraction, feature fusion, and recognition. In the detail section of the feature extraction process, the settings proposed by Ringeval et al. were employed in conjunction with the experimental apparatus  44  .We adopted a 2-layer LSTM-RNN (64/32 units) as the time-dependent regressor. We utilized the RMSprop optimizer, implementing a 10% dropout rate over 50 epochs. The model parameter settings were aligned with the literature  72  . And the detailed part of feature fusion and emotion recognition, the network configuration and learning settings of the proposed method are as follows:\n\n• GRU is used as the backbone network to extract features from the input graph representation network. GCN is updated during the end-to-end training of the entire system. • The end-to-end training of the entire network, and batch normalization is applied to each fully connected (FC) layer. RMSprop is used as the optimizer, the initial parameter learning rate was set at 0.005 and after obtaining the experimental results, it was subsequently increased by 0.001 for each experiment up to 0.01. This process was repeated a number of times to determine the best results. • The final classifier consists of a fully connected layer.\n\n• The method we propose is trained in an end-to-end manner, using the fused feature vectors as inputs.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Comparison With Existing Approaches",
      "text": "We examined the performance of the following model. The baseline algorithm primarily draws inspiration from the studies by Ringeval et al, MFCCs, eGeMAPS, BoAW-M, BoAW-e and DS  44  . The reason for using the above results as baseline is that the experimental results of this study are among the better ones in the speech emotion recognition literature using the SEWA dataset in the last few years  70, 71  . Table  2  shows the overall performance compared to the baseline. From the results, we observe that the proposed model consistently outperforms all the baselines, indicating the effectiveness of the fusion method in speech feature fusion. More precisely, the method proposed in this paper improves upon the baseline in the AVEC 2019 Workshop and Challenge: on the SEWA dataset, the CCC scores for German are improved by 17.28% for arousal and 7.93% for liking, and for Hungarian, the CCC scores are improved by 11.15% for arousal and 131.11% for valence. It is noteworthy that the efficacy of our method is less pronounced in the 'DE+HU' dataset, with a mere 2.5% improvement observed in Arousal and a more pronounced decline in Valence compared to the previous baseline. This may be attributed to the existence of significant linguistic differences between languages, which are not adequately captured by the multidimensional edge features. Consequently, the resulting feature representation may be less accurate. This indicates that by integrating speech features through GRU and multi-dimensional edge feature learning, potential feature information can be extracted, thereby enhancing prediction accuracy.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Ablation Analysis",
      "text": "Additionally, a series of ablation studies were conducted to investigate various aspects of the proposed approach. Five distinct methods for fusing audio features were considered, as illustrated in the ablation model below:\n\n1. A fusion model based on five audio features, in which the feature vectors are averaged pairwise (Feature fusion-AVE). 2. Feature Fusion (FC) using fully connected networks (Feature fusion-FC) 3. Only use one-dimensional edge to complete the construction of the graph, and perform feature fusion in vector dimensions (GNN+OD-edge) 4. Use vector stitching to complete feature fusion, and the output dimension of the vector changes from N × K to N × (K * 5)(GNN+ST). 5. Based on the five types of audio features, the output prediction values are averaged to complete the calculation of the result. (Baseline-ave).\n\nTo more accurately assess the extent to which the different modules influence the final result, three distinct data sources were employed for the purpose of speech emotion recognition. The first approach utilises solely the task-specific vertex features derived from the TTF part for linear layer processing, thereby completing the prediction. The second approach employs the original vertex features that have not undergone processing by the TTF module, in conjunction with the initial topology, to complete the AMEF module. Additionally, the new feature vectors derived from the aforementioned training are subjected to linear layer processing, thereby completing the prediction. The training data are subjected to linear layer processing and complete the prediction. The third method is based on the aforementioned complete method and uses the task-specific topology and vertex features as inputs. The new feature vectors obtained after training through the AMEF module are processed in linear layers and complete the prediction.\n\nCCC Score results: As shown in the Table  3 , the results indicate that the fusion method consistently outperforms the models proposed in ablation studies in terms of CCC Scores. The fusion method further improves the fusion of all speech features, most notably by accomplishing dimensionality unification, and the new features do not increase the computational complexity of the emotion recognition model as deep spectral features do. To compare data more intuitively, this study averaged the three emotion classification results (Arousal, Valence, and Liking) of the \"DE+HU\" cross-cultural dataset.\n\nAs shown in Figure  3 , based on the average value, it can be found that our fusion method is 13% better than the results GNN+OD-edge methods. This also reflects that these simple series and parallel methods result in a decrease in model accuracy due to the neglect of edge features. The GNN+ST method and GNN+ODedge method has considered the importance of edge features, so their results are better than the previously mentioned Feature fusion-FC method that only considers point features. However, our multi-dimensional edge feature fusion results are   4 , to demonstrate the efficacy of our method, we utilize t-SNE to illustrate the impact of the aforementioned approach in the context of an ablation experiment. To this end, we employ a dataset comprising 40,032 frames of speech belonging to the \"DE+HU\" group. Each data point is represented by a 10-dimensional output vector, which is subsequently transformed into two dimensions through the use of a dimensionality reduction tool. This transformation allows us to represent the horizontal and vertical coordinates of the points in the graph, respectively. The color of the point is represented by the label value of the point's sentiment category (arousal, validity, and liking). The larger the value, the redder the color, and the smaller the value, the bluer the color. As can be seen from the figure, our method has better clustering results, as the darker points (indicating strong emotions) have higher overlapping coordinates in the plot. In contrast, for methods with suboptimal experimental results, such as the \"Feature fusion-AVE\" group, the distribution of points with strong emotions is more dispersed in the t-SNE plots of arousal and validity.\n\nIn summary, we found that the TTF feature and the AMEF feature achieved good results in the results, while the TTF feature was superior to the AMEF feature. In order to test our research hypothesis that TTF and AMEF can better capture the best relational cues between different types of speech features, we conducted an experiment where TTF features and AMEF features were fused with two, three, and four sets of speech features, respectively.In this fusion, we discarded the speech features that yielded the worst results: three (MFCC, BoWA-M, DS), two (BoWA-M, DS), and one (BoWA-M).By examining the CCC scores of the prediction outcomes, we observed that the scores decreased as the number of retained  prediction performance.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a novel approach to speech feature fusion that employs graphical representation learning and graphical neural networks to model speech features derived from disparate datasets.The objective of this study is to investigate the impact of various speech features on speech emotion recognition in the SEWA dataset. Initially, features are extracted from each frame of the input audio in order to obtain five features that capture different aspects of speech emotion information. Subsequently, these features are processed through a two-layer long short-term memory (LSTM) network to extract vectors. This paper presents an innovative approach to speech feature fusion that uses the multidimensional edge feature approach. This approach combines speech feature vectors into a unified representation that captures the interactions between features through edge information and the uniqueness of features through point information. Furthermore, ablation studies were conducted using different feature merging and fusion methods. The results of the experiments conducted on the SEWA dataset demonstrate the accuracy of the method. Furthermore, two sets of experiments, t-SNE and different combinations, were performed to illustrate the effectiveness of the method.\n\nThe main limitations of this study are: (1) the dataset utilised is limited to the SEWA dataset; and (2) the data features of other modalities, such as video, have not been explored. In the future, we intend to incorporate a wider range of speech features and cross-corpus datasets to further optimise our method.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison between different speech feature fusion methods, with examples shown as",
      "page": 3
    },
    {
      "caption": "Figure 2: The framework of the graph-based multi-feature fusion method. For simplicity, only the",
      "page": 7
    },
    {
      "caption": "Figure 2: First, our Audio",
      "page": 7
    },
    {
      "caption": "Figure 2: (AFG), the study achieve the extraction of handcrafted",
      "page": 8
    },
    {
      "caption": "Figure 3: The values on the vertical axis are the average CCC scores of the three emotional dimen-",
      "page": 17
    },
    {
      "caption": "Figure 3: , based on the average value, it can be found that our",
      "page": 17
    },
    {
      "caption": "Figure 4: , to demonstrate the efficacy of our",
      "page": 18
    },
    {
      "caption": "Figure 4: Ablation experiments on the DE+HU cultural dataset with t-SNE.",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Number of subjects and duration of the video chats",
      "page": 14
    },
    {
      "caption": "Table 2: shows the overall performance compared to the baseline. From the re-",
      "page": 15
    },
    {
      "caption": "Table 2: Baseline results evaluated with CCC for the AVEC 2019 CES(MFCCs,",
      "page": 16
    },
    {
      "caption": "Table 3: , the results indicate that the",
      "page": 17
    },
    {
      "caption": "Table 3: Ablation analysis results evaluated with CCC Score.",
      "page": 18
    },
    {
      "caption": "Table 4: Different combinations of audio features results evaluated with CCC Score.",
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Enforcing semantic consistency for cross corpus emotion prediction using adversarial discrepancy learning",
      "authors": [
        "C.-M Chang",
        "G.-Y Chao",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "2",
      "title": "Vocalizing search: How speech technologies alter consumer search processes and satisfaction",
      "authors": [
        "S Melumad"
      ],
      "year": "2023",
      "venue": "Journal of Consumer Research"
    },
    {
      "citation_id": "3",
      "title": "Aiwac: Affective interaction through wearable computing and cloud technology",
      "authors": [
        "M Chen",
        "Y Zhang",
        "Y Li",
        "M Hassan",
        "A Alamri"
      ],
      "year": "2015",
      "venue": "IEEE Wireless Communications"
    },
    {
      "citation_id": "4",
      "title": "Vision based hand gesture recognition for human computer interaction: a survey",
      "authors": [
        "S Rautaray"
      ],
      "year": "2015",
      "venue": "Artificial intelligence review"
    },
    {
      "citation_id": "5",
      "title": "Learning salient segments for speech emotion recognition using attentive temporal pooling",
      "authors": [
        "X Xia",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Graph isomorphism network for speech emotion recognition",
      "authors": [
        "J Liu",
        "H Wang"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using fourier parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Automatic speech emotion recognition system using mfccbased lpc approach with deep learning cnn",
      "authors": [
        "K Jagadeeshwar",
        "T Sreenivasarao",
        "P Pulicherla",
        "K Satyanarayana",
        "K Lakshmi",
        "P Kumar"
      ],
      "year": "2023",
      "venue": "International Journal of Modeling, Simulation, and Scientific Computing"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Beris: An mbert-based emotion recognition algorithm from indian speech",
      "authors": [
        "P Mehra",
        "S Verma"
      ],
      "year": "2022",
      "venue": "Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised temporal feature learning based on sparse coding embedded boaw for acoustic event recognition",
      "authors": [
        "L Zhang",
        "J Han",
        "S Deng"
      ],
      "year": "2018",
      "venue": "Unsupervised temporal feature learning based on sparse coding embedded boaw for acoustic event recognition"
    },
    {
      "citation_id": "12",
      "title": "Robust sound event classification using deep neural networks",
      "authors": [
        "I Mcloughlin",
        "H Zhang",
        "Z Xie",
        "Y Song",
        "W Xiao"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Soil carbon content prediction using multisource data feature fusion of deep learning based on spectral and hyperspectral images",
      "authors": [
        "X Li",
        "Z Li",
        "H Qiu",
        "G Chen",
        "P Fan"
      ],
      "year": "2023",
      "venue": "Chemosphere"
    },
    {
      "citation_id": "14",
      "title": "Multi-modal depression detection based on emotional audio and evaluation text",
      "authors": [
        "J Ye",
        "Y Yu",
        "Q Wang",
        "W Li",
        "H Liang",
        "Y Zheng",
        "G Fu"
      ],
      "year": "2021",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "15",
      "title": "Deepspectrumlite: A power-efficient transfer learning framework for embedded speech and audio processing from decentralized data",
      "authors": [
        "S Amiriparian",
        "T Hübner",
        "V Karas",
        "M Gerczuk",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition based on image enhancement",
      "authors": [
        "D Wang",
        "J Dong",
        "D Zhou",
        "X Wei",
        "Q Zhang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE"
    },
    {
      "citation_id": "18",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "M Jain",
        "S Narayan",
        "P Balaji",
        "A Bhowmick",
        "R Muthu"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition using support vector machine",
      "arxiv": "arXiv:2002.07590"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition using acoustic features and textual content",
      "authors": [
        "Z.-J Chuang",
        "C.-H Wu"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition approaches: A systematic review",
      "authors": [
        "A Hashem",
        "M Arif",
        "M Alghamdi"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "25",
      "title": "Trap of feature diversity in the learning of mlps",
      "authors": [
        "D Liu",
        "S Wang",
        "J Ren",
        "K Wang",
        "S Yin",
        "H Deng",
        "Q Zhang"
      ],
      "year": "2021",
      "venue": "Trap of feature diversity in the learning of mlps",
      "arxiv": "arXiv:2112.00980"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition from human speech using temporal information and deep learning",
      "authors": [
        "J Kim",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Stressed speech emotion recognition using teager energy and spectral feature fusion with feature optimization",
      "authors": [
        "S Bandela",
        "S Siva Priyanka",
        "K Sunil",
        "Y Kumar",
        "A Vijay Bhaskar Reddy",
        "Berhanu"
      ],
      "year": "2023",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "28",
      "title": "Machine learning approach of speech emotions recognition using feature fusion technique",
      "authors": [
        "B Paul",
        "S Bera",
        "T Dey",
        "S Phadikar"
      ],
      "year": "2023",
      "venue": "Machine learning approach of speech emotions recognition using feature fusion technique"
    },
    {
      "citation_id": "29",
      "title": "A multi-feature fusion speech emotion recognition method based on frequency band division and improved residual network",
      "authors": [
        "Y Guo",
        "Y Zhou",
        "X Xiong",
        "X Jiang",
        "H Tian",
        "Q Zhang"
      ],
      "year": "2023",
      "venue": "A multi-feature fusion speech emotion recognition method based on frequency band division and improved residual network"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition framework using a decision-level fusion and feature-level fusion approach",
      "authors": [
        "C Devi",
        "D Renuka"
      ],
      "year": "2023",
      "venue": "IETE Journal of Research"
    },
    {
      "citation_id": "31",
      "title": "Developing a physiological signal-based, mean threshold and decision-level fusion algorithm (pmd) for emotion recognition",
      "authors": [
        "Q Zhang",
        "H Zhang",
        "K Zhou",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "Tsinghua Science and Technology"
    },
    {
      "citation_id": "32",
      "title": "Conversational emotion recognition using self-attention mechanisms and graph neural networks",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang",
        "Z Yang",
        "R Li"
      ],
      "year": "2020",
      "venue": "Conversational emotion recognition using self-attention mechanisms and graph neural networks"
    },
    {
      "citation_id": "33",
      "title": "Gratis: Deep learning graph representation with task-specific topology and multi-dimensional edge features",
      "authors": [
        "S Song",
        "Y Song",
        "C Luo",
        "Z Song",
        "S Kuzucu",
        "X Jia",
        "Z Guo",
        "W Xie",
        "L Shen",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "Gratis: Deep learning graph representation with task-specific topology and multi-dimensional edge features",
      "arxiv": "arXiv:2211.12482"
    },
    {
      "citation_id": "34",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition using scalogram based deep structure",
      "authors": [
        "K Aghajani",
        "I Esmaili Paeen Afrakoti"
      ],
      "year": "2020",
      "venue": "International Journal of Engineering"
    },
    {
      "citation_id": "36",
      "title": "Sex differences in the timing of identification among children and adults with autism spectrum disorders",
      "authors": [
        "S Begeer",
        "D Mandell",
        "B Wijnker-Holmes",
        "S Venderbosch",
        "D Rem",
        "F Stekelenburg",
        "H Koot"
      ],
      "year": "2013",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition: a comprehensive survey",
      "authors": [
        "M Al-Dujaili",
        "A Ebrahimi-Moghadam"
      ],
      "year": "2023",
      "venue": "Wireless Personal Communications"
    },
    {
      "citation_id": "38",
      "title": "A distributed optimisation framework combining natural gradient with hessian-free for discriminative sequence training",
      "authors": [
        "A Haider",
        "C Zhang",
        "F Kreyssig",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition based on dnn-decision tree svm model",
      "authors": [
        "L Sun",
        "B Zou",
        "S Fu",
        "J Chen",
        "F Wang"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "40",
      "title": "Implementation and comparison of speech emotion recognition system using gaussian mixture model (gmm) and k-nearest neighbor (k-nn) techniques",
      "authors": [
        "R Lanjewar",
        "S Mathurkar",
        "N Patel"
      ],
      "year": "2015",
      "venue": "Procedia computer science"
    },
    {
      "citation_id": "41",
      "title": "Svm-based feature se-Graph-based multi-Feature fusion method for speech emotion recognition 23 lection methods for emotion recognition from multimodal data",
      "authors": [
        "C Torres-Valencia",
        "M Álvarez-López",
        "Á Orozco-Gutiérrez"
      ],
      "year": "2017",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "42",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovská",
        "M Jakubec",
        "R Jarina",
        "M Chmulík"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "43",
      "title": "A cross-cultural study on emotion expression and the learning of social norms",
      "authors": [
        "S Hareli",
        "K Kafetsios",
        "U Hess"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "44",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Proceedings"
    },
    {
      "citation_id": "45",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for speech research and affective computing",
      "authors": [
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Collective classification in network data",
      "authors": [
        "P Sen",
        "G Namata",
        "M Bilgic",
        "L Getoor",
        "B Galligher",
        "T Eliassi-Rad"
      ],
      "year": "2008",
      "venue": "AI magazine"
    },
    {
      "citation_id": "47",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Z Wu",
        "S Pan",
        "F Chen",
        "G Long",
        "C Zhang",
        "S Philip"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "48",
      "title": "Multi-dimensional graph convolutional networks",
      "authors": [
        "Y Ma",
        "S Wang",
        "C Aggarwal",
        "D Yin",
        "J Tang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 siam international conference on data mining"
    },
    {
      "citation_id": "49",
      "title": "Graph fusion network for text classification",
      "authors": [
        "Y Dai",
        "L Shou",
        "M Gong",
        "X Xia",
        "Z Kang",
        "Z Xu",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "50",
      "title": "Exploiting edge features for graph neural networks",
      "authors": [
        "L Gong",
        "Q Cheng"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "51",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "52",
      "title": "On inductive-transductive learning with graph neural networks",
      "authors": [
        "G Ciano",
        "A Rossi",
        "M Bianchini",
        "F Scarselli"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "53",
      "title": "Edgenets: Edge varying graph neural networks",
      "authors": [
        "E Isufi",
        "F Gama",
        "A Ribeiro"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "55",
      "title": "Benchmarking graph neural networks",
      "authors": [
        "V Dwivedi",
        "C Joshi",
        "A Luu",
        "T Laurent",
        "Y Bengio",
        "X Bresson"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "56",
      "title": "Automating the construction of internet portals with machine learning",
      "authors": [
        "A Mccallum",
        "K Nigam",
        "J Rennie",
        "K Seymore"
      ],
      "year": "2000",
      "venue": "Information Retrieval"
    },
    {
      "citation_id": "57",
      "title": "Edge-featured graph neural architecture search",
      "authors": [
        "S Cai",
        "L Li",
        "X Han",
        "Z -J. Zha",
        "Q Huang"
      ],
      "year": "2021",
      "venue": "Edge-featured graph neural architecture search",
      "arxiv": "arXiv:2109.01356"
    },
    {
      "citation_id": "58",
      "title": "Multi-dimensional edge features graph neural network on few-shot image classification",
      "authors": [
        "C Xiong",
        "W Li",
        "Y Liu",
        "M Wang"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "59",
      "title": "Learning person-specific cognition from facial reactions for automatic personality recognition",
      "authors": [
        "S Song",
        "Z Shao",
        "S Jaiswal",
        "L Shen",
        "M Valstar",
        "H Gunes"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Personality recognition by modelling person-specific cognitive processes using graph representation",
      "authors": [
        "Z Shao",
        "S Song",
        "S Jaiswal",
        "L Shen",
        "M Valstar",
        "H Gunes"
      ],
      "year": "2021",
      "venue": "proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "61",
      "title": "Learning coarse-to-fine graph neural networks for video-text retrieval",
      "authors": [
        "W Wang",
        "J Gao",
        "X Yang",
        "C Xu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "62",
      "title": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "C Luo",
        "S Song",
        "W Xie",
        "L Shen",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "arxiv": "arXiv:2205.01782"
    },
    {
      "citation_id": "63",
      "title": "Multi-dimensional edge-based audio event relational graph representation learning for acoustic scene classification",
      "authors": [
        "Y Hou",
        "S Song",
        "C Yu",
        "Y Song",
        "W Wang",
        "D Botteldooren"
      ],
      "year": "2022",
      "venue": "Multi-dimensional edge-based audio event relational graph representation learning for acoustic scene classification",
      "arxiv": "arXiv:2210.15366"
    },
    {
      "citation_id": "64",
      "title": "Contextualised graph attention for improved relation extraction",
      "authors": [
        "A Mandya",
        "D Bollegala",
        "F Coenen"
      ],
      "year": "2020",
      "venue": "Contextualised graph attention for improved relation extraction",
      "arxiv": "arXiv:2004.10624"
    },
    {
      "citation_id": "65",
      "title": "Smef: Social-aware multidimensional edge features-based graph representation learning for recommendation",
      "authors": [
        "X Liu",
        "S Meng",
        "Q Li",
        "L Qi",
        "X Xu",
        "W Dou",
        "X Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "66",
      "title": "Me-gcn: multi-dimensional edge-embedded graph convolutional networks for semi-supervised text classification",
      "authors": [
        "K Wang",
        "S Han",
        "S Long",
        "J Poon"
      ],
      "year": "2022",
      "venue": "Me-gcn: multi-dimensional edge-embedded graph convolutional networks for semi-supervised text classification",
      "arxiv": "arXiv:2204.04618"
    },
    {
      "citation_id": "67",
      "title": "Revisiting graph based social recommendation: A distillation enhanced social graph network",
      "authors": [
        "Y Tao",
        "Y Li",
        "S Zhang",
        "Z Hou",
        "Z Wu"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM Web Conference 2022"
    },
    {
      "citation_id": "68",
      "title": "Combining evidence from residual phase and mfcc features for speaker recognition",
      "authors": [
        "K Murty",
        "B Yegnanarayana"
      ],
      "year": "2006",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "69",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "70",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "71",
      "title": "Investigating word affect features and fusion of probabilistic predictions incorporating uncertainty in avec 2017",
      "authors": [
        "T Dang",
        "B Stasak",
        "Z Huang",
        "S Jayawardena",
        "M Atcheson",
        "M Hayat",
        "P Le",
        "V Sethu",
        "R Goecke",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge, AVEC '17"
    },
    {
      "citation_id": "72",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "73",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "venue": "Proceedings of the 21st"
    },
    {
      "citation_id": "75",
      "title": "Graph-based multi-Feature fusion method for speech emotion recognition 25",
      "year": "2013",
      "venue": "ACM International Conference on Multimedia, MM '13"
    },
    {
      "citation_id": "76",
      "title": "openXBOW -Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "openXBOW -Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
      "arxiv": "arXiv:1605.06778"
    },
    {
      "citation_id": "77",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Publisher Copyright: Copyright © 2017 ISCA.; 18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "78",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "A Baird",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Snore sound classification using image-based deep spectrum features"
    },
    {
      "citation_id": "79",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "80",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "81",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "82",
      "title": "Reversible graph neural network-based reaction distribution learning for multiple appropriate facial reactions generation",
      "authors": [
        "T Xu",
        "M Spitale",
        "H Tang",
        "L Liu",
        "H Gunes",
        "S Song"
      ],
      "year": "2019",
      "venue": "Reversible graph neural network-based reaction distribution learning for multiple appropriate facial reactions generation",
      "arxiv": "arXiv:2305.15270"
    }
  ]
}