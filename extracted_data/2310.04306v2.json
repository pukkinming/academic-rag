{
  "paper_id": "2310.04306v2",
  "title": "Towards A Robust Group-Level Emotion Recognition Via Uncertainty-Aware Learning",
  "published": "2023-10-06T15:05:41Z",
  "authors": [
    "Qing Zhu",
    "Qirong Mao",
    "Jialin Zhang",
    "Xiaohua Huang",
    "Wenming Zheng"
  ],
  "keywords": [
    "Group-level emotion recognition",
    "Robust representation learning",
    "Uncertainty learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty, we adopt stochastic embedding sourced from a Gaussian distribution instead of deterministic point embedding. It helps capture the probabilities of emotions and facilitates diverse inferences. Additionally, we adaptively assign uncertainty-sensitive scores as the fusion weights for individuals' faces within a group. Moreover, we developed an image enhancement module to evaluate and filter samples, strengthening the model's data-level robustness against uncertainties. The overall three-branch model, encompassing face, object, and scene components, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final grouplevel output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Group-Level Emotion Recognition",
      "text": "Compared to individual-level emotion recognition, GER involves comprehending complex emotions expressed by multiple individuals. Various efforts have been made to enhance GER performance by harnessing diverse emotion-related information from multiple sources and subsequently aggregating individual features into group-level insights  [10, [15] [16] [17] . Among these sources, facial feature learning holds prominence in influencing the inference of the ultimate group-level emotion. This is primarily due to facial expressions serving as the most explicit signals that convey emotional states during human interaction  [18] . Facial features have been employed to estimate happiness intensity within a group  [19, 20] .  Khan et al.  proposed a four-stream hybrid network, incorporating a multi-scale face stream to handle variations in face size and exploring distinct global streams to capture scene information  [21] . Notably, facial information plays a pivotal role in recognizing strong group emotions such as positive and negative. In the course of GER's evolution and in-depth research, recent studies suggest that, apart from facial features, additional information stemming from group-related factors, including objects and scenes, holds potential for enhancing GER. Fujii et al. adopted a hierarchical classification approach, where facial expression features initially underwent binary classification, followed by incorporation of object and scene information into GER  [14] . Guo et al. devised a Graph Neural Network (GNN) to leverage emotional cues from faces, objects, scenes, and skeletons  [7] . The advancements in GER research underscore the increasing recognition of the significance of information beyond facial features, indicating that elements like scene context and object interactions have valuable contributions to make in improving GER accuracy and comprehensiveness.\n\nTo integrate diverse individual contributions, certain traditional approaches have employed arithmetic-based methods such as averaging  [22] [23] [24]  or voting  [11, 25, 26] . Rassadin et al. developed a strategy involving multiple classifiers to derive the GER outcome by averaging the facial expressions of individuals, their facial landmarks, and the corresponding scene features  [27] . Dejian et al. combined facial features from individuals and inputted them into the fully-connected (FC) layer to create a representation of facial features utilized for group-level emotion recognition  [28] . With the advent of deep learning, some approaches turned to Recurrent Neural Networks (RNNs) and their variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks  [12, 29, 30] . Bawa et al. utilized an LSTM-based approach to aggregate facial features extracted from various regions of the image  [29] . Additionally, several methods introduced Graph Neural Networks (GNNs) to leverage discriminative emotion representation and capture correlations among individuals  [7, 13] . Notably, the recent work by Fujii et al.  [14]  introduced an attention mechanism to assess the relative importance of individuals within a group. A new perspective emerges as Zhang et al. present a contrastive learning-based semi-supervised framework for group emotion recognition, aimed at efficiently extracting features from both labeled and unlabeled images  [31] .\n\nIndeed, GER research extends beyond acquiring diverse emotion-related information to manage factors that may not contribute effectively to emotion representation. Thus, our work is equally dedicated to aggregating rich information from individuals (face and object) and scenes. However, the crucial distinction lies in our method's objective, which is to enhance the diversity and robustness of representations by explicitly modeling uncertainty among individuals for GER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Learning With Uncertainties",
      "text": "The concept of uncertainty learning has garnered significant attention in the computer vision domain due to its effectiveness in learning robust and interpretable features. Within the Bayesian framework, uncertainty learning can be broadly categorized into two types: model uncertainty and data uncertainty. Data uncertainty pertains to the inherent noise present in training data, capturing uncertainties originating from data noise. On the other hand, model uncertainty arises from the lack of knowledge concerning potential noise in model parameters. Numerous tasks have embraced the integration of uncertainty to enhance model robustness and interpretability. This trend is evident across various domains, including face recognition  [32] , semantic segmentation  [33, 34] , and ReID tasks  [35, 36] .\n\nIn visual classification tasks, which align closely with our current objective, prior works predominantly address uncertainty through two primary avenues. One is focused on designing a mechanism to quantify uncertainty to address challenges arising from low-quality samples and label noise. Wang et al. proposed the SCN to suppress the impact of uncertainties via a self-attention mechanism to weight each sample with a ranking regularization. She et al. employed a method to model latent label distribution of input samples and identify uncertain samples through a cosine similarity learning branch  [37] . Otherwise, in human-centered visual tasks  [36, [38] [39] [40] , several studies have employed uncertaintyaware learning to model uncertainty, particularly utilizing the method of modeling samples as Gaussian distribution to address the challenges like congestion, occlusion, illumination changes, and inconsistent predictions. Specifically, to handle noisy face images, Chang et al. employed a Gaussian distribution estimate for individual face images in the latent space and acquired identity features (mean) while also capturing the uncertainty (variance) associated with the estimated mean  [38] . Dou et al. proposed an uncertainty-aware learning method for jointly learning data uncertainty and model uncertainty in a unified network, simultaneously addressing both issues of low-quality samples and inconsistent predictions in the person image retrieval task  [36] . Zhao et al. proposed a contextaware contrastive loss to learn more robust representations and introduced data uncertainty learning to distinguish congested and overlapped pedestrians  [39] .\n\nSimilar with the typical setting of learning in the uncertainty learning area, we employ the Gaussian sampling for generating stochastic representations and employing KL divergence for regularization. on this basis, we introduce a reconstruction loss to regularize uncertainty-sensitive scores generated through covariance approximation. This introduction of the reconstruction loss results in a similarity to VAE. To further clarify the differences between our approach and VAEs, it is important to highlight two key distinctions: 1) For Representation Learning: VAEs generate a latent space representation of inputs by encoding them into a distribution, typically Gaussian, and then sampling from this distribution to reconstruct the input. Our method also uses Gaussian sampling, but unlike VAEs, our focus is not on reconstructing the original input. Instead, our goal is to produce a robust emotional representation, where the emphasis is on capturing the variability and uncertainty in group emotion recognition. 2) For Regularization Techniques: In VAEs, the KL divergence is used as a regularization term to ensure that the latent distribution closely matches a prior distribution, aiding in the reconstruction of inputs. Our approach employs KL divergence for regularization, but the crucial difference lies in our unique regularization term, a reconstruction loss term, designed to stabilize training. This term aims to control the instability in training due to uncertainty fluctuations, rather than ensuring the fidelity of reconstructed data to the input.\n\nActually, nearly all existing GER methods represent grouplevel emotions deterministically, which leads to a lack of research on uncertainty approximation within the realm of GER. Our approach is inspired by the interaction of the aforementioned aspects. Building upon the generation of stochastic representations using Gaussian distributions, distinct from prior art, we design a mechanism to produce uncertainty scores, enabling adaptive fusion weights in real-world scenarios characterized by high uncertainty, thereby enhancing the robustness of group-level emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "The goal of the proposed method is to recognize group-level emotion in a crowd scene by aggregating more robust emotion features. With the estimated uncertainty, the representation of each individual is based on probabilistic distribution to infer the group-level emotion. In this section, we first present all descriptors of our method. Then we emphatically detail an uncertainty-aware learning module and apply it to model faces and objects. Finally, we describe a simple but effective image enhancement module and the proportional-weighted fusion module used to aid in GER.\n\nThe proposed framework including the UAL module is illustrated in Fig.  2 . The overview is shown in Fig.  2(a) , which consists of face-level, object-level, and scene-level branches for GER. Given an image, firstly, the detectors are utilized to generate a set of face and object proposals. A simple image enhancement module is attached to the face branch. Second, the CNN-based feature extractors are used to extract local representations for each face and object on face and object branches and global representation in the scene branch, respectively. Next, the extracted individual features are mapped into the corresponding distributions by using the UAL module. Then, in the inference stage, the Monte Carlo sampling operation is used to obtain the diversity prediction of the individual. After that, the individual features output from the UAL module are aggregated as the corresponding final group-level predictions. Finally, the predicted emotion categories of the three branches are fused by a proportionalweighted fusion module to refine the final prediction of the input image.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Feature Extractor",
      "text": "To recognize the group-level emotion, we solve it by recognizing individual emotion and scene emotion simultaneously. For individual emotion recognition, we need to drive the detector to obtain face and object images as a premise. In this work, we adopt MTCNN  [41]  as the detector to obtain faces, which is a multi-task cascaded convolutional network widely used in face and landmark detection. For the acquisition of objects, the Faster R-CNN  [42]  is utilized to generate a set of object proposals and train on the MSCOCO dataset.\n\nTo capture not only the local individual representation but the global scene representation, we utilize three distinct encoders to severally extract the features for each branch. For the face branch, we select ResNet18 pre-trained on the MS-Celeb-1M  [43]  dataset as the encoder as it was recently efficiently used for the facial expression recognition tasks  [44] [45] [46]  while achieving remarkable results on uncertainty estimation benchmarks. For object and scene branches, we adopt the VGG19 network pre-trained with the ImageNet dataset as the encoder. We define the output of three feature extractor corresponding branches as x f i , x o i and x s , which represent the extracted face feature of the ith cropped face image, object feature of the ith cropped object image, and scene feature of the corresponding whole image, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Uncertainty Modeling",
      "text": "The UAL module is illustrated in Fig.  2(b-d ). Conventionally, the individual feature obtained by the feature extractor is represented as a deterministic point in space. However, it is difficult to estimate an accurate point embedding for the individuals affected by uncertain factors, which is attributed to the complexity of the GER datasets collection scenarios. Furthermore, only a single group-level label is available in the GER task, which means that every individual represents the common emotion category in a group. Nevertheless, individuals in a group may perform emotion with different forms and may spring up several possible emotion categories, which are reflected by the uncertainty, in other words. The conventional GER methods cannot naturally express the uncertainty and distribution of individuals and are unable to effectively quantify the diversity of emotion prediction. Therefore, UAL is proposed to address this issue. UAL is implanted in the face and object branches to reduce the interference of uncertainty for more robust feature learning. Uncertainty Embedding. To explicitly represent the individual feature and the uncertainty simultaneously, the individual feature is modeled as a multivariate Gaussian distribution. In particular, we define the representation z n in latent space of n-th individual x I n (i.e., x f i , x o i ) as a Gaussian distribution,\n\nwhere µ n ∈ R D and σ 2 n ∈ R D represent mean vector and diagonal covariance matrix, respectively. D is the individual representation length. The µ n and σ 2 n are input-dependent predicted by two separate feature projectors by:\n\nwhere θ µ and θ σ denote corresponding probabilistic parameters respectively w.r.t. output µ n and σ n . The predicted Gaussian distribution is a diagonal multivariate normal distribution. The µ n can be regarded as an individual feature, while σ n refers to the intensity of the embedding variance that represents the uncertainty of the individuals. Now, the representation of each individual serves as a stochastic instead of a deterministic point embedding sampled from N (z n ; µ n , σ 2 n I). However, the sampling operation is not differentiable for training these probabilistic parameters through the gradient descent, we adopt the re-parameterization trick  [47]  to ensure the operation of back-propagation. To further capture the diversity and uncertainty present in the underlying data distribution, we utilize Monte-Carlo sampling to obtain the final generated stochastic feature representations z * n :\n\nwhere ϵ m is a random number sampled from a normal distribution, M is the number of samples drawn from the Gaussian distribution N (0, I). The representations z * n obtained through Monte Carlo sampling are generated by sampling from a Gaussian distribution, with each sampling iteration producing distinct feature representations. Indeed, during the training phase, the µ n and σ n learned by the f θµ ( * ) and f θσ ( * ) in Eq. 2 and Eq. 3 while fixed during the testing phase, as the model parameters have reached their optimal values through training. The repetitive sampling process, yielding diverse representations, aids in quantifying uncertainty, and addressing inherent uncertainties in the data, and enhances the model's robustness. Uncertainty-sensitive score. Based on Eq. 4, we obtain the random variable z * n as a stochastic representation sample instead of the µ n sampling from the original distribution. The uncertainty-aware learning module is proposed primarily to decrease the disturbance of the uncertain individuals in the image. Hence, in the face branch, we formulate the uncertainty-sensitive score as the source of the weight for the corresponding face individuals. Specifically, the uncertaintysensitive score is computed by the harmonic mean of the Hadamard product of the estimated variance σ n and random noise ϵ. We denote the uncertainty-sensitive score as s n , which acquired by:\n\nwhere σ n,d and ϵ n,d represent the d-th compositions of σ n and ϵ n , respectively. Hence, a face individual with higher s n generally corresponds to the larger uncertainty and the other way around.\n\nOnce the uncertainty-sensitive score of each face individual is estimated, it is regarded as the criterion of weight. To be specific, we project the uncertainty-sensitive score s n to α n = β n s min + (1 -β n )s max , where β n = sn-smin smax-smin and the s min and s max represent the maximum and minimum value of s n , respectively. Here, we note α n as the importance scalar for each face individual in a group. Hence, our model can adaptively assign weights to every face individual, downweighting the individual with the high uncertainty-sensitive score.\n\nThe weighted group-level features in the face branch can be expressed as:\n\nwhere x f group are the final group-level representations in the face branch. Apparently, the importance scalar α n plays a role similar to the attention mechanism, enabling the grouplevel representation is not disturbed by individuals with large uncertainty to a great extent.\n\nSince the object individuals in a group cannot express emotions as intuitively as human faces and to better compare object individuals with the same semantic information between different groups, we directly predict the emotion of a single sampled object individual and average the sum of all object individual predictions as the group-level emotion prediction.\n\nUncertainty-aware Loss. Since x f group is the final group-level representation in the face branch, we feed it to a classifier to minimize the following softmax loss, which is formulated as,\n\nwhere W c is the c-th classifier and C is the number of emotion categories.\n\nFor the object branch, we treat the µ n as the original deterministic representation and feed it into the classifier along with the sampled stochastic representation z * n to greatly enrich the semantic information in the object branch. The classification loss is formulated as:\n\nNevertheless, only the L cls series is employed to constrain the model for classification that easily falls into the trivial solution and reverts our distribution-based embedding back into the deterministic embedding. Hence, it is necessary to constrain ϵ to avoid the trivial solution by outputting negligible uncertainties. This problem can be alleviated by introducing the regularization term KL divergence during the optimization, it explicitly bounds the learned distribution N (µ n , σ 2 n I) from the normal distribution N (0, I). This KL divergence term is:\n\nFurthermore, to explicitly constrain the importance scalar of each face individual, we sort the α n in a high-to-low order. Then, similar to  [44]  the sorted face individuals are divided into two groups of high and low importance according to a ratio β. A margin is used to ensure that the average values of the two groups are maintained in their present size order. Here, the rank regularization loss can be formulated as:\n\nInspired by  [40] , we introduce a regularization loss, denoted as L rec , to mitigate excessive oscillations in uncertaintysensitive scores due to the approximation of variance, which may lead to unstable model training. Specifically, L rec is computed as the difference between the original face individual feature µ n and the sampling z * n , formally defined as\n\nThe goal of L rec is to preserve the content information of the original feature µ n to the greatest extent possible.\n\nIt is important to clarify that while our method incorporates a regularization approach and a loss component that may resemble those in VAE, there are fundamental differences in their applications. Our model's primary objective is not to reconstruct the input from its latent representation. Instead, there are two main purposes for designing the L rec : One is to serve as a regularized term to mitigate instabilities caused by fluctuations in uncertainty. Another is to learn a more stable and deterministic representation of the input data. We aim to leverage the benefits of uncertainty-aware learning  [36, 38]  while ensuring stability and convergence in the training process.\n\nTo sum it up, the final loss functions for training the network with the joint loss function of uncertainty and recognition in the face and object branches are severally formulated as follows:\n\nIn addition, for the scene branch, the cross-entropy loss is used for the training stage.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Image Enhancement Module",
      "text": "In complex scenarios with diverse people, uncertainties make it hard to get reliable facial data and undermine face detection systems, often leading to low-quality samples entering the face individual branch. To solve this, we developed an image enhancement module. It uses face quality assessment as a preliminary safeguard to suppress and filter out bad face samples at the data level, acting like data augmentation and improving dataset quality. But just filtering isn't enough. Despite removing many poor samples by scores, it can't handle all uncertainties like lighting, individual differences and emotions. Also, a strict threshold would create a sparse dataset, limiting training as good data is already scarce. So, this module works with the uncertainty modeling module, with it enhancing input quality first and the latter handling remaining uncertainties for more robust GER.\n\nConcretely, we adopt a SER-FIQ face quality assessment  [48] . This methodology efficiently filters out nearly unrecognizable facial samples before passing them to the feature extractor, thus enhancing the quality of the input samples in a unique manner. This ensures that only data of the highest possible reliability and quality is utilized, thereby enhancing the robustness and performance of the model.\n\nFormally, given a face sample I, the face quality score s(I) can be obtained by the pre-trained face recognition model. Let F raw = {I N } denote N raw facial samples directly detected from the image. The quality scores S = {s 1 , s 2 , • • • , s N } of F raw are acquired by using face quality assessment strategy in  [48] . Concretely, the quality of an image is estimated by calculating the pairwise distances between different stochastic embedding, which is obtained through different random sub-networks of face recognition. The face quality score is formulated as,\n\nwhere S k is the quality score corresponding to the k-th face image. X(•) is a set with m face embeddings acquired from different face recognition model. d(x ki , x kj ) means the Euclidean distance between the randomly selected embeddings pairs x ki and x kj . Especially, the image enhancement module equals face image quality estimation (FIQE) to fetch the samples with the high score by setting a threshold. The filtered F input can be defined as,\n\nwhere F input is the facial set as the final input of the face branch, which the severely low-quality samples have been discarded, δ 2 is a pre-defined threshold.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Proportional-Weighted Fusion Strategy",
      "text": "The previous methods  [7, 14, 21]  on the GER task demonstrated the improvements in performance that can be obtained by fusing multiple emotion cues of different compositions which contain complementary information. Similarly, we incorporate three branches (i.e., face, object, and scene) into the proposed framework, which from the global and local perspectives acquire group-level emotion features. Given the group-level features extracted from each branch (i.e., x f group , x o group , x s group ), the prediction scores for GER via the classifier are obtained,\n\n, where sc f , sc o , and sc s are prediction scores for face, object, and scene branch, respectively. C f (•), C o (•), and C s (•) are the corresponding classifiers.\n\nThe extensive score-level fusion strategy mainly focuses on two forms.  (1)  The first is to fuse scores by using arithmetic- based measures, i.e., a weighted average.  (2)  The second is to employ a grid search approach to find the optimal fusion weights. Crucially, both strategies are required to learn fusion weights empirically from the validation set and fixed when testing. However, the label of the test set for whole GAFF databases in our experiment is available only for those participating in the EmotiW competitions, we cannot but obey the official protocol to use the validation set to verify the performance of our model. Hence, neither of the abovementioned two fusion strategies is usable for our method.\n\nIn our problem, we design a proportional-weighted fusion strategy (PWFS) to account for the corresponding proportion as a weight for each branch which is able to adequately utilize the potential and complementary information. Specifically, we approximately calculate the final group-level emotion prediction score as:\n\nwhere w f , w o , and w s are the proportion of each corresponding branch prediction score to the sum of the whole branches. And sc group represents the final classification score, in which every branch score is integrated by the proportional weight. Furthermore, due to the small size of the GER dataset, the proportional-weighted fusion separately processes each branch for GER which can avoid causing the fusion results to be worse than a single branch attributed to over-fitting. To reason more independently produces better results.\n\nIV. EXPERIMENTAL AND DISCUSSION A. Databases and Evaluation Metrics GAFF Databases. The GAFF databases consist of two series benchmark databases: the Group AFFective 2.0 (GAFF2)  [49]  database and the Group AFFective 3.0 (GAFF3)  [50]  database. All samples in GAFF databases are collected from the Internet by searching for keywords such as protest, violence, festival, etc, and each sample contains at least two people. The statistics of the two databases are summarized in Table  I . The total number of images for each category is listed in this table. All the samples are annotated with three emotion categories: positive, negative, and neutral. As the label is not released in the test set, and while only available to those participating in the EmotiW competitions  [49, 50] . We only conduct all experiments on the training and validation sets. More specifically, we train our model on the training set and use the validation set to verify the performance of our model. MultiEmoVA Database. The MultiEmoVA database  [51]  was collected by using keywords such as graduation, party, etc, from Google Images and Twitter. It was fused by arousallevel and valence-level to annotate six categories as highpositive, medium-positive, high-negative, medium-negative, characteristics into the following types: arithmetic averaging  [22] [23] [24] , voting  [11, 25, 26] , Recurrent Neural Networks (RNNs)  [12, 29, 30] , and attention mechanisms  [14] . It is worth noting that our method shares similar with attention-based approaches, as both aim to learn efficient fusion weights for aggregating individual features. However, our method explicitly models uncertainty among individuals and approximates the uncertainty of each individual as weights for aggregating individuals, enhancing the diversity and robustness of representations to GER.\n\n1) Results on the GAFF2 dataset: The results on the GAFF2 dataset are reported in Table  II . From Table  II , our proposed method outperforms nearly all SOTA approaches, with only slight variations compared to  [14]  in certain emotion categories. Specifically, our method exhibits a marginal decrease in performance, with average decreases of 0.87%, 1%, and 0.72% in terms of Recall, Precision, and F-measure, respectively. It is noteworthy that  [14]  employs a hierarchical classification approach. Initially, it conducts binary classification to determine whether a sample is \"Positive\" or \"Not Positive\" based on facial expression features. Subsequently, for samples categorized as \"Not Positive,\" they further conduct the final three-class emotion recognition task using scene and object emotion features. Therefore,  [14]  achieve superior metrics for the \"Positive\" category, attributed to its hierarchical classification approach effectively distinguishing \"Positive\" images. Additionally, their approach sacrifices high computational power to extract more useful information in the object branch, contributing to their superior performance. In contrast, our method not only abstains from using a hierarchical framework but also solely employs a straightforward CNN-based network as the encoder to acquire object information, significantly reducing computational demands. As comparative analysis of method complexity between our approach and  [14]  is conducted in Table  V . This analysis illustrates that our approach achieves a competitive recognition performance while simultaneously reducing execution time compared to  [14] . Furthermore, our method surpasses several RNNs-based methods, such as  [29, 30] , indicating that our method's ability to discriminatively learn the importance of different individuals and utilize this information to aggregate individual features is crucial for achieving high performance in recognition accuracy.\n\n2) Results on the GAFF3 dataset: We further provide detailed comparisons with previous methods listed in Table  III  on the GAFF3 dataset. From Table  III , our method achieves state-of-the-art-comparable performance. In particular, we surpass the current best method, the hierarchical framework proposed by  [14] , with improvements of 0.96% in terms of Recall, 0.49% in terms of Precision, and 0.81% in terms of F-measure. This advancement can be attributed to our approach of modeling latent uncertainty across all individuals, specifically formulating uncertainty-sensitive score allocations to facilitate the aggregation of individual features, rather than relying excessively on the utilization of attention modules as in approach  [14] . This allows us to better enhance the robustness of individual and group representation synchronously.\n\n3) Results on the MultiEmoVA dataset: The aggregate which emphasizes global alignment kernels and formulates the Support Vector Machine with combined global alignment kernels, aims to enhance the recognition of group-level emotion and currently achieves the highest accuracy. However, our method surpasses it by 6.82%. To provide deeper insights into the classification results, we visualize the confusion matrices on the MultiEmoVA dataset. Fig.  3  illustrates the visualization results. According to this figure, four included group emotions, i.e., high-positive, medium-positive, high-negative, and neutral, are easier to distinguish compared to mediumnegative. We attribute this observation to class imbalance, as well as tendency for medium-negative samples to be classified as medium-positive.\n\nMeanwhile, our method achieves a performance of 60.77% in terms of the F-measure metric. It supports our conclusion that our method enhances feature discrimination ability and improves the model's generalization, even under conditions of extreme sample scarcity and severe class imbalance, while still achieving good performance. In summary, the results across multiple datasets demonstrate the effectiveness of our proposed method and the feasibility of mitigating uncertainty in group-level emotion recognition. 4) Results on the GAFF2 Contracted Dataset: We built a sub-database comprising challenging scenarios, such as occlusions, to evaluate our method and other methods on the GER task. For the construction of the sub-database, we manually selected approximately 40% of the samples from each class of the validation set of the GAFF2 dataset, focusing on instances where individual occlusions or occlusions between individuals were present. Regarding the selected comparative methods, as there are currently no publicly accessible codes for these methods, we replicated experiments comparing our method with two benchmark models (i.e., VGG16 and ResNet50) that performed well on the GER task. The experimental results are shown in Table VI. Our method exhibits a minor decrease in performance under the prevalent occlusions in the group and maintains superior performance. The values reflect classification accuracy for every category. \"HighPos\", \"MedPos\",\" HighNeg\", \"MedNeg\", and \"Neu\" are abbreviations for \"High-Positive\", \"Medium-Positive\", \"High-Negative\", \"Medium-Negative\", and \"Neutral\" in the group emotion labels for the MultiEmoVA dataset, respectively.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "D. Ablation Study",
      "text": "We conduct ablation studies (marked by \"No.\") to investigate the contributions of L kl , L rank , and L rec in Eq. 11, the module components and the fusion strategy by removing constituent components on the GAFF2 database, to validate the effectiveness and respective contributions of the model. Furthermore, we also verify the effect of the total sample time N of the inference stage.\n\nImpact of different loss terms. As shown in Eq. 11, 4 loss terms are considered in our proposed method. The L cls represents the baseline cross-entropy loss for general GER methods. Besides, L kl , L rank , and L rec are proposed for uncertainty-aware learning module. We start with the exploration of the effectiveness of different loss terms. The results are reported in Table  VII .\n\nCompared with sole utilizing L cls as shown in \"No. 1\" of Table VII, the KL divergence term L kl (in \"No. 2\" of Table  VII ) gains the considerable improvements in terms of all metrics. The performance improvement emphasizes the importance of alleviating negative uncertainties. The rank regularization term L rank (in \"No. 3\" of Table  VII ) is employed to explicitly constrain the uncertainty-sensitive score of the face individuals, and further regularize the important scale weights which are devised to aggregate the individuals in a group. Compared with \"No. 2\" of Table  VII , it yields considerable improvements of 0.35%, 0.59%, and 0.67% in terms of Recall, Precision, and F-measure, respectively. The reconstruction loss L rec (\"No. 4\" of Table  VII ) is introduced to drop as much ambiguous information as possible, calculating the L1 distance between the original face individual feature and the sampled stochastic features. Intuitively, the introduction of L rec contributes to better performances and improves the average Recall and UAR significantly. Compared with \"No. 1\" of Table  VII , better results are obtained using the uncertaintyaware loss terms, with increases of 3.96%, 2.86%, and 3.88% in terms of Recall, Precision, and F-measure, respectively. The above results prove the effectiveness of the designed UAL module with the uncertainty-aware loss terms in improving the robust representation ability of individuals, which can further increase the final emotion recognition performance.\n\nImpact of different components. We further conduct the ablation studies on different compositions for GER, which involves two aspects, investigate the effect of appending two different modules (i.e., UAL and FIQE) to the face branch or UAL module to the object branch, and explore the performance of single branch or all branches are attached to the overall framework. Note that FIQE is the simple expression for the method used in the image enhancement module. For variants of FIQE and UAL in the face branch, (  1    4 ) OnlyFace: with the addition of FIQE and UAL modules to the face branch simultaneously, the more robust group-level representations aggregated by face individuals and only the face branch is used for group-level emotion inference. For the variant of UAL in the object branch, (5) w/o UAL: the detected object individuals are straightly fed into the pre-trained VGG network to capture the deterministic embedding for classification with the cross-entropy loss;  (6)  OnlyObject: the deterministic representation of each object individual is extended to a probabilistic distribution via the UAL and then aggregated to represent the group-level emotion in this experiment with the single object branch.  (7)  OnlyScene: this variant is to directly learn the global emotion information from the whole image which just depends on the scene branch.  (8)  Ours: this variant contains three branches used in our method, which handle each branch separately for GER and fuse them by the proposed PWFS.\n\nAs shown in Table  VIII , the face branch makes remarkable performances integrally boost by 6.0% compared to the baseline on all metrics. The first three lines of the table (from \"No. 5\" to \"No. 7\" in Table  VIII ) indicate that appending the FIQE and UAL modules to the baseline improves performance from 68.77% to 69.76% and 74.96% on the metric of UAR,      VIII ) improves substantially overall on each metric. These reflect that explicitly modeling the uncertainties brings significant performance improvement. Moreover, combining the above two modules (in \"No. 8\" of Table  VIII ) can steadily improve the performance of the face branch. In addition, there is around 1.0% performance gain (in \"No. 9\" vs in \"No. 10\" of Table  VIII ) when attaching UAL to the object branch on all metrics, which further indicates the effectiveness of the proposed UAL module for the GER task. The result of the scene branch (in \"No. 9\" of Table  VIII ) shows a performance not much different from the complete face branch. Obviously, the face and scene branches together occupy an important position in the GER task. The last line is the final GER result in our method, which aggregates all information from three branches, resulting in the performance raising dramatically. It indicates that all the ingredients reinforce each other and the combination is important to get remarkable final results.\n\nImpact of different fusion. We also conduct an ablation study on four fusion strategies to combine the face, object, and scene branches in our model. For equal proportion fusion, the weight of each branch is equal, we straightforwardly add all the predictions. Due to the relatively good performance in the face and scene branches (shown in Table  VIII ), we adopted the strategy of choosing one of the two branches as the priority, respectively. For global priority fusion, we set the weight for the scene branch (global information) to be twice as large as the weight for the sum of the object and face branches (local information), which assumes that the global information contains more information about the group-level emotion-related pieces of information. For face priority fusion, we set the weight for the face branch to be twice as the object and scene branches, which assumes that the face is the most representative carrier of emotion in an image. The results are reported in Table  IX   results of 79.32%, 79.19%, and 79.23% in terms of Recall, Precision, and F-measure, respectively. The equal proportion fusion strategy obtained the results of 76.70%, 77.08%, and 76.86% (in \"No. 13\" of Table  IX ) in terms of Recall, Precision, and F-measure, respectively. It indicates that the equal proportion fusion strategy is detrimental to our model, slightly better than using a single face branch. Compared with the global priority fusion strategy, the face priority fusion strategy achieves better performance with an increase of 0.42% (in \"No. 14\" vs \"No. 15\"of Table  IX ) in terms of Recall, demonstrating the leading role of the face branch in GER. The experiment results also show that the different branches e.g., face, and object, cannot be equally treated. Compared with the first three fusion strategies, our PWFS accounts for the corresponding proportion as a weight for each branch, which helps the model to maximally explore the benefit of each branch.\n\nImpact of different Monte Carlo sampling time. To study the effectiveness of sample time M mentioned in Eq. 4, four different values are chosen to show how M affects the performance on the face branch. Fig.  4  shows the results on the GAFF2 database. From Fig.  4 , when M is too small (e.g., M = 5 or 10), it leads to diminished performance. This is because the limited number of samples may not sufficiently capture the variability within the feature space. Conversely, setting M to a large value, e.g., M = 20, may better capture the variability, but the increased sampling would result in higher computational costs and potentially diminish performance. The most suitable value for M is 15, which yields benefits to recognition performance. We hypothesize that this optimal equilibrium allows for effective representation within the feature space while mitigating the influence of randomness, ultimately resulting in improved recognition performance. In all of our experiments, we set M = 15 as the Monte Carlo sampling time unless specifically defined. Furthermore, as M changes, the recognition performance shows minimal fluctuation, suggesting the robustness and derivability of our model in handling uncertainty.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Visualization Analysis",
      "text": "Visualization of image enhancement samples. To better explore the image enhancement module in our method, we  illustrate the face quality scores for two examples of the GAFF2 dataset in Fig.  5 . In example (a), the sample quality scores for each face individual are generally high, indicating successful passage through the filtering process into the backbone network. Conversely, in example (b), the overall quality scores are lower, suggesting inadequate quality at different levels due to uncertainty. Consequently, discarding a significant number of samples could potentially affect the performance of the facial branch to some extent. To tackle this issue, we will explore methods to enhance resolution directly from the sample source.\n\nVisualization of correctly classified and misclassified results. Fig.  6  illustrates the classification results of four samples from the GAFF2 dataset comparing the baseline with our method. In examples (a) and (b), our predictions align with the ground truth, whereas the baseline contradicts it. Our uncertainty modeling, through adaptively assigning fusion weights based on uncertainty-sensitive scores enables robust learning of group-level emotion representations, increasing alignment between ground truth. Furthermore, in examples (c) and (d), show misclassification, where the ground truth \"Neutral\" is predicted as \"Positive\" and \"Negative\" respectively, by both in the baseline and our method. This could be attributed to the mutual reinforcement of semantic information between dominant individuals and the scene, leading to the model's misjudgment. Alternatively, there exists a potential hypothesis: the subjectivity inherent in label annotations may lead to ambiguity in the ground truth itself. To address this problem, we will explore improvements in the direction of sample label correction and cleaning.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "The lack of research on uncertainty approximation within the realm of GER has been a driving force behind our work. The role of uncertainty approximation is of utmost importance in extending the applicability of emotionally intelligent AI agents to contexts that demand high dependability. This paper introduces our approach, an uncertainty-aware learning method, which seeks to encode latent uncertainty across all individuals, encompassing both faces and objects, while also incorporating scene information for group-level emotion recognition. Our unique contribution lies in explicitly modeling the uncertainty of individual samples as Gaussian random variables, leading to the generation of diverse samples and predictions. We have formulated uncertainty-sensitive score allocations to facilitate the aggregation of individual facial features, thereby yielding more robust GER representations. By employing a sampling operation, we ensure the derivability of the module, while a series of constraints are introduced to mitigate the adverse impact of uncertainty. An image enhancement module has been developed to counteract severe noise in each face individual sample. Additionally, we've designed a PWFS to effectively combine the outputs of three branches, enhancing group-level emotion predictions in GER. Extensive experimentation across three benchmarks validates the efficacy of our approach in managing uncertainty and advancing GER performance.\n\nIn summary, our proposed UAL method shows promising success in robust group-level emotion recognition, particularly in congested scenes and noisy environments, but still has limitations in generalizing to extremely complex scenarios with high levels of occlusion, congestion, or other challenging environmental factors not adequately represented in the training data. Further research could focus on refining the model to better address these challenges and improve its overall performance.\n\nIt is worth noting that uncertainty estimation within GER still holds substantial potential for improvement. We acknowledge that despite the robustness of our uncertainty-aware learning (UAL) method in handling uncertainties, there are still challenges in situations with extreme occlusion or congestion. These scenarios can severely impact the quality of facial data and subsequently hinder accurate emotion classification. This limitation should be carefully considered, and future research may focus on developing enhanced techniques to improve emotion classification performance in such challenging conditions. Furthermore, it's crucial to acknowledge that our method may not be readily applicable to multi-modal group emotion tasks. This presents a key consideration for deploying group emotion analysis tasks in practical applications. Our commitment to uncertainty-aware learning in GER will continue, and we aspire to extend this methodology to other computer vision tasks. By addressing the critical aspect of uncertainty within the GER domain, our work contributes to the broader field of AI, facilitating the creation of more reliable and dependable AI agents in emotionally charged applications.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Observation and Motivation: Low-quality examples",
      "page": 1
    },
    {
      "caption": "Figure 1: , mutual occlusion and self-occlusion",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), the gentleman on",
      "page": 2
    },
    {
      "caption": "Figure 1: (d), the lady exhibits a smiling face, while the bespec-",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) and Fig. 1(c), yet bears different",
      "page": 2
    },
    {
      "caption": "Figure 2: The overview of our proposed method is depicted. The framework of the proposed method is illustrated in (a),",
      "page": 4
    },
    {
      "caption": "Figure 2: The overview is shown in Fig. 2(a),",
      "page": 4
    },
    {
      "caption": "Figure 2: (b-d). Convention-",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the visualiza-",
      "page": 9
    },
    {
      "caption": "Figure 3: Confusion matrices on the MultiEmoVA dataset.",
      "page": 10
    },
    {
      "caption": "Figure 4: Impact of total sample time M on GAFF2 database.",
      "page": 12
    },
    {
      "caption": "Figure 4: shows the results on",
      "page": 12
    },
    {
      "caption": "Figure 4: , when M is too small (e.g.,",
      "page": 12
    },
    {
      "caption": "Figure 5: Illustration of face quality scores for face individ-",
      "page": 12
    },
    {
      "caption": "Figure 6: Visualization of the group emotion recognition results.",
      "page": 12
    },
    {
      "caption": "Figure 5: In example (a), the sample quality",
      "page": 12
    },
    {
      "caption": "Figure 6: illustrates the classification results of four",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "Recall": "Pos.\nNeu.\nNeg.",
          "Precision": "Pos.\nNeu.\nNeg.",
          "F-measure": "Pos.\nNeu.\nNeg."
        },
        {
          "Methods": "Dhall et al.\n[49]\nShamsi et al.\n[15]\nSokolov et al.\n[52]\nSurace et al.\n[22]\nBawa et al.\n[29]\nBalaji et al.\n[23]\nHuang et al.\n[10]\nAbbas et al.\n[24]\nFujii et al.\n[11]\nRassadin et al.\n[27]\nTarasov et al.\n[16]\nWei et al.\n[30]\nZhang et al.\n[31]\nFujii et al.\n[14]\nOurs",
          "Recall": "-\n-\n-\n-\n-\n-\n-\n-\n-\n79.43\n61.26\n65.33\n-\n-\n-\n-\n-\n-\n-\n-\n-\n78.53\n65.28\n72.70\n86.93\n67.45\n64.73\n80.00\n80.00\n66.00\n80.00\n72.00\n74.00\n-\n-\n-\n-\n-\n-\n88.41\n72.51\n79.64\n75.18\n84.16\n78.62",
          "Precision": "-\n-\n-\n-\n-\n-\n-\n-\n-\n68.61\n59.63\n76.05\n-\n-\n-\n-\n-\n-\n-\n-\n-\n79.76\n66.20\n69.97\n77.33\n75.68\n69.64\n-\n-\n-\n-\n-\n-\n-\n-\n-\n84.49\n85.38\n60.89\n87.84\n77.55\n74.10\n87.57\n73.93\n76.08",
          "F-measure": "-\n-\n-\n-\n-\n-\n-\n-\n-\n73.62\n60.43\n70.29\n-\n-\n-\n-\n-\n-\n-\n-\n-\n79.14\n65.74\n71.30\n80.92\n68.53\n70.46\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n88.12\n74.95\n76.76\n77.33\n85.83\n74.55"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "Recall": "Pos.\nNeu.\nNeg.",
          "Precision": "Pos.\nNeu.\nNeg.",
          "F-measure": "Pos.\nNeu.\nNeg."
        },
        {
          "Methods": "Dhall et al.\n[50]\nGarg et al.\n[25]\nNagarajan et al.\n[17]\nFujii et al.\n[11]\nGupta et al.\n[26]\nQuach et al.\n[12]\nDejian et al.\n[28]\nZhang et al.\n[31]\nFujii et al.\n[14]\nOurs",
          "Recall": "-\n-\n-\n-\n-\n-\n-\n-\n-\n88.31\n60.40\n58.85\n-\n-\n-\n84.00\n85.00\n53.00\n-\n-\n-\n-\n-\n-\n89.83\n70.92\n67.41\n74.09\n87.12\n69.81",
          "Precision": "-\n-\n-\n-\n-\n-\n-\n-\n-\n72.12\n69.51\n71.52\n-\n-\n-\n-\n-\n-\n-\n-\n-\n76.61\n79.85\n73.44\n74.32\n82.88\n72.64\n84.46\n72.62\n74.21",
          "F-measure": "-\n-\n-\n-\n-\n-\n-\n-\n-\n79.40\n64.64\n64.57\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n86.21\n71.77\n70.69\n74.15\n85.77\n71.19"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Methods": "",
          "Recall": "Pos.\nNeu.\nNeg.",
          "Precision": "Pos.\nNeu.\nNeg.",
          "F-measure": "Pos.\nNeu.\nNeg."
        },
        {
          "Dataset": "w/o cut",
          "Methods": "ResNet50\nResNet50*\nVGG16\nVGG16*\nOurs",
          "Recall": "82.29\n76.46\n61.21\n76.84\n67.70\n75.88\n78.53\n65.28\n72.70\n78.14\n62.91\n78.85\n84.16\n75.18\n78.62",
          "Precision": "-\n-\n-\n87.35\n64.95\n67.94\n79.76\n66.20\n69.97\n83.89\n67.17\n66.72\n87.57\n73.93\n76.08",
          "F-measure": "-\n-\n-\n81.76\n66.30\n71.69\n79.14\n65.74\n71.30\n80.91\n64.97\n72.28\n85.83\n74.55\n77.33"
        },
        {
          "Dataset": "w/ cut",
          "Methods": "ResNet50*\nVGG16*\nOurs",
          "Recall": "64.55\n62.12\n78.18\n85.91\n58.19\n55.30\n75.92\n65.19\n83.18",
          "Precision": "86.94\n57.59\n62.77\n86.57\n60.00\n55.43\n89.02\n68.71\n65.59",
          "F-measure": "74.09\n59.77\n69.64\n69.60\n57.55\n67.38\n81.95\n66.90\n73.35"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Loss terms\nRecall\nPrecision": "No. Lcls Lkl Lrank Lrec",
          "F-measure": "Pos.\nNeu.\nNeg."
        },
        {
          "Loss terms\nRecall\nPrecision": "(cid:33) (cid:37) (cid:37)\n(cid:37)\n1\n(cid:33) (cid:33) (cid:37)\n(cid:37)\n2\n(cid:33) (cid:33) (cid:33)\n(cid:37)\n3\n(cid:33) (cid:33) (cid:33)\n(cid:33)\n4",
          "F-measure": "81.22\n61.45\n69.01\n83.07\n62.64\n70.09\n83.41\n66.09\n69.93\n83.86\n67.64\n71.84"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.\nMethods": "",
          "Recall": "Pos.\nNeu.\nNeg.",
          "Precision": "Pos.\nNeu.\nNeg.",
          "F-measure": "Pos.\nNeu.\nNeg."
        },
        {
          "No.\nMethods": "Face w/o\n5\nUAL&FIQE\n6\nFace w/o UAL\n7\nFace w/o FIQE\n8\nOnlyFace",
          "Recall": "70.26\n63.33\n73.79\n73.77\n61.35\n75.09\n82.99\n65.02\n76.58\n85.71\n65.73\n74.72",
          "Precision": "83.49\n60.03\n63.93\n84.27\n62.59\n62.35\n84.75\n70.49\n67.65\n86.16\n71.25\n67.34",
          "F-measure": "76.30\n61.63\n68.51\n78.67\n61.97\n68.13\n83.86\n67.64\n71.84\n85.94\n68.38\n70.84"
        },
        {
          "No.\nMethods": "9\nObject w/o UAL\n10\nOnlyObject",
          "Recall": "76.97\n72.36\n63.08\n76.71\n72.21\n67.35",
          "Precision": "80.84\n62.48\n73.28\n81.23\n64.08\n73.78",
          "F-measure": "78.86\n67.06\n67.80\n78.91\n67.90\n70.42"
        },
        {
          "No.\nMethods": "11\nOnlyScene\nOurs\n12",
          "Recall": "75.97\n69.82\n78.07\n75.18\n78.62\n84.16",
          "Precision": "84.91\n67.44\n70.71\n87.57\n73.93\n76.08",
          "F-measure": "80.19\n68.61\n74.20\n74.55\n77.33\n85.83"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.\nMethods": "",
          "Recall": "Pos.\nNeu.\nNeg.\nAve.\nUAR",
          "Precision": "Pos.\nNeu.\nNeg.\nAve.",
          "F-measure": "Pos.\nNeu.\nNeg.\nAve."
        },
        {
          "No.\nMethods": "13\nEqual proportion\n14\nGlobal priority\n15\nFace priority\nPWFS (ours)\n16",
          "Recall": "83.25\n74.19\n72.68\n76.70\n77.24\n82.73\n74.89\n73.79\n77.14\n77.59\n84.42\n73.91\n74.35\n77.56\n78.04\n75.18\n78.62\n79.32\n79.52\n84.16",
          "Precision": "85.47\n70.13\n75.63\n77.08\n86.31\n70.33\n75.76\n77.47\n86.55\n71.39\n75.19\n77.71\n87.57\n73.93\n76.08\n79.19",
          "F-measure": "84.34\n72.10\n74.12\n76.86\n84.48\n72.54\n74.76\n77.26\n85.47\n72.63\n74.77\n77.62\n85.83\n74.55\n77.33\n79.23"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "2",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "3",
      "title": "Group affect",
      "authors": [
        "S Barsade",
        "A Knight"
      ],
      "year": "2015",
      "venue": "Annu. Rev. Organ. Psychol. Organ. Behav"
    },
    {
      "citation_id": "4",
      "title": "Group affect: Its influence on individual and group outcomes",
      "authors": [
        "S Barsade",
        "D Gibson"
      ],
      "year": "2012",
      "venue": "Curr. Dir. Psychol"
    },
    {
      "citation_id": "5",
      "title": "Revisiting crowd behaviour analysis through deep learning: Taxonomy, anomaly detection, crowd emotions, datasets, opportunities and prospects",
      "authors": [
        "F Sánchez",
        "I Hupont",
        "S Tabik",
        "F Herrera"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "6",
      "title": "Automatic emotion recognition for groups: A review",
      "authors": [
        "E Veltmeijer",
        "C Gerritsen",
        "K Hindriks"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "Graph neural networks for image understanding based on multiple cues: Group emotion recognition and event recognition as use cases",
      "authors": [
        "X Guo",
        "L Polania",
        "B Zhu",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "8",
      "title": "Grouplevel emotion recognition using hybrid deep models based on faces, scenes, skeletons and visual attentions",
      "authors": [
        "X Guo",
        "B Zhu",
        "L Polanía",
        "C Boncelet",
        "K Barner"
      ],
      "year": "2018",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "9",
      "title": "Group-level emotion recognition using deep models on image scene, faces, and skeletons",
      "authors": [
        "X Guo",
        "L Polanía",
        "K Barner"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "10",
      "title": "Analyzing group-level emotion with global alignment kernel based approach",
      "authors": [
        "X Huang",
        "A Dhall",
        "R Goecke",
        "M Pietikäinen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical group-level emotion recognition in the wild",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Autom. Face Gesture Recognit"
    },
    {
      "citation_id": "12",
      "title": "Nonvolume preserving-based fusion to group-level emotion recognition on crowd videos",
      "authors": [
        "K Quach",
        "N Le",
        "C Duong",
        "I Jalata",
        "K Roy",
        "K Luu"
      ],
      "year": "2022",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "13",
      "title": "ConGNN: Context-consistent cross-graph neural network for group emotion recognition in the wild",
      "authors": [
        "Y Wang",
        "S Zhou",
        "Y Liu",
        "K Wang",
        "F Fang",
        "H Qian"
      ],
      "year": "2022",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "14",
      "title": "Hierarchical group-level emotion recognition",
      "authors": [
        "K Fujii",
        "D Sugimura",
        "T Hamamoto"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multi"
    },
    {
      "citation_id": "15",
      "title": "Group affect prediction using emotion heatmaps and scene information",
      "authors": [
        "S Shamsi",
        "B Rawat",
        "M Wadhwa"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition of a group of people in video analytics using deep off-the-shelf image embeddings",
      "authors": [
        "A Tarasov",
        "A Savchenko"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. Anal"
    },
    {
      "citation_id": "17",
      "title": "Group emotion recognition in adverse face detection",
      "authors": [
        "B Nagarajan",
        "V Oruganti"
      ],
      "year": "2019",
      "venue": "Proc. Int. Conf. Autom. Face Gesture Recognit"
    },
    {
      "citation_id": "18",
      "title": "Silent messages: Implicit communication of emotions and attitudes",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1981",
      "venue": "Silent messages: Implicit communication of emotions and attitudes"
    },
    {
      "citation_id": "19",
      "title": "Automatic group happiness intensity analysis",
      "authors": [
        "A Dhall",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "20",
      "title": "Rieszbased volume local binary pattern and a novel group expression model for group happiness intensity analysis",
      "authors": [
        "X Huang",
        "A Dhall",
        "G Zhao",
        "R Goecke",
        "M Pietikäinen"
      ],
      "year": "2015",
      "venue": "Proc. Br. Mach. Vis. Conf"
    },
    {
      "citation_id": "21",
      "title": "Grouplevel emotion recognition using deep models with A four-stream hybrid network",
      "authors": [
        "A Khan",
        "Z Li",
        "J Cai",
        "Z Meng",
        "J O'reilly",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition in the wild using deep neural networks and bayesian classifiers",
      "authors": [
        "L Surace",
        "M Patacchiola",
        "E Battini",
        "W Sönmez",
        "A Spataro",
        "Cangelosi"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "23",
      "title": "Multi-level feature fusion for group-level emotion recognition",
      "authors": [
        "B Balaji",
        "V Oruganti"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "24",
      "title": "Group emotion recognition in the wild by combining deep neural networks for facial expression classification and scene-context analysis",
      "authors": [
        "A Abbas",
        "S Chalup"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "25",
      "title": "Group emotion recognition using machine learning",
      "authors": [
        "S Garg"
      ],
      "year": "1118",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "An attention model for group-level emotion recognition",
      "authors": [
        "A Gupta",
        "D Agrawal",
        "H Chauhan",
        "J Dolz",
        "M Pedersoli"
      ],
      "year": "2018",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "27",
      "title": "Group-level emotion recognition using transfer learning from face identification",
      "authors": [
        "A Rassadin",
        "A Gruzdev",
        "A Savchenko"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "28",
      "title": "Group-level emotion recognition based on faces, scenes, skeletons features",
      "authors": [
        "D Li",
        "R Luo",
        "S Sun"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Graph. Image Process"
    },
    {
      "citation_id": "29",
      "title": "Emotional sentiment analysis for a group of people based on transfer learning with a multi-modal system",
      "authors": [
        "V Bawa",
        "V Kumar"
      ],
      "year": "2019",
      "venue": "Neural. Comput. Appl"
    },
    {
      "citation_id": "30",
      "title": "A new deeplearning framework for group emotion recognition",
      "authors": [
        "Q Wei",
        "Y Zhao",
        "Q Xu",
        "L Li",
        "J He",
        "L Yu",
        "B Sun"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "31",
      "title": "Semi-supervised group emotion recognition based on contrastive learning",
      "authors": [
        "J Zhang",
        "X Wang",
        "D Zhang",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "32",
      "title": "Dual spoof disentanglement generation for face anti-spoofing with depth uncertainty learning",
      "authors": [
        "H Wu",
        "D Zeng",
        "Y Hu",
        "H Shi",
        "T Mei"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "33",
      "title": "UCC: uncertainty guided crosshead cotraining for semi-supervised semantic segmentation",
      "authors": [
        "J Fan",
        "B Gao",
        "H Jin",
        "L Jiang"
      ],
      "year": "2022",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "34",
      "title": "Uncertainty-aware contrastive distillation for incremental semantic segmentation",
      "authors": [
        "G Yang",
        "E Fini",
        "D Xu",
        "P Rota",
        "M Ding",
        "M Nabi",
        "X Alameda-Pineda",
        "E Ricci"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "35",
      "title": "Exploiting sample uncertainty for domain adaptive person re-identification",
      "authors": [
        "K Zheng",
        "C Lan",
        "W Zeng",
        "Z Zhang",
        "Z Zha"
      ],
      "year": "2021",
      "venue": "Proc. Int. AAAI Conf"
    },
    {
      "citation_id": "36",
      "title": "Reliability-aware prediction via uncertainty learning for person image retrieval",
      "authors": [
        "Z Dou",
        "Z Wang",
        "W Chen",
        "Y Li",
        "S Wang"
      ],
      "year": "2022",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "37",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "38",
      "title": "Data uncertainty learning in face recognition",
      "authors": [
        "J Chang",
        "Z Lan",
        "C Cheng",
        "Y Wei"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "39",
      "title": "Contextaware feature learning for noise robust person search",
      "authors": [
        "C Zhao",
        "Z Chen",
        "S Dou",
        "Z Qu",
        "J Yao",
        "J Wu",
        "D Miao"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "40",
      "title": "Uncertainty-aware representation learning for action segmentation",
      "authors": [
        "L Chen",
        "M Li",
        "Y Duan",
        "J Zhou",
        "J Lu"
      ],
      "year": "2022",
      "venue": "Proc. Int. Jt. Conf. Artif. Intell"
    },
    {
      "citation_id": "41",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "42",
      "title": "Faster R-CNN: towards real-time object detection with region proposal networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "43",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Y Guo",
        "L Zhang",
        "Y Hu",
        "X He",
        "J Gao"
      ],
      "year": "2016",
      "venue": "Proc. Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "44",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "45",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Neural Inf. Process"
    },
    {
      "citation_id": "46",
      "title": "Mitigating labelnoise for facial expression recognition in the wild",
      "authors": [
        "H Yan",
        "Y Gu",
        "X Zhang",
        "Y Wang",
        "Y Ji",
        "F Ren"
      ],
      "year": "2022",
      "venue": "Proc. IEEE Int. Conf. Multimed. Expo"
    },
    {
      "citation_id": "47",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2014",
      "venue": "Proc. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "48",
      "title": "Serfiq: Unsupervised estimation of face image quality based on stochastic embedding robustness",
      "authors": [
        "P Terhorst",
        "J Kolf",
        "N Damer",
        "F Kirchbuchner",
        "A Kuijper"
      ],
      "year": "2020",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "49",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Ghosh",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "50",
      "title": "Emotiw 2018: Audiovideo, student engagement and group-level affect prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "Proc. ACM Int. Conf. Multimodal Interact"
    },
    {
      "citation_id": "51",
      "title": "Group-level arousal and valence recognition in static images: Face, body and context",
      "authors": [
        "W Mou",
        "H Gunes"
      ],
      "year": "2015",
      "venue": "Proc. Int. Conf. Autom. Face Gesture Recognit"
    },
    {
      "citation_id": "52",
      "title": "Real-time emotion recognition on mobile devices",
      "authors": [
        "D Sokolov",
        "M Patkin"
      ],
      "year": "2018",
      "venue": "Proc. IEEE Int. Conf. Autom. Face Gesture Recognit"
    }
  ]
}