{
  "paper_id": "2304.06351v1",
  "title": "Neuromorphic Event-Based Facial Expression Recognition",
  "published": "2023-04-13T09:02:10Z",
  "authors": [
    "Lorenzo Berlincioni",
    "Luca Cultrera",
    "Chiara Albisani",
    "Lisa Cresti",
    "Andrea Leonardo",
    "Sara Picchioni",
    "Federico Becattini",
    "Alberto Del Bimbo"
  ],
  "keywords": [
    "{x1",
    "x2",
    "y1",
    "y2} {x1",
    "x2",
    "y1",
    "y2}"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, event cameras have shown large applicability in several computer vision fields especially concerning tasks that require high temporal resolution. In this work, we investigate the usage of such kind of data for emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based Facial Expression Recognition. NEFER is composed of paired RGB and event videos representing human faces labeled with the respective emotions and also annotated with face bounding boxes and facial landmarks. We detail the data acquisition process as well as providing a baseline method for RGB and event data. The collected data captures subtle micro-expressions, which are hard to spot with RGB data, yet emerge in the event domain. We report a double recognition accuracy for the event-based approach, proving the effectiveness of a neuromorphic approach for analyzing fast and hardly detectable expressions and the emotions they conceal.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expression recognition is important for a large variety of applications  [3, 20, 34] . Different kinds of sensors have been used to analyze faces such as depth cameras  [9]  or sensors with high framerate such as high-speed structured light sensors  [67]  and extremely fast RGB cameras  [43] . In particular, the necessity for elevated framerates stems from the fact that emotions are often conveyed by micro-expressions, which can manifest in short timespans up to 1/25 of a second  [12] . Recently, an exploratory approach has studied the capability of neuromorphic sensors, i.e. event cameras, to capture facial expressions  [2] . It suggested better recognition rates for event-based approaches compared to RGB.\n\nEvent cameras are bio-inspired sensors that, instead of generating streams of synchronous frames, produce asynchronous events for single pixels where illumination Figure  1 . NEFER is a dataset for Neuromorphic Event-based Facial Expression Recognition. We collect paired Event streams and RGB videos, providing for both modalities face bounding boxes, facial landmarks and emotion labels. Emotion labels are provided in two versions: using an a-priori assignment based on the visual stimulus shown to the user and based on actual user feelings. changes occur. An advantage is the extremely high rate of events, with temporal resolutions that reach the microsecond. However, due to a lack of data, emotion recognition through event-based videos is still a problem not widely addressed in the literature. In order to cope with the aforementioned lack of data, several attempts have been made to generate synthetic event-based datasets  [23, 28, 46] . The authors of  [2]  framed the recognition setting as a facial reaction recognition system, aiming at understanding whether an expression is positive or negative when using an interactive recommendation system. The authors however, collected a dataset using a VGA event camera, thus with limited spatial resolution. We believe that this poses a strong limit for facial-expression analysis applications since micro-expressions can be very localized in space as much as in time. Reactions are also labeled just as positive, neutral and negative, without providing details about emotions. A few additional works have addressed similar problems, yet focusing only on face detection and tracking alone, without analyzing expressions or emotions  [33, 51] .\n\nIn our work we present NEFER (Neuromorphic Eventbased Facial Expression Recognition)  1  , the first release of an RGB and event dataset for emotion recognition. The dataset is fully labelled with bounding boxes from face detection and facial landmark.\n\nAs traditional annotation methods are not ideal for eventbased data, we chose a hybrid approach. This involved using the ESIM  [46]  simulator to obtain aligned RGB images and event streams, which we could then analyze using supervision signals obtained directly from RGB vision methods for face detection and face landmark estimation. We also provide a simple baseline to underline the difficulty of the task and the capabilities of an event-based model with reference to an RGB counterpart. To the best of our knowledge, we are the first to publicly release an event camera facial expression recognition dataset. To summarize, the main contributions of this paper are:\n\n• We propose a dataset for emotion recognition recorded with an high resolution event camera. The dataset consists of more than 600 RGB and events-based videos from more than 30 individuals of different genders and ages.\n\n• We provide labels for face and landmark detection in both RGB and event data.\n\n• For each sample in the dataset we provide two different kind of labels: a-priori labels (pre-defined emotion assignment) and user labels (emotion felt by the user).\n\n• We provide a baseline model to foster future research in the field, underlying the potential of event-based analysis compared to standard RGB approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "The event camera is a neuromorphic sensor that is based on a novel bio-inspired vision paradigm  [11, 44] . In contrast to traditional vision systems, it does not produce a synchronous sequence of frames, but instead generates an asynchronous stream of events. Each event is characterized by a local change in brightness and can occur at very short time intervals (in the order of microseconds) with very low latency  [36] . Moreover, unlike traditional vision systems, the event camera does not produce any output if there is no change in brightness, thereby conserving resources. To summarize, utilizing a neuromorphic sensor results in reduced motion blur, high temporal resolution, and high dynamic range (up to 140 dB). Additionally, it enables a reduction in bandwidth consumption  [15, 16] .\n\nDespite the fact that event cameras have not been on the market for an extended period, and their large-scale use is still somewhat limited, there are examples of their application in fields such as robotics and computer vision that can be found in the literature  [11, 16] . In fact, in these contexts, the benefits offered by event cameras can be fully leveraged. In  [45]  the authors propose an event-based descriptor for event camera data and show its results in some vision problems such as object classification, tracking, detection and feature matching. Also  [30, 35, 40]  propose event-based approaches for object detection and recognition. Event cameras are widely used in literature for tracking  [48, 53, 70] . In  [69]  they propose a trasformer-based architecture to fuse temporal and spatial information encoded in the events for single object tracking. Neuromorphic sensors are extensively utilized in surveillance  [37, 49, 56]  due to their distinctive characteristics and low power consumption. In fact, one of the most desirable properties of these sensors in surveillance is their ability to transmit information solely when changes occur.  [6]  propose a neuromorphic vision-based system for autonomous vehicles. Event cameras are also used in a wide range of scenarios in robotics and computer vision such as video superresolution  [22, 27] , depth and optical flow prediction  [17] , monocular and stereo depth estimation  [19, 59] , SLAM  [26]  and visual odometry  [38, 62, 72] , and human pose estimation  [7, 52] .\n\nOf particular interest for this work, is the fact that neuromorphic sensors have a compelling application scenario in face detection and emotion recognition. The distinct characteristics and properties of event cameras enable them to capture even the subtlest variations and microexpressions in human emotions at remarkably high temporal resolution and with minimal latency. Nevertheless, this aspect has not received widespread attention in the literature. In general, facial images possess crucial features that can serve several biometric applications  [39]  and therefore face and landmark detection through deep learning algorithms is a problem widely addressed in the literature  [57, 64, 65] . Training deep learning models to perform well in face and landmark detection tasks requires a large amount of data:  [68]  proposes a dataset composed by 0.5M images from 10,575 individuals,  [21]  use more than 100k individuals to generate approximately 10M of images, and  [29]  includes 1M images from more the 690k individuals. The usage of synthetic face images has also been explored  [18] , whereas  [63]  instead proposed a dataset for masked face recognition, as a response to safety mandates during the covid pandemic. Several other datasets have also been published addressing the study of faces  [5, 24, 61] .\n\nAs for the event-based domain, despite all this interest in the topic, not many datasets can be found in the literature. In fact in  [50] , to make up for the lack of data, they use a synthetic event-based dataset starting from  [32] . Several attempts have been made in the literature to generate simulated data for event cameras. In  [46]  the authors propose ESIM, an event camera simulator with the ability to accurately and efficiently simulate events, while also offering the flexibility to simulate any camera trajectory within a 3D scene of any nature.  [28]  extends  [46]  with the goal to reduce the gap between simulation and real sensors by directly mapping noise distributions from real pixels.  [23]  instead, proposes a tool for generating synthetic event data and demonstrates its effectiveness in two computer vision object recognition and detection tasks. Also  [71]  and  [42]  propose event camera simulators. On the one hand,  [71]  introduces a multiple event simulator method suitable to be used in real-time robotics applications.  [42] , on the other hand, suggest an approach to emulate the behavior of an attention-based camera sensor. In conclusion, to the best of our knowledge, only three datasets containing facial images captured using a real event camera are present in the literature  [2, 33, 51] . In  [51]  the problem of face pose alignment is analyzed. The authors provide a dataset consisting of 108 videos of extreme head rotations with varying motion intensity, totaling just over 10 minutes of frames acquired.\n\nIn  [33]  on the other hand, the authors collected data with an event camera for eye blink detection. The dataset consists of 48 videos (total duration of about 13 minutes). The authors of  [2]  instead collected a dataset of 455 videos of facial reactions where the recorded users react to garment images. Reactions are classified in three classes: positive, neutral and negative. However, both  [51] ,  [33]  and  [2]  use low resolution event cameras with resolution of 304×240px or 640 × 480px. We are of the opinion that this presents a significant constraint for facial expression analysis applications as micro-expressions can be highly localized both spatially and temporally.\n\nIn this paper, we introduce NEFER (Neuromorphic Event-based Facial Expression Recognition), a dataset composed of paired RGB and event data for emotion recognition. We collected the dataset with high-resolution RGB and event cameras, providing also facial bounding box and landmark annotations in addition to emotion labels following the frequently used Ekman's emotion classification  [13] . As far as our knowledge extends we are the first to publicly release an event camera-based facial expression recognition  dataset. A comparison with existing datasets is presented in Tab. 1.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Nefer: Neuromorphic Event-Based Facial Expression Recognition",
      "text": "The purpose of NEFER is to capture genuine microexpressions associated to specific emotions with both an event camera and a standard RGB camera. We considered the 7 primary emotions defined by Ekman  [13] , namely Disgust, Contempt, Happiness, Fear, Anger, Surprise and Sadness, since these have been identified as independent from culture, history and personality and are performed in a similar way by everyone.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Setting And Protocol",
      "text": "In order to obtain realistic and non-simulated expressions, we asked a set of volunteers to maintain a neutral facial expression while watching a selection of videos. A reward has been offered to the participants to encourage a proper behavior during the test (high-stakes situation). The volunteers that took part in the creation of NEFER are both males and females of age ranging between 24 and 52 years, for a total of 29 users.\n\nWe showed to each user 21 different videos, 3 for each of Ekman's basic emotions. The videos have been selected from online streaming platforms (e.g. YouTube). Each video was trimmed to the same length of 7s to keep the recording sessions as short as possible so not to induce unwanted expressions due to, for instance, boredom. This choice also simplifies training schemes with deep learning frameworks which process data in mini-batches of the same size. Tab. 2 lists the videos that have been shown to the users with the correspondent emotion label. The overall procedure for the data acquisition and video selection was inspired by previously collected dataset from the state of the art  [10, 66] .\n\nFor the recording we used two capturing devices: a GO-PRO Hero+ action camera, recording videos at 60FPS and 1920 × 1080px resolution, and a Prophesee Evaluation Kit HD, recording event videos at a resolution of 1280 × 720px. The cameras have been mounted on a fixed recording rig in a room lit with natural light. We specifically avoided any presence of artificial light to avoid background noise that could alter the event-based recordings. Users are also isolated from other people which could generate distractions.\n\nUsers have been asked to sit in front of the screen at approximately 60cm from the cameras. The RGB and event streams have been programmatically synchronized in order to capture two videos of the same duration and content. After viewing each video, we asked the volunteers to provide a personal evaluation of the observed footage. In particular, we asked two questions: (i) select among the 7 basic emotions, plus a \"None\" option, the most suitable one to describe the emotions stemmed from viewing the video; (ii) the intensity, on a 1 to 5 scale, of such emotion. We used the collected answers to create two alternative versions of the annotations, one considering the labeling of the user and one following our a-priori video-emotion assignment. In Fig.  2  a confusion matrix is presented showing the differences between the two label versions. The two versions mostly differ in the fact that following user labelings we have the additional neutral emotion and a slight unbalance in the sample distribution as shown in Fig.  3 . Overall, recording sessions lasted 18 minutes on average. Fig.  4  shows a few samples from the dataset.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Video Annotation Through Simulated Events",
      "text": "The wide range of off-the-shelf functionalities for RGBbased computation is not available for event-based data. This includes modules that nowadays are common building blocks in computer vision pipelines such as face detectors and landmark estimators. In addition, it is necessary to preprocess the raw data of the neuromorphic sensor in order to use it with frame-based computational tools. Bridging this gap is not trivial, since due to the asynchronous nature of the domain, the usual annotation process for many different tasks becomes cumbersome and expensive. Even generating relatively simple annotations such as facial bounding boxes, which are reliably obtainable with RGB data, would require lots of manual annotation.\n\nTo provide additional annotations for event-based data we exploit RGB data and an event camera simulator, ESIM  [46] . Through the use of the ESIM simulator we convert the RGB videos into physically accurate simulated event streams. We then run a face detector and facial landmark estimator on the RGB frames, which is easily done with tools such as FaceAlignment  [4] . We train a face detector (Yolov2)  [47]  and a landmark estimator  [4]  on simulated data and test it on real event streams. This approach provides satisfactory results on most frames, decimating the annotation time. The final annotations are manually refined and validated using CVAT  [60] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Esim",
      "text": "ESIM  [46]  is an event-based camera simulator that can generate a synthetic event-based stream from its RGB video counterpart in a physically realistic way. The images are rendered by the simulator at a high frame rate, interpolating pixel brightness along the camera trajectory using an adaptive sampling technique, which is adapting the frame rate based on a prediction of the previous signals. We feed to the simulator all the RGB frames to generate a synthetic event-based version of each stream. In this way, we are able to associate the bounding boxes provided by face alignment on RGB frames with event data. The simulator-generated outputs are encoded using an exponential time surface  [31] . Note the synthetic event-based videos obtained from the RGB data are used only as a mean for training models to quickly collect annotations. These are not pixel-wise aligned with the real event streams and we do not treat them as part of the final dataset, which only comprises real event data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Face Detection",
      "text": "Using the synthetic data from the simulator, we generated an annotated dataset in the event spectrum to train a face detector. First, we generated face annotation for RGB frames using FaceAlignment  [4] , an open-source tool for face analysis  2  . We then bound the face labels with the corresponding synthetic event frames obtained with ESIM. This allowed us to train a YOLOv2  [47]  on the synthetic version of NEFER. We found the detector to have good generalization capabilities from synthetic to real event data, which yielded high-quality annotations at a slight cost of manual validation using CVAT  [60] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Landmark Detection",
      "text": "The facial landmark detection is performed by an Xception  [8]  architecture trained on the synthetic data from ESIM to regress the position of 68 landmarks of the face. Similarly to face detection, we obtained the ground truth labels from the RGB videos by using FaceAlignment  [4] . The Xception architecture is composed of three stages, all of them employing depthwise separable convolutions along skip connections, resulting in a faster convergence training  [8] . The final linear layer outputs the 136 normalized numbers representing the coordinates of the standard 68 facial landmarks. The model is optimized using Adam with a learning rate of 8 × 10 -4 for 10 epochs over 30K frame samples with the use of standard augmentation techniques (random changes in brightness, contrast, rotation, translation, and crop).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Method",
      "text": "We provide a simple baseline for the dataset. This baseline architecture is based on a 3D convolutional network C3D  [58] . It has been chosen as it has been a long-standing, simple, standard approach for video-based action and activity recognition tasks  [1, 14, 41, 58] . The C3D model is implemented using 5 3D convolutional blocks, all with kernel size 3 and padding 1, followed by a 3D max-pooling of size 2 and stride 2. This chain of sequential blocks reduces the input stacked sequence of images down to a 72 channels feature map, which is then flattened and fed to two fully connected layers of size 512 and 64 before a final classification layer. ReLU activations are present between all layers. The model architecture is depicted in Fig.  6 .\n\nWe train the same model separately with RGB-framebased data and with event data obtained by converting events into frame-wise representations using Temporal Binary Representation (TBR)  [25]  (see Sec. 5.1). We detect the face using our pre-trained detector (see Sec. 4.2), and resize the bounding box to a 200 × 200px patch before feeding it as input to the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Temporal Binary Representation",
      "text": "Temporal Binary Representation  [25]  (TBR) is an aggregation strategy to map the asynchronous events into a stream of synchronous frames that can be then processed by a standard computer vision pipeline. Given a fixed ∆t we can build the binary representation b i of a pixel at (x, y) by checking for an event in such a time interval, b i\n\nx,y = 1(x, y).\n\nWe can then collect N consecutive representations and stack them together as B ∈ R H×W ×N forming for each pixel a binary string [b 0\n\nx,y , b 1 x,y , ..., b N x,y ], as shown in Fig.  7 . This approach manages to create a frame processable by traditional Computer Vision algorithms with a minimal memory footprint and by retaining temporal information within the value of each pixel.\n\nFor our experiments, we used this representation setting",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "We implemented our C3D model using PyTorch and trained it using the Adam optimizer initialized at the default learning rate value of 1 × 10 -4 which is then reduced following the scheduling technique presented in  [55]  with the annealing strategy. As loss, we adopt the Binary Cross-Entropy Loss, regularized with weight decay.\n\nWe compare the performances of our model by training it separately first on the RGB videos and then on the event streams, using both the self-reported user annotations and the a-priori expected one as labels for the target emotion. We define a validation split by selecting 20% of the users at random (thus keeping each user either in the training set or in the validation set to avoid unwanted biases), for a total of 126 videos.\n\nWe found that the RGB model results in poor accuracy, obtaining an average of 14.37% using the user labels and 14.60% using the expected ones. The event-based model  instead showed much better performances, reaching an accuracy of 22.95% with the user labels and of 30.95% using the expected ones. We report these experimental results in Tab. 3. This confirms that neuromorphic cameras are well suited for analyzing faces and that event footage carries valuable information for identifying subtle microexpressions that are not easily detectable with RGB data. Interestingly, we observed that our baseline model, just as the human a-priori assumptions, tends to confuse classes that share similar expressions, such as fear with surprise or anger with contempt even when trained on the self-reported emotions.\n\nFinally, we perform a control experiment by running a frame-based pre-trained state of the art emotion recognition framework on the RGB data. As a model we adopt Deepface  [54] , a recent facial attribute analysis framework. The model uses the same categories as we do, following Ekman's emotion classification, with the only exception of the Contempt category, which is missing in Deepface. As shown in Fig.  8 , we note its tendency towards classifying most of the frames with the neutral class None. This underlines the difficulty of the task in the setting that we propose: most frames do not carry a very polarized expression and most emotion cues happen very quickly, in a way that it is difficult to grasp them with RGB cameras. We argue that to fully comprehend the underlying emotions of humans from a vision-based point of view, event cameras will play an important role in the near future due to their ability to capture fine-grained micro-expressions and micro-movements of the face.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we presented a first release of NEFER, a dataset for expression recognition based on event camera data. This dataset is composed of paired visual spectrum images and event camera streams. For every sequence of frames, both the expected emotion and the self reported one by the user are given. Every frame has multiple annotations, namely the user face bounding box and the respective facial landmarks that we collected by leveraging models trained on synthetic data obtained using a simulator. Finally, we presented and discussed a 3D convolutional baseline, trained on both version of our dataset, which achieved improved results on event camera data with respect to the RGB frame based data.\n\nWe consider this a starting point for a future larger collection of data in the event camera domain for similar hightime resolution tasks. The large interest given by the computer vision community towards understanding facial expressions and emotions proves the importance of the task, yet the neuromorphic community and the traditional RGB vision one still have several gaps to be bridged. We believe that pursuing this line of research will bring attention to an emerging field, bringing together the best of both worlds and providing multiple modalities to approach problems that, based on experimental results, appear to be better addressed in the event domain rather than in the RGB domain alone.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: NEFER is a dataset for Neuromorphic Event-based Fa-",
      "page": 1
    },
    {
      "caption": "Figure 2: Confusion matrix of the two sets of labels from the NE-",
      "page": 3
    },
    {
      "caption": "Figure 3: Class distribution with user labels.",
      "page": 4
    },
    {
      "caption": "Figure 2: a confusion matrix is presented showing the",
      "page": 4
    },
    {
      "caption": "Figure 4: shows a few samples from the dataset.",
      "page": 4
    },
    {
      "caption": "Figure 4: Four samples from the NEFER dataset. First row: happiness; Second row: fear; third row: disgust; fourth row: surprise. Subtle",
      "page": 5
    },
    {
      "caption": "Figure 5: Examples of detected faces and estimated landmarks on real event videos of NEFER. Better viewed in color on a PC screen.",
      "page": 6
    },
    {
      "caption": "Figure 6: We train the same model separately with RGB-frame-",
      "page": 6
    },
    {
      "caption": "Figure 6: Illustration of our C3D model. The stacked frames form the input to the ﬁrst of 5 Conv3D + ReLU + 3DMaxPooling blocks.",
      "page": 7
    },
    {
      "caption": "Figure 7: Visual diagram illustrating the TBR encoding aggregat-",
      "page": 7
    },
    {
      "caption": "Figure 8: Distribution of predicted labels on frames of the NEFER",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID User: user_04\nBbox Face: {x1, x2, y1, y2}\nFace Landmarks:\nUser’s Label: Sadness\nA-priori Label: Sadness": "ID User: user_28\nBbox Face: {x1, x2, y1, y2}\nFace Landmarks:\nUser’s Label: happiness\nA-priori Label: happiness"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Savran et al. [51]\nLenz et al. [33]\nBecattini et al. [2]\nNEFER",
          "Videos": "108\n48\n455\n609",
          "Users": "30\n10\n25\n29",
          "Resolution": "304 × 204\n640 × 480\n640 × 480\n1280 × 720",
          "Bounding Boxes": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)",
          "Landmarks": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)",
          "Emotions": "(cid:55)\n(cid:55)\n(cid:55)\n(cid:51)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Disgust": "Contempt",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Cops Killing Protestant\nDog Being Abandoned\nDog Being Mugged"
        },
        {
          "Disgust": "Happiness",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Dogs playing\nLaughing Child\nOld Man Dancing with Boys"
        },
        {
          "Disgust": "Fear",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Suddenly Appearing Ghost\nHidden Clown Attacking Camera\nGiant Snake Attacking Camera"
        },
        {
          "Disgust": "Anger",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Man Attacking Companion\nBoy destroying Brother’s PC\nProfessor Assaulted by Student’s Parents"
        },
        {
          "Disgust": "Surprise",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Baseball Coming Towards Camera\nGirl with Unexpected Makeup\nPresentation Concluding with a Cat"
        },
        {
          "Disgust": "Sadness",
          "Spyder in a man’s mouth\nCrushed Pimple on Cheek\nMan Eating a Larva": "Death of Mufasa in the Lion King\nDeath of Elly in Up\nBoy who has to Undergo an Operation"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Temporal convolutional 3d network for real-time action recognition",
      "venue": "Temporal convolutional 3d network for real-time action recognition"
    },
    {
      "citation_id": "2",
      "title": "Understanding human reactions looking at facial microexpressions with an event camera",
      "authors": [
        "Federico Becattini",
        "Federico Palai",
        "Alberto Bimbo"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "3",
      "title": "Plm-ipe: A pixel-landmark mutual enhanced framework for implicit preference estimation",
      "authors": [
        "Federico Becattini",
        "Xuemeng Song",
        "Claudio Baecchi",
        "Shi-Ting",
        "Claudio Fang",
        "Liqiang Ferrari",
        "Alberto Nie",
        "Bimbo"
      ],
      "venue": "ACM Multimedia Asia"
    },
    {
      "citation_id": "4",
      "title": "How far are we from solving the 2d & 3d face alignment problem?",
      "authors": [
        "Adrian Bulat",
        "Georgios Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "6",
      "title": "Neuroiv: Neuromorphic vision meets intelligent vehicle towards safe driving with a new database and baseline evaluations",
      "authors": [
        "Guang Chen",
        "Fa Wang",
        "Weijun Li",
        "Lin Hong",
        "Jörg Conradt",
        "Jieneng Chen",
        "Zhenyan Zhang",
        "Yiwen Lu",
        "Alois Knoll"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "7",
      "title": "Efficient human pose estimation via 3d event point cloud",
      "authors": [
        "Jiaan Chen",
        "Hao Shi",
        "Yaozu Ye",
        "Kailun Yang",
        "Lei Sun",
        "Kaiwei Wang"
      ],
      "year": "2022",
      "venue": "Efficient human pose estimation via 3d event point cloud",
      "arxiv": "arXiv:2206.04511"
    },
    {
      "citation_id": "8",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu Simón",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "Adrian Davison",
        "Cliff Lansley",
        "Nicholas Costen",
        "Kevin Tan",
        "Moi Hoon"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "11",
      "title": "Neuromorophic vision sensing and processing",
      "authors": [
        "Tobi Delbruckl"
      ],
      "year": "2016",
      "venue": "ESSCIRC Conference 2016: 42nd European Solid-State Circuits Conference"
    },
    {
      "citation_id": "12",
      "title": "Telling lies: Clues to deceit in the marketplace, politics, and marriage",
      "authors": [
        "Paul Ekman"
      ],
      "year": "2009",
      "venue": "Telling lies: Clues to deceit in the marketplace, politics, and marriage"
    },
    {
      "citation_id": "13",
      "title": "Universal facial expressions in emotion",
      "authors": [
        "Ekmann"
      ],
      "year": "1973",
      "venue": "Studia Psychologica"
    },
    {
      "citation_id": "14",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Yin Fan",
        "Xiangju Lu",
        "Dian Li",
        "Yuanliu Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI '16"
    },
    {
      "citation_id": "15",
      "title": "10 a 1280×720 back-illuminated stacked temporal contrast event-based vision sensor with 4.86µm pixels, 1.066geps readout, programmable event-rate controller and compressive dataformatting pipeline",
      "authors": [
        "Thomas Finateu",
        "Atsumi Niwa",
        "Daniel Matolin",
        "Koya Tsuchimoto",
        "Andrea Mascheroni",
        "Etienne Reynaud",
        "Pooria Mostafalu",
        "Frederick Brady",
        "Ludovic Chotard",
        "Florian Legoff",
        "Hirotsugu Takahashi",
        "Hayato Wakabayashi",
        "Yusuke Oike",
        "Christoph Posch"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Solid-State Circuits Conference -(ISSCC)"
    },
    {
      "citation_id": "16",
      "title": "Event-based vision: A survey",
      "authors": [
        "Guillermo Gallego",
        "Tobi Delbrück",
        "Garrick Orchard",
        "Chiara Bartolozzi",
        "Brian Taba",
        "Andrea Censi",
        "Stefan Leutenegger",
        "Andrew Davison",
        "Jörg Conradt",
        "Kostas Daniilidis"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "17",
      "title": "Focus is all you need: Loss functions for eventbased vision",
      "authors": [
        "Guillermo Gallego",
        "Mathias Gehrig",
        "Davide Scaramuzza"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Semi-supervised adversarial learning to generate photorealistic face images of new identities from 3d morphable model",
      "authors": [
        "Baris Gecer",
        "Binod Bhattarai",
        "Josef Kittler",
        "Tae-Kyun Kim"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "19",
      "title": "Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction",
      "authors": [
        "Daniel Gehrig",
        "Michelle Rüegg",
        "Mathias Gehrig",
        "Javier Hidalgo-Carrió",
        "Davide Scaramuzza"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "20",
      "title": "Macro-and microexpressions facial datasets: A survey",
      "authors": [
        "Hajer Guerdelli",
        "Claudio Ferrari",
        "Walid Barhoumi",
        "Haythem Ghazouani",
        "Stefano Berretti"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "21",
      "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
      "authors": [
        "Yandong Guo",
        "Lei Zhang",
        "Yuxiao Hu",
        "Xiaodong He",
        "Jianfeng Gao"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "22",
      "title": "Evintsr-net: Event guided multiple latent frames reconstruction and super-resolution",
      "authors": [
        "Jin Han",
        "Yixin Yang",
        "Chu Zhou",
        "Chao Xu",
        "Boxin Shi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "From video frames to realistic dvs events. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "authors": [
        "Yuhuang Hu",
        "Shih-Chii Liu",
        "Tobi Delbrück"
      ],
      "year": "2021",
      "venue": "From video frames to realistic dvs events. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "24",
      "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
      "authors": [
        "Marwan Gary B Huang",
        "Tamara Mattar",
        "Eric Berg",
        "Learned-Miller"
      ],
      "year": "2008",
      "venue": "Workshop on faces in'Real-Life'Images: detection, alignment, and recognition"
    },
    {
      "citation_id": "25",
      "title": "Temporal binary representation for event-based action recognition",
      "authors": [
        "Simone Undri",
        "Federico Becattini",
        "Federico Pernici",
        "Alberto Bimbo"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "26",
      "title": "Comparing representations in tracking for event camera-based slam",
      "authors": [
        "Jianhao Jiao",
        "Huaiyang Huang",
        "Liang Li",
        "Zhijian He",
        "Yilong Zhu",
        "Ming Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Turning frequency to resolution: Video super-resolution via event cameras",
      "authors": [
        "Yongcheng Jing",
        "Yiding Yang",
        "Xinchao Wang",
        "Mingli Song",
        "Dacheng Tao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Event camera simulator improvements via characterized parameters",
      "authors": [
        "Damien Joubert",
        "Alexandre Marcireau",
        "Nicholas Ralph",
        "A Jolley",
        "André Van Schaik",
        "Gregory Cohen"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "29",
      "title": "The megaface benchmark: 1 million faces for recognition at scale",
      "authors": [
        "Ira Kemelmacher-Shlizerman",
        "Steven Seitz",
        "Daniel Miller",
        "Evan Brossard"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "N-imagenet: Towards robust, fine-grained object recognition with event cameras",
      "authors": [
        "Junho Kim",
        "Jaehyeok Bae",
        "Gangin Park",
        "Dongsu Zhang",
        "Young Min"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Hots: A hierarchy of event-based time-surfaces for pattern recognition",
      "authors": [
        "Xavier Lagorce",
        "Garrick Orchard",
        "Francesco Galluppi",
        "Bertram Shi",
        "Ryad Benosman"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Interactive facial feature localization",
      "authors": [
        "Jonathan Vuong Le",
        "Zhe Brandt",
        "Lubomir Lin",
        "Thomas Bourdev",
        "Huang"
      ],
      "year": "2012",
      "venue": "Computer Vision-ECCV 2012: 12th European Conference on Computer Vision"
    },
    {
      "citation_id": "33",
      "title": "Eventbased face detection and tracking using the dynamics of eye blinks",
      "authors": [
        "Gregor Lenz",
        "Sio-Hoi Ieng",
        "Ryad Benosman"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "34",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "35",
      "title": "Graph-based asynchronous event processing for rapid object recognition",
      "authors": [
        "Yijin Li",
        "Han Zhou",
        "Bangbang Yang",
        "Ye Zhang",
        "Zhaopeng Cui",
        "Hujun Bao",
        "Guofeng Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor",
      "authors": [
        "Patrick Lichtsteiner",
        "Christoph Posch",
        "Tobi Delbruck"
      ],
      "year": "2008",
      "venue": "IEEE Journal of Solid-State Circuits"
    },
    {
      "citation_id": "37",
      "title": "Estimation of vehicle speed based on asynchronous data from a silicon retina optical sensor",
      "authors": [
        "M Litzenberger",
        "B Kohn",
        "A Belbachir",
        "N Donath",
        "G Gritsch",
        "H Garn",
        "C Posch",
        "S Schraml"
      ],
      "year": "2006",
      "venue": "2006 IEEE Intelligent Transportation Systems Conference"
    },
    {
      "citation_id": "38",
      "title": "Spatiotemporal registration for event-based visual odometry",
      "authors": [
        "Daqi Liu",
        "Alvaro Parra",
        "Tat-Jun Chin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "A comprehensive analysis of deep learning based representation for face recognition",
      "authors": [
        "Mostafa Mehdipour",
        "Hazim Kemal"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "40",
      "title": "Moving object detection for event-based vision using graph spectral clustering",
      "authors": [
        "Anindya Mondal",
        "H Jhony",
        "Thierry Giraldo",
        "Bouwmans",
        "Ananda S Chowdhury"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Temporal activity detection in untrimmed videos with recurrent neural networks",
      "authors": [
        "Alberto Montes",
        "Amaia Salvador",
        "Santiago Pascual",
        "Xavier Giro-I Nieto"
      ],
      "year": "2016",
      "venue": "1st NIPS Workshop on Large Scale Computer Vision Systems"
    },
    {
      "citation_id": "42",
      "title": "Event camera simulator design for modeling attention-based inference architectures",
      "authors": [
        "Md Jubaer",
        "Hossain Pantho",
        "Joel Mandebi Mbongue",
        "Pankaj Bhowmik",
        "Christophe Bobda"
      ],
      "year": "2021",
      "venue": "Journal of Real-Time Image Processing"
    },
    {
      "citation_id": "43",
      "title": "Facial micro-expressions recognition using high speed camera and 3d-gradient descriptor",
      "authors": [
        "Senya Polikovsky",
        "Yoshinari Kameda",
        "Yuichi Ohta"
      ],
      "year": "2009",
      "venue": "Facial micro-expressions recognition using high speed camera and 3d-gradient descriptor"
    },
    {
      "citation_id": "44",
      "title": "Retinomorphic eventbased vision sensors: Bioinspired cameras with spiking output",
      "authors": [
        "Christoph Posch",
        "Teresa Serrano-Gotarredona",
        "Bernabe Linares-Barranco",
        "Tobi Delbruck"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "45",
      "title": "Dart: distribution aware retinal transform for event-based cameras",
      "authors": [
        "Bharath Ramesh",
        "Hong Yang",
        "Garrick Orchard",
        "Ngoc Anh",
        "Le Thi",
        "Shihao Zhang",
        "Cheng Xiang"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "46",
      "title": "ESIM: an open event camera simulator",
      "authors": [
        "Henri Rebecq",
        "Daniel Gehrig",
        "Davide Scaramuzza"
      ],
      "year": "2005",
      "venue": "Conf. on Robotics Learning (CoRL)"
    },
    {
      "citation_id": "47",
      "title": "Yolo9000: better, faster, stronger",
      "authors": [
        "Joseph Redmon",
        "Ali Farhadi"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "48",
      "title": "Event-based attention and tracking on neuromorphic hardware",
      "authors": [
        "Alpha Renner",
        "Matthew Evanusa",
        "Garrick Orchard",
        "Yulia Sandamirskaya"
      ],
      "year": "2020",
      "venue": "2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "49",
      "title": "Asynchronous event-based clustering and tracking for intrusion monitoring in uas",
      "authors": [
        "J Rodríguez-Gomez",
        "A Gómez Eguíluz",
        "J Martínezde Dios",
        "A Ollero"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "50",
      "title": "Real-time face & eye tracking and blink detection using event cameras",
      "authors": [
        "Cian Ryan",
        "Brian 'sullivan",
        "Amr Elrasad",
        "Aisling Cahill",
        "Joe Lemley",
        "Paul Kielty",
        "Christoph Posch",
        "Etienne Perot"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "51",
      "title": "Face pose alignment with event cameras",
      "authors": [
        "Arman Savran",
        "Chiara Bartolozzi"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "52",
      "title": "Lifting monocular events to 3d human poses",
      "authors": [
        "Gianluca Scarpellini",
        "Pietro Morerio",
        "Alessio Del Bue"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "Robust feature tracking in dvs event stream using bezier mapping",
      "authors": [
        "Hochang Seok",
        "Jongwoo Lim"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "54",
      "title": "Hyperextended lightface: A facial attribute analysis framework",
      "authors": [
        "Sefik Ilkin",
        "Alper Ozpinar"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Engineering and Emerging Technologies (ICEET)"
    },
    {
      "citation_id": "55",
      "title": "Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications",
      "authors": [
        "N Leslie",
        "Nicholay Smith",
        "Topin"
      ],
      "year": "2019",
      "venue": "SPIE"
    },
    {
      "citation_id": "56",
      "title": "Iot-guard: Event-driven fog-based video surveillance system for real-time security management",
      "authors": [
        "Tanin Sultana",
        "A Khan",
        "Wahid"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "57",
      "title": "Face recognition: Past, present and future (a review)",
      "authors": [
        "Nihan Murat Taskiran",
        "Cigdem Kahraman",
        "Erdem"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "58",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "59",
      "title": "Unsupervised deep event stereo for depth estimation",
      "authors": [
        "Soikat Sm Nadim Uddin",
        "Yong Hasan Ahmed",
        "Jung Ju"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "60",
      "title": "Efficiently scaling up crowdsourced video annotation: A set of best practices for high quality, economical video labeling",
      "authors": [
        "Carl Vondrick",
        "Donald Patterson",
        "Deva Ramanan"
      ],
      "year": "2013",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "61",
      "title": "The devil of face recognition is in the noise",
      "authors": [
        "Fei Wang",
        "Liren Chen",
        "Cheng Li",
        "Shiyao Huang",
        "Yanjie Chen",
        "Chen Qian",
        "Chen Loy"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "62",
      "title": "Visual odometry with an event camera using continuous ray warping and volumetric contrast maximization",
      "authors": [
        "Yifu Wang",
        "Jiaqi Yang",
        "Xin Peng",
        "Peng Wu",
        "Ling Gao",
        "Kun Huang",
        "Jiaben Chen",
        "Laurent Kneip"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "63",
      "title": "Masked face recognition dataset and application",
      "authors": [
        "Zhongyuan Wang",
        "Guangcheng Wang",
        "Baojin Huang",
        "Zhangyang Xiong",
        "Qi Hong",
        "Hao Wu",
        "Peng Yi",
        "Kui Jiang",
        "Nanxi Wang",
        "Yingjiao Pei"
      ],
      "year": "2020",
      "venue": "Masked face recognition dataset and application",
      "arxiv": "arXiv:2003.09093"
    },
    {
      "citation_id": "64",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Yandong Wen",
        "Kaipeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "65",
      "title": "Facial landmark detection: A literature survey",
      "authors": [
        "Yue Wu",
        "Qiang Ji"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "66",
      "title": "Casme database: A dataset of spontaneous microexpressions collected from neutralized faces",
      "authors": [
        "Wen-Jing Yan",
        "Qi Wu",
        "Yong-Jin Liu",
        "Su-Jing Wang",
        "Xiaolan Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "67",
      "title": "Facial microexpression analysis via a high speed structured light sensing system",
      "authors": [
        "Yuping Ye",
        "Zhan Song",
        "Juan Zhao"
      ],
      "year": "2021",
      "venue": "Journal of Image and Graphics"
    },
    {
      "citation_id": "68",
      "title": "Learning face representation from scratch",
      "authors": [
        "Dong Yi",
        "Zhen Lei",
        "Shengcai Liao",
        "Stan Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "69",
      "title": "Spiking transformers for event-based single object tracking",
      "authors": [
        "Jiqing Zhang",
        "Bo Dong",
        "Haiwei Zhang",
        "Jianchuan Ding",
        "Felix Heide",
        "Baocai Yin",
        "Xin Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "70",
      "title": "Object tracking by jointly exploiting frame and event domain",
      "authors": [
        "Jiqing Zhang",
        "Xin Yang",
        "Yingkai Fu",
        "Xiaopeng Wei",
        "Baocai Yin",
        "Bo Dong"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "71",
      "title": "Real-time event simulation with frame-based cameras",
      "authors": [
        "Andreas Ziegler",
        "Daniel Teigland",
        "Jonas Tebbe",
        "Thomas Gossard",
        "Andreas Zell"
      ],
      "year": "2022",
      "venue": "Real-time event simulation with frame-based cameras"
    },
    {
      "citation_id": "72",
      "title": "Devo: Depth-event camera visual odometry in challenging conditions",
      "authors": [
        "Yi-Fan Zuo",
        "Jiaqi Yang",
        "Jiaben Chen",
        "Xia Wang",
        "Yifu Wang",
        "Laurent Kneip"
      ],
      "year": "2022",
      "venue": "2022 International Conference on Robotics and Automation (ICRA)"
    }
  ]
}