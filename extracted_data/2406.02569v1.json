{
  "paper_id": "2406.02569v1",
  "title": "Cluster-To-Predict Affect Contours From Speech",
  "published": "2024-05-14T12:34:30Z",
  "authors": [
    "Gökhan Kuşçu",
    "Engin Erzin"
  ],
  "keywords": [
    "speech emotion recognition",
    "affective computing",
    "clustering"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Continuous emotion recognition (CER) aims to track the dynamic changes in a person's emotional state over time. This paper proposes a novel approach to translating CER into a prediction problem of dynamic affect-contour clusters from speech, where the affect-contour is defined as the contour of annotated affect attributes in a temporal window. Our approach defines a cluster-to-predict (C2P) framework that learns affect-contour clusters, which are predicted from speech with higher precision. To achieve this, C2P runs an unsupervised iterative optimization process to learn affect-contour clusters by minimizing both clustering loss and speech-driven affect-contour prediction loss. Our objective findings demonstrate the value of speech-driven clustering for both arousal and valence attributes. Experiments conducted on the RECOLA dataset yielded promising classification results, with F1 scores of 0.84 for arousal and 0.75 for valence in our four-class speech-driven affect-contour prediction model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As human-computer interaction becomes more integrated into everyday life, the research community places a strong emphasis on speech-emotion recognition (SER) research. SER shows promise in enhancing the natural exchange of communication between individuals and computer systems. The applications of SER extend across diverse domains, such as driver assistance, healthcare, entertainment, chatbots, and more. The knowledge derived from SER technology facilitates more authentic interactions and provides valuable insights into understanding human behavior for intelligent systems  [1] .\n\nCategorical and dimensional affect models are commonly utilized  [2] . According to Ekman's categorical model  [3] , there are six primary emotions: anger, contempt, fear, happiness, sorrow, and surprise. The categorical model, which describes the classification of basic emotion categories, has been extensively studied in SER literature. In contrast, Russell's dimensional circumplex model expresses basic emotions on continuous dimensions of arousal (low vs high) and valence (unpleasant vs pleasant) attributes  [4] . The 2D arousal-valence model has advantages over the categorical model, as it can describe continuously fluctuating intensity levels across categorical emotions. SER models typically define the regression task of predicting emotional attributes (arousal, valence) and the classification task of predicting emotional categories (happy, sad, ...). While unweighted/weighted accuracy, F1 score, or similar metrics are used in assessing the classification task, the output of the regression task is assessed with correlation-based metrics to match the trends on the continuous ground truth and the regressed emotional dimensions. A recent survey of SER tasks on extensions of deep representation learning highlights the popularity of LSTM/GRU-RNNs combined with CNNs for supervised tasks, and Denoising Autoencoders (DAEs), Variational Autoencoders (VAEs), and GANbased models for unsupervised representation learnings  [5] .\n\nDynamic temporal modeling appears to be another important aspect of the SER problem. A recent study presents a chunking-based approach for sentence-level information extraction from varied-length acoustic feature sequences to better cope with dynamic temporal modeling for the prediction of emotional dimensions  [6] . Another study presents two architectures for capturing long-term temporal dependencies in acoustic features  [7] . Dilated convo-lutions maintain the input signal's length while incorporating long-term information via filters with varying dilation factors. Down/upsampling networks downsample the signal to grasp global features, then reconstruct the output to match the uncompressed input length. Deep neural networks are widely explored for SER tasks as well. In a recent study, Wu et.al explores neural architecture search (NAS) for automatically generating customized SER models  [8] . Some studies combine regression and classification problems of SER. In such a study, the regression problem of emotional attributes was translated into a classification problem by discretizing the training labels at different resolutions  [9] . Then, a multi-task bidirectional LSTM network is trained to jointly predict label sequences at different resolutions, and an emotion decoding algorithm produces more robust continuous emotional attribute estimates. In another study, AlBadawy and Kim propose a joint modeling method merging discrete and continuous emotion representations  [10] . Ensemble and end-to-end approaches balance between discretized and continuous representations using D-BLSTM architectures.\n\nSome other studies are different from the typical regression and classification problems of SER as well. A recent study explores emotional similarity measurement by estimating the most similar emotional content among two alternatives to a given anchor  [11] . This task states a different SER problem that leads to learning representations of emotional similarity. Another variation of the SER task defines emotionally salient regions relying on the qualitative agreement of raters for arousal and valence and then uses an ensemble of BLSTM predictors to detect emotionally salient regions  [12] .\n\nSemi-supervised learning (SSL) has been used to resolve the generalization problem for SER through pseudo-labeling classification. DeepEmoCluster presents such an SSL-based framework to learn latent representations to define emotional clusters from pseudo-labeling classification  [13] . In DeepEmoCluster, a CNN-based feature extraction forms latent representations from speech to perform emotional attribute regression, and a second network forms clusters of latent representations to assign pseudo-class labels via SSL. A recent study extends DeepEmoCluster by introducing sentence-level temporal modeling by integrating temporal constraints via temporal network and triplet loss function  [14] .\n\nIn this paper, our main motivation is to define a new SER problem, different from regression and classification tasks, which aims to discover temporal affect-contour clusters that are highly predictable from speech representations. Here, we define the affect contour as the contour of annotated affect attributes in a temporal window. To solve this problem, we propose a novel SSL approach, which defines a cluster-to-predict (C2P) framework. The proposed C2P framework is expected to discover novel affect contour clusters that are predictable from speech with higher precision. The main building blocks of the proposed C2P framework include i) the affect network, which extracts latent representations of affect contours and maps them to the affect clusters; ii) the speech network, which extracts latent representations of the speech signal and maps them to the affect clusters, and iii) the clustering block, which runs k-means clustering update on the latent representations of affect contours. Note that although the DeepEmoCluster and the proposed C2P frameworks have similar SSL approaches, the proposed C2P framework differs with key distinctions. The C2P framework defines the pseudo-labeled clusters from the latent representations of the affect contour but not from speech. Furthermore, the C2P framework learns to predict the affect-contour cluster labels from speech by defining a classification problem. Experimental results show the effectiveness of our approach in classification and provide insightful information on arousal-valence space through the discovered affect contour clusters.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this study, we aim to discover temporal affect-contour clusters that are highly predictable from speech representations. We propose a novel SSL approach by defining a cluster-to-predict (C2P) network.\n\nThe proposed C2P network consists of two main blocks: an affect network and a speech network. The speech network (SpeechNet) receives the speech signal as input and predicts affect-contour cluster labels. The affect network (AffectNet) receives emotional attribute vectors as input and employs two tasks. Firstly, AffectNet extracts a reduced dimensional latent affect representation and updates the k-means centroids defining the affect-contour cluster labels. Secondly, AffectNet predicts affect-contour cluster labels from the latent affect representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speechnet",
      "text": "The SpeechNet predicts affect-contour cluster labels from the speech input in two phases. The first phase includes a pre-trained acoustic feature extractor. The second phase maps high-dimensional acoustic feature embeddings into low-dimensional acoustic latent representations and then to the affect-contour cluster labels.\n\nWav2Vec is a powerful learned acoustic feature extractor for speech data  [15] . Unlike traditional hand-crafted acoustic features, Wav2Vec operates directly on the raw audio waveform to provide a compressed representation of the speech signal. We choose Wav2Vec as the pre-trained acoustic feature extractor, where the acoustic feature embeddings are taken from block 15 of the pre-trained Wav2Vec 2.0 Large model as 1024-dimensional vectors for 20 ms frames. The choice of block 15 is motivated by its proven efficacy in decreasing phoneme error rates  [15] . The Wav2Vec block receives a temporal window of N s speech frames to construct [N s , 1024] dimensional acoustic feature embeddings for each temporal window. In this study, we chose temporal window duration as 2 seconds with N s = 99.\n\nThe second phase includes a multi-layer CNN network and a fully connected predictor (SpeechCNN). The CNN network extracts low-dimensional acoustic latent representations from the Wav2Vec-driven acoustic feature embeddings. The fully connected layer predicts the affect-contour cluster labels from the acoustic latent representations. The network architecture of the SpeechCNN predictor is given in Table  1 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Affectnet",
      "text": "Similar to the construction of the speech network, our affect clustering network (AffectNet) is designed as a convolutional neural network with fully connected layers responsible for predicting affect classes. This network operates with dual objectives. The primary objective involves acting as an autoencoder  [16]  without the decoder component. The absence of the decoder is compensated by employing the classification part to compute the loss and update the weights. This design compels the convolutional network to generate features that enhance classification performance. The first target is achieved by extracting a lower-dimensional representation of the affect contour by mapping the N a -unit long affect contour into informative latent affect representations. With the 2 second temporal window setting, we take N a = 50. These latent representations are subsequently clustered into distinct affect classes through k-means clustering. Simultaneously, as a secondary objective, the entire network functions as a classifier, predicting the affectcluster labels. The loss function, comprising cross-entropy loss for multi-class classification, is aggregated with the speech network loss. Subsequently, the weights of both networks are updated accordingly. Given the task definition of the affect network, the weight convergence occurs at updated points that enhance the model's ability to produce superior latent representations of the affect contour, facilitating improved clustering in subsequent training iterations.\n\nThe AffectNet is constructed by concatenating convolutional networks with the incorporation of dropout layers as given in Table  2 . Notably, the kernel sizes of the convolutional layers in the earlier segments gradually diminish Regarding the latent representations extracted for the clustering component, we investigated the impact of selecting intermediate layers and identified a trade-off. Deeper layers contain more informative features about the AffectNet; however, the lower-dimensional space proves less effective when applying the k-means algorithm. In seeking an optimal balance between these considerations, we select an 8-dimensional latent affect representation that performs well for both objectives.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C2P Network",
      "text": "The proposed C2P network is jointly defined by the AffectNet and SpeechNet networks. The training routine of the proposed C2P network is given in Algorithm 1.\n\nThe joint loss of the C2P network is defined as L = αL a + (1 -α)L s , where α is a weighting factor, and L a and L s are correspondingly cross-entropy losses of the AffectNet and SpeechNet. During the training, the network parameters and the k-means clusters of the latent affect representations are jointly updated in an alternating order. The latent affect representations are clustered once for echo epoch using the k-means clustering to maintain stability across batches. These clusters are applied to speech segments, which are then fed into the C2P network in batches. The network converges more smoothly by conducting the clustering step separately from the training loop and using the clustering centroids as initial points for subsequent epochs. The RECOLA dataset  [17]  is widely acknowledged as a significant resource extensively utilized in various multimodal emotion recognition tasks, prominently featured in AVEC challenges spanning different years, including  [18]  and  [19] . It comprises a collection of 9.5 hours of multimodal recordings, incorporating audio, visual, and physiological data. These recordings capture real-time dyadic interactions involving 46 French-speaking participants collaboratively engaging in a task. The audio subset of the dataset, crucial for our study, consists of 27 five-minute speech utterances distributed into training, development, and test sets, each accompanied by corresponding affect contours. The audio modality and its associated affect contours for arousal and valence were annotated by six gender-balanced annotators at a frame rate of 40 milliseconds. This study adopts the mean value obtained from these six annotations as the reference point for our evaluation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "The whole dataset is split into training, development, and test parts, where each part consists of nine five-minute speech data segments concatenated to produce a total of 45 minutes of speech data. Similarly, the corresponding arousal and valence attributes form 1-dimensional vectors of 45 minutes. The data processing runs over 2-second temporal windows, which are sliding with 1-second intervals over time. Since the test part of the RECOLA dataset is not public, we run all training and testing evaluations on the training and development partitions of the RECOLA dataset, respectively. The C2P network is trained for various numbers of clusters k = 2, 2, ..., 10 both for arousal and valence attributes. For both attributes, the within-cluster variance is observed to reach a close-minimum value by k = 4. Hence, we fixed the k = 4 for both arousal and valence attributes in the experiments. Furthermore, the weight for the joint loss is selected as α = 0.2 by running a grid search to minimize the joint loss.\n\nThe training utilizes the Adam optimizer with a learning rate of 0.001. It operates in batches of 256 samples across 50 epochs, with early stopping implemented to cease training if consecutive epochs do not show significant improvement beyond a certain threshold in terms of cumulative loss. Hyperparameters are adjusted iteratively during experimentation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C2P Learned Affect Contours",
      "text": "The self-supervised learning structure of the proposed C2P network discovers affect contour clusters that are highly predictable from speech. We investigate the k = 4 cluster setting in our experimental evaluations. Figure  2  presents the resulting affect contour clusters (mean and std contours) for arousal and valence attributes. For both attributes, flat (no-change), increase, and decrease trends can be observed from contour clusters. This sets an interesting observation of the discovered clusters.\n\nFigure  3  presents affect cluster pairing distributions (in percent scores) for arousal and valence over the RECOLA dataset with contour cluster trends indicated as flat, increasing, or decreasing. We can observe that the arousal contour 1 (A1) is a rear event, and valence contours 1 and 2 (V1, V2) have both flat (no-change) characteristics. On the other hand, the most dominant occurrences are i) A2-V3 with 19.6% with a move towards south-east on the AV plane (calm), ii) A4-V2 with 16.3% with a move towards north (increased arousal), iii) A2-V4 with 15.6% with a move towards south-west (sad or bored), iv) A4-V3 with 15.2% with a move towards north-east (happy), and v) A4-V4 with 14.6% with a move towards north-west (angry).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C2P Classification Results",
      "text": "We investigate the performance of the C2P network with the classification performances of the k-class arousal and valence contour classification tasks. We also define two baseline classification systems compared to the proposed C2P network. The first baseline, affect-contour clusters (ACC), skips the AffectNet, and applies k-means clustering to the 50-unit long temporal affect attribute contours. The second baseline, average affect clusters (AAC), skips the AffectNet and applies k-means clustering to the mean affect attribute value of the 50-unit long temporal affect attribute contours.\n\nClassification performances for the proposed C2P, and baselines ACC and AAC models for arousal and valence attributes are given in",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "The literature has extensively studied predicting emotional attributes (regression) and emotional categories (classification) from speech. This remains an active area of research due to the inherent challenges in representing and quantifying emotions across different modalities, like speech. We propose a novel SSL-based cluster-to-predict (C2P) approach to address this. This approach aims to identify clusters of temporal affect contours that can be effectively pre-dicted from speech representations. We evaluated our approach on the RECOLA dataset. The results demonstrate that affect-contour clusters learned using SSL with a target on predictability from speech achieve promising performance.\n\nIn our four-class classification tasks, classification performances of the learned affect clusters achieved F1 scores of 0.84 for arousal and 0.75 for valence. Additionally, we observed dominant trends in the movement of arousal-valence contours across the four quadrants of the AV plane. This new perspective opens doors for future research on exploring novel emotion representations using SSL-based approaches.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed C2P network",
      "page": 4
    },
    {
      "caption": "Figure 3: presents affect cluster pairing distributions (in percent scores) for arousal and valence over the RECOLA",
      "page": 5
    },
    {
      "caption": "Figure 2: Arousal and valence contour mean and standard deviations for each C2P cluster",
      "page": 6
    },
    {
      "caption": "Figure 3: Affect cluster pairing percents for arousal and valence over the RECOLA dataset with contour cluster trends",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Backpropagation"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Backpropagation"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An Engineering View on Emotions and Speech: From Analysis and Predictive Models to Responsible Human-Centered Applications",
      "authors": [
        "Chi-Chun Lee",
        "Theodora Chaspari",
        "Emily Provost",
        "Shrikanth Narayanan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven synchronization",
      "authors": [
        "David Didier Grandjean",
        "Klaus Sander",
        "Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness and Cognition"
    },
    {
      "citation_id": "3",
      "title": "Are There Basic Emotions?",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "4",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "5",
      "title": "Survey of Deep Representation Learning for Speech Emotion Recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Chunk-Level Speech Emotion Recognition: A General Framework of Sequence-to-One Dynamic Temporal Modeling",
      "authors": [
        "Wei-Cheng Lin",
        "Carlos Busso"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition",
      "authors": [
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition"
    },
    {
      "citation_id": "8",
      "title": "Neural Architecture Search for Speech Emotion Recognition",
      "authors": [
        "Xixin Wu",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network",
      "authors": [
        "Duc Le",
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network"
    },
    {
      "citation_id": "10",
      "title": "Joint Discrete and Continuous Emotion Prediction Using Ensemble and End-to-End Approaches",
      "authors": [
        "A Ehab",
        "Yelin Albadawy",
        "Kim"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Quantifying Emotional Similarity in Speech",
      "authors": [
        "John Harvill",
        "Seong-Gyun Leem",
        "Mohammed Abdelwahab",
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Predicting Emotionally Salient Regions Using Qualitative Agreement of Deep Neural Network Regressors",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "DeepEmoCluster: A semi-supervised framework for latent cluster representation of speech emotions",
      "authors": [
        "Wei-Cheng Lin",
        "Kusha Sridhar",
        "Carlos Busso"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Deep temporal clustering features for speech emotion recognition",
      "authors": [
        "Wei-Cheng Lin",
        "Carlos Busso"
      ],
      "year": "2024",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2006",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "Transforming auto-encoders",
      "authors": [
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Sida Wang"
      ],
      "year": "2011",
      "venue": "ICANN"
    },
    {
      "citation_id": "17",
      "title": "Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "18",
      "title": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "19",
      "title": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Shahin Schmitt",
        "Nicholas Amiriparian",
        "Denis Cummins",
        "Adrien Lalanne",
        "Elvan Michaud",
        "Hüseyin Ciftc ¸i",
        "¸ Gülec",
        "Albert Ali Salah",
        "Maja Pantic"
      ],
      "year": "2018",
      "venue": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition"
    }
  ]
}