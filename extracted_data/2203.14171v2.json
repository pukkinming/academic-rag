{
  "paper_id": "2203.14171v2",
  "title": "A Speech Representation Anonymization Framework Via Selective Noise Perturbation",
  "published": "2022-03-26T23:28:55Z",
  "authors": [
    "Minh Tran",
    "Mohammad Soleymani"
  ],
  "keywords": [
    "privacy",
    "speech representations",
    "transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Privacy and security are major concerns when communicating speech signals to cloud services such as automatic speech recognition (ASR) and speech emotion recognition (SER). Existing solutions for speech anonymization mainly focus on voice conversion or voice modification to convert a raw utterance into another one with similar content but different, or no, identity-related information. However, an alternative approach to share speech data under the form of privacy-preserving representation has been largely underexplored. In this paper, we propose a speech anonymization framework that achieves privacy via noise perturbation to a selected subset of the high-utility representations extracted using a pre-trained speech encoder. The subset is chosen with a Transformer-based privacy-risk saliency estimator. We validate our framework on four tasks, namely, Automatic Speaker Verification (ASV), ASR, SER and Intent Classification (IC) for privacy and utility assessment. Experimental results show that our approach is able to achieve a competitive, or even better, utility compared to the speech anonymization baselines from the VoicePrivacy2022 Challenges, providing the same level of privacy. Moreover, the easily-controlled amount of perturbation allows our framework to have a flexible range of privacy-utility trade-offs without re-training any component.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Collecting and sharing speech data from local devices are essential for cloud-based speech processing models. From the service providers' perspective, gaining access to more data enables them to train more robust models and provide better services. From the users' perspective, sharing personal data allows experiencing more personalized services such as healthcare  [1]  or automatic speech recognition (ASR)  [2] . However, privacy and security are major concerns when communicating personal speech data, as raw utterances often contain sensitive identifiable information such as ethnicity, gender, or even health condition. One promising line of solution focuses on client-side privacy  [3] , providing users full control to protect their privacy by sanitizing raw utterances locally before uploading the data for cloud-based processing without the need for a trusted server.\n\nAnonymization, which aims at removing identifiable information from speech signals while retaining other information, is a commonly used approach. The majority of speech anonymization methods focus on voice anonymization that either modifies audio inputs via signal processing techniques  [4, 5]  or utilizes speech synthesis to perturb the x-vector  [6]  of the inputs  [7, 3, 8] . On the other hand, existing studies on privacy-preserving representation learning for speech generally leverage adversarial training to remove identity-related information from representations, but are limited to a single target application such as ASR  [9]  or emotion recognition  [10] . To the best of our knowledge, there has been no prior work on privacy-preserving representation learning for multiple applications (as in this work).\n\nIn this study, we propose a speech anonymization framework to sanitize the representations extracted from pre-trained speech encoders. At the core of our approach is a Privacy-risk Saliency Estimator (PSE) that learns to predict the importance of individual representation positions from a speaker identification system. Based on the estimations from PSE, the top k% positions with the highest estimated privacy-risk would be perturbed by adding easily-controlled Laplacian noise. We validate our method on four tasks, namely, Automatic Speaker Verification (ASV) for privacy evaluation, Automatic Speaker Verification (ASR), Emotion Recognition (ER), and Intent Classification (IC) for utility performance. Our work provides preliminary evidence that using speech data in the form of extracted representations results in systems achieving competitive performance compared to the commonly-used voice conversion/modification approaches. Moreover, our approach enables a flexible privacy-utility trade-off.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "Figure  1  shows a high-level overview of the proposed method. We first train a Speaker Identification model on a closed set of speakers with dataset D, which is used to construct a Saliency Map Dataset. We then train a Privacy-risk Saliency Estimator (PSE) to predict the privacy-salient positions given the extracted representations. The PSE module allows our framework to be extensible to unseen speakers outside D. Finally, we add controlled noise to selected positions within the input representations to produce sanitized output representations. Privacy-risk Saliency Map Dataset. The idea of saliency maps was originally proposed in computer vision to identify the most salient regions in images for trained classification models' decisions. Gradient-based methods construct the saliency map for a given image as the absolute value of the gradient of the input pixels with respect to the loss for the true class of the image, which is accessible via backpropagation. In this work, we use SmoothGrad  [11]  to compute saliency maps. It is important to note that we can only compute saliency maps if ground-truth labels (speaker identities in this case) are available. We do not have this information during the inference process, which motivates the development of a model that can estimate the saliency maps directly from the input representations.\n\nFor a given 2D representation x ∈ R t×d of an utterance, we hypothesize that some positions in x contain more sensitive information than the others (i.e., these positions can leak more identifiable information than the others). Note that these positions can change given different speech signals with different representations. We use SmoothGrad to find these positions. Specifically, we first train a Speaker Identification (SID) model M on a dataset D = {(x i , y i )}, with x i being the extracted features from a pre-trained model (e.g., wav2vec2) and y i being the identity of x i . For each sample (x i , y i ) in D, we compute the privacy-risk saliency map s i with SmoothGrad, which is the average of | δLy i δxi+ |, where L yi is the loss of M on the true class y i of x i with different versions of noise . As a result, we construct a privacy-risk saliency map dataset D = {(x i , s i )} containing the same number of samples as D with x i and s i of the same sizes. This dataset is then used to train our PSE module. Privacy-risk Saliency Estimator. The purpose of the Privacy-risk Saliency Estimator (PSE) is to take in the extracted feature representations x i for an utterance u and output an estimate for the privacy-risk saliency map s i such that s i ≈ s i . A good PSE should be able to (a) model the dynamics of the saliency maps given different speech representations; and (b) generalize to unseen speakers outside D. We use a Transformer encoder  [12]  as our architecture for the PSE. As a high-level overview, the Transformer encoder is a stack of Transformer layers, each of which consists of a selfattention module followed by a feed-forward neural network. For a given x i ∈ R t×d , we expect the self-attention modules to capture information along the temporal dimension while the feed-forward neural networks capture important signals from the feature dimension, which enables the architecture to make precise position-level predictions. The model's parameters are optimized with the L1 Loss between the outputs of PSE and the ground-truth saliency maps extracted with a trained Speaker Identification model, as explained above. Noise Perturbation. We assume that not all positions within the extracted representations from pre-trained encoders carry the same amount of speaker-related information. By injecting noise to the more sensitive positions, we hope to reduce the sensitive information carried within these dimensions to achieve privacy. To improve the utility of the output representations, we keep the less sensitive, i.e., insensitive positions, intact. In this study, we define the group of sensitive positions as positions with the top k% highest privacy-risk values estimated by PSE. We later investigate the effect of k on the privacy and utility of the sanitized outputs. Motivated by Differential Privacy and the Laplace mechanism  [13] , we set a fixed input-independent bound of [-1, 1] and add Laplacian noises iid ∼ Lap( 2 ) independently to the selected positions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setups",
      "text": "In this study, we focus on sanitizing the extracted representations from wav2vec2  [14] . However, the proposed framework should be applicable to other pre-trained speech encoders. Our implementations and training procedures follow the SUPERB benchmark  [15]  to enhance reproducibility. To show that the sanitized representations is applicable to multiple applications, we pick three downstream tasks with emphasis on different aspects of speech, namely, ASR for content, ER for paralinguistics, and IC for semantics. Our code is publicly available 1  .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Tasks, Datasets & Models",
      "text": "VoxCeleb1 (SID & ASV). We use the VoxCeleb1 dataset  [16]  for training the SID and ASV models. It contains approximately 150, 000 utterances from 1, 211 speakers collected from around 21, 819 videos on YouTube. For a fair privacy evaluation, we first split the dataset into two partitions. The first subset contains approximately 50, 000 utterances from 400 speakers, and is used to train the SID model and construct the Privacy-risk Saliency Map dataset. The second subset, which contains around 100, 000 utterances from the remaining 811 speakers, is used to train the ASV model for privacy evaluation. The output representations from wav2vec2 is fed to the SID model consisting of a mean-pooling layer followed by a fully-connected layer to make the speaker identity predictions. The SID model is trained using a cross-entropy loss. For ASV, we train the x-vector model  [6]  with an AM-Softmax loss  [17] . Following prior work  [18, 3, 5] , we report the equal error rate (EER) as our privacy metric (a higher EER implies a better privacy-preserving representation). IEMOCAP (ER). We use the IEMOCAP dataset  [19]  for training the ER model. The original dataset contains approximately 12 hours of data from 10 actors performing improvised or scripted scenarios that are designed to invoke emotions. Following the SUPERB benchmark's setting, we only keep four emotion classes (neutral, happy, sad, angry) from the IEMOCAP dataset to get a more balanced dataset. The ER model adds a mean-pooling layer followed by a fullyconnected layer to the encoder for emotion recognition. We report the classification accuracy as the utility metric. Fluent Speech Commands (IC). We use the Fluent Speech Commands dataset  [20]  to train our IC model. The dataset contains around 30, 000 utterances from 97 speakers with 31 unique intents. Similar to ER, the IC model contains a meanpooling layer followed by a fully-connected layer. We report the classification accuracy as the utility metric. LibriSpeech (ASR). We use the commonly used LibriSpeech audiobook dataset  [21]  to train our ASR model. Specifically, we use LibriSpeech's train-clean-100/dev-clean/test-clean subsets to train, validate and test our model. The training set contains more than 100 hours of transcribed speech from 251 speakers while both the validation and test set contains more than 5 hours of transcribed speech from 40 speakers. Our ASR model is a two-layer 1024-unit Bidirectional LSTM, trained with the CTC loss  [22]  on characters. We do not use any language model to improve the model's performance during inference. The Word Error Rate (WER) is used as the utility metric.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Baselines",
      "text": "We use the baselines provided in the VoicePrivacy2022 Challenge  [23] , in addition to a signal processing-based methods from  [5]  as our baseline. Baseline 1. We use the first baseline provided in the VoicePri-vacy2022 Challenge, which attempts to modify the x-vector for input speech signals. The method first extracts the xvector  [6] , the fundamental frequency (F0), and bottleneck features from the input speech signal. The method then modifies the extracted x-vector with an external pool of x-vectors before using a speech synthesizer to produce an anonymized speech from the extracted features. We use a variant of the first baseline with features extracted from a finetuned wav2vec2.0 model  [14]  and a HifiGAN-based speech synthesizer (more details can be found from Section 6.4.3 in  [23] ). Baseline 2. We use the second baseline provided in the VoicePrivacy2022 Challenge, which is a signal processing approach using McAdams transformation  [4] . The McAdam Coefficient is uniformly sampled from a fixed range to get a randomized version of the anonymization method in  [24] . Baseline 3. We use the R+MS method proposed by Kai et al.  [5] . It is a combination of Resampling and Modulation Spectrum Smoothing  [25] , and is the method suggested by Kai et al. to achieve strong privacy-utility trade-offs. Baseline 4. We use the R+MS+M+CH+CL method proposed by Kai et al.  [5] . The method consists of Resampling, Modulation Spectrum Smoothing, McAdams Transformation  [4] , clipping, and Chorus. It is the most secured approach from their provided toolkit  [5] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Details.",
      "text": "Implementation. Our PSE is a standard Transformer encoder, which consists of 6 layers with a dropout rate of 0.1. Each transformer encoder layer contains a self-attention module with 12 heads. The sizes of the feed-forward layers in each Transformer block are 3072. The model is optimized with the L1 loss using the Adam optimizer with a learning rate of 1e -4 and a batch size of 32 for 60 epochs. Before training, we split the Privacy-risk Saliency Map dataset into a training and validation sets of approximately 45, 000 and 5, 000 samples, respectively. We perform model selection based on the lowest loss on the validation set to avoid overfitting. For downstream tasks, we follow the training configurations provided in  [15] . Evaluation. For each task, we train a single model on the extracted features of wav2vec2 on the original utterances from the corresponding dataset, and use the trained model for assessing the privacy-utility performances of both our framework and the baselines. We report the performance of these models in the Original row in Table  1 . Assuming an Ignorant Attack model might underestimate the risk of speaker-reidentification, however, we are unable to train separate ASV models given the large number of (k, ) combinations (25 in total) and computation constraints. Training a single ASV (or ASR) model takes around 2 days on a Tesla V100 GPU. Moreover, although the model is unaware of the added noises to the representation, it is also not aware that the input speech are synthesized, making the comparisons fairer. We believe that the EER evaluated by the Ignorant Attack model can still inform crucial information about the relative privacy rankings of different methods. For example, as shown in Table  1 , Baseline 1 is more privacy-preserving than Baseline 2, which is consistent with the findings in  [23] . It is also important to note that the Lazy-informed and Semi-informed attack models provided in the VoicePrivacy2022 Challenge are not applicable to our work as we are evaluating ASV at a feature-level, i.e., we do not produce speech outputs.  Table  1 . Performance comparisons between the proposed method and the baselines.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  1  shows the privacy-utility evaluation for the baseline models. Figure  2  and 3  show the evaluation results for the utility and privacy metrics, respectively. To highlight our PSE's ability to correctly identify positions with sensitive information, we add another baseline that randomly chooses k% of the positions in the extracted wav2vec2 representations for perturbation with similar settings to our approach.\n\nWe can observe that the proposed approach enables a very flexible range of privacy-utility trade-offs, with EER ranging from 0.13 to 0.50 while the utility metrics ranging from random guessing performance to approximately the original performance. On the higher EER range (> 40%), the approach (k = 20%, = 1) shows better utility performance (51.23%, 44.18%, and 39.60% in IC, ER, and ASR respectively) com-pared to Baseline 4. Compared to Baseline 1 and Baseline 2 in the lower EER range (around 30%), our approach achieves superior ER performance (52.20% vs. 43.89%), comparable ASR WER performance (10.38% vs. 10.7%), and lower IC performance (74.19% vs. 86.97%) with k = 20%, = 4. In general, feature-level anonymization can be a competitive alternative to voice anonymization when voice outputs are not required, e.g., supervised machine learning applications.\n\nFrom Figure  3 , we can see that our method achieves superior (higher) EER curves compared to the random perturbation baselines in all and perturbation ratio variations. It is also important to note that the gap between our method and the random perturbation baselines gets smaller as the proportion of perturbed positions k gets larger, with the most significant gap observed at k = 20%. These confirm our assumption that certain positions within the extracted representations carry more sensitive information, and show that our PSE is able to identify high privacy-risk positions for perturbations. However, from Figure  2 , we can see that the dashed curves generally have better utility performances compared to the solid curves, with the exception of Speech Recognition with a reversed effect. This implies that the selected positions from PSE might potentially be also high-utility positions for downstream tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose a framework to perturb the extracted representations from pre-trained speech models, with the goal of generating privacy-preserving representation for data sharing. The framework consists of three stages: generating a privacy-risk saliency map dataset for representations extracted from a selected pre-trained model, training a Privacyrisk Saliency Estimator (PSE), and perturbing the extracted representations based on the selected positions from PSE. We validate our framework on a wide range of downstream tasks and find that sharing speech data in the form of extracted representations is competitive to the existing approaches that share speech via voice conversion or voice modification. We hope the results motivate further exploration in the direction of privacy-preserving pre-trained representation learning.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a high-level overview of the proposed method.",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of our framework. Solid lines show the",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed framework utility performance. Dashed lines represent the “random perturbed positions” baselines.",
      "page": 4
    },
    {
      "caption": "Figure 3: Proposed framework privacy performance. Dashed",
      "page": 4
    },
    {
      "caption": "Figure 2: and 3 show the evaluation results for the",
      "page": 4
    },
    {
      "caption": "Figure 3: , we can see that our method achieves su-",
      "page": 4
    },
    {
      "caption": "Figure 2: , we can see that the dashed",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Assuming an Igno-",
      "page": 3
    },
    {
      "caption": "Table 1: Performance comparisons between the proposed",
      "page": 4
    },
    {
      "caption": "Table 1: shows the privacy-utility evaluation for the baseline",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Fedhealth: A federated transfer learning framework for wearable healthcare",
      "authors": [
        "Yiqiang Chen"
      ],
      "year": "2020",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "3",
      "title": "Personalized speech recognition on mobile devices",
      "authors": [
        "Ian Mcgraw"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Understanding the tradeoffs in clientside privacy for speech recognition",
      "authors": [
        "Peter Wu"
      ],
      "year": "2021",
      "venue": "Understanding the tradeoffs in clientside privacy for speech recognition",
      "arxiv": "arXiv:2101.08919"
    },
    {
      "citation_id": "5",
      "title": "Spectral fusion, spectral parsing and the formation of auditory images",
      "authors": [
        "Stephen Edward"
      ],
      "year": "1984",
      "venue": "Spectral fusion, spectral parsing and the formation of auditory images"
    },
    {
      "citation_id": "6",
      "title": "Lightweight voice anonymization based on data-driven optimization of cascaded voice modification modules",
      "authors": [
        "Hiroto Kai"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "7",
      "title": "in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)",
      "authors": [
        "David Snyder"
      ],
      "year": "2018",
      "venue": "in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Design choices for x-vector based speaker anonymization",
      "authors": [
        "Brij Mohan",
        "Lal Srivastava"
      ],
      "year": "2020",
      "venue": "Design choices for x-vector based speaker anonymization"
    },
    {
      "citation_id": "9",
      "title": "Voice privacy using cyclegan and time-scale modification",
      "authors": [
        "P Gauri",
        "Prajapati"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "10",
      "title": "Privacy-preserving adversarial representation learning in asr: Reality or illusion?",
      "authors": [
        "Brij Mohan",
        "Lal Srivastava"
      ],
      "year": "2019",
      "venue": "Privacy-preserving adversarial representation learning in asr: Reality or illusion?"
    },
    {
      "citation_id": "11",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "authors": [
        "Mimansa Jaiswal"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Smoothgrad: removing noise by adding noise",
      "authors": [
        "Daniel Smilkov"
      ],
      "year": "2017",
      "venue": "Smoothgrad: removing noise by adding noise",
      "arxiv": "arXiv:1706.03825"
    },
    {
      "citation_id": "13",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "The algorithmic foundations of differential privacy",
      "authors": [
        "Cynthia Dwork"
      ],
      "year": "2014",
      "venue": "Found. Trends Theor. Comput. Sci"
    },
    {
      "citation_id": "15",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "17",
      "title": "Voxceleb: A large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Additive margin softmax for face verification",
      "authors": [
        "Feng Wang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "19",
      "title": "Introducing the VoicePrivacy Initiative",
      "authors": [
        "N Tomashenko"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Speech model pre-training for end-to-end spoken language understanding",
      "authors": [
        "Loren Lugosch"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "23",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "24",
      "title": "The voiceprivacy 2022 challenge evaluation plan",
      "authors": [
        "Natalia Tomashenko"
      ],
      "year": "2022",
      "venue": "The voiceprivacy 2022 challenge evaluation plan",
      "arxiv": "arXiv:2203.12468"
    },
    {
      "citation_id": "25",
      "title": "Speaker anonymisation using the mcadams coefficient",
      "authors": [
        "Jose Patino"
      ],
      "venue": "Interspeech 2021. ISCA, 2021"
    },
    {
      "citation_id": "26",
      "title": "The naist text-to-speech system for the blizzard challenge 2015",
      "authors": [
        "Shinnosuke Takamichi"
      ],
      "year": "2015",
      "venue": "Proc. Blizzard Challenge workshop"
    }
  ]
}