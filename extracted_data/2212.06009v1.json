{
  "paper_id": "2212.06009v1",
  "title": "An Approach For Improving Automatic Mouth Emotion Recognition",
  "published": "2022-12-12T16:17:21Z",
  "authors": [
    "Giulio Biondi",
    "Valentina Franzoni",
    "Osvaldo Gervasi",
    "Damiano Perri"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The study proposes and tests a technique for automated emotion recognition through mouth detection via Convolutional Neural Networks (CNN), meant to be applied for supporting people with health disorders with communication skills issues (e.g. muscle wasting, stroke, autism, or, more simply, pain) in order to recognize emotions and generate real-time feedback, or data feeding supporting systems. The software system starts the computation identifying if a face is present on the acquired image, then it looks for the mouth location and extracts the corresponding features. Both tasks are carried out using Haar Feature-based Classifiers, which guarantee fast execution and promising performance. If our previous works focused on visual micro-expressions for personalized training on a single user, this strategy aims to train the system also on generalized faces data sets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In this work, we present a system for mouth-based visual emotion recognition. Our purpose is to lay the basis for a health-care system for people who suffer from severe disease, e.g., strokes, or conditions such as autism, who may benefit from automated support of emotion recognition. Such systems can detect basic emotions from smartphone or computer camera devices, to produce feedback, either text, audio or visual for other humans, or a digital output to support other connected services. Connecting such an architecture to appropriate services could help users to convey their or others' emotions more effectively, providing augmented emotional stimuli, e.g., in case of users affected from a pathology which involve social relationship abilities, or when users experiment difficulties in recognizing emotions expressed by others. The system could also call a human assistant, e.g., for hospitalized patients feeling intense pain. In this paper, we focus on the mouth expression in correctly determining the emotion expressed by a subject. A crucial step is the selection of a reference model which classifies emotions, e.g., Ekman,  [54]  Plutchik, and Lovheim  [15] . We selected a basic subset of the Ekmann emotions: Joy and Disgust, together with the Neutral condition (i.e., no emotion expressed). Joy is among the simplest emotions to recognize through face expression, thus an ideal candidate for results comparisons concerning the state-of-the-art. Disgust, instead, is present in much fewer instances in available data sets, because it is more difficult to stimulate, and it is a less ideal but more interesting example of computation. We include the neutral state as a control state for recognition results on both emotions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Problem Description And Proposed Solution",
      "text": "Our study exploits the high precision of CNN processing to process mouth images to recognize emotional states. On one hand, we expect the system's capability to exploit best on the single user with personalized training; on the other hand, in this work we also test the technique on generic faces data sets, in order to find solutions to the following research questions:\n\n-With which precision it is possible to recognize facial emotions solely from the mouth? -Is the proposed technique capable of recognizing emotions if trained on a generalized set of facial images? In a user-centered implementation, the user trains the network on her/his facial expressions and the software supports personalized emotional feedback for each particular user: personal traits, such as scars or flaws, or individual variations in emotional feeling and expression, help the training to precise recognition. Then, we train the software also to recognize different users. In order to obtain optimized results, the ambient light setting needs a proper setup:\n\n-Robustness: The algorithm must be able to operate even in the presence of low-quality data (e.g., low resolution, bad light conditions); -Scalability: The user position should not be necessarily fixed in front of the camera, in order to avoid constraining the person. Therefore, the software should be able to recognize the user despite her/his position. -Luminosity: an important problem is precisely that of the variation of light. In computer vision (  [1] ), the variation of the lens involves an alteration of the information (  [41] . No complete control of the detected information is achieved: the system will be able to withstand variations in brightness without compromising the original information.\n\nThe proposed solution has been implemented in C++ and OpenCV graphics libraries; hence, it is compatible with all operating systems, with high reliability and constant support from the community.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "State Of The Art",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Deep Learning And Image Classification",
      "text": "The recent scientific focus on Deep Learning towards the end of the XX century has contributed to the rebirth of significant interest in neural networks. The real impact of Deep Learning began in the context of speech recognition around the year 2010, when two Microsoft Research employees, Lil Deng and Geoenix Hinton, realized that using large amounts of data for training a deep neural network resulted in lowering error rates far below the state of the art  [20] . Discoveries in the field of hardware have certainly contributed to the rise of interest in Deep Learning. In particular, the ever-powerful GPUs seem to be able to perform the countless mathematical calculations of matrices and vectors in Deep Learning  [11, 12, 13] . Actual GPUs allow reducing workout times from the weeks to a day. Recently, deep learning has been used for several types of research aiming at the classification of images and learning, trying to solve the limitations of machine learning, which reside in overfitting and domain dependence, with image adaptation, kernel randomization  [14]  and transfer learning  [21] . Commitment has been dedicated by researchers to exploit domain dependence as a feature, where personalized classification can quickly exploit a particular user or entity, especially for smart-home systems  [35]  and microblog sentiment tagging  [37] . Alternative approaches consider evolutionary algorithms,  [27][58] [59] random walks on semantic networks of images  [25] [26]  [60]  and max-product neural networks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "History And Description Of Neural Networks",
      "text": "Convolutional Neural Networks are among the most used methods for affective image classification  [22] [2] thanks to their flexibility for transfer learning, and easy tools available on the Web  [34] . An artificial Neural Network (NN), composed of artificial neurons 1, or nodes, can be used for solving artificial intelligence (AI) problems. NNs are biologically inspired, where a neural network is a network or circuit of neurons in the brain. The connections of the biological neuron are modeled as weights: a positive weight reflects an excitatory connection, while negative values mean inhibitory connections. In 1983 Geoff Hinton, now an emeritus professor at University of Toronto, co-invented Boltzmann machines,  [45]  one of the first types of neural networks to use statistical probabilities, then updating the strength of the connections within a neural network with backpropagation.  [46]  In the late-1970s and early-1980s, Hinton began working with neural networks when they were deeply unfashionable, because most computer scientists believed the technique was a dead end, while a better approach to Artificial Intelligence (AI) could be to explicitly encode human expertise in rules sets. Today we know that deep neural networks using backpropagation underpin most advances in AI, from Facebook friends automatic tagging, to the voice recognition capabilities of Amazon Alexa and Google Home, to its translation capability from previously difficult languages, such as Mandarin. LeCun, then, was a post-doc with Hinton's supervision, developing Convolutional Neural Networks as an improvement of the work on backpropagation. Bengio, who worked with LeCun on computer vision at Bell Labs, applied neural networks to natural language processing, leading to enormous advances in computer translation. Recently, he also built a model to allow neural networks to create novel and realistic images. In March 2019, Hinton, LeCun and Bengio received together the Turing Award, considered the Nobel prize for computing, for their advances in Artificial Intelligence with Deep Learning.  [47]  As we can see in Fig.  1 , a single perceptron (i.e., the NN atom) takes several binary inputs, x 1 , x 2 , . . . , x n and produces a single binary output. Weights w 1 , w 2 , . . . , w n can be introduced to express the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined whether the weighted sum j w j x j is less/greater than a threshold parameter value of the network:\n\nBy varying the weights and threshold, we can get different models of decisionmaking, thus different devices device capable to make decisions by weighting up evidence. A real NN will have several perceptrons in each column, and several cascade columns, where each columns is called a layer. A several-layers NN of perceptrons can engage sophisticated decision-making, adding variations to the comparison to the threshold. Several types of layers can be adapted to different calculation aims.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Convolutional Neural Networks",
      "text": "A Convolutional Neural Network (CNN) is a class of deep neural networks, most commonly applied to analyzing visual content, with excellent results on image recognition, segmentation, detection and retrieval.  [10, 44]  The key enabling factors behind such relevant results were principally techniques to scale up the networks to millions of parameters, where labeled data sets are needed to support the learning process. CNNs are able, under such conditions, to learn powerful and interpretative image features. Convolutional layers apply an operation of convolution to the input, which emulates the response of an individual neuron to visual stimuli, processes data only for its receptive field. A set of kernels (i.e., learning parameters), with a small receptive field, extend through the full depth of the input volume. A forward pass convolutes each filter across width and height of the volume in input, calculating the dot product between the filter entries and the input, thus producing a 2-dimensional activation map. The network results to learn filters activating when some specific type of feature is detected in a particular position of the input. Although fully connected neural networks can be used to learn features as well as classify data, a relevant amount of neurons is necessary due to the large input sizes of images, where compression is not always a good idea because any pixel may be relevant. E.g., a fully connected layer for an image of size 100 x 100 will have 10000 weights for each neuron. The operation of convolution offers a great solution to the problem, so tat the network can be deeper with fewer parameters. E.g., tiling regions of size 5 x 5, regardless of image size, having the same shared weights, require just 25 kernels. The problem of exploding gradients in traditional NNs with many layers is solved using backpropagation. Convolutional networks may also include local or global pooling layers, to reduce data dimension using a combination of the outputs of neuron clusters obtaining one neuron in the following layer.  The neurons in a layer will be connected to a small region of the previous layer, as illustrated in Fig.  2 , instead of all of the neurons, as happens in the fully-connected layer, which connects all the neurons in one layer to every neuron in another layer. A simple CNN is a sequence of layers, each of which transforms one volume of activations to another through a differentiable function. Typically, three types of layers are used: Convolutional Layer, Pooling Layer, and Fully-Connected Layer, which are then stacked together to form a full CNN architecture (see Fig.  3 ). In this way, CNN transforms the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters, and others do not.  [43]  In particular, the convolutional and the fully-connected layers perform transformations as a function of both the activations in the input volume, both the the weights and biases of the neurons (i.e. the parameters). On the other hand, the pooling and RELU layers, which apply an element-wise activation function, such as the max(0, x), will implement a fixed function.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Artificial Intelligence Assisting Health Care",
      "text": "For computerized health care assisting, multidisciplinary studies in Artificial Intelligence, Augmented Reality and Robotics stressed out the importance of computer science for automatizing real-life tasks for assistive and learning objects,  [56]  such as detecting words from labial movements (i.e. automated lip detection)  [29] , Virtual reality for prosthetic training  [24]  or neural telerehabilitation of patients with stroke  [6] , vocal interfaces for robotics applications  [30] . As an application of complex networks, it is possible to predict bacteria diffusion patterns,  [36] [61] as well as epidemiology data  [23] , having a viral spread. To be mentioned, huge advances are happening on medical image recognition and multi-stage feature selection for classification of cancer data  [32] , and of text corpora for medical or patient feedback in social networks. One of the most promising advances of recent years for AI-assisted health care is the opportunity to have light-implementation Mobile Apps, that can be quickly developed to be used in a friendly manner  [5] , to assist and support disabled users for communication and learning tasks. Such applications can be run directly on personal smartphones or wearable devices, for health monitoring and prognosis  [8]  as well as for interactive support for people with disabilities or conditions that can influence communication and learning, such as autism spectrum disorders  [7] . Using cloud services or networks in the Internet of Things (IoT), makes possible both to connect such devices to high capability servers, both to collect data in a distributed collaborative perspective  [19] , in order to feed big knowledge-bases, and increase the capability of the single object, i.e. of its owner, as a member of a vast interactive collective dynamic knowledge (i.e. a Big Data) network.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Affective Computing And Emotion Recognition",
      "text": "Multidisciplinary approaches recently stressed out the importance of recognizing and extracting affective and mental states, in particular emotions, for commu-nication, understanding, and supporting humans in any task with automated detectors and artificial assistants having machine emotional intelligence  [3] . In real-life problems, individuals transform overwhelming amounts of heterogeneous data in a manageable and personalized subset of classified items. The process of recognition of moods and sentiments is mostly complex. Recent research underlines that primary emotional states such as happiness, sadness, anger, disgust, or neutral state  [38]  can be recognized based on text,  [31]  physiological clues such as heart rate, skin conductance and face expression, differently from sentiment, moods and affect, which are more complex states and can be better managed with a multidimensional approach  [39]    [15] . Since Rosalind Picard defined the challenges for Affective Computing in 2003  [4] , numerous advances have been made in the task of emotion recognition, such as defining collective influence of emotions expressed online  [9] , stating that emotional expressiveness is the crucial fuel that sustains communities; studying cultural aspects of emotions in art  [16]  and its variations; create emotionally engaging experiences in games  [33] , where affective changes are crucial to the conscious experience of the world around us. Some of the more ethical and critical challenges defined by Rosalind Picard, however, remain open. For example, many of the modalities for emotion recognition (e.g., blood chemistry, brain activity, neurotransmitters) are not readily available, commercial tools are limited  [18] , data sets for training are not general  [17]  and people's emotion is so idiosyncratic and variable, that there is difficult to recognize an individual's emotional states from available data  [4] . Moreover, the challenge to use Affective Computing to help people, e.g., with self-aid tools is not widely faced in research, preferring applications to marketing.  [55]",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "The Proposed Emotion Recognition Engine",
      "text": "We based our Emotion Recognition Engine on popular open-source libraries: the image processing features are provided by OpenCV  [48] , version 3.1.0. First, the software recognizes the presence of a face in the image; when a face is found, the algorithm looks for the mouth location and extracts the corresponding subframe.  [28]  Both tasks are carried out using Haar Feature-based Classifiers  [49] , which guarantee fast execution and promising performance. A face detection pre-trained classifier is integrated into OpenCV; the mouth classifier, instead, is the one used in  [50] . During the training phase, samples of the subject are captured from the camera at regular intervals (or fed from disk) and used to produce a set of mouth snapshots. Such snapshots will form, after shuffling, the training, validation, and test set, with the first two used to train the networks with the Deep Learning framework Caffe  [34] . The remaining images are subsequently used to test the performances of the networks. The system has been designed to perform both offline and online recognition, i.e. recognize emotions from a series of pre-stored images or directly from a video feed.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Structure Of The Emex2 Cnn",
      "text": "In order to have a direct approach to the world of conundrum neural networks, the EmEx  [42]  approach, which we used in part of our tests, focuses on detecting user-centered emotions from the mouth. Network layers are set up to extract the specific information of the image data, accurately setting the parameters to have a valid recognition. In our CNN, training data are labeled with emotions, and the results of the layer computation are evaluated in terms of accuracy and loss. The neural network architecture is based on the popular LeNet-5,  [62] , consisting of several layers connected. The main level is for the data set and the corresponding tags. The second layer is the convoluted layer, where the convolution operations are performed on the input images, extracting features about each frame in each class. Then, a pooling layer is used to reduce the parameter magnitude, reducing in width and height, the volume of previously created data, with a time gain in computation. For scaling, a max function of a variable set is used. Different convolutional and pooling layers follow. An inner product layer innerProduct groups the information in a single numeric value, to be processed again in the following phases. The system is now capable of returning a vector representation of neurons, and it will no longer be possible to apply unambiguous layers. Another layer of innerProduct is then applied to put the layers in sequence. Thus, the last one will have an output parameter that equal to the number of classes needed for the classification. The K final values will be the parameters of a probability function that allows the final classification. In the training phase, the network ends with an Accuracy layer for network accuracy calculation, and with a Loss layer for the calculation of the error function needed for a correct and useful training phase. In the classification phase, instead, a SoftMax layer is the final layer to classify new images, which are not included in the data set (i.e., the test set images in our experiments). This layer calculates the likelihood of the most appropriate class in the grading phase, and therefore, its output represents the final solution.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "The Structure Of The Alexnet Cnn",
      "text": "AlexNet  [10] , the second network that we used in our experiments, is a network presented as a winner of the 2012 ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), on the ImageNet  [52]  data set, which includes ≈ 1.2 million pictures representing 1000 different objects in over 22000 categories. [53] Feed-forward networks could offer the power needed for such a huge data set, requiring much preprocessing work. Using still modern techniques, such as data augmentation and dropout, AlexaNet exploited the benefits of CNN and backed them up with record-breaking performance in the competition. The AlexaNet CNN is used in several applications; it consists of five convolutional layers, followed by three fully connected layers. Also, three max-pooling layers are inserted after, respectively, the first, second, and fifth convolutional layer, while the first two fully-connected layers are followed by a dropout layer, to avoid overfitting.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Image Collection And Training Phase",
      "text": "The first data set is a generalized faces data set, including faces from different ethnicity, gender, age: the 10k US Adult Faces database  [57]  from the Maryland Laboratory of Brain and Cognition of the USA National Institute of Mental Health, which includes 637 faces images labeled with Neutral state and 1511 with Joy emotion. For the experiments regarding a single user, we collected images for Joy, Disgust, and Neutral. During the training phase, the subject was presented with a list of videos and pictures, selected to elicit a particular emotion in the audience. In particular, a set of 62 short videos was used to elicit Joy, whereas 140 images were selected for Disgust. The participants were asked to sit, one by one, and watch the videos/images, while their reactions were recorded by the camera. Later, samples were extracted from the sections of the videos, which showed an evident reaction to the stimuli, at a rate of three frames per second. For the Neutral state, the test subject's expression was recorded watching relaxing images, where no particular emotion was elicited. As a basic rule, we decided that no media could be watched more than once, as the reaction would not be spontaneous anymore in case of multiple views. The collected samples were then shuffled, to equitably distribute frames belonging to the same sequences between training set, validation set, and test set.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experiments Design And Results",
      "text": "Three experiments were performed for this work, using the previously described two networks, i.e., AlexNet and Emex. In the first experiment, the networks were trained and tested on the data set composed of samples that we collected from our test subjects; in the second experiment, the same test was repeated on the 10k US Adult Faces database. Finally, a cross-domain experiment was conducted, testing the networks trained on 10k US Adult Faces database sample to recognize emotions in our single-users data sets.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Single-User Test",
      "text": "The first test was conducted on the samples collected from each test subjects, in order to see how the networks perform training and testing on the same user in different conditions, e.g., before and after a degenerative pathology, which may prevent the patient from expressing his own emotions and related needs in words. Results, shown in Table  1 , show that both networks easily overfit. During the training phase, a perfect accuracy (i.e., 1) was achieved after a few iterations: 50 for the EmEx network, and 150 for the Alexnet network. Further iterations were not necessary, because both networks showed a constant behavior, correctly classifying all the test images. The different training time to obtain the best performances is due to the much higher complexity of the AlexNet network with respect to EmEx,  [40]   [42]  in terms of the number of parameters to be optimized. AlexNet was originally designed for a much more complex problem, as stated in section 4.2, i.e., the identification of objects belonging to an extremely high number of classes. It is worth noticing that, although our task was quicker to tackle, inter-class differences may be less evident for our task than for the original ImageNet data. Therefore, our problem is more difficult to solve. Furthermore, the number of training samples used in our experiment was purposefully small, to assess performances in a context where high computing capabilities and data sets are not available, e.g., where the user can train emotional expressions on a mobile environment or a common desktop/laptop, with a relatively small number of images. Moreover, if such images are shot through a video, they will have less intra-dataset differences.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multiple-Users Test",
      "text": "The second test was performed on the 10k US Adult Faces database  [57] , including multiple-users images.   consistingly better than AlexNet, showing a better generalization capability in a completely different environment from the one it was trained for, e.g., with respect to light, user position, and image quality. This is probably due to the ability of AlexNet to better adapt to the peculiar characteristics of the data set it is trained on, thanks to its complexity, but having problems in a fairly different settings when no samples are given.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this work, we described a framework for Emotion Recognition from mouth expressions. Experiments, conducted on both single-user and generalized faces data sets, show good recognition performances of the framework, which can correctly identify the chosen emotions, using limited computational resources and doing it both online and offline. Results show that mouth expressions play an essential role in defining the emotion conveyed by the subject, and can be exploited with low computational power and complexity of systems. Both the tested networks achieved high recognition performances, with the AlexNet network better adapting to the single data set, and a seemingly better ability of the EmEx network to generalize the domain. Future works will investigate the ability of the framework in recognizing more emotions, and include the publication of our single-users image collection.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A simple example of a 3-input perceptron",
      "page": 4
    },
    {
      "caption": "Figure 2: A single CNN layer",
      "page": 5
    },
    {
      "caption": "Figure 3: Left: A regular 3-layer Neural Network. Right: A Convolutional Network ar-",
      "page": 5
    },
    {
      "caption": "Figure 2: , instead of all of the neurons, as happens in the",
      "page": 6
    },
    {
      "caption": "Figure 3: ). In this way, CNN transforms the original image layer by",
      "page": 6
    },
    {
      "caption": "Figure 4: Recognition performance of the two networks on the test samples",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results of tests using both test networks on the single-user data set",
      "data": [
        {
          "Network Training Steps Accuracy Micro-Averaged F1": ""
        },
        {
          "Network Training Steps Accuracy Micro-Averaged F1": "50\n100\n150\n200"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: Resultsoftestsusingthetwonetworksonthe10kUSAdultFacesdatabase",
      "data": [
        {
          "AlexNet": "Steps Accuracy F1",
          "EmEx": "Accuracy F1"
        },
        {
          "AlexNet": "0.0644\n0.0644\n0.8713\n0.8506\n0.8172\n0.8897\n0.8966\n0.7770\n0.8207\n0.6931",
          "EmEx": "0.8264\n0.8161\n0.7621\n0.7816\n0.7575\n0.7540\n0.8517\n0.8115\n0.7701"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Interestingly, the EmEx network performed",
      "data": [
        {
          "AlexNet": "Steps Accuracy F1",
          "EmEx": "Accuracy F1"
        },
        {
          "AlexNet": "0.5054\n0.7098\n0.6237\n0.6990\n0.5484\n0.6667\n0.6667\n0.6129\n0.7204\n0.5699",
          "EmEx": "0.6344\n0.4301\n0.8065\n0.8602\n0.8925\n0.8495\n0.8280\n0.8172\n0.8172"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Active Shape Models-Their Training and Application",
      "authors": [
        "T Cootes",
        "C Taylor",
        "D Cooper",
        "J Graham"
      ],
      "year": "1995",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "2",
      "title": "A Model-Based Gaze Tracking System",
      "authors": [
        "Rainer Stiefelhagen",
        "Jie Yang",
        "Alex Waibel"
      ],
      "year": "1997",
      "venue": "International Journal of Artificial Intelligence Tools"
    },
    {
      "citation_id": "3",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Affective computing: Challenges",
      "authors": [
        "R Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human Computer Studies",
      "doi": "10.1016/S1071-5819(03)00052-1"
    },
    {
      "citation_id": "5",
      "title": "Guidelines for web usability and accessibility on the Nintendo Wii, Transactions in Computer Science VI, part of",
      "authors": [
        "V Franzoni",
        "O Gervasi"
      ],
      "year": "2009",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-642-10649-12"
    },
    {
      "citation_id": "6",
      "title": "Virtual reality in neuro telerehabilitation of patients with traumatic brain injury and stroke",
      "authors": [
        "O Gervasi",
        "R Magni",
        "M Zampolini",
        "Nu!rehavr"
      ],
      "year": "2010",
      "venue": "Virtual Reality",
      "doi": "10.1007/s10055-009-0149-7"
    },
    {
      "citation_id": "7",
      "title": "Interactive visual supports for children with autism, Personal and Ubiquitous Computing",
      "authors": [
        "G Hayes",
        "S Hirano",
        "G Marcu",
        "M Monibi",
        "D Nguyen",
        "M Yeganyan"
      ],
      "year": "2010",
      "venue": "Interactive visual supports for children with autism, Personal and Ubiquitous Computing",
      "doi": "10.1007/s00779-010-0294-8"
    },
    {
      "citation_id": "8",
      "title": "A survey on wearable sensor-based systems for health monitoring and prognosis",
      "authors": [
        "A Pantelopoulos",
        "N Bourbakis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews"
    },
    {
      "citation_id": "9",
      "title": "Collective emotions online and their influence on community life",
      "authors": [
        "A Chmiel",
        "J Sienkiewicz",
        "M Thelwall",
        "G Paltoglou",
        "K Buckley",
        "A Kappas",
        "J Ho Lyst"
      ],
      "year": "2011",
      "venue": "PLoS ONE",
      "doi": "10.1371/journal.pone.0022207"
    },
    {
      "citation_id": "10",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoff Hinton"
      ],
      "year": "2012",
      "venue": "25th International Conference on Advance in Neural Information Processing System,pag"
    },
    {
      "citation_id": "11",
      "title": "The AES Implantation Based on OpenCL for Multi/many Core Architecture",
      "authors": [
        "O Gervasi",
        "D Russo",
        "F Vella"
      ],
      "year": "2010",
      "venue": "International Conference on Computational Science and Its Applications, Fukuoka, ICCSA 2010",
      "doi": "10.1109/ICCSA.2010.44"
    },
    {
      "citation_id": "12",
      "title": "A simulation framework for scheduling performance evaluation on CPU-GPU heterogeneous system",
      "authors": [
        "F Vella",
        "I Neri",
        "O Gervasi",
        "S Tasso"
      ],
      "year": "2012",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-642-31128-434"
    },
    {
      "citation_id": "13",
      "title": "Strategies and systems towards grids and clouds integration:A DBMS-based solution",
      "authors": [
        "M Mariotti",
        "O Gervasi",
        "F Vella",
        "A Cuzzocrea",
        "A Costantini"
      ],
      "year": "2018",
      "venue": "Future Generation Computer Systems",
      "doi": "10.1016/j.future.2017.02.047"
    },
    {
      "citation_id": "14",
      "title": "Efficient graph kernels by randomization Lecture Notes in Computer Science, 7523 LNAI",
      "authors": [
        "M Neumann",
        "N Patricia",
        "R Garnett",
        "K Kersting"
      ],
      "year": "2012",
      "venue": "Efficient graph kernels by randomization Lecture Notes in Computer Science, 7523 LNAI"
    },
    {
      "citation_id": "15",
      "title": "Automated classification of book blurbs according to the emotional tags of the social network Zazie CEUR Workshop Proceedings",
      "authors": [
        "V Franzoni",
        "V Poggioni",
        "F Zollo"
      ],
      "year": "2013",
      "venue": "Automated classification of book blurbs according to the emotional tags of the social network Zazie CEUR Workshop Proceedings",
      "doi": "10.13140/RG.2.1.3194.7689"
    },
    {
      "citation_id": "16",
      "title": "Emotional responses to artworks in online collections UMAP Workshops Proceedings",
      "authors": [
        "F Bertola",
        "V Patti"
      ],
      "year": "2013",
      "venue": "Emotional responses to artworks in online collections UMAP Workshops Proceedings"
    },
    {
      "citation_id": "17",
      "title": "Evaluation datasets for Twitter sentiment analysis a survey and a new dataset, the STS-Gold",
      "authors": [
        "H Saif",
        "M Fernandez",
        "Y He",
        "H Alani"
      ],
      "year": "1096",
      "venue": "Evaluation datasets for Twitter sentiment analysis a survey and a new dataset, the STS-Gold"
    },
    {
      "citation_id": "18",
      "title": "Potential and limitations of commercial sentiment detection tools",
      "authors": [
        "M Cieliebak",
        "O Dürr",
        "F Uzdilli"
      ],
      "year": "1096",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "19",
      "title": "Learning objects efficient handling in a federation of science distributed repositories",
      "authors": [
        "S Tasso",
        "S Pallottelli",
        "M Rui",
        "A Laganá"
      ],
      "year": "2014",
      "venue": "LNCS",
      "doi": "10.1007/978-3-319-09144-042"
    },
    {
      "citation_id": "20",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "All Rights Reserved",
      "doi": "10.1038/nature14539"
    },
    {
      "citation_id": "21",
      "title": "Visual Domain Adaptation: A survey of recent advances",
      "authors": [
        "V Patel",
        "R Gopalan",
        "R Li",
        "R Chellappa"
      ],
      "year": "2015",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "22",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Combining high-resolution contact data with virological data to investigate influenza transmission in a tertiary care hospital",
      "authors": [
        "N Voirin",
        "C Payet",
        "A Barrat",
        "C Cattuto",
        "N Khanafer",
        "C Regis",
        "B.-A Kim",
        "B Comte",
        "J.-S Casalegno",
        "B Lina",
        "P Vanhems"
      ],
      "year": "2015",
      "venue": "Infection Control and Hospital Epidemiology"
    },
    {
      "citation_id": "24",
      "title": "Exploring virtual reality and prosthetic training",
      "authors": [
        "I Phelan",
        "M Arden",
        "C Garcia",
        "C Roast"
      ],
      "year": "2015",
      "venue": "IEEE Virtual Reality Conference"
    },
    {
      "citation_id": "25",
      "title": "Context-based image semantic similarity 12th International Conference on Fuzzy Systems and Knowledge Discovery",
      "authors": [
        "V Franzoni",
        "A Milani",
        "S Pallottelli",
        "C Leung",
        "Y Li"
      ],
      "year": "2015",
      "venue": "Context-based image semantic similarity 12th International Conference on Fuzzy Systems and Knowledge Discovery"
    },
    {
      "citation_id": "26",
      "title": "Multi-path traces in semantic graphs for latent knowledge elicitation",
      "authors": [
        "S Pallottelli",
        "V Franzoni",
        "A Milani"
      ],
      "year": "2016",
      "venue": "Proceedings -International Conference on Natural Computation",
      "doi": "10.1109/ICNC.2015.7378004"
    },
    {
      "citation_id": "27",
      "title": "Semantic context extraction from collaborative networks",
      "authors": [
        "V Franzoni",
        "A Milani"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design",
      "doi": "10.1109/CSCWD.2015.7230946"
    },
    {
      "citation_id": "28",
      "title": "lip contour detection techniques based on front view of face",
      "authors": [
        "Trent Lewis",
        "David Powers"
      ],
      "year": "2011",
      "venue": "Journal of Global Research in Computer Science"
    },
    {
      "citation_id": "29",
      "title": "A Method for Predicting Words by Interpreting Labial Movements",
      "authors": [
        "Osvaldo Gervasi",
        "Riccardo Magni",
        "Matteo Ferri"
      ],
      "year": "2016",
      "venue": "ICCSA",
      "doi": "10.1007/978-3-319-42108-734"
    },
    {
      "citation_id": "30",
      "title": "Speaky for robots: the development of vocal interfaces for robotic applications",
      "authors": [
        "E Bastianelli",
        "D Nardi",
        "L Aiello",
        "F Giacomelli",
        "N Manes"
      ],
      "year": "2016",
      "venue": "Applied Intelligence",
      "doi": "10.1007/s10489-015-0695-5"
    },
    {
      "citation_id": "31",
      "title": "Web-based similarity for emotion recognition in web objects",
      "authors": [
        "Giulio Biondi",
        "Valentina Franzoni",
        "Yuanxi Li",
        "Alfredo Milani"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th International Conference on Utility and Cloud Computing"
    },
    {
      "citation_id": "32",
      "title": "Multistage feature selection approach for highdimensional cancer data",
      "authors": [
        "A Alkuhlani",
        "M Nassef",
        "I Farag"
      ],
      "year": "2017",
      "venue": "Soft Computing"
    },
    {
      "citation_id": "33",
      "title": "Eliciting emotions in design of games -A theory driven approach",
      "authors": [
        "A Canossa",
        "J Badler",
        "M El-Nasr",
        "E Anderson"
      ],
      "year": "1680",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "34",
      "title": "last visited on Sept",
      "authors": [
        "Caffe Framework"
      ],
      "year": "2018",
      "venue": "last visited on Sept"
    },
    {
      "citation_id": "35",
      "title": "Personalized gesture interactions for cyber-physical smart-home environments",
      "authors": [
        "Y Lou",
        "W Wu",
        "R.-D Vatavu",
        "W Tsai"
      ],
      "year": "2017",
      "venue": "Science China Information Sciences",
      "doi": "10.1007/s11432-015-1014-7"
    },
    {
      "citation_id": "36",
      "title": "A multistrain bacterial diffusion model for link prediction",
      "authors": [
        "V Franzoni",
        "A Chiancone",
        "A Milani"
      ],
      "year": "2017",
      "venue": "International Journal of Pattern Recognition and Artificial Intelligence",
      "doi": "10.1142/S0218001417590248"
    },
    {
      "citation_id": "37",
      "title": "Personalized microblog recommendation using sentimental features",
      "authors": [
        "W Cui",
        "Y Du",
        "Z Shen",
        "Y Zhou",
        "J Li"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Big Data and Smart Computing",
      "doi": "10.1109/BIGCOMP.2017.7881756"
    },
    {
      "citation_id": "38",
      "title": "Cybernetics of the Mind, Learning Individual's Perceptions Autonomously",
      "authors": [
        "P Angelov",
        "X Gu",
        "J Iglesias",
        "A Ledezema",
        "A Sanchis",
        "O Sipele",
        "R Ramezani"
      ],
      "year": "2017",
      "venue": "IEEE Systems, Man, and Cybernetics Magazine",
      "doi": "10.1109/MSMC.2017.2664478"
    },
    {
      "citation_id": "39",
      "title": "Emotional Affordances in Human-Machine Interactive Planning and Negotiation",
      "authors": [
        "V Franzoni",
        "A Milani",
        "J Vallverdu"
      ],
      "year": "2017",
      "venue": "Proceedings of WI 2017, Workshop on Affective Computing and Emotion Recognition (ACER)",
      "doi": "10.1145/3106426.3109421"
    },
    {
      "citation_id": "40",
      "title": "EmEx, a Tool for Automated Emotive Face Recognition Using Convolutional Neural Networks, ICCSA 2017",
      "authors": [
        "M Riganelli",
        "V Franzoni",
        "O Gervasi",
        "S Tasso"
      ],
      "year": "2017",
      "venue": "Lecture Notes in Computer Science",
      "doi": "10.1007/978-3-319-62398-649"
    },
    {
      "citation_id": "41",
      "title": "Autonomous Hexapod Robot With Artificial Vision and Remote Control by Myo-Electric Gestures. Cyber-Physical Systems for Next-Generation Networks",
      "authors": [
        "V Franzoni"
      ],
      "year": "2018",
      "venue": "Autonomous Hexapod Robot With Artificial Vision and Remote Control by Myo-Electric Gestures. Cyber-Physical Systems for Next-Generation Networks",
      "doi": "10.4018/978-1-5225-5510-0.ch007"
    },
    {
      "citation_id": "42",
      "title": "Automating facial emotion recognition",
      "authors": [
        "O Gervasi",
        "V Franzoni",
        "A Riganelli",
        "S Tasso"
      ],
      "year": "2019",
      "venue": "Web Intelligence",
      "doi": "10.3233/WEB-190397"
    },
    {
      "citation_id": "43",
      "title": "Design and Experimentation of Target-Driven Visual Navigation in Simulated and Real Environment via Deep Reinforcement Learning Architecture for Robotics Applications",
      "authors": [
        "G Mezzetti"
      ],
      "year": "2019",
      "venue": "Design and Experimentation of Target-Driven Visual Navigation in Simulated and Real Environment via Deep Reinforcement Learning Architecture for Robotics Applications"
    },
    {
      "citation_id": "44",
      "title": "Learning hierarchical features for scene labeling",
      "authors": [
        "Clement Farabet",
        "Camille Couprie",
        "Laurent Najman",
        "Yann Lecun"
      ],
      "year": "2013",
      "venue": "Learning hierarchical features for scene labeling"
    },
    {
      "citation_id": "45",
      "title": "Massively parallel architectures for Al: NETL, Thistle, and Boltzmann machines",
      "authors": [
        "Scott Fahlman",
        "Geoffrey Hinton",
        "Terrence Sejnowski"
      ],
      "year": "1983",
      "venue": "National Conference on Artificial Intelligence, AAAI"
    },
    {
      "citation_id": "46",
      "title": "Experiments on learning by Backpropagation",
      "authors": [
        "D Plaut",
        "S Nowlan",
        "G Hinton"
      ],
      "year": "1986",
      "venue": "Experiments on learning by Backpropagation"
    },
    {
      "citation_id": "47",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "48",
      "title": "The OpenCV Library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal of Software Tools"
    },
    {
      "citation_id": "49",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2005",
      "venue": "Rapid object detection using a boosted cascade of simple features"
    },
    {
      "citation_id": "50",
      "title": "Using Incremental Principal Component Analysis to Learn a",
      "authors": [
        "Castrillón Santana",
        "M Déniz Suárez",
        "O Hernández Sosa",
        "Lorenzo Navarro"
      ],
      "year": "2007",
      "venue": "Gender Classifier Automatically. In 1st Spanish Workshop on Biometrics"
    },
    {
      "citation_id": "51",
      "title": "",
      "authors": [
        "D Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "52",
      "title": "ImageNet: A largescale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "An Argument for Basic Emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "54",
      "title": "Emotion Recognition for Self-aid in Addiction Treatment, Psychotherapy, and Nonviolent Communication",
      "authors": [
        "V Franzoni",
        "A Milani"
      ],
      "year": "2019",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "55",
      "title": "Emotional machines: The next revolution",
      "authors": [
        "V Franzoni",
        "A Milani",
        "D Nardi",
        "J Vallverdú"
      ],
      "year": "2019",
      "venue": "Web Intelligence"
    },
    {
      "citation_id": "56",
      "title": "The intrinsic memorability of face images",
      "authors": [
        "W Bainbridge",
        "P Isola",
        "A Oliva"
      ],
      "year": "2013",
      "venue": "Journal of Experimental Psychology: General. Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "57",
      "title": "Planning in reactive environments, Computational Intelligence",
      "authors": [
        "A Milani",
        "V Poggioni"
      ],
      "year": "2007",
      "venue": "Planning in reactive environments, Computational Intelligence",
      "doi": "10.1111/j.1467-8640.2007.00315.x"
    },
    {
      "citation_id": "58",
      "title": "Experimental evaluation of pheromone models in ACOPlan",
      "authors": [
        "M Baioletti",
        "A Milani",
        "V Poggioni",
        "F Rossi"
      ],
      "year": "2011",
      "venue": "Annals of Mathematics and Artificial Intelligence",
      "doi": "10.1007/s10472-011-9265-7"
    },
    {
      "citation_id": "59",
      "title": "A Bidirectional Heuristic Search for web service composition with costs",
      "authors": [
        "N Ukey",
        "R Niyogi",
        "K Singh",
        "A Milani",
        "V Poggioni"
      ],
      "year": "2010",
      "venue": "International Journal of Web and Grid Services",
      "doi": "10.1504/IJWGS.2010.033790"
    },
    {
      "citation_id": "60",
      "title": "Improving Link Ranking Quality by Quasi-Common Neighbourhood",
      "authors": [
        "A Chiancone",
        "V Franzoni",
        "R Niyogi",
        "A Milani"
      ],
      "year": "2015",
      "venue": "Proc. of 15th ICCSA 2015",
      "doi": "10.1109/ICCSA.2015.19"
    },
    {
      "citation_id": "61",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/5.726791"
    }
  ]
}