{
  "paper_id": "2510.02320v1",
  "title": "Wee-Therapy: A Mixture Of Weak Encoders Framework For Psychological Counseling Dialogue Analysis",
  "published": "2025-09-24T05:43:53Z",
  "authors": [
    "Yongqi Kang",
    "Yong Zhao"
  ],
  "keywords": [
    "Psychological Counseling Analysis",
    "Audio Language Models",
    "Domain Adaptation",
    "Multi-task Learning",
    "WEE Architecture"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The advancement of computational psychology requires AI tools capable of deeply understanding counseling dialogues. Existing audio language models (AudioLLMs) often rely on single speech encoders pre-trained on general data, struggling to capture domain-specific features like complex emotions and professional techniques. To address this, we propose WEE-Therapy, a multi-task AudioLLM incorporating a Weak Encoder Ensemble (WEE) mechanism. This supplements a powerful base encoder with a pool of lightweight, specialized encoders. A novel dual-routing strategy combines stable, data-independent domain knowledge with dynamic, data-dependent expert selection. Evaluated on emotion recognition, technique classification, risk detection, and summarization, WEE-Therapy achieves significant performance gains across all tasks with minimal parameter overhead, demonstrating strong potential for AI-assisted clinical analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Mental health is a core pillar of human well-being, and psychological counseling, as a crucial safeguard, faces multiple challenges such as resource shortages, high supervision costs, and subjective analysis methods. In recent years, breakthrough advancements in artificial intelligence, particularly in natural language processing (NLP)-especially the powerful dialogue and reasoning capabilities demonstrated by large language models (LLMs)  [1] -have provided a new paradigm for developing computational tools. Among these, audio language models (AudioLLMs), which can directly understand raw speech rich in paralinguistic information (such as tone, pauses, and emotions), are particularly well-suited for in-depth analysis of psychological counseling dialogues. They hold the potential to enable objective quantification of dialogue processes, automatic identification of intervention techniques, and timely warnings of high-risk moments.\n\nHowever, directly applying advanced AudioLLMs to the highly specialized domain of psychological counseling reveals a significant \"domain adaptation\" gap. Existing mainstream methods typically rely on large speech encoders (e.g., Whisper  [2] ) pre-trained on general corpora (e.g., Lib-riSpeech), whose representational spaces are not optimized for capturing domain-specific features in psychological counseling. Counseling dialogues are filled with complex emotional fluctuations, specific professional terminology, subtle turn-taking, and silences and sighs that carry critical information-nuances that are difficult for general-purpose encoders to fully capture. Although scaling up the model or conducting comprehensive domain-specific pre-training could mitigate this issue, these approaches face significant obstacles in terms of data acquisition, computational costs, and deployment feasibility.\n\nA promising solution is the Mixture of Weak Encoders (MoWE) architecture  [3] . Instead of seeking a single \"allpowerful\" giant encoder, this approach employs a powerful base encoder supplemented by a set of lightweight \"expert\" encoders, dynamically integrating their features through a routing mechanism. This architecture has already demonstrated its ability to efficiently expand model capabilities in general audio tasks. However, its effectiveness, adaptation methods, and potential value in specialized domains such as psychological counseling remain an unexplored open question.\n\nTo address this research gap, this paper introduces WEE-Therapy, a multi-task AudioLLM framework specifically tailored for psychological counseling analysis. Our core idea is to leverage domain knowledge-driven integration and adaptation to transform the existing WEE paradigm into an effective tool for addressing domain-specific challenges. Specifically, the main contributions of this study are as follows:\n\n• Pioneering Domain Application: To the best of our knowledge, this is the first systematic application of the MoWE architecture in the field of computational psychology, providing a novel and efficient solution to the arXiv:2510.02320v1 [eess.AS] 24 Sep 2025 domain adaptation challenges in psychological counseling analysis.\n\n• Domain-Adapted Design: Rather than simply reusing existing models, we made key adaptations based on domain insights. These include constructing a mixed pool integrating an emotion expert encoder and designing a dual-routing strategy that incorporates domain priors to ensure stable extraction of psychologically critical features.\n\n• Systematic Empirical Evaluation: We built a comprehensive analysis system and conducted a thorough evaluation across four core tasks: emotion recognition, counseling technique classification, crisis risk detection, and dialogue summarization. The experimental results not only validate the effectiveness of the framework but also provide in-depth insights into the specialized behaviors of different \"expert\" encoders, offering valuable references for future research.\n\n• Application Value Orientation: This study clearly highlights the significant application potential of the proposed framework for developing low-cost, highefficiency AI-assisted clinical supervision and analysis systems. The methodology also offers broad implications for adapting general large models to other vertical domains.\n\nThe remainder of this paper is structured as follows: Section 2 reviews the WEE-THERAPY framework; Section 3 elaborates on the experimental setup; Section 4 got conclusion;",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Wee-Therapy Framework",
      "text": "This section will elaborate in detail on the proposed WEE-Therapy framework. The core idea of this framework is to enhance the base audio language model (AudioLLM) through a WEE module, enabling it to better adapt to the complexity and specificity of psychological counseling dialogues. The overall architecture of the system is illustrated in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overall Framework (Wee-Therapy Framework)",
      "text": "Our framework primarily consists of the following three core components:\n\n1. Encoder Layer: Responsible for converting the input raw counseling dialogue audio into high-dimensional feature representations.\n\n• Strong Base Encoder (E base ): We employ a large-scale, high-performance general-purpose speech encoder as the backbone, such as Whisperlarge-v3  [2] . This encoder has a substantial number of parameters (∼637M) and excels in general speech recognition tasks, providing us with a stable and powerful base audio representation z base = E base (a i ), where a i represents the i-th input audio segment.\n\n• Weak Encoder Pool ({E k } M k=1 ): To supplement the fine-grained features that the base encoder might miss in the vertical domain of psychological counseling, we introduce a pool of M lightweight encoders. These \"weak\" encoders have significantly fewer parameters than the base encoder (typically an order of magnitude less), such as HuBERT-base  [4] , Wav2Vec2.0base  [5] , or specialized encoders fine-tuned on emotion datasets (e.g., IEMOCAP  [6] ). They each have their own strengths, collectively forming a flexible \"committee of experts.\" 2. WEE Routing and Fusion Layer: This is the innovative core of this work. This layer contains a Router, whose function is to intelligently select and activate the most relevant subset from the weak encoder pool based on the input audio. Specifically, we designed a dual-routing strategy (detailed in Section 3.2), which generates both data-dependent and data-independent weak encoder features z dep and z indep . Subsequently, these weak encoder features are concatenated (Concatenation) with the base encoder features along the feature dimension to form the final enhanced audio representation:\n\nThis approach greatly enriches the information content of the input features without increasing the sequence length (and thus without significantly increasing the computational burden on the LLM).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Large Language Model Layer:",
      "text": "• Adapter & Projection: Since the output embedding dimensions of the audio encoder typically do not match the input space of the LLM, we use a lightweight adapter (e.g., a linear layer plus GELU activation) to perform downsampling on the concatenated features z i , and then map them to the LLM's token embedding space via a projection layer, generating audio tokens token ai = proj(adapter(z i )).\n\n• Text Generation: Simultaneously, the text instruction t i (e.g., \"Analyze the counselor's techniques in this dialogue\") is converted into text tokens token ti via the LLM's tokenizer. The audio tokens and text tokens are concatenated along the sequence dimension and fed into a large language model (e.g., Llama-3-8B-Instruct  [7] ). The LLM generates the analysis result in text form ŷi = LLM ([token ai ; token ti ]) in an autoregressive manner using prefix-conditioned generation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Weak Encoder Ensemble",
      "text": "The workflow of the dual-routing strategy in the MoWE module is as follows:\n\n• Data-Independent Router: The goal of this router is to select a fixed weak encoder that provides a global, content-agnostic supplement of domain knowledge for every input sample. For example, it might always prefer the encoder fine-tuned on emotional data to ensure that the emotional features of all counseling dialogues are enhanced. Its computation process is as follows:\n\nHere, w indep ∈ R M is a learnable parameter vector, which can be initialized with priors (e.g., setting a higher initial value for the emotion encoder).\n\nKeepTop1 is an operator that returns a one-hot vector where only the position with the highest weight is 1, and the others are 0.\n\n• Data-Dependent Router: The goal of this router is to act as an \"on-site conductor,\" dynamically selecting the most appropriate \"on-site expert\" based on the specific content of the current input audio. Its decision relies on the global audio features extracted by the base encoder. The computation process is as follows:\n\nzi,base = MeanPool(z i,base )\n\nHere, W dep ∈ R d base ×M is a learnable projection matrix that maps the features of the base encoder to a routing score space corresponding to the number of weak encoders M .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Objective",
      "text": "The model is trained using a multi-task learning paradigm. The total loss function consists of two parts:\n\n1. Next-Token Prediction Loss (L next-token ): This is the standard autoregressive loss for training the LLM, i.e., maximizing the likelihood of the target response sequence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wee Routing Loss (L Wee ):",
      "text": "To train the routers to make good and balanced decisions, we design a specialized auxiliary loss:\n\n• Entropy Loss (L indep-ent & L dep-ent ): Encourages the router to make \"confident\" decisions, i.e., producing a sharper output distribution.\n\n• Diversity Loss (L dep-div ): Prevents the datadependent router from always selecting the same encoder, encouraging the utilization of all weak encoders.\n\nHere, B is the training batch size.\n\nDuring training, we freeze most parameters of the base encoder and the LLM. We primarily fine-tune the routing networks, the adapter, the projection layer, and a small number of trainable parameters injected into the LLM via LoRA (Low-Rank Adaptation)  [8] . This is an efficient parameter fine-tuning strategy that effectively prevents overfitting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "To comprehensively evaluate the effectiveness of our proposed WEE-Therapy framework, we designed multi-task experiments and conducted tests on several representative psychological counseling datasets. This section elaborates in detail on the tasks and datasets used in the experiments, evaluation metrics, model implementation details, and training configurations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tasks And Datasets",
      "text": "We selected four tasks that comprehensively reflect the core requirements of psychological counseling analysis. Due to the sensitivity of psychological counseling data, publicly available datasets are limited. Our experiments are partially based on existing public datasets and partially on simulated data. Table  1  summarizes the tasks, datasets, and evaluation metrics used in the experiments. All audio inputs were uniformly cropped or padded to 30 seconds during preprocessing and resampled to 16kHz to meet the input requirements of the encoders.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "• Base Models:\n\n-Strong Base Encoder: We adopted Whisperlarge-v3 [2] (∼637M parameters) as the default strong base encoder. Its strong performance on general speech tasks provides a solid foundation for our system.\n\n-Large Language Model: We primarily used Llama-3-8B-Instruct  [7]  as the core backbone for text generation. To validate the generality of the method, we also conducted supplementary experiments on Zephyr-7B  [11]  and Phi-3-mini-4k-instruct  [12]  (3.8B parameters).\n\n• Weak Encoder Pool:\n\n-In our main experiments, the WEE pool contained 3 weak encoders to balance performance and efficiency:\n\n1. Whisper-tiny  [2]  (39M parameters): A lightweight general-purpose speech encoder that provides efficient speech content perception. 2. HuBERT-base  [4]  (95M parameters): Trained based on self-supervised learning, it excels at learning discrete representations of speech and is sensitive to phonemes and acoustic content. 3. Emotion-Finetuned-HuBERT (95M parameters): An encoder obtained by finetuning HuBERT-base on the IEMOCAP  [6]  emotion recognition dataset, specifically designed to capture emotional features.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 2.1. Overall Framework (WEE-Therapy Framework)",
      "page": 2
    },
    {
      "caption": "Figure 1: Overall architecture of the proposed MoWe-Therapy",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sichuan Unversity": "Department of Computer Science"
        },
        {
          "Sichuan Unversity": "dialogue processes,\nautomatic identification of\nintervention"
        },
        {
          "Sichuan Unversity": "techniques, and timely warnings of high-risk moments."
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "However,\ndirectly\napplying\nadvanced AudioLLMs\nto"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "the highly specialized domain of psychological\ncounsel-"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "ing reveals a significant “domain adaptation” gap. Existing"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "mainstream methods typically rely on large speech encoders"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "(e.g., Whisper [2]) pre-trained on general corpora (e.g., Lib-"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "riSpeech), whose representational spaces are not optimized"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "domain-specific\nfeatures\nfor\ncapturing\nin\npsychological"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "counseling.\nCounseling dialogues are filled with complex"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "emotional\nfluctuations,\nspecific\nprofessional\nterminology,"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "subtle turn-taking, and silences and sighs that carry critical"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "information—nuances\nthat are difficult\nfor general-purpose"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "encoders\nto fully capture.\nAlthough scaling up the model"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "or\nconducting\ncomprehensive\ndomain-specific\npre-training"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "could mitigate this\nissue,\nthese approaches\nface significant"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "obstacles in terms of data acquisition, computational costs,"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "and deployment feasibility."
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "A promising solution is the Mixture of Weak Encoders"
        },
        {
          "Sichuan Unversity": "(MoWE) architecture [3].\nInstead of seeking a single “all-"
        },
        {
          "Sichuan Unversity": "powerful” giant encoder,\nthis approach employs a powerful"
        },
        {
          "Sichuan Unversity": "base encoder supplemented by a set of lightweight “expert”"
        },
        {
          "Sichuan Unversity": "encoders,\ndynamically integrating their\nfeatures\nthrough a"
        },
        {
          "Sichuan Unversity": "routing mechanism.\nThis architecture has already demon-"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "strated its ability to efficiently expand model capabilities in"
        },
        {
          "Sichuan Unversity": "its effectiveness, adaptation\ngeneral audio tasks. However,"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "methods, and potential value in specialized domains such"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "as psychological counseling remain an unexplored open"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "question."
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "To address this research gap, this paper introduces WEE-"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "Therapy,\na multi-task AudioLLM framework specifically"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "tailored\nfor\npsychological\ncounseling\nanalysis.\nOur\ncore"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "idea is to leverage domain knowledge-driven integration"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "and adaptation to transform the existing WEE paradigm"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "into an effective tool for addressing domain-specific chal-"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "lenges. Specifically,\nthe main contributions of this study are"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "as follows:"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "• Pioneering Domain Application: To the best of our"
        },
        {
          "Sichuan Unversity": ""
        },
        {
          "Sichuan Unversity": "knowledge, this is the first systematic application of the"
        },
        {
          "Sichuan Unversity": "MoWE architecture in the field of computational psy-"
        },
        {
          "Sichuan Unversity": "chology, providing a novel and efficient solution to the"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "domain adaptation challenges\nin psychological coun-": "seling analysis."
        },
        {
          "domain adaptation challenges\nin psychological coun-": "• Domain-Adapted Design: Rather than simply reusing"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "existing models, we made key adaptations based on do-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "main insights. These include constructing a mixed pool"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "integrating an emotion expert encoder and designing a"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "dual-routing strategy that\nincorporates domain priors"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "to ensure stable extraction of psychologically critical"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "features."
        },
        {
          "domain adaptation challenges\nin psychological coun-": "• Systematic Empirical Evaluation: We built a com-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "prehensive analysis system and conducted a thorough"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "evaluation across four core tasks: emotion recognition,"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "counseling technique classification,\ncrisis\nrisk detec-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "tion, and dialogue summarization.\nThe experimental"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "results not only validate the effectiveness of the frame-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "work but also provide in-depth insights into the special-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "ized behaviors of different “expert” encoders, offering"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "valuable references for future research."
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "• Application Value Orientation:\nThis\nstudy clearly"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "highlights the significant application potential of the"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "proposed\nframework\nfor\ndeveloping\nlow-cost,\nhigh-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "efficiency AI-assisted clinical supervision and analysis"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "systems. The methodology also offers broad implica-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "tions for adapting general large models to other vertical"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "domains."
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "The remainder of this paper is structured as follows: Sec-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "tion 2 reviews\nthe WEE-THERAPY framework; Section 3"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "elaborates on the experimental setup; Section 4 got conclu-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "sion;"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "2. THE WEE-THERAPY FRAMEWORK"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "This section will elaborate in detail on the proposed WEE-"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "Therapy framework. The core idea of this framework is to"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "enhance the base audio language model (AudioLLM) through"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "a WEE module, enabling it to better adapt to the complexity"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "and specificity of psychological counseling dialogues.\nThe"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "overall architecture of the system is illustrated in Figure 1."
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "2.1. Overall Framework (WEE-Therapy Framework)"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "Our framework primarily consists of the following three core"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "components:"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "1. Encoder Layer: Responsible for converting the input"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "raw counseling dialogue audio into high-dimensional"
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "feature representations."
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": ""
        },
        {
          "domain adaptation challenges\nin psychological coun-": "• Strong Base Encoder\nemploy a\n(Ebase): We"
        },
        {
          "domain adaptation challenges\nin psychological coun-": "large-scale,\nhigh-performance\ngeneral-purpose"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "do not match the input\nspace of\nthe LLM, we": "use\na\nlightweight\nadapter\n(e.g.,\na\nlinear\nlayer",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "trix that maps the features of the base encoder to a rout-"
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "plus GELU activation)\nto\nperform downsam-",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "ing score space corresponding to the number of weak"
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "and then\npling on the concatenated features zi,",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "map them to the LLM’s token embedding space",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "via\na projection layer,\ngenerating audio tokens",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "tokenai = proj(adapter(zi)).",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "• Text Generation:\nSimultaneously,\nthe\ntext\nin-",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "is trained using a multi-task learning paradigm."
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "(e.g., “Analyze the counselor’s tech-\nstruction ti",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "niques in this dialogue”) is converted into text to-",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "kens tokenti via the LLM’s tokenizer. The audio",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "tokens and text tokens are concatenated along the",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "sequence dimension and fed into a large language",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "1. Next-Token Prediction Loss (Lnext-token): This is the"
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "model\n(e.g., Llama-3-8B-Instruct\n[7]).\nThe",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "i.e.,"
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "LLM generates\nthe analysis\nresult\nin text\nform",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "response se-"
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "yi = LLM ([tokenai; tokenti]) in an autoregres-",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "sive manner using prefix-conditioned generation.",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": ""
        },
        {
          "do not match the input\nspace of\nthe LLM, we": "",
          "Here, Wdep ∈ Rdbase×M is a learnable projection ma-": "2. WEE Routing Loss (LWEE): To train the routers to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": "1 2"
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": "• Entropy Loss (Lindep-ent & Ldep-ent): Encourages"
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": "the router to make “confident” decisions, i.e., pro-"
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": "ducing a sharper output distribution."
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": ""
        },
        {
          "cialized auxiliary loss:": "Lindep-ent = −"
        },
        {
          "cialized auxiliary loss:": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: summarizes the tasks, datasets, and evaluation",
      "data": [
        {
          "Used for emotion state classification.": "Metric: Macro F1-Score, with fo-",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "3 weak encoders to balance performance and effi-"
        },
        {
          "Used for emotion state classification.": "cus on negative emotion recognition",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "ciency:"
        },
        {
          "Used for emotion state classification.": "(anxiety, depression).",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "1. Whisper-tiny\n[2]\n(39M parameters):\nA"
        },
        {
          "Used for emotion state classification.": "Description: Simulated dataset with",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "lightweight general-purpose speech encoder"
        },
        {
          "Used for emotion state classification.": "counseling\ntechnique\nlabels\n(Ques-",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "that provides efficient speech content percep-"
        },
        {
          "Used for emotion state classification.": "tioning, Empathizing, Restating, Af-",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "tion."
        },
        {
          "Used for emotion state classification.": "firming, etc.).",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "Metric: Accuracy and Macro F1-",
          "–": "",
          "In our main experiments, the WEE pool contained": "2. HuBERT-base [4] (95M parameters): Trained"
        },
        {
          "Used for emotion state classification.": "Score.",
          "–": "",
          "In our main experiments, the WEE pool contained": "based on self-supervised learning,\nit excels"
        },
        {
          "Used for emotion state classification.": "Description: Identifies high-risk mo-",
          "–": "",
          "In our main experiments, the WEE pool contained": "at learning discrete representations of speech"
        },
        {
          "Used for emotion state classification.": "ments revealing suicidal or self-harm",
          "–": "",
          "In our main experiments, the WEE pool contained": "and is\nsensitive\nto phonemes\nand acoustic"
        },
        {
          "Used for emotion state classification.": "intentions.\nFine-grained annotations",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "content."
        },
        {
          "Used for emotion state classification.": "on data segments.",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "3. Emotion-Finetuned-HuBERT\n(95M pa-"
        },
        {
          "Used for emotion state classification.": "Metric:\nPrecision@K (due\nto\nex-",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "rameters):\nAn\nencoder\nobtained\nby\nfine-"
        },
        {
          "Used for emotion state classification.": "treme class imbalance).",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "tuning HuBERT-base on the IEMOCAP [6]"
        },
        {
          "Used for emotion state classification.": "Description: Generates concise sum-",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "emotion recognition dataset, specifically de-"
        },
        {
          "Used for emotion state classification.": "maries capturing core content, client",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "",
          "–": "",
          "In our main experiments, the WEE pool contained": "signed to capture emotional features."
        },
        {
          "Used for emotion state classification.": "issues, and intervention strategies.",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        },
        {
          "Used for emotion state classification.": "Metric: ROUGE-L [10] score.",
          "–": "",
          "In our main experiments, the WEE pool contained": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: summarizes the tasks, datasets, and evaluation",
      "data": [
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "To comprehensively evaluate\nthe\neffectiveness of our pro-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "posed WEE-Therapy framework, we designed multi-task"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "experiments\nand conducted tests on several\nrepresentative"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "psychological counseling datasets. This section elaborates in"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "detail on the tasks and datasets used in the experiments, eval-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "uation metrics, model\nimplementation details,\nand training"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "configurations."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "3.1. Tasks and Datasets"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "We selected four tasks that comprehensively reflect\nthe core"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "requirements of psychological counseling analysis. Due to"
        },
        {
          "3. EXPERIMENTAL SETUP": "the\nsensitivity\nof\npsychological\ncounseling\ndata,\npublicly"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "available datasets are limited. Our experiments are partially"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "based on existing public datasets and partially on simulated"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "data. Table 1 summarizes the tasks, datasets, and evaluation"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "metrics used in the experiments."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Table 1.\nSummary of Experimental Tasks, Datasets,\nand"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Evaluation Metrics"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Task\nDataset\nDescription & Metric"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Emotion\nDAIC-\nDescription:\nAudio\nrecordings\nof"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Recogni-\nWOZ [9]\nclinical\ndiagnostic\ninterviews,\nan-"
        },
        {
          "3. EXPERIMENTAL SETUP": "tion (ER)\nnotated with\npsychological\ndistress"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "states\n(e.g.,\nanxiety,\ndepression)."
        },
        {
          "3. EXPERIMENTAL SETUP": "Used for emotion state classification."
        },
        {
          "3. EXPERIMENTAL SETUP": "Metric: Macro F1-Score, with fo-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "cus on negative emotion recognition"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "(anxiety, depression)."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Counselor\nSimulated\nDescription: Simulated dataset with"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Technique\nDataset\ncounseling\ntechnique\nlabels\n(Ques-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Classifica-\ntioning, Empathizing, Restating, Af-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "tion (CTC)\nfirming, etc.)."
        },
        {
          "3. EXPERIMENTAL SETUP": "Metric: Accuracy and Macro F1-"
        },
        {
          "3. EXPERIMENTAL SETUP": "Score."
        },
        {
          "3. EXPERIMENTAL SETUP": "Crisis Risk\nSelf-\nDescription: Identifies high-risk mo-"
        },
        {
          "3. EXPERIMENTAL SETUP": "Detection\nAnnotated\nments revealing suicidal or self-harm"
        },
        {
          "3. EXPERIMENTAL SETUP": "(CMD)\nintentions.\nFine-grained annotations"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "on data segments."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Metric:\nPrecision@K (due\nto\nex-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "treme class imbalance)."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Dialogue\nSelf-\nDescription: Generates concise sum-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Summa-\nAnnotated\nmaries capturing core content, client"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "rization\nissues, and intervention strategies."
        },
        {
          "3. EXPERIMENTAL SETUP": "(DS)\nMetric: ROUGE-L [10] score."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "Overall\nAll Datasets\nMetric: GPT-4 as\njudge providing"
        },
        {
          "3. EXPERIMENTAL SETUP": "Judgment\n0-5 score based on alignment, profes-"
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        },
        {
          "3. EXPERIMENTAL SETUP": "sionalism, and completeness."
        },
        {
          "3. EXPERIMENTAL SETUP": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Trained with a specialized routing loss, our framework ef-": "ficiently learns to perform multiple counseling tasks—including",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Martinet, Marie-Anne\nLachaux,\nTimoth´ee\nLacroix,"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "emotion recognition,\ntechnique classification, risk detection,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "and dialogue summarization—while minimizing overfitting",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Azhar, et al., “Llama: Open and efficient foundation lan-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "through selective parameter tuning.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "guage models,” arXiv preprint arXiv:2302.13971, 2023."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Experimental results demonstrate that WEE-Therapy ef-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "[8] Edward\nJ Hu, Yelong Shen,\nPhillip Wallis, Zeyuan"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "fectively handles the nuanced requirements of counseling di-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "alogue analysis. Our work provides a promising foundation",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Chen, et al.,\n“Lora: Low-rank adaptation of large lan-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "for developing AI assistants that can enhance mental health",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "guage models.,” ICLR, vol. 1, no. 2, pp. 3, 2022."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "support services.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Future work will explore the framework’s application to",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "[9] Sergio\nBurdisso,\nErnesto\nReyes-Ram´ırez,\nEsa´u"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "larger-scale\nreal-world counseling datasets\nand other\nlow-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Villatoro-Tello, Fernando S´anchez-Vega, Pastor L´opez-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "resource domains.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Monroy, and Petr Motlicek, “Daic-woz: On the validity"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "of using the therapist’s prompts in automatic depression"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "5. REFERENCES",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "arXiv\npreprint\ndetection\nfrom clinical\ninterviews,”"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "arXiv:2404.14463, 2024."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[1]\nJacob Devlin, Ming-Wei Chang, Kenton\nLee,\nand",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "[10] Chin-Yew Lin,\n“Rouge: A package for automatic eval-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Kristina Toutanova, “Bert: Pre-training of deep bidirec-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "uation of summaries,”\nin Text summarization branches"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "tional transformers for language understanding,” in Pro-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "ceedings of\nthe 2019 conference of\nthe North American",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "out, 2004, pp. 74–81."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "chapter of the association for computational linguistics:",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "[11] Lewis\nTunstall,\nEdward\nBeeching,\nNathan\nLam-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "human language technologies, volume 1 (long and short",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "bert, Nazneen Rajani, Kashif Rasul, Younes Belkada,"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "papers), 2019, pp. 4171–4186.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Shengyi Huang, Leandro Von Werra, Cl´ementine Four-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[2] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "rier, Nathan Habib, et al., “Zephyr: Direct distillation of"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "man, Christine McLeavey, and Ilya Sutskever,\n“Robust",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "lm alignment,” arXiv preprint arXiv:2310.16944, 2023."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "speech recognition via large-scale weak supervision,” in",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "[12] Marah Abdin,\nJyoti Aneja, Harkirat Behl, S´ebastien"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "International conference on machine learning. PMLR,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Har-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "2023, pp. 28492–28518.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "rison, Russell J Hewett, Mojan Javaheripi, Piero Kauff-"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "arXiv preprint\nmann, et al.,\n“Phi-4 technical\nreport,”"
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[3] Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F Chen, and",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": "arXiv:2412.08905, 2024."
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Ai Ti Aw, “Mowe-audio: Multitask audiollms with mix-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "ture of weak encoders,” in ICASSP 2025-2025 IEEE In-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "ternational Conference on Acoustics, Speech and Signal",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Processing (ICASSP). IEEE, 2025, pp. 1–5.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[4] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "man Mohamed,\n“Hubert: Self-supervised speech rep-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "resentation learning by masked prediction of hidden",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "IEEE/ACM transactions on audio, speech, and\nunits,”",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "language processing, vol. 29, pp. 3451–3460, 2021.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "and Michael Auli,\n“wav2vec 2.0: A framework for",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "self-supervised learning of speech representations,” Ad-",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "vances\nin neural\ninformation processing systems, vol.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "33, pp. 12449–12460, 2020.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "[6] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "“Iemocap:\nInteractive emotional dyadic motion capture",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "database,” Language resources and evaluation, vol. 42,",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        },
        {
          "Trained with a specialized routing loss, our framework ef-": "no. 4, pp. 335–359, 2008.",
          "[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "3",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "Mowe-audio: Multitask audiollms with mixture of weak encoders",
      "authors": [
        "Wenyu Zhang",
        "Shuo Sun",
        "Bin Wang",
        "Xunlong Zou",
        "Zhuohan Liu",
        "Yingxu He",
        "Geyu Lin",
        "Nancy Chen",
        "Ai Ti"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "9",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "10",
      "title": "Daic-woz: On the validity of using the therapist's prompts in automatic depression detection from clinical interviews",
      "authors": [
        "Sergio Burdisso",
        "Ernesto Reyes-Ramírez",
        "Esaú Villatoro-Tello",
        "Fernando Sánchez-Vega",
        "Pastor López-Monroy",
        "Petr Motlicek"
      ],
      "year": "2024",
      "venue": "Daic-woz: On the validity of using the therapist's prompts in automatic depression detection from clinical interviews",
      "arxiv": "arXiv:2404.14463"
    },
    {
      "citation_id": "11",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    },
    {
      "citation_id": "12",
      "title": "Zephyr: Direct distillation of lm alignment",
      "authors": [
        "Lewis Tunstall",
        "Edward Beeching",
        "Nathan Lambert",
        "Nazneen Rajani",
        "Kashif Rasul",
        "Younes Belkada",
        "Shengyi Huang",
        "Leandro Von Werra",
        "Clémentine Fourrier",
        "Nathan Habib"
      ],
      "year": "2023",
      "venue": "Zephyr: Direct distillation of lm alignment",
      "arxiv": "arXiv:2310.16944"
    },
    {
      "citation_id": "13",
      "title": "Phi-4 technical report",
      "authors": [
        "Jyoti Marah Abdin",
        "Harkirat Aneja",
        "Sébastien Behl",
        "Ronen Bubeck",
        "Suriya Eldan",
        "Michael Gunasekar",
        "Russell Harrison",
        "Mojan Hewett",
        "Piero Javaheripi",
        "Kauffmann"
      ],
      "year": "2024",
      "venue": "Phi-4 technical report",
      "arxiv": "arXiv:2412.08905"
    }
  ]
}