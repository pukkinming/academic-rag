{
  "paper_id": "2103.02484v1",
  "title": "Deepfn: Towards Generalizable Facial Action Unit Recognition With Deep Face Normalization",
  "published": "2021-03-03T15:50:51Z",
  "authors": [
    "Javier Hernandez",
    "Daniel McDuff",
    "Ognjen",
    "Rudovic",
    "Alberto Fung",
    "Mary Czerwinski"
  ],
  "keywords": [
    "facial action units",
    "person-independent models",
    "generalization",
    "bias",
    "deep neural networks",
    "data normalization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial action unit recognition has many applications from market research to psychotherapy and from image captioning to entertainment. Despite its recent progress, deployment of these models has been impeded due to their limited generalization to unseen people and demographics. This work conducts an in-depth analysis of performance across several dimensions: individuals (40 subjects), genders (male and female), skin types (darker and lighter), and databases (BP4D and DISFA). To help suppress the variance in data, we use the notion of self-supervised denoising autoencoders to design a method for deep face normalization (DeepFN) that transfers facial expressions of different people onto a common facial template which is then used to train and evaluate facial action recognition models. We show that person-independent models yield significantly lower performance (55% average F1 and accuracy across 40 subjects) than person-dependent models (60.3%), leading to a generalization gap of ∆5.3%. However, normalizing the data with the newly introduced DeepFN significantly increased the performance of person-independent models (59.6%), effectively reducing the gap. Similarly, we observed generalization gaps when considering gender (∆2.4%), skin type (∆5.3%), and dataset (∆9.4%), which were significantly reduced with the use of DeepFN. These findings represent an important step towards the creation of more generalizable facial action unit recognition systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "F ACIAL expression recognition technology offers the op- portunity to comfortably capture the expressed emotional experience of people and facilitate unique interaction experiences  [1] . While the specific meaning of facial expressions may vary depending on the context  [2] , these signals have been successfully used in a wide variety of settings such as promoting engagement in human-robot interaction  [3] ,  [4] , monitoring depression of patients  [5] ,  [6] , estimating experienced pain  [7] ,  [8] ,  [9] , measuring engagement of TV viewers  [10] ,  [11] , and promoting driver safety  [12] ,  [13]  among others.\n\nTo help quantify facial activity, researchers often rely on the Facial Action unit Coding System (a.k.a., FACS)  [14] ,  [15]  which decomposes facial movements into different muscle activations called action units (AUs). For instance, AU12 indicates the activation of the zygomaticus major muscle which pulls the corner of the lip and is frequently seen during smiles. However, obtaining high quality labels for each person can be very expensive and time consuming. For instance, FACS labels are usually provided by expert coders which may spend up to 30 minutes to label around oneminute of video data  [16] . To help provide scalable FACS labeling, researchers have proposed and developed a wide variety of methods and tools that automatically detect AUs from face images (e.g.,  [1] ,  [17] ,  [18] ,  [19] ,  [20] ,  [21] ).\n\nDespite the significant advancements in the recent years, deploying such models in the wild is still challenging. One of the biggest hurdles, which also applies to most human-centered AI applications, involves the development of models that can generalize well across individuals or groups of people despite their differences. In the context of facial action unit recognition, some of the most readily observable differences are those associated with individual facial appearance such as head shape, amount of facial hair or skin type which can significantly vary across people and impair the ability of models to recognize relevant facial activity. As a consequence, machine learning models trained/tested with (non-overlapping) data of the same person (a.k.a., person-dependent models) usually outperform those trained/tested on data from different people (a.k.a., person-independent models)  [22] ,  [23] . Similar differences have been observed when considering groupindependent and dependent models that split groups according to different criterion that may similarly impact facial appearances (e.g., gender  [24] , skin type  [25] ).\n\nPoor cross-group generalization performance of machine learning models can partly be explained due to the violation of the independent and identically distributed (iid) data assumption, since the training/test data are sampled from highly correlated data (multiple images of the same person) and data with non-stationary distribution (multiple persons and over different periods of time). This is particularly pronounced when considering certain groups of people who may be underrepresented in existing training sets (e.g., older age, darker skin)  [26] ,  [27] ,  [28] . Interestingly, humans have been shown to be similarly impaired with the out-group homogeneity bias  [29]  and the cross-race effect  [30]  -suggesting that people are usually better at identifying facial variation of in-group members versus out-across members as well. Furthermore, people are usually better at recognizing emotions associated with expressions in faces from people within their own demographics group  [30] . In both cases, the underlying biases in the data itself may result in poor decision making and the reinforcement of harmful stereotypes  [26] .\n\nMotivated by the previous limitations, this work explores whether we can minimize appearance differences across people while maintaining their facial expressions with the goal of improving cross-group generalization in the context of AU recognition. The contributions of this work are as follows.\n\n1) This is the first work to systematically examine and compare cross-group generalization across multiple group splits within the same experimental conditions. To do so, we evaluated group-dependent and independent models based on individuals (40 people), genders (male and female), skin types (light and dark), and datasets (BP4D and DISFA) which capture some of the typical sources of variance of real-life deployments. Among the different conditions, we found cross-dataset generalization to be the most challenging one. 2) This is the first work to explore the use of face transfer in the context of face normalization. In particular, we propose DeepFN which leverages selfsupervised denoising autoencoders to transfer facial expressions of different people onto a common facial template (a.k.a., template of reference). This simple yet effective method was successful in significantly closing the gap between group-dependent and independent models, especially when considering different skin types and genders. 3) In light of our findings, we discuss the limitations of this work and draw important ethical considerations around reducing algorithmic biases, facilitating AI interpretability, and preserving data privacy to help guide future research efforts in this area.\n\nThe remainder of the paper is organized as follows. First, we review prior work on data normalization methods in the context of facial expression recognition. Second, we describe the proposed methodology which includes the appearance normalization, the template selection, and the facial action unit recognition model. Third, we describe the experimental protocol which includes the considered groups, datasets, and evaluation. Fourth, we provide the results for each one of the considered groups. Finally, we discuss the results, limitations of the work, and ethical considerations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Performance differences between person-dependent and person-independent models have been reported in several studies  [22] ,  [23] ,  [31] . One the one hand, person-dependent models tend to perform better but offer limited practical use as obtaining high quality annotations for each person can be very expensive and time consuming. On the other hand, person-independent models tend to perform worse but they are more scalable as they do not require the training data from target (test) subjects. To help improve performance while still maintaining scalability, researchers have explored a wide variety of methods to help minimize individual differences in the context of person-independent models.\n\nArguably one of the most simple and commonly explored normalization methods involves applying some sort of intuitive correction on the face images or sensed features that helps minimize differences across people. For instance, several studies have considered using the distance between the eyes to help correct geometric or shape-based features such as distances between different facial landmarks  [32] ,  [33] ,  [34] . Similarly, other studies have considered computing differences across different regions of the face to help correct appearance-based features such as texture  [34]  or applying color correction methods such as histogram equalization to reduce the data distribution mismatch in the color space  [35] . Another correction method involves the use of face alignment, especially in the context of face recognition. For instance, Banerjee et al.  [36]  validated recent 2D/3D face-alignment methods to quantify the effect of this preprocessing step in the context of face recognition. As noted by the authors, however, the quality of alignment did not directly translate into the performance of the face-recognizers, as various (individual) facial details could be lost during this processing step. This may be even more critical in the case of facial action recognition as facial appearance and dynamics of AUs are very person-specific. The work presented in this paper similarly explores correcting the input images but explores using more complex methods to normalize facial appearance. In addition, we found that pre-processing the images with histogram equalization helped facilitate the normalization process.\n\nAn alternative normalization method involves the use of unlabeled testing data to help calibrate the methods. For instance, Baltrusaitis et al.  [37]  explored using unlabeled testing data to estimate the neutral-looking face image and serve as a reference to capture potential activations. While this led to successful calibration of the AU classifiers, this method depends on tracking methods such as the Constrained Local Models (CLMs) which are highly sensitive to the performance of the facial landmark detection. Other works have explored the smart selection or weighting of training samples to ensure that they are representative of the testing samples. For instance, Chu et al,  [38]  proposed a Selective Transfer Machine (STM) approach for personalized facial expression analysis -an inductive learning approach that tries to align the distributions of the facial features' of training and test subjects during training. In a separate effort, Zen et al.  [39]  proposed to learn a regression function that would learn from training subjects the optimal parameters of a facial expression classifier for a specific person. Similarly, Feffer et al.  [40]  proposed a Mixture-of-Experts deep learning approach that would tackle the individual differences by selecting the most similar training subjects and their corresponding expert-classifiers for the task of valence/arousal estimation. Different from these works, Yang et al.  [41]  proposed a two-step approach for personalized modeling of facial AU intensity from spontaneously displayed facial expressions. In the first step, a matrix decomposition algorithm (unsupervised) was applied to separate the identity from facial expression of target subjects. The obtained representations (expression plus identity features) were then jointly modeled using the framework of Conditional Ordinal Random Fields, resulting in a personalized model for intensity estimation of AUs. The work presented in this paper similarly requires the use of unlabeled testing data but differs from previous efforts in several important ways: 1) the normalization step is completely separated from the task of facial action unit recognition task allowing easier optimization, 2) the normalization is performed at the image level allowing easier inspection of the output data, and 3) the normalization does not require the use of facial landmark detection reducing the dependency on other methods. Due to these, the proposed method could be used as a pre-processing step for any of the facial action unit recognition methods. To start exploring the potential value of the proposed method, however, we selected a simple and lightweight LeNet-5 which allowed quickly evaluation of generalization performance across different groups.\n\nWhile the previous methods mainly focus on variations in facial appearance and features, it is worth noting that researchers have also investigated other methods to normalize other sources of variation such as out-of-plane head rotations, illumination changes, and other (typically) external artifacts that may be in the data  [42] ,  [43] ,  [44] . In the case of large-out-of-plane head-rotations, the majority of studies attempt face frontalization -effectively mapping the non-frontal faces to a frontal reference frame. For instance, Werner et al.  [45]  proposed a face normalization method which also depends on the quality of the facial landmark detection and texture-warping. The authors showed that with their face-normalization method they could train betterperforming CNN models for facial expression recognition and AU detection, compared to when no face-normalization was applied. The work presented here focuses on minimizing differences in facial appearance across target people while preserving information about their facial expressions. Even though there are examples of other sources of variation (such as the extreme head poses) in data used in this work, the majority of data falls under the in-plane head rotations. We show empirically that the proposed method based on self-supervised autoencoders can successfully deal with this range of head pose variation without adversely affecting the AU estimation performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "This section describes the methodological details of the proposed approach, DeepFN, as well as the facial action unit recognition algorithm.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Facial Appearance Normalization",
      "text": "Some of the most popular methods for facial expression transfer and expression synthesis start by detecting facial landmarks or action units to help guide the transfer process (e.g.,  [46] ,  [47] ,  [48] ,  [49] ,  [50] ). As this work explores ways to improve the task of facial action unit recognition, we chose a method that does not require any explicit indication of facial landmarks or action units. In particular, we selected a self-supervised autoencoder approach which is composed of three main components: (i) a shared encoder network (E), used to reduce the dimensionality of face images onto a lower dimensional latent space, (ii) one decoder network (D y ), used to recover images of the reference template selected for the normalization, and (iii) a second decoder network, used to recover images of the individual, the face image of whom we wish to normalize (D x ). Figure  1  (left) illustrates the main components and how they are combined.\n\nDuring training, the face images (the reference template and the image of the subject to be normalized) were iteratively compressed with the encoder and recovered with their corresponding decoder, while minimizing the root mean squared error (RMSE) between the original input and recovered image pixel values. As the same encoder is used to embed the images of the two appearances, the latent space learns to capture the sources of variance shared by the two inputs (e.g., head poses and facial expressions). As a preprocessing step, all the input images were converted to gray scale and the pixels values were normalized using the histogram equalization approach in order to facilitate a more consistent distribution of pixel values across individuals, and also remove artifacts such as illumination changes, skin colour variation, etc. In addition, random affine transformations and Gaussian warps were used as image augmentation techniques to help increase the amount of training data (and thus, the variation of input images). Specifically, the training process involved optimizing the following joint self-supervised loss function:\n\n) where x i represents the i-th image of the person to be normalized, y j represents the j-th image of the template of reference, and x i and y j represent their pre-processed versions, respectively. This process was repeated for each person that needed to be normalized. During the testing phase, the images of the individual to be normalized were similarly compressed (without augmentation) but recovered with the alternate decoder (D y ) as follows:\n\nwhere X y represents the normalized images of the person. Figure  1  shows the deep neural network architecture used which was built on top of an existing code base 1 . To ensure appropriate transfer, we allowed the network to learn for 50K iterations which included a batch of 64 randomly selected template images followed by another batch of 64 randomly selected images from the person to be normalized. Table  1  and Table  2  show the specific architecture implementation for the encoder and decoders, respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Template Selection",
      "text": "To normalize the facial appearance across people, DeepFN requires a template (Y ) that provides the appearance to be shared. While there are many potential options for the selection of the template, this work leveraged the images of the most expressive subject of the BP4D dataset which was more likely to capture a rich gamut of facial variations while still resembling some of the main dataset characteristics (e.g., camera angle, illumination). In particular, we first counted the number of facial action unit activations for each person, and then selected the participant for which 1. https://github.com/joshua-wu/deepfakes faceswap the median across all action units was the highest in the dataset (see y in Figure  1  and bottom row of Figure  2 ). This participant was excluded from the action unit classification analysis to help provide a fair comparison. While not included in this work, we also explored the creation of synthetic reference models such as avatars that can be easily controlled  [51] ,  [52]  or synthetic methods such as StyleGAN  [53] . However, we found that the demographics as well as range of facial expressions were limited compared to when considering real subjects  [54] , lowering the quality of the expressions transfer results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Action Unit Recognition",
      "text": "Once all the images were transferred into a common template of reference, we fed them into a facial action unit classifier that only sees a single appearance (the images mapped to the template). For the purpose of our analysis, we selected the LeNet-5 convolutional neural network architecture (see right of Figure  1 ) which was originally proposed by LeCun et al.  [55] . Among the different possibilities, we selected LeNet-5 as it provided a simple and lightweight solution (388K parameters) to quickly evaluate different experimental conditions while avoiding overfitting. In terms of loss function, we used the binary cross entropy function which allows for multiple probabilistic activations for a single image. Table  3  shows the specific architecture implementation of the classifier.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Protocol",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Groups",
      "text": "The main motivation of this work is to improve the generalization of facial action unit models across different groups of people. To help evaluate this, we performed multiple within-group and cross-group evaluations across different group splits, and then applied DeepFN to evaluate whether it helped bridge the performance gap. In particular, we considered the following group splits.\n\nPerson. The first group split considers each individual as their own separate group as it captures the most frequently considered source of facial appearance variance (e.g.,  [22] ,  [23] ), especially in the context of face recognition  [56] . The within-group evaluations included models that were trained and tested with data from the same person (a.k.a., persondependent models). The cross-group evaluations included models that were trained and tested with data from different people (a.k.a., person-independent models). In this case, person-dependent models capture the optimal performing scenario in which identity labels and data from a specific subject are available and, consequently, good model generalization can be more easily achieved.\n\nGender. The second group split divides participants according to their gender (male and female) which captures facial variance in terms of sex-related facial characteristics such as the amount of hair or the shape of the jaw  [24] . The within-group evaluations included models that were trained and tested with male participants as well as models that were trained and tested with female participants. The crossgroup evaluations included models that were trained with males and tested with females, as well as the opposite. For convenience, we will refer to these experiments as genderdependent and gender-independent models, respectively. However, it is important to note that both of them fall under the category of person-independent models as the subjects used for training and validation were different than the ones used for testing.\n\nSkin Type. The third group split divides participants according to their skin skin type (lighter and darker skins) which captures facial variance in terms of skin pigmentation and other facial characteristics that may be correlated correlated with it (e.g., shape of the nose  [25] ). This group split has been more frequently been examined in the context of algorithmic biases of computer vision  [26] ,  [27] ,  [28] . The within-group evaluations included models that were trained and tested with participants with lighter skin (e.g., skin type I and II) as well as models that were trained and tested with participants with darker skin (e.g., skin type V and VI) (a.k.a., skin-dependent models). The cross-group evaluations included models that were trained with participants with lighter skin and tested with those with darker skin, as well as the opposite (a.k.a., skin-independent models). To annotate skin type for each of the participants, we used the Fitzpatrick Phototype Scale  [57]  which separates skin types into six main categories. To help amplify differences associated with skin types, we group participants into lighter skin (types below or equal to II) and darker skin (types above or equal to V).\n\nDataset. The fourth group split divides participants according to the dataset in which they participated (BP4D and DISFA) which captures facial variance in terms of a wide variety of factors such participant demographics and specific data collection settings (e.g., camera angle, illumination)  [37] . The within-group evaluations included models that were trained and tested with participants from the same dataset (a.k.a., database-dependent models). The cross-group evaluations included models that were trained with one dataset and tested with those from another one (a.k.a., database-independent models).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "To systematically evaluate performance under the different group splits, this work mainly leverages the benchmark Binghamton-Pittsburgh 4D Spontaneous Expression Dataset (BP4DSpontaneous)  [58]  due to its diverse set of demographics and wide use in the context of facial action unit recognition. In particular, the dataset contains a total of around 140K images distributed across 41 participants (23 females) who were instructed to perform 8 different tasks designed to elicit authentic emotions (e.g., playing games, watching a film, social interview). The images were annotated by expert FACS coders. In our study, we focused on the following 12 facial action units: AU01 (inner brow raiser), AU02 (outer brow raiser), AU04 (brow lowerer), AU06 (cheek raiser), AU07 (lid tightener), AU10 (upper lip raiser), AU12 (lip corner puller), AU14 (dimpler), AU15 (lip corner depressor), AU17 (chin raiser), AU23 (lip tightener), and AU24 (lip pressor). In addition to providing binary occurrence values for each of the actions units, the dataset also includes intensity values (ranging from 1 to 8) for a smaller subset of the action units (AU06, AU10, AU12, AU14 and AU17) which will be considered in part of the analysis. Figure  3  shows visual examples of the different action units.\n\nTo study cross-dataset generalization, we also use the Denver Intensity of Spontaneous Facial Actions (DISFA) benchmark dataset  [59]  which includes around 130K images distributed across 27 participants (12 females) watching emotive video stimulus. Similarly, the images were annotated by expert coders. For the purpose of our experiments, we focused on the following five facial action units which were also provided in the BP4D dataset: AU01, AU02, AU04, AU06, and AU12. See * in Figure  3 .\n\nTable  4  shows the number of participants considered for each of the group splits discussed in the previous section. In particular, there were seven participants that met the criteria of having lighter skin type (two females), eight with darker skin type (six females), 22 female, 18 males, 40 in BP4D, and 27 in DISFA. To help illustrate appearance differences across different group splits, Figure  2  shows the average facial appearance (middle row) as well as their luminance histograms (blue bars on the top row). The figure also includes the same information after applying DeepFN (bottom row and red bars on the histogram) showing greater consistency across the different group splits.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "Table  5  shows a summary of all the experiments. For each of the conditions, we randomly selected four participants for training, two for validation, and the remaining participants for testing from their corresponding pool of participants (see Table  4 ). These numbers were mostly determined by the size of the smaller group (seven people with lighter skin) and were kept the same across conditions to help facilitate a fair performance comparison. Each model was trained for a total of 50 epochs and testing predictions were obtained using the model that yielded the highest F1-score in the validation set. Each of the conditions was repeated 20 times to help minimize potential selection effects. All the neural networks were optimized with Adam with a learning rate of 0.00005 and exponential decay rates of 0.5 and 0.999 for the first and second-moment estimates, respectively. The specific implementation was built in Python 3.5.6, Keras 2.2.4, and Tensorflow 1.14.0.\n\nTo capture the overall performance for each model, we first computed the average between the F1-score and accuracy for each of the action units (at a threshold of 0.5), and then aggregated them for each of the participants. For  each of the conditions, we then computed the average and standard deviation across all the participants. To compare performance across conditions, we used two-sample t-test comparisons with a significance score when p < 0.05.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "Table  5  provides an overview of the results obtained for each of the experiments across conditions. In this section we will review the results for each of these groups.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cross-Person Generalization",
      "text": "When evaluating the models with the original (unnormalized) data, person-independent models achieved an average score of 55% and person-dependent models achieved an average score of 60.3% which were significantly different (p < 0.001). This difference indicates that the performance impact of having access to person-specific data is around 5.3% in this dataset. This finding is consistent with previous work showing that person-dependent models lead to greater performance as both training and testing data follow the same distribution. In this case, 60.3% can be considered as the optimal performing results for our experimental setting in which no individual differences exists.\n\nWhen evaluating the models with DeepFN (normalized) data, we observe that person-independent models increased to 59.6% which was significantly higher than its unnormalized person-independent counterpart (p < 0.001) and very similar to the unnormalized person-dependent results (p = 0.375). This finding suggests that DeepFN can effectively minimize individual differences associated with  appearance. Person-dependent models with DeepFN maintained a performance of 61.4% which was similar than its unnormalized counterpart (p = 0.388), suggesting that the face transfer process did not lose relevant facial expression information.\n\nFigure  4  shows the average performance per action unit when using person-independent models with and without DeepFN. As can be seen, DeepFN provided an average improvement of around 4.6% (STD: 2.9) for the considered AU with the greatest gains for AU04 (11.4%) and the smallest gains for AU14 (0.8%).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Cross-Gender Generalization",
      "text": "When using the original data, gender-independent models achieved an average score of 52.6% and gender-dependent models achieved an average score of 55% which were significantly different (p = 0.009). This difference indicates that the impact of having different genders across training and testing sets is around 2.4% in this dataset.\n\nWhen using DeepFN, we observe that genderindependent models increased to 57.7% which was significantly higher than its unnormalized counterpart (p < 0.001) and gender-dependent models increased to 60.2% which was also significantly higher than its unnormalized counterpart (p < 0.001). The fact that gender-independent models with DeepFN yielded higher results than gender-dependent models without DeepFN indicates that the normalization is helping address individual differences beyond gender which is to be expected as the proposed method normalizes differences at the individual level.\n\nTo further explore these results, Figure  5  shows the results for each of the training/testing combinations with and without DeepFN. As can be seen, the use of DeepFN yielded a consistent average improvement of 5.2% across the different conditions. While we originally hypothesized that within-group evaluations would outperform cross-group evaluations, however, we observe that experiments in which women were used as part of the testing set yielded better performance than those in which men were used as part of the testing set, irrespective of the training group and normalization method (p <= 0.001). When considering the intensity of facial action units, we observe that the activations of female participants were more intense than those associated with male participants on average (p <= 0.001), suggesting that recognizing expressions in females may be easier.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Skin Generalization",
      "text": "When using the original data, skin-independent models achieved an average score of 49.9% and skin-dependent models achieved an average score of 55.2% which were significantly different (p = 0.025). This difference indicates that the impact of having different skin types across training and testing sets is around 5.3% which is a bit larger than the generalization gap associated with gender (2.4%). While we have not seen prior work comparing these two types of the generalization, this finding seems to suggest that skin type may have a greater impact than gender in the context of model generalization. However, it is important to note that the number of subjects in the skin type condition is smaller than in the gender condition (see Table  4 ).\n\nWhen using DeepFN, we observe that skin-independent models increased to 57.4% which was significantly higher than its unnormalized counterpart (p < 0.001) and skindependent models increased to 58.7% which was similar to its unnormalized counterpart (p = 0.110). In this case, skin-independent models with DeepFN yielded higher but comparable results than skin-dependent without DeepFN (p = 0.290), suggesting that the normalization method addressed the main source of data variance in this condition.\n\nFigure  6  shows the results for each of the training/testing combinations with and without DeepFN. We see that the use of DeepFN yielded a consistent average improvement of 5.7% across the different conditions. This difference was the smallest when training and testing with people with darker skin, in which performance was already at the level of person-dependent models (around 60%). Similarly, we observe that experiments in which people with darker skin were used as part of the testing set yielded better performance than those considering lighter skin, irrespective of the training group and normalization method (p <= 0.020). This finding is consistent with the gender differences as the majority of participants with darker skin were female (6 out of 8) and can be also observed on the average facial appearance of Figure  2 .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Cross-Dataset Generalization",
      "text": "When using the original data, dataset-independent models achieved an average score of 44.6% and dataset-dependent models achieved an average score of 54% which were significantly different (p < 0.001). This difference indicates that the impact of having different datasets across training and testing sets is around 9.4%. This difference was the largest observed gap across all the group conditions indicating that cross-dataset generalization is one of the most difficult challenges to address.\n\nWhen using DeepFN, we observe that datasetindependent models increased to 51.1% which was significantly higher than the unnormalized counterpart (p < 0.001) and dataset-dependent models increased to 61.5% which was also significantly higher than the unnormalized counterpart (p < 0.001). However, the gap between dataset-independent models and dataset-dependent models was significant (p < 0.001) indicating that even though DeepFN was able to close a significant part of the gap, there are potentially other sources of unaddressed variance such as illumination and camera angle as shown in Figure  2  (right) that could help further normalize facial appearance.\n\nFigure  7  shows the results for each of the training/testing combinations with and without DeepFN. Note that the right-most condition (i.e., BP4D → BP4D) is equivalent to the person-independent models considered in section 5.1 but considering a smaller set of facial action units, leading to a slightly higher score. In this condition, withingroup evaluations yielded significantly better performance than cross-group evaluations as hypothesized (p < 0.012). The use of DeepFN also yielded average improvements of 8.3% for three of the conditions but only 2.1% when training on BP4D and testing on DISFA. We believe the smaller improvement may be due to two main factors. On the one hand, the difference in class priors across the two datasets may have limited the ability to learn facial action units. On the other hand, the template of reference was selected from the same distribution as the training data (BP4D) and negatively impacted the expression transfer of more dissimilar images (DISFA). The latter seems to be supported by Figure  2  that shows greater differences between the original (middle) and normalized (bottom) DISFA images.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "This work has proposed and evaluated DeepFN, a deep facial appearance normalization method that minimizes appearance differences across people to help facilitate the task of facial action unit recognition. In particular, we explored the use of self-supervised denoising autoencoders that enable us to transfer facial expressions in a self-supervised manner. To evaluate the potential impact of DeepFN in the context of generalization, we performed multiple withingroup and cross-group evaluations with both unnormalized and normalized data.\n\nWhen considering the optimal machine learning scenario in which both labels and images of the testing subjects were available (i.e., person-dependent models), we obtained an average F1 and accuracy performance of 60.3%. In contrast, person-independent models with unnormalized data yielded a significantly lower performance of 55%, which was successfully corrected when normalizing the data with DeepFN (59.6%). To analyze different types of person-independent models, we further constrained the groups of participants selected for training and testing sets in terms of gender (male and female), skin type (lighter and darker), and dataset (BP4D and DISFA). Overall, we observed that cross-group evaluations performed worse than within-group evaluations on average which is consistent with the iid assumption that requires that both training and testing data follow the same distribution. The gap between within and cross-evaluations was the largest for the evaluations examining different datasets (9.4%) followed by skin type (5.3%) and gender (2.4%). For all the cases, DeepFN helped close a significant part of the gap which was more pronounced when considering gender and skin type.\n\nThe previous results seem to support that the use of DeepFN can effectively minimize individual differences while keeping relevant facial expression information. However, we observed some differences across certain conditions that suggest that further improvements could be made in terms of template selection. In our study, we selected the most expressive subject of the BP4D dataset as a template, which helped capture a large range of facial action units. However, the specific attributes of the selected subject more closely resemble certain groups of subjects which could have differently impacted others (e.g., Asian female). For instance, we observed that testing on female subjects and people with darker skin, which were predominantly female, led to slightly better performance than its male and lighter skin counterparts. While the use of DeepFN helped slightly close this gap, the differences across groups still existed. In addition, we observed that transferring faces of the DISFA dataset, which was collected under different settings, led to an observable decrease in transfer quality (see right images of Figure  2 ) which could be expected due to the additional sources of variance (e.g., camera angle, illumination) which were not present in the template of reference. Future efforts will need to systematically study the role of template selection to enable more consistent normalization results across different groups. We believe the appearance of the ideal template should have similar resemblance across all the groups (e.g., average face) as well as capture a rich range of different sources of variance (e.g., facial expressions, illumination, body poses). Recent computer vision efforts such as controllable avatars or generative adversarial networks (e.g.,  [51] ,  [52] ,  [61] ,  [62] ) could be helpful in this space. In addition, we believe there exist some complementary efforts in the context of face frontalization (e.g.,  [56] ) that could be used in combination with the proposed approach to help provide a more comprehensive facial appearance normalization.\n\nTo facilitate facial expression transfer in a self-supervised manner, this work iteratively transferred the data from each person to the template of reference. This approach yielded good qualitative performance but the training process can be considered too temporally costly for real-time analysis. While not explored in this work, there exist multiple methods that can be used to help speed-up the learning process such a re-using an existing pre-trained autoencoder and fine-tuning it with the new data, and/or building a supervised version of the autoencoder that allows normalizing any face without the need of seeing unlabeled data. Training the supervised approach would require obtaining large amounts of paired images (original and normalized faces) which could be provided with the method discussed in this work and/or some of the recent efforts focused on systematic manipulation of synthetic faces (e.g.,  [62] ,  [63] ,  [64] ). Due to the quick advancements of computer vision, we believe the temporal cost associated with training DeepFN will be significantly reduced in the near term.\n\nTo help evaluate the performance of DeepFN across different conditions, this work has made some experimental decisions that are important to consider. For instance, this work has mainly considered evaluations using the BP4D dataset which provided a rich collection of images from different subjects required for the normalization. In addition, the dataset contains a very diverse set of participants in terms of demographics which facilitated exploring different generalization across different groups. However, the number of available subjects for some of the groups was relatively small (7 for the people with lighter skin) which constrained the training size across all the other conditions for consistency purposes. This work has also considered the LeNet-5 network to recognize facial action units which facilitated performing a large number of experiments and repetitions (see Reps. on Table  5 ) in a manageable amount of time, However, it is important to note that this network is relatively simple and that there is a wide variety of more complex network architectures which are currently being used to obtain state-of-the-art-performance (e.g.,  [17] ,  [65] ). Due to the exploratory nature of this work, we believe these experimental decisions were necessary to facilitate a fair and comprehensive evaluation but acknowledge that they could have impacted the results. For instance, we would expect to see greater DeepFN benefits when reducing the number of training subjects as well as reducing the complexity of the network, as it may be more difficult to learn generalizable features. In contrast, we may expect decreased benefits when significantly increasing the number of training subjects as well as increasing the complexity of the neural network. While future efforts will need to consider different experimental decisions to further understand the role of DeepFN, especially when considering state-of-theart methods, we believe the results of this work highlight the potential value of the proposed methodology.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethical Considerations",
      "text": "This work studies the problem of model generalization across different groups which is important to not only improve performance when deploying the models in the real-world but also to help prevent potential biases that can differently impact underrepresented groups (e.g., security screening, job interviews). While this work has shown significant performance improvement when considering different groups, it is important to note that there were still some significant gaps across different data splits (e.g., testing with female subjects yielded better performance) which should be further minimized to help prevent algorithmic biases. In addition, the evaluations presented in this work were performed in semi-controlled datasets which suffer from their own biases (e.g., young population) and may not be representative of real-world demographics. To address these limitations, it is important to consider not only recent advancements in computer vision such as the generation of synthetic images  [62]  but also acknowledge their unique potential biases  [54]  which can indirectly increase some of the gaps.\n\nAnother challenge when deploying AI systems like the one considered in this work, is the ability to interpret and understand what the models are doing. In contrast to prior work that frequently considered normalizing features with intuitive methods (e.g., range correction, relative changes), this work separates the learning process into two main phases: one fully dedicated towards minimizing individual differences, and another one fully dedicated towards learning facial action unit recognition. This separation offers an opportunity to examine the output of the models after the normalization process which can be used to isolate potential failures in the generalization process. Furthermore, the use of a shared facial appearance provided a familiar channel of model introspection (faces) that facilitates the intuitive detection of AI limitations. For instance, Figure  2  easily allowed us to identify potential limitations when transferring faces across datasets (right). In the future, we expect to see more approaches further separating the learning process with the hope of better isolating and debugging potential failures.\n\nFinally, it is important to note that facial expression transfer algorithms like the one considered in this work can pose serious risks to the privacy of individuals as their appearance could be easily manipulated without their consent. For instance, there has been an increase of manipulated videos in sensitive scenarios such as politics and pornography with very profound and worrisome implications to society  [50] . While there has been recent progress towards the automated detection of these misuses (e.g.,  [66] ,  [67] ,  [68] ), it is critical to find ways to obtain the informed consent of users. In this work, we mitigated this by only considering the appearance of participants who explicitly contributed their data for research purposes as a template of reference. However, future work considering different template selections will need to consider the importance of obtaining consent too. Despite the shortcomings of expression transfer methods, however, we believe there are several positive applications such as helping preserve the privacy of individuals (e.g.,  [69] ,  [70] ,  [71] ) and helping minimize potential algorithmic biases as explored in this work.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of main the processing phases: 1) face images are ﬁrst normalized with DeepFN to ensure individual differences are minimized,",
      "page": 3
    },
    {
      "caption": "Figure 1: (left) il-",
      "page": 3
    },
    {
      "caption": "Figure 1: shows the deep neural network architecture",
      "page": 4
    },
    {
      "caption": "Figure 1: and bottom row of Figure 2).",
      "page": 4
    },
    {
      "caption": "Figure 1: ) which was originally proposed",
      "page": 4
    },
    {
      "caption": "Figure 3: shows visual examples of the different action units.",
      "page": 5
    },
    {
      "caption": "Figure 3: Table 4 shows the number of participants considered for",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the",
      "page": 5
    },
    {
      "caption": "Figure 2: Average facial appearance before (middle row) and after applying DeepFN normalization (bottom row) for different groups. The top row",
      "page": 6
    },
    {
      "caption": "Figure 3: Subset of facial action units [15], [60] considered in the study.",
      "page": 6
    },
    {
      "caption": "Figure 4: Average and standard error for person-independent models with (DeepFN) and without (original) normalization across facial action units.",
      "page": 7
    },
    {
      "caption": "Figure 4: shows the average performance per action unit",
      "page": 7
    },
    {
      "caption": "Figure 5: Average and standard error bars for different training/testing splits",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the",
      "page": 8
    },
    {
      "caption": "Figure 6: Average and standard error bars for different training/testing splits",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the results for each of the train-",
      "page": 8
    },
    {
      "caption": "Figure 7: Average and standard error bars for different training/testing splits",
      "page": 9
    },
    {
      "caption": "Figure 2: (right) that could help further normalize facial",
      "page": 9
    },
    {
      "caption": "Figure 7: shows the results for each of the train-",
      "page": 9
    },
    {
      "caption": "Figure 2: that shows greater differences between the original",
      "page": 9
    },
    {
      "caption": "Figure 2: ) which could be expected due to the additional",
      "page": 9
    },
    {
      "caption": "Figure 2: easily al-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Encoder network architecture used to compress facial information.",
      "page": 4
    },
    {
      "caption": "Table 2: Decoder network architecture used to recover facial information.",
      "page": 4
    },
    {
      "caption": "Table 1: and Table 2 show the speciﬁc",
      "page": 4
    },
    {
      "caption": "Table 3: LeNet-5 architecture used for facial action unit recognition.",
      "page": 4
    },
    {
      "caption": "Table 3: shows the speciﬁc architecture imple-",
      "page": 4
    },
    {
      "caption": "Table 4: shows the number of participants considered for",
      "page": 5
    },
    {
      "caption": "Table 5: shows a summary of all the experiments. For each of",
      "page": 5
    },
    {
      "caption": "Table 4: ). These numbers were mostly determined by",
      "page": 5
    },
    {
      "caption": "Table 5: provides an overview of the results obtained for each",
      "page": 6
    },
    {
      "caption": "Table 4: Number of available participants for each for the data splits",
      "page": 6
    },
    {
      "caption": "Table 5: (Top) Average and standard error bars for different experimental conditions and (bottom) overview of experimental details for each of the",
      "page": 7
    },
    {
      "caption": "Table 5: ) in a manageable amount",
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "Automatic analysis of facial actions: A survey"
    },
    {
      "citation_id": "2",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "3",
      "title": "CultureNet: A Deep Learning Approach for Engagement Intensity Estimation from Face Images of Children with Autism",
      "authors": [
        "O Rudovic",
        "Y Utsumi",
        "J Lee",
        "J Hernandez",
        "E Ferrer",
        "B Schuller",
        "R Picard"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "4",
      "title": "Personalized machine learning for robot perception of affect and engagement in autism therapy",
      "authors": [
        "O Rudovic",
        "J Lee",
        "M Dai",
        "B Schuller",
        "R Picard"
      ],
      "year": "2018",
      "venue": "Science Robotics"
    },
    {
      "citation_id": "5",
      "title": "Detecting depression from facial actions and vocal prosody",
      "authors": [
        "J Cohn",
        "T Kruez",
        "I Matthews",
        "Y Yang",
        "M Nguyen",
        "M Padilla",
        "F Zhou",
        "F De",
        "La Torre"
      ],
      "year": "2009",
      "venue": "Proceedings -2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "6",
      "title": "Automatic nonverbal behavior indicators of depression and ptsd: Exploring gender differences",
      "authors": [
        "G Stratou",
        "S Scherer",
        "J Gratch",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "7",
      "title": "Automatically detecting pain in video through facial action units",
      "authors": [
        "P Lucey",
        "J Cohn",
        "I Matthews",
        "S Lucey",
        "S Sridharan",
        "J Howlett",
        "K Prkachin"
      ],
      "year": "2011",
      "venue": "IEEE transactions on systems, man, and cybernetics"
    },
    {
      "citation_id": "8",
      "title": "Continuous pain intensity estimation from facial expressions",
      "authors": [
        "S Kaltwang",
        "O Rudovic",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "International Symposium on Visual Computing"
    },
    {
      "citation_id": "9",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Automatic analysis of facial actions: A survey"
    },
    {
      "citation_id": "10",
      "title": "Predicting Ad Liking and Purchase Intent: Large-Scale Analysis of Facial Responses to Ads",
      "authors": [
        "D Mcduff",
        "R Kaliouby",
        "J Cohn",
        "R Picard"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Measuring the engagement level of TV viewers",
      "authors": [
        "J Hernandez",
        "Z Liu",
        "G Hulten",
        "D Debarr",
        "K Krum",
        "Z Zhang"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "12",
      "title": "Driver drowsiness detection using face expression recognition",
      "authors": [
        "M Assari",
        "M Rahmati"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Signal and Image Processing Applications, ICSIPA 2011"
    },
    {
      "citation_id": "13",
      "title": "Detecting emotional stress from facial expressions for driving safety",
      "authors": [
        "H Gao",
        "A Yuce",
        "J Thiran"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "14",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "15",
      "title": "Observer-based measurement of facial expression with the Facial Action Coding System",
      "authors": [
        "J Cohn",
        "Z Ambadar",
        "P Ekman"
      ],
      "year": "2007",
      "venue": "Series in affective science"
    },
    {
      "citation_id": "16",
      "title": "Learning facial action units from web images with scalable weakly supervised clustering",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "A Martinez"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "19",
      "title": "A multimodal emotion sensing platform for building emotion-aware applications",
      "authors": [
        "D Mcduff",
        "K Rowan",
        "P Choudhury",
        "J Wolk",
        "T Pham",
        "M Czerwinski"
      ],
      "year": "2019",
      "venue": "A multimodal emotion sensing platform for building emotion-aware applications",
      "arxiv": "arXiv:1903.12133"
    },
    {
      "citation_id": "20",
      "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications",
      "authors": [
        "C Corneanu",
        "M Sim Ón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition from video sequences: Temporal and static modeling",
      "authors": [
        "I Cohen",
        "N Sebe",
        "A Garg",
        "L Chen",
        "T Huang"
      ],
      "year": "2003",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "23",
      "title": "The first facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "B Jiang",
        "M Mehu",
        "M Pantic",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011"
    },
    {
      "citation_id": "24",
      "title": "Second to fourth digit ratio and face shape",
      "authors": [
        "B Fink",
        "K Grammer",
        "P Mitteroecker",
        "P Gunz",
        "K Schaefer",
        "F Bookstein",
        "J Manning"
      ],
      "year": "2005",
      "venue": "Proceedings of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "25",
      "title": "Facial anthropometric differences among gender, ethnicity, and age groups",
      "authors": [
        "Z Zhuang",
        "D Landsittel",
        "S Benson",
        "R Roberge",
        "R Shaffer"
      ],
      "year": "2010",
      "venue": "Annals of Occupational Hygiene"
    },
    {
      "citation_id": "26",
      "title": "Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers",
      "authors": [
        "J Buolamwini"
      ],
      "year": "2017",
      "venue": "Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers"
    },
    {
      "citation_id": "27",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on Fairness, Accountability and Transparency"
    },
    {
      "citation_id": "28",
      "title": "Predictive Inequity in Object Detection",
      "authors": [
        "B Wilson",
        "J Hoffman",
        "J Morgenstern"
      ],
      "year": "2019",
      "venue": "Predictive Inequity in Object Detection",
      "arxiv": "arXiv:1902.11097"
    },
    {
      "citation_id": "29",
      "title": "The perception of variability within in-groups and out-groups: Implications for the law of small numbers",
      "authors": [
        "G Quattrone",
        "E Jones"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "30",
      "title": "On the universality and cultural specificity of emotion recognition: A meta-analysis",
      "authors": [
        "H Elfenbein",
        "N Ambady"
      ],
      "year": "2002",
      "venue": "On the universality and cultural specificity of emotion recognition: A meta-analysis"
    },
    {
      "citation_id": "31",
      "title": "Learning person-specific models for facial expression and action unit recognition",
      "authors": [
        "J Chen",
        "X Liu",
        "P Tu",
        "A Aragones"
      ],
      "year": "2013",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "32",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "Y Tian",
        "T Kanade",
        "J Conn"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Measuring the engagement level of tv viewers",
      "authors": [
        "J Hernandez",
        "Z Liu",
        "G Hulten",
        "D Debarr",
        "K Krum",
        "Z Zhang"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "DeepCoder: Semi-parametric Varia-tional Autoencoders for Facial Action Unit Intensity Estimation",
      "authors": [
        "D Tran",
        "R Walecki",
        "Ognjen",
        "S Rudovic",
        "B Eleftheriadis",
        "M Schuller",
        "Pantic"
      ],
      "year": "2017",
      "venue": "DeepCoder: Semi-parametric Varia-tional Autoencoders for Facial Action Unit Intensity Estimation"
    },
    {
      "citation_id": "36",
      "title": "To frontalize or not to frontalize: Do we really need elaborate pre-processing to improve face recognition?",
      "authors": [
        "S Banerjee",
        "J Brogan",
        "J Krizaj",
        "A Bharati",
        "B Webster",
        "V Struc",
        "P Flynn",
        "W Scheirer"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "37",
      "title": "Cross-dataset learning and person-specific normalisation for automatic Action Unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015"
    },
    {
      "citation_id": "38",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W Chu",
        "F La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Unsupervised domain adaptation for personalized facial emotion recognition",
      "authors": [
        "G Zen",
        "E Sangineto",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th international conference on multimodal interaction"
    },
    {
      "citation_id": "40",
      "title": "A mixture of personalized experts for human affect estimation",
      "authors": [
        "M Feffer",
        "R Picard"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning and Data Mining in Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Personalized modeling of facial action unit intensity",
      "authors": [
        "S Yang",
        "O Rudovic",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "International Symposium on Visual Computing"
    },
    {
      "citation_id": "42",
      "title": "Unsupervised face normalization with extreme pose and expression in the wild",
      "authors": [
        "Y Qian",
        "W Deng",
        "J Hu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Fully automatic face normalization and single sample face recognition in unconstrained environments",
      "authors": [
        "M Haghighat",
        "M Abdel-Mottaleb",
        "W Alhalabi"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "44",
      "title": "Robust statistical frontalization of human and animal faces",
      "authors": [
        "C Sagonas",
        "Y Panagakis",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "45",
      "title": "Generalizing to unseen head poses in facial expression recognition and action unit intensity estimation",
      "authors": [
        "P Werner",
        "F Saxen",
        "A Al-Hamadi",
        "H Yu"
      ],
      "year": "2019",
      "venue": "Proceedings -14th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "46",
      "title": "Real-time expression transfer for facial reenactment",
      "authors": [
        "J Thies",
        "M Zollh Öfer",
        "M Nießner",
        "L Valgaerts",
        "M Stamminger",
        "C Theobalt"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "47",
      "title": "Deep video portraits",
      "authors": [
        "H Kim",
        "P Garrido",
        "A Tewari",
        "W Xu",
        "J Thies",
        "M Niessner",
        "P Pérez",
        "C Richardt",
        "M Zollh",
        "C Theobalt"
      ],
      "year": "2018",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "48",
      "title": "Fewshot adversarial learning of realistic neural talking head models",
      "authors": [
        "E Zakharov",
        "A Shysheya",
        "E Burkov",
        "V Lempitsky"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "49",
      "title": "FSGAN: Subject agnostic face swapping and reenactment",
      "authors": [
        "Y Nirkin",
        "Y Keller",
        "T Hassner"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "The Creation and Detection of Deepfakes: A Survey",
      "authors": [
        "Y Mirsky",
        "W Lee"
      ],
      "year": "2020",
      "venue": "ACM Comput. Surv. 1, 1, Article"
    },
    {
      "citation_id": "51",
      "title": "Facsvatar: An Open Source Modular Framework for Real-Time FACS based Facial Animation",
      "authors": [
        "S Van Der Struijk",
        "M Mirzaei",
        "H Huang",
        "T Nishida"
      ],
      "year": "2018",
      "venue": "Proceedings of the 18th International Conference on Intelligent Virtual Agents, IVA 2018"
    },
    {
      "citation_id": "52",
      "title": "A high-fidelity open embodied avatar with lip syncing and expression capabilities",
      "authors": [
        "D Aneja",
        "D Mcduff",
        "S Shah"
      ],
      "year": "2019",
      "venue": "ICMI 2019 -Proceedings of the 2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "53",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "T Karras",
        "S Laine",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "54",
      "title": "Analyzing demographic bias in artificially generated facial pictures",
      "authors": [
        "J Salminen",
        "S Jung",
        "S Chowdhury",
        "B Jansen"
      ],
      "year": "2020",
      "venue": "Confer-ence on Human Factors in Computing Systems -Proceedings"
    },
    {
      "citation_id": "55",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "56",
      "title": "Unsupervised face normalization with extreme pose and expression in the wild",
      "authors": [
        "Y Qian",
        "W Deng",
        "J Hu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "The validity and practicality of sun-reactive skin types i through vi",
      "authors": [
        "T Fitzpatrick"
      ],
      "year": "1988",
      "venue": "Archives of dermatology"
    },
    {
      "citation_id": "58",
      "title": "BP4D-Spontaneous: A high-resolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "59",
      "title": "DISFA: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Y Tian"
      ],
      "year": "2000",
      "venue": "Proceedings -4th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2000"
    },
    {
      "citation_id": "61",
      "title": "Learning to Generate 3D Stylized Character Expressions from Humans",
      "authors": [
        "D Aneja",
        "B Chaudhuri",
        "A Colburn",
        "G Faigin",
        "L Shapiro",
        "B Mones"
      ],
      "year": "2018",
      "venue": "Proceedings -2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018"
    },
    {
      "citation_id": "62",
      "title": "StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images",
      "authors": [
        "A Tewari",
        "M Elgharib",
        "G Bharaj",
        "F Bernard",
        "H.-P Seidel",
        "P Perez",
        "M Zollhofer",
        "C Theobalt"
      ],
      "year": "2020",
      "venue": "StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images"
    },
    {
      "citation_id": "63",
      "title": "Semi-Supervised StyleGAN for Disentanglement Learning",
      "authors": [
        "W Nie",
        "T Karras",
        "A Garg",
        "S Debnath",
        "A Patney",
        "A Patel",
        "A Anandkumar"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "64",
      "title": "Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning Generated image Pose Expression Illumination Random corpus",
      "authors": [
        "Y Deng",
        "J Yang",
        "D Chen",
        "F Wen",
        "X Tong"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "65",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "FaceForensics++: Learning to detect manipulated facial images",
      "authors": [
        "A Rossler",
        "D Cozzolino",
        "L Verdoliva",
        "C Riess",
        "J Thies",
        "M Niessner"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "67",
      "title": "Deepfake Video Detection Using Recurrent Neural Networks",
      "authors": [
        "D Guera",
        "E Delp"
      ],
      "year": "2019",
      "venue": "Proceedings of AVSS 2018 -2018 15th IEEE International Conference on Advanced Video and Signal-Based Surveillance"
    },
    {
      "citation_id": "68",
      "title": "FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals",
      "authors": [
        "U Ciftci",
        "I Demir",
        "L Yin"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "69",
      "title": "De-identification without losing faces",
      "authors": [
        "Y Li",
        "S Lyu"
      ],
      "year": "2019",
      "venue": "IH and MMSec 2019 -Proceedings of the ACM Workshop on Information Hiding and Multimedia Security"
    },
    {
      "citation_id": "70",
      "title": "DeepPrivacy: A Generative Adversarial Network for Face Anonymization",
      "authors": [
        "H Hukkelås",
        "R Mester",
        "F Lindseth"
      ],
      "year": "2019",
      "venue": "LNCS"
    },
    {
      "citation_id": "71",
      "title": "Deepfakes for medical video de-identification: Privacy protection and diagnostic information preservation",
      "authors": [
        "B Zhu",
        "H Fang",
        "Y Sui",
        "L Li"
      ],
      "year": "2020",
      "venue": "AIES 2020 -Proceedings of the AAAI/ACM Conference on AI"
    }
  ]
}