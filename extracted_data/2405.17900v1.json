{
  "paper_id": "2405.17900v1",
  "title": "Enhancing Emotion Recognition In Conversation Through Emotional Cross-Modal Fusion And Inter-Class Contrastive Learning",
  "published": "2024-05-28T07:22:30Z",
  "authors": [
    "Haoxiang Shi",
    "Xulong Zhang",
    "Ning Cheng",
    "Yong Zhang",
    "Jun Yu",
    "Jing Xiao",
    "Jianzong Wang"
  ],
  "keywords": [
    "Emotion recognition",
    "Multi-modal fusion",
    "Contrastive learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The purpose of emotion recognition in conversation (ERC) is to identify the emotion category of an utterance based on contextual information. Previous ERC methods relied on simple connections for cross-modal fusion and ignored the information differences between modalities, resulting in the model being unable to focus on modalityspecific emotional information. At the same time, the shared information between modalities was not processed to generate emotions. Information redundancy problem. To overcome these limitations, we propose a crossmodal fusion emotion prediction network based on vector connections. The network mainly includes two stages: the multi-modal feature fusion stage based on connection vectors and the emotion classification stage based on fused features. Furthermore, we design a supervised inter-class contrastive learning module based on emotion labels. Experimental results confirm the effectiveness of the proposed method, demonstrating excellent performance on the IEMOCAP and MELD datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are a crucial element in human communication  [20, 15] . In the realm of dialogues between humans and machines, the ability to discern variations in a user's emotional state is of significant importance. The incorporation of emotion recognition techniques  [5, 24]  allows for the development of more empathetic and sensitive intelligent systems. This, in turn, can lead to a more pleasant user experience and a more efficient exchange between humans and machines.\n\nPrevious emotion recognition (ERC) models in conversations only used singlemodal information (like text and audio) for recognition  [5, 9]  , such as simply applying speech emotion recognition (SER)  [8]  to conversations, which is useful to some extent. In daily conversations, the information we are exposed to is diverse, and the use of multiple modalities, such as text, audio, and vision, greatly improves the inference of human emotions because each modality captures different aspects of emotional expression  [12] . Therefore, the effective fusion of multi-modal information is crucial to improving the accuracy and comprehensiveness of predictions. Many works have introduced multi-modality into emotion recognition, which has greatly improved the performance of emotion recognition. For example, Zhao et al.  [22]  proposed a speaker-perceived multi-modal emotion recognition network, and Kim et al.  [7]  integrated multiple audio features and text features to enhance emotion recognition.\n\nHowever, these works still have the following problems with the utilization of multi-modal information: 1) Previous ERC methods use the same modeling approaches to process text and audio data  [23] , ignoring differences between modalities  [18] . 2) During modal fusion, previous models tend to directly connect the two  [2, 17] , causing information redundancy, as the text modality primarily contains content information, while the audio modality captures more prosodic information  [21] . Additionally, this approach overlooks the interaction between modalities. Information between different modalities is often complementary, and a reasonable combination can improve performance.\n\nTo address these issues, we propose a multi-step fusion model based on joint vectors to identify emotions in multi-party conversations. Specifically, the model consists of two stages. In the first stage, we extract the mel-spectrogram of the audio and text embeddings to represent the information of each modality. In the second stage, based on the dual-modality information input, we constructed a multi-modal fusion module using trainable joint vectors and separately encoded the two modalities through pre-trained large language and visual models. Multimodal information interaction is facilitated by joint vectors, thereby enabling information interaction while ensuring the independence of each modality. The emotion recognition task is then completed based on the final generated fusion features. Additionally, to address the problem of imbalanced emotional category samples, we proposed an inter-class contrastive learning module. Using emotional sample labels, we supervised the separation of inter-class samples while simultaneously reducing the distance between intra-class samples to improve the representational ability of multi-modal features. The main contributions of this work are summarized as follows:\n\n-We propose a multi-modal emotion recognition model utilizing joint vectors.\n\nWe employ pre-trained models for modeling and joint vectors for modal interaction, ensuring the independence of each modality.  2 Methods",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Definition",
      "text": "Emotion Recognition in Conversation (ERC). Within a multi-modal discourse denoted by D, we encompass both textual and auditory components, organized as D = {(u 1 , s 1 ), (u 2 , s 2 ), ..., (u N , s N )}. Each tuple (u i , s i ) corresponds to the i th distinct utterance attributed to a speaker within the interaction, with N signifying the number of utterances. The goal of Emotion Recognition in Conversations is to identify the emotion category label E = {e 1 , e 2 , ..., e N } associated with the dialogue.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Overview",
      "text": "The model is mainly divided into three parts: the multi-modal fusion part, the emotion recognition part, and the contrastive learning part, as shown in Fig.  1 . Specifically, in the multi-modal fusion module, we design cross-modal joint vectors to integrate information from dual modalities and generate the final multimodal fusion representation through multiple stacked large model-based fusion modules. We then perform emotion recognition in conversations based on the generated multi-modal fusion representation. In addition, to further improve the multi-modal emotional representation capabilities, we design a contrastive learning module to use emotional labels for inter-class optimization on the generated multi-modal representations, thereby enhancing the recognition effect. Some modules in these components are introduced below:\n\n-RoBERTa encoder operates as an encoder for the extraction of semantic content from text data. For this purpose, we select the output from the 12-th layer of the large-RoBERTa  [11]  model as the text embedding F t . -Feature extractor is used to extract deeper text features and consists of two layers of stacked transformer  [16]  blocks. -Spectrum extraction executes a short-time Fourier transform (STFT) on the input audio signal, subsequently translating the frequency axis to the mel scale, thereby obtaining the mel-spectrogram.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Joint-Based Multi-Modal Fusion",
      "text": "As mel-spectrograms contain rich prosodic information, as detailed in  [19] .To explore the potential of audio frequency domain data and combine it with text features, we developed a joint-based fusion module (JFM), as illustrated in Fig.  1 . First, in order to deal with the text and spectral domain data, we leverage pretrained models from the Language  [3]  and Visual  [4]  Transformer domains, denoted by LT rans and V T rans, respectively. For processing the mel-spectrogram, the initial step involves segmenting it into multiple patches to align with the transformer layer's input requirements. This segmentation enables us to derive the feature embeddings F m via a linear projection method, as referenced in  [4] . Following this, we introduce a trainable joint vector, initialized in accordance with the approach outlined in  [10] , and termed v j . This vector aids in the seamless blending of multi-modal fusion specifics. Our JFM contains N joint-based fusion (JF) blocks. Taking the l th layer as an example, we concatenate v l j with the input F l m→t (the output of the (l -1) th JF layer, where F 0 m→t = F m ), and then feed them into the visual transformer layer V T rans l to building a joint:\n\nwhere ⊕ stands for the concatenate operation. Then, the obtained joint vl j with spectral domain information is mapped to the feature space of the F l t→m (where F 0 t→m = F t ) through an MLP layer, and concatenated with the embedded F l t→m . These are then fed into language Transformer layer LT rans l to obtain the onesided fusion features F l+1 m→t :\n\nwhere vl j = M LP (v l j ). Similarly, as for the other-sided fusion features F l+1 t→m , we adapt symmetric operations:\n\nWhere v ′l j represents different joint vectors from v l j , LT rans ′l and V T rans ′l share the same pre-trained weights as LT rans l and V T rans l .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition",
      "text": "After acquiring the bilateral fusion features, we take the output representation of CLS token of F N m→t and F N t→m as inputs to two distinct linear classifiers. The logits are then averaged to determine the final recognition ri emotions, respectively. The classifier consists of a fully connected layer and a softmax layer. Finally, our classification loss is formulated as:\n\nwhere r i represent emotion labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Inter-Class Contrastive Learning",
      "text": "In the ERC task, due to the imbalance in the number of samples of each category in the dataset, some emotional categories may be ignored in the process of calculating the loss. Therefore, we construct inter-class contrastive learning (ICL) to enable the model to focus on the differences between samples of different emotional categories  [6] . Using label information, all samples with the same emotional label in the same batch are regarded as positive samples, and emotional samples of other categories are regarded as negative samples, thereby shortening the distance between samples of the same category and more clearly distinguishing them from samples of other categories. Assuming there are K samples in a batch, firstly, to comprehensively consider the cross-fusion features output by the multi-modal fusion module, we first concatenate the two fusion features and obtain feature F:\n\nthen we calculate the inter-class contrastive learning loss L ICL using the following method:\n\nwhere i ∈ I = {1, 2, . . . , K} represents the index corresponding to samples within a batch, τ signifies a positive real-valued temperature parameter that is utilized to regulate the spacing among the samples. P (i) refers to the collection of samples that share the same emotional category as the i th sample, N P (i) denotes the number of samples in P (i).\n\n3 Experiments",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Baselines",
      "text": "We have performed experimental evaluations utilizing two distinct datasets: the IEMOCAP  [1]  and the MELD  [13] . The IEMOCAP is a renowned audiovisual database employed extensively in the realm of affective computing, encompassing around 12 hours of recorded material. It is specifically curated for interactions involving two participants. The IEMOCAP dataset segments each dialogue into distinct speech units, applying continuous labeling within the Valence-Arousal emotional spectrum and categorical labeling across various emotions such as anger, happiness, sadness, and neutrality. On the other hand, the MELD dataset is a leading resource for tasks involving the analysis of emotions expressed by multiple speakers. It comprises over 1400 conversations and approximately 13000 speech instances excerpted from the television show F riends. The emotional labeling within this dataset encompasses a broader spectrum with seven distinct emotion labels, which include neutral, happiness, surprise, sadness, anger, disgust, and f ear.\n\nTo demonstrate the effectiveness of our model in both single-channel and multi-channel contexts, we have chosen the following benchmark models:\n\n-SCFA(2023)  [22]  advances an innovative architecture that integrates crossmodal information with a focus on speaker-specific characteristics to enhance the emotion recognition. -MultiEMO(2023)  [14]  proposed a new attention-based correlation-aware multi-modal fusion framework.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "In our research, we leverage the RoBERTa model, which has been pre-trained as referenced in  [11] , to derive features with 768 dimensions from textual data. For constructing the feature extractor, we have employed a stack of two transformer layers, with each layer comprising 8 attention heads and generating embeddings of 1024 dimensions. For the audio modality, we process the source audio to obtain an 80-dimensional mel-spectrogram. Within the JFM, we configure the parameter N to be 2 and define the length of the v j as 4. For an in-depth examination of these parameters, please refer to the parameter analysis in Section 3.6. The entire neural network was optimized using the Adam, with a batch size set to 32 and a learning rate of 0.0001.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results Analysis",
      "text": "To validate the model's performance, we conducted experiments on the two datasets with accuracy (Acc) and weighted F1 (W-F1) metrics, as shown in Table 1. Our model achieves comparable performance across all three modal combinations. Specifically, for example on MELD, our model outperforms SCFA's Acc by 4.50% under the single text modality, slightly lower than MultiEMO by 0.55% In the audio modality, our model demonstrates improved performance compared to SCFA by 1.24% and significantly outperforms MultiEMO by 11.87%. Notably, MultiEMO's inferior performance may be due to its lack of audio-specific feature modeling. Finally, in multi-modal experiments, our model exhibits comparable performance to SCFA and MultiEMO, surpassing both works on the MELD",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted ablation experiments by removing the JFM and ICL modules to assess the effectiveness of our proposed module, as depicted in Table  2 . Taking the results from the IEMOCAP dataset as an example, initially, we removed the JFM module, which involved directly utilizing RoBERTa and feature extraction to output text features and mel-spectrum for late fusion. We employed a separate ViT  [4]  to encode the mel-spectrogram. The results indicated that Acc and W-F1 decreased by 4.67% and 4.53% respectively, demonstrating the significant enhancement of modal fusion efficacy by the multi-modal fusion module. Additionally, we attempted to remove the joint vector v j , disrupting the interaction between the two modalities. This resulted in Acc and W-F1 decreasing by 2.08% and 1.76% respectively, indicating that the proposed joint vector effectively facilitates information exchange between modalities. Finally, we removed the ICL module, resulting in Acc and W-F1 decreasing by 1.32% and 1.04% respectively. This indicates that while the multi-modal fusion features incorporate some emotional information, removing the ICL module diminishes the model's ability to resist sample imbalances, thereby reducing its robustness.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Fusion Method Comparison",
      "text": "In addition, to validate the effectiveness of JFM as the fusion module, we conducted experiments based on two different fusion methods: cross-modal fusion attention (CFA)  [22]  and feature concatenation. Like JFM, we employed a pretrained ViT  [4]  to extract features from the mel-spectrogram for subsequent modal interaction. The experimental results, presented in Table  3 , indicate that replacing JFM with CFA led to a decrease in W-F1 on IEMOCAP by 1.23%. This suggests that JFM demonstrates superior modal information fusion capability compared to CFA, and the joint vector effectively facilitates inter-modal information transmission. Additionally, directly concatenating the mel-spectrogram features with the text features F t and inputting them into the classifier resulted in a decrease in accuracy by 2.73%. This means that direct concatenation has limited effect on modal interaction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Parameter Analysis",
      "text": "We conducted experiments to assess the sensitivity of our model to hyperparameters, focusing mainly on two parameters in multi-modal fusion: the number of JF blocks N and the length of joint vector v j . We varied N from (1, 2, 3, 4, 5) and the length of v j from (1, 2, 4, 8, 16, 24, 32). The experimental results on the IEMOCAP dataset, depicted in Fig.  3 , illustrate the impact of these parameters. In Fig.  3 (a), we observe that increasing the number of JF modules generally improves accuracy indicators, suggesting enhanced extraction of emotional information through stacked JF modules. However, the effectiveness of improvement becomes limited as N > 2, and an increase in N is accompanied by higher memory usage. Conversely, in Fig.  3 (b), it is evident that the performance initially improves and then declines with the increase in the length of the joint vector v j . This phenomenon may arise from the incorporation of redundant modal information in excessively long joint vectors, leading to decreased accuracy. Similar to the effect of N , augmenting the vector length also results in higher memory consumption. Therefore, to strike a balance between memory usage and model performance, we opted for 2 layers of JF blocks for stacking, with the length of the joint vector set to 4.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a multi-modal fusion model for emotion recognition based on joint vectors. Text features and audio features are individually modeled using pre-trained large models to preserve the unique information within each modality, and joint vectors facilitate modal interaction. Additionally, we introduce inter-class contrastive learning based on emotional labels to further enhance the clustering of fused features. Experimental results validate the effectiveness of each proposed module and illustrate the overall superiority of the model performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Acknowledgement",
      "text": "This paper is supported by the Key Research and Development Program of Guangdong Province under grant No.2021B0101400003. Corresponding author is Jianzong Wang (jzwang@188.com) from Ping An Technology (Shenzhen) Co., Ltd..",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our model. The left and right sides represent text",
      "page": 3
    },
    {
      "caption": "Figure 1: Specifically, in the multi-modal fusion module, we design cross-modal joint vec-",
      "page": 3
    },
    {
      "caption": "Figure 1: First, in order to deal with the text and spectral domain data, we leverage pre-",
      "page": 4
    },
    {
      "caption": "Figure 2: Accuracy confusion matrix on two datasets.",
      "page": 7
    },
    {
      "caption": "Figure 3: Model performance under different numbers of JF blocks and different",
      "page": 9
    },
    {
      "caption": "Figure 3: , illustrate the impact of these pa-",
      "page": 9
    },
    {
      "caption": "Figure 3: (a), we observe that increasing the number of JF modules",
      "page": 9
    },
    {
      "caption": "Figure 3: (b), it is evident that the perfor-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of performance. ‘-’ means there is no relevant data in the",
      "data": [
        {
          "0.70": "0.09",
          "0.08": "0.67",
          "0.07": "0.04",
          "0.05": "0.08"
        },
        {
          "0.70": "0.04",
          "0.08": "0.03",
          "0.07": "0.69",
          "0.05": "0.10"
        },
        {
          "0.70": "0.01",
          "0.08": "0.09",
          "0.07": "0.16",
          "0.05": "0.68"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Comparison of performance. ‘-’ means there is no relevant data in the",
      "data": [
        {
          "0.64": "0.05",
          "0.05": "0.66",
          "0.08": "0.06",
          "0.07": "0.09"
        },
        {
          "0.64": "0.08",
          "0.05": "0.03",
          "0.08": "0.68",
          "0.07": "0.15"
        },
        {
          "0.64": "0.09",
          "0.05": "0.08",
          "0.08": "0.13",
          "0.07": "0.67"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Modeling hierarchical uncertainty for multimodal emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "A Zhu",
        "D Ouyang",
        "X Liu",
        "H Shen"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Cybern"
    },
    {
      "citation_id": "3",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019"
    },
    {
      "citation_id": "4",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale. In: 9th International Conference on Learning Representations",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale. In: 9th International Conference on Learning Representations"
    },
    {
      "citation_id": "5",
      "title": "A review on speech emotion recognition: A survey, recent advances, challenges, and the influence of noise",
      "authors": [
        "S George",
        "P Ilyas"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Supervised contrastive learning",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "year": "2020",
      "venue": "Supervised contrastive learning"
    },
    {
      "citation_id": "7",
      "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
      "authors": [
        "K Kim",
        "N Cho"
      ],
      "year": "2023",
      "venue": "Interspeech 2023, 24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "Selective acoustic feature enhancement for speech emotion recognition with noisy speech",
      "authors": [
        "S Leem",
        "D Fulford",
        "J Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "MAGDRA: A multi-modal attention graph network with dynamic routing-by-agreement for multi-label emotion recognition",
      "authors": [
        "X Li",
        "J Liu",
        "Y Xie",
        "P Gong",
        "X Zhang",
        "H He"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "10",
      "title": "Efficient multimodal fusion via interactive prompting",
      "authors": [
        "Y Li",
        "R Quan",
        "L Zhu",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023"
    },
    {
      "citation_id": "11",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "12",
      "title": "CARAT: contrastive feature reconstruction and aggregation for multi-modal multi-label emotion recognition",
      "authors": [
        "C Peng",
        "K Chen",
        "L Shou",
        "G Chen"
      ],
      "year": "2024",
      "venue": "Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024"
    },
    {
      "citation_id": "13",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "14",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023"
    },
    {
      "citation_id": "15",
      "title": "Qi-tts: Questioning intonation control for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Leveraging Label Information for Multimodal Emotion Recognition",
      "authors": [
        "P Wang",
        "S Zeng",
        "J Chen",
        "L Fan",
        "M Chen",
        "Y Wu",
        "X He"
      ],
      "year": "2023",
      "venue": "Interspeech 2023, 24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "18",
      "title": "Multi-scale receptive field graph model for emotion recognition in conversations",
      "authors": [
        "J Wei",
        "G Hu",
        "L Tuan",
        "X Yang",
        "W Zhu"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "19",
      "title": "Speech representation disentanglement with adversarial mutual information learning for one-shot voice conversion",
      "authors": [
        "S Yang",
        "M Tantrawenith",
        "H Zhuang",
        "Z Wu",
        "A Sun",
        "J Wang",
        "N Cheng",
        "H Tang",
        "X Zhao",
        "J Wang",
        "H Meng"
      ],
      "year": "2022",
      "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "20",
      "title": "Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "authors": [
        "T Zhang",
        "Z Chen",
        "M Zhong",
        "T Qian"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023"
    },
    {
      "citation_id": "21",
      "title": "Aia-net: Adaptive interactive attention network for text-audio emotion recognition",
      "authors": [
        "T Zhang",
        "S Li",
        "B Chen",
        "H Yuan",
        "C Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "22",
      "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
      "authors": [
        "H Zhao",
        "B Li",
        "Z Zhang"
      ],
      "year": "2023",
      "venue": "Interspeech 2023, 24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "Knowledge-aware bayesian co-attention for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "24",
      "title": "Improving eeg-based emotion recognition by fusing time-frequency and spatial representations",
      "authors": [
        "K Zhu",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    }
  ]
}