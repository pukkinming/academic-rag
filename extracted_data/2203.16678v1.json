{
  "paper_id": "2203.16678v1",
  "title": "Knowledge-Spreader: Learning Facial Action Unit Dynamics With Extremely Limited Labels",
  "published": "2022-03-30T21:12:13Z",
  "authors": [
    "Xiaotian Li",
    "Xiang Zhang",
    "Taoyue Wang",
    "Lijun Yin"
  ],
  "keywords": [
    "Semi-supervised learning",
    "facial action unit detection",
    "knowledge distillation",
    "sparsely labeled data",
    "Spatial-Temporal",
    "pseudo-labeling Spatial knowledge distillation Temporal knowledge distillation Labeled key frames The line dividing Branch A and Branch B"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent studies on the automatic detection of facial action unit (AU) have extensively relied on large-sized annotations. However, manually AU labeling is difficult, time-consuming, and costly. Most existing semi-supervised works ignore the informative cues from the temporal domain, and are highly dependent on densely annotated videos, making the learning process less efficient. To alleviate these problems, we propose a deep semi-supervised framework Knowledge-Spreader (KS), which differs from conventional methods in two aspects. First, rather than only encoding human knowledge as constraints, KS also learns the Spatial-Temporal AU correlation knowledge in order to strengthen its out-of-distribution generalization ability. Second, we approach KS by applying consistency regularization and pseudo-labeling in multiple student networks alternately and dynamically. It spreads the spatial knowledge from labeled frames to unlabeled data, and completes the temporal information of partially labeled video clips. Thus, the design allows KS to learn AU dynamics from video clips with only one label allocated, which significantly reduce the requirements of using annotations. Extensive experiments demonstrate that the proposed KS achieves competitive performance as compared to the state of the arts under the circumstances of using only 2% labels on BP4D and 5% labels on DISFA. In addition, we test it on our newly developed large-scale comprehensive emotion database, which contains considerable samples across well-synchronized and aligned sensor modalities for easing the scarcity issue of annotations and identities in human affective computing. The code and new database will be released to the research community.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human facial action dynamics is crucial for human-computer interaction (HCI), which reflects the individuals' affective states to enhance the affinity of communication. Facial action unit (AU) detection plays a vital role in automatic facial action analysis. Over the past few years, a substantial amount of works  [11]  [47]  [1]  based on deep learning have shown the superior power of constructing informative features against the traditional methods. The majority of the recent advances  [22]  [43]  [27]  [6]  [39]  [48]  [12]  in this area are heavily relied on large-scale labeled datasets for achieving remarkable learning performance. However, a lab-controlled AU video typically contains thousands of frames that need to be densely labeled by human annotators. As a result, datasets for dynamic facial action detection suffer high redundancy both content-wise and annotation-wise.\n\nThere are only limited works attempting to mitigate the demand for dense AU occurrence annotations. For example, the researchers  [41]    [46]  [32]  [31]  summarize the distribution from existing ground-truth AU labels as the prior for detecting AUs with partially labeled data. However, the approaches of applying the distribution prior may fall into sub-optimal due to lacking adaptation mechanism. Deep semi-supervised learning has become a new choice to overcome these issues for their powerful representation and generalization ability. The existing deep semi-supervised learning methods can be roughly categorised into three methods (i.e. generative method, consistency regularization method, and pseudo-labeling methods). Recently, some hybrid methods attract the attention from many researchers. MixMatch  [23]  combines entropy minimization and consistency regularization in an unified loss function. FixMatch  [37]  combines consistency regularization and pseudo-labeling considering both labeled and unlabeled data should be trained simultaneously. Whereas, applying these image-level models to dynamic datasets is challenging due to some nuisance factors such as motion blur, video defuse, and frequent pose occlusions. Besides, human facial action characteristics present strong dependencies and mutual exclusive relation among spatially local regions  [21] . For instance, the AU1 and AU2 are usually co-existence due to the constraint of facial muscles. Some recent works  [24]    [3]  prove that the relationship of different AUs in temporal domain is also an important factor for robust AU detection. Both of the two factors have been studied for fully supervised methods, but there lacks a framework to explore if learning both spatial and temporal AU correlation knowledge can boost the semi-supervised process.\n\nIn this paper, we propose an end-to-end trainable framework Knowledge-Spreader (KS) for learning semi-supervised facial action dynamics. Knowledge-Spreader improves conventional semi-supervised learning in three ways. First, different from previous video-level works that need fully supervisory information or at least a few labeled frames as input segments, KS just needs limited sparsely labeled video clips, with only one label allocated. This alleviates the demand on densely labeled data effectively. Second, incorporating the weak supervisory information from both human prior and AU correlation knowledge makes KS more efficient and robust for inferring out-of-distribution information. KS learns both spatial and temporal AU correlation knowledge by transformers. The powerful generalization capability of deep learning can reduce some side effects caused by incorrect knowledge summarized by humans. For instance, some AU intensity estimation works  [50]  assume that there are only one peak or valley in a video sequence, while ignore the special situation that multiple peaks and valleys exist in a video. Third, our work improves the hybrid semi-supervised method (consistency regularization and pseudo-labeling) to a video level model by introducing an operation \"Knowledge Spreading\". The overall pipeline of Knowledge Spreading is shown in Figure  1 . The qualitative training in Figure  1  refers that the network is trained with a small amount of labeled images, while the quantitative training indicates the training with a large-sized unlabeled data. Our contribution lies in three-fold:  (1)  We propose a deep semi-supervised architecture for AU detection by jointly utilizing the learned Spatial-Temporal AU correlation knowledge and human knowledge. (2) This is the first work to incorporate knowledge spreading by dynamic knowledge distillation and temporal confirmed pseudo-label in a self-supervised manner, to learn spatio-temporal information using extremely limited single-frame labeled video clips.  (3)  We have built a new spontaneous emotion database by capturing 3D geometric facial sequences, 2D facial videos, thermal videos, and physiological data sequences from 233 participants across two years period. The new database will be released to the research community along with the paper being published.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Facial Au Detection",
      "text": "Different from general computer vision tasks, facial AUs are defined to be associated with atomic local facial muscles, which in turn correspond to the appearance features of different regions in the face. Thus, previous research  [53]  [42]  [35]  [43]  [52]  has extensively studied how to use manually-defined regions, local patches, facial landmarks, heatmap, attention mechanism, and other methods to localize detailed facial parts. However, considering the structural information and dependencies among different AUs, conventional CNN is criticized to be incapable of fully characterizing their correlations. Recent advances  [6]  [40]  [48]  [12]  [17]  start to explore the AU relations by using data distribution, conditional random field, Bayesian Network, graph neural networks, attention mechanism, and other approaches.  [3] [19] [26]  attempt to integrate the inter-relationship of AUs on both spatial and temporal dimensions for learning more robust AU dependencies.\n\nSome of the existing semi-supervised or self-supervised methods  [22]  [43] [27]  [6]  still rely on large-sized annotations and extra unlabeled data. Only a small amount of research has attempted to ease the intensive demand for AU annotations.  [41]  and  [46]  summarize the distribution from ground-truth AU labels as the prior for detecting AUs with limited labeled data with semisupervised learning.  [32]  [31] leverage the joint distribution of features, AUs, and facial expression to explore the global dependencies among them. Different from all aforementioned works, KS can learn the informative Spatial-Temporal cues from sparse annotations by incorporating dynamic knowledge distillation and pseudo label.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Semi-Supervised Learning",
      "text": "Semi-supervised learning is an approach to combine a few labeled data with a large amount of unlabeled data during training. Deep semi-supervised learning is a fast-growing filed with a wide broad of practical applications. FixMatch  [37]  has been proved to be a simple but efficient SSL method by combining both consistency regularization and pseudo-labeling. Noisy Student improves the idea of self-training and distillation with the use of noise added to the student networks. However, these works are incapable to capture the facial action dynamics as an image-level model. Recent works  [36]  [44]  [30]  propose to recognize actions from only a handful of sparsely labeled videos with the rich supervisory information in a semi-supervised way. Unlike these approaches that still need continuous and dense annotation, we aim to allocate only single-frame label for each video clip to handle the condition that the labels are extremely sparse, missing, or scarce.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview",
      "text": "Our goal is to detect facial action units with video clips with single-frame labels. Specifically, we assume the facial action labels in training data are available every k frames. The ith video clip consisted of n frames is a set\n\nwhere F i u means unlabeled frame and F i l means labeled frame. Knowledge-Spreader is consisted of three key modules: (1) Spatial-Temporal information learning module (SIL), (2) knowledge spreading module (KSM), and (3) temporal confirmed pseudo-label module (TPL). The network structure of KS is depicted in Figure  2 . Two level distillation (grey) and pseudo-labeling (green) consist the proposed \"knowledge spreading\". A self-supervised module \"temporal confirmed pseudolabel\" (yellow) is designed for sensing the temporal perturbation and selecting pseudo labels with high confidence. Details are described in Section 3.1.\n\nSpatial-Temporal Information Learning (SIL) SIL is an self-attentionbased module for learning semi-supervised AU dependencies in both spatial and temporal domain. Transformers have been proved to be an effective method of learning informative temporal context in natural language processing. In order to address the limitation of applying conventional transformers on computer vision, ViT  [7]  splits an image into fixed-sized patches to linearly embed them with position embedding. We extend the original ViT to learn the semantic relationship of AUs in both spatial-wise and temporal-wise. As shown in Figure  2 , the features from branch A and branch B are extracted by base models which are ResNet-18  [9]  pre-trained on ImageNet  [33] . Model S a is designed for learning the spatial AU correlation. We firstly decouple the extracted global features p s into multiple branches by applying global-average-pooling. The collection of decoupled features is represented as\n\nwhere u means the number of facial action units. Afterward, we assign u 1D learnable positional embedding to be added with these features as AU-specific embedding. Then, the AU-specific embedding sequence is fed to a standard Transformer encoder for learning the spatial-wise AU correlation knowledge. Note that, the module does not need any extra classification token, for no obvious performance gains are observed by applying this design. Likewise, the temporal model T b in branch B also adopts the same model design. The collection of frame-specific features is depicted as\n\nwhere n is the number of frames in each input sequence. The frame-specific features are extracted by the sub-networks in S n b and added with the corresponding temporal embedding.\n\nKnowledge Spreading Module (KSM) KSM is the core module of the proposed Knowledge-Spreader. Inspired by FixMatch, our model also adopts a hybrid semi-supervised learning method by combining consistency regularization and pseudo-labeling. In this paper, we improve it to a video level method in the purpose of learning efficient AU dynamics. We approach KSM by several main steps: (1) train a group of Spatial Students S b which accommodate all the frames of input sequence, (2) let the length of input clip equals to the number of Spatial Students, (3) couple the input image in branch A with the key frame of input video clip in branch B, (4) shift the location of key frame during training different batch of training samples, and (5) process distillation when Spatial Student gets the key frames, otherwise process pseudo-labeling. Furthermore, in terms of the distillation, we adopt two-level and bidirectional knowledge distillation design. The first level is from Spatial Teacher S a to Spatial Students S b . For the MLP-based Spatial Students S b , we apply data random augmentation and model dropout as the noise. We assume the knowledge learned by Spatial Teacher without noise is consistent with that learned by noisy Spatial Students. The second level is from Temporal Teacher T b to Temporal Student S a . Instead of applying any artificial perturbation for temporal distillation, we utilize the original difference of input data for T b and S a . We assume the spatial knowledge learned by S a should be consistent with the temporal knowledge learned by T b . Thus, our model forces the Temporal Student S a to learn the temporal AU knowledge harder, even if it only gets the image-level inputs. Here, Model S a plays different roles in the two level distillation. Inspired by recent work  [8] , we adopt an online distillation method using KL divergence. The KL divergence loss in this work is used to minimize the probability distribution of student networks and their corresponding ensembles. The KL loss function of spatial knowledge distillation is defined as\n\nwhere b is the batch size, T is the temperature parameter. p and w denote the soften probability distribution calculated by the Spatial Teacher S a and one Spatial Student selected from S 1 b , S 2 b , ..., S n b . The soft target q is expressed as q = sof tmax(z s /T ) where z s is performed by the mean pooling of the outputs from both S a and one selected S b . m is the key frame position used for controlling which Spatial Student is selected as the target of knowledge spreading. m equals B mod n, where B is the Bth batch of training samples, n is the clip length. In this paper, temperature parameter is set as 1. The KL loss function of temporal knowledge distillation is defined as\n\np and w denote the soften probability distribution calculated by the Temporal Teacher T b and the Temporal Student S a respectively. Considering the data imbalance issues from skewing the training process and affect the performance of the model. We choose weighted BCE with logits as the multi-label classification loss of student networks and the function can be described as:\n\nwhere x refers to the output O a and O b for the loss function L s and L t in Equation (  4 ). x also refers to the final ensemble output O output for L bce in Equation  (7) . σ(x) is the corresponding predicted probability. y is the AU occurrence ground truth. The loss functions of spatial knowledge distillation and temporal knowledge distillation are expressed as:\n\nwhere α is the trade-off weight, z is the number of student networks. Although branch B contains multiple student networks S 1 b , S 2 b , ..., S n b , only one of them is selected for calculating the loss function at the same time. Thus, z equals 2. In this paper, α is set as 0.5.\n\nPseudo-label  [15]  is a simple but efficient formulation of training models in a semi-supervised way. We deploy it to train the Spatial Students in S 1 b , S 2 b , ..., S n b when unlabeled frames are assigned as the input. Note that, the labeled data and unlabeled data are jointly trained. The loss function is defined as\n\nwhere ŷ denotes the pseudo labels of unlabeled frames by picking up the class which has the maximum predicted probability. The total loss function of semisupervised learning can be denoted as\n\nTemporal Confirmed Pseudo-label (TPL) We exploit a simple self-supervised method for confirming confident pseudo labels by predicting if the features have any temporal perturbation. The smooth evolution of facial muscles leads that the facial appearance also moves gradually and smoothly over time. Thus, the pseudo labels generated by incorrect features contains anomalies in the temporal domain. Researchers  [44]  utilize the self-supervised sequential perturbation to improve their model's robustness and generalization. Inspired by this work, we use the temporal feature shuffling to simulate the sequential perturbation and generate negative feature samples. Although a few video clips contain fully still frames which may diminish the contribution of temporal feature shuffling, they only occupy a small portion in the whole dataset. In addition, these samples do not contain any temporal information. Therefore, labeling them incorrectly does not affect the learning ability of the spatial model. The auxiliary task is defined as a binary classification task that is jointly trained with the AU classifier on Model T b . L ssl denotes the loss of the binary classification task. Specifically, We first label the ith collection of sequential features D i b as true while the shuffled feature collection D i pb as false. Afterward, D i b and D i pb are sequentially fed into Model T b . The loss L ssl encourages the model to classify them correctly. Figure  3  illustrates the pipeline of the proposed module. As shown in Figure  6 , the negative samples show the irregular pattern that AU occurs and disappears repeatedly in a short-term period. By detecting the temporal perturbation of the features and filtering out untrustable pseudo labels, TPL reliefs the confirmation bias issue existing in conventional pseudo-labeling methods.\n\nOverall loss function and algorithm The total loss is composed of the losses from previous sections, as follows:\n\nwhere w ramp is a ramp-up function to make sure the semi-supervised learning and self-supervised learning converge relatively slowly compared with the fully-supervised task. It is inspired by  [14] . The function µ, as a simple Gaussian curve function, is defined as:\n\nwhere x is the epoch number. In this paper, we set ω = 2, µ = 0, and σ = 5.\n\nHere the w ramp is set as 1 after 5 epochs' warming-up for getting the optimal effect. The algorithm of KS is shown in Algorithm 1.",
      "page_start": 4,
      "page_end": 9
    },
    {
      "section_name": "Algorithm 1 Pseudocode Of Knowledge-Spreader",
      "text": "Require: The input frame F i l , the input clip V i and its frame number N , the position of key frame m. B means the Bth batch of training sample. Functions of the models in branch A: base model b θ (x), Model Sa f θ (x). Functions of the models in branch B: base model bσ(x), sub-networks in S n b with supervision lσ(x) and pseudo-labeling kσ(x), The AU detection classifier of Model T b fσ(x) and the binary classifier for self-supervised learning gσ(x).\n\n1: for each epoch E do 2:\n\nfor each mini-batch b do 3:\n\n4:\n\n5:\n\nfor q = 1,...,N do",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "6:",
      "text": "if q == B mod N then 7:\n\nbbase )",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "8:",
      "text": "process spatial KD with O i a and O i k 9:\n\nelse 10:\n\n11:\n\nend if",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "12:",
      "text": "end for 13:\n\n14:\n\n15:\n\n16:",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "17:",
      "text": "if E <= 2 or O i ssl == 1 then",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "18:",
      "text": "Let the weight of Lsemi, λ4 = 0",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "19:",
      "text": "end if",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "20:",
      "text": "process temporal KD with O i b and O i a 21:\n\n22:\n\nUpdate θ and σ via SGD of Equation (  7 )",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "23:",
      "text": "end for 24: end for 4 Experiments",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Datasets",
      "text": "BP4D and DISFA: BP4D  [49]  and DISFA  [25]  are are widely used benchmark databases for dynamic AU detection. We followed the experimental setting of the previous work  [17]  to evaluate our approach for a fair comparison.\n\nNew MME: The existing facial action datasets are limited in terms of subjects number, diversity, and metadata. Thanks to the existing available multimodal datasets  [49]  [51], we extend to develop a new larger-scale multi-modal emotion (MME) database, which consists of 233 participants (132 females and 101 males). The data is significantly expanded in terms of participants number as compared to the existing databases: DISFA (27 subjects)  [25] , MMI (44 subjects)  [29] , BP4D (41 subjects)  [49] , BP4D+ (140 subjects)  [51] . Following ethical principles, our data collection was approved by the institutional review board (IRB). Each subject signed an informed consent form. A professional performer/interviewer applied a procedure containing 10 seamlessly-integrated tasks as  [49]  [51] that resulted in effective elicitation of spontaneous emotions. The dataset was well-synchronized and aligned with multi-modalities including 3D geometric facial model, 2D facial videos, thermal videos, and physiology data sequences (e.g. heart rate, blood pressure, skin conductance (EDA), and respiration rate). Around 94,000 frames were well-annotated by three expert FACS coders for AU coding. More details are described in the supplemental material. The new database is ready for public and will be released to the research community by the time of the paper being published.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Implementation Details",
      "text": "We process the image by cropping off redundant area which is not relevant to face recognition. Then the images are resized to 224 × 224 × 3 (H × W × C) to fit the model. Each of the training images is randomly rotated, flipped horizontally, and with color jitters (saturation, contrast, and brightness) for data augmentation. We choose SGD as the optimizer with a learning rate of 0.01 for 50 epochs. The model was implemented with Pytorch framework. The hyper-parameters in Equation (  7 ) are set as λ 1 = 0.5, λ 2 = 0.5, λ 3 = 0.2, and λ 4 = 0.25. We use video clips with 5 frames for learning the temporal information.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Model Analysis",
      "text": "Comparison with semi-supervised methods Figure  4  shows the performance comparison with semi-supervised methods from two areas (AU detection and general action recognition). We carefully investigated the existing works that adopt limited labels for AU detection. BGCS  [46]  and DAUR  [41]  are selected for comparison. Figure  4 (d) , (e), and (f) shows the proposed model achieves significant performance improvement. Considering our foundation model may have advantages in generalization ability, we can compare the performance trend. With the available labels decreasing (from 90% to 50%), Knowledge-Spreader shows no obvious performance attenuation. Note that some other semi-supervised models from  [32]  [43] [27]  [5]  are not selected for comparison considering they use full annotation pools, extra data, or other jointly trained tasks. We further report the comparison results with some semi-supervised methods from general action recognition for a comprehensive evaluation, including Pseudo-label  [15] , FixMatch  [37] , and a video-level TCL  [36] . Compared with the conventional setting of previous AU works  [46]  [41]  [31]  [32], the percentages of available AU annotations are significantly reduced (1%, 2%, 5%, 10%, 20%, 50%, 60%, 70%, 80%, 90%, and 100%) to explore where the limit of KS is. Figure  4   (c) shows prominent improvement of KS, especially when extremely limited annotations are available (1%, 2%, 5% , 10%, 15% and 20% on BP4D; 1%, 2%, and 5% on DISFA; 1%, 2%, 5% , 10%, 15% and 20% on MME). More quantitative reports are shown in the supplementary. Comparison with supervised methods We report the results under two training setups as  [45] : (1) Compare KS against the fully-supervised state-ofthe-art methods with 100% labeled data. (2) Compare KS against a supervised counterpart under different training label ratios. As shown in Table  4 , a collection of recent and strong benchmark algorithms are selected for better evaluation. Knowledge-Spreader outperforms all other advances using only 10% labels on BP4D and 60% labels on DISFA. KS still performs competitively using only 2% labels on BP4D and 5% labels on DISFA. In addition, the experiment conducted with 100% labels shows the effeteness of Knowledge-Spreader in a supervised manner, where the main contribution comes from Spatial-Temporal information learning module. Especially, it shows that KS surpasses the best benchmark (FAUDT) by 1.3 f1-score on DISFA. Table  5  further shows the comparison results in terms of individual AUs. The proposed method performs best on 8 out of 12 AUs on BP4D and 3 out of 8 AUs on DISFA.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Data Structure Analysis",
      "text": "Through the statistical experiments, we find the non-overlapping AU annotations (different AU combinations) account for only a small proportion of the overall frames number (1693 out of 140,000 frames in BP4D, 102 out of 130,000 frames in DISFA, 748 out of 94,000 frames in MME). A large number of similar labels and data densely exist across adjacent frames. It reveals that why using only a few sparsely sampled clips and annotations can achieve competitive or even better performance, which is consistent with the \"less is better\" principle from  [16] . Different from existing video-level semi-supervised works  [2]  [44]  [36]  [13] that adopt continuous annotations, we sparsely sample the annotations and allocate only one annotation for every k frames. Figure  5  demonstrates that applying our method can reserve more non-overlapping AU annotations than conventional approaches with using the same number of annotations. Consequently, by utilizing abundant non-overlapping AU annotations, semi-supervised models testing on BP4D and MME is relatively more robust to resist the interference caused by missing annotations.  Effect of the Spatial-Temporal information learning module (SIL): We replace the Transformer-based SIL design with a vanilla MPL module to learn the spatial information and integrate them for learning temporal cues for a fair comparison. As shown in Table  3 , the simple design leads to obvious performance degradation of KS due to being incapable of learning the AUs correlation knowledge.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "Effect of the knowledge spreading module (KSM): We keep the design of key frame shifting operation, while remove the spatial knowledge and temporal knowledge distillation by setting the corresponding loss L s and L t to be unavailable. As a core module of Knowledge-Spreader, it plays an important role in fully spreading the AU semantic knowledge. There is an obvious performance gap between the model with and without KSM. For instance, without this module, the F1 score decreases 2.2, 2.3, and 1.8 on three databases with 5% labeled data. Similarly, it decreases 1.8, 1.3, and 1.8 with 50% labels. It demon- strates that the performance degradation of the model without KSM becomes more serious as the affordable labels decrease.\n\nEffect of the temporal confirmed pseudo-label (TPL): As shown in Table  3 , with the assistant of TPL, the performance of KS is improved under different circumstances. In addition, we compare the accuracy of pseudo labels generated by TPL and naïve pseudo-label design on BP4D using 10% labels. We get 76.35% accuracy with TPL and 73.36% with the original pseudo-label, which shows the TPL achieves prominent improvement by discarding the incorrect pseudo labels with temporal perturbation. A sample is given in Figure  6  to illustrate how TPL senses and processes the incorrect pseudo labels. All the above ablation studies prove that our complete model performs the best by integrating the three key components.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a new unified Knowledge-Spreader architecture to learn the interactive Spatial-Temporal correlation knowledge with sparsely labeled videos by incorporating dynamic knowledge distillation, pseudo-labeling, and AU semantic encoding modules. Results show that the proposed model using extremely limited annotations achieves superior performance than existing methods. Comprehensive studies have demonstrated the key factor that leads to the success of Knowledge-Spreader in terms of model design and data structure, in hope of inspiring future works. In addition, a large-scale dataset for spontaneous and dynamic facial action analysis is introduced to alleviate the scarcity issue of AU annotation and subject samples. In the future, we plan to explore the application of Knowledge-Spreader on multi-modal recognition tasks. This material is based on the work supported by the US National Science Foundation.\n\n6 Supplementary of Model Design The Spatial Teacher (or Temporal Student) S a and the Temporal Teacher T b , in Figure  7 , are designed for modeling the relationships between AUs in both the intra-frame and inter-frame levels. In Spatial-Temporal Information Learning (SIL) module, we first assign U 1D learnable AU positional embedding to be added with input feature set D s as AU-specific embedding, where U means the number of facial action units. For uth AU-specific features, a ViT transformer encoder S a generate a set of query, key, and value tensors (Q u , K u , V u ), each with dimension R C×W . Afterwards, a C × U × U learnable attention matrix is obtained as follows:\n\nSimilarly, we assign N 1D learnable frame positional embedding to be added with input feature set D b as frame-specific embedding, where N means the length of the input sequence. For nth frame in a sequence, another set of query, key, and value tensors (Q n , K n , V n ) are generated with the dimension R C×W . A C ×N ×N attention matrix is obtained as follows:\n\nThrough a series of dimensional changes, model S a and T b outputs U dimensional tensors O a and O b respectively. Finally, we take the mean ensemble output O output of O a and O b for multi-label prediction.\n\nBesides, we design N small-sized MLP-based models for the Spatial Students S b . The dimension of each output feature p b for Spatial Student is C × 1 × W .",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Q&A For Model Design And Experiments",
      "text": "In this section, we explain some doubts of the model design and experiments.\n\nWhy the knowledge spreading operation matters for semi-supervised learning? How to maximize the use of a small but credible learned knowledge to spread into a large amount of unknown data is the core problem solved by KS. The dynamic knowledge spread with two-level distillation, which can also be seen as a two-level lever, can allow KS to transform a few spatial knowledge into a large amount of spatial knowledge, and then form more temporal knowledge. Compared with static knowledge distillation, dynamic knowledge spread contains multiple level of leverage to infer more out-of-distribution knowledge with the fewest labels.\n\nWhat's the difference between general Spatial-Temporal information and our Spatial-Temporal AU correlation knowledge? Two attention matrices, in Figure  7 , contains the relevance of each atomic AU class and frame-level class. By modeling the spatial and temporal AU dependency in attention matrices, the video-level and frame-level AU co-occurrence and mutual exclusive relation is refined and learned to improve the the general Spatial-Temporal information.\n\nWhy do we choose transformer? First, self-attention based methods (i.e., JAAnet  [35] ) have been proved to be very effective in learning AUs semantic relation. Second, the residual connection, multi-head attention, and positional embedding designs make it an efficient tool to learn long-range temporal cues and more discriminative representations. Third, our experiments show that transformer performs better than other popular models such as regular GCNs.\n\nWhy do we need both consistency regularization and pseudo-labeling? The consistency regularization is used for building two level knowledge distillation. It distills the fully modeled spatial AU relationship knowledge from branch A to branch B, and distill the temporal AU correlation knowledge from branch B to branch A. However, without pseudo-labeling, some of the Spatial Students stay idle and the unlabeled data is not fully utilized, making the training process less efficient. By combining consistency regularization and pseudo-labeling, KS can accelerate the speed and effect of knowledge distillation and spread.\n\nWhy are Spatial Student models based on MLP instead of transformer? In most cases, KS need feed Spatial Student models with unlabeled data, which means the results derived by Spatial Students is less credible. Thus, a weak design of Spatial Student is necessary. In additional, different design of Spatial Student amd Spatial Teacher can be recognized as a noise or a model-wise perturbation for better applying the consistency regularization.\n\nWhy do we need the temporal constrain for pseudo-labeling? First, automatic AU detection, as a multi-label task, faces some difficulties in selecting and retaining the pseudo labels when only partial labels are with high confidence score. Compared with multi-class tasks, the one-hot format of multi-labels makes it hard to decide if the pseudo labels are confident in a holistic way or a local way. Setting the temporal constrain makes it easier to filter the cases which are not consist with the regular pattern of facial action movements. Second, the temporal label smoothness is also a soft constrains from human knowledge for better generalizing out-of-distribution AU data.\n\nHow to generate pseudo labels? We pick up the class which has maximum predicted probability for each binary class of unlabeled samples.\n\nCan KS be trained with video clips without any labels? If yes, how? Yes, it can. When feeding KS with the unlabeled video clips, we only update our model with the loss of pseudo-labeling and Temporal Teacher. It worth noting that the unlabeled video clips are not allowed to use before the model training is stable (10 epochs in this project). Otherwise learning unreliable knowledge first will lead the model to the error-prone issue.\n\nWhy using 20% labels achieves nearly the same performance as using 100% on BP4D? This is due to the large portion of overlapped annotations, using 20% labels with sparsely sampled annotations makes the performance reach the \"saturation\" quickly, hence there is no significant performance gain after 20% towards the use of 100% labels. Our finding complies with the \"less is better\" principle confirmed by the other existing works  [16] .\n\nHow about applying different sequential perturbation for the pseudo label confirmation module? The fact is that using or mixing certain perturbations (e.g., temporal feature shift, random mask, and flip) does not bring obvious performance improvement. We speculate that other perturbations can not model the temporal fluctuations caused by incorrect pseudo labels well. Applying inappropriate or excessive perturbation operation can even degrade the performance.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Inference Strategy",
      "text": "We adopt the inductive learning manner at testing stage. A random key frame number is assigned to each batch of input video clips. Given a video clip, both branch A and branch B are used to extract the feature O output for prediction.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Additional Implementation Details",
      "text": "All training images in the same video clips are randomly rotated (-45 to 45 degrees), flipped horizontally (50% possibility), and with color jitters (saturation, contrast, and brightness) simultaneously. The detailed specification of Knowledge-Spreader is shown in the original code (model designing part). The complete code will be released to the research community by the time of the paper being published. We choose 5 as the frame length of each input video clip for the optimal time-and-accuracy trading-off. The analysis of the hyper-parameters can be seen in Section 6.6. We implement our Knowledge-Spreader (KS) with the Pytorch framework and perform training and testing on the NVIDIA GeForce 2080Ti GPU.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Additional Quantitative Evaluation",
      "text": "The quantitative results with different label ratios are shown in Table  4  for reference. It corresponds to Figure  4  in the original paper. In addition, due to the page limitation, only partial comparison results with supervised methods in terms of individual AU are shown in Table  2  of the original paper. Table  5  shows the complete comparison results. To investigate the influence of the input clip length, we perform experiments by the proposed model with 10% sparsely sampled annotations on BP4D and DISFA. Figure  8  shows the F1 score curve with n changes. Overall, the performance improve with the n increases from 2 to a certain threshold. A long video clip, on the other hand, results in high computational and memory costs. For optimal trading-off, 5 to 7 is a proper setting for the video clip length n.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Parameter Scale Analysis",
      "text": "The trainable parameter size of the proposed model is around 25 million, which makes KS a very light-weighted model. Compared with the baseline algorithm EACnet  [20] , which contains 138 million parameters, Knowledge-Spreader, as a video-level model, reduces considerable parameter (80%) but achieves excellent performance improvement.\n\n7 Supplementary of New MME",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Participants",
      "text": "233 participants were recruited from our University. There are 132 females and 101 males, with ages ranging from 18 to 70 years old. Ethnic/Racial Ancestries include Asian, Black, Hispanic/Latino, White, and others (e.g., Native American).",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Recording System And Synchronization",
      "text": "Our data collection system consists of a 3D dynamic imaging camera system, a thermal sensor, a physiological signal sensor system, and a studio-quality audio recorder. The system setup and synchronization method are basically consistent with BP4D+  [51] .",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Emotion Stimulus",
      "text": "Ten tasks were performed to elicit a wide range of spontaneous emotion expression (from positive, to neutral, and to negative) and inter-personal facial action behavior by a professional interviewer. Table  6  illustrates the detailed description for the designed tasks.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall pipeline of Knowledge Spreading. (1) Train a Spatial Teacher",
      "page": 3
    },
    {
      "caption": "Figure 1: The qualitative training in Figure 1",
      "page": 3
    },
    {
      "caption": "Figure 2: Base Model",
      "page": 5
    },
    {
      "caption": "Figure 2: Illustration of the detailed architecture for Knowledge-Spreader.",
      "page": 5
    },
    {
      "caption": "Figure 3: Pipeline of the temporal confirmed pseudo-label. The module is based",
      "page": 8
    },
    {
      "caption": "Figure 4: shows the perfor-",
      "page": 10
    },
    {
      "caption": "Figure 4: (d), (e), and (f) shows the proposed model achieves signifi-",
      "page": 10
    },
    {
      "caption": "Figure 4: (a), (b), and",
      "page": 10
    },
    {
      "caption": "Figure 4: Comparison with other advanced semi-supervised algorithms using differ-",
      "page": 11
    },
    {
      "caption": "Figure 5: demonstrates that ap-",
      "page": 12
    },
    {
      "caption": "Figure 5: Evaluation of the label sampling methods. The left top (a) indicates our",
      "page": 13
    },
    {
      "caption": "Figure 6: Pseudo labels discarded by TPL. The dotted and solid circles indicate the",
      "page": 14
    },
    {
      "caption": "Figure 7: Illustration of transformer-based model Sa and model Tb for learning",
      "page": 15
    },
    {
      "caption": "Figure 7: , are designed for modeling the relationships between AUs in both the",
      "page": 15
    },
    {
      "caption": "Figure 7: , contains the relevance of each atomic AU class and",
      "page": 16
    },
    {
      "caption": "Figure 4: in the original paper. In addition, due to",
      "page": 18
    },
    {
      "caption": "Figure 8: shows the F1 score curve with n changes. Overall, the perfor-",
      "page": 18
    },
    {
      "caption": "Figure 8: Effect of the video clip length n. (a) and (b) indicates the F1 score with",
      "page": 20
    },
    {
      "caption": "Figure 9: A sample sequence from our MME. 2D texture image, 3D mesh model, 3D",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "JAA [35]\nDSIN [4]\nLP [28]\nARL [34]\nSRERL [18]\nUGN [38]\nSEV [48]\nHMP-PS [40]\nFAUDT [12]",
          "Reference": "ECCV’18\nECCV’18\nCVPR’19\nAC’19\nAAAI’19\nAAAI’21\nCVPR’21\nCVPR’21\nCVPR’21",
          "BP4D": "60.0\n58.9\n61.0\n61.1\n62.1\n63.3\n63.9\n63.4\n64.2",
          "DISFA": "56.0\n53.6\n56.9\n58.7\n55.9\n60.0\n58.8\n61.0\n61.5"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "31.8 KS (1%)\n33.3 KS (2%)\n39.4 KS (5%)\n43.9 KS (10%)\n45.1 KS (15%)\n48.0 KS (50%)\n48.7 KS (60%)\n51.2 KS (100%) 64.5 62.8",
          "B\nD": "59.9\n49.4\n62.5\n52.8\n63.9\n56.9\n64.4\n58\n64.5\n58.8\n64.3\n61.4\n64.4 62.6"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: Comparison with state-of-the-art methods using F1 score in terms of",
      "data": [
        {
          "Model": "ARL\nSRERL\nUGN\nHMP-PS\nFAUDT",
          "Used labels AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg.": "45.8\n39.8\n55.1\n75.7\n77.2\n82.3\n86.6\n58.8\n47.6\n62.1\n47.4\n55.4\n46.9\n45.3\n55.6\n77.1\n78.4\n83.5\n87.6\n63.9\n52.2\n63.9\n47.1\n53.3\n54.2\n46.4\n56.8\n76.2\n76.7\n82.4\n86.1\n64.7\n51.2\n63.1\n48.5\n53.6\n53.1\n46.1\n56.0\n76.5\n76.9\n82.1\n86.4\n64.8\n51.5\n63.0\n49.9\n54.5\n51.7\n49.3\n61.0\n77.8\n79.5\n82.9\n86.3\n67.6\n51.9\n63.0\n43.7\n56.3"
        },
        {
          "Model": "Our KS\nOur KS",
          "Used labels AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg.": "58.7 50.3 62.0 79.5 75.4\n84.9\n87.1\n65.9\n45.5\n62.9\n48.3\n53.3\n55.1\n48.9\n56.2\n77.3 81.8\n83.3\n86.4\n62.6\n51.9\n61.3\n51.0"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 2: Comparison with state-of-the-art methods using F1 score in terms of",
      "data": [
        {
          "Model": "ARL\nSRERL\nUGN\nHMP-PS\nFAUDT",
          "Used labels": "100%\n100%\n100%\n100%\n100%",
          "AU1\nAU2\nAU4\nAU6\nAU9\nAU12\nAU25\nAU26": "43.9\n42.1\n63.6\n41.8\n40.0\n76.2\n95.2\n66.8\n45.7\n47.8\n59.6\n47.1\n45.6\n73.5\n84.3\n43.6\n43.3\n48.1\n63.4\n49.5\n48.2\n72.9\n90.8\n59.0\n38.0\n45.9\n65.2\n50.9\n50.8\n76.0\n93.3\n67.6\n46.1\n48.6\n72.8\n56.7\n50.0\n72.1\n90.8\n55.4",
          "Avg.": "58.7\n55.9\n60.0\n61.0\n61.5"
        },
        {
          "Model": "Our KS\nOur KS",
          "Used labels": "15%\n100%",
          "AU1\nAU2\nAU4\nAU6\nAU9\nAU12\nAU25\nAU26": "41.7\n53.5\n69.7\n41.3\n46.2\n72.0\n92.3\n54.0\n53.8\n59.9\n69.2\n54.2\n50.8\n75.8\n92.2\n46.8",
          "Avg.": "58.8\n62.8"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "FixMatch (1%)\nFixMatch (2%)\nFixMatch (5%)\nFixMatch (10%)\nFixMatch (15%)\nFixMatch (20%)\nFixMatch (50%)\nFixMatch (60%)\nFixMatch (70%)\nFixMatch (80%)\nFixMatch (90%)\nFixMatch (100%)",
          "BP4D": "49.9\n55.1\n59.2\n60.5\n62.1\n62\n62\n62.1\n61.9\n62.2\n61.9\n62.7",
          "DISFA": "35.6\n46.2\n52.7\n55\n57.7\n58.4\n57.9\n56\n57.8\n56.9\n57.5\n58.8",
          "MME": "41.6\n46.5\n52.6\n55.4\n55.6\n56.4\n58.3\n56.4\n57.2\n55.5\n55.3\n56.9"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Pseudo-label (1%)\nPseudo-label (2%)\nPseudo-label (5%)\nPseudo-label (10%)\nPseudo-label (15%)\nPseudo-label (20%)\nPseudo-label (50%)\nPseudo-label (60%)\nPseudo-label (70%)\nPseudo-label (80%)\nPseudo-label (90%)\nPseudo-label (100%)",
          "BP4D": "54.3\n57.8\n59.7\n60.7\n61.2\n62\n63.6\n62.7\n63.3\n62.4\n62.3\n62.7",
          "DISFA": "40.4\n50.8\n51.5\n56.8\n57.1\n58.5\n57.9\n56.7\n57.9\n58.3\n57.5\n58.8",
          "MME": "45.8\n47.5\n52.1\n54.2\n54.9\n55.2\n55.3\n55.3\n55.3\n56.6\n55.5\n56.9"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "TCL (1%)\nTCL (2%)\nTCL (5%)\nTCL (10%)\nTCL (15%)\nTCL (20%)\nTCL (50%)\nTCL (60%)\nTCL (70%)\nTCL (80%)\nTCL (90%)\nTCL (100%)",
          "BP4D": "55.6\n58.9\n60.5\n61.7\n62.3\n62.7\n63.2\n62.8\n63.0\n62.9\n62.7\n63.1",
          "DISFA": "42.3\n51.2\n53.6\n55.8\n56.7\n57.9\n59.2\n60.1\n59.6\n60.4\n58.3\n59.7",
          "MME": "43.3\n48.2\n53.4\n55.7\n56.2\n55.6\n57.6\n57.9\n57.9\n58.3\n57.8\n58.1"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Our KS (1%)\nOur KS (2%)\nOur KS (5%)\nOur KS (10%)\nOur KS (15%)\nOur KS (20%)\nOur KS (50%)\nOur KS (60%)\nOur KS (70%)\nOur KS (80%)\nOur KS (90%)\nOur KS (100%)",
          "BP4D": "59.9\n62.5\n63.9\n64.4\n64.5\n64.6\n64.3\n64.4\n64.5\n64.4\n64.4\n64.5",
          "DISFA": "64.9\n52.8\n56.9\n58\n58.8\n59.5\n61.4\n62.6\n61.9\n62\n61.9\n62.8",
          "MME": "51.2\n54.8\n57.6\n58.4\n58.7\n58.9\n59.5\n59.2\n58.8\n59.3\n59.4\n59.7"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: Comparison with state-of-the-art methods using F1 score in terms of",
      "data": [
        {
          "Model": "DSIN\nJAA\nLP\nARL\nSRERL\nUGN\nHMP-PS\nFAUDT",
          "Used labels AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg.": "51.7\n40.4\n56.0\n76.1\n73.5\n79.9\n85.4\n62.7\n37.3\n62.9\n38.8\n41.6\n47.2\n44.0\n54.9\n77.5\n74.6\n84.0\n86.9\n61.9\n43.6\n60.3\n42.7\n41.9\n43.4\n38.0\n54.2\n77.1\n76.7\n83.8\n87.2\n63.3\n45.3\n60.5\n48.1\n54.2\n45.8\n39.8\n55.1\n75.7\n77.2\n82.3\n86.6\n58.8\n47.6\n62.1\n47.4\n55.4\n46.9\n45.3\n55.6\n77.1\n78.4\n83.5\n87.6\n63.9\n52.2\n63.9\n47.1\n53.3\n54.2\n46.4\n56.8\n76.2\n76.7\n82.4\n86.1\n64.7\n51.2\n63.1\n48.5\n53.6\n53.1\n46.1\n56.0\n76.5\n76.9\n82.1\n86.4\n64.8\n51.5\n63.0\n49.9\n54.5\n51.7\n49.3\n61.0\n77.8\n79.5\n82.9\n86.3\n67.6\n51.9\n63.0\n43.7\n56.3"
        },
        {
          "Model": "Our KS\nOur KS",
          "Used labels AU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23 AU24 Avg.": "58.7 50.3 62.0 79.5 75.4\n84.9\n87.1\n65.9\n45.5\n62.9\n48.3\n53.3\n55.1\n48.9\n56.2\n77.3 81.8\n83.3\n86.4\n62.6\n51.9\n61.3\n51.0\n58.3"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 5: Comparison with state-of-the-art methods using F1 score in terms of",
      "data": [
        {
          "Model": "DSIN\nJAA\nLP\nARL\nSRERL\nUGN\nHMP-PS\nFAUDT",
          "Used labels": "100%\n100%\n100%\n100%\n100%\n100%\n100%\n100%",
          "AU1\nAU2\nAU4\nAU6\nAU9\nAU12\nAU25\nAU26": "42.4\n39.0\n68.4\n28.6\n46.8\n70.8\n90.4\n42.2\n43.7\n46.2\n56.0\n41.4\n44.7\n69.6\n88.3\n58.4\n29.9\n24.7\n72.7\n46.8\n49.6\n72.9\n93.8\n65.0\n43.9\n42.1\n63.6\n41.8\n40.0\n76.2\n95.2\n66.8\n45.7\n47.8\n59.6\n47.1\n45.6\n73.5\n84.3\n43.6\n43.3\n48.1\n63.4\n49.5\n48.2\n72.9\n90.8\n59.0\n38.0\n45.9\n65.2\n50.9\n50.8\n76.0\n93.3\n67.6\n46.1\n48.6\n72.8\n56.7\n50.0\n72.1\n90.8\n55.4",
          "Avg.": "53.6\n56.0\n56.9\n58.7\n55.9\n60.0\n61.0\n61.5"
        },
        {
          "Model": "Our KS\nOur KS",
          "Used labels": "15%\n100%",
          "AU1\nAU2\nAU4\nAU6\nAU9\nAU12\nAU25\nAU26": "41.7\n53.5\n69.7\n41.3\n46.2\n72.0\n92.3\n54.0\n53.8\n59.9\n69.2\n54.2\n50.8\n75.8\n92.2\n46.8",
          "Avg.": "58.8\n62.8"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 6: The stimulus tasks designed for the data collection.",
      "data": [
        {
          "1": "2",
          "Have a pleasant chat with the interviewer": "Watch a 3D face model of the participant",
          "Happiness": "Surprise"
        },
        {
          "1": "3",
          "Have a pleasant chat with the interviewer": "Watch an audio recording of 911 emergency call",
          "Happiness": "Sadness"
        },
        {
          "1": "4",
          "Have a pleasant chat with the interviewer": "Experience a sudden sound from a horn",
          "Happiness": "Startle or Surprise"
        },
        {
          "1": "5",
          "Have a pleasant chat with the interviewer": "React to a fake news",
          "Happiness": "Skeptical"
        },
        {
          "1": "6",
          "Have a pleasant chat with the interviewer": "Asked to sing an impromptu song",
          "Happiness": "Embarrassment"
        },
        {
          "1": "7",
          "Have a pleasant chat with the interviewer": "Experience physical\nfear of the threat in a dart game",
          "Happiness": "Fear or Nervous"
        },
        {
          "1": "8",
          "Have a pleasant chat with the interviewer": "Experience the cold feeling by submerging\nhands into a bucket with ice water",
          "Happiness": "Pain"
        },
        {
          "1": "9",
          "Have a pleasant chat with the interviewer": "React to the blame from the interviewer",
          "Happiness": "Offended or Unpleasant"
        },
        {
          "1": "10",
          "Have a pleasant chat with the interviewer": "Experience a bad smell\nfrom decaying food",
          "Happiness": "Disgust"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "High-performance large-scale image recognition without normalization",
      "authors": [
        "A Brock",
        "S De",
        "S Smith",
        "K Simonyan"
      ],
      "year": "2021",
      "venue": "High-performance large-scale image recognition without normalization"
    },
    {
      "citation_id": "2",
      "title": "Unsupervised and semisupervised domain adaptation for action recognition from drones",
      "authors": [
        "J Choi",
        "G Sharma",
        "M Chandraker",
        "J Huang"
      ],
      "year": "2020",
      "venue": "Unsupervised and semisupervised domain adaptation for action recognition from drones"
    },
    {
      "citation_id": "3",
      "title": "Learning spatial and temporal cues for multi-label facial action unit detection",
      "authors": [
        "W Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "Learning spatial and temporal cues for multi-label facial action unit detection"
    },
    {
      "citation_id": "4",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu"
      ],
      "year": "2018",
      "venue": "Proceedings of ECCV"
    },
    {
      "citation_id": "5",
      "title": "Knowledge augmented deep neural networks for joint facial expression and action unit recognition",
      "authors": [
        "Z Cui",
        "T Song",
        "Y Wang",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "NIPS"
    },
    {
      "citation_id": "6",
      "title": "Label error correction and generation through label relationships",
      "authors": [
        "Z Cui",
        "Y Zhang",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2010",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "8",
      "title": "Online knowledge distillation via collaborative learning",
      "authors": [
        "Q Guo",
        "X Wang",
        "Y Wu",
        "Z Yu",
        "D Liang",
        "X Hu",
        "P Luo"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "11",
      "title": "Gpipe: Efficient training of giant neural networks using pipeline parallelism",
      "authors": [
        "Y Huang"
      ],
      "year": "2019",
      "venue": "Gpipe: Efficient training of giant neural networks using pipeline parallelism"
    },
    {
      "citation_id": "12",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "Videossl: Semi-supervised learning for video classification",
      "authors": [
        "L Jing",
        "T Parag",
        "Z Wu",
        "Y Tian",
        "H Wang"
      ],
      "year": "2021",
      "venue": "Videossl: Semi-supervised learning for video classification"
    },
    {
      "citation_id": "14",
      "title": "Temporal ensembling for semi-supervised learning",
      "authors": [
        "S Laine",
        "T Aila"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks",
      "authors": [
        "D Lee"
      ],
      "year": "2013",
      "venue": "ICML 2013 Workshop : Challenges in Representation Learning (WREPL)"
    },
    {
      "citation_id": "16",
      "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "authors": [
        "J Lei",
        "L Li",
        "L Zhou",
        "Z Gan",
        "T Berg",
        "M Bansal",
        "J Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "17",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "19",
      "title": "Action unit detection with region adaptation, multilabeling learning and optimal temporal fusing",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "20",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Your \"attention\" deserves attention: A self-diversified multi-channel attention for facial action analysis",
      "authors": [
        "X Li",
        "Z Li",
        "H Yang",
        "G Zhao",
        "L Yin"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021"
    },
    {
      "citation_id": "22",
      "title": "Self-supervised representation learning from videos for facial action unit detection",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "23",
      "title": "Integrating Semantic and Temporal Relationships in Facial Action Unit Detection",
      "authors": [
        "Z Li",
        "X Deng",
        "X Li",
        "L Yin"
      ],
      "year": "2021",
      "venue": "Integrating Semantic and Temporal Relationships in Facial Action Unit Detection"
    },
    {
      "citation_id": "24",
      "title": "Integrating Semantic and Temporal Relationships in Facial Action Unit Detection",
      "authors": [
        "Z Li",
        "X Deng",
        "X Li",
        "L Yin"
      ],
      "year": "2021",
      "venue": "Integrating Semantic and Temporal Relationships in Facial Action Unit Detection"
    },
    {
      "citation_id": "25",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Region and temporal dependency fusion for multi-label action unit detection",
      "authors": [
        "C Mei",
        "F Jiang",
        "R Shen",
        "Q Hu"
      ],
      "year": "2018",
      "venue": "24th International Conference on Pattern Recognition (ICPR"
    },
    {
      "citation_id": "27",
      "title": "Multi-label co-regularization for semisupervised facial action unit recognition",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Multi-label co-regularization for semisupervised facial action unit recognition"
    },
    {
      "citation_id": "28",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "29",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "30",
      "title": "Learning dynamic network using a reuse gate function in semi-supervised video object segmentation",
      "authors": [
        "H Park",
        "J Yoo",
        "S Jeong",
        "G Venkatesh",
        "N Kwak"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "31",
      "title": "Weakly supervised facial action unit recognition through adversarial training",
      "authors": [
        "G Peng",
        "S Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "32",
      "title": "Dual semi-supervised learning for facial action unit recognition",
      "authors": [
        "G Peng",
        "S Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2014",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing p"
    },
    {
      "citation_id": "35",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao"
      ],
      "year": "2018",
      "venue": "Proceedings of ECCV"
    },
    {
      "citation_id": "36",
      "title": "Semi-supervised action recognition with temporal contrastive learning",
      "authors": [
        "A Singh",
        "O Chakraborty",
        "A Varshney",
        "R Panda",
        "R Feris",
        "K Saenko",
        "A Das"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "37",
      "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
      "authors": [
        "K Sohn",
        "D Berthelot",
        "C Li",
        "Z Zhang",
        "N Carlini",
        "E Cubuk",
        "A Kurakin",
        "H Zhang",
        "C Raffel"
      ],
      "year": "2020",
      "venue": "NIPS"
    },
    {
      "citation_id": "38",
      "title": "Uncertain graph neural networks for facial action unit detection",
      "authors": [
        "T Song",
        "L Chen",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "39",
      "title": "Hybrid message passing with performancedriven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "40",
      "title": "Hybrid message passing with performancedriven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "41",
      "title": "Exploiting sparsity and cooccurrence structure for action unit recognition",
      "authors": [
        "Y Song",
        "D Mcduff",
        "D Vasisht",
        "A Kapoor"
      ],
      "year": "2015",
      "venue": "11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG"
    },
    {
      "citation_id": "42",
      "title": "Joint action unit localisation and intensity estimation through heatmap regression",
      "authors": [
        "E Sánchez-Lozano",
        "G Tzimiropoulos",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "Joint action unit localisation and intensity estimation through heatmap regression"
    },
    {
      "citation_id": "43",
      "title": "Piap-df: Pixel-interested and anti personspecific facial action unit detection net with discrete feedback learning",
      "authors": [
        "Y Tang",
        "W Zeng",
        "D Zhao",
        "H Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV"
    },
    {
      "citation_id": "44",
      "title": "Self-supervised learning for semi-supervised temporal action proposal",
      "authors": [
        "X Wang",
        "S Zhang",
        "Z Qing",
        "Y Shao",
        "C Gao",
        "N Sang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "45",
      "title": "Self-supervised learning for semi-supervised temporal action proposal",
      "authors": [
        "X Wang",
        "S Zhang",
        "Z Qing",
        "Y Shao",
        "C Gao",
        "N Sang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "46",
      "title": "Deep facial action unit recognition from partially labeled data",
      "authors": [
        "S Wu",
        "S Wang",
        "B Pan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV"
    },
    {
      "citation_id": "47",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "S Xie"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on CVPR"
    },
    {
      "citation_id": "48",
      "title": "Exploiting semantic embedding and visual feature for facial action unit detection",
      "authors": [
        "H Yang",
        "L Yin",
        "Y Zhou",
        "J Gu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR"
    },
    {
      "citation_id": "49",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "50",
      "title": "Joint representation and estimator learning for facial action unit intensity estimation",
      "authors": [
        "Y Zhang",
        "B Wu",
        "W Dong",
        "Z Li",
        "W Liu",
        "B Hu",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "51",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang",
        "J Cohn",
        "Q Ji",
        "L Yin"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "52",
      "title": "Region of interest based graph convolution: A heatmap regression approach for action unit detection",
      "authors": [
        "Z Zhang",
        "T Wang",
        "L Yin"
      ],
      "year": "2020",
      "venue": "Region of interest based graph convolution: A heatmap regression approach for action unit detection"
    },
    {
      "citation_id": "53",
      "title": "Joint patch and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR"
    }
  ]
}