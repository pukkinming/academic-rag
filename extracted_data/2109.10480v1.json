{
  "paper_id": "2109.10480v1",
  "title": "Dialoguebert: A Self-Supervised Learning Based Dialogue Pre-Training Encoder",
  "published": "2021-09-22T01:41:28Z",
  "authors": [
    "Zhenyu Zhang",
    "Tao Guo",
    "Meng Chen"
  ],
  "keywords": [
    "Dialogue Pre-training Model",
    "Dialogue Representation",
    "Intent Recognition",
    "Emotion Recognition",
    "Named Entity Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the rapid development of artificial intelligence, conversational bots have became prevalent in mainstream E-commerce platforms, which can provide convenient customer service timely. To satisfy the user, the conversational bots need to understand the user's intention, detect the user's emotion, and extract the key entities from the conversational utterances. However, understanding dialogues is regarded as a very challenging task. Different from common language understanding, utterances in dialogues appear alternately from different roles and are usually organized as hierarchical structures. To facilitate the understanding of dialogues, in this paper, we propose a novel contextual dialogue encoder (i.e. DialogueBERT) based on the popular pre-trained language model BERT. Five selfsupervised learning pre-training tasks are devised for learning the particularity of dialouge utterances. Four different input embeddings are integrated to catch the relationship between utterances, including turn embedding, role embedding, token embedding and position embedding. DialogueBERT was pre-trained with 70 million dialogues in real scenario, and then fine-tuned in three different downstream dialogue understanding tasks. Experimental results show that DialogueBERT achieves exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition, which outperforms several strong baselines by a large margin. \n CCS CONCEPTS â€¢ Computing methodologies â†’ Discourse, dialogue and pragmatics; Natural language processing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Nowadays, conversational bots are widely applied in mainstream online shopping platforms such as Jingdong, Taobao, and Amazon. In the Chinese leading online shopping platform Jingdong, over 70% of the customers are served by the smart shopping assistant AlphaSales 1  for E-commerce customer service, covering user's various queries such as product questions, shipping policies, and promotion activities etc. Language understanding in dialogues is a challenging problem. To provide good user experience, the conversational bot needs to understand the intent of users in dialogues, recognize the entities to fill slots in conversations, and detect the user's emotion to generate empathetic responses. Unlike regular language understanding, conversational utterances appear alternately from different parties and are usually organized as hierarchical structures. The underlying difference of linguistic patterns between general text and dialogue utterances makes existing language understanding approaches less satisfying.\n\nTo understand the dialogue utterances accurately, lots of efforts have been made by devising various dialogue encoders and remarkable progresses have been achieved recently. One line of researches leverage the memory-networks  [7, 14, 16, 19]  to encode the dialogues for downstream tasks such as intent recognition, emotion recognition, etc. With the great success of pre-trained language models such as BERT  [3, 4] , RoBERTa  [9] , etc., some researchers transformed the dialogue modeling problem into a Machine Reading Comprehension (MRC) problem  [1, 2, 12] . Recently, some dialoguespecific pre-training tasks are also proposed and obtain state-of-theart (SOTA) results in the dialogue relevant downstream tasks. Qu et al.  [10]  incorporate history utterance embeddings into BERT model to find answer for user's question. The PT-CoDE  [6]  model leverages a hierarchical neural network to encode the conversation and pre-trains the model with conversation completion tasks. The TOD-BERT  [18]  pre-trains the BERT model from scratch by using both masked language modeling and response contrastive loss. DialoGPT  [21]  generates conversation responses by pre-training GPT2  [11]  with huge amount dialogue corpus, which is mainly dedicated to response generation task instead of language understanding. Those pre-trained language models in open-domain and dialogue-specific domain all follow the self-supervised learning (SSL)  [8]  paradigm, where the input text is supposed to be recovered from partially observed input context.\n\nAlthough the previous researches have proposed several SSL methods to pre-train the dialogue encoder, the dialogue-specific SSL is far from well-explored. There is still lack of enough researches for exploration of new SSL methods and comparison between different SSL pre-training tasks for dialogue encoding. To address this issue, in this paper, we propose a novel dialogue encoder (denoted as Di-alogueBERT) based on the prevalent pre-trained language model BERT  [4] . To catch the particularity of dialogue utterances, we devise five self-supervised learning based pre-training tasks and train the DialogueBERT with large-scale conversation corpus. Inspired by  [22] , a convolutional pooler based on convolutional neural network (CNN) and 2D pooling is designed to extract the dialogue feature representations. To learn the relationship between different utterances in the dialogue, four different input embeddings are integrated, including token embedding, position embedding, role embedding and turn embedding. DialogueBERT is pre-trained with over 70 million of dialogues, which are collected from real conversations between users and customer service staffs in E-commerce scenario. To verify the effectiveness of our pre-trained dialogue encoder, extensive experiments were conducted by fune-tuning DialogueBERT on three downstream dialogue understanding tasks, including intent recognition, emotion recognition, and named entity recognition. Experimental results show that the proposed method outperforms several strong dialogue encoding approaches and achieves 88.63% accuracy score for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition. Ablation study was also conducted to figure out the contribution of each SSL pre-training task, which can further inspire the future researches to build better dialogue encoder.\n\nSpecifically, our contributions include:\n\nâ€¢ We propose a novel pre-trained dialogue encoder based on BERT by devising five self-supervised learning pre-training tasks, including masked language modeling, masked utterance prediction, utterance replacement, turn swapping, and response selection. The model was pre-trained with more than 70 million of dialogues. â€¢ We verify the effectiveness of DialogueBERT on three representative downstream dialogue tasks, including intent recognition, emotion recognition, and named entity recognition.\n\nExperimental results show the superiority and competitiveness of our proposed model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "In this Section, we will introduce the model input, model structure and self-supervised learning based model pre-training. Figure  1  demonstrates the overall architecture of our proposed model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Input",
      "text": "In order to pre-train the model, we firstly collect a large-scale dialogue corpus from real conversations between customers and customer service staffs. After that, we desensitized and anonymized the private information based on very detailed rules. Finally, We collected over 70 million conversations with 8.59 utterances on average for each conversation. For each conversation utterance, we prepend one of the two special tags, i.e. \"<Q>\" (customer's query) and \"<A>\" (customer service staff's response). Then we concatenate the utterances to form the input text as \"<Q>xxxx<A>xxx<Q>xxxx...\". We trained a BPE sentencepiece tokenizer  [13]  with 5 million randomly sampled conversations. The vocabulary size is set to 50,000.\n\nThen we split the conversation input text into tokens. To catch the relationship between utterances, we combine the token, role and turn information for model input. As shown in Figure  1 , tokens (denoted as \"ğ‘‡ 1 ,ğ‘‡ 2 \", etc.) in each utterance are assigned with the corresponding turn information (indexed from \"1\" for the first utterance) and role information (\"q\" represents question and \"a\" denotes response). We restrict the maximum utterance length to 15 tokens and maximum conversation input text length to 128 tokens, where extra tokens are clipped.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Structure",
      "text": "The proposed dialogue encoder is illustrated in Figure  1  which consists of four input embeddings, the Transformer Encoder, the Model Output and five SSL pre-training headers. The embedding layer consists of token embedding, position embedding, role embedding and turn embedding that can map the token, token position, role information and turn information of input sequence into dense vectors (i.e. ğ‘£ğ‘’ğ‘ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› , ğ‘£ğ‘’ğ‘ ğ‘ğ‘œğ‘  , ğ‘£ğ‘’ğ‘ ğ‘Ÿğ‘œğ‘™ğ‘’ and ğ‘£ğ‘’ğ‘ ğ‘¡ğ‘¢ğ‘Ÿğ‘› respectively). The embedding layer then sums up the four embeddings together and apply a layer normalization to obtain the final input embedding ğ‘£ğ‘’ğ‘ ğ‘’ğ‘šğ‘ .\n\nThe Transformer Encoder is same as the BERT that contains 12 Transformer blocks  [15] . It takes ğ‘£ğ‘’ğ‘ ğ‘’ğ‘šğ‘ of each token as input and outputs ğ‘ contextual vectors (denoted as ğ‘¢ 1 , ğ‘¢ 2 , ...ğ‘¢ ğ‘ and ğ‘¢ ğ‘– âˆˆ ğ‘… ğ‘‘ , 1 â‰¤ ğ‘– â‰¤ ğ‘ ). Besides the contextual vectors, the model output also contains a dialogue representation feature vector. We use a CNN pooler to get the dialogue representation. There are six 1-D convolution kernels (filter sizes are 5, 7, 9, 11, 15, 20, where the output channel size is the same as BERT, i.e. each kernel âˆˆ ğ‘… ğ¾Ã—ğ‘‘ and ğ‘‘ is the dimension of 768) in the CNN. Inspired by  [22] , we apply a 2D-pooling on the CNN output to get the dialogue encoder pooling feature vector (ğ‘£ğ‘’ğ‘ ğ‘‘ğ‘–ğ‘ğ‘” âˆˆ ğ‘… ğ‘‘ ). While the self-attention mechanism of BERT can capture the global semantics of input sequence, the convolution kernels applied along the input sequence dimension can capture the local ngram-level lexical and semantic information. The five SSL headers are used to pre-train the DialogueBERT with the huge conversation corpus, which will be described in detail in Section 2.3.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ssl-Based Model Pre-Training",
      "text": "Considering the dialogue involves the token/word level feature, utterance level feature, and conversation order feature, etc., we design five different SSL pre-training tasks. As shown in Figure  1 , the proposed SSL pre-training tasks are as follows:\n\nâ€¢ MUM is a masked utterance modeling SSL task, where we randomly mask one utterance in a conversation with several special tokens, i.e. \"[MASK]\" (replace the ğ‘š tokens in the utterance), then the model is required to generate the tokens of the masked utterance. The SSL method of the work in  [6]  selects the utterance from a candidate set (we denote this as MSM) while we directly predict the utterance in token level. We argue that because the candidate set is randomly sampled in  [6] , it is not a good choice for a contrastive SSL task due to poor contrastive samples  [8] . We feed the masked token vectors (ğ‘¢ ğ‘šğ‘ğ‘ ğ‘˜ 1 , ğ‘¢ ğ‘šğ‘ğ‘ ğ‘˜ 2 , ...ğ‘¢ ğ‘šğ‘ğ‘ ğ‘˜ ğ‘š ) to the masked language header to predict the original tokens by following the approach in BERT  [4] . â€¢ MLM is a masked language modeling SSL task, where we follows the setting in  [3] . This SSL task aims to learn the language modeling structure of the conversation text. â€¢ ReplDisc randomly (with probability of 0.5) replaces one utterance in a conversation with a randomly selected utterance from another conversation in the same training batch, and then discriminates whether the new conversation is the replaced one or the original one. This SSL task is devised to learn the contextual feature of the conversation. â€¢ TurnDisc randomly swaps two utterances in a conversation and then discriminates whether the new conversation is swapped, where the turn in new conversation is still indexed in ascending order (we do not swap the turn information of the two utterances). This SSL task is designed to learn the utterance order feature of conversation. â€¢ ResSel is the same as the response constrastive loss (RCL) in  [18] , which can help learning a better representation for the [CLS] token, and capturing underlying dialogue sequential order, structure information, and response similarity. Differently, we only use the model to select the last utterance of a conversation from a set of candidate responses that are from the training batch. For ReplDisc and TurnDisc, we use the logistic regression to predict the results with ğ‘£ğ‘’ğ‘ ğ‘‘ğ‘–ğ‘ğ‘” (Section 2.2) as input feature vector. The cross entropy is applied to compute loss for each SSL task. Then we sum up all the losses of the five SSL tasks in pre-training and apply LAMB optimizer  [20]  with maximum learning rate as 1e-4, warm-up steps as 10K and batch size as 256. We trained the DialogueBERT for 10 epochs with all the 70 million conversation corpus. For the downstream NLP tasks, we follow the settings of common language modeling task  [17]",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments 3.1 Datasets",
      "text": "To evaluate the DialogueBERT in downstream conversation understanding tasks, we experiment on three representative tasks, including intent recognition, emotion recognition, and named entity recognition. All experiments were conducted on our in-house datasets  3  . Table  1  shows the statistics of the datasets, where the term Å‚IntR\", Å‚NER\" and Å‚EmoR\" refer for intent recognition, named entity recognition and emotion recognition respectively. We split the data into 95% for training to fine-tune the model and 5% for testing randomly. For the training set, we hold out 1000 samples as validation set to select the best model in fine-tuning. There are 102 intents, 5 emotions (angry, satisfied, sad, curious, neural) and 28 kinds of named entities in the corresponding datasets.\n\nFor intent recognition and emotion recognition, we feed the encoder pooling feature vector (ğ‘£ğ‘’ğ‘ ğ‘‘ğ‘–ğ‘ğ‘” âˆˆ ğ‘… ğ‘‘ , Section 2.2) from CNN pooler (Figure  1 ) into a following classification head ğ‘Š âˆˆ ğ‘… ğ‘‘Ã—ğ¶ (ğ¶ is the number of intents or emotions, i.e. C is 102 and 5 for IntR and EmoR respectively) for prediction. We get the predictions via ğ‘Š ğ‘£ğ‘’ğ‘ ğ‘‘ğ‘–ğ‘ğ‘” âˆˆ ğ‘… ğ¶ and calculate the classification loss via Equation 2 as follows:\n\nFor the named entity recognition (NER), we use the BIO tagging schema to create the annotated corpus, where ğµ represents the beginning of a named entity, ğ¼ represents the inside tokens of a named entity and ğ‘‚ represents the non-entity tokens. We use the ğ‘ output vectors of input tokens, denoted as ğ‘ˆ âˆˆ ğ‘… ğ‘ Ã—ğ‘‘ (ğ‘ˆ = {ğ‘¢ 1 , ğ‘¢ 2 , ..., ğ‘¢ ğ‘ }), and then apply a standard token classification head (ğ‘Š ğ‘›ğ‘’ğ‘Ÿ âˆˆ ğ‘… ğ‘‘Ã—ğ‘ƒ ) to get the token-level classification outputğ‘Š ğ‘›ğ‘’ğ‘Ÿ ğ‘ˆ âˆˆ ğ‘… ğ‘ Ã—ğ‘ƒ , where ğ‘ƒ is the number of tagging labels (i.e. 28 ğµ-ğ‘›ğ‘’ tags, 28 ğ¼ -ğ‘›ğ‘’ tags and 1 ğ‘‚ tag, totally 28 Ã— 2 + 1). We compute the NER loss via Equation 3 during fine-tuning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiment Setup",
      "text": "We evaluate the effectiveness of DialogueBERT in three tasks: intent recognition (IntR), named enity recognition (NER) and emotion recognition (EmoR accuracy as evaluation metric. For NER, the Macro-F1 score is reported as evaluation metric. We compared our method with the following four strong baselines:\n\nâ€¢ DMN  [7] : the dynamic memory network formulates dialogue understanding as QA tasks, which updates the memory iteratively with a dynamic gate via attention mechanism. â€¢ KVNet  [19] : it applied key-value memory neural network as a semantic parsing module to approach the open-domain KB-QA task, which selects relevant history utterances to generate a feature vector. â€¢ PT-CoDE  [6]  is a hierarchical dialogue encoder and is pretrained with a conversation completion task (CoCo). â€¢ TOD-BERT  [18]  learns the dialogue structure and utterance order via both masked language modeling loss and response contrastive loss. As to the implementation details, we leveraged the works of Huggingface 2  [17]  and texar 4  [5]  to implement those models. We set the model dimension as 768 for DialogueBERT and all the baselines.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "As shown in Table  2 , DialogueBERT outperforms the baselines in all the three downstream dialogue understanding tasks. Our method gets 88.63% accuracy score for intent recognition, 97.04% Macro-F1 score for named entity recognition and 94.25% accuracy for emotion recognition, which outperforms the TOD-BERT by 0.72%, 0.19% and 1.42% respectively (statistically significant difference with p < 0.05). It's also observed that all the SSL based methods (TOD-BERT, PT-CoDE, DialogueBERT) outperform the memory networks based methods significantly, which demonstrate the superiority of SSL methods in dialogue encoding.\n\nThe ğ·ğ‘–ğ‘ğ‘™ğ‘œğ‘”ğ‘¢ğ‘’ğµğ¸ğ‘…ğ‘‡ ğ¶ğ¿ğ‘† in Table  2  denotes that we prepend a special classification token \"[CLS]\" token to the conversation input text and use the transformer encoder output ğ‘¢ ğ¶ğ¿ğ‘† as pooling vector rather than applying our CNN pooler (Section 2.2). The \"CLS\" pooling strategy is previously used by the open-domain BERT  [4]  and dialogue-specific encoder TOD-BERT  [18]  (TOD-BERT outperforms the original open-domain BERT in dialoguespecific tasks). Compared with the DialogueBERT, the performance of ğ·ğ‘–ğ‘ğ‘™ğ‘œğ‘”ğ‘¢ğ‘’ğµğ¸ğ‘…ğ‘‡ ğ¶ğ¿ğ‘† drops 0.31%, 0.04% and 0.09% for intent recognition, named entity recognition and emotion recognition respectively while it still outperforms the other baselines. It indicates that, on the one hand, the performance gain of DialogueBERT is mainly from the five SSL tasks. Meanwhile, it also demonstrates the contribution of our CNN pooler.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "Table  3  shows the experimental results for each SSL pre-training task. We found that the model is hard to converge without MLM, which results in a poor pre-trained dialogue encoder that could barely fit any downstream task. Therefore, we report the model performance for each SSL task together with MLM. Because the research in  [18]  has proved that MLM and ResSel based method outperforms the pure MLM based pre-training method, here we skip repeating the similar experiments for space limitation. We found that the MUM is the most effective SSL pre-training task, which can learn the dialogue structure to facilitate both the token level and dialogue level pre-training. The ResSel achieves similar performance with MUM but better score than MSM, which demonstrates that the last utterance in the conversation is very important for building the dialogue encoder. Please note that the research in  [6]  applies the MSM (conversation completion; refer to Section 2.2) for pre-training, while our experiments show that the MUM SSL method is better for dialogue encoder. The MUM outperforms the MSM by 0.29%, 0.36% and 0.91% in the three tasks (i.e. IntR, NER, EmoR) respectively. We argue that because there are usually within 15 tokens for each utterance, it is easier for the model to generate the utterance in token level, where the model can avoid suffering from poor constrastive candidate samples in  [6] . Overall speaking, each of the proposed SSL task contributes to the final gain of DialogueBERT to some degree. Considering the different contributions for each task, we will try to combine the five losses with different weights in the future.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we propose a novel dialogue pre-training encoder, i.e. DialogueBERT. Five different self-supervised learning based pre-training tasks are devised to catch the particularity of conversation utterances and enhance the dialogue representations. Then we verify the effectiveness of DialogueBERT on three representative downstream dialogue understanding tasks, including intent recognition, named entity recognition and emotion recognition. We also analyze the contribution of each SSL task independently. In the future, we plan to design query rewriting task in the pre-training stage to enhance the context modeling.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: demonstrates the overall architecture of our proposed model.",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview architecture of DialogueBERT.",
      "page": 2
    },
    {
      "caption": "Figure 1: which con-",
      "page": 2
    },
    {
      "caption": "Figure 1: ) into a following classification head ğ‘Šâˆˆ",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zhenyu Zhang\nTao Guo": "JD AI, Chengdu, China\nXiaoduo AI, Chengdu, China",
          "Meng Chen": "JD AI, Beijing, China"
        },
        {
          "Zhenyu Zhang\nTao Guo": "zhangzhenyu47@jd.com\nguotao@xiaoduotech.com",
          "Meng Chen": "chenmeng20@jd.com"
        },
        {
          "Zhenyu Zhang\nTao Guo": "ABSTRACT",
          "Meng Chen": "1\nINTRODUCTION"
        },
        {
          "Zhenyu Zhang\nTao Guo": "With the rapid development of artificial intelligence, conversational",
          "Meng Chen": "Nowadays, conversational bots are widely applied in mainstream"
        },
        {
          "Zhenyu Zhang\nTao Guo": "bots have became prevalent in mainstream E-commerce platforms,",
          "Meng Chen": "online shopping platforms such as Jingdong, Taobao, and Ama-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "which can provide convenient customer service timely. To satisfy",
          "Meng Chen": "zon.\nIn the Chinese leading online shopping platform Jingdong,"
        },
        {
          "Zhenyu Zhang\nTao Guo": "the user, the conversational bots need to understand the userâ€™s in-",
          "Meng Chen": "over 70% of the customers are served by the smart shopping as-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "tention, detect the userâ€™s emotion, and extract the key entities from",
          "Meng Chen": "sistant AlphaSales1\nfor E-commerce customer service, covering"
        },
        {
          "Zhenyu Zhang\nTao Guo": "the conversational utterances. However, understanding dialogues",
          "Meng Chen": "userâ€™s various queries such as product questions, shipping policies,"
        },
        {
          "Zhenyu Zhang\nTao Guo": "is regarded as a very challenging task. Different from common lan-",
          "Meng Chen": "and promotion activities etc. Language understanding in dialogues"
        },
        {
          "Zhenyu Zhang\nTao Guo": "guage understanding, utterances in dialogues appear alternately",
          "Meng Chen": "is a challenging problem. To provide good user experience,\nthe"
        },
        {
          "Zhenyu Zhang\nTao Guo": "from different roles and are usually organized as hierarchical struc-",
          "Meng Chen": "conversational bot needs to understand the intent of users in di-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "tures. To facilitate the understanding of dialogues, in this paper, we",
          "Meng Chen": "alogues, recognize the entities to fill slots in conversations, and"
        },
        {
          "Zhenyu Zhang\nTao Guo": "propose a novel contextual dialogue encoder (i.e. DialogueBERT)",
          "Meng Chen": "detect the userâ€™s emotion to generate empathetic responses. Unlike"
        },
        {
          "Zhenyu Zhang\nTao Guo": "based on the popular pre-trained language model BERT. Five self-",
          "Meng Chen": "regular language understanding, conversational utterances appear"
        },
        {
          "Zhenyu Zhang\nTao Guo": "supervised learning pre-training tasks are devised for learning the",
          "Meng Chen": "alternately from different parties and are usually organized as hier-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "particularity of dialouge utterances. Four different input embed-",
          "Meng Chen": "archical structures. The underlying difference of linguistic patterns"
        },
        {
          "Zhenyu Zhang\nTao Guo": "dings are integrated to catch the relationship between utterances,",
          "Meng Chen": "between general text and dialogue utterances makes existing lan-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "including turn embedding, role embedding, token embedding and",
          "Meng Chen": "guage understanding approaches less satisfying."
        },
        {
          "Zhenyu Zhang\nTao Guo": "position embedding. DialogueBERT was pre-trained with 70 million",
          "Meng Chen": "To understand the dialogue utterances accurately, lots of efforts"
        },
        {
          "Zhenyu Zhang\nTao Guo": "dialogues in real scenario, and then fine-tuned in three different",
          "Meng Chen": "have been made by devising various dialogue encoders and remark-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "downstream dialogue understanding tasks. Experimental results",
          "Meng Chen": "able progresses have been achieved recently. One line of researches"
        },
        {
          "Zhenyu Zhang\nTao Guo": "show that DialogueBERT achieves exciting results with 88.63%",
          "Meng Chen": "leverage the memory-networks [7, 14, 16, 19] to encode the dia-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "accuracy for intent recognition, 94.25% accuracy for emotion recog-",
          "Meng Chen": "logues for downstream tasks such as intent recognition, emotion"
        },
        {
          "Zhenyu Zhang\nTao Guo": "nition and 97.04% F1 score for named entity recognition, which",
          "Meng Chen": "recognition, etc. With the great success of pre-trained language"
        },
        {
          "Zhenyu Zhang\nTao Guo": "outperforms several strong baselines by a large margin.",
          "Meng Chen": "models such as BERT [3, 4], RoBERTa [9], etc., some researchers"
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "transformed the dialogue modeling problem into a Machine Reading"
        },
        {
          "Zhenyu Zhang\nTao Guo": "CCS CONCEPTS",
          "Meng Chen": "Comprehension (MRC) problem [1, 2, 12]. Recently, some dialogue-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "specific pre-training tasks are also proposed and obtain state-of-the-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "â€¢ Computing methodologies â†’ Discourse, dialogue and prag-",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "art (SOTA) results in the dialogue relevant downstream tasks. Qu et"
        },
        {
          "Zhenyu Zhang\nTao Guo": "matics; Natural language processing.",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "al. [10] incorporate history utterance embeddings into BERT model"
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "to find answer for userâ€™s question. The PT-CoDE [6] model lever-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "KEYWORDS",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "ages a hierarchical neural network to encode the conversation and"
        },
        {
          "Zhenyu Zhang\nTao Guo": "Dialogue Pre-training Model, Dialogue Representation, Intent Recog-",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "pre-trains the model with conversation completion tasks. The TOD-"
        },
        {
          "Zhenyu Zhang\nTao Guo": "nition, Emotion Recognition, Named Entity Recognition",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "BERT [18] pre-trains the BERT model from scratch by using both"
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "masked language modeling and response contrastive loss. DialoGPT"
        },
        {
          "Zhenyu Zhang\nTao Guo": "ACM Reference Format:",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "[21] generates conversation responses by pre-training GPT2 [11]"
        },
        {
          "Zhenyu Zhang\nTao Guo": "Zhenyu Zhang, Tao Guo, and Meng Chen. 2021. DialogueBERT: A Self-",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "Supervised Learning based Dialogue Pre-training Encoder . In Proceedings",
          "Meng Chen": "with huge amount dialogue corpus, which is mainly dedicated to"
        },
        {
          "Zhenyu Zhang\nTao Guo": "of the 30th ACM International Conference on Information and Knowledge",
          "Meng Chen": "response generation task instead of language understanding. Those"
        },
        {
          "Zhenyu Zhang\nTao Guo": "Management (CIKM â€™21), November 1Å›5, 2021, Virtual Event, QLD, Australia.",
          "Meng Chen": "pre-trained language models in open-domain and dialogue-specific"
        },
        {
          "Zhenyu Zhang\nTao Guo": "ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.3482085",
          "Meng Chen": "domain all follow the self-supervised learning (SSL) [8] paradigm,"
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "where the input text is supposed to be recovered from partially"
        },
        {
          "Zhenyu Zhang\nTao Guo": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Meng Chen": "observed input context."
        },
        {
          "Zhenyu Zhang\nTao Guo": "classroom use is granted without fee provided that copies are not made or distributed",
          "Meng Chen": "Although the previous researches have proposed several SSL"
        },
        {
          "Zhenyu Zhang\nTao Guo": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "methods to pre-train the dialogue encoder, the dialogue-specific SSL"
        },
        {
          "Zhenyu Zhang\nTao Guo": "on the first page. Copyrights for components of this work owned by others than ACM",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,",
          "Meng Chen": "is far from well-explored. There is still lack of enough researches for"
        },
        {
          "Zhenyu Zhang\nTao Guo": "to post on servers or to redistribute to lists, requires prior specific permission and/or a",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "exploration of new SSL methods and comparison between different"
        },
        {
          "Zhenyu Zhang\nTao Guo": "fee. Request permissions from permissions@acm.org.",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "",
          "Meng Chen": "SSL pre-training tasks for dialogue encoding. To address this issue,"
        },
        {
          "Zhenyu Zhang\nTao Guo": "CIKM â€™21, November 1Å›5, 2021, Virtual Event, QLD, Australia",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "Â© 2021 Association for Computing Machinery.",
          "Meng Chen": ""
        },
        {
          "Zhenyu Zhang\nTao Guo": "ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00",
          "Meng Chen": "1AlphaSales is an intelligent conversational bot for answering customersâ€™ inquiries"
        },
        {
          "Zhenyu Zhang\nTao Guo": "https://doi.org/10.1145/3459637.3482085",
          "Meng Chen": "during shopping scenario, https://xiaozhi.jd.com/."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "each SSL pre-training task, which can further inspire the future": "researches to build better dialogue encoder."
        },
        {
          "each SSL pre-training task, which can further inspire the future": "Specifically, our contributions include:"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "â€¢ We propose a novel pre-trained dialogue encoder based on"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "BERT by devising five self-supervised learning pre-training"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "tasks, including masked language modeling, masked utter-"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "ance prediction, utterance replacement, turn swapping, and"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "response selection. The model was pre-trained with more"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "than 70 million of dialogues."
        },
        {
          "each SSL pre-training task, which can further inspire the future": "â€¢ We verify the effectiveness of DialogueBERT on three repre-"
        },
        {
          "each SSL pre-training task, which can further inspire the future": ""
        },
        {
          "each SSL pre-training task, which can further inspire the future": "sentative downstream dialogue tasks, including intent recog-"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "nition, emotion recognition, and named entity recognition."
        },
        {
          "each SSL pre-training task, which can further inspire the future": "Experimental results show the superiority and competitive-"
        },
        {
          "each SSL pre-training task, which can further inspire the future": "ness of our proposed model."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "alogueBERT) based on the prevalent pre-trained language model"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "BERT [4]. To catch the particularity of dialogue utterances, we de-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "vise five self-supervised learning based pre-training tasks and train"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "the DialogueBERT with large-scale conversation corpus. Inspired by"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "[22], a convolutional pooler based on convolutional neural network"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "(CNN) and 2D pooling is designed to extract the dialogue feature rep-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "resentations. To learn the relationship between different utterances"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "in the dialogue,\nfour different\ninput embeddings are integrated,"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "including token embedding, position embedding, role embedding"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "and turn embedding. DialogueBERT is pre-trained with over 70"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "million of dialogues, which are collected from real conversations be-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "tween users and customer service staffs in E-commerce scenario. To"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "verify the effectiveness of our pre-trained dialogue encoder, exten-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "sive experiments were conducted by fune-tuning DialogueBERT on"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "three downstream dialogue understanding tasks, including intent"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "recognition, emotion recognition, and named entity recognition."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "Experimental results show that the proposed method outperforms"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "several strong dialogue encoding approaches and achieves 88.63%"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "accuracy score for intent recognition, 94.25% accuracy for emotion"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "recognition and 97.04% F1 score for named entity recognition. Ab-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "lation study was also conducted to figure out the contribution of"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "each SSL pre-training task, which can further inspire the future"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "researches to build better dialogue encoder."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "Specifically, our contributions include:"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "â€¢ We propose a novel pre-trained dialogue encoder based on"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "BERT by devising five self-supervised learning pre-training"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "tasks, including masked language modeling, masked utter-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "ance prediction, utterance replacement, turn swapping, and"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "response selection. The model was pre-trained with more"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "than 70 million of dialogues."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "â€¢ We verify the effectiveness of DialogueBERT on three repre-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "sentative downstream dialogue tasks, including intent recog-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "nition, emotion recognition, and named entity recognition."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "Experimental results show the superiority and competitive-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "ness of our proposed model."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "2\nAPPROACH"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "In this Section, we will introduce the model input, model structure"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "and self-supervised learning based model pre-training. Figure 1"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "demonstrates the overall architecture of our proposed model."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "2.1\nModel Input"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": ""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "In order to pre-train the model, we firstly collect a large-scale"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "dialogue corpus from real conversations between customers and"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "customer service staffs. After that, we desensitized and anonymized"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "the private information based on very detailed rules. Finally, We col-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "lected over 70 million conversations with 8.59 utterances on average"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "for each conversation. For each conversation utterance, we prepend"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "one of the two special tags, i.e. \"<Q>\" (customerâ€™s query) and \"<A>\""
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "(customer service staffâ€™s response). Then we concatenate the ut-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "terances to form the input text as \"<Q>xxxx<A>xxx<Q>xxxx...\"."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "We trained a BPE sentencepiece tokenizer [13] with 5 million ran-"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "domly sampled conversations. The vocabulary size is set to 50,000."
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "Then we split the conversation input text into tokens. To catch the"
        },
        {
          "in this paper, we propose a novel dialogue encoder (denoted as Di-": "relationship between utterances, we combine the token, role and"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "can capture the local ngram-level lexical and semantic information."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "The five SSL headers are used to pre-train the DialogueBERT with"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "the huge conversation corpus, which will be described in detail in"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "Section 2.3."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "2.3\nSSL-based Model Pre-Training"
        },
        {
          "convolution kernels applied along the input sequence dimension": "Considering the dialogue involves the token/word level feature,"
        },
        {
          "convolution kernels applied along the input sequence dimension": "utterance level\nfeature, and conversation order feature, etc., we"
        },
        {
          "convolution kernels applied along the input sequence dimension": "design five different SSL pre-training tasks. As shown in Figure 1,"
        },
        {
          "convolution kernels applied along the input sequence dimension": "the proposed SSL pre-training tasks are as follows:"
        },
        {
          "convolution kernels applied along the input sequence dimension": "â€¢ MUM is a masked utterance modeling SSL task, where we"
        },
        {
          "convolution kernels applied along the input sequence dimension": "randomly mask one utterance in a conversation with several"
        },
        {
          "convolution kernels applied along the input sequence dimension": "special tokens,\ni.e. \"[MASK]\" (replace the ğ‘š tokens in the"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "utterance), then the model is required to generate the tokens"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "of the masked utterance. The SSL method of the work in [6]"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "selects the utterance from a candidate set (we denote this"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "as MSM) while we directly predict the utterance in token"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "level. We argue that because the candidate set is randomly"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "sampled in [6], it is not a good choice for a contrastive SSL"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "task due to poor contrastive samples [8]. We feed the masked"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "token vectors (ğ‘¢ğ‘šğ‘ğ‘ ğ‘˜1, ğ‘¢ğ‘šğ‘ğ‘ ğ‘˜2, ...ğ‘¢ğ‘šğ‘ğ‘ ğ‘˜ğ‘š ) to the masked lan-"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "guage header to predict the original tokens by following the"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "approach in BERT [4]."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "â€¢ MLM is a masked language modeling SSL task, where we"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "follows the setting in [3]. This SSL task aims to learn the"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "language modeling structure of the conversation text."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "â€¢ ReplDisc randomly (with probability of 0.5) replaces one"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "utterance in a conversation with a randomly selected utter-"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "ance from another conversation in the same training batch,"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "and then discriminates whether the new conversation is the"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "replaced one or the original one. This SSL task is devised to"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "learn the contextual feature of the conversation."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "â€¢ TurnDisc randomly swaps two utterances in a conversation"
        },
        {
          "convolution kernels applied along the input sequence dimension": "and then discriminates whether the new conversation is"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "swapped, where the turn in new conversation is still indexed"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "in ascending order (we do not swap the turn information of"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "the two utterances). This SSL task is designed to learn the"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "utterance order feature of conversation."
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "â€¢ ResSel is the same as the response constrastive loss (RCL) in"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "[18], which can help learning a better representation for the"
        },
        {
          "convolution kernels applied along the input sequence dimension": "[CLS] token, and capturing underlying dialogue sequential"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "order, structure information, and response similarity. Differ-"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "ently, we only use the model to select the last utterance of a"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "conversation from a set of candidate responses that are from"
        },
        {
          "convolution kernels applied along the input sequence dimension": ""
        },
        {
          "convolution kernels applied along the input sequence dimension": "the training batch."
        },
        {
          "convolution kernels applied along the input sequence dimension": "For ReplDisc and TurnDisc, we use the logistic regression to"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Performance comparison of different models on",
      "data": [
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "MUM",
          "IntR": "87.92",
          "NER": "96.95",
          "EmoR": "93.56"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "MSM",
          "IntR": "87.63",
          "NER": "96.59",
          "EmoR": "92.65"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "ResSel",
          "IntR": "87.97",
          "NER": "96.85",
          "EmoR": "92.88"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "ReplDisc",
          "IntR": "87.24",
          "NER": "96.11",
          "EmoR": "92.15"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "TurnDisc",
          "IntR": "86.25",
          "NER": "96.03",
          "EmoR": "92.07"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "DialogueBERT",
          "IntR": "88.63",
          "NER": "97.04",
          "EmoR": "94.25"
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "",
          "IntR": "Table 3: Ablation Study of each SSL pre-training task. MLM",
          "NER": "",
          "EmoR": ""
        },
        {
          "SSL task": "",
          "IntR": "",
          "NER": "",
          "EmoR": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Performance comparison of different models on",
      "data": [
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "TOD-BERT [18]",
          "86.36": "87.91",
          "-": "96.85",
          "92.95": "92.83"
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "ğ·ğ‘–ğ‘ğ‘™ğ‘œğ‘”ğ‘¢ğ‘’ğµğ¸ğ‘…ğ‘‡ğ¶ğ¿ğ‘†",
          "86.36": "88.32",
          "-": "97.00",
          "92.95": "94.16"
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "DialogueBERT",
          "86.36": "88.63",
          "-": "97.04",
          "92.95": "94.25"
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "Table 2: Performance comparison of different models on",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "three downstream dialogue understanding tasks.",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "accuracy as evaluation metric. For NER, the Macro-F1 score is re-",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "ported as evaluation metric. We compared our method with the",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "following four strong baselines:",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "",
          "86.36": "",
          "-": "",
          "92.95": ""
        },
        {
          "PT-CoDE [6]": "â€¢ DMN [7]:",
          "86.36": "the dynamic memory network formulates dia-",
          "-": "",
          "92.95": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Performance comparison of different models on",
      "data": [
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": "ported as evaluation metric. We compared our method with the"
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": "following four strong baselines:"
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": "â€¢ DMN [7]:"
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": "generate a feature vector."
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": "contrastive loss."
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        },
        {
          "accuracy as evaluation metric. For NER, the Macro-F1 score is re-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer"
        },
        {
          "REFERENCES": "Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, and\n[1]",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Linguistics."
        },
        {
          "REFERENCES": "Eneko Agirre. 2020. DoQAÅ›Accessing Domain-Specific FAQs via Conversational",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[14]\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. 2015. End-"
        },
        {
          "REFERENCES": "QA. arXiv preprint arXiv:2005.01328 (2020).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "to-end memory networks. arXiv preprint arXiv:1503.08895 (2015)."
        },
        {
          "REFERENCES": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy\n[2]",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[15] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones,"
        },
        {
          "REFERENCES": "Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. arXiv",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you"
        },
        {
          "REFERENCES": "preprint arXiv:1808.07036 (2018).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Need. In Advances in Neural Information Processing Systems 30: Annual Conference"
        },
        {
          "REFERENCES": "[3] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach,"
        },
        {
          "REFERENCES": "2020. Revisiting Pre-Trained Models for Chinese Natural Language Processing.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,"
        },
        {
          "REFERENCES": "arXiv preprint arXiv:2004.13922 (2020).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 5998Å›6008."
        },
        {
          "REFERENCES": "[4]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[16]\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks."
        },
        {
          "REFERENCES": "Pre-training of Deep Bidirectional Transformers for Language Understanding. In",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "arXiv preprint arXiv:1410.3916 (2014)."
        },
        {
          "REFERENCES": "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[17] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,"
        },
        {
          "REFERENCES": "tion for Computational Linguistics: Human Language Technologies, NAACL-HLT",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz,\nJoe"
        },
        {
          "REFERENCES": "2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,"
        },
        {
          "REFERENCES": "Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,"
        },
        {
          "REFERENCES": "tional Linguistics, 4171Å›4186.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language"
        },
        {
          "REFERENCES": "[5] Zhiting Hu, Haoran Shi, Zichao Yang, Bowen Tan, Tiancheng Zhao, Junxian He,",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural"
        },
        {
          "REFERENCES": "Wentao Wang, Lianhui Qin, Di Wang, et al. 2018. Texar: A Modularized, Versatile,",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Language Processing: System Demonstrations. Association for Computational"
        },
        {
          "REFERENCES": "and Extensible Toolkit\nfor Text Generation.\narXiv preprint arXiv:1809.00794",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Linguistics, Online, 38Å›45."
        },
        {
          "REFERENCES": "(2018).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[18] Chien-Sheng Wu, Steven C. H. Hoi, Richard Socher, and Caiming Xiong. 2020."
        },
        {
          "REFERENCES": "[6] Wenxiang Jiao, Michael R. Lyu, and Irwin King. 2019. PT-CoDE: Pre-trained",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented"
        },
        {
          "REFERENCES": "Context-Dependent Encoder for Utterance-level Emotion Recognition. CoRR",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Dialogue. In Proceedings of the 2020 Conference on Empirical Methods in Natural"
        },
        {
          "REFERENCES": "abs/1910.08916 (2019).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber,"
        },
        {
          "REFERENCES": "[7] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational"
        },
        {
          "REFERENCES": "Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Linguistics, 917Å›929."
        },
        {
          "REFERENCES": "anything: Dynamic memory networks for natural language processing. In Inter-",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[19] Kun Xu, Yuxuan Lai, Yansong Feng, and Zhiguo Wang. 2019. Enhancing key-"
        },
        {
          "REFERENCES": "national conference on machine learning. PMLR, 1378Å›1387.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "value memory neural networks for knowledge based question answering.\nIn"
        },
        {
          "REFERENCES": "[8] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Proceedings of the 2019 Conference of the North American Chapter of the Association"
        },
        {
          "REFERENCES": "Jie Tang. 2020.\nSelf-supervised Learning: Generative or Contrastive.\nCoRR",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long and"
        },
        {
          "REFERENCES": "abs/2006.08218 (2020). arXiv:2006.08218",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Short Papers). 2937Å›2947."
        },
        {
          "REFERENCES": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n[9]",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[20] Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui"
        },
        {
          "REFERENCES": "Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Hsieh. 2019. Reducing BERT Pre-Training Time from 3 Days to 76 Minutes. CoRR"
        },
        {
          "REFERENCES": "Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "abs/1904.00962 (2019)."
        },
        {
          "REFERENCES": "arXiv:1907.11692",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[21] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang"
        },
        {
          "REFERENCES": "[10] Chen Qu, Liu Yang, Minghui Qiu, W Bruce Croft, Yongfeng Zhang, and Mohit",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Large-Scale"
        },
        {
          "REFERENCES": "Iyyer. 2019. BERT with history answer embedding for conversational question",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Generative Pre-training for Conversational Response Generation. In Proceedings"
        },
        {
          "REFERENCES": "answering.\nIn Proceedings of the 42nd International ACM SIGIR Conference on",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "of the 58th Annual Meeting of the Association for Computational Linguistics: System"
        },
        {
          "REFERENCES": "Research and Development in Information Retrieval. 1133Å›1136.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Demonstrations, ACL 2020, Online, July 5-10, 2020, Asli Celikyilmaz and Tsung-"
        },
        {
          "REFERENCES": "[11] Alec Radford,\nJeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Hsien Wen (Eds.). Association for Computational Linguistics, 270Å›278."
        },
        {
          "REFERENCES": "Sutskever. 2019. Language Models are Unsupervised Multitask Learners.\n(2019).",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "[22]\nPeng Zhou, Zhenyu Qi, Suncong Zheng,\nJiaming Xu, Hongyun Bao, and Bo"
        },
        {
          "REFERENCES": "[12]\nSiva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversa-",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Xu. 2016. Text Classification Improved by Integrating Bidirectional LSTM with"
        },
        {
          "REFERENCES": "tional question answering challenge. Transactions of the Association for Compu-",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Two-dimensional Max Pooling. In COLING 2016, 26th International Conference"
        },
        {
          "REFERENCES": "tational Linguistics 7 (2019), 249Å›266.",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "on Computational Linguistics, Proceedings of\nthe Conference: Technical Papers,"
        },
        {
          "REFERENCES": "[13] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.\nNeural Machine",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "December 11-16, 2016, Osaka, Japan, Nicoletta Calzolari, Yuji Matsumoto, and"
        },
        {
          "REFERENCES": "Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": ""
        },
        {
          "REFERENCES": "",
          "Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,": "Rashmi Prasad (Eds.). ACL, 3485Å›3495."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DoQAÅ›Accessing Domain-Specific FAQs via Conversational QA",
      "authors": [
        "Jon Ander Campos",
        "Arantxa Otegi",
        "Aitor Soroa",
        "Jan Deriu",
        "Mark Cieliebak",
        "Eneko Agirre"
      ],
      "year": "2020",
      "venue": "DoQAÅ›Accessing Domain-Specific FAQs via Conversational QA",
      "arxiv": "arXiv:2005.01328"
    },
    {
      "citation_id": "2",
      "title": "Quac: Question answering in context",
      "authors": [
        "Eunsol Choi",
        "He He",
        "Mohit Iyyer",
        "Mark Yatskar",
        "Wen-Tau Yih",
        "Yejin Choi",
        "Percy Liang",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Quac: Question answering in context",
      "arxiv": "arXiv:1808.07036"
    },
    {
      "citation_id": "3",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2020",
      "venue": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "arxiv": "arXiv:2004.13922"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019"
    },
    {
      "citation_id": "5",
      "title": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
      "authors": [
        "Zhiting Hu",
        "Haoran Shi",
        "Zichao Yang",
        "Bowen Tan",
        "Tiancheng Zhao",
        "Junxian He",
        "Wentao Wang",
        "Lianhui Qin",
        "Di Wang"
      ],
      "year": "2018",
      "venue": "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
      "arxiv": "arXiv:1809.00794"
    },
    {
      "citation_id": "6",
      "title": "PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2019",
      "venue": "PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition"
    },
    {
      "citation_id": "7",
      "title": "Ask me anything: Dynamic memory networks for natural language processing",
      "authors": [
        "Ankit Kumar",
        "Ozan Irsoy",
        "Peter Ondruska",
        "Mohit Iyyer",
        "James Bradbury",
        "Ishaan Gulrajani",
        "Victor Zhong",
        "Romain Paulus",
        "Richard Socher"
      ],
      "year": "2016",
      "venue": "Ask me anything: Dynamic memory networks for natural language processing"
    },
    {
      "citation_id": "8",
      "title": "Self-supervised Learning: Generative or Contrastive",
      "authors": [
        "Xiao Liu",
        "Fanjin Zhang",
        "Zhenyu Hou",
        "Zhaoyu Wang",
        "Li Mian",
        "Jing Zhang",
        "Jie Tang"
      ],
      "year": "2020",
      "venue": "Self-supervised Learning: Generative or Contrastive",
      "arxiv": "arXiv:2006.08218"
    },
    {
      "citation_id": "9",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "10",
      "title": "BERT with history answer embedding for conversational question answering",
      "authors": [
        "Chen Qu",
        "Liu Yang",
        "Minghui Qiu",
        "Bruce Croft",
        "Yongfeng Zhang",
        "Mohit Iyyer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "11",
      "title": "Language Models are Unsupervised Multitask Learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language Models are Unsupervised Multitask Learners"
    },
    {
      "citation_id": "12",
      "title": "Coqa: A conversational question answering challenge",
      "authors": [
        "Siva Reddy",
        "Danqi Chen",
        "Christopher Manning"
      ],
      "year": "2019",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Endto-end memory networks",
      "authors": [
        "Sainbayar Sukhbaatar",
        "Arthur Szlam",
        "Jason Weston",
        "Rob Fergus"
      ],
      "year": "2015",
      "venue": "Endto-end memory networks",
      "arxiv": "arXiv:1503.08895"
    },
    {
      "citation_id": "15",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "16",
      "title": "Memory networks",
      "authors": [
        "Jason Weston",
        "Sumit Chopra",
        "Antoine Bordes"
      ],
      "year": "2014",
      "venue": "Memory networks",
      "arxiv": "arXiv:1410.3916"
    },
    {
      "citation_id": "17",
      "title": "Transformers: State-of-the-Art Natural Language Processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "RÃ©mi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "18",
      "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue",
      "authors": [
        "Chien-Sheng Wu",
        "C Steven",
        "Richard Hoi",
        "Caiming Socher",
        "Xiong"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Enhancing keyvalue memory neural networks for knowledge based question answering",
      "authors": [
        "Kun Xu",
        "Yuxuan Lai",
        "Yansong Feng",
        "Zhiguo Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "Reducing BERT Pre-Training Time from 3 Days to 76 Minutes",
      "authors": [
        "Yang You",
        "Jing Li",
        "Jonathan Hseu",
        "Xiaodan Song",
        "James Demmel",
        "Cho-Jui Hsieh"
      ],
      "year": "2019",
      "venue": "Reducing BERT Pre-Training Time from 3 Days to 76 Minutes"
    },
    {
      "citation_id": "21",
      "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation",
      "authors": [
        "Yizhe Zhang",
        "Siqi Sun",
        "Michel Galley",
        "Yen-Chun Chen",
        "Chris Brockett",
        "Xiang Gao",
        "Jianfeng Gao",
        "Jingjing Liu",
        "Bill Dolan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2020"
    },
    {
      "citation_id": "22",
      "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling",
      "authors": [
        "Peng Zhou",
        "Zhenyu Qi",
        "Suncong Zheng",
        "Jiaming Xu",
        "Hongyun Bao",
        "Bo Xu"
      ],
      "year": "2016",
      "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers"
    }
  ]
}