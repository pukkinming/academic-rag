{
  "paper_id": "2510.01662v1",
  "title": "Discrete Facial Encoding: A Framework For Data-Driven Facial Display Discovery",
  "published": "2025-10-02T04:44:45Z",
  "authors": [
    "Minh Tran",
    "Maksim Siniukov",
    "Zhangyu Jin",
    "Mohammad Soleymani"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identityinvariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Quantitative representation of facial expressions, or facial encoding, is fundamental to psychological and affective computing  [25, 33, 48] . By providing structured and interpretable representations of facial expressions, facial expression coding enables objective analysis of human emotion  [29] , cognition  [7] , and behavior  [1, 13, 16] . These repre-sentations facilitate the scientific study of social and mental states and enhance the transparency and interpretability of AI applications ranging from clinical diagnostics to humancomputer interaction and behavioral health monitoring.\n\nAmong facial coding methods, the Facial Action Coding System (FACS)  [9]  remains the most widely adopted and influential framework. FACS decomposes facial behavior into a standardized set of Action Units (AUs), each corresponding to the activation of a specific facial muscle or group of muscles, thereby enabling principled analysis of how facial patterns relate to underlying psychological processes. Its structured representation has supported a wide range of applications requiring objective interpretation of expressive behaviors  [1, 7, 13, 16, 29] . However, traditional FACS coding relies on time-intensive and costly manual annotation, motivating the development of automated AU detection systems  [21, 38, 39] . Despite recent progress in computer vision, such systems remain limited by moderate accuracy (with state-of-the-art F1 scores typically around 70%  [20] ) and sensitivity to in-the-wild conditions  [47] .\n\nTo overcome these limitations, we propose a novel datadriven facial expression coding approach utilizing Residual Vector-Quantized Variational Autoencoders  [30, 45]  (RVQ-VAE). Our method automatically discovers a comprehensive set of expressive facial templates directly from large-scale facial image data  [26] , enabling complete encoding of observable facial expressions beyond the scope of predefined AU combinations. Unlike FACS-based systems that rely heavily on supervised annotation, our approach is entirely unsupervised, significantly reducing the need for manual labeling and enhancing scalability across diverse datasets. To ensure interpretability and isolate expression-related variations, we operate on 3D Morphable Model (3DMM) features  [8, 18] , which allow us to reduce confounding factors such as facial identity (shape) and head pose. We further compress these 3DMM expression features into discrete facial tokens using vector quantization. The resulting discrete facial tokens function as hidden states that influence the reconstructed face. Importantly, each token can be visualized by comparing its associated reconstruction to a neutral template, revealing the specific facial regions it modulates.\n\nWe validate our approach through comprehensive experiments across three key psychological tasks: stress detection  [6] , personality trait prediction  [3] , and depression assessment  [32] . Using a simple Bag-of-Words model  [50]  over our learned facial tokens, we demonstrate that our representation consistently outperforms traditional FACS-based features, alternative data-driven facial template discovery systems, and powerful deep image representation learning models such as Masked Autoencoders  [4, 22] . Our analysis shows that the discovered codebook captures a broader and more precise spectrum of facial displays, effectively representing both subtle and complex expressions that are often overlooked by predefined AU-based methods. These findings suggest that learning data-driven facial representations offers a promising and scalable alternative to FACS, opening new avenues for robust, interpretable, and task-relevant facial analysis in psychological and affective computing. Source code and model weights will be released upon publication.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "2. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2.1. Facial Action Coding Systems",
      "text": "Facial action coding is an established visual behavior analysis tool, with the Facial Action Coding System (FACS)  [9]  serving as the longstanding gold standard. FACS decomposes facial expressions into discrete Action Units (AUs), enabling a structured investigation of the relationship between facial muscle movements and internal states such as emotion, cognition, and mental health. In psychological and affective computing research, FACS remains widely adopted due to its interpretability and comprehensive coverage of facial expressions  [25, 33, 48] . It allows researchers to quantify facial behavior in a principled and objective manner, facilitating the study of correlations between facial actions and psychological phenomena. However, AU Coding traditionally relies on labor-intensive manual coding by certified experts, limiting its scalability. This challenge has led to increasing interest in automating AU detection  [2, 10, 19, 21, 38, 39] . Despite recent progress, even state-of-the-art AU detection systems remain imperfect, typically reporting average F1 scores around 0.7  [20] . The task is further complicated by the imbalanced distribution of AUs  [15] , where less frequent units are significantly harder to detect accurately  [49] . Most vision-based AU coding systems detect a subset of 44 AUs  [5] . Moreover, current AU detection models exhibit limited generalization across domains  [47] , often suffering substantial performance drops when evaluated under distribution shifts. These generalization challenges hinder the deployment of AU-based systems in real-world, critical contexts, suggesting that conventional AU representations may not be sufficiently robust for broad behavioral applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Interpretable Data-Driven Facial Coding",
      "text": "Given the limitations of Action Units (AU), researchers have explored unsupervised, data-driven representations to capture facial displays more comprehensively. In this direction, Sariyanidi et al. propose Facial Bases  [34] , a method that models facial expressions as linear combinations of localized basis functions, each corresponding to a distinct facial movement (e.g., eyebrow raise). These bases are learned from Gabor phase shifts  [12]  extracted from facial video sequences, effectively capturing fine-grained temporal motion patterns. The resulting basis coefficients directly reflect movement intensity, enabling the model to represent the gradual evolution of facial expressions over time. However, since this approach operates on 2D pixel intensities, it struggles to disentangle expression-specific deformations from confounding factors such as head pose, illumination, and facial morphology. As a result, the learned bases may inadvertently encode non-expression-related variations, reducing both interpretability and generalizability, particularly in cross-subject or in-the-wild scenarios. To address these limitations, the authors extend their framework  [36]  by leveraging 3D Morphable Model (3DMM) expression features  [35] , which inherently separate out identity, pose, and lighting variations. This allows the learning process to focus exclusively on expression-related dynamics. Using dictionary learning  [23]  on these 3DMM-derived representations, the method constructs a set of facial bases and derives sparse activation coefficients for each input. These coefficients are then used as features for downstream behavioral prediction tasks, such as autism diagnosis. Our method differs from Facial Basis in two key ways: first, we leverage deep learning to model the complex, non-linear structure of the 3DMM expression space; and second, we produce a more interpretable, discrete representation by assigning each input to a small set of discrete codebook entries, rather than representing it as a weighted combination of basis templates.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "An overview of our proposed model is available in Figure  1 . Given a face image, our goal is to decompose the expression into interpretable, discrete components. To this end, we first reduce the influence of identity and other factors such as face shape and head pose by extracting expression parameters using a 3D Morphable Model (3DMM)  [8] . 3DMMs are designed to disentangle expression from identity, and their expression parameters mostly contain information about expression (they may contain residual identity information due to their limitations). We then encode these 3DMM expression vectors using a Residual Vector-Quantized Variational Autoencoder (RVQ-VAE)  [30, 45] , which maps each input to a set of discrete tokens. These tokens provide a compact and interpretable representation of facial behavior, and can be visualized by decoding them back into facial expressions using the 3DMM.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction With Emoca",
      "text": "We use EMOCA  [8]  to extract expression features from facial images due to its optimization to capture expressions of emotions, popularity in facial behavior generation  [27, 44]  and its straightforward process for converting expression vectors into facial meshes. EMOCA is a 3D face reconstruction model built on top of the DECA 3D Morphable Model  [11] , which represents a face as a deformation of a neutral template mesh T ∈ R 3×N , where N is the number of vertices. The final mesh M is computed as:\n\nwhere β, ψ, and θ are the shape, expression, and pose vectors, respectively. B s and B e are the shape and expression blendshape functions, J(β) defines how to compute joint locations from mesh vertices, T is the \"zero pose\" template mesh, and W (•, J, θ) is the linear blend skinning function that applies pose-dependent deformations. EMOCA improves upon DECA by enhancing the expressivity of reconstructed faces. It introduces an emotion consistency loss, which encourages the emotion features of the input image to match those of the rendered reconstruction. Specifically, the model minimizes the mean squared error (MSE) between emotion embeddings extracted from the input and the rendered image. This regularization helps EMOCA better preserve the emotional content and subtle expressive details of the original input. Given the high emotional fidelity of EMOCA and its strong ability to disentangle expression from identity and pose, we use the extracted expression parameters as the input to our framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Discrete Expression Encoding With Residual Vq-Vae",
      "text": "Our model maps the EMOCA expression vector into a discrete latent code using a Residual Vector-Quantized Variational Autoencoder (RVQ-VAE)  [30] , combining transformer-based encoding and multi-stage residual quantization.\n\nEncoder. We reshape the input expression vector into a sequence of T tokens of dimension d. Each token is projected into a hidden space via a linear layer, then passed through a Transformer encoder  [46] . The output is mean-pooled and projected into a latent vector z 0 ∈ R D .\n\nResidual Quantization. We apply residual quantization over L stages using a shared codebook Q ∈ R K×D with K entries. At each stage i, we quantize the residual:\n\n(2)\n\nThis process converts z 0 into a sequence of L discrete tokens with an additive property. The first token represents the face template most similar to the given facial input, and each subsequent token encodes finer residual details, progressively refining the facial representation. The final quantized vector is the sum of selected codes: z q = L i=1 e ki . Decoder. Unlike traditional VQ-VAEs  [45]  that have symmetric encoder and decoder architectures, our decoder consists of a simple linear projection layer that maps the quantized latent code z q back to the input dimension.\n\nThis design is motivated by two factors: 1) the additive structure of the architecture enhances interpretability when visualizing the contributing components of a facial display, and 2) the encoder's representation is the primary focus during training. A more complex decoder does not significantly improve the encoder's ability to learn a rich latent representation, as the decoder's role is primarily to reconstruct the input once the latent space has been learned.\n\nTraining losses. Our model's training objective includes a reconstruction loss and a commitment loss, following the formulation introduced in the original VQ-VAE framework  [46] . The reconstruction loss ensures fidelity between the input and the output, while the commitment loss encourages the encoder outputs to remain close to their assigned codebook vectors:\n\nTo encourage the model to capture fine-grained and localized facial details, we incorporate two regularization terms during training: an ℓ 1 -penalty and an orthogonality loss.\n\nThe ℓ 1 -penalty promotes sparsity in the codebook usage, encouraging each token to specialize in distinct facial regions. Meanwhile, the orthogonality loss ensures that the decoded codebook embeddings remain diverse and non-redundant. It is defined as:\n\nwhere e i ∈ R d is the embedding of the i-th codebook entry, and K is the size of the codebook. Overall, our model training objective is",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visualization Of Facial Templates",
      "text": "After training, each input expression is represented as a discrete code sequence [k 1 , k 2 , . . . , k L ], providing a compact and interpretable tokenization of facial expressions. Each token corresponds to a quantized latent vector that can be decoded into a 3D facial mesh using the 3DMM decoder  [8] , enabling direct visual inspection. Since the 3DMM effectively disentangles expression from identity, pose, and lighting, we can manipulate only the expression coefficients while keeping other factors fixed. This allows us to isolate and visualize the specific facial deformation induced by each token in a controlled manner. Specifically, to visualize how each discrete token contributes to facial geometry, we render a deformation heatmap by comparing the reconstructed 3D mesh against a neutral face template (i.e., ψ = 0). For each discrete code, we decode it via EMOCA to obtain the reconstructed face mesh. We then compute a per-vertex Euclidean distance between the reconstructed mesh and the neutral mesh:\n\nwhere v v ∈ R 3 is the position of vertex v in the reconstructed mesh, and v ref v is the corresponding vertex in the neutral mesh. These distances are normalized and mapped to a perceptual colormap to produce interpretable heatmaps that highlight localized expression-driven deformations.\n\nWe demonstrate examples of our interpretability pipeline in Figure  7 . As shown, it is often difficult to determine which facial regions are activated by simply inspecting the reconstructed face mesh. However, by comparing it with the neutral face template and visualizing the deformation as a heatmap, we can clearly localize the regions influenced by each token, offering a more interpretable and spatially grounded understanding of the token's effect on facial expression. In Figure  3 , we show several example expressions encoded by our system. The results indicate that the model effectively captures diverse expression patterns, with different tokens corresponding to localized facial deformations that resemble distinct combinations of muscle activations. Finally, Figure  4  illustrates how the expression of a given input image can be decomposed into a set of additive components, demonstrating both the model's accuracy and the interpretability of its token-based representation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "We compare the proposed facial expression coding system with existing approaches along three key dimensions: (1) accuracy in preserving facial expressions, (2) utility as a feature representation for downstream psychological tasks, and (3) diversity in capturing a wide range of facial expressions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4. Datasets",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4.1.1. Pre-Training Dataset",
      "text": "We pre-train our RVQ-VAE model on the AffectNet  [26] , a large-scale collection of approximately 350K face images annotated with both categorical and dimensional emotion labels. AffectNet offers substantial variability in appearance, expression, ethnicity and pose, making it well-suited for learning robust and generalizable expression representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Datasets",
      "text": "For evaluation, we use several datasets, organized by their specific purposes: Expression Preservation and Diversity:\n\n• Aff-Wild2  [15] : This in-the-wild video dataset is annotated with frame-level Action Unit (AU) labels, providing a rich and dynamic set of facial behaviors. Its scale and expressive variety enable a comprehensive assessment of how well different coding systems capture subtle and complex facial motions. • SmileStimuli  [24] : This dataset contains 45 video recordings from 15 professional actors, each portraying one of three categories of smiles (dominance, affiliation or reward). The balanced distribution of smile types allows us to further explore the diversity of expressions captured by our system. Downstream Psychological Tasks: • Stress Identification  [6] : We evaluate on the StressID dataset  [6] , a recent multimodal benchmark designed to assess stress levels in real-world human interactions. The dataset comprises over 1,200 annotated video segments collected from 65 participants undergoing stress-inducing conditions such as cognitive load and public speaking. Each segment is rated on a perceived stress scale from 1 to 10 and subsequently converted into binary or three-class stress labels. Given the presence of rich facial expressions throughout the recordings, this dataset is well-suited for testing facial encoding systems. • Depression Detection  [32] : We use the dataset from the AVEC 2019 challenge  [32] , which targets automatic depression analysis. Specifically, we consider two tasks: (1) depression severity regression, and (2) binary classification of depressed vs. non-depressed subjects. The dataset consists of video recordings from 275 subjects, totaling approximately 73 hours of audiovisual data. Each subject underwent a semi-structured clinical interview conducted by a virtual agent, with depression severity assessed using the PHQ-8 questionnaire. The interviews were performed in a Wizard-of-Oz (WoZ) setup, where the virtual agent was controlled by a human operator. • ChaLearn First Impressions  [3] : We use the ChaLearn First Impressions dataset  [3] , a large-scale benchmark for apparent personality recognition from short videos. It contains over 10,000 video segments featuring individuals speaking in unconstrained settings, each annotated with continuous scores for the Big Five personality traits: openness, conscientiousness, extraversion, agreeableness and neuroticism. These scores range from 0 to 1, indicating the perceived strength of each trait.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "Our primary baseline is the widely used Facial Action Unit (AU) system  [9] . For datasets lacking annotated AU labels, we use LibreFace  [5]  to extract AU features. We compare our method with automatically tracked (rather than humancoded) AU features, as both approaches are automated and do not require human input in the pipeline. In addition, we include Facial Basis  [36]  as a baseline for evaluating utility in downstream psychological tasks. However, due to its continuous, non-discrete representation, we do not include it in experiments focused on facial accuracy preservation or expression diversity.\n\nTo contextualize our method's performance in broader representation learning, we also report results from popular image and video encoding models, including MAE-Face  [22] , VideoMAE  [43] , and MARLIN  [4] . While these models are not designed for interpretability, they serve as strong representation learning baselines for assessing utility in psychological inference tasks. Their inclusion highlights the trade-off between interpretability and raw representational power in modern deep learning approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Our model is implemented in PyTorch and trained on a single NVIDIA H100 GPU. The input to the model is a 3DMM expression vector reshaped to size T = 10 and d = 5. Our transformer encoder contains 6 layers with 4 attention heads, and a hidden dimension D = 128. We apply residual vector quantization (RVQ) with L = 4 quantization stages and a shared codebook C ∈ R K×D of size K = 64 and D = 50. The decoder f dec : R D → R 50 is a single linear projection that reconstructs the original expression vector. We set hyperparameters as follows: β = 0.25, λ orth = 1.0, λ sparse = 0.1, and λ 1 = 1 × 10 -4 . We provide ablation studies on our hyper-parameter choices in the supplemental materials. The model is trained using the Adam optimizer  [14]  with learning rate 1 × 10 -4 , batch size 512 for 500 epochs. For downstream psychological tasks with videos, we represent each video using a Bag-of-Words (BoW) approach, encoding it as a frequency distribution over the codebook entries. Detailed modeling procedures for each downstream task are provided in the corresponding discussion sections.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluating Expression Accuracy",
      "text": "To assess how accurately our VQ-VAE-based encoding system captures facial expressions, we conduct a retrieval-based evaluation against a baseline system based on Facial Action Units (AUs). The goal of this experiment is to evaluate how well the learned representations preserve expression-relevant information.\n\nGiven a query facial image, we use both the humanannotated AU-based and VQ-VAE-based systems to retrieve visually similar samples from a large database consisting of 300K frames from a subset of the Aff-Wild2 dataset  [15]  with human-annotated AU labels. Due to the large scale of Aff-Wild2  [15] , we randomly subsampled overlapping short clips of 64 frames to construct this retrieval set. For both systems, each image is first converted into a binary encoding vector: for the AU-based system, each element indicates whether a specific AU is activated; for the VQ-VAE system, each element indicates whether a discrete token is present in the coded sequence. Using these binary vectors, we retrieve all database images that have the exact same encoding as the query. If fewer than five matches are found, the query is excluded from evaluation.\n\nTo fairly assess the quality of retrievals, we employ SMIRK  [31] , a recently introduced 3DMM-based system for extracting expression features from both the query and retrieved images. We intentionally avoid using EMOCA  [8]  to prevent evaluation bias, as our model is trained to reconstruct EMOCA-derived features. Instead, SMIRK-derived expression vectors serve as a neutral ground truth for measuring retrieval quality, allowing us to focus exclusively on expression similarity while disregarding confounding factors such as head pose and identity. Additionally, we use MAE-Face  [22] , a state-of-the-art self-supervised facial representation model, to extract features from the same retrieval sets. Unlike 3DMM-based encodings, MAE-Face  [22]  captures   Lower values suggest that the system captures more finegrained and consistent expression features.\n\nThe quantitative results in Table  1  and qualitative results in Figure  5  demonstrate that our VQ-VAE-based representation significantly outperforms the AU-based encoding in all retrieval metrics. The higher cosine similarity and lower Euclidean distance indicate that our method retrieves samples with more accurate expression matches. Additionally, the reduced standard deviation shows that our system captures more consistent and fine-grained expression variations across retrieved sets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluating Expression Diversity",
      "text": "To quantify the diversity of expressions captured by each coding system, we compute the normalized entropy over their respective feature vocabularies. We conduct this analysis on a fixed subset of approximately 300K video frames from the Aff-Wild2 dataset  [15]  with human-annotated AU labels. For each system, we count the frequency of occurrence for each discrete unit-AUs in the baseline system and tokens in our VQ-VAE model.\n\nLet p = [p 1 , p 2 , . . . , p K ] denote the empirical distribution over a vocabulary of size K, we first compute the entropy of p. To account for different vocabulary sizes across systems, we normalize the entropy by dividing by the dimensionality K, yielding the normalized entropy:\n\nA low normalized entropy indicates that the system predominantly activates a small subset of features (collapse), leading to poor expressive coverage and redundancy. In contrast, higher entropy implies that the system utilizes a broader range of features across inputs, suggesting greater expressiveness and diversity in facial representation.\n\nAs a second measure of expression diversity, we assess the redundancy among features in each encoding system by computing the average normalized mutual information (NMI) between features. Mutual information  [37]  quantifies the amount of shared information between two variables, while the normalization accounts for differences in feature entropy, making the measure directly comparable across different encoding systems  [40] . The key intuition is that lower normalized mutual information suggests more independent and disentangled features, indicating a more expressive representation  [28] .\n\nGiven an encoding matrix X ∈ R N ×K , where each row is a binary vector of feature activations, we compute the normalized mutual information between all unique feature pairs (columns of X). The average normalized mutual information is then calculated as:\n\nwhere I(X i ; X j ) is the mutual information between feature i and feature j, and H(X i ) is the entropy of feature i. A lower value of avg NMI indicates that features tend to vary independently across samples, reflecting higher diversity and lower redundancy. Conversely, a higher NMI suggests that many features are co-activated and share overlapping information.\n\nWe provide the quantitative results of the two diversity metrics in Table  2 . Our VQ-VAE representation achieves the highest normalized entropy (0.926), indicating that it activates a broader range of tokens across samples compared to both manually and automatically extracted AUs. Furthermore, it exhibits the lowest average normalized mutual information (0.004), suggesting that the learned tokens are highly independent and minimally redundant. Together, these results demonstrate that our system achieves superior diversity with minimal redundancy, offering a more expressive and disentangled facial representation.\n\nFinally, we validate the diversity of expressions captured by our system using the SmileStimuli dataset  [24] , which contains posed smiles categorized into dominance, affiliation, Table  3 . Smile-type classification performance on the SmileStimuli dataset  [24] . All values are multiplied by 100 for readability. and reward smiles. Given the limited size of the dataset (45 samples in total), we employ a Logistic Regression model with a leave-one-out cross-validation strategy. We compare the performance of models using our VQ-based token representations against models using traditional Action Unit (AU) features. The results, summarized in Table  3 , show that our system consistently outperforms the AU-based model across all evaluation metrics, including Accuracy, F1 Score, and AUC. This demonstrates that our learned token representations offer stronger discriminative power for differentiating subtle social smiles, involving asymmetry. However, it is important to note that our system encodes only geometric information, and prior research suggests that geometry alone may not fully capture facial expressions  [41] , which may explain the imperfect performance. To further illustrate the interpretability of our system, we visualize the most important facial templates-identified based on the log of the absolute values of the learned logistic regression coefficients-for each smile type. The top-4 templates are shown in Figure  6 , highlighting the diversity of expressions captured by our approach.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluating Feature Utility",
      "text": "We evaluate the usefulness of the learned tokens on three downstream high-level psychological tasks: depression detection, stress identification, and personality trait prediction.\n\nFor Depression Detection, we follow the official AVEC 2019 evaluation protocol with train/validation/test splits and report two standard metrics: Root Mean Square Er- We present the results for personality detection in Table  4 , depression detection in Table  5 , and stress identification in Table  6 . Across all tasks, our proposed method consistently outperforms baseline approaches, demonstrating the effectiveness and generalizability of our discrete token representation. Notably, our model surpasses even end-to-end, non-interpretable image and video representation learning models, indicating that the learned tokens are not only compact and interpretable but also semantically rich and highly informative for psychological inference.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Limitation",
      "text": "While our method offers interpretable and effective facial expression encoding, it has several limitations. First, the quality and expressivity of our learned tokens are inherently dependent on the richness of the 3DMM features used during training; limited or biased 3DMM expression representations may constrain the model's capacity. Furthermore, 3DMM features may still contain residual identity information, which can limit the effectiveness of our method in modeling fully identity-independent facial templates. Addressing this limitation and further reducing identity leakage remains an important direction for future work. Second, although our framework is currently developed and evaluated on static images, it is naturally extensible to video inputs by incorporating temporal modeling-an avenue we leave for future work. Third, the facial display templates are biased by the dataset on which the RQ-VAE is trained, which might not capture all cultural and individual variations. Fourth, our model ignores skin color changes that contain information about human inner states  [42] . Finally, our current model focuses solely on facial expression and does not account for other behavioral cues such as eye gaze, head pose, or body movement, which are often critical in psychological and affective understanding.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We introduced a novel framework for interpretable facial expression encoding using a VQ-VAE architecture trained on 3DMM-derived expression features. By representing facial expressions as discrete token sequences, our method enables both semantic interpretability and effective downstream use in psychological applications. Through comprehensive experiments, we demonstrate that our learned tokens outperform existing facial encoding systems-including Action Units and recent self-supervised models-across metrics of expression fidelity, feature diversity, and predictive utility. Furthermore, our approach offers a structured and visualizable representation space, bridging the gap between human-interpretable codes and machine-learned representations. Future work will explore integrating temporal dynamics and multimodal signals such as gaze and head movement to enhance behavioral modeling.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "No Orthogonal Loss No Sparse Loss One Quantizer Ours",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Ablation Studies",
      "text": "We conducted an ablation study on the StressID dataset to estimate the impact of model design choices. Results are shown in Table  7 . Decreasing the number of quantizers has the largest effect: reducing from four to a single quantizer, effectively turning it into a VQ-VAE, results in a significant drop in performance. Codebook size also influences performance-both overly small (16) and overly large (256) codebooks result in lower performance. We chose a simple decoder, as the focus of this approach is on the encoder. To demonstrate this, we trained the same model with a more complex and deeper decoder (a 6-layer transformer with a hidden dimension of 128 and 4 attention heads). The larger decoder does not result in improved downstream performance, demonstrating the adequacy of a simple decoder in enabling the training of our encoder. Removing the orthogonality or L1 loss leads to improved performance on the binary Stress ID task but reduced performance on the multiclass Stress ID task; in addition, this trade-off is associated with decreased model interpretability.\n\nIn Figure  7  we show that these design choices also affect the learned codebook representations. Without orthogonality loss, the regions of interest captured by different codewords largely overlap, leading to redundant templates. Without sparsity loss, the regions cover broad global areas of the face rather than focusing on local discriminative regions, which reduces interpretability. Finally, with only one quantizer, the model fails to capture diverse facial patterns; faces become non-decomposable to a combination of templates, limiting representational capacity to a single global template per face. We also plot the percentile curve representing the distribution of vertex displacements between the learned facial codebook mesh and the neutral mesh (Figure  8 ), as detailed in Section 3.3. This visualization illustrates the number of vertices that undergo a given amount of displacement, providing insight into the variability of the rendered vertices in the learned facial templates. Notably, our method consistently yields the lowest curve, indicating that it produces significantly fewer vertices with large displacements compared to the other two ablation settings. This result demonstrates that our rendered mesh is substantially less scattered.\n\nIn Table  8 , we evaluate the impact of various components on the orthogonality of the learned representations. Specifically, we assess the similarity between the displacement vectors of facial templates associated with each codeword. To quantify this, we compute both the dot product and cosine similarity for all pairs of codewords in the codebook, reporting the average values. Higher scores indicate greater similarity (i.e., more redundant templates), whereas lower scores reflect increased diversity among the templates. Our method achieves the lowest average dot product and cosine similarity, showing lower redundancy and more unique facial templates compared to configuration without sparsity loss.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B. Visualization Of All Learned Facial Templates",
      "text": "We provide a visualization of all learned facial templates in Figure  9 . Our system successfully captures high-frequency facial movements, including both symmetric and asymmetric motions. This stands in contrast to existing automated Action Unit tools, which are trained on symmetric annotations and thus tend to be biased toward decoding only symmetric facial motions.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Given a face image, our goal is to decompose the expression",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of our proposed expression coding framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: Some examples of deformation heatmap.",
      "page": 3
    },
    {
      "caption": "Figure 3: Some expression templates discovered by our system.",
      "page": 4
    },
    {
      "caption": "Figure 4: Example facial images and their corresponding token",
      "page": 4
    },
    {
      "caption": "Figure 7: As shown, it is often difficult to determine",
      "page": 4
    },
    {
      "caption": "Figure 3: , we show several example expressions",
      "page": 4
    },
    {
      "caption": "Figure 4: illustrates how the expression of a given",
      "page": 4
    },
    {
      "caption": "Figure 5: Qualitative retrieval examples comparing our token-based",
      "page": 6
    },
    {
      "caption": "Figure 5: demonstrate that our VQ-VAE-based represen-",
      "page": 6
    },
    {
      "caption": "Figure 6: Top-4 discriminative templates for smile classification.",
      "page": 7
    },
    {
      "caption": "Figure 7: Impact of design choices on learned codewords. Without orthogonality loss, codewords capture overlapping regions, resulting",
      "page": 11
    },
    {
      "caption": "Figure 7: we show that these design choices also affect",
      "page": 12
    },
    {
      "caption": "Figure 8: ), as detailed in Section",
      "page": 12
    },
    {
      "caption": "Figure 9: Our system successfully captures high-frequency",
      "page": 12
    },
    {
      "caption": "Figure 8: Percentile curve of displacement points distribution. Shows percentage of points with displacements grater than current value",
      "page": 13
    },
    {
      "caption": "Figure 9: Visualization of the learned facial templates.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure3.Someexpressiontemplatesdiscoveredbyoursystem.\nformulationintroducedintheoriginalVQ-VAEframework\n[46]. Thereconstructionlossensuresfidelitybetweenthe\ninput and the output, while the commitment loss encour-\nagestheencoderoutputstoremainclosetotheirassigned\ncodebookvectors:": "",
          "Column_2": "ystem.\nmework\neenthe\nencour-\nssigned"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "no orthogonal\nloss",
          "Column_2": "",
          "Column_3": "no sparse\nloss",
          "Column_4": "one quantizer",
          "Column_5": "ours",
          "Column_6": ""
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic deception detection in rgb videos using facial action units",
      "authors": [
        "Danilo Avola",
        "Luigi Cinque",
        "Gian Foresti",
        "Daniele Pannone"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Conference on Distributed Smart Cameras"
    },
    {
      "citation_id": "2",
      "title": "Measuring facial expressions by computer image analysis",
      "authors": [
        "Marian Stewart",
        "Joseph Hager",
        "Paul Ekman",
        "Terrence Sejnowski"
      ],
      "year": "1999",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "3",
      "title": "The youtube lens: Crowdsourced personality impressions and audiovisual analysis of vlogs",
      "authors": [
        "Joan-Isaac Biel",
        "Daniel Gatica-Perez"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Masked autoencoder for facial video representation learning",
      "authors": [
        "Zhixi Cai",
        "Shreya Ghosh",
        "Kalin Stefanov",
        "Abhinav Dhall",
        "Jianfei Cai",
        "Hamid Rezatofighi",
        "Reza Haffari",
        "Munawar Hayat",
        "Marlin"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "5",
      "title": "Libreface: An open-source toolkit for deep facial expression analysis",
      "authors": [
        "Di Chang",
        "Yufeng Yin",
        "Zongjian Li",
        "Minh Tran",
        "Mohammad Soleymani"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "6",
      "title": "Stressid: a multimodal dataset for stress identification",
      "authors": [
        "Hava Chaptoukaev",
        "Valeriya Strizhkova",
        "Michele Panariello",
        "Bianca Dalpaos",
        "Aglind Reka",
        "Valeria Manera",
        "Susanne Thümmler",
        "Esma Ismailova",
        "Massimiliano Todisco",
        "Maria Zuluaga"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Emote aloud during learning with autotutor: Applying the facial action coding system to cognitive-affective states during learning",
      "authors": [
        "Scotty D Craig",
        "Sidney D' Mello",
        "Amy Witherspoon",
        "Art Graesser"
      ],
      "year": "2008",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "8",
      "title": "Emoca: Emotion driven monocular face capture and animation",
      "authors": [
        "Radek Daněček",
        "Michael Black",
        "Timo Bolkart"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "10",
      "title": "Coding, analysis, interpretation, and recognition of facial expressions",
      "authors": [
        "Irfan Essa",
        "Alex Paul"
      ],
      "year": "1997",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "11",
      "title": "Learning an animatable detailed 3d face model from in-thewild images",
      "authors": [
        "Yao Feng",
        "Haiwen Feng",
        "Michael Black",
        "Timo Bolkart"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (ToG)"
    },
    {
      "citation_id": "12",
      "title": "Computation of component image velocity from local phase information",
      "authors": [
        "J David",
        "Allan Fleet",
        "Jepson"
      ],
      "year": "1990",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "13",
      "title": "Beamer: Behavioral encoder to generate multiple appropriate facial reactions",
      "authors": [
        "Ximi Hoque",
        "Adamay Mann",
        "Gulshan Sharma",
        "Abhinav Dhall"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "15",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2006",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "A systematic cross-corpus analysis of human reactions to robot conversational failures",
      "authors": [
        "Dimosthenis Kontogiorgos",
        "Minh Tran",
        "Joakim Gustafson",
        "Mohammad Soleymani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "Kuei Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "18",
      "title": "Learning a model of facial shape and expression from 4d scans",
      "authors": [
        "Tianye Li",
        "Timo Bolkart",
        "J Michael",
        "Hao Black",
        "Javier Li",
        "Romero"
      ],
      "year": "2017",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "19",
      "title": "Automated facial expression recognition based on facs action units",
      "authors": [
        "Takeo James J Lien",
        "Jeffrey Kanade",
        "Ching-Chung Cohn",
        "Li"
      ],
      "year": "1998",
      "venue": "Proceedings third IEEE international conference on automatic face and gesture recognition"
    },
    {
      "citation_id": "20",
      "title": "Norface: Improving facial expression analysis by identity normalization",
      "authors": [
        "Hanwei Liu",
        "Rudong An",
        "Zhimeng Zhang",
        "Bowen Ma",
        "Wei Zhang",
        "Yan Song",
        "Yujing Hu",
        "Wei Chen",
        "Yu Ding"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Learning multi-dimensional edge featurebased au relation graph for facial action unit recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "22",
      "title": "Facial action unit detection and intensity estimation from self-supervised representation",
      "authors": [
        "Bowen Ma",
        "Rudong An",
        "Wei Zhang",
        "Yu Ding",
        "Zeng Zhao",
        "Rongsheng Zhang",
        "Tangjie Lv",
        "Changjie Fan",
        "Zhipeng Hu"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Online learning for matrix factorization and sparse coding",
      "authors": [
        "Julien Mairal",
        "Francis Bach",
        "Jean Ponce",
        "Guillermo Sapiro"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "24",
      "title": "Evidence for distinct facial signals of reward, affiliation, and dominance from both perception and production tasks",
      "authors": [
        "Adrienne Jared D Martin",
        "Wood",
        "T William",
        "Scott Cox",
        "Robert Sievert",
        "Eva Nowak",
        "Fangyun Gilboa-Schechtman",
        "Zachary Zhao",
        "Witkower",
        "Paula Andrew T Langbehn",
        "Niedenthal"
      ],
      "year": "2021",
      "venue": "Affective Science"
    },
    {
      "citation_id": "25",
      "title": "A model of the perception of facial expressions of emotion by humans: Research overview and perspectives",
      "authors": [
        "Aleix Martinez",
        "Shichuan Du"
      ],
      "year": "2012",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Can language models learn to listen?",
      "authors": [
        "Evonne Ng",
        "Sanjay Subramanian",
        "Dan Klein",
        "Angjoo Kanazawa",
        "Trevor Darrell",
        "Shiry Ginosar"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "authors": [
        "Hanchuan Peng",
        "Fuhui Long",
        "Chris Ding"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "29",
      "title": "Facial action units-based image retrieval for facial expression recognition",
      "authors": [
        "Trinh Thi",
        "Doan Pham",
        "Sesong Kim",
        "Yucheng Lu",
        "Seung-Won Jung",
        "Chee-Sun Won"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems",
      "authors": [
        "Ali Razavi"
      ],
      "year": "2019",
      "venue": "Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "Victoria F Abrevaya, Anastasios Roussos, Timo Bolkart, and Petros Maragos. 3d facial expressions through analysis-byneural-synthesis",
      "authors": [
        "George Retsinas",
        "P Panagiotis",
        "Radek Filntisis",
        "Danecek"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "33",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "34",
      "title": "Learning bases of activity for facial expression recognition",
      "authors": [
        "Evangelos Sariyanidi",
        "Hatice Gunes",
        "Andrea Cavallaro"
      ],
      "year": "1965",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Inequality-constrained 3d morphable face model fitting",
      "authors": [
        "Evangelos Sariyanidi",
        "Casey Zampella",
        "Robert Schultz",
        "Birkan Tunc"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Beyond facs: Data-driven facial expression dictionaries, with application to predicting autism",
      "authors": [
        "Evangelos Sariyanidi",
        "Lisa Yankowitz",
        "Robert Schultz",
        "John Herrington",
        "Birkan Tunc",
        "Jeffrey Cohn"
      ],
      "year": "2025",
      "venue": "Beyond facs: Data-driven facial expression dictionaries, with application to predicting autism",
      "arxiv": "arXiv:2505.24679"
    },
    {
      "citation_id": "37",
      "title": "A mathematical theory of communication",
      "authors": [
        "Claude Shannon"
      ],
      "year": "1948",
      "venue": "The Bell system technical journal"
    },
    {
      "citation_id": "38",
      "title": "Jaanet: Joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "39",
      "title": "Uncertain graph neural networks for facial action unit detection",
      "authors": [
        "Tengfei Song",
        "Lisha Chen",
        "Wenming Zheng",
        "Qiang Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Cluster ensembles-a knowledge reuse framework for combining multiple partitions",
      "authors": [
        "Alexander Strehl",
        "Joydeep Ghosh"
      ],
      "year": "2002",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "41",
      "title": "Emotion-color associations in the context of the face",
      "authors": [
        "Andrew Christopher A Thorstenson",
        "Adam Elliot",
        "David Pazda",
        "Dengke Perrett",
        "Xiao"
      ],
      "year": "2018",
      "venue": "Emotion"
    },
    {
      "citation_id": "42",
      "title": "Emotion-color associations in the context of the face",
      "authors": [
        "Christopher Thorstenson",
        "Andrew Elliot",
        "Adam Pazda",
        "David Perrett",
        "Dengke Xiao"
      ],
      "year": "2018",
      "venue": "Emotion"
    },
    {
      "citation_id": "43",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Dim: Dyadic interaction modeling for social behavior generation",
      "authors": [
        "Minh Tran",
        "Di Chang",
        "Maksim Siniukov",
        "Mohammad Soleymani"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Aaron Van Den",
        "Oriol Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Fgnet: Facial action unit detection with generalizable pyramidal features",
      "authors": [
        "Yufeng Yin",
        "Di Chang",
        "Guoxian Song",
        "Shen Sang",
        "Tiancheng Zhi",
        "Jing Liu",
        "Linjie Luo",
        "Mohammad Soleymani"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "48",
      "title": "A survey of affect recognition methods: audio, visual and spontaneous expressions",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2007",
      "venue": "Proceedings of the 9th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "49",
      "title": "Transformerbased multimodal information fusion for facial expression analysis",
      "authors": [
        "Wei Zhang",
        "Feng Qiu",
        "Suzhen Wang",
        "Hao Zeng",
        "Zhimeng Zhang",
        "Rudong An",
        "Bowen Ma",
        "Yu Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Understanding bag-of-words model: a statistical framework",
      "authors": [
        "Yin Zhang",
        "Rong Jin",
        "Zhi-Hua Zhou"
      ],
      "year": "2010",
      "venue": "International journal of machine learning and cybernetics"
    }
  ]
}