{
  "paper_id": "2508.05231v2",
  "title": "Fdc-Net: Rethinking The Association Between Eeg Artifact Removal And Multi-Dimensional Affective Computing",
  "published": "2025-08-07T10:19:16Z",
  "authors": [
    "Wenjia Dong",
    "Xueyuan Xu",
    "Tianze Yu",
    "Junming Zhang",
    "Li Zhuo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalogram (EEG)-based emotion recognition holds significant value in affective computing and braincomputer interfaces. However, in practical applications, EEG recordings are susceptible to the effects of various physiological artifacts. Current approaches typically treat denoising and emotion recognition as independent tasks using cascaded architectures, which not only leads to error accumulation, but also fails to exploit potential synergies between these tasks. Moreover, conventional EEG-based emotion recognition models often rely on the idealized assumption of \"perfectly denoised data\", lacking a systematic design for noise robustness. To address these challenges, a novel framework that deeply couples denoising and emotion recognition tasks is proposed for end-to-end noise-robust emotion recognition, termed as Feedback-Driven Collaborative Network for Denoising-Classification Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic collaborative mechanism between artifact removal and emotion recognition through: (1) bidirectional gradient propagation with joint optimization strategies; (2) a gated attention mechanism integrated with frequency-adaptive Transformer using learnable band-position encoding. Two most popular EEGbased emotion datasets (DEAP and DREAMER) with multidimensional emotional labels were employed to compare the artifact removal and emotion recognition performance between FDC-Net and nine state-of-the-art methods. In terms of the denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of 96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the emotion recognition task under physiological artifact interference, FDC-Net achieves emotion recognition accuracies of 82.3±7.1% on DEAP and 88.1±0.8% on DREAMER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are key drivers of human cognition and behavior, influencing not only decision-making and social interactions but also being closely linked to mental health  (Davidson 2003) . Since traditional emotion assessment methods rely on subjective reports, they are susceptible to individual differences in expression and social desirability biases  (Paulhus 1991) .\n\nIn contrast, Electroencephalogram (EEG)-based emotion recognition technology, with its objectivity, high temporal resolution, and sensitivity to emotional states, has gradually become a research focus in affective computing and brain-computer interface (BCI) fields. This technology has been successfully applied in auxiliary diagnosis of depression(de Aguiar Neto and Rosa 2019), development of intelligent human-computer interaction systems  (Frey et al. 2013) , and monitoring of drivers' emotional states  (Gamage, Kalansooriya, and Sandamali 2022) , providing an objective and reliable physiological basis for mental health assessment and affective computing  (Wang et al. 2022) .\n\nHowever, practically acquired EEG recordings inevitably suffer from contamination by various physiological artifacts, such as Electromyography (EMG), Electrooculography (EOG), and motion artifacts  (Shad, Molinas, and Ytterdal 2020) . These noises overlap with emotion-related neural activities in both frequency and time domains, significantly reducing the signal-to-noise ratio of raw signals. Existing EEG denoising methods, which mainly rely on techniques like independent component analysis  (Albera et al. 2012) , wavelet transform  (Adeli, Zhou, and Dadmehr 2003) ], and adaptive filtering  (Kher and Gandhi 2016) , could separate noise components from clean ones to a certain extent. However, these methods often require manual intervention and parameter adjustment, making them difficult to adapt to inter-individual variability. Recently, with the devel-opment of deep learning, several end-to-end denoising methods based on convolutional neural networks (CNN) and recurrent neural networks (RNN) have emerged, enabling automatic learning of complex mapping relationships between artifacts and effective signals, thus providing new solutions for EEG signal processing.\n\nCurrently, deep learning-based EEG denoising methods are mainly categorized into three types: (1) CNN was employed to capture local time-frequency features and performs well in end-to-end denoising but may ignore longrange dependencies  (Sun et al. 2020 ); (2) RNN was utilized to model temporal dependencies but faces gradient vanishing issues when processing long sequences  (Yang et al. 2020 ); (3) and Transformers could achieve global context modeling through self-attention mechanisms  (Chen et al. 2023) , showing advantages in joint denoising and classification tasks but with high computational complexity. Additionally, in terms of emotion recognition models, hybrid architectures (e.g., CNN-RNN or CNN-Transformer) have become mainstream  (Zamani, Wulansari et al. 2021) . By combining local feature extraction and temporal modeling capabilities, they could achieve state-of-the-art performance in the EEG-based emotion recognition task  (Yao et al. 2024 ). Nevertheless, these methods mostly optimize denoising and classification modules independently, failing to fully exploit the synergistic effects between dual tasks, and there remains room for improvement in noise robustness.\n\nTo address these key challenges, as shown in Fig.  1 , this paper proposes a novel Feedback-Driven Collaborative Network for Denoising-Classification Nexus (FDC-Net). Its core innovation is deep coupling denoising and emotion recognition tasks through a bidirectional feedback learning mechanism, enabling end-to-end noise-robust emotion recognition. Specifically, FDC-Net uses a bidirectional gradient propagation and joint optimization strategy: the denoising module adaptively retains discriminative features based on emotional semantic information fed back by the emotion recognition task, while the classification module adjusts its robustness to noise via classification loss. Notably, we also design a frequency-adaptive Transformer with learnable frequency-band positional encoding (EEGSPTransformer), which optimizes feature weights of different frequency bands in the joint task via feedback. Experimental results show that under severe noise interference, FDC-Net outperforms traditional cascaded methods in multi-dimensional emotion recognition performance. This work provides a new paradigm for EEG analysis in noisy environments, and its collaborative framework could extend to other physiological signal processing fields.\n\nThe main contributions of this paper are as follows:\n\n• A dynamic feedback-driven collaborative learning framework for robust EEG-based emotion recognition is proposed. To the best of our knowledge, this is the first study to design a collaborative mechanism between EEG artifact removal and multi-dimensional emotion recognition tasks to achieve interaction via feedback. Through a joint optimization strategy, mutual promotion and performance improvement of the dual tasks are enabled.  significantly undermines their robustness and generalizability in noisy environments. In summary, existing methods tend to treat denoising and emotion recognition as independent objectives, neglecting their potential interplay. Such decoupling can inadvertently lead to the removal of emotion-relevant neural components (e.g., gamma-band oscillations) during denoising, thereby impairing emotion recognition performance when noise is present. To bridge this gap, we propose an endto-end joint learning framework that explicitly integrates a feature-interaction attention mechanism and simultaneously optimizes a multi-task loss function. FDC-Net synergistically enhances both artifact suppression and emotion recognition performance, offering a robust and practical solution for EEG-based affective computing in real-world settings.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We propose FDC-Net to synergistically handle EEG artifact removal and emotion recognition tasks through dualtask feedback interaction. Let X = {x i } N i=1 denotes the raw EEG signals where each sample x i ∈ R C×T contains C channels and T time points. The model takes noisy EEG signals x noisy ∈ R C×1×T as input, and outputs denoised signals xdenoised and emotion labels ŷ ∈ {0, 1} 2 (valence and arousal). The FDC-Net model comprises three components: the denoise module, the classify module, and the interaction of the dual path feature with feedback. Fig.  2  illustrates the FDC-Net architecture. Each module will be elaborated on in detail as follows.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Denoise Module",
      "text": "Transformer-based methods show promise for EEG denoising but face challenges like mismatched positional encoding, inadequate spatial modeling, and underdeveloped timefrequency collaboration. These limitations drive our proposal of an EEG-specific Transformer module (EEGSP Transformer) with innovative designs to achieve breakthroughs. Below is a detailed introduction to this module.\n\n1. Band-Limited PE: Based on the constraints of the physiological characteristics of EEG signals, we have redesigned the spectral distribution of positional encoding:\n\nwhere α k = Softmax(W ω k) denotes the learnable weight coefficient for each frequency band. This design strictly restricts frequency components to the effective EEG frequency band (4-45Hz) while preserving the multi-scale characteristics of positional encoding. It also introduces learnable weight parameters α k to enable dynamic selection and enhancement of key frequency bands, thereby achieving an optimal balance between spectral adaptability and feature expression capability. This mechanism not only avoids interference from invalid highfrequency noise but also adaptively enhances the information representation of feature bands (such as θ/α bands) that are closely related to emotion recognition.\n\n2. Channel-Aware Dynamic Gating: To make rational use of spatial relationships, we constructed a channel attention gate and a feedback mechanism to achieve implicit modeling of channel relationships.\n\n(1) Channel statistics extraction\n\n(2) Adaptive weight generation\n\n(3) Feature modulation\n\n3. Time-Frequency Attention: Multi-scale feature extraction is achieved by computing temporal and frequency domain attention in parallel:\n\nwhere\n\nThe temporal domain head processes the original signal to preserve transient features, and the frequency domain head analyzes rhythmic activities and employs DCT to achieve efficient transformation.\n\nThe decoder reconstructs clean EEG signals via transposed convolution:\n\nResidual learning is adopted to preserve high-frequency components, where the information h denoise from the denoising path.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Classify Module",
      "text": "The classification head processes features from the classification path:\n\nwhere h classify in the features are enhanced by channel attention. We propose an adaptive Binary Cross-Entropy (BCE) loss based on the inverse square weighting of label frequencies:\n\nwhere the weight coefficient w c = 1/ √ f c , f c is the class frequency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dual-Path Feature Interaction With Feedback",
      "text": "In traditional deep learning training, we observe that the gradient of the denoising task (∇L den ) conflicts with the gradient of the classification task (∇L cla ) in 30% of the parameter space. During the feature disentanglement process, orthogonal constraint experiments revealed that noise-related features and emotional features in EEG signals overlap by 15% in the frequency domain (specifically in the 4-8Hz theta band)  (Kendall, Gal, and Cipolla 2018) .\n\nTo address these issues, we propose an interactive feedback module that balances both tasks and achieves optimal results through task synergy. This module enables collaborative optimization of denoising and classification via bidirectional feature flows, with mathematical formulation:\n\nwhere t denotes iteration steps and Φ represents feature transformation functions.\n\nLet the output of the valence / arousal probability of the classifier be p ∈ R 2 , and the feedback projection process is:\n\nwhere x ∈ R D is the input high-dimensional feature vector, W e ∈ R d×D is the embedding weight matrix, b e is the bias term, z ∈ R d is the resulting low-dimensional representation, and D ≫ d ensures the reduction of dimensionality. The high-dimensional data is projected into a lowdimensional latent space using a weight matrix W e and the ReLU activation function, which retains key features while reducing computational complexity.\n\n2. Feature Enhancement:\n\nwhere ⊙ denotes the Hadamard (element-wise) product, σ(•) denotes the Sigmoid activation function (σ(x) = The feature interaction module processes inputs through two parallel paths, with the following specifications:\n\n(1) Denoising path: It employs position encoding based on EEG frequency band characteristics and Transformer layers to capture spectro-temporal patterns (within the 4-45Hz range).\n\n(2) Classification path: It utilizes a temporal attention mechanism to emphasize time segments associated with emotions.\n\nThe innovative feedback mechanism projects the classification results into the feature space and adjusts the encoder output:\n\nwhere W f ∈ R d×c , and in the dimensionality-reduced prediction, it is transformed into the feature dimension.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Joint Training:",
      "text": "The total loss combines the denoising Mean Squared Error (MSE) and the classification loss:\n\nThe experimental setup balances task weights with α = 0.6.\n\nDuring training, a curriculum learning strategy is employed, gradually reducing the Signal-to-Noise Ratio (SNR) from 3 dB to -3 dB.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments Datasets",
      "text": "We evaluated FDC-Net on two public datasets: DEAP and DREAMER. DEAP induces emotions via music videos, collecting 32-channel EEG (per 10-20 system) and peripheral signals from 32 participants. EEG was sampled at 512 Hz with a 0.5-45 Hz band-pass filter, annotated with 4 dimensions (valence, arousal, etc.) using 1-9 continuous ratings. DREAMER gathers 14-channel EEG (Emotiv EPOC+) from 23 participants watching emotional videos, sampled at 128 Hz with a 0.1-45 Hz filter, and annotated with 3 dimensions (valence, arousal, and dominance) using 1-5 discrete ratings. Experiments focused on valence and arousal, discretized into high/low binary classification tasks. Both datasets were expanded via 50% overlapping 128-length sliding windows, yielding 160,000 and 55,062 EEG segments for DEAP and DREAMER, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Noise Addition",
      "text": "In this study, we employed a composite noise addition approach to simulate the EEG signals with artifacts in realworld scenarios  (Lai et al. 2018) . Specifically, the noise signal was formed by mixing EMG and EOG signals at a 1:1 ratio. For the generation of EMG noise, one channel was randomly selected from the 2 EMG channels in the original data for sampling, and this process was repeated 32 times.\n\nThe same strategy was applied to generate EOG noise from the 2 EOG channels. To ensure precise control over noise intensity, we dynamically adjusted the noise amplitude coefficient λ noise according to the preset SNR by calculating the ratio of the root mean square values of the original EEG signal and the mixed noise. Furthermore, to enhance the robustness of the model, a small amount of Gaussian white noise (with a standard deviation of 0.01) was introduced in addition to the bioelectrical noise. All noise injection processes were completed before sliding window segmentation. This processing method not only preserves the temporal integrity of EEG signals but also effectively simulates complex interference conditions in actual acquisition environments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "Experiments were performed on the DEAP and DREAMER datasets, with data randomly split into 80% training and 20% test sets, and results averaged over 10 repetitions. Training used the AdamW optimizer (initial learning rate 0.001, batch size 32). Evaluation metrics included SNR, CC, and",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Denoising Methods",
      "text": "As reflected in the trend analysis (Fig.  3  and 4 ), across varying SNR levels of added noise on both DEAP and DREAMER datasets, the post-denoising SNR and CC curves of FDC-Net remain consistently high and exhibit a steady upward trend. The results indicates that FDC-Net maintains stable and effective noise suppression performance even under fluctuating noise intensities, outperforming comparative methods such as EEGINET and RNN LSTM in enhancing signal quality.\n\nTable  1  shows average result comparison of different denoising methods. As shown in Table  1 , in the average results across multiple SNR conditions, FDC-Net outperforms others in all key metrics. It achieves higher post-denoising SNR and CC, along with lower MSE, which further confirms its robust noise suppression capability. Furthermore, the waveform comparison in (Fig.  5 ) intuitively illustrates the superiority of FDC-Net. In Fig.  5",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With Classification Methods",
      "text": "As shown in the trend analysis (Fig.  6 ), whether under different SNR conditions of added noise or in the comparison between the two datasets, its classification accuracy is higher than that of comparative methods, especially at lower SNR.\n\nThe DMMR method has a certain noise suppression effect, and it can be seen from the results that the accuracy increases with the increase of initial SNR on DEAP. The accuracy of our method decreases to a certain extent as SNR increases, which implies that in our noisy data, partial noise still contains information useful for the downstream emotion classification task, and thus, denoising is not necessarily better when done \"cleaner\" to an excessive extent.\n\nIn the average classification results (Table  2 ), FDC-Net maintains higher accuracy, which can be attributed to its synergistic design: effective denoising provides cleaner input features for classification, while the classification task guides denoising to retain emotion-related information. Notably, the visualization of representations before and after classification (Fig.  7 ) clearly shows that FDC-Net generates more discriminative feature distributions, with distinct clustering of high/low valence/arousal classes, further validating its superiority in capturing emotion-related patterns.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Running Time And Parameter Size Analysis",
      "text": "Table  3  compares FDC-Net with other methods in running time and memory. In terms of computational efficiency, FDC-Net exhibits a trade-off between parameter size and running time. Due to the integration of a Transformer architecture to model complex spatiotemporal dependencies, its parameter size is relatively larger compared to lightweight methods (e.g., FCNN), which is a result of the Transformer's inherent need for sufficient capacity to capture long-range correlations. However, FDC-Net still maintains fast running speed. This is mainly due to the optimized design of its internal modules: the cross-modal interaction and feedback mechanisms avoid redundant computations, and the streamlined Transformer structure balances expressive power and efficiency. As shown in Table  3 , despite the larger parameter size, the running time of FDC-Net is comparable to or even faster than some comparative methods with similar performance, making it suitable for practical application scenarios requiring real-time processing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "To analyze the effectiveness of FDC-Net and collaborative architecture, we conducted ablation experiments on two datasets, and the results are shown in Table  5 . Table  4  presents the results of the ablation experiments. In the ablation experiments on the DEAP and DREAMER datasets, \"Origin\" (without ablation) serves as the performance baseline. When comparing the indicators after the ablation of modules such as \"abl cross\" and \"abl denoise\", there is a downward trend to varying degrees. This indicates that these modules play a positive role in maintaining data quality and model accuracy. In particular, the effects of the cross module and the EEGSPTrans module are more significant.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "For EEG-based emotion recognition, FDC-Net addresses noise removal and emotion recognition task collaboration via deep-coupled denoising-classification and dynamic collaboration. Bidirectional gradient optimization avoids cascaded error accumulation, and a gated Transformer with learnable encoding enables balanced synergy. The experimental results under heavy noise show superior performance, validating enhanced denoising/classification and offering a novel physiological signal processing paradigm.\n\nIn future work, we will further simplify the complexity of our model, and explore lightweight attention mechanisms and knowledge distillation to balance efficiency and performance. Additionally, we will extend FDC-Net to other physiological signals. Finally, we aim to integrate the framework with more EEG downstream tasks, such as clinical mood disorder diagnosis and cognitive load prediction, to amplify its practical utility.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of frameworks: (a) Existing EEG de-",
      "page": 1
    },
    {
      "caption": "Figure 2: The FDC-Net framework: (1) The denoising module processes noisy EEG using an encoder with an EEGSP Trans-",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of the denoising performance under",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparison of the denoising performance under",
      "page": 5
    },
    {
      "caption": "Figure 5: Comparison of noisy signals and denoised signals.",
      "page": 6
    },
    {
      "caption": "Figure 6: Comparison of the emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 7: Feature visualization: On the left are the represen-",
      "page": 6
    },
    {
      "caption": "Figure 3: and 4), across",
      "page": 6
    },
    {
      "caption": "Figure 5: ) intuitively illustrates the supe-",
      "page": 6
    },
    {
      "caption": "Figure 5: , Fig. 5(a) and Fig. 5 (b) are",
      "page": 6
    },
    {
      "caption": "Figure 5: (c) and Fig. 5(d) are selected from Fp1 electrode data",
      "page": 7
    },
    {
      "caption": "Figure 6: ), whether under dif-",
      "page": 7
    },
    {
      "caption": "Figure 7: ) clearly shows that FDC-Net generates",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "10\n8\nR\nN\nS 6\ndeveihcA\n4\n2\n0\n-3 -2 -1 0 1 2 3\nSNR of Noise Addition",
          "Column_2": "",
          "NR (b)CC\nFDC-Net\nEEGIFNET\nRNN-LSTM\nFCNN\nDMMR\n0 1 2 3\nNoise Addition ART\nSE\narison of the denoising performance under\no-noiseratiosonDEAP.\n1\n0.8": "",
          "Column_4": "",
          "Column_5": "1\n0.8"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "NR (b)CC\nFDC-Net\nEEGIFNET\nRNN-LSTM\nFCNN\nDMMR\n0 1 2 3\nNoise Addition ART\nSE\narison of the denoising performance under\no-noiseratiosonDEAP.\n1\n0.8": "",
          "Column_4": "",
          "Column_5": "0.6\nC\nC\n0.4\n0.2\n0\n-3 -2 -1 0 1 2 3\nSNR (dB) of Noise Addition"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Analysis of EEG records in an epileptic patient using wavelet transform",
      "authors": [
        "H Adeli",
        "Z Zhou",
        "N Dadmehr"
      ],
      "year": "2003",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "2",
      "title": "RNN-LSTM: From applications to modeling techniques and beyond-Systematic review",
      "authors": [
        "S Al-Selwi",
        "M Hassan",
        "S Abdulkadir",
        "A Muneer",
        "E Sumiea",
        "A Alqushaibi",
        "M Ragab"
      ],
      "year": "2024",
      "venue": "RNN-LSTM: From applications to modeling techniques and beyond-Systematic review"
    },
    {
      "citation_id": "3",
      "title": "ICA-based EEG denoising: a comparative analysis of fifteen methods",
      "authors": [
        "L Albera",
        "A Kachenoura",
        "P Comon",
        "A Karfoul",
        "F Wendling",
        "L Senhadji",
        "I Merlet"
      ],
      "year": "2012",
      "venue": "Bulletin of the Polish Academy of Sciences: Technical Sciences"
    },
    {
      "citation_id": "4",
      "title": "DHCT-GAN: Improving EEG signal quality with a dual-branch hybrid CNN-transformer network",
      "authors": [
        "Y Cai",
        "Z Meng",
        "D Huang"
      ],
      "year": "2025",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "Denosieformer: A transformer-based approach for single-channel EEG artifact removal",
      "authors": [
        "J Chen",
        "D Pi",
        "X Jiang",
        "Y Xu",
        "Y Chen",
        "X Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "6",
      "title": "Augmenting brain-computer interfaces with ART: An artifact removal transformer for reconstructing multichannel EEG signals",
      "authors": [
        "C.-H Chuang",
        "K.-Y Chang",
        "C.-S Huang",
        "A.-M Bessas"
      ],
      "year": "2025",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "7",
      "title": "A Dual-Branch Interactive Fusion Network to Remove Artifacts From Single-Channel EEG",
      "authors": [
        "H Cui",
        "C Li",
        "A Liu",
        "R Qian",
        "X Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "8",
      "title": "Affective neuroscience and psychophysiology: Toward a synthesis",
      "authors": [
        "R Davidson"
      ],
      "year": "2003",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "9",
      "title": "Depression biomarkers using non-invasive EEG: A review",
      "authors": [
        "F De Aguiar Neto",
        "J Rosa"
      ],
      "year": "2019",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "10",
      "title": "EmT: A novel transformer for generalized cross-subject EEG emotion recognition",
      "authors": [
        "Y Ding",
        "C Tong",
        "S Zhang",
        "M Jiang",
        "Y Li",
        "K Lim",
        "C Guan"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "11",
      "title": "Review of the use of electroencephalography as an evaluation method for human-computer interaction",
      "authors": [
        "J Frey",
        "C Mühl",
        "F Lotte",
        "M Hachet"
      ],
      "year": "2013",
      "venue": "Review of the use of electroencephalography as an evaluation method for human-computer interaction",
      "arxiv": "arXiv:1311.2222"
    },
    {
      "citation_id": "12",
      "title": "An emotion classification model for driver emotion recognition using electroencephalography (EEG)",
      "authors": [
        "T Gamage",
        "L Kalansooriya",
        "E Sandamali"
      ],
      "year": "2022",
      "venue": "2022 international research conference on smart computing and systems engineering (SCSE)"
    },
    {
      "citation_id": "13",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "A Kendall",
        "Y Gal",
        "R Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Adaptive filtering based artifact removal from electroencephalogram (EEG) signals",
      "authors": [
        "R Kher",
        "R Gandhi"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Communication and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Artifacts and noise removal for electroencephalogram (EEG): A literature review",
      "authors": [
        "C Lai",
        "H Ibrahim",
        "M Abdullah",
        "J Abdullah",
        "S Suandi",
        "A Azman"
      ],
      "year": "2018",
      "venue": "2018 IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE)"
    },
    {
      "citation_id": "16",
      "title": "CIT-EmotionNet: CNN interactive transformer network for EEG emotion recognition",
      "authors": [
        "W Lu",
        "H Ma",
        "T.-P Tan"
      ],
      "year": "2023",
      "venue": "CIT-EmotionNet: CNN interactive transformer network for EEG emotion recognition",
      "arxiv": "arXiv:2305.05548"
    },
    {
      "citation_id": "17",
      "title": "Measurement and control of response bias",
      "authors": [
        "D Paulhus"
      ],
      "year": "1991",
      "venue": "Measurement and control of response bias"
    },
    {
      "citation_id": "18",
      "title": "Fcnn: Fourier convolutional neural networks",
      "authors": [
        "H Pratt",
        "B Williams",
        "F Coenen",
        "Y Zheng"
      ],
      "year": "2017",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases"
    },
    {
      "citation_id": "19",
      "title": "DeepFilterNet: A low complexity speech enhancement framework for full-band audio based on deep filtering",
      "authors": [
        "H Schroter",
        "A Escalante-B",
        "T Rosenkranz",
        "A Maier"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Impedance and noise of passive and active dry EEG electrodes: a review",
      "authors": [
        "E Shad",
        "M Molinas",
        "T Ytterdal"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "21",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "A novel endto-end 1D-ResCNN model to remove artifact from EEG signals",
      "authors": [
        "W Sun",
        "Y Su",
        "X Wu",
        "X Wu"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "23",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "GCTNet: a graph convolutional transformer network for major depressive disorder detection based on EEG signals",
      "authors": [
        "Y Wang",
        "Y Peng",
        "M Han",
        "X Liu",
        "H Niu",
        "J Cheng",
        "S Chang",
        "T Liu"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "25",
      "title": "DMMR: Cross-subject domain generalization for EEG-based emotion recognition via denoising mixed mutual reconstruction",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "26",
      "title": "Decoding visual motions from EEG using attention-based RNN",
      "authors": [
        "D Yang",
        "Y Liu",
        "Z Zhou",
        "Y Yu",
        "X Liang"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "27",
      "title": "Emotion classification based on transformer and CNN for EEG spatial-temporal feature learning",
      "authors": [
        "X Yao",
        "T Li",
        "P Ding",
        "F Wang",
        "L Zhao",
        "A Gong",
        "W Nan",
        "Y Fu"
      ],
      "year": "2024",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "28",
      "title": "Hierarchical dynamic graph convolutional network with interpretability for EEG-based emotion recognition",
      "authors": [
        "M Ye",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "Hierarchical dynamic graph convolutional network with interpretability for EEG-based emotion recognition"
    },
    {
      "citation_id": "29",
      "title": "Emotion classification using 1D-CNN and RNN based on deap dataset",
      "authors": [
        "F Zamani",
        "R Wulansari"
      ],
      "year": "2021",
      "venue": "Nat. Lang. Process"
    },
    {
      "citation_id": "30",
      "title": "Beyond mimicking under-represented emotions: Deep data augmentation with emotional subspace constraints for EEG-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    }
  ]
}