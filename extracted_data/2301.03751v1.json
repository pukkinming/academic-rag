{
  "paper_id": "2301.03751v1",
  "title": "Generative Emotional Ai For Speech Emotion Recognition: The Case For Synthetic Emotional Speech Augmentation",
  "published": "2023-01-10T02:03:26Z",
  "authors": [
    "Abdullah Shahid",
    "Siddique Latif",
    "Junaid Qadir"
  ],
  "keywords": [
    "Tacotron",
    "WaveRNN",
    "speech synthesis",
    "text-to-speech",
    "emotional speech synthesis",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite advances in deep learning, current state-of-the-art speech emotion recognition (SER) systems still have poor performance due to a lack of speech emotion datasets. This paper proposes augmenting SER systems with synthetic emotional speech generated by an end-to-end text-to-speech (TTS) system based on an extended Tacotron architecture. The proposed TTS system includes encoders for speaker and emotion embeddings, a sequence-to-sequence text generator for creating Mel-spectrograms, and a WaveRNN to generate audio from the Mel-spectrograms. Extensive experiments show that the quality of the generated emotional speech can significantly improve SER performance on multiple datasets, as demonstrated by a higher mean opinion score (MOS) compared to the baseline. The generated samples were also effective at augmenting SER performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is a rapidly growing field with many applications in fields such as healthcare, customer service, media, education, and forensics. While deep learning (DL) has shown promise in developing SER systems, their performance is still limited by the scarcity of emotion datasets  [20, 24] . Existing SER corpora are small since the process of creating emotional data is costly and time-consuming, as multiple annotators have to manually listen to and annotate the material  [26, 31] . To increase data size, some studies have used multiple corpora, but the number of standard benchmark datasets is also limited, hindering progress in SER systems  [19] .\n\nResearchers have long been interested in creating naturalsounding TTS systems. TTS technology has come a long way from early TTS systems that often used pre-recorded waveforms pieced together based on input text  [11] . Such systems were prone to boundary artefact issues and statistical techniques were later developed to generate smoothed audio features for the vocoder to synthesise speech  [37, 44] . More recently, end-to-end neural network-based approaches have been proposed that can synthesise more natural-sounding human speech  [3, 34] . Current state-of-the-art TTS systems are trained using DL algorithms in an end-to-end fashion, with popular models including Tacotron  [41] , Deepvoice  [3] , Fastspeech  [33, 34] , Fastpitch  [18] , to name a few.\n\nUnlike traditional systems, end-to-end TTS models can learn to generate a spectrogram directly from text without any complex pre-processing. These models, however, are currently only able to synthesise natural speech. Using generative DL techniques such as generative adversarial networks (GANs)  [8]  for emotional speech synthesis is also challenging, as it requires a large amount of time-aligned data of a single speaker speaking the same content in dif-ferent emotions and complex equations to guide the model in converting emotions using audio features. Some studies have achieved promising results in single-speaker emotional speech synthesis using TTS models  [17] , but the quality of synthetic speech in augmenting SER has not been evaluated.\n\nIn this paper, we propose a method for augmenting SER systems using an emotional text-to-speech (TTS) system and make two main contributions. Firstly, we develop an end-toend multi-speaker emotional TTS system that does not require any alignment of audio files for emotion conversion or complex pre-processing of input data. Inspired by the success of end-to-end TTS models, we adopt a similar architecture to Tacotron. We propose to use a condition encoder to control the speakers' voices and emotions in the output speech. We generate speaker voice feature vectors using the encoder network. These feature vectors are modulated with one of the encoded emotional feature representations. These modulated feature vectors are used to condition the Tacotron to synthesise speech in different speaker voices and emotions. Subjective evaluation tasks show that our proposed model improves controllability and successfully synthesises emotional speech. Secondly, we use the synthesised emotional speech to augment an SER system and conduct multiple experiments to evaluate the generated data quantitatively. Results show that the synthesised data can help improve SER performance in both within-corpus and crosscorpus settings.\n\nThe rest of the paper is organised as follows. In Section 2, we briefly introduce the related work to change different features of audio. The model's architecture, loss functions, and flow of our architecture are described in Section 3. The details of the dataset and experimental condition in which we trained our model and hyper-parameters are provided in Section 4. We report our results in Section 5. Finally, this paper is concluded in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Work",
      "text": "In this section, we review the literature that has emerged around (1) the use of Tacotron for TTS, and for (2) emotional speech synthesis, and (3) the process of augmenting SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Tacotron Based Tts Systems",
      "text": "Many recent studies have focused on modifying the Tacotron model in order to better control the output of TTS systems. For instance,  [13]  presented a Tacotron-based model that synthesises multi-speaker speech by conditioning the Tacotron on the speaker's voice embedding, which was generated from a speaker verification model  [40] .  [42]  introduced a Tacotron variant that can change the speaking style, by learning different styles and saving them as vectors or tokens. These tokens are obtained by clustering similar accents and representing each cluster with an average. During synthesis, the Tacotron is conditioned on one of these tokens to produce speech with a specific style.  [35]  presented a multi-speaker Tacotron that can change accents (e.g., American, Indian, British). Their model uses two encoder networks with the Tacotron and requires two audio samples (one for the accent and one for the speaker's voice) as input to generate the desired output.  [36]  proposed a Tacotron model that is trained with encoded output audio from a variational autoencoder as input. This not only improves the multi-speaker performance of Tacotron but also allows for control over the energy of the generated audio through the mean-variance property of the variational autoencoder.  [43]  developed a Tacotron model that can learn more complex vocalisations by using the self-attention mechanism in Tacotron to learn complex dependencies related to pitch in different accents. They claim that their model outperforms traditional end-toend approaches for languages with more pitch-dependent accents, such as Japanese. Our proposed model also generates speech in a multi-speaker setting and includes additional control over the emotions in the output.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Tacotron Based Emotional Tts Systems",
      "text": "Several previous works have attempted to generate emotional speech using TTS systems. For example,  [38]  developed an emotion control method for a TTS system based on the GST-Tacotron network  [35] , and demonstrated its effectiveness in synthesising emotional speech in a single-speaker setting in Korean.  [27]  also evaluated a Tacotron-based emotional speech synthesizer in Korean, and found improvements in the quality of the generated speech for a single speaker. Other studies, such as  [15, 17] , have also proposed methods for controlling emotional speech synthesis, but these approaches only synthesise emotional speech in a single speaker's voice. In contrast, our proposed method achieves control over emotional speech synthesis for multi-speaker TTS and we also evaluate the quality of the synthesised data to augment the SER system.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Augmenting Techniques For Ser",
      "text": "Speed perturbation  [16]  is a popular data augmentation technique that has been widely studied in different contexts  [2, 23] . It has been found to improve speech emotion recognition (SER) performance by creating copies of input data with different speed effects. Mixup  [45]  is another data augmentation technique that generates augmented samples as a linear combination of original samples from the input data. Several studies have demonstrated the effectiveness of mixup in SER, including Latif et al.  [25] , who used the technique to augment an SER system and achieve improved performance and robustness. A recent method called SpecAugment  [30] , originally proposed for automatic speech recognition, has also been applied to SER  [4] . In this study, the authors augmented the SER system with duplicate samples by a factor of two and found that SpecAugment improved model performance. Other studies  [2, 21, 23]  have also achieved improved performance by using input perturbation-based data augmentation techniques to increase the training data.\n\nFurther research is required to explore data-driven approaches to increase the training data for SER. In this paper, we propose to explore TTS based data augmentation method where we explored different variations in the training data by changing the speaker and gender voices in different emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Framework",
      "text": "We propose to generate synthetic speech using a Tacotronbased emotional TTS system. We use synthetic speech data to augment the speech emotion classifier. The details of both emotional TTS and classifier are presented next.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotional Speech Synthesis",
      "text": "Our model consists of an encoder which conditions Tacotron (as depicted in Figure  1 ) to alter the speaker's voice and emotion in the output. Tacotron generates a Mel-spectrogram from a given text and embedding vector, while a Wave-RNNbased vocoder is used to generate an audio signal from the Mel-spectrogram",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Condition Encoder",
      "text": "We propose using a condition encoder to create an embedding that represents both speaker identity and emotion.\n\nTo do this, we use a speaker identification model presented in  [40] , which creates a fixed-dimensional embedding, known as a d-vector  [10, 39] , using a sequence of Mel-spectrograms computed from a speech signal of arbitrary length. We train this model using an end-to-end speaker verification loss that maximises the cosine similarity between utterances from the same speaker while minimising the cosine similarity between utterances from different speakers. We fine-tune this network on an emotional corpus to create an emotional embedding as well. Thus, the condition encoder is optimised to maximise the cosine similarity between embeddings of the same speaker with different emotions and to minimise the similarity between different emotions and different speakers. In this way, the model learns to generate a feature vector that contains both emotion and speaker identity information. The speaker's voice audio and emotion audio are embedded using the condition encoder and combined to generate a final embedding, which is used to condition the synthesizer to output speech with the selected emotion and speaker's voice.\n\nFor each unique emotion of every speaker in dataset, a centroid is calculated by taking the average of embedding for each unique emotion of every unique speaker. Loss for an embedding when the embedding and the centroid have the same speaker and emotion is calculated as:\n\nWhen have different emotion or different speaker for centroid then loss is calculated as:-\n\nEquation 1 maximises the cosine similarity between embeddings for the same speaker voice and same emotion. Equation 2 represents the cosine similarity between embedding and centroid when they have different speaker voices or different emotions or both. Equation 3 represents the final loss over every embedding, which is calculated as the sum of the loss for every embedding with every centroid.\n\nThe condition encoder consists of three LSTM layers with 768 cells each, and a final 256-length fully connected layer. The input to the model is the Mel-spectrogram generated from a speech utterance of a reference speaker's audio sample, and its output is an embedding vector of size 256 which represents the speaker's identity. After training the model, we use it to extract speaker and emotional information from a given audio. To separate the emotion from the speaker's voice, we generate vectors that only contain emotional information by using the trained condition encoder to generate embedding vectors for both the neutral and emotional voices of the same speaker. The neutral embedding vector is then subtracted from the emotional ones using Equation (4), resulting in a vector that only contains emotional information. This vector can be used at inference time to control the emotion of the synthesised audio.\n\nWhere en represents the embedding with emotion and voice information generated from the emotional voice of a speaker;\n\nneu is generated from neutral audio of the same speaker, and em represents the embedding that only contains emotional information. During inference, reference audio embedding (voice in which we want our output sample to be synthesised) and emotional embedding are added to generate a final embedding vector.\n\nFinally, the modulated embedding vector and text are fed to Tacotron, which generates the Mel-spectrograms. These Mel-spectrograms are converted to the time domain using a vocoder, resulting in an audio signal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Synthesizer Architecture",
      "text": "The synthesizer is a variation of Tacotron  [41] , which is a sequence-to-sequence model that generates output one frame at a time based on the input. In addition, we condition this synthesizer on an embedding vector generated by the condition encoder, which contains information about the desired output emotion and the speaker's voice. The condition embedding is concatenated with the text embedding of the synthesizer and then passed through a decoder to synthesise the output Mel-spectrogram. The synthesizer was trained on 80-channel Mel-spectrograms with a window size of 50 ms and a hop size of 12.5 ms. The synthesizer encodes the input characters into a hidden representation using three convolution layers, which learn longer-term context like an n-gram. The output of these convolution layers is passed to a single bi-directional LSTM layer with 256 units, which learns time dependencies from these n-gram-like features. The LSTM layer returns an encoded vector that fully represents the input text sequence. This vector is concatenated with a vector of emotional and speaker embeddings from the encoder.\n\nIt is worth noting that at this point, the encoder has been trained and its weights are not updated. The combined text, speaker, and emotion embedding is passed to the decoder to generate a Mel-spectrogram. The decoder architecture includes a location-sensitive attention mechanism that transforms the input embedding into a fixed-length vector. The output frame from the previous step is passed through two fully connected layers and concatenated with the embedding vector to ensure that sequences are generated without any time artefacts. This vector is then passed through two LSTM layers, and a linear transformation is applied to generate the next frame of the Mel-spectrogram. The output from this LSTM is also projected down to a single scalar, which serves as a stop token and indicates when to stop generating further frames. Once the Mel-spectrogram has been generated, it is passed through a 5-layer convolution network called the PostNet to improve overall reconstruction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vocoder",
      "text": "Traditionally, the Griffin-Lim algorithm  [9]  was used to generate time-domain audio from a spectrogram, but it was slow and the output speech lacked naturalness. To address this, we use a vocoder based on the WaveRNN architecture  [14] , which is a faster and more powerful recurrent network for sequential modelling of high-fidelity audio. It employs residual convolutions and GRU layers to generate a timedomain audio signal frame by frame from a Mel-spectrogram.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Classifier",
      "text": "To evaluate the synthesised emotions, we trained a deep neural network (DNN) for SER. We implemented a convolutional neural network (CNN)-based classifier that consists of a convolutional layer, a batch normalisation layer, and a dense layer before the softmax layer. Mel-frequency cepstral coefficients (MFCCs) are used as the input to the classifier. The CNN layers learn high-level features from the input features, which are then transformed by the dense layer into a more discriminative space for better emotion classification after passing through the normalisation layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Protocol",
      "text": "This section describes the details of the dataset, input feature, and model training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We used the Librispeech dataset  [29]  to train our TTS model. It consists of 1000 hours of speech data from various speakers, sampled at 16 kHz. For the emotion embeddings, we used the Emotional Voices Database (EVD)  [1]  and the Toronto Emotional Speech Set (TESS)  [7] , which contain six different speakers reading different sentences with different emotions. We conducted multiple experiments to evaluate the performance of our model. For emotion classification experiments, we used the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [28]  and TESS. For cross-corpus emotion classification, we used the CREMA-D  [6] , SAVEE  [12] , EmoDB  [5] , and synthesised audio. The details of these datasets are presented in Table  1 . We used one speaker from Librispeech, as well as all the speakers from EVD and TESS with two samples that were not included in the training set, to determine the mean opinion score. For emotion classification experiments, we use speakerindependent emotion classification. We randomly select 70% of CREMA-D for training, 10% for validation, and 20% for testing. The full corpora including RAVDESS and EmoDB were used as the test set in the emotion classification experiments, and the SAVEE dataset was used as the test set in the cross-corpus emotion classification experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Input Features",
      "text": "Tacotron takes text strings as input, which are sequences of characters. Each character is encoded into a one-hot encoded vector and embedded in a continuous vector. The other input to Tacotron is a condition embedding vector that contains speaker and emotion information. This vector is obtained from an encoder, which takes speaker audio as input and converts it into Mel-frequency cepstral coefficients (MFCCs). These MFCCs have 40 log filter banks, 80 frames, and no overlapping window. To generate t-distributed stochastic neighbour embedding (t-SNE) plots of synthesised audio, we encoded our synthesised audio using the model presented in  [13] . The input to this model is also MFCCs with 40 log filter banks, 80 frames, and no overlapping window, resulting in an 80x40-dimensional feature vector. This model is also used in evaluating the equal error rate (EER) in speaker verification. In emotion classification and cross-corpus emotion classification, we use MFCCs with 40 log filter banks and a hop size of 64 milliseconds. The MFCC array is transposed and the arithmetic mean is calculated across its horizontal axis as in a previous work  [32] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Synthesis Models Training",
      "text": "First, the encoder is trained on the Librispeech dataset to learn to generate a speaker embedding that is distinct for each speaker. It takes a Mel-spectrogram as input and outputs an embedding vector of size 256. From these embedding vectors, a similarity matrix is constructed such that each column contains an embedding vector for a unique speaker, and cosine similarity is maximised in all cells of the columns and minimised in all cells of the rows. Cosine similarity is maximised along the columns because they contain audio embeddings for the same person, whereas it is minimised along the rows because they contain audio embeddings for different people. In this way, the embeddings of the same people are similar and those of different people are different.\n\nAfter training the encoder on the Librispeech data, it is fine-tuned on the EVD and TESS datasets to generate distinct embedding vectors for different emotions. This time, a similarity matrix is constructed such that a column contains embedding vectors generated for a single emotion for the same speaker, and other emotions are placed in other columns. This is done for all speakers, and then cosine similarity is maximised along a column and minimised across columns. This is done to increase the distance between different emotions of the same person, so cosine similarity is minimised by adding it across columns rather than within the same columns. We used a batch size of 30 and a learning rate of 10 -4 .\n\nDuring training, the synthesizer model is first trained on the Librispeech data so that it can learn to generate audio of different speakers from a diverse range of text. This is because the EVD and TESS datasets combined only have six speakers. Once the synthesizer is trained enough that it can generate audio resembling the reference speaker, we finetune it to generate different emotional Mel-spectrograms by training it on the EVD and TESS datasets. We use a learning rate of 10 3 that exponentially decays to 10 -5 , and a batch size of 30 for training the synthesizer. The Adam optimiser with 1 = 0.9, 2 = 0.999, and = 10 -6 is used as the optimiser. The teacher forcing ratio is set to 1 (meaning the original previous sequence is shown to the model for prediction of the next sequence). The mean squared error is minimised for the predicted Mel-spectrogram.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In this section, we evaluate the performance of our proposed model in terms of the similarity of the synthesized speakers and the granularity of synthesized emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluating Synthetic Speech Quality",
      "text": "To evaluate the quality of synthetic speech, we conducted multiple experiments. The details of these experiments are presented below.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speaker Verification",
      "text": "We evaluated the speaker similarity of synthesised audios with real speech using speaker verification and measured the equal error rate (EER) following  [13] . The EER is used to measure the performance of a speaker verification system by comparing the false reject rate (FRR) and false accept rate (FAR) at different sensitivity levels. The EER is the point at which the FRR and FAR are equal. To calculate the EER, we used 100 audio samples, 40 of which were synthesised. We enrolled only synthesised speakers in the system and calculated the EER. We achieved an EER of 0.10% by performing voice conversion using a multi-speaker Tacotron model  [13] . We also generated emotional audio samples using a base model, and the speaker verification model gave an EER of 0.24% on these synthesised audios. In contrast, we achieved an EER of 0.16% when using the proposed model for both emotion and voice conversion. The EER on real samples using the approach in  [13]  was 0.04%. We have compared the EER of these models in Table  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Listening Experiments",
      "text": "We performed mean opinion score (MOS) evaluations to measure the quality of synthesised speech. We asked subjects with post-graduate exposure to give a score after listening to the audio based on the following standard: 1 = Bad; 2 = Poor; 3 = Fair; 4 = Good; and 5 = Excellent. The results, shown in Table  3 , indicate that the proposed model can synthesise high-quality emotional speech compared to the baseline model. The proposed model significantly improves the MOS score for emotions including angry, sad, and happy compared to the baseline. However, it achieves slightly lower MOS scores for natural speech compared to the baseline. This may be because the baseline model is specifically designed to generate natural speech and therefore performs better for neutral speech. Nevertheless, our proposed model performs well for all emotions. Readers can listen to samples of the generated speech at this URL  1  .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speaker And Emotion Visualisation",
      "text": "During this experiment, we did not use teacher forcing and generated audio as described in the inference part. The synthesised Mel-spectrograms for different emotions by the baseline and proposed models were plotted in Figure  2 , and the results were compared with the target Mel-spectrograms. In contrast to the baseline, our proposed model did not smooth the generated Mel-spectrograms that help produce a better quality of emotional speech using WaveRNN vocoder.\n\nFor the purpose of evaluation, we present the t-SNE plot, which was generated by embedding vectors generated from synthesised output samples using a speaker verification model as the encoder. Note that the speaker encoder was not trained with the synthesizer, so it is not optimised for synthesizer output. We generated t-SNE plots for emotional audio synthesised using the model from the base papers and compared the results with the proposed model. These t-SNE plots for synthesised speech in both male and female voices are shown in Figure  3  and 4 , respectively. These plots demonstrate that our model is able to synthesise distinct emotions compared to the base model. It can be observed that different emotions are separated and similar emotions are clustered together, indicating similarity between emotions.\n\nSince the angry emotion has more expression compared to the sad and happy emotions, which are tone variations, the cluster of angry emotions is farther from the happy emotions. We also visualise the t-SNE plot of multiple speakers in neutral speech using our proposed model in Figure  5 . It shows distinct clusters for different speakers indicating that the model is able to learn the multiple speaker embeddings effectively.\n\n.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Augmenting Speech Emotion Recognition (Ser)",
      "text": "In this section, we used the synthetic speech to augment the SER system. We performed our evaluations using corpus",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Within Corpus Evaluations",
      "text": "We used the RAVDESS and TESS datasets for evaluations. We combined both datasets and then randomly split the data into a ratio of 70:10:20 for train, validation, and test sets, respectively. We trained the model for 45 epochs. We compared the results for speaker recognition on real and synthesised speech in Figure  6 . We achieved an accuracy of 80% for synthesised speech, while the accuracy for the real speech test set was 92.4%. This demonstrates that our model can synthesise the emotional characteristics of output speech. We also augmented the classifier with synthetic data and performed training using both real and synthesised speech data. We achieved an accuracy of 94.6%, which is better compared to the classifier trained on real data alone. This experiment shows that our model can also be used to  generate additional audio data which can be used to augment speaker recognition systems to improve their performance. We have also plotted confusion matrices in Figure  7  for emotion classification on real audio, synthetic audio, and a combination of real and synthetic data in the training set. The confusion matrix shows that the model augmented with synthetic data is able to better classify speech emotions. The accuracy of other emotions has also been improved, but the most significant improvement can be seen in the classification of happy emotions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Cross-Corpus Corpus Evaluations",
      "text": "We also evaluated the effect of augmenting with synthetic data by performing cross-corpus emotion classification. To do this, we implemented a classifier consisting of an LSTM layer, three dense layers, and a softmax layer for emotion classification. We also used two dropout layers between dense layers to learn more generalised representations. We selected the architecture of the model based on previous research findings  [22, 23] . We trained the classifier on MFCC features extracted from the input audio. The model was trained with a sparse categorical cross-entropy loss and Adam optimiser for 100 epochs. The model was trained using the CREMA-D dataset and the CREMA-D dataset augmented with synthetic data and was evaluated on the CREMA-D, SAVEE, and EMODB datasets. The results, shown in Figure  8 , demonstrate that adding synthesised data increases accuracy not only on the SAVEE and EMODB datasets without fine-tuning the model but also on the CREMA-D test set as well.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Changing Gender And Speaker Distributions",
      "text": "In this experiment, we compare the results of data augmentation with new speaker voices that are not present in the given corpus. For instance, the SAVEE corpus has four male speakers, and synthetic data can be created either in the voices of these four male speakers or in the voices of additional male and female speakers to bring diversity to the data and augment speech emotion classification. We present the results in Table  4 . We compared the results with the baseline model, which was trained without any augmentation, and also with the application of speed perturbation to the training data. We followed  [19]  and created two copies of augmented samples using the speed perturbation data augmentation technique. We found that augmenting the data with different speaker voices helps improve performance compared to the baseline and the widely used data augmentation technique of speed perturbation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "This paper proposes to utilise an emotional text-to-speech (TTS) system to augment a speech emotion recognition (SER) system. We present a Tacotron-based multi-speaker emotional TTS system for synthetic speech generation in different speaker voices and use it for data augmentation in speech emotion recognition to improve performance. The results showed that the proposed TTS system can generate high-quality emotionally discriminative samples. When we augment the SER system with these augmented samples, we find that using synthetic data in different emotional voices   can help improve performance compared to the widely used speech data augmentation technique in SER. Our future work will focus on investigating the learning of a unified embedding for controlling style and emotions for all people, regardless of age, background, and gender.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) to alter the speaker’s voice and emo-",
      "page": 2
    },
    {
      "caption": "Figure 1: Architectural ﬂow diagram. The reference speaker’s voice is ﬁrst encoded and then modulated to desired emotion as",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of target and synthesized Mel-spectrograms for various emotions in Male and Female audios.",
      "page": 6
    },
    {
      "caption": "Figure 3: and 4, respectively. These plots demonstrate that",
      "page": 6
    },
    {
      "caption": "Figure 3: Comparison of t-SNE plots of male audio for various",
      "page": 6
    },
    {
      "caption": "Figure 6: We achieved an accuracy",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of t-SNE plots of female audio for var-",
      "page": 7
    },
    {
      "caption": "Figure 5: The t-SNE plot for speaker voice of synthesised re-",
      "page": 7
    },
    {
      "caption": "Figure 6: Bar plot which shows that our synthesized audio’s",
      "page": 7
    },
    {
      "caption": "Figure 8: , demonstrate that adding synthesised data increases accu-",
      "page": 7
    },
    {
      "caption": "Figure 7: Confusion matrix for the test set of real, synthetic, and combined synthetic and real audio. The addition of synthetic",
      "page": 8
    },
    {
      "caption": "Figure 8: Test results in cross-corpus setting, which shows",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Description of all the considered datasets. Speaker verification EERs of different synthesizers.",
      "data": [
        {
          "# of samples": "100",
          "EER": "0.16"
        },
        {
          "# of samples": "100",
          "EER": "0.24"
        },
        {
          "# of samples": "100",
          "EER": "0.10"
        },
        {
          "# of samples": "100",
          "EER": "0.04"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Description of all the considered datasets. Speaker verification EERs of different synthesizers.",
      "data": [
        {
          "Name": "CREMA-D",
          "Number of\nSpeakers": "91",
          "Number of\nUtterances": "7,442"
        },
        {
          "Name": "EmoDB",
          "Number of\nSpeakers": "10",
          "Number of\nUtterances": "535"
        },
        {
          "Name": "EVD",
          "Number of\nSpeakers": "5",
          "Number of\nUtterances": "7,590"
        },
        {
          "Name": "Librispeech",
          "Number of\nSpeakers": "2484",
          "Number of\nUtterances": "281,241"
        },
        {
          "Name": "REVDESS",
          "Number of\nSpeakers": "24",
          "Number of\nUtterances": "7,356"
        },
        {
          "Name": "SAVEE",
          "Number of\nSpeakers": "4",
          "Number of\nUtterances": "480"
        },
        {
          "Name": "TESS",
          "Number of\nSpeakers": "2",
          "Number of\nUtterances": "2,800"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Results using different distributions of synthetic data for speakers and gender",
      "data": [
        {
          "Dataset": "",
          "Accuracy (%)": "Speed perturbation\nMale spakers\nFemale speakers\nBoth female and\nBaseline\naugmentation\nsynthetic data\nsynthetic data\nmale synthetic data"
        },
        {
          "Dataset": "SAVEE",
          "Accuracy (%)": "65.4\n66.8\n68.2\n69.4\n72.3"
        },
        {
          "Dataset": "CREMA-D",
          "Accuracy (%)": "68.3\n70.1\n72.7\n72.9\n74.3"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "2",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "3",
      "title": "Deep voice: Real-time neural text-to-speech",
      "authors": [
        "S Arik",
        "M Chrzanowski",
        "A Coates",
        "G Diamos",
        "A Gibiansky",
        "Y Kang",
        "X Li",
        "J Miller",
        "A Ng",
        "J Raiman"
      ],
      "year": "2017",
      "venue": "Deep voice: Real-time neural text-to-speech",
      "arxiv": "arXiv:1702.07825"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in public speaking scenarios utilising an lstm-rnn approach with attention",
      "authors": [
        "A Baird",
        "S Amiriparian",
        "M Milling",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Toronto emotional speech set (tess)-younger talker_happy",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (tess)-younger talker_happy"
    },
    {
      "citation_id": "8",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Signal estimation from modified shorttime fourier transform",
      "authors": [
        "D Griffin",
        "J Lim"
      ],
      "year": "1984",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "End-to-end text-dependent speaker verification",
      "authors": [
        "G Heigold",
        "I Moreno",
        "S Bengio",
        "N Shazeer"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Unit selection in a concatenative speech synthesis system using a large speech database",
      "authors": [
        "A Hunt",
        "A Black"
      ],
      "year": "1996",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, IEEE"
    },
    {
      "citation_id": "12",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "13",
      "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
      "authors": [
        "Y Jia",
        "Y Zhang",
        "R Weiss",
        "Q Wang",
        "J Shen",
        "F Ren",
        "P Nguyen",
        "R Pang",
        "I Moreno",
        "Y Wu"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Efficient neural audio synthesis",
      "authors": [
        "N Kalchbrenner",
        "E Elsen",
        "K Simonyan",
        "S Noury",
        "N Casagrande",
        "E Lockhart",
        "F Stimberg",
        "A Oord",
        "S Dieleman",
        "K Kavukcuoglu"
      ],
      "year": "2018",
      "venue": "Efficient neural audio synthesis",
      "arxiv": "arXiv:1802.08435"
    },
    {
      "citation_id": "15",
      "title": "Emotional voice conversion using multitask learning with text-to-speech",
      "authors": [
        "T Kim",
        "S Cho",
        "S Choi",
        "S Park",
        "S Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "17",
      "title": "An effective style token weight control technique for end-to-end emotional speech synthesis",
      "authors": [
        "O Kwon",
        "I Jang",
        "C Ahn",
        "H Kang"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "18",
      "title": "Fastpitch: Parallel text-to-speech with pitch prediction",
      "authors": [
        "A Łańcucki"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Deep representation learning for improving speech emotion recognition. Doctoral Consortium",
      "authors": [
        "S Latif"
      ],
      "year": "2020",
      "venue": "Deep representation learning for improving speech emotion recognition. Doctoral Consortium"
    },
    {
      "citation_id": "20",
      "title": "2022a. A survey on deep reinforcement learning for audiobased applications",
      "authors": [
        "S Latif",
        "H Cuayáhuitl",
        "F Pervez",
        "F Shamshad",
        "H Ali",
        "E Cambria"
      ],
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "21",
      "title": "Federated learning for speech emotion recognition applications",
      "authors": [
        "S Latif",
        "S Khalifa",
        "R Rana",
        "R Jurdak"
      ],
      "year": "2020",
      "venue": "2020 19th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)"
    },
    {
      "citation_id": "22",
      "title": "Unsupervised adversarial domain adaptation for cross-lingual speech emotion recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "2019 8th international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "23",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Direct Modelling of Speech Emotion from Raw Speech",
      "doi": "10.21437/Interspeech.2019-3252"
    },
    {
      "citation_id": "24",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "26",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Emotional end-to-end neural speech synthesizer",
      "authors": [
        "Y Lee",
        "A Rabiee",
        "S Lee"
      ],
      "year": "2017",
      "venue": "Emotional end-to-end neural speech synthesizer",
      "arxiv": "arXiv:1711.05447"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "30",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio"
    },
    {
      "citation_id": "32",
      "title": "Emo-tions understanding model from spoken language using deep neural networks and mel-frequency cepstral coefficients",
      "authors": [
        "M De Pinto",
        "M Polignano",
        "P Lops",
        "G Semeraro"
      ],
      "year": "2020",
      "venue": "2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)"
    },
    {
      "citation_id": "33",
      "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
      "authors": [
        "Y Ren",
        "C Hu",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T Liu"
      ],
      "year": "2020",
      "venue": "Fastspeech 2: Fast and high-quality end-to-end text to speech"
    },
    {
      "citation_id": "34",
      "title": "Fastspeech: Fast, robust and controllable text to speech",
      "authors": [
        "Y Ren",
        "Y Ruan",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T Liu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Towards end-toend prosody transfer for expressive speech synthesis with Tacotron",
      "authors": [
        "R Skerry-Ryan",
        "E Battenberg",
        "Y Xiao",
        "Y Wang",
        "D Stanton",
        "J Shor",
        "R Weiss",
        "R Clark",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Towards end-toend prosody transfer for expressive speech synthesis with Tacotron",
      "arxiv": "arXiv:1803.09047"
    },
    {
      "citation_id": "36",
      "title": "Fullyhierarchical fine-grained prosody modeling for interpretable speech synthesis",
      "authors": [
        "G Sun",
        "Y Zhang",
        "R Weiss",
        "Y Cao",
        "H Zen",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Speech parameter generation algorithms for HMMbased speech synthesis",
      "authors": [
        "K Tokuda",
        "T Yoshimura",
        "T Masuko",
        "T Kobayashi",
        "T Kitamura"
      ],
      "year": "2000",
      "venue": "2000 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 00CH37100"
    },
    {
      "citation_id": "38",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Deep neural networks for small footprint textdependent speaker verification",
      "authors": [
        "E Variani",
        "X Lei",
        "E Mcdermott",
        "I Moreno",
        "J Gonzalez-Dominguez"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Generalized endto-end loss for speaker verification",
      "authors": [
        "L Wan",
        "Q Wang",
        "A Papir",
        "I Moreno"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Tacotron: Towards end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Tacotron: Towards end-to-end speech synthesis",
      "arxiv": "arXiv:1703.10135"
    },
    {
      "citation_id": "42",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R Skerry-Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "F Ren",
        "Y Jia",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "arxiv": "arXiv:1803.09017"
    },
    {
      "citation_id": "43",
      "title": "Investigation of enhanced Tacotron text-to-speech synthesis systems with selfattention for pitch accent language",
      "authors": [
        "Y Yasuda",
        "X Wang",
        "S Takaki",
        "J Yamagishi"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Statistical parametric speech synthesis",
      "authors": [
        "H Zen",
        "K Tokuda",
        "A Black"
      ],
      "year": "2009",
      "venue": "Statistical parametric speech synthesis"
    },
    {
      "citation_id": "45",
      "title": "International Conference on Learning Representations",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    }
  ]
}