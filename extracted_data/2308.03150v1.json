{
  "paper_id": "2308.03150v1",
  "title": "\"We Care\": Improving Code Mixed Speech Emotion Recognition In Customer-Care Conversations",
  "published": "2023-08-06T15:56:12Z",
  "authors": [
    "N V S Abhishek",
    "Pushpak Bhattacharyya"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is the task of identifying the emotion expressed in a spoken utterance. Emotion recognition is essential in building robust conversational agents in domains such as law, healthcare, education, and customer support. Most of the studies published on SER use datasets created by employing professional actors in a noise-free environment. In natural settings such as a customer care conversation, the audio is often noisy with speakers regularly switching between different languages as they see fit. We have worked in collaboration with a leading unicorn in the Conversational AI sector to develop Natural Speech Emotion Dataset (NSED). NSED is a natural code-mixed speech emotion dataset where each utterance in a conversation is annotated with emotion, sentiment, valence, arousal, and dominance (VAD) values. In this paper, we show that by incorporating word-level VAD value we improve on the task of SER by 2%, for negative emotions, over the baseline value for NSED. High accuracy for negative emotion recognition is essential because customers expressing negative opinions/views need to be pacified with urgency, lest complaints and dissatisfaction snowball and get out of hand. Escalation of negative opinions speedily is crucial for business interests. Our study then can be utilized to develop conversational agents which are more polite and empathetic in such situations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Conversational agents which can participate in a dialogue effectively have massive applications across multiple domains.  Mensio et al. (2018)  discussed three steps of evolution for conversational agents: textual interaction, vocal interaction and embodied interaction. Recently, OpenAI released ChatGPT, a multi-lingual textual conversational model based on the large language model (LLM) GPT 3.5. Chat-GPT can \"answer follow-up questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests\" effectively while retaining knowledge from the conversational context as well as the pre-training phase  (Bang et al., 2023) . ChatGPT has outperformed state-of-the-art LLMs for various tasks in the zero-shot setting. It was found that, through interactivity, one can improve the performance of ChatGPT by 8% ROUGE-1 on summarization tasks and 2% ChrF++ on the machine translation tasks  (Bang et al., 2023) . With the integration of interactability, ChatGPT has leaped over traditional LLMs with applications across several domains such as law, healthcare, finance and education.\n\nIn many situations, conversation through the speech modality is favorable and convenient as compared to the textual modality. ChatGPT, while a great conversational agent, can only work with the textual modality. A conversational agent which can take speech input and give speech responses that are polite and empathetic, in an end-to-end fashion, is the next phase of evolution for interactive chatbots.\n\nConversational agents such as ChatGPT need to recognize the emotion of the human interlocutor correctly in order to give responses which are polite and empathetic in nature. Emotion recognition, when done efficiently by chatbots, make the conversations more human-like. Speech emotion recognition is an important sub-task while developing speech-to-speech chatbots.\n\nOur specific problem statement is to solve Speech Emotion Recognition (SER) where the input is the raw audio of a spoken utterance in a dyadic conversation and the output is its corresponding emotion label, valence, arousal and dominance for a natural code-mixed speech dataset. Speech Emotion Recognition (SER) is the task of identifying the emotion of a spoken utterance. Dimensional models plot emotions across the three dimensions of arousal, dominance and valence. Arousal, valence and dominance signify the intensity, polarity and control exerted by an emotion, respectively.\n\nFor example, anger has high arousal, negative valence and high dominance whereas fear has low arousal, negative valence and low dominance. Categorical models define discrete emotion classes such as anger, happy and sad for various downstream tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Our Contributions Are:",
      "text": "1. A model trained on a natural code-mixed speech emotion dataset, Natural Speech Emotion Dataset (NSED), for the task of Speech Emotion Recognition (SER). NSED has over 5000 conversational utterances annotated for emotion, sentiment, valence, arousal, and dominance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Technique Of Incorporating Word-Level",
      "text": "VAD values to improve the performance for SER by 2% for negative emotions, in an industry setting. High accuracy for negative emotion recognition is essential, because customers expressing negative opinions/views need to be pacified with urgency, lest complaints and dissatisfaction snowball and get out of hand. Escalation of negative opinions speedily is crucial for business interests.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Motivation",
      "text": "SER has been an important yet challenging task for researchers. Whenever there is a human-machine interaction in environments where only speech can be propagated, SER becomes a key step for the machine to generate an appropriate response. The task of Emotion Recognition in Conversation (ERC) has many controlling variables such as the context, topic, argumentation logic and speaker/listener personalities, describe the emotional states of the interlocutors.\n\nA recent study  (Catania and Garzotto, 2022)  explored the benefits of using an emotion-aware chatbot to help people with alexithymia, a condition which makes it difficult to understand and express emotions. Alexithymia is common in people with neurodevelopmental disorders (NDD).\n\nThe chatbot provided different utterances to the users and asked them to imitate those utterances by inducing some kind of emotion such as joy or anger. It was found that the interaction with the chatbot became more straightforward as users acquired familiarity: 17 of the 19 participants could perform all emotional activities with progressively decreasing help from the facilitator.\n\nMost of the SER datasets available today are created by employing professional actors in a clean noise-free environment. In a natural setting, conversations are impromptu, often involving frequent code-mixing and code-switching between multiple languages such as Hindi, English, Marathi, etc. In a customer care setting, it is essential for conversational agents to be polite and empathetic in response to the emotion expressed by the customer. This leads to better overall customer satisfaction and customer retention rates.\n\nOur industry-partner is a unicorn company in the Conversational AI sector which empowers over 45000 businesses across the world through their conversational messaging platform. This platform helps businesses engage with customers effectively across commerce, marketing and support with over 9 Billion messages per month. Their mission is to \"build the most advanced and innovative platform for conversational engagement with a focus on delivering customer delight\".\n\nWe are collaborating with them to work on speech emotion recognition. Through our discussions with them, we explored various ways to approach this problem. They gave us a clear picture of the real-world challenges that are existent in the conversational AI sector. Some of the major challenges are: frequent code-mixing, low-quality recordings and a lack of annotated natural conversational datasets.\n\nAs we will discuss further, the dataset annotated for our experiments, NSED, contains customer care conversations from the escalation department of a customer care service. High accuracy for negative emotion recognition is essential, because customers expressing negative opinions/views need to be pacified with urgency, lest complaints and dissatisfaction snowball and get out of hand. Escalation of negative opinions speedily is crucial for business interests. This tells us that a speech emotion recognition model operating for the escalation department should be very good in detecting negative emotions in conversations.\n\nAn SER model which is capable of capturing contextual information well and is robust to the variations introduced by a natural code-mixed conversation dataset needs to be developed. This model then can be utilised in making speech-tospeech conversational agents more polite and empathetic in an escalation department setting. Figure  1  depicts the importance of emotion recognition while developing emotion aware conversational agents.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Traditionally acoustic speech features have been used along with a statistical machine-learning model for the task of SER  (Schuller et al., 2003) . However, selecting the appropriate combination of these low-level features for any given task demands a lot of domain knowledge. Pre-trained deep learning based models trained for other speech processing tasks such as ASR were fine-tuned for SER to get better results  (Lu et al., 2020) . Recently, self-supervised techniques such as Wav2Vec 2.0  (Baevski et al., 2020)  have emerged which learn appropriate speech representations automatically for speech recognition. In  Pepino et al. (2021)  learned speech representations from Wav2Vec 2.0 are utilized in a downstream model for speech emotion recognition. The proposed model outperformed the state-of-the-art for IEMOCAP  (Busso et al., 2008)  and RAVDESS (Livingstone and Russo, 2018) datasets. The study also showed that combining low-level acoustic features with the Wav2Vec 2.0 speech representations resulted in performance gains. In  Poria et al. (2019)  it was shown that detecting an emotional shift in conversations is still a bottleneck for SER. In  Tian et al. (2015)  non-verbal features were combined with low-level descriptors to improve the performance of emotion recognition in dialogue conversations of the IEMOCAP dataset. In Vaudable and Devillers (2012) the impact of negative emotions on the quality of a call center dialogue was investigated.\n\nA study has shown that including dialogue features such as turn number, the topic of discussion ad customer/agent response time can significantly improve the performance of text-based emotion recognition systems  (Herzig et al., 2016)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modeling",
      "text": "Our model can be mathematically represented using the below argmax equation.\n\nHere, E * is the emotion class that maximizes the probability function given a feature set, < F >, word-level VAD values of an utterance, < V AD >.\n\nOur work aims to show that including the feature set < V AD > improves the performance of SER for a natural code-mixed dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Block Diagram And Architecture",
      "text": "In Figure  2 , the overall architecture of the proposed technique is presented. Speech-based features are extracted using the Wav2Vec2 model. Textual features are extracted from the ASR transcripts using the multilingual-BERT model. Word-level valence, arousal, and dominance (VAD) values are extracted from the ASR transcripts using the NRG-VAD lexicon. All these features, once extracted, are fused together and fed into a BiLSTM model. Then a fully-connected layer along with the softmax layer is used to finally generate the predicted emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "Customer care conversations were recorded and annotated for emotion recognition. The annotation methodology followed is described below.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Natural Call Center Speech Emotion Dataset",
      "text": "Natural Speech Emotion Dataset (NSED) is a codemixed dyadic customer care conversation dataset created in collaboration with our industry partner. Below are the steps followed to create this dataset.\n\n• Data Recording: Our industry partner provided us with over 18000 dyadic customer care audio recordings with duration ranging between a few seconds to about an hour and their corresponding machine-generated text transcripts. All the audio recordings were single-channel (mono) with a sampling rate of 8000Hz. The conversations are interactions between a customer and a customer care executive from the complaint escalation team of a car servicing company. Both the speakers, in most of the audio recordings, switch between Hindi and English freely with some occasional use of regional words in languages such as Marathi.\n\n• Data Processing: Thirty audio recordings were chosen, each of which was 8-10 minutes long making a total of 4.5 hours long audio recordings. The audacity tool was used to process audio files. Each of these audio recordings was clipped into smaller audio clips corresponding to each speaking turn.\n\nA speaking turn is defined as the utterance corresponding to a particular speaker before and after any other speaker speaks. Each of these audio clips were then aligned with their corresponding machine-generated transcripts and were tagged with either \"customer\" or \"executive\" depending on who was speaking.\n\nThe machine-generated transcripts contained many crucial mistakes such as wrongly transcribing the word \"escalation\" as \"cancellation\". So, the transcripts were corrected, manually, in order to achieve a better quality of textual data. In some instances, the audio quality drops drastically, making it very difficult to understand the words that are being spoken.\n\nIn this case, a tag, <inaudible> is used in place of its transcript and further annotations are not performed.\n\n• Emotion Annotation: The emotion annotations were performed by a group of annotators with a graduate degree, proficient in both English and Hindi. The annotators worked in pairs to listen and annotate these clips with emotion (neutral, happy, sad, excited, anger, fear, surprised, frustrated, disgust), sentiment (neutral, positive, negative), valence, arousal and dominance (VAD). VAD values were annotated in a scale from 1 to 10 where (5, 5, 5) corresponds to the VAD values of a completely neutral emotion. For VAD, 1 represents the minimum value and 10 represents the maximum value any of the dimensions can have e.g. for valence, 1 represents the most negative and 10 represents the most positive any emotion can get. As we can represent 1000 emotions using the VAD dimensional model and only 9 using the categorical emotion model, not all utterances tagged as \"neutral\" will have VAD values of (5, 5, 5). For a subset of dataset, consisiting of 1989 utterances, annotated by pair of annotators, the inter-annotator agreement was found to be 0.33 and 0.37 for emotion and sentiment labels respectively by using the cohen-kappa metric of agreement.\n\n• Dataset Examples: Two of the examples from the annotated NSED dataset are given below:\n\nExample 1: The utterance is \"Do you want someone to get arrested? Haan?\" and its corresponding emotion, sentiment, valence, arousal and dominance are respectivelyanger, negative, 2, 8, 9.\n\nExample 2: The utterance is \"Mai samajhta hun aapko jo bhi problem hui hai. Aage se aapko ye nahi hoga nischint rahiye.\" and its corresponding emotion, sentiment, valence, arousal and dominance are respectively-neutral, positive, 6, 5, 5.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methodology",
      "text": "Text features, Wav2vec2 features, and word-level VAD values are extracted and fused together. Indic-Wav2Vec2 is used to extract speech features that constitute a 768-dimensional vector. Whisperlarge  (Radford et al., 2022)  is used to generate transcripts for each utterance in a conversation. The multi-lingual BERT model is used to generate textual embeddings for each utterance resulting in a 768-dimensional vector. The fused features are then passed through a BiLSTM layer and a fullyconnected layer. Finally, a softmax layer is used to predict the corresponding emotion for an utterance. Before extracting the speech features, the wav2vec2 architecture is continually pre-trained as described below.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pre-Training Wav2Vec2",
      "text": "Data annotation is a cost-intensive task that is not feasible to do for the whole unlabeled speech dataset (~18000 customer care audio files) provided by our industry partner. Wav2vec2 is a self-supervised speech model which learns speech representations from raw audio signals directly. These speech representations have been shown to be very useful for several speech-processing tasks. Wav2vec2 is pre-trained on 52,0000 hours of Librispeech dataset because of which it has already learned various characteristics of speech present in that dataset. To get even better representations for our dataset we apply a technique called continual pre-training where we continue the pre-training phase with our own unlabeled speech dataset.  Kessler et al. (2022)  show that using an adapter-based continual pre-training approach for the wav2vec2 architecture reduces computational cost significantly. We use a similar approach to pre-train the Wav2vec2 architecture using the unlabelled NSED dataset. After pre-training, the Wav2Vec2 architecture is fine-tuned for NSED to evaluate the performance for SER with and without continual pre-training. Table  2  shows the precision for the neutral class and the weighted average precision for negative and positive emotions for Wav2Vec2-xlsr and Indic-Wav2Vec2  (Javed et al., 2022 (to appear) . Indic-Wav2Vec2 gives the best performance with continual pre-training. We use this continually pre-trained indic-wav2vec2 model for our experiments. Figure  3  shows the pipeline of continual pre-training utilised in our experiments.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "The NSED dataset was split into train, dev, and test sets in the proportions of 80%, 10%, and 10% respectively. In each experimental run, the dataset was shuffled with a different seed value before feeding it to the model. The NViDia RTXA6000 GPU was used for all the experiments. A single experimental run took approximately 1 hour to complete. Hyper-parameter tuning was performed using the random search technique. The hyper-parameters which gave the best overall performance for the negative emotions were used in the end. The results shown in this paper give the performance of the best experimental run for the negative emotions in terms of weighted-average precision.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis",
      "text": "Table  3  gives the performance of the BiLSTM model using different types of features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analysis",
      "text": "Only by using the Wav2Vec2 (W) features, our model achieves an average precision of 0.61 over all the negative emotions. This forms the baseline for our experiments. When both the Wav2Vec2 (W) and the textual BERT (T) features are concatenated together, our model achieves a weighted-average precision of 0.64 over all negative emotions. This shows that textual features have additional emotional information which is absent from only the speech features. When word-level VAD values (VAD), extracted from the NRG-VAD lexicon, are also concatenated along with Wav2Vec2 (W) and textual BERT (T) features, we see an improvement of 2% with a weighted-average precision of 0.66 over all the negative emotions. This shows that by utilizing word-level VAD values we can improve the performance of our SER model for negative emotions. For the neutral emotion class, all the models achieve a precision over 90%. Results for positive emotions are unsatisfactory, with our proposed model giving a weighted-average precision of 0.16 for all the positive emotions. This can be attributed to the low amount of utterances with positive emotion in NSED. Even though the performance of our model is poor for positive emotions, it performs well for negative emotions, which is ideal, as we are dealing with customer call conversations where the customer is usually unsatisfied with a product or a service.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Challenges",
      "text": "We faced a number of challenges that one might expect while dealing with a natural code-mixed speech dataset. Some of the challenges are described below:\n\n• Audio Quality: Poor quality of the audio recordings made our task even more challenging. Due to network irregularities many recordings' audio quality dropped drastically",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we discussed the effect of incorporating word-level VAD values on SER for the Natural Speech Emotion Dataset (NSED). We also described the steps involved in creating NSED.\n\nFrequent code-mixing and noisy environments are some the biggest challenges for performing SER on natural datasets. By incorporating word-level VAD values we were able to achieve an improvement of 2% over the baseline in SER for negative emotions.\n\nIn future, we look forward to expanding this dataset so that all the emotions have substantial examples.\n\nOur SER system can be used to develop conversational agents which generate polite and empathetic statements to pacify a frustrated/angry customer. Different unsupervised techniques for SER can be explored. With this, we can possibly reduce the cost of annotations. Speech-based data augmentation techniques can also be used to increase the amount of data available to us.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "Our work has certain limitations as described in this section. The NSED dataset used in our experiments is small in comparison to some of the publicly available emotion recognition datasets. The number of utterances which do not belong to the neutral class is low. Positive classes like happy and excited constitute less than 2% of our dataset. Negative classes like anger, frustration, sad, disgust, and fear constitute 37% of our dataset. We also acknowledge that the emotions annotated for each utterance might not be the exact emotion intended by the speaker. The emotion annotations are in accordance with the interpretations of the annotators. The Automatic Speech Recognition (ASR) step is a bottleneck to our pipeline. As all the conversations are code-mixed, code-switched, natural, and often with a lot of noise, the ASR model couldn't generate an accurate transcript sometimes which lead to poor text features and omission of important words for emotion recognition. We acknowledge that the model might have possibly learnt some sensitive customer information. In future, we will include experiments in our study to remove such sensitive information. We also understand that there are state-of-the-art transformer models which can be experimented on. But due to the limited size of our dataset, we couldn't perform those experiments now. In future, as we expand the dataset, we will also include experiments utilising these transformer based models in our study.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethics Statement",
      "text": "The Natural Speech Emotion Dataset (NSED) dataset used in our experiments was annotated by a team of 4 annotators. Each annotator had to listen to an audio conversation between a customer and a customer-care executive and annotate each speaking turn with emotion, sentiment, valence, arousal, and dominance values. The conversational audio files were provided to us by our industry partner because of which NSED remains a proprietary dataset. Consent was taken from both customers and customer-care executives before recording their conversations. The annotators were paid for the time and effort they spent on the annotation task.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Flow-diagram of an ideal conversational agent that generates polite and empathetic responses. Emotion",
      "page": 2
    },
    {
      "caption": "Figure 1: depicts the importance of emotion",
      "page": 3
    },
    {
      "caption": "Figure 2: , the overall architecture of the proposed",
      "page": 4
    },
    {
      "caption": "Figure 2: Overall architecture of the proposed model. Speech features are extracted from the Wav2Vec2 model.",
      "page": 5
    },
    {
      "caption": "Figure 3: Continual pre-training and fine-tuning of the Wav2Vec2 architecture with unlabeled and labeled NSED",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the pipeline of",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Neutral",
          "Utterance Count": "3510",
          "%age": "61%"
        },
        {
          "Emotion": "Anger",
          "Utterance Count": "863",
          "%age": "15%"
        },
        {
          "Emotion": "Frustration",
          "Utterance Count": "748",
          "%age": "13%"
        },
        {
          "Emotion": "Disgust",
          "Utterance Count": "116",
          "%age": "2%"
        },
        {
          "Emotion": "Sad",
          "Utterance Count": "403",
          "%age": "7%"
        },
        {
          "Emotion": "Fear",
          "Utterance Count": "19",
          "%age": "< 1%"
        },
        {
          "Emotion": "Happy",
          "Utterance Count": "57",
          "%age": "< 1%"
        },
        {
          "Emotion": "Surprised",
          "Utterance Count": "13",
          "%age": "< 1%"
        },
        {
          "Emotion": "Excited",
          "Utterance Count": "25",
          "%age": "< 1%"
        },
        {
          "Emotion": "Total",
          "Utterance Count": "5754",
          "%age": "100%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Results for Wav2Vec2-xlsr and Indic-",
      "data": [
        {
          "Model Name": "Wav2Vec2-XLSR\n(w/o pre-training)",
          "Neu": "0.81",
          "Pos": "0.05",
          "Neg": "0.40"
        },
        {
          "Model Name": "Wav2Vec2-XLSR\n(with\ncontinual\npre-\ntraining)",
          "Neu": "0.89",
          "Pos": "0.05",
          "Neg": "0.53"
        },
        {
          "Model Name": "Indic-Wav2Vec2 (w/o\npre-training)",
          "Neu": "0.92",
          "Pos": "0.10",
          "Neg": "0.57"
        },
        {
          "Model Name": "Indic-Wav2Vec2\n(with\ncontinual\npre-training)",
          "Neu": "0.92",
          "Pos": "0.14",
          "Neg": "0.61"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: gives the performance of the BiLSTM twocolumnsgivetheweighted-averageprecisionover",
      "data": [
        {
          "Model Name": "W (Baseline)",
          "Neu": "0.92",
          "Ang": "0.74",
          "Sad": "0.63",
          "Fru": "0.69",
          "Neg": "0.61",
          "Pos": "0.14"
        },
        {
          "Model Name": "T+W",
          "Neu": "0.93",
          "Ang": "0.76",
          "Sad": "0.64",
          "Fru": "0.71",
          "Neg": "0.64",
          "Pos": "0.15"
        },
        {
          "Model Name": "W+VAD",
          "Neu": "0.95",
          "Ang": "0.75",
          "Sad": "0.64",
          "Fru": "0.71",
          "Neg": "0.65",
          "Pos": "0.15"
        },
        {
          "Model Name": "T+VAD",
          "Neu": "0.95",
          "Ang": "0.78",
          "Sad": "0.65",
          "Fru": "0.72",
          "Neg": "0.65",
          "Pos": "0.15"
        },
        {
          "Model Name": "T+W+VAD",
          "Neu": "0.96",
          "Ang": "0.79",
          "Sad": "0.67",
          "Fru": "0.74",
          "Neg": "0.66",
          "Pos": "0.16"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "authors": [
        "Yejin Bang",
        "Samuel Cahyawijaya",
        "Nayeon Lee",
        "Wenliang Dai",
        "Dan Su",
        "Bryan Wilie",
        "Holy Lovenia",
        "Ziwei Ji",
        "Tiezheng Yu",
        "Willy Chung"
      ],
      "year": "2023",
      "venue": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
      "arxiv": "arXiv:2302.04023"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "A conversational agent for emotion expression stimulation in persons with neurodevelopmental disorders",
      "authors": [
        "Fabio Catania",
        "Franca Garzotto"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "5",
      "title": "Investigating transformer encoders and fusion strategies for speech emotion recognition in emergency call center conversations",
      "authors": [
        "Theo Deschamps-Berger",
        "Lori Lamel",
        "Laurence Devillers"
      ],
      "year": "2022",
      "venue": "Companion Publication of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "6",
      "title": "Vad-assisted multitask transformer framework for emotion recognition and intensity prediction on suicide notes",
      "authors": [
        "Soumitra Ghosh"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "7",
      "title": "Ordinal learning for emotion recognition in customer service calls",
      "authors": [
        "Wenjing Han",
        "Tao Jiang",
        "Yan Li",
        "Björn Schuller",
        "Huabin Ruan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Classifying emotions in customer support dialogues in social media",
      "authors": [
        "Jonathan Herzig",
        "Guy Feigenblat",
        "Michal Shmueli-Scheuer",
        "David Konopnicki",
        "Anat Rafaeli",
        "Daniel Altman",
        "David Spivak"
      ],
      "year": "2016",
      "venue": "SIGDIAL Conference"
    },
    {
      "citation_id": "9",
      "title": "Towards building asr systems for the next billion users",
      "authors": [
        "Tahir Javed",
        "Sumanth Doddapaneni",
        "Abhigyan Raman",
        "Kaushal Santosh Bhogale",
        "Gowtham Ramesh",
        "Anoop Kunchukuttan",
        "Pratyush Kumar",
        "Mitesh Khapra"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "An adapter based pre-training for efficient and scalable self-supervised speech representation learning",
      "authors": [
        "Samuel Kessler",
        "Bethan Thomas",
        "Salah Karout"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Retrofitting of pre-trained emotion words with VADdimensions and the Plutchik emotions",
      "authors": [
        "Manasi Kulkarni",
        "Pushpak Bhattacharyya"
      ],
      "year": "2021",
      "venue": "Proceedings of the 18th International Conference on Natural Language Processing (ICON)"
    },
    {
      "citation_id": "12",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "13",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Zhiyun Lu",
        "Liangliang Cao",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "James Fan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "The rise of emotion-aware conversational agents: threats in digital emotions",
      "authors": [
        "Martino Mensio",
        "Giuseppe Rizzo",
        "Maurizio Morisio"
      ],
      "year": "2018",
      "venue": "Companion Proceedings of the The Web Conference 2018"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "18",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "Björn Schuller",
        "Gerhard Rigoll",
        "Manfred Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in spontaneous and acted dialogues",
      "authors": [
        "Leimin Tian",
        "Johanna Moore",
        "Catherine Lai"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "Negative emotions detection as an indicator of dialogs quality in call centers",
      "authors": [
        "Christophe Vaudable",
        "Laurence Devillers"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}