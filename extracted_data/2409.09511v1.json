{
  "paper_id": "2409.09511v1",
  "title": "Explaining Deep Learning Embeddings For Speech Emotion Recognition By Predicting Interpretable Acoustic Features",
  "published": "2024-09-14T19:18:56Z",
  "authors": [
    "Satvik Dixit",
    "Daniel M. Low",
    "Gasser Elbanna",
    "Fabio Catania",
    "Satrajit S. Ghosh"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Explainable machine learning",
    "Self-supervised learning",
    "Feature importance",
    "Paralinguistic analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Pre-trained deep learning embeddings have consistently shown superior performance over handcrafted acoustic features in speech emotion recognition (SER). However, unlike acoustic features with clear physical meaning, these embeddings lack clear interpretability. Explaining these embeddings is crucial for building trust in healthcare and security applications and advancing the scientific understanding of the acoustic information that is encoded in them. This paper proposes a modified probing approach to explain deep learning embeddings in the SER space. We predict interpretable acoustic features (e.g., f0, loudness) from (i) the complete set of embeddings and (ii) a subset of the embedding dimensions identified as most important for predicting each emotion. If the subset of the most important dimensions better predicts a given emotion than all dimensions and also predicts specific acoustic features more accurately, we infer those acoustic features are important for the embedding model for the given task. We conducted experiments using the WavLM embeddings and eGeMAPS acoustic features as audio representations, applying our method to the RAVDESS and SAVEE emotional speech datasets. Based on this evaluation, we demonstrate that Energy, Frequency, Spectral, and Temporal categories of acoustic features provide diminishing information to SER in that order, demonstrating the utility of the probing classifier method to relate embeddings to interpretable acoustic features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER) involves automatically identifying emotional states from spoken language  [1]   is an important task in several fields, including humancomputer interaction  [2]  and mental health assessments  [3] . While conventional methods rely on handcrafted features, recent breakthroughs in this domain have come from deep neural networks, especially those trained in a self-supervised manner  [1] . These networks learn speech embeddings that outperform traditional features in terms of SER accuracy  [4] ,  [5] , but the mechanisms behind their success remain unclear. In particular, the question of what kind of acoustic information deep learning (DL) models use for a particular task remains largely unanswered  [6] .\n\nThis paper focuses on explainability by using probing classifiers to investigate the acoustic information contained in DL embeddings. Using a novel tiered prediction strategy, we aim to identify the specific interpretable acoustic feature information that is more relevant for distinguishing emotions using DL embeddings. This can enhance our understanding of the mechanisms driving the success of DL models in SER.\n\nIn this study, our contributions are that we first provide insights into what types of acoustic features characterize different emotions, using the standard eGeMAPS feature set in a purely interpretable model. Second, we quantify how well these interpretable features are predicted from WavLM DL embeddings, offering insights into the information contained by these embeddings. Our methodological contribution is that we demonstrate which type of acoustic features are better represented in the subset of embedding dimensions that most characterize a given emotion and provide a new metric, information increase, to quantify this. We hypothesize that these acoustic features are relevant to that emotion. Our primary focus is explainability: gaining an understanding of the information encoded in these models can support their further improvement and their applications in various tasks and bolster user trust in the predictions. A tutorial is made available  1  and could be used to probe different self-supervised embeddings on different acoustic features in any audio-related classification task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Probing classifiers have been widely used to analyze text embeddings  [7] . However, less work has been done in the audio domain. One recent study probed transformer-based audio models for emotion recognition content to understand how much information related to emotions is contained in different models and layers  [8] , but did not probe for specific acoustic information. Another study fine-tuned pre-trained models to detect emotional properties (a multitask output: arousal, valence, and dominance)  [9] . They then probed these models for a set of acoustic features, comparing a pre-trained Wav2Vec 2.0  [10]  model fine-tuned with an added output head versus additionally fine-tuning the transformer layers. If a feature is represented more effectively after fine-tuning the transformer layers, resulting in improved predictions of acoustic features, then it is hypothesized that this information is encoded in the model or captured by the model. Since fine-tuning these layers improved performance, more information about certain acoustic features would indicate that they are relevant to the task. However, they did not find changes in information except that audio duration information became less important for the improved model. Another recent study compares Wav2Vec 2.0 representations with selected eGeMAPS features  [11] , however they used canonical correlation analysis instead of probing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "We selected two well-established emotional speech datasets to evaluate how outcomes vary between them: the Ryer-son Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [12]  and the Surrey Audio-Visual Expressed Emotion (SAVEE)  [13] . The choice of these datasets was motivated by their numerous similarities, which allow for a direct and meaningful comparison of different feature extraction techniques. Both datasets are in English. Also, they are balanced in actors' gender, spoken sentences, and expressed emotions (the Big Six  [14]  plus neutrality), minimizing potential biases due to data imbalance. Additionally, they were recorded in controlled laboratory environments, ensuring high-quality, noise-free audio samples, which can reduce bias during feature extraction and training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Audio Representations",
      "text": "We looked at two categories of audio representations: Handcrafted acoustic features: These are interpretable features designed to capture specific aspects of the audio signal, such as intensity, frequency, spectral, and temporal elements. For this study, we used eGeMAPS, a widely adopted standard set of acoustic features (implemented using OpenSMILE eGeMAPSv02  [15] ), which has proven to be somewhat effective for emotion recognition  [16] .\n\nDeep learning embeddings: These are representations learned through neural networks that can capture complex and abstract patterns in the audio signal. In this study, we used WavLM Large  [17] , a pre-trained speech self-supervised model that has demonstrated state-of-the-art performance on the SUPERB benchmark for emotion recognition  [5] . The embeddings were mean-pooled over time to get one embedding per utterance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Ser Classification Using Egemaps And Wavlm",
      "text": "As displayed in Figure  1 , we employ a binary classifier for each distinct emotion category (emotion vs neutrality). Specifically, we divide the dataset such that half of the samples represent one particular emotion, while the remaining half are utterances of the neutral emotion. This is done so we can focus on the features that characterize an emotion versus neutral, which we find simpler to interpret than what characterizes an emotion versus other emotions (e.g., anger should be louder than neutral).\n\nPrior to classification, we perform speaker normalization. For the classification task, we employ Logistic Regression with L2 regularization -this helps reduce collinearity issues  [18] . We perform hyperparameter tuning on the regularization parameter or 'C' with the values from the set {0.01, 0.1, 1, 10, 100} using use 5-fold nested cross-validation (i.e., the optimal parameter is chosen within the training set). Samples of a specific speaker are either in the training or test set, not both. We use F1 score to assess classifier performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Determining Top Egemaps Features And Wavlm Embedding Dimensions Using Shap",
      "text": "We rank the eGeMAPS features in order of importance for SER classification using SHAP  [19]  in order to determine the most important features and feature categories for predicting each emotion, albeit in a less optimal, but interpretable model. The eGeMAPS feature categories  [16]  are described in Table  I .\n\nWe also find the most important dimensions for the WavLM DL model for each emotion in the same way to rank each of the 1024 WavLM embedding dimensions in order of importance. We perform a post-hoc analysis to determine the minimal set of most important dimensions by taking the lowest number of features at which the performance is the highest in classifying each emotion (also with Logistic Regression with L2 regularization). To determine this set for the WavLM embedding dimensions, we sweep the feature importance vector in steps of 10 starting from the 10th most important feature. This number for the estimated minimal set for each emotion is reported in Table  II . Next we described our probing approach to better understand the information encoded in these subsets of embedding dimensions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Probing Handcrafted Egemaps Features From Top Wavlm Embedding Dimensions",
      "text": "We estimate how much of each acoustic feature is contained in the WavLM embeddings by the ability of the embedding to predict the feature. We train our model on the minimal subset of top dimensions to predict the eGeMAPS features one at a time. For prediction, we use a Ridge regression model and do hyperparameter tuning on the regularization strength coefficient or 'alpha' with the values {0.001, 0.01, 0.1, 1, 10, 100}. We only used a linear classification model, as is common in probing classification, to avoid allowing more flexible models to infer new features as we wish to only look at information in the DL embeddings  [7] . For every feature, we compute the information increase between all WavLM embedding dimensions and top WavLM embedding dimensions weighted by how well the feature is encoded in the minimum subset (to avoid highlighting features that are not encoded well) using the following custom metric:\n\nHere RM SE all and RM SE top are the RMSE for predicting the given acoustic feature using all dimensions and top dimensions of the WavLM embedding. We are trying to identify handcrafted features which are encoded much better in the top dimensions of the embedding compared to all dimensions and therefore have a high value of RM SE all RM SEtop . These features should also be encoded significantly well in the top dimensions of the embedding (have low error); to enforce this, we add a 1 RM SEtop term which weighs the score down if the prediction by the top dimensions has a large error. This would indicate that the feature is not well captured by the top dimensions, even if they are better than using all dimensions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "The DL-based embeddings outperform the handcrafted features for every emotion for both datasets in terms of F1 scores (see Table  II ), which justifies using the DL models. Next, we determine top eGeMAPS features and WavLM embedding dimensions using SHAP. Figure  2  shows the most important eGeMAPS feature categories for every emotion. There is variability as to which categories are most important for each emotion (e.g., energy for anger; temporal for disgust).\n\nTo compare this to the information used by WavLM model, we probe handcrafted eGeMAPS features from the subset of top WavLM embedding dimensions. For every emotion, we compute the average information increase to show the eGeMAPS feature categories that are most relevant to the task as shown in Figure  3 . In general, the median information V. DISCUSSION By providing a novel probing method and metric, we demonstrate how to estimate interpretable acoustic information contained in DL-based embeddings. Our method helped us find that energy-based features have the most information increase across datasets across almost all emotions. Therefore, we hypothesize that the WavLM embeddings use this information to better perform the SER task. Even though the SHAP scores suggest different feature categories are important for every emotion, the energy based features always have the highest median information gain. The lower information increase for temporal features is consistent with the time-pooled nature of the embeddings, suggesting that some time-dependent information may be lost in the process. Furthermore, this method can help us understand why a given emotion is detected better by higher-performing DL-based models. For instance, the embeddings seem to use energy features to classify sadness more than other feature categories, but energy is similarly important to other categories in the handcrafted model, which may explain its lower performance as shown in Table  II . More generally, we see an ordering of feature categories that correlates with performance; when this fails in eGeMAPS models for sad and disgust, performance drops. Overall, we can leverage methods that compare models or embeddings that are trained on the same task, but one performs better than the other, and we can compare their relative information content.\n\nA limitation of this method is that while it estimates what information from eGeMAPS is and is not encoded in DLbased embeddings, it does not imply that this is the most important information the DL-based embeddings is using for classification; it might be using other information beyond the eGeMAPS features we tested. However, given a black-box, providing even part of the information encoded using our method can improve explainability.\n\nAn important future direction is investigating the generalizability of our results across more datasets and languages (including those with recordings of naturalistic emotion production such as MSP  [20] ). The analysis of other handcrafted features or DL-based embeddings could also be explored.",
      "page_start": 3,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The training and testing pipeline for our SER models and the top vs. all dimensions information increase method.",
      "page": 2
    },
    {
      "caption": "Figure 1: , we employ a binary classifier",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the most",
      "page": 3
    },
    {
      "caption": "Figure 3: In general, the median information",
      "page": 3
    },
    {
      "caption": "Figure 2: Normalised SHAP scores to show feature importance of eGeMAPS feature categories in interpretable model",
      "page": 4
    },
    {
      "caption": "Figure 3: Average information increase per eGeMAPS feature category for all emotions",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM",
      "doi": "10.1145/3129340"
    },
    {
      "citation_id": "2",
      "title": "The age of artificial emotional intelligence",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Computer"
    },
    {
      "citation_id": "3",
      "title": "Speech prosody in mental disorders",
      "authors": [
        "H Ding",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Annual Review of Linguistics"
    },
    {
      "citation_id": "4",
      "title": "Hear: Holistic evaluation of audio representations",
      "authors": [
        "J Turian",
        "J Shier",
        "H Khan",
        "B Raj",
        "B Schuller",
        "C Steinmetz",
        "C Malloy",
        "G Tzanetakis",
        "G Velarde",
        "K Mcnally"
      ],
      "year": "2022",
      "venue": "NeurIPS 2021 Competitions and Demonstrations Track"
    },
    {
      "citation_id": "5",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "6",
      "title": "Selfsupervised speech representation learning: A review",
      "authors": [
        "A Mohamed",
        "H -Y. Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S.-W Li",
        "K Livescu",
        "L Maaløe"
      ],
      "year": "2022",
      "venue": "Selfsupervised speech representation learning: A review",
      "arxiv": "arXiv:2205.10643"
    },
    {
      "citation_id": "7",
      "title": "Probing classifiers: Promises, shortcomings, and advances",
      "authors": [
        "Y Belinkov"
      ],
      "year": "2021",
      "venue": "Probing classifiers: Promises, shortcomings, and advances"
    },
    {
      "citation_id": "8",
      "title": "Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition",
      "authors": [
        "A Singh",
        "A Gupta"
      ],
      "year": "2023",
      "venue": "Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition",
      "arxiv": "arXiv:2308.08713"
    },
    {
      "citation_id": "9",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Interspeech 2022. ISCA"
    },
    {
      "citation_id": "10",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "11",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Y Li",
        "Y Mohamied",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "Exploration of a self-supervised speech model: A study on emotional corpora"
    },
    {
      "citation_id": "12",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "13",
      "title": "Multimodal emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2011",
      "venue": "Machine audition: principles, algorithms and systems"
    },
    {
      "citation_id": "14",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "15",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3188113"
    },
    {
      "citation_id": "18",
      "title": "Ridge regression and multicollinearity: An in-depth review",
      "authors": [
        "D Schreiber-Gregory"
      ],
      "year": "2018",
      "venue": "Model Assisted Statistics and Applications"
    },
    {
      "citation_id": "19",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee ; I. Guyon",
        "U Luxburg",
        "S Bengio",
        "H Wallach",
        "R Fergus",
        "S Vishwanathan",
        "R Garnett"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}