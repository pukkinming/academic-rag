{
  "paper_id": "2107.13231v1",
  "title": "On Perceived Emotion In Expressive Piano Performance: Further Experimental Evidence For The Relevance Of Mid-Level Perceptual Features",
  "published": "2021-07-28T09:22:18Z",
  "authors": [
    "Shreyan Chowdhury",
    "Gerhard Widmer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite recent advances in audio content-based music emotion recognition, a question that remains to be explored is whether an algorithm can reliably discern emotional or expressive qualities between different performances of the same piece. In the present work, we analyze several sets of features on their effectiveness in predicting arousal and valence of six different performances (by six famous pianists) of Bach's Well-Tempered Clavier Book 1. These features include low-level acoustic features, score-based features, features extracted using a pre-trained emotion model, and Mid-level perceptual features. We compare their predictive power by evaluating them on several experiments designed to test performance-wise or piece-wise variations of emotion. We find that Mid-level features show significant contribution in performance-wise variation of both arousal and valence -even better than the pre-trained emotion model. Our findings add to the evidence of Mid-level perceptual features being an important representation of musical attributes for several tasks -specifically, in this case, for capturing the expressive aspects of music that manifest as perceived emotion of a musical performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A musical performance, particularly in the Western music tradition, is not merely a literal acoustic rendering of a notated piece or composition. Rather, the piece is transformed by the performer's own expressive performance choices, relating to such dimensions as the choice of tempo, expressive tempo and timing variations, dynamics, articulation, and so on. The emotional effect of a performance on a listener can be a consequence both of the composition itself, with its musical properties and structures, and of the performance, the way the piece was played. In fact, it has been convincingly demonstrated  [1, 2]  that performers are capable of communicating, with high accuracy, intended emotional qualities by their playing.\n\nThe analysis of emotion in music recordings has a long history in Music Information Retrieval, with many works addressing content-based emotion regression and classification typically using low-level or hand-crafted audio and musical features  [3] [4] [5] [6]  or using deep learning based methods  [7] [8] [9] . However, there has been little research on the more subtle problem of identifying emotional aspects that are due to the actual performance, and even less on models that can automatically recognize this from audio recordings. On the latter problem -the one to be addressed in this paper -the most directly relevant prior work we are aware of is  [10] , where 324 6-second audio snippets of different genres (classical, jazz, blues, metal, etc.) were annotated in terms of perceived emotion (valence and arousal), and various regressors were trained to predict these two dimensions from a set of standard audio features. The regression models were then used to predict valence-arousal trajectories over 5 different recordings of 4 Chopin pieces, but no ground truth in terms of human emotion annotations was collected. The relevance of the model predictions was evaluated only indirectly, by comparing similarity scores between predicted profiles with overall performance similarity ratings by three human listeners, which showed some non-negligible correlations.\n\nIn a recent focused study  [11] , Battcock & Schutz (referred to as \"B&S\" henceforth) investigate how three specific score-based cues (Mode, Pitch Height, and Attack Rate 1  ) work together to convey emotion in J.S.Bach's preludes and fugues collected in his Well-tempered Clavier (WTC). They used recordings of the complete WTC Book 1 (48 pieces) of one famous pianist (Friedrich Gulda) as stimuli for human listeners to rate each performance on perceived arousal and valence. Their findings suggest that within this set of performances, arousal is significantly correlated with attack rate and valence is affected by both the attack rate and the mode. However, that study was based on only one set of performances, making it impossible to decide whether the human emotion ratings used as ground truth really reflect aspects of the compositions themselves, or whether they were also (or even predominantly) affected by the specific (and, in some cases, rather unconventional)  way in Friedrich Gulda plays the pieces -that is, whether the emotion ratings reflect piece or performance aspects.\n\nThe purpose of the present paper is to try to disentangle the possible contributions and roles of different features in capturing composer-(piece-)specific and performer-(recording-)specific aspects. To this end, we collected human ratings of perceived valence and arousal in six complete sets of recordings of WTC Book 1, and then performed a systematic study with feature sets derived from various levels of musical abstraction, including some extracted by pre-trained deep neural networks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Collection",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pieces And Recordings",
      "text": "J.S.Bach's Well-tempered Clavier (WTC) is ideally suited for systematic and controlled studies of this kind, as it comprises a stylistically coherent set of keyboard pieces from a particular period, evenly distributed over all keys and major/minor modes, with a pair of two pieces (a prelude, followed by a fugue) in each of the 24 possible keys, for a total of 48 pieces. Each piece has its own distinctive musical character, and despite being written in a rather strict style and not meant to be played in 'romantic' ways, the music offers pianists (or pianists take) lots of liberties in ornamentation, but also overall performance parameters (e.g., tempo and articulation). For example, there are pieces in our set of recordings that one pianist takes more than twice (!) as fast as another.\n\nFor a broad set of diverse performances, we selected six recordings of the complete WTC Book 1, by six famous and highly respected pianists, all of whom can be considered Bach specialists to various degrees. The recordings are listed in Table  1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Annotations And Pre-Processing",
      "text": "In accordance with B&S, we will only use the first 8 bars of each recording for the annotation process and our experiments. These were cut out manually. The participants of our annotation exercise were students of a course at a university, without a specifically musical background. Each participant heard a subset of the recordings (all 48 pieces as played by one pianist) and was asked to rate the valence on a scale of -5 to +5 (11 levels) and the arousal on a scale of 0 to 100 (increments of 10; a total of 11 levels). They could listen to a recording as many times as they liked. Each recording was rated by 29 participants. In total, we collected 8,352 valence-arousal annotation pairs. For the purposes of this paper, we take the mean arousal and mean valence ratings for each recording, and these values serve as our ground-truth values for all following experiments. The distributions (over the 6 performances) of these mean ratings for each piece are summarised in Fig-  ure 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Sets",
      "text": "In this section, we briefly describe the four feature sets that we use to model arousal and valence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Low-Level Features",
      "text": "These consist of hand-crafted musical features (such as onset rate, tempo, pitch salience) as well as generic audio descriptors (such as spectral centroid, loudness). Taken together, they reflect several musical characteristics such as tone colour, dynamics, and rhythm. A brief description of all low-level features that we use is given in Table  2 . We use Essentia  [12]  and Librosa  [13]  for extracting these. The audio is sampled at 44.1kHz and the spectra computed (when required) with a frame size of 1024 samples and a hop size of 512 samples. Each feature is aggregated over the entire duration of an audio clip by computing the mean and standard deviation over all the frames of the clip (a 'clip' being an 8 bar initial segment from a recording).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Score Features",
      "text": "The following set of features was computed directly from the musical score (i.e., sheet music) of the pieces instead of the audio files. The unit of score time, \"beat\", is defined by the time signature of the piece (e.g., 4/4 means that there are 4 beats of duration 1 quarter in a bar). The score information and the audio files were linked using automatic score-to-performance alignment. Table  3  describes the score features in detail.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dissonance",
      "text": "Total harmonic dissonance computed from pairwise dissonance of all spectral peaks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Complexity",
      "text": "The average absolute deviation from the global loudness level estimate in dB.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loudness",
      "text": "Mean loudness of the signal computed from the signal amplitude.\n\nOnset Rate Number of onsets (note beginnings or transients) per second.\n\nPitch Salience A measure of tone sensation, computed from the harmonic content of the signal.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Spectral Centroid",
      "text": "The weighted mean frequency in the signal, with frequency magnitudes as the weights.\n\nSpectral Flatness A measure to quantify how much noise-like a sound is, as opposed to being tone-like. Spectral Bandwidth\n\nThe second order bandwidth of the spectrum.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Spectral Rolloff",
      "text": "The frequency under which 85% of the total energy of the spectrum is contained.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Spectral Complexity",
      "text": "The number of peaks in the input spectrum.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tempo (Bpm)",
      "text": "Tempo estimate from audio in beats per minute.\n\nTable  2 : Low-level Features",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Inter Onset Interval",
      "text": "The time interval between consecutive notes per beat.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Duration",
      "text": "Two features describing the empirical mean and standard deviation of the notated duration per beat in the snippet.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Onset Density",
      "text": "The number of note onsets per beat. A chord constitutes a single onset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pitch Density",
      "text": "The number of unique notes per beat.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mode",
      "text": "Binary feature denoting major/minor modality, computed using the Krumhansl-Schmuckler key finding algorithm  [14]  (to reflect the fact that the dominant key over the segment may be different from the given key signature).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Key Strength",
      "text": "This feature represents how much does the tonality by the \"Mode\" feature fit the snippet.\n\nTable  3 : Score Features",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mid-Level Features",
      "text": "Mid-level features, described in  [15] , are perceptual musical features that are intuitively understandable to the average listener. They seem well-suited to bridge the \"semantic gap\" between low-level audio features and high-level descriptors such as emotion and have been shown to be useful in explainable music emotion recognition  [16] . We learn these features from the Mid-level Dataset  [15]  using a receptive-field regularised residual neural network (RF-ResNet) model  [17] . Since we intend to use this model to extract features from solo piano recordings (a genre that is not covered by the original training data), we use a domainadaptive training approach as described in  [18] . We use an input audio length of 30 seconds, padded or cropped as required. As these features cannot be strictly defined, Table 4 lists a rough description adapted from the questions in  [15]  that were shown to the annotators of the dataset to help them rate the audio clips.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Deamresnet Emotion Features",
      "text": "To compare the mid-level features with another deep neural network based feature extractor, we train a model with Melodiousness How singable is this music?",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Articulation",
      "text": "Overall impression of articulation in terms of staccato or legato playing style. Higher means more staccato.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Rhythmic Stability",
      "text": "How easy is it to march-along with the music?",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Rhythmic Complexity",
      "text": "How difficult is it to follow the music by tapping? Rhythmic layers and different meters correlate with higher complexity.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dissonance",
      "text": "Noisier timbre or presence of dissonant intervals (tritones, seconds, etc.) Tonal Stability How clear or apparent the tonic and key are.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Minorness",
      "text": "Relates to the perceived tonality. More \"minorsounding\" music will have higher minorness. Features are extracted from the penultimate layer of the model, which gives us 512 features. Since these are too many features to use for our dataset containing only 288 data points, we perform dimensionality reduction using PCA (Principal Component Analysis), to obtain 9 components explaining at least 98% of the variance. These 9 features are named as pca_x with x being the principal component number.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Evaluation Experiments",
      "text": "In this section, we evaluate the four feature sets. The aim of this section is to answer the following questions:\n\n1. How well can each feature set fit the arousal and valence ratings? How do these feature sets compare to the ones used by B&S? (Sections 4.1 and 4.2)\n\n2. In each feature set, which features are the most important? (Section 4.3) 3. Which feature set best explains variation of arousal and valence between pieces? (Section 4.4) 4. Which feature set best explains variation of arousal and valence between different performances of the same piece? (Section 4.5)\n\nWe use ordinary least squares fitting on the dataset in question and calculate the regression metrics. The metrics we report are adjusted coefficient of determination ( R2 ), root mean squared error between true and predicted values (RMSE), and Pearson's correlation coefficient between true and predicted values (Corr).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation On B&S Data",
      "text": "As a starting point, we take the data used by B&S in Experiment 3 of their paper -Gulda's performances rated on valence and arousal We perform regression with our feature sets and compare with the values obtained by B&S We can see that all three audio-based features perform considerably well for both arousal and valence to motivate further analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation On Our Dataset",
      "text": "Next, we perform regression on our complete dataset (comprising of 288 unique recordings -48 pieces × 6 pianists). The results summary can be seen in Table  6a . Here again, we observe that while DEAMResNet Emotion features perform best on arousal and Score features perform best on valence, Mid-level features show a balanced performance across both the emotion dimensions.\n\nTo evaluate generalizability, we perform crossvalidation with three different kinds of splits -piece-wise (all 6 performances of a piece are test samples in a fold, for a total of 48 folds), pianist-wise (all 48 pieces of a pianist are test samples in a fold, for a total of 6 folds), and leave-one-out (one recording is the test sample per fold, for a total of 288 folds). This is summarized in Table  6b   We see that Mid-level features show good generalization for arousal and are robust to different kinds of splits. They also show balanced performance between arousal and valence for all splits. The good performance of the Score features on the valence dimension (V), here and in the previous experiment, is mostly due to the Mode feature; there is a substantial correlation in the annotations between major/minor mode and positive/negative valence.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Importance Within Feature Sets",
      "text": "We use the absolute value of the t-statistic of a feature as the importance measure. T-statistic is defined as the estimated weight scaled with its standard error. We focus on the audio-based feature sets here, as in most realistic applications scenarios, the score information will not be available (and, being constant across different performances, will not be able to distinguish performance aspects). We perform a regression using all audio-based features (numbering 39 in total) and compare the t-values in Figure  2 .\n\nWe see that the top-4 and top-2 features in arousal and valence, respectively, are Mid-level features. These features also make obvious musical sense -modality is often correlated with valence (positive or negative emotional quality), and tempo, rhythm, and articulation with arousal (intensity or energy of the emotion).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Explaining Piece-Wise Variation",
      "text": "We observe from the annotation data (see Figure  1 ) that the distribution of emotion ratings of each piece is distinct (here, the 6 performances for each piece form the \"distribution\" of the piece). In essence, the mean value of arousal or valence depends on the piece in question. Therefore, to take into account this factor of variation, we use linear mixed models  [20]  to model arousal and valence.\n\nIn this linear mixed effect model, the piece id is considered as a \"random effect\" intercept, which models part of the residual remaining unexplained by the features we are evaluating (\"fixed effects\"). A feature set that models piece-wise variation better than another set would naturally have a lesser residual variation to be explained by the random effect. We therefore look at which feature set has the least fraction of residual variance explained by the random effect of piece id, defined as:\n\nVar random Var random + Var residual  (1)  where Var random is the variance of the random effect intercept and Var residual is the variance of the residual that remains after mixed effects modeling. We see from Table  7  that the DEAMResNet emotion features best explain piece-wise variation in arousal, followed closely by Mid-level features. For valence, the performance of all three audio-based features are close, with Mid-level features performing the best, however, score features outperform them with a large margin. This is again due to the relationship between mode and valence, and mode covarying tightly with the piece ids.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Explaining Performance-Wise Variation",
      "text": "Evaluation of performance-wise variation modelling cannot be done with the mixed effects approach as in the previous section because the means (of arousal or valence across all pieces) are nearly identical for each pianist.\n\nTherefore, we look at one piece at a time and compute the fraction of variance unexplained (FVU) and Pearsons's correlation coefficient (Corr) between predicted and true values across performances for each such test piece. This is done as leave-one-piece-out cross-validation, and aggregated by taking the means. The p-values of the correlation coefficients are counted and we report the percentage of pieces for which p < 0.1. With only 6 performances per piece, a significance level of p < 0.05 is obtained for only a handful of pieces. Since score-features based predictions are exactly equal for all performances of a piece, these metrics are not meaningful, and hence the Score feature set is not included here.  Again, Mid-level features come out at the top in most measures. To illustrate the modelling of performance-wise variation, we select a few example pieces that have a high variation of emotion between performances and plot them together with the predicted values using mid-level features in Figure  3 . The predicted emotion dimensions follow the ratings closely, even for performances that deviate from the average (e.g. the arousals of Gulda's performance of Prelude in A major and Tureck's performance of Fugue in E minor.) Figure  3 : Some example pieces with high emotion variability between performances which are modeled particularly well using mid-level features.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Probing Further",
      "text": "We now describe two additional experiments designed to further probe the predictive power of our feature sets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Predicting Emotion Of Outlier Performances",
      "text": "Figure  4  shows two examples of pieces where one performance has a vastly different emotional character than the others -in the first example, Gould even produces a negative valence effect (mostly through tempo and articulation) in the E-flat major prelude, which the others play in a much more flowing fashion. A challenge for any model would thus be to predict the emotion of such idiosyncratic performances, not having seen them during training.\n\nWe therefore create a test set by picking out the outlier performance for each piece in arousal-valence space using the elliptic envelope method  [21] . This gives us a split of 240 training and 48 test samples (the outliers). We train a linear regression model using each of our feature sets and report the performance on the outlier test set in Figure  5 . We see again that Mid-level features outperform the others, for both emotion dimensions. We take this as another piece of evidence for the ability of the mid-level features to capture performance-specific aspects. The surprisingly good performance of score features for valence can be attributed to the fact that for most pieces, the outlier points are separated mostly in the arousal dimension -the spread of valence is rather small (though not always: see the Gould case in Fig.  4 ) -and the score feature \"mode\" is an important predictor of valence (see earlier sections).\n\nFigure  5 : R2 scores on outlier performances. The outliers were selected using elliptic envelope on the rated arousal-valence space. Out of 48 pieces, the number of times each pianist was an outlier are Gould: 13, Gulda: 10, Tureck: 10, Schiff: 5, Hewitt: 5, Richter: 5.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Predicting Discrete Emotions",
      "text": "Finally, we evaluate how the feature sets perform in a discrete emotion classification task, which might be relevant in a music recommendation setting, for instance. The emotion ratings are converted to classes simply by reducing them to quadrants in the arousal-valence space. In the literature, these are often associated with the basic emotion labels happy, relaxed, sad, and angry (in clockwise fashion, starting at upper right). We then train logistic regres-sion models using our feature sets and report the leave-oneout cross-validation accuracy in Figure  6 . We observe that in this task, all feature sets perform more-or-less equally well, again with a slight advantage for the Mid-level features. Note that the random choice baseline accuracy is 0.25. An emotion classification model based on mid-level perceptual features might be attractive for performanceemotion-aware music recommendation, being able to offer the mid-level concepts not only as explanations but also as 'handles' to search for performances with certain characteristics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we evaluated four feature sets -mid-level perceptual features, pre-trained emotion features, low-level audio features, and score-based features on their ability to model and predict emotion in terms of arousal and valence. Specific focus was given on the three audio-based features and their modelling power over performance-wise variation of emotion. Mid-level features emerge as the most robust and important among these.\n\nThe search for good features to model music emotion is a worthwhile objective since emotional effect is a very fundamental human response to music. Features that provide a better handle on content-based emotion recognition can have a significant impact on applications such as search and recommendation. Modelling emotion is also becoming increasingly relevant in generative music, allowing possibilities such as expressivity-or emotion-based snippet continuation and emotion-aware human-computer collaborative music.\n\nFrom the experiments presented in this paper, it is clear that deep-learning-based feature extractors are strong competition to the audio features typically used for emotion recognition  [3] . Here the importance of Mid-level features is even more pronounced -in addition to being able to model both arousal and valence well under different conditions, they also provide intuitive musical meaning to each feature, and have been previously used as the basis for explainable emotion recognition in  [16] .",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of emotion ratings across pianists for ev-",
      "page": 2
    },
    {
      "caption": "Figure 2: We see that the top-4 and top-2 features in arousal and",
      "page": 4
    },
    {
      "caption": "Figure 2: Feature importance for audio features using T-statistic. Only features with p<0.05 are shown.",
      "page": 5
    },
    {
      "caption": "Figure 3: The predicted emotion dimensions follow the",
      "page": 5
    },
    {
      "caption": "Figure 3: Some example pieces with high emotion variability",
      "page": 5
    },
    {
      "caption": "Figure 4: shows two examples of pieces where one perfor-",
      "page": 5
    },
    {
      "caption": "Figure 4: Two examples of outlier performances: Prelude # 7 in",
      "page": 6
    },
    {
      "caption": "Figure 5: We see again that Mid-level features outperform",
      "page": 6
    },
    {
      "caption": "Figure 4: ) – and the score feature “mode” is",
      "page": 6
    },
    {
      "caption": "Figure 5: ˜R2 scores on outlier performances.",
      "page": 6
    },
    {
      "caption": "Figure 6: We observe that",
      "page": 6
    },
    {
      "caption": "Figure 6: Discrete emotion classiﬁcation.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Melodiousness": "Articulation",
          "How singable is this music?": "Overall\nimpression of articulation in terms of\nstaccato or legato playing style. Higher means\nmore staccato."
        },
        {
          "Melodiousness": "Rhythmic\nStability",
          "How singable is this music?": "How easy is it to march-along with the music?"
        },
        {
          "Melodiousness": "Rhythmic\nComplexity",
          "How singable is this music?": "How difﬁcult\nis it\nto follow the music by tap-\nping?\nRhythmic layers and different meters\ncorrelate with higher complexity."
        },
        {
          "Melodiousness": "Dissonance",
          "How singable is this music?": "Noisier\ntimbre or presence of dissonant\ninter-\nvals (tritones, seconds, etc.)"
        },
        {
          "Melodiousness": "Tonal Stability",
          "How singable is this music?": "How clear or apparent the tonic and key are."
        },
        {
          "Melodiousness": "Minorness",
          "How singable is this music?": "Relates to the perceived tonality. More “minor-\nsounding\" music will have higher minorness."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dissonance": "Dynamic\nComplexity",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "The\naverage\nabsolute\ndeviation\nfrom the\nglobal loudness level estimate in dB."
        },
        {
          "Dissonance": "Loudness",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "Mean loudness of the signal computed from\nthe signal amplitude."
        },
        {
          "Dissonance": "Onset Rate",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "Number of onsets (note beginnings or tran-\nsients) per second."
        },
        {
          "Dissonance": "Pitch Salience",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "A measure\nof\ntone\nsensation,\ncomputed\nfrom the harmonic content of the signal."
        },
        {
          "Dissonance": "Spectral Centroid",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "The weighted mean frequency in the signal,\nwith frequency magnitudes as the weights."
        },
        {
          "Dissonance": "Spectral Flatness",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "A measure to quantify how much noise-like\na sound is, as opposed to being tone-like."
        },
        {
          "Dissonance": "Spectral\nBandwidth",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "The second order bandwidth of\nthe spec-\ntrum."
        },
        {
          "Dissonance": "Spectral Rolloff",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "The frequency under which 85% of the total\nenergy of the spectrum is contained."
        },
        {
          "Dissonance": "Spectral\nComplexity",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "The number of peaks in the input spectrum."
        },
        {
          "Dissonance": "Tempo (BPM)",
          "Total harmonic dissonance computed from\npairwise dissonance of all spectral peaks.": "Tempo\nestimate\nfrom audio\nin\nbeats\nper\nminute."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Inter Onset\nInterval": "Duration",
          "The time interval between consecutive notes per\nbeat.": "Two features describing the empirical mean and\nstandard deviation of\nthe notated duration per\nbeat in the snippet."
        },
        {
          "Inter Onset\nInterval": "Onset Density",
          "The time interval between consecutive notes per\nbeat.": "The number of note onsets per beat. A chord\nconstitutes a single onset."
        },
        {
          "Inter Onset\nInterval": "Pitch Density",
          "The time interval between consecutive notes per\nbeat.": "The number of unique notes per beat."
        },
        {
          "Inter Onset\nInterval": "Mode",
          "The time interval between consecutive notes per\nbeat.": "Binary\nfeature\ndenoting major/minor modal-\nity, computed using the Krumhansl-Schmuckler\nkey ﬁnding algorithm [14]\n(to reﬂect\nthe fact\nthat the dominant key over the segment may be\ndifferent from the given key signature)."
        },
        {
          "Inter Onset\nInterval": "Key Strength",
          "The time interval between consecutive notes per\nbeat.": "This\nfeature\nrepresents\nhow much\ndoes\nthe\ntonality by the \"Mode\" feature ﬁt the snippet."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 7: that the DEAMResNet emotion",
      "data": [
        {
          "Arousal\nR2\nRMSE\nCorr": "0.84\n0.36\n0.93\n0.91\n0.27\n0.96\n0.86\n0.29\n0.96\n0.31\n0.74\n0.67\n0.48\n-\n-",
          "Valence\nR2\nRMSE\nCorr": "0.79\n0.42\n0.91\n0.69\n0.50\n0.86\n0.67\n0.45\n0.89\n0.61\n0.55\n0.83\n0.75\n-\n-"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional expression in music performance: Between the performer's intention and the listener's experience",
      "authors": [
        "A Gabrielsson",
        "P Juslin"
      ],
      "year": "1996",
      "venue": "Psychology of Music"
    },
    {
      "citation_id": "3",
      "title": "Decoding emotions in expressive music performances: A multi-lab replication and extension study",
      "authors": [
        "J Akkermans",
        "R Schapiro",
        "D Müllensiefen",
        "K Jakubowski",
        "D Shanahan",
        "D Baker",
        "V Busch",
        "K Lothwesen",
        "P Elvers"
      ],
      "year": "2019",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "4",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Emotional analysis of music: A comparison of methods",
      "authors": [
        "M Soleymani",
        "A Aljanaki",
        "Y.-H Yang",
        "M Caro",
        "F Eyben",
        "K Markov",
        "B Schuller",
        "R Veltkamp",
        "F Weninger",
        "F Wiering"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        "J Speck",
        "D Turnbull"
      ],
      "year": "2010",
      "venue": "Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010"
    },
    {
      "citation_id": "7",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "8",
      "title": "DNN based music emotion recognition from raw audio signal",
      "authors": [
        "R Orjesek",
        "R Jarina",
        "M Chmulik",
        "M Kuba"
      ],
      "year": "2019",
      "venue": "29th International Conference Radioelektronika 2019 (RADIOELEKTRONIKA)"
    },
    {
      "citation_id": "9",
      "title": "Music emotion recognition by using chroma spectrogram and deep visual features",
      "authors": [
        "M Er",
        "I Aydilek"
      ],
      "year": "2019",
      "venue": "International Journal of Computational Intelligence Systems"
    },
    {
      "citation_id": "10",
      "title": "Multi-view neural networks for raw audio-based music emotion recognition",
      "authors": [
        "N He",
        "S Ferguson"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Symposium on Multimedia (ISM)"
    },
    {
      "citation_id": "11",
      "title": "Musical performance analysis in terms of emotions it evokes",
      "authors": [
        "J Grekow"
      ],
      "year": "2018",
      "venue": "Journal of Intelligent Information Systems"
    },
    {
      "citation_id": "12",
      "title": "Acoustically expressing affect",
      "authors": [
        "A Battcock",
        "M Schutz"
      ],
      "year": "2019",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "13",
      "title": "Essentia: An audio analysis library for music information retrieval",
      "authors": [
        "D Bogdanov",
        "N Wack",
        "E Gómez Gutiérrez",
        "S Gulati",
        "H Boyer",
        "O Mayor"
      ],
      "year": "2013",
      "venue": "International Society for Music Information Retrieval"
    },
    {
      "citation_id": "14",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "15",
      "title": "Cognitive foundations of musical pitch",
      "authors": [
        "C Krumhansl"
      ],
      "year": "2001",
      "venue": "Cognitive foundations of musical pitch"
    },
    {
      "citation_id": "16",
      "title": "A Data-driven Approach to Mid-level Perceptual Musical Feature Modeling",
      "authors": [
        "A Aljanaki",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "17",
      "title": "Proceedings of the 20th International Society for Music Information Retrieval Conference",
      "authors": [
        "S Chowdhury",
        "A Vall",
        "V Haunschmid",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "18",
      "title": "The Receptive Field as a Regularizer in Deep Convolutional Neural Networks for Acoustic Scene Classification",
      "authors": [
        "K Koutini",
        "H Eghbal-Zadeh",
        "M Dorfer",
        "G Widmer"
      ],
      "year": "2019",
      "venue": "The Receptive Field as a Regularizer in Deep Convolutional Neural Networks for Acoustic Scene Classification"
    },
    {
      "citation_id": "19",
      "title": "Towards explaining expressive qualities in piano recordings: Transfer of explanatory features via acoustic domain adaptation",
      "authors": [
        "S Chowdhury",
        "G Widmer"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Developing a Benchmark for Emotional Analysis of Music",
      "authors": [
        "A Aljanaki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "J Pinheiro",
        "D Bates",
        "S Debroy",
        "D Sarkar",
        "R Core Team"
      ],
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "A fast algorithm for the minimum covariance determinant estimator",
      "authors": [
        "P Rousseeuw",
        "K Driessen"
      ],
      "year": "1999",
      "venue": "Technometrics"
    }
  ]
}