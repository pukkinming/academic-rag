{
  "paper_id": "2401.12925v1",
  "title": "Emotion-Aware Contrastive Adaptation Network For Source-Free Cross-Corpus Speech Emotion Recognition",
  "published": "2024-01-23T17:21:43Z",
  "authors": [
    "Yan Zhao",
    "Jincen Wang",
    "Cheng Lu",
    "Sunan Li",
    "Björn Schuller",
    "Yuan Zong",
    "Wenming Zheng"
  ],
  "keywords": [
    "Source-free cross-corpus speech emotion recognition",
    "speech emotion recognition",
    "contrastive learning",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-corpus speech emotion recognition (SER) aims to transfer emotional knowledge from a labeled source corpus to an unlabeled corpus. However, prior methods require access to source data during adaptation, which is unattainable in real-life scenarios due to data privacy protection concerns. This paper tackles a more practical task, namely source-free cross-corpus SER, where a pre-trained source model is adapted to the target domain without access to source data. To address the problem, we propose a novel method called emotion-aware contrastive adaptation network (ECAN). The core idea is to capture local neighborhood information between samples while considering the global class-level adaptation. Specifically, we propose a nearest neighbor contrastive learning to promote local emotion consistency among features of highly similar samples. Furthermore, relying solely on nearest neighborhoods may lead to ambiguous boundaries between clusters. Thus, we incorporate supervised contrastive learning to encourage greater separation between clusters representing different emotions, thereby facilitating improved class-level adaptation. Extensive experiments indicate that our proposed ECAN significantly outperforms state-of-the-art methods under the source-free cross-corpus SER setting on several speech emotion corpora.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Over the course of the last decade, diverse SER applications have gained lots of attention with tremendous progress of deep learning  [1, 2, 3, 4] . Though achieving huge successes, conventional SER methods may encounter performance degradation, even when the training data and the test data deviate slightly from each other. Thus, researchers turn their attention to cross-corpus SER, where the training data and test data come from different corpora, and multiple methods have been proposed for cross-corpus SER  [5, 6] . Conventional cross-corpus SER methods trend to alleviate the domain discrepancy by domain matrices  [7, 8]  or adversarial training  [9, 10] . Taking the adversarial training based methods as an example, this kind of methods obfuscates the domain discriminator to prevent it from distinguishing between the source and target corpus samples.\n\nCommon cross-corpus SER algorithms assume all data is available during adaptation. In real-life scenarios, this assumption is rarely possible due to data privacy protection. Labeled emotional voices can be treated as a form of identification for specific individuals  [11] . Improper disclosure of data with corresponding labels could unduly influence data providers. This paper focuses on a more practical and interesting task called source-free cross-corpus SER, where source data is inaccessible during adaptation. The goal is to adapt a pre-trained model, originally trained on a source corpus, to perform well on a target corpus without any labeled source data. Traditional cross-corpus SER methods focus on matching the target feature distribution with the source one to alleviate domain gaps. However, here, this does not work since source-free cross-corpus SER faces the challenge of source distribution estimation without access to the source data. In this context, the main dilemma of this task is how to effectively utilize the pre-trained source model to identify target samples correctly despite the presence of domain shifts.\n\nTo tackle the more practical and previously unexplored sourcefree cross-corpus SER problem, we propose a simple, yet effective method called emotion-aware contrastive adaptation network (ECAN). It is worth noting that this is the first work dedicated to addressing the source-free cross-corpus SER problem. The key idea is to update the target model using the pre-trained source model from both local and global perspectives. Building upon previous research  [12, 13] , we find that target data of the same emotion tends to form a cluster in the feature space despite domain shifts between source and target corpora. To exploit this inherent local structure between target data, we propose a novel nearest neighbor contrastive learning algorithm to enhance the semantic consistency among neighboring samples. Besides, solely relying on nearest neighbor information for adaptation may result in ambiguous category boundaries, as the local structure may not capture the full complexity of emotion distribution within the target data. To address this limitation, we incorporate supervised contrastive learning into the network, which aims to pull away clusters of different emotions, to achieve emotion-wise global adaptation. Both nearest neighbor and supervised contrastive learn- ing modules work in a promotion way, jointly considering enhancing of nearest neighbor information and emotion-level global adaptation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we will present the details of the proposed ECAN for coping with the problem of source-free cross-corpus SER. Formally, we denote the labeled source speech emotion corpus and unlabeled target one as Ds and Dt, respectively. Both corpora have the same predefined C emotion classes. In source-free cross-corpus SER setting, the source corpus Ds is only available for source model pretraining, while in target adaptation we merely access the pre-trained source model and the unlabeled target corpus Dt. The feature extractor takes a speech sample xi as input and produces a feature representation denoted as fi = f (xi) ∈ R d , where d is the dimension of the feature space. The output of the classifier is denoted as pi = σ(fi) ∈ R C , where σ(.) is the softmax function. Fig.  1  shows the proposed structure.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Nearest Neighbor Contrastive Learning",
      "text": "As mentioned before, even though the source classifier may not be suitable for the target domain, target speech samples tend to form distinct clusters in the feature space. It indicates that similar samples are expected to be close to each other. Thus, we propose a nearest neighbor contractive learning module to leverage the local information within the target data and enhance semantic consistency. This module focuses on the adaptation between samples and aims to reinforce the relationships among nearest neighbors.\n\nTo retrieve nearest neighbors during training, we build a feature memory bank F = [f1, f2, ..., fN t ] to store features of Nt target samples. The cosine similarity is used for retrieving k-nearest neighbors:\n\nIt is important to note that before every batch training iteration, we update the feature bank F by replacing the existing items with their corresponding counterparts from the current batch. Additionally, we set the number of nearest neighbors k for each sample as 1.\n\nWe treat the nearest neighbors as positive pairs and other samples as negative ones. Based on the InfoNCE loss [14], we define the nearest neighbor contrastive learning loss as:\n\nwhere k + is the feature in the k-nearest neighbors set N i K of fi, and N f i denotes the feature set except fi. ϕ(.) denotes the cosine similarity. τ is a temperature hyper-parameter and is empirically set as 0.05. By minimizing the loss function L ncl , our ECAN can push features towards their nearest neighbors and pull them away from dissimilar ones.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Supervised Contrastive Learning",
      "text": "In the nearest neighbor contrast approach, there is a possibility of encountering noisy neighbors that belong to different emotion categories. This can lead to incorrect supervision. To address this issue, we propose a supervised contrastive learning module, which aims to bring features belonging to the same category closer and push features from different categories farther apart. To implement this, a score bank S = [p1, p2, ..., pN t ] is introduced to store the softmax prediction scores of all target data points. Similar to the feature bank F, the score bank S is updated before each batch training iteration. In the supervised contrastive learning process, the samples with the same category are searched within the score bank S. The corresponding features in the feature bank F are extracted to form the i th emotion feature set Ci. We define the supervised contrastive learning loss L scl as follows:\n\nwhere q + is the feature in the Ci, and NC i is the number of features in Ci. Through optimizing L scl , ECAN reduces the impact of unreasonable nearest neighbors via enhancing the inter-class discrimination and intra-class compactness of target features. Update the model based on SGD algorithm",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Total Objective",
      "text": "We further encourage the diversity in model predictions to avoid collapsing solutions, where the model predicts some specific classes for all target samples. The diversity loss L div that encourages the prediction balance is defined below:\n\nwhere KL (•||•) denotes the Kullback-Leibler divergence  [15] . The p\n\nis the predicted score of the c th emotion class of xi, and pc represents the predicted probability of class c, which is regularized by the uniform distribution.\n\nFinally, the total adaptive loss function combining Eqs. (  2 ), (3), and (4) is as follows:\n\nwhere λ, β are trade-off coefficients to balance local structural clustering and emotion discrimination. The training process of our proposed ECAN is illustrated in Algorithm 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Corpora And Evaluation Protocols",
      "text": "We utilize four public available speech emotion corpora, i.e., EMOVO (O)  [16] , EmoDB (B)  [17] , eNTERFACE (E)  [18] , and CASIA (C)  [19]  In source-free cross-corpus SER, one speech corpus serves as the source one and the other corpus is the target one. By alternatively choosing either two of the above four speech emotion corpora, we design twelve tasks. The sample statistics is shown in Tab 1. For evaluation metric, we adopt unweighted average recall (UAR), which is defined as the average of the prediction accuracy per class. We also report the average results of all tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines And Implementation Details",
      "text": "To validate the effectiveness of our proposed ECAN, we adopt several domain adaptation methods as baselines for comparison, i.e., SHOT  [12] , NRC  [13] , DAN  [20] , G-SFDA  [21] , CoWA-JMDS  [22] , USFAN  [23] , DaC  [24] , and AaD  [25] . Besides, we also compare to the source-only model, which refers to the pre-trained model by source data.\n\nFor the aforementioned methods, we refer to the official code implementation to conduct experiments. To ensure a fair comparison, we choose VGG-11  [26]  as the backbone architecture and leverage the Mel spectrum with a size of 224 × 224 as the network input. For pre-training the source model, we employ label smoothing and stochastic gradient descent with a momentum of 0.9. The training process is conducted for 100 epochs. To obtain the optimal results, we perform a parameter search by exploring different trade-offs within fixed intervals for the mentioned methods. Specifically, for SHOT, USFAN and DAN, the search interval is {0.0001:0.0001:0.001, 0.001:0.001:0.01, 0.01:0.01:0.1, 0.1:0.1:1, 2, 5, 10, 100}. For G-SFDA and AaD, we search the nearest neighbor number from [1:1:10]. For NRC, both numbers of reciprocal neighbor and expanded neighbor are searched from [1:2:9]. The mixup weight parameter set of CoWA-JMDS is {0.1:0.1:1, 5, 50, 100}. As for ECAN and DaC, λ and β are searched from {0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100} and {0.1, 0.3, 0.6, 0.9, 2}, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison With State-Of-The-Arts",
      "text": "Experimental results of different methods are reported in Tab 2. We can observe some interesting findings. Firstly, it can be obviously seen that our proposed ECAN achieves the best performance compared to all other comparison methods, where the average UAR reaches 37.19%. Diving deeper into each task, our ECAN outperforms other methods in seven out of twelve cross-corpus SER tasks, especially in B→C (39.00%) and C→E (34.53%) tasks. Despite not achieving the best results in the remaining tasks, ECAN is still competitive with the best performing methods, e.g., in the C→O task (ECAN 35.91% v.s. DAN 36.51%). Moreover, ECAN remains superior even when compared to the DAN method that has access to source corpus data. These findings demonstrate the effectiveness of our proposed ECAN in dealing with the source-free cross-corpus SER problem.\n\nBased on the experimental results, it is evident that most methods struggle to perform well on the C→E and E→C tasks. It can be attributed to the variations in emotion induction methods and recorded language between the CASIA and eNTERFACE datasets. Specifically, CASIA consists of Chinese corpora where the speech  samples are acted by the speakers, while eNTERFACE comprises English corpora where emotions are induced using pre-prepared materials. A similar trend can also be observed in the tasks between EMOVO and eNTERFACE (E→O and O→E tasks), where EMOVO consists of acted Italian that significantly differs from eNTERFACE. These differences in language, emotion induction techniques, and cultural nuances could affect the performance of the methods on these specific tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Cross-Corpus Ser Methods",
      "text": "In this section, we conduct a comparison between our proposed ECAN and previous cross-corpus SER algorithms on six tasks, as depicted in Fig.  2 . Notably, even without the presence of source data, ECAN achieves comparable performance to source-available methods such as DIDAN  [6]  and DTTRN  [7] . Specifically, our ECAN surpasses methods that utilize source data in the C→B and C→E tasks, while slightly trailing behind in the remaining tasks. These results highlight the superiority of our proposed ECAN algorithm in successfully addressing cross-corpus SER challenges.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the effectiveness of our proposed functions, we perform ablation experiments on six tasks. Tab 3 shows results of the ablation study. It is evident that when L ncl and L scl are considered together, the performance is consistently better compared to using either loss alone. Moreover, the results decrease when L div is removed, indicating the importance of L div that promotes diversity among samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Visualization",
      "text": "To provide a more visually compelling demonstration of our algorithm's performance, we employ t-SNE  [27]  for feature visualiza-  tion. Specifically, we focus on the features extracted by our model in the C→B task and present the results in Fig.  3 . Each colored dot represents a different emotion category. From Fig.  3 (a), we can observe that the target features are initially scattered in the feature space, which can be attributed to the domain shift before adaptation. However, after the adaptation, the target features form clearer and more compact clusters that correspond to specific emotion categories. This outcome clearly demonstrates the effectiveness of our proposed strategy, which simultaneously considers nearest neighbors and separates clusters of different emotion categories. The improved clustering of target features showcases the efficacy of our algorithm in addressing the challenges of cross-corpus SER tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a simple yet effective method called ECAN to solve a practical and previously unexplored source-free cross-corpus SER problem. ECAN leverages the local structure of the target data by utilizing nearest neighbor contrastive learning. This approach allows for successful adaptation without relying on the source data. Besides, ECAN enhances class-level adaptation by aggregating features within the same emotion category and separating different emotion clusters to create clear classification boundaries. Extensive experiments were conducted on four widely used speech emotion corpora, and the results verify the effectiveness of our proposed ECAN in dealing with the source-free cross-corpus SER task. In future work, we will further explore neighboring information and compare with other advanced algorithms.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview Structure of the Proposed ECAN in Dealing with Source-free Cross-Corpus SER.",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the proposed structure.",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison with Cross-Corpus SER Methods.",
      "page": 4
    },
    {
      "caption": "Figure 2: Notably, even without the presence of source data,",
      "page": 4
    },
    {
      "caption": "Figure 3: The t-SNE visualization on the task of C→B.",
      "page": 4
    },
    {
      "caption": "Figure 3: Each colored",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), we can",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "Source Only\nDAN (ICML’ 15)",
          "Source-free": "-\n✘",
          "B→C\nC→B\nB→E\nE→B\nC→E\nE→C\nB→O\nO→B\nC→O\nO→C\nE→O\nO→E": "30.10\n41.78\n25.34\n27.83\n20.75\n22.90\n30.24\n20.83\n25.79\n25.92\n25.20\n22.35\n36.51\n32.42\n32.14\n36.30\n56.72\n33.58\n43.50\n32.17\n29.30\n35.53\n44.24\n28.93",
          "Avg.": "26.59\n36.78"
        },
        {
          "Methods": "SHOT (ICML’ 20)\nG-SFDA (ICCV’ 21)\nNRC (NeurIPS’ 21)\nUSFAN (ECCV’ 22)\nCoWA-JMDS (ICML’ 22)\nDaC (NeurIPS’ 22)\nAaD (NeurIPS’ 22)",
          "Source-free": "✔\n✔\n✔\n✔\n✔\n✔\n✔",
          "B→C\nC→B\nB→E\nE→B\nC→E\nE→C\nB→O\nO→B\nC→O\nO→C\nE→O\nO→E": "34.80\n55.83\n31.92\n46.08\n30.99\n31.80\n35.52\n44.63\n34.13\n30.00\n27.98\n28.76\n27.90\n50.41\n23.21\n35.79\n27.09\n24.30\n25.20\n36.32\n24.40\n20.42\n25.00\n25.36\n44.93\n37.80\n60.16\n31.87\n48.42\n32.22\n31.40\n35.12\n32.74\n29.00\n29.76\n27.73\n51.73\n32.70\n53.72\n30.77\n30.60\n29.00\n33.93\n44.65\n31.94\n30.08\n29.17\n27.53\n34.50\n55.26\n28.49\n36.74\n31.61\n25.80\n29.96\n27.78\n29.96\n25.75\n25.99\n24.08\n31.90\n47.42\n26.42\n39.03\n25.60\n27.00\n24.40\n30.19\n25.00\n25.83\n28.37\n24.63\n44.93\n35.00\n55.50\n31.47\n48.12\n32.17\n29.10\n34.52\n34.92\n27.30\n25.60\n26.50",
          "Avg.": "36.04\n28.78\n36.76\n35.49\n31.33\n29.65\n35.43"
        },
        {
          "Methods": "ECAN (Ours)",
          "Source-free": "✔",
          "B→C\nC→B\nB→E\nE→B\nC→E\nE→C\nB→O\nO→B\nC→O\nO→C\nE→O\nO→E": "39.00\n61.37\n34.21\n34.53\n31.90\n36.51\n29.16\n46.87\n40.86\n35.91\n27.42\n28.57",
          "Avg.": "37.19"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Wenming Zheng",
        "Yang Li",
        "Chuangao Tang",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "Cheng Lu",
        "Wenming Zheng",
        "Hailun Lian",
        "Yuan Zong",
        "Chuangao Tang",
        "Sunan Li",
        "Yan Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "6",
      "title": "Crosscorpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee",
        "Jong Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "7",
      "title": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "authors": [
        "Yan Zhao",
        "Jincen Wang",
        "Yuan Zong",
        "Wenming Zheng",
        "Hailun Lian",
        "Li Zhao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Deep transductive transfer regression network for cross-corpus speech emotion recognition",
      "authors": [
        "Yan Zhao",
        "Jincen Wang",
        "Ru Ye",
        "Yuan Zong",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Cross-corpus speech emotion recognition using joint distribution adaptive regression",
      "authors": [
        "Jiacheng Zhang",
        "Lin Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Bjorn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "Bo-Hao Su",
        "Chi-Chun Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Seeing voices and hearing faces: Cross-modal biometric matching",
      "authors": [
        "Arsha Nagrani",
        "Samuel Albanie",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "Jian Liang",
        "Dapeng Hu",
        "Jiashi Feng"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "14",
      "title": "Exploiting the intrinsic neighborhood structure for source-free domain adaptation",
      "authors": [
        "Shiqi Yang",
        "Joost Van De Weijer",
        "Luis Herranz",
        "Shangling Jui"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "15",
      "title": "Rényi divergence and kullback-leibler divergence",
      "authors": [
        "Tim Van Erven",
        "Peter Harremos"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "16",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the ninth international conference on language resources and evaluation (LREC'14)"
    },
    {
      "citation_id": "17",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "18",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "Olivier Martin",
        "Irene Kotsia",
        "Benoît Macq",
        "Ioannis Pitas"
      ],
      "year": "2006",
      "venue": "ICDE Workshops"
    },
    {
      "citation_id": "19",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge"
    },
    {
      "citation_id": "20",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "Mingsheng Long",
        "Yue Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Generalized source-free domain adaptation",
      "authors": [
        "Shiqi Yang",
        "Yaxing Wang",
        "Joost Van De Weijer",
        "Luis Herranz",
        "Shangling Jui"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021"
    },
    {
      "citation_id": "22",
      "title": "Confidence score for source-free unsupervised domain adaptation",
      "authors": [
        "Jonghyun Lee",
        "Dahuin Jung",
        "Junho Yim",
        "Sungroh Yoon"
      ],
      "venue": "International Conference on Machine Learning, ICML 2022"
    },
    {
      "citation_id": "23",
      "title": "Uncertainty-guided source-free domain adaptation",
      "authors": [
        "Subhankar Roy",
        "Martin Trapp",
        "Andrea Pilzer",
        "Juho Kannala",
        "Nicu Sebe",
        "Elisa Ricci",
        "Arno Solin"
      ],
      "year": "2022",
      "venue": "Computer Vision -ECCV 2022 -17th European Conference"
    },
    {
      "citation_id": "24",
      "title": "Divide and contrast: Sourcefree domain adaptation via adaptive contrastive learning",
      "authors": [
        "Ziyi Zhang",
        "Weikai Chen",
        "Hui Cheng",
        "Zhen Li",
        "Siyuan Li",
        "Liang Lin",
        "Guanbin Li"
      ],
      "year": "2022",
      "venue": "Divide and contrast: Sourcefree domain adaptation via adaptive contrastive learning"
    },
    {
      "citation_id": "25",
      "title": "Attracting and dispersing: A simple approach for source-free domain adaptation",
      "authors": [
        "Shiqi Yang",
        "Yaxing Wang",
        "Kai Wang",
        "Shangling Jui",
        "Joost Van De Weijer"
      ],
      "year": "2022",
      "venue": "Attracting and dispersing: A simple approach for source-free domain adaptation"
    },
    {
      "citation_id": "26",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}