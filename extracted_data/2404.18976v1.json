{
  "paper_id": "2404.18976v1",
  "title": "Foundations Of Multisensory Artificial Intelligence",
  "published": "2024-04-29T14:45:28Z",
  "authors": [
    "Paul Pu Liang"
  ],
  "keywords": [
    "Multimodal Machine Learning",
    "Multisensory Artificial Intelligence",
    "Deep Learning",
    "Information Theory",
    "Quantification",
    "Generalization",
    "Affective Computing",
    "AI and Healthcare 283",
    "713] & raw [47",
    "502] fusion Coordination (2.3.2) Strong [181",
    "498] & partial [635",
    "728] coordination Fission (2.3.3) Modality-level [235",
    "615] & fine-grained [1",
    "92] fission Alignment (2.4) Discrete connections (2.4.1) Local [121",
    "247] & global [351] alignment Continuous alignment (2.4.2) Warping [224",
    "252] & segmentation [577] Contextualization (2.4.3) Joint [348]",
    "cross-modal [232",
    "390] & graphical [688] Reasoning (2.5) Structure modeling (2.5.1) Hierarchical [26]",
    "temporal [677]",
    "interactive [394]",
    "discovery [479] Intermediate concepts (2.5.2) Attention [681]",
    "discrete symbols [22",
    "633]",
    "language [265",
    "723] Inference paradigm (2.5.3) Logical [203",
    "587] & causal [8",
    "448",
    "699] External knowledge (2.5.4) Knowledge graphs [213",
    "740] & commonsense [467",
    "720] Generation (2.6) Summarization (2.6.1) Extractive [96",
    "629] & abstractive [345",
    "461] Translation (2.6.2) Exemplar-based [294",
    "331] & generative [13",
    "281",
    "503] Creation (2.6.3) Conditional decoding [142",
    "452",
    "738] Transference (2.7) Cross-modal transfer (2.7.1) Tuning [501",
    "623]",
    "multitask [370",
    "552] & transfer [391] Co-learning (2.7.2) Representation [285",
    "717] & generation [483",
    "590] Model Induction (2.7.3) Co-training [65",
    "159] & co-regularization [564",
    "693] Quantification (2.8) Heterogenity (2.8.1) Importance [192",
    "466]",
    "bias [231",
    "474] & noise [398] Interconnections (2.8.2) Connections [7",
    "79",
    "602] & interactions [235",
    "374",
    "655] Learning (2.8.3) Generalization [370",
    "506]",
    "optimization [652",
    "671]",
    "tradeoffs [367]"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Building multisensory artificial intelligence systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions often present in multimodal problems  [375] . Using these principles as a foundation, we propose a taxonomy of six core challenges in multimodal research: representation, alignment, reasoning, generation, transference, and quantification. Recent technical achievements will be presented through this taxonomy, allowing researchers to understand the similarities and differences across approaches, and identifying open problems for future research. The bulk of the thesis covers our recent progress towards tackling two key problems in multimodal learning: the machine learning foundations of multimodal interactions, as well as practical methods for building multisensory foundation models that generalize to many modalities and tasks in the real world. In the first part, we study the foundations of multimodal interactions: the basic principle of how modalities combine to give rise to new information for a task. We present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task, such as sarcasm identified from the incongruity between spoken words and vocal expressions  [371] . Using this theoretical framework, we propose two practical estimators to quantify the interactions in real-world datasets. Quantifying the types of interactions a multimodal task requires enables researchers to decide which modality to collect  [376] , design suitable approaches to learn these interactions  [373] , and analyze whether their model has succeeded in learning  [374] . In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities. We first introduce MULTIBENCH, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas  [367] . We will also present the cross-modal attention  [101, 359]  and multimodal transformer  [614]  architectures that now underpin many of today's multimodal foundation models. Scaling these architectures on MULTIBENCH enables the creation of general-purpose multimodal multitask models across a variety of tasks, and we have collaborated broadly with practitioners to apply these models for real-world impact on affective computing, mental health, and cancer prognosis. We conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multimodal artificial intelligence. I owe my greatest acknowledgments to my advisors, mentors, and thesis committee members for their invaluable guidance during my PhD. To Louis-Philippe Morency and Ruslan Salakutdinov, who have closely guided my research and personal development at every stage over the past 5 years. LP has mentored me closely in all aspects of research -brainstorming ideas, idea execution, and written and oral presentations. Some of the best memories I've had during my PhD have been whiteboard brainstorming sessions, coming up with good names for problems and models, and collaboratively figuring out the best ways to visually depict technical concepts. Russ's incredibly sharp insight and keen eye for impactful problems has shaped my thinking and forced me to work on problems that matter in practice, and I've thoroughly enjoyed our recent push towards interactive multimodal agents with other folks in the group and at CMU. Thank you LP and Russ for additionally giving me the opportunity to co-instruct and guest lecture CMU courses multimodal ML, deep learning, and socially intelligent AI. I also had the pleasure of working closely with Manuel Blum and Lenore Blum during the senior years of my PhD. I have learned a lot from our discussions at the intersection of artificial intelligence, consciousness, and neuroscience, which have changed how I look at long-term problems and approach them. Manuel and Lenore have also inspired me to think big and make broad impact across CS and beyond, giving me many opportunities to communicate my ideas and contributions to a wide audience in neuroscience, psychology, and more. Finally, Trevor Darrell has been a source of inspiration as a senior faculty and has given me great advice for my PhD research and broadly for my research career. Some of his early works in multimodal machine learning and multimodal interaction are still some of my favorite works in this space. Beyond my committee members, I would like to acknowledge other CMU faculty and students with whom I've had fruitful collaborations, discussions, and received helpful feedback on ideas, paper drafts, and presentations. Fantastic students in LP and Russ's research groups:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "List Of Figures",
      "text": "1.1 This thesis is designed to advance the theoretical and computational foundations of multimodal machine learning, and enable the creation of next-generation multimodal technologies. It starts by identifying the common themes and open questions in the field, through a taxonomy of six core challenges in multimodal research: representation, alignment, reasoning, generation, transference, and quantification. The bulk of the thesis studies two core challenges in multimodal learning: (1) building a foundation for multimodal interactions that enables the quantification of multimodal interactions in data and their principled modeling using machine learning methods, and (2) the data requirements and model building blocks enabling generalization of knowledge between modalities, tasks, and their representations. (1) Representation studies how to summarize multimodal data to reflect the heterogeneity and interconnections between individual modality elements, before (2) alignment captures the connections and interactions between multiple local elements according to their structure. After representation and alignment comes (3) reasoning, which aims to combine the information from multimodal evidence in a principled way that respects the structure of the problem to give more robust and interpretable predictions. While most systems aim to predict the label y, there are also cases where the goal is (4) generation, to learn a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence, or (5) transference, to transfer information from high-resource modalities to low-resource ones and their representations. Finally,  (6)  quantification revisits the previous challenges to give deeper empirical and theoretical understanding of modality heterogeneity, interconnections, and the learning process. . . . . . . . . . . . . . . . . . . . . . . xi 2.2 The information present in different modalities will often show diverse qualities, structures, and representations. Dimensions of heterogeneity can be measured via differences in individual elements and their distribution, the structure of elements, as well as modality information, noise, and task relevance. . . . . . . .",
      "page_start": 4,
      "page_end": 12
    },
    {
      "section_name": "2.3",
      "text": "Modality connections describe how modalities are related and share commonalities, such as correspondences between the same concept in language and images or dependencies across spatial and temporal dimensions. Connections can be studied through both statistical and semantic perspectives. . . . . . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.4",
      "text": "Several dimensions of modality interactions: (1) Interaction information studies whether common redundant information or unique non-redundant information is involved in interactions;\n\n(2) interaction mechanics study the manner in which interaction occurs, and (3) interaction response studies how the inferred task changes in the presence of multiple modalities. . . . . . . . . . . . . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.5",
      "text": "Challenge 1 aims to learn representations that reflect cross-modal interactions between individual modality elements, through (1) fusion: integrating information to reduce the number of separate representations, (2) coordination: interchanging cross-modal information by keeping the same number of representations but improving multimodal contextualization, and (3) fission: creating a larger set of decoupled representations that reflects knowledge about internal structure. . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.6",
      "text": "We categorize representation fusion approaches into (1) fusion with abstract modalities, where unimodal encoders first capture a holistic representation of each element before fusion at relatively homogeneous representations, and (2) fusion with raw modalities which entails representation fusion at very early stages, perhaps directly involving heterogeneous raw modalities. . . . . . . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.7",
      "text": "There is a spectrum of representation coordination functions: strong coordination aims to enforce strong equivalence in all dimensions, whereas in partial coordination only certain dimensions may be coordinated to capture more general connections such as correlation, order, hierarchies, or relationships. . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.8",
      "text": "Representation fission creates a larger set of decoupled representations that reflects knowledge about internal structure. (1) Modality-level fission factorizes into modality-specific information primarily in each modality, and multimodal information redundant in both modalities, while (2) fine-grained fission attempts to further break multimodal data down into individual subspaces. . . . . . . . . .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2.9",
      "text": "Alignment aims to identify cross-modal connections and interactions between modality elements. Recent work has involved (1) discrete alignment to identify connections among discrete elements, (2) continuous alignment of continuous signals with ambiguous segmentation, and (3) contextualized representation learning to capture these cross-modal interactions between connected elements. . occurs, which can be (1) hierarchical (i.e., more abstract concepts are defined as a function of less abstract ones), (  2 ) temporal (i.e., organized across time), (  3 ) interactive (i.e., where the state changes depending on each step's decision), and (4) discovered when the latent structure is unknown and instead directly inferred from data and optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.15 How can we learn a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence? Generation involves (1) summarizing multimodal data to highlight the most salient parts, (2) translating from one modality to another while being consistent with modality connections, and (3) creating multiple modalities simultaneously while maintaining coherence. 2.  16  Transference studies the transfer of knowledge between modalities, usually to help a noisy or limited primary modality, via (1) cross-modal transfer from models trained with abundant data in the secondary modality, (2) multimodal co-learning to share information across modalities by sharing representations, and (3) model induction that keeps individual unimodal models separate but induces behavior in separate models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.17 Quantification: what are the empirical and theoretical studies we can design to better understand (1) the dimensions of heterogeneity, (2) the presence and type of interconnections, and (3) the learning and optimization challenges? . . . . . . 2.  18  The subchallenge of heterogeneity quantification aims to understand the dimensions of heterogeneity commonly encountered in multimodal research, such as (1) different quantities and usages of modality information, (2) the presence of modality biases, and (3) quantifying and mitigating modality noise. . . . . . . . . 2.  19  Quantifying modality interconnections studies (1) connections: can we discover what modality elements are related to each other and why, and (2) interactions: can we understand how modality elements interact during inference? . . . . . . .",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "2.20",
      "text": "Studying the multimodal learning process involves understanding (1) generalization across modalities and tasks, (2) optimization for balanced and efficient training, and (3) tradeoffs between performance, robustness, and complexity in the real-world deployment of multimodal models. . . . . . . . . . . . . . . . . . .\n\n3.1 PID decomposes I(X 1 , X 2 ; Y ) into redundancy R between X 1 and X 2 , uniqueness U 1 in X 1 and U 2 in X 2 , and synergy S in both X 1 and X 2 . . . . . . . . . . .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "3.2",
      "text": "We propose BATCH, a scalable estimator for PID over high-dimensional continuous distributions. BATCH parameterizes q using a matrix A learned by neural networks such that mutual information objectives over q can be optimized via gradient-based approaches over minibatches. Marginal constraints q ∈ ∆ p are enforced through a variant of the Sinkhorn-Knopp algorithm on A. . . . . . . . .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "3.4",
      "text": "We find high correlation (ρ = 0.8) between the performance drop when X i is missing and the model's U i value: high U i coincides with large performance drops (red), but low U i can also lead to performance drops. The latter can be further explained by large S so X i is necessary (green). . . . . . . . . . . . . . . . . . . .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Left:",
      "text": "We define S = I(X 1 ; X 2 ; Y ) as task-relevant shared information and U 1 = I(X 1 ; Y |X 2 ), U 2 = I(X 2 ; Y |X 1 ) as task-relevant unique information. Right: On controllable datasets with varying ratios of S, U 1 , and U 2 , standard CL captures S but struggles when there is more U 1 and U 2 . Our FACTORCL approach maintains best performance, whereas SimCLR  [103]  and SupCon  [300]  see performance drops as unique information increases, and Cross+Self  [258, 278, 337, 710]  recovers in fully unique settings but suffers at other ratios. . . . . . . . . . . . . .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Factorcl:",
      "text": "We propose a self-supervised CL method to learn factorized representations Z S 1 , Z S 2 , Z U 1 , and Z U 2 to capture task-relevant information shared in both X 1 and X 2 , unique to X 1 , and unique to X 2 . By starting with informationtheoretic first principles of shared and unique information, we design contrastive estimators to both capture task-relevant and remove task-irrelevant information, where a notion of task-relevance without explicit labels is afforded by a new definition of multimodal augmentations X ′ 1 , X ′ 2 . Lower bounds are in green and upper bounds are in red. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  [453]  and our proposed upper bound I NCE-CLUB on sample distributions with changing mutual information: our upper bound is tighter, more accurate, and more stable than I CLUB upper bound  [110] , and also comes for 'free' via jointly estimating both lower and upper bounds simultaneously. We find that as dimension increases, the I CLUB estimator collapses to zero and no longer tracks true MI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Standard vs. unique augmentations for the figurative language  [701]  dataset. After augmenting text modality X 1 independently (same for both augmentation types), we illustrate their differences for image augmentation: unique augmentation on images should avoid removing information referred to by X 1 (the text). The text mentions that the car is fast so unique augmentation for images should not remove the highway pixels of the image which can suggest the car is fast. . . . . . . . . .",
      "page_start": 83,
      "page_end": 83
    },
    {
      "section_name": "Estimated I Nce Lower Bound",
      "text": "",
      "page_start": 83,
      "page_end": 83
    },
    {
      "section_name": "Left:",
      "text": "We scaffold the problem of multimodal interpretability and propose MUL-TIVIZ, a comprehensive analysis method encompassing a set of fine-grained analysis stages: (1) unimodal importance identifies the contributions of each modality, (2) cross-modal interactions uncover how different modalities relate with each other and the types of new information possibly discovered as a result of these relationships, (3) multimodal representations study how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction studies how these features are composed to make a prediction. Right: We visualize multimodal representations through local and global analysis. Given an input datapoint, local analysis visualizes the unimodal and cross-modal interactions that activate a feature. Global analysis informs the user of similar datapoints that also maximally activate that feature, and is useful in assigning human-interpretable concepts to features by looking at similarly activated input regions (e.g., the concept of color). . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Examples of cross-modal interactions discovered by our proposed second-order gradient approach: first taking a gradient of model f with respect to an input word (e.g., x 1 = birds), before taking a second-order gradient with respect to all image pixels (highlighted in green) or bounding boxes (in red boxes) x 2 indeed results in all birds in the image being highlighted. . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 93,
      "page_end": 94
    },
    {
      "section_name": "Multiviz Provides An Interactive Visualization Api Across Multimodal Datasets",
      "text": "and models. The overview page shows general unimodal importance, cross-modal interactions, and prediction weights, while the features page enables local and global analysis of specific user-selected features. . . . . . . . . . . . . . . . . . . . 5.4 Examples of human-annotated concepts using MULTIVIZ on feature representations. We find that the features separately capture image-only, language-only, and multimodal concepts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Examples of human-annotated error analysis using MULTIVIZ on multimodal models. Using all stages provided in MULTIVIZ enables fine-grained classification of model errors (e.g., errors in unimodal processing, cross-modal interactions, and predictions) for targeted debugging. . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 15,
      "page_end": 21
    },
    {
      "section_name": "5.6",
      "text": "A case study on model debugging: we task 3 human users to use MULTIVIZ visualizations and highlight the errors that a pretrained LXMERT model finetuned on VQA 2.0 exhibits, and find 2 penultimate-layer neurons highlighting the model's failure to identify color (especially blue). Targeted localization of the error to this specific stage (prediction) and representation concept (blue) via MULTIVIZ enabled us to identify a bug in the popular Hugging Face LXMERT repository. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 101,
      "page_end": 101
    },
    {
      "section_name": "6.1",
      "text": "We study the relationships between (left) synergy and redundancy as a result of the task Y either increasing or decreasing the shared information between X 1 and X 2 (i.e., common cause structures as opposed to redundancy in common effect), as well as (right) synergy and uniqueness due to the disagreement between unimodal predictors resulting in a new prediction y ≠ y 1 ≠ y 2 (rather than uniqueness where y = y 2 ≠ y 1 ). . . 2 MULTIBENCH provides a standardized machine learning pipeline across data processing, data loading, multimodal models, evaluation metrics, and a public leaderboard to encourage future research in multimodal representation learning. MULTIBENCH aims to present a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. . . . . . . . . . . . . . . .",
      "page_start": 108,
      "page_end": 108
    },
    {
      "section_name": "7.",
      "text": "3 MULTIZOO provides a standardized implementation of a suite of multimodal methods in a modular fashion to enable accessibility for new researchers, compositionality of approaches, and reproducibility of results. . . . . . . . . . . . . . . .",
      "page_start": 109,
      "page_end": 109
    },
    {
      "section_name": "7.4",
      "text": "Relative performance of each model across in-domain (red dots) and out-domain datasets (blue dots). In-domain refers to the performance on datasets that the method was previously proposed for and out-domain shows performance on the remaining datasets. We find that many methods show strongest performance on in-domain datasets which drop when tested on different domains, modalities, and tasks. In general, we also observe high variance in the performance of multimodal methods across datasets in MULTIBENCH, which suggest open questions in building more generalizable models. . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 130,
      "page_end": 130
    },
    {
      "section_name": "7.5",
      "text": "Relative performance of each model across different domains. We find that the performance of multimodal models varies significantly across datasets spanning different research areas and modalities. Similarly, the best-performing methods on each domain are also different. Therefore, there still does not exist a one-sizefits-all model, especially for understudied modalities and tasks. . . . . . . . . . . The second stage selects the loud voice behavior which is locally interpreted as emphasis before being fused with previous stages into a strongly negative representation. Finally, the third stage selects the shrugging and speech elongation behaviors that reflect ambivalence and when fused with previous stages is interpreted as a representation for the disappointed emotion. .",
      "page_start": 130,
      "page_end": 130
    },
    {
      "section_name": "8.3",
      "text": "The RECURRENT MULTISTAGE FUSION NETWORK for multimodal language analysis. The Multistage Fusion Process has three modules: HIGHLIGHT, FUSE and SUMMARIZE. Multistage fusion begins with the concatenated intra-modal network outputs h l t , h v t , h a t . At each stage, the HIGHLIGHT module identifies a subset of multimodal signals and the FUSE module performs local fusion before integration with previous fusion representations. The SUMMARIZE module translates the representation at the final stage into a cross-modal representation z t to be fed back into the intra-modal recurrent networks. . . . . . . . . . . . . . . .",
      "page_start": 140,
      "page_end": 140
    },
    {
      "section_name": "Visualization Of Sample Crossmodal Attention Weights From Layer 3 Of [V → L]",
      "text": "crossmodal transformer on CMU-MOSEI. We found that the crossmodal attention has learned to correlate certain meaningful words (e.g., \"movie\", \"disappointing\") with segments of stronger visual signals (typically stronger facial motions or expression change), despite the lack of alignment between original L/V sequences. Note that due to temporal convolution, each textual/visual feature contains the representation of nearby elements. . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.9 Visualization of learned attention weights across stages 1,2 and 3 of the multistage fusion process and across time of the multimodal sequence. We observe that the attention weights are diverse and evolve across stages and time. In these three examples, the red boxes emphasize specific moments of interest. (a) Synchronized interactions: the positive word \"fun\" and the acoustic behaviors of emphasis and elongation (t = 5) are synchronized in both attention weights for language and acoustic features. (b) Asynchronous trimodal interactions: the asynchronous presence of a smile (t = 2 ∶ 5) and emphasis (t = 3) help to disambiguate the language modality. (c) Bimodal interactions: the interactions between the language and acoustic modalities are highlighted by alternating stages of fusion (t = 4 ∶ 7). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.1 Heterogeneity quantification: Efficiently learning from many modalities requires measuring (1) modality heterogeneity: which modalities are different and should be separately processed, and (2) interaction heterogeneity: which modality pairs interact differently and should be separately fused. HIGHMMT uses these measurements to dynamically group parameters balancing performance and efficiency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "page_start": 18,
      "page_end": 22
    },
    {
      "section_name": "Highmmt Workflow:",
      "text": "(1) We estimate modality and interaction heterogeneity via modality transfer to determine which modalities should be processed and fused differently.  (2)  Using the inferred heterogeneity, we determine the optimal grouping of parameters balancing both total performance and parameter efficiency, which (3) informs our design of a heterogeneity-aware model with dynamic parameter sharing across many modalities and tasks. HIGHMMT enables statistical strength sharing, efficiency, and generalization to new modalities and tasks. . . . 9.3 HIGHMMT architecture: Given arbitrary modalities, (1) the inputs are standardized into a sequence and padded, (2) modality embeddings and positional encodings are added to the input sequence, (3) a single shared unimodal Perceiver encoder is applied to all modalities to learn modality-agnostic representations, (4) each pair of unimodal representations is fed through a shared multimodal crossattention layer to learn multimodal representations, and finally  (5)  all outputs are concatenated, batch-normalized, and fed into task-specific classification heads. . 9.4 HIGHMMT training involves 2 steps: (1) homogeneous pre-training of a fully shared model across all modalities, before (2) heterogeneity-aware fine-tuning of modality and interaction parameters in different groups to respect modality and interaction heterogeneity respectively. . . . . . . . . . . . . . . . . . . . . . . . . . 9.5 Modality and interaction heterogeneity matrices color coded by distances, with green showing smaller distances and dark red larger distances. We find clear task outliers (AV-MNIST has high difficulty transferring to others), and that there is generally more interaction heterogeneity than unimodal heterogeneity. Otherwise, the same modality and modality pairs across different tasks are generally similar to each other. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.6 Overall tradeoff. HIGHMMT pushes forward the Pareto front of performance and efficiency as compared to all possible (> 10 5 ) combinations of task-specific models across multiple datasets  [367] . The x-axis denotes (inverted) total parameters and y-axis denotes performance scaled to a 0 -1 range before averaging across datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   [367]  datasets. Many of the estimated interactions align well with human judgement as well as unimodal performance.",
      "page_start": 158,
      "page_end": 158
    },
    {
      "section_name": "List Of Tables",
      "text": "3.4 Average interactions (R/U /S) learned by models alongside their average performance on interaction-specialized datasets (D R /D U /D S ). Synergy is the hardest to capture and redundancy is relatively easier to capture by existing models. . . . .   [367]  datasets with varying shared and unique information: FACTORCL achieves strong results vs self-supervised (top 5 rows) and supervised (bottom 3 rows) baselines that do not have unique representations, factorization, upper-bounds to remove irrelevant information, and multimodal augmentations. It covers a diverse range of technical challenges, research areas, dataset sizes, input modalities (in the form of a: audio, e: embodied environment, f : force sensor, g: graph, i: image ℓ: language, o: optical flow, p: proprioception sensor, π: policy/action, q: question (for question-answering tasks), s: set, t: time-series, ta: tabular, v: video), and prediction tasks. We provide a standardized data loader for datasets in MULTIBENCH, along with a set of state-of-the-art multimodal models.",
      "page_start": 159,
      "page_end": 159
    },
    {
      "section_name": "Results On Multibench",
      "text": "",
      "page_start": 159,
      "page_end": 159
    },
    {
      "section_name": "9.3",
      "text": "Cross-modal few-shot transfer to new modalities and tasks. We train multitask HIGHMMT on 1/2/3 datasets and find that it generalizes few-shot to new modalities and tasks on the 4th dataset, with improved performance over single-task training on the 4th dataset. Cross-modal transfer improves with more pretraining tasks and works best on the smallest target tasks (UR-FUNNY). . . . . . . . . . 9.4 HIGHMMT achieves strong performance on overall performance and efficiency (mean and deviation over 10 runs), sometimes even beating (shown in bold) the task-specific state-of-the-art, especially on the relatively understudied modalities (time-series, robotics sensors, and sets) from the robotics (PUSH, V&T) HCI (ENRICO), and healthcare (MIMIC) research areas, while using 10× fewer parameters due to parameter sharing and multitask learning. SOTA captures the max performance and parameters of more than 20 task-specific multimodal models: [1] GRADBLEND  [652] ,  [2]  LF-LSTM  [148] ,  [3]  LF  [184] ,  [4]  MULT  [614] ,  [5]  MFAS  [479] ,  [6]  MFM  [687] , and  [7]  LRTF  [724] . . . . . . . . . . . . . . .  144 9.5  We conduct in-depth ablation studies and find strong evidence for (1) having separate unimodal and interaction layers, (2) determining parameter sharing via feature transfer, and (3) homogeneous pre-training before heterogeneity-aware fine-tuning into parameter groups (mean and standard deviation over 10 runs). .  145 9.6  We find evidence of significant parameter overlap across unimodal encoders:\n\n> 92% of neurons are involved in at least 3 of the 4 tasks, while the multimodal layers are more task-specific: only 10% of neurons are involved in 3 or 4 tasks. . 146 9.7 Parameter interference: we observe different performance drops on each task (columns) after training on one task with flipped labels (rows). Training the shared unimodal encoders causes the most harm, which implies that unimodal encoders contain more shared neurons sensitive to task changes. Red for drops greater than 20%, yellow for drops between 10 and 20%, and green for drops below 10%.147",
      "page_start": 166,
      "page_end": 167
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal artificial intelligence is a vibrant multi-disciplinary research field that aims to design computer agents that can perceive, reason, and interact through multiple communicative modalities, including linguistic, acoustic, visual, tactile, sensory, and physiological messages  [46, 375] .\n\nMultimodal AI systems can bring great impact in many scientific areas with practical benefits, such as in supporting human health and well-being  [360, 427, 716] , enabling multimedia content processing  [11, 486, 514] , and enhancing real-world autonomous agents  [63, 93, 334, 523, 546] .\n\nHowever, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of theoretical frameworks and application domains from both historical and recent perspectives, this thesis is designed to advance the theoretical and computational foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions often present in multimodal problems which brings unique challenges to machine learning. The heterogeneity of multimodal data makes learning challenging, for example, language is often seen as symbolic while audio and video are represented as continuous signals. At the same time, these modalities contain overlapping connected information, and interact to give rise to new information relevant for a task. It is crucial to learn these connections and interactions for systems to perform well. Using these principles as a foundation, we propose a taxonomy of six core challenges in multimodal research: representation, alignment, reasoning, generation, transference, and quantification. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches, and enabling us to identify key open problems for future research.\n\nUsing our taxonomy for multimodal machine learning, we highlight two key challenges that are important for progress in multimodal learning: (1) building the foundations of multimodal interactions so we can quantify the interactions present in datasets and model these interactions correctly using machine learning methods, and (2) constructing multimodal models and datasets that enable generalization across a large number of modalities and tasks for real-world societal impact (Figure  1 .1).  The bulk of the thesis studies two core challenges in multimodal learning: (1) building a foundation for multimodal interactions that enables the quantification of multimodal interactions in data and their principled modeling using machine learning methods, and (2) the data requirements and model building blocks enabling generalization of knowledge between modalities, tasks, and their representations.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Foundations Of Multimodal Interactions",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Generalization Across Modalities And Tasks",
      "text": "",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Foundations Of Multimodal Interactions",
      "text": "Multimodal interactions can be categorized into redundancy, uniqueness, and synergy: redundancy quantifies information shared between modalities, such as smiling while telling an overtly humorous joke; uniqueness quantifies the information present in only one, such as each medical sensor designed to provide new information; and synergy quantifies the emergence of new information using both, such as conveying sarcasm through disagreeing verbal and nonverbal cues  [375] . These interactions are the basic principles of how modalities combine to give rise to new information for a task, which is present in all multimodal problems. While there have been intuitive definitions of these multimodal interactions, we still lack a formal foundation and systematic understanding of how to learn these interactions from data. As a result, there remain basic open questions like:\n\nWhat interactions are in my data? What interactions are learned by different models?\n\nWhat models are suitable for my data? To answer these questions, the first part of the thesis presents a theoretical framework formalizing the useful information in each modality and how modalities interact with each other to give rise to new information for a task  [371] . Based on this theoretical framework, we propose two practical estimators to quantify the interactions in high-dimensional datasets, which can also be used more broadly for estimating information-theoretic quantities in real-world distributions. These estimators allow us to understand the information and interactions in multimodal datasets, and design the right models that provably learn the desired interactions in data.\n\nWe further show several broader implications that quantifying multimodal interactions can have on practitioners. Firstly, we operationalize the learning of multimodal interactions through a new approach called Factorized Contrastive Learning to capture both shared and unique information across modalities  [373] . Secondly, a formal definition of multimodal interactions also enables us to analyze through qualitative visualizations whether a trained model has succeeded in learning the desired interactions from data  [374] . Finally, we show how to use this information-theoretic framework to estimate the performance of optimal multimodal models given only unimodal data, which can inform practitioners which modalities to collect, and whether multimodal modeling is worth it for maximum increase in performance  [376] . We release all code for quantifying multimodal interaction (both exact and approximate), and their implications on understanding datasets and models at https://github.com/pliang279/PID, code for Factorized Contrastive Learning at https://github.com/pliang279/FactorCL, and code for visualizing and debugging multimodal models at https://github.com/pliang279/MultiViz, which can help practitioners navigate the multimodal modeling pipeline.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Multisensory Foundation Models",
      "text": "There has been substantial impact of foundation models (e.g., large language models) trained on vast amounts of unlabeled data to obtain general-purpose capabilities over many prediction tasks. The future will lie in multisensory foundation models that are grounded in the world: being able to simultaneously process a large number of modalities beyond language, to vision, audio  [11, 360, 381, 503] , and leveraging advances in sensing technologies such as cellphones  [366] , wearable devices  [218] , autonomous vehicles  [698] , healthcare technologies  [287] , and robots  [53, 304] . The large number of heterogeneous modalities creates challenges in building multisensory foundation models. For example, the healthcare domain typically collects tabular data and high-frequency sensors  [287] , and it remains an open question how to best combine large language models with tabular data and sensors  [547] . In the second part of this thesis, we take steps towards both data and modeling requirements to build the next generation of multisensory foundation models: What data sources do we need to train foundation models over many heterogeneous modalities?\n\nWhat modeling architectures are suitable for scaling to many heterogeneous modalities? To answer the first question, we introduce MULTIBENCH, the largest and most comprehensive multimodal benchmark enabling the training of multisensory foundation models. MULTI-BENCH collects and standardizes 15 realistic datasets across 10 diverse modalities, 20 prediction tasks, and 6 research areas from multimedia, affective computing, robotics, HCI, finance, and healthcare. MULTIBENCH is publicly available at https://github.com/pliang279/ MultiBench, and has been broadly used in the community to train and evaluate multimodal architectures.\n\nOn the modeling side, prior work on multimodal learning has focused on a fixed set of modalities (e.g., image and text), without tackling generalization to many heterogeneous modalities and tasks necessary for truly multisensory models. To tackle the heterogeneity across many different modalities, we treat modalities in their most general form as sequences of elements, and present the cross-modal attention  [101, 359]  and multimodal transformer  [614]  architectures to learn interactions between all sequences of elements. These multimodal transformers are scalable and achieve strong results over a wide range of modalities, and we show their applications to image, text, video, sensors, and medical data. Finally, using MULTIBENCH, we scale multimodal transformers to the high-modality setting, resulting in a single model architecture with the same set of parameters that can function across a large number of modalities partially observed for different tasks  [370]  (e.g., image and text on the internet, video and audio in human communication, video and sensors in robotics, and so on). This represents the most realistic setting of how humans process the multisensory world, and we believe that general-purpose AI systems will also need to be trained in the high-modality setting. Our collection of high-modality models, available at https://github.com/pliang279/HighMMT, has already been extended for learning over many modalities in the medical, internet-of-things, and affective computing domains.\n\nWe end the thesis by discussing our collaborative efforts in applying these multisensory models for real-world impact on affective computing, mental health, and cancer prognosis.",
      "page_start": 26,
      "page_end": 173
    },
    {
      "section_name": "Summary Of Contributions",
      "text": "In this section, we provide a highlight of our main thesis contributions.\n\n1. Literature survey and taxonomy of multimodal challenges (Chapter 2) (a) Three key principles: We begin by defining three key principles that have driven technical challenges and innovations: (1) modalities are heterogeneous because the information present often shows diverse qualities, structures, and representations,  (2)  modalities are connected since they are often related and share commonalities, and (3) modalities interact to give rise to new information when used for task inference. (b) Six technical challenges: Building upon these principles, we propose a new taxonomy of six core challenges in multimodal learning: (1) Representation studies how to summarize multimodal data to reflect the heterogeneity and interconnections between individual modality elements, before (2) alignment captures the connections and interactions between multiple local elements according to their structure. After representation and alignment comes (3) reasoning, which aims to combine the information from multimodal evidence in a principled way that respects the structure of the problem to give more robust and interpretable predictions. While most systems aim to predict the label y, there are also cases where the goal is (4) generation, to learn a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence, or (  5 ) transference, to transfer information from high-resource modalities to low-resource ones and their representations. Finally,  (6)  quantification revisits the previous challenges to give deeper empirical and theoretical understanding of modality heterogeneity, interconnections, and the learning process. (c) Current work and open directions: For each challenge, we create a taxonomy of subchallenges and categorize recent advances in the field. This new taxonomy will enable researchers to better understand the state of research, and we identify several key directions for future work.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Foundations Of Multimodal Interactions (Chapter 3)",
      "text": "(a) Multimodal interactions can be categorized into redundancy, uniqueness, and synergy: redundancy quantifies information shared between modalities, such as smiling while telling an overtly humorous joke; uniqueness quantifies the information present in only one, such as each medical sensor designed to provide new information; and synergy quantifies the emergence of new information using both, such as conveying sarcasm through disagreeing verbal and nonverbal cues  [375] . (b) Formal framework and estimation: By introducing a new connection between information theory and multimodal interactions  [371] , I designed scalable estimators to quantify the interactions in large-scale multimodal datasets and those learned by multimodal models. These estimators are based on max-entropy convex optimization and a scalable end-to-end estimator suitable for high-dimensional continuous data. (c) Model selection: We show that quantifying the interactions enables practitioners to analyze their datasets and select the most appropriate model that captures the right interactions in the data. We implemented these methods in two real-world case studies in mental health assessment  [366]  and cancer prognosis  [371]  from multimodal data. Domain experts appreciated the transparency that these methods convey as opposed to black-box neural networks, resulting in trust and adoption in real-world practice. 3. Learning multimodal interactions using self-supervised learning (Chapter 4) (a) From estimation to learning: Naturally, a formal definition of multimodal interactions also translates to new training objectives to learn these interactions using neural networks. We show how to better learn task-relevant unique information  [373, 615]  using self-supervised learning, going beyond shared information between modalities. (b) Factorized learning of each interaction: FACTORCL is built from three new contribu-tions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. (c) Real-world settings with unique information: On large-scale real-world datasets, FACTORCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks, including tasks involving medical sensors or robotics with force sensors that provide unique information, or cartoon images and figurative captions (i.e., not literal but metaphoric or idiomatic descriptions of the images). (a) Modality selection: We extended our analysis to quantify interactions in a semisupervised setting with only labeled unimodal data (x 1 , y), (x 2 , y) and naturally co-occurring multimodal data (x 1 , x 2 ) (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming  [376] . We show how to approximately estimate the multimodal interactions in the unseen full distribution (x 1 , x 2 , y), which enables practitioners to prioritize collecting data for modalities that has the most synergy with existing ones. (b) Estimating performance: Our approximation is based on lower and upper bounds for synergy: a lower bound based on the disagreement between modality predictors, and an upper bound based on a connection to min-entropy couplings. Lower and upper bounds on synergistic information translate to bounds on multimodal performance. (c) On disagreement: Finally, we show that disagreement is a critical quality that can result in synergy between modalities, and propose a learning algorithm that captures disagreement between modalities beyond agreement that is typically done. 6. MULTIBENCH: A benchmark for real-world generalization (Chapter 7) (a) Real-world benchmarks: We describe MULTIBENCH, the largest unified benchmark for multimodal representation learning  [367] . MULTIBENCH provides an end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation, while ensuring reproducibility and ease of use. (b) Standardized building blocks: To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning spanning innovations in fusion paradigms, optimization objectives, and training approaches. (c) Benefits of standardization: We find that standardizing and sharing methods pro-posed in different research areas can improve performance on several datasets. MULTI-BENCH also provides a better understanding of the capabilities and limitations of multimodal models. 7. Learning multimodal interactions across time (Chapter 8) (a) Temporal interactions: To tackle heterogeneity across many different modalities, we treat modalities in their most general form as a sequence of elements, such as words in a sentence, patches in an image, frames in a video, and time steps in time-series data. This introduces a critical challenge of learning multimodal interactions across sequences, such as relating a word with a facial expression within a long video. (b) Recurrent cross-modal attention: While prior work summarized temporal modalities into a single static feature before fusion, I developed a new method for fine-grained temporal fusion to learn interactions between all elements across the sequence, such as between individual words, gestures, and vocal expressions  [101, 359] . We call this module recurrent cross-modal attention, by using attention weights to recursively learn interactions based on the current input and previous signals. (c) Multimodal transformers: We extended recurrent attention into multimodal transformers that learn all interactions across sequences in parallel  [614] . The multimodal transformer learns a cross-modal attention matrix to highlight related signals across time (e.g., rolling eyes and sighing). This matrix is used to learn a new representation for each modality fused with other modalities in parallel over the entire sequence, which provides huge efficiency gains when trained on modern GPUs. 8. Multimodal and multitask foundation models (Chapter 9) (a) High-modality learning: Chapter 9 builds upon the diverse modalities and tasks provided by MULTIBENCH by designing methods for high-modality learning: where there are a large number of modalities partially observed for different tasks  [370] . This represents the most realistic setting of how humans process the multisensory world, and we believe that general-purpose AI systems will also need to be multisensory. (b) A single model for many modalities and tasks: We propose HIGHMMT, a single shared high-modality model that achieves generalization over more than 10 modalities and 15 tasks, and transfers to new modalities and tasks. (c) Tackling extreme heterogeneity: We've seen two extremes -full parameter sharing across everything, and no sharing at all across modality and task-specific models. A key idea in HIGHMMT is to find the optimal amount of parameter sharing balancing performance and efficiency. We do this by defining a new measure of which modalities are similar, and which modality pairs interact similarly, to inform parameter sharing.",
      "page_start": 28,
      "page_end": 30
    },
    {
      "section_name": "Other Contributions",
      "text": "I have also pursued the following selected research directions during my Ph.D. studies, which are excluded from this thesis. The first major direction lies in datasets and methods for learning representations from a fixed set of input modalities (i.e., without modeling generalization). To that end, I have contributed core resources and models for multimodal representation learning, especially in the application domain of modeling human communication. I have also engaged in collaborations with real-world stakeholders particularly in the healthcare and affective computing space where multimodal learning paradigms offer opportunities to learn from high-dimensional multimodal data. Finally, I have also worked on addressing the real-world societal concerns these models, such as improving their robustness, fairness, and privacy.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Multimodal Representation Learning",
      "text": "Computational modeling of human multimodal language: From a computational perspective, the modeling of human communication across both verbal and nonverbal behaviors enables real-world tasks such as multimodal sentiment analysis  [427] , emotion recognition  [74] , and personality traits recognition  [468] . To comprehend human communication, there is a need for 1) large multimodal resources with diversity in training samples, topics, speakers, and annotations, as well as 2) powerful models for multimodal communication.\n\nAs a first step, we have worked towards addressing the lack of multimodal resources by collecting and releasing the largest dataset of multimodal sentiment and emotion recognition enabling generalizable studies of human communication. CMU-MOSEI contains 23, 500 annotated video segments from 1, 000 distinct speakers and 250 topics. The diversity in topics, speakers, annotations, and modalities allows for generalizable studies of speaker and topic-independent features. The multimodal dataset and a general multimodal data loading framework are provided to the scientific community to encourage valuable research in human communication analysis  [360, 718] . Since then, the dataset has also been the subject of two workshop challenges in modeling human multimodal language at ACL 2018 and ACL 2020, and has been a standard benchmark dataset for the multimodal machine learning community.\n\nMultimodal gated fusion: With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community  [427, 478, 664] . We develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level  [101] . We proposed the GME-LSTM model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding layer in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.\n\nFactorized multimodal representations: Using MULTIBENCH and other related multimodal benchmarks enables us a deeper study of the desiderata for multimodal representations beyond discriminative performance  [615] . While the two main pillars of research in multimodal representation learning have considered discriminative  [88, 101, 181, 554, 713]  and generative  [442, 483, 556, 566, 586]  objectives individually, we demonstrate that factorizing multimodal representations into multimodal discriminative and modality-specific generative factors marries the strengths of discriminative learning of joint features across modalities that achieves state-ofthe-art performance for affect analysis with controllable generation of human language based on individual factors, robustness to partially missing modalities, and interpretable local contributions from each modality during prediction. Our resulting Multimodal Factorization Model (MFM) defines a flexible latent variable framework balancing prediction with robustness and understandability for real-world human multimodal language.\n\nEfficient statistical baselines: The constraints of real-world edge devices have created a demand for data and compute-efficient multimodal learning via simple yet strong models  [570] . We proposed an approach based on stronger statistical baselines rather than black-box neural networks. By assuming a fully-factorized probabilistic generative model of multimodal data from a latent representation, careful model design allows us to capture expressive unimodal, bimodal, and trimodal interactions while at the same time retaining simplicity and efficiency during learning and inference  [362] . These models show strong performance on both supervised and semi-supervised multimodal prediction, as well as significant (10 times) speedups over neural models during inference.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Applications In Affective Computing, Social Intelligence, And Healthcare",
      "text": "Improving the generalization and quantification of multimodal models enables a step towards real-world models capturing the benefits of multimodal data while mitigating its risks. However, tangible real-world impact requires direct collaboration with real-world stakeholders to determine their precise computational needs. During my PhD, I have had the pleasure of collaborating on the following real-world applications:\n\nMultimodal affective computing: As an application-specific instantiation of multimodal learning, we studied the problem of continuous-time human affect analysis and proposed a new perspective by modeling both person-independent and person-dependent signals through insights from human psychology  [361] . Some emotional expressions are almost universal personindependent behaviors and can be recognized directly from a video  [145, 305] . For example, an open mouth with raised eyebrows and a loud voice is likely to be associated with surprise. However, emotions are also expressed in a person-dependent fashion with idiosyncratic behaviors where it may not be possible to directly estimate absolute emotion intensities. Instead, it would be easier to compare two video segments of the same person and judge whether there was a relative change in emotion intensities  [163, 419, 568] . For example, a person could have naturally furrowed eyebrows and we should not always interpret this as a display of anger, but rather compare two video segments to determine relative changes in anger. By designing a model combining both signals, we are able to achieve state-of-the-art audio-visual emotion recognition performance and allow for fine-grained investigation of person-independent and person-dependent behaviors.\n\nSocial intelligence question-answering: As intelligent systems increasingly blend into our everyday life, artificial social intelligence becomes a prominent area of research. Intelligent systems must be socially intelligent in order to comprehend human intents and maintain a rich level of interaction with humans  [268, 302, 601, 637] . Human language offers a unique unconstrained approach to probe through questions and reason through answers about social situations  [11, 343] . This unconstrained approach extends previous attempts to model social intelligence through numeric supervision (e.g. sentiment and emotions labels). We introduced the Social-IQ dataset  [716] , an unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies. By providing a rich source of open-ended questions and answers, Social-IQ opens the door to explainable social intelligence. The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ contains 1, 250 natural in-the-wild social situations, 7, 500 questions and 52, 500 correct and incorrect answers. Although humans can reason about social situations with very high accuracy (95.08%), existing state-of-the-art computational models struggle on this task. As a result, Social-IQ brings novel challenges that will spark future research in social intelligence modeling, visual reasoning, and multimodal question answering (QA).\n\nPrivacy-preserving mood prediction from mobile data: Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care  [178, 328] . The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders  [200, 433] . One promising data source to help monitor human behavior is daily smartphone usage  [493] . However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes  [313, 365, 540, 581] . Through data collected via a collaboration with psychiatrists and psychologists at the University of Oregon, Columbia University, and the University of Pittsburgh, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors  [366] . Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood. However, we find that models trained to predict mood often also capture private user identities in their intermediate representations. To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive. By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.",
      "page_start": 32,
      "page_end": 34
    },
    {
      "section_name": "Real-World Robustness, Fairness, And Privacy",
      "text": "Finally, the third major direction studies the real-world concerns of deploying multimodal models in the face of real-world noise topologies, dataset biases, and privacy concerns.\n\nRobustness to noisy modalities: Different modalities often display different noise topologies, and real-world multimodal signals possibly suffer from missing or noisy data in at least one of the modalities  [46, 150, 336] . Human-centric data is also often imperfect due to personal idiosyncrasies which affect the contribution of certain modalities during social interactions  [204, 525] . For example, multimodal dialogue systems trained on acted TV shows are susceptible to poor performance when deployed in the real world where users might be less expressive in using facial gestures. This calls for robust models that can still make accurate predictions despite only having access to a (possibly noisy) subset of signals.\n\nAs a step towards robustness, we propose a tensor representation learning method to deal with noisy modalities in time-series data (e.g., text, videos, audio)  [364] . This method is based on the observation that multimodal time series data often exhibits correlations across time and modalities which lead to low-rank multimodal representations  [237, 327, 691] . However, the presence of noise or incomplete values breaks these correlations and results in tensor representations of higher rank. Regularizing the rank of tensor representations therefore provides a denoising effect which achieves strong results across various levels of imperfection. We show how to integrate an upperbound of tensor rank minimization as a simple regularizer for training in the presence of imperfect data, thereby combining the strength of temporal non-linear transformations of multimodal data with principled regularization on tensor structures. Through experiments on multimodal video data, our results back up our intuitions that imperfect data increases tensor rank and demonstrates strong results across various levels of imperfection.\n\nLearning fair sentence representations: To safely deploy human-centric multimodal models in real-world scenarios such as healthcare, legal systems, and social science, it is also necessary to recognize the role they play in shaping social biases and stereotypes. Previous work has revealed the presence of representational biases in widely used word embeddings -harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs  [64, 193, 233, 432, 519, 543] . While some methods were proposed to debias these word-level embeddings  [67, 405] , there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo  [480]  and BERT  [144]  which have become core components in both real-world language  [19, 257, 657]  and multimodal prediction systems  [348, 390] . We investigated the presence of social biases in sentence-level representations and proposed a new method, SENT-DEBIAS, to reduce these biases  [365] . We show that SENT-DEBIAS is effective in reducing biases from the geometry of contextual representation spaces, and at the same time, preserves performance on sentence-level downstream NLP tasks such as sentiment analysis, linguistic acceptability, and natural language understanding.\n\nMitigating social biases in language models: In addition to sentence representations deployed primarily for discriminative tasks, large-scale pretrained language models (LMs) have also become widely-deployed for generative applications such as text generation  [497] , dialog systems  [731] , recommendation systems  [534] , and search engines  [43, 455] . Recent work has found that these language models can potentially generate text propagating negative generalizations about particular social groups  [432] , language that is denigrating to particular social groups  [543] , and toxic speech  [193] , while at the same time also being unable to reason about human-aligned values such as ethics  [233] , social bias implications  [519] , and allocational harms across social groups  [386] . As a step towards improving the fairness of LMs, we carefully defined several sources of representational biases before proposing new benchmarks and metrics to measure them  [368] . With these tools, we propose A-INLP, an approach towards post-hoc debiasing of large pretrained LMs. The key to our approach lies in dynamically finding bias-sensitive tokens rather than relying on a predefined set of bias-sensitive words that are common in existing literature  [67] . Our empirical results and human evaluation on large language models such as GPT-2 demonstrate effectiveness in mitigating bias while retaining crucial context information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier. These steps are critical towards improving the safety of language and multimodal models.\n\nPrivacy-preserving federated learning: More broadly, federated learning is a method of training models on private data distributed over multiple devices  [68, 356, 413, 553] . To keep device data private, a single global model is trained by only communicating parameters and updates which poses scalability challenges for large models  [446] . Furthermore, current approaches use the same model architecture across all local models and the global aggregated model, which causes federated learning to struggle with data heterogeneity across devices  [248, 356, 732] . This is made worse when each device contains multimodal data sources that are used unequally across users  [426] . To this end, we propose a new federated learning algorithm, Local Global Federated Averaging (LG-FEDAVG), that jointly learns compact local representations on each device and a global model across all devices  [363] . As a result, the global model can be smaller since it only operates on local representations, reducing the number of communicated parameters. Furthermore, well-designed local models enable learning of personalized representations for user-specific behavior modeling while enjoying the benefit of global model learning across many users' data. Theoretically, we provide a generalization analysis which shows that a combination of local and global models reduces both variance in the data as well as variance across device distributions. Empirically, we demonstrate that local models enable communication-efficient training while retaining performance. We also evaluate on the task of personalized mood prediction from realworld mobile data where privacy is key. Finally, we show that local models handle heterogeneous data from new devices, and learn fair representations that obfuscate protected attributes such as race, age, and gender  [67] .",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Chapter 2",
      "text": "Literature Survey and Taxonomy of Multimodal Challenges",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Introduction",
      "text": "It has always been a grand goal of artificial intelligence to develop computer agents with intelligent capabilities such as understanding, reasoning, and learning through multimodal experiences and data, similar to how humans perceive and interact with our world using multiple sensory modalities. With recent advances in embodied autonomous agents  [69, 523] , self-driving cars  [675] , image and video understanding  [18, 577] , image and video generation  [503, 551] , and multisensor fusion in application domain such as robotics  [335, 408]  and healthcare  [287, 367] , we are now closer than ever to intelligent agents that can integrate and learn from many sensory modalities. This vibrant multi-disciplinary research field of multimodal machine learning brings unique challenges given the heterogeneity of the data and the interconnections often found between modalities, and has widespread applications in multimedia  [436] , affective computing  [490] , robotics  [308, 335] , human-computer interaction  [450, 539] , and healthcare  [76, 429] .\n\nHowever, the rate of progress in multimodal research has made it difficult to identify the common themes underlying historical and recent work, as well as the key open questions in the field. By synthesizing a broad range of research, this paper is designed to provide an overview of the methodological, computational, and theoretical foundations of multimodal machine learning. We begin by defining (in §2.2) three key principles that have driven technical challenges and innovations: (1) modalities are heterogeneous because the information present often shows diverse qualities, structures, and representations, (2) modalities are connected since they are often related and share commonalities, and (3) modalities interact to give rise to new information when used for task inference. Building upon these definitions, we propose a new taxonomy of six core challenges in multimodal learning: representation, alignment, reasoning, generation, transference, and quantification (see Figure  2 .1). These core multimodal challenges are understudied in conventional unimodal machine learning and need to be tackled in order to progress the field forward: 1. Representation ( §2.3): Can we learn representations that reflect heterogeneity and interconnections between modality elements? We will cover approaches for (1) representation fusion: integrating information from two or more modalities to capture cross-modal interactions, (2) (1) Representation studies how to summarize multimodal data to reflect the heterogeneity and interconnections between individual modality elements, before (2) alignment captures the connections and interactions between multiple local elements according to their structure. After representation and alignment comes (3) reasoning, which aims to combine the information from multimodal evidence in a principled way that respects the structure of the problem to give more robust and interpretable predictions. While most systems aim to predict the label y, there are also cases where the goal is (4) generation, to learn a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence, or (5) transference, to transfer information from high-resource modalities to low-resource ones and their representations. Finally, (6) quantification revisits the previous challenges to give deeper empirical and theoretical understanding of modality heterogeneity, interconnections, and the learning process.\n\nrepresentation coordination: interchanging cross-modal information to keep the same number of representations but improve multimodal contextualization, and (3) representation fission: creating a larger set of disjoint representations that reflects knowledge about internal structure such as data clustering or factorization. 2. Alignment ( §2.4): How can we identify the connections and interactions between modality elements? Alignment is challenging since it may depend on long-range dependencies, involves ambiguous segmentation (e.g., words or utterances), and could be either one-to-one, many-to-many, or not exist at all. We cover (1) discrete alignment: identifying connections between discrete elements across modalities, (2) continuous alignment: modeling alignment between continuous modality signals with ambiguous segmentation, and (3) contextualized representations: learning better representations by capturing cross-modal interactions between elements. 3. Reasoning ( §2.5) is defined as composing knowledge, usually through multiple inferential steps, that exploits the problem structure for a specific task. Reasoning involves (1) modeling the structure over which composition occurs, (2) the intermediate concepts in the composition process, (3) understanding the inference paradigm of more abstract concepts, and (4) leveraging large-scale external knowledge in the study of structure, concepts, and inference. 4. Generation ( §2.6) involves learning a generative process to produce raw modalities. We categorize its subchallenges into (1) summarization: summarizing multimodal data to reduce information content while highlighting the most salient parts of the input, (2) translation: translating from one modality to another and keeping information content while being consistent with cross-modal connections, and (3) creation: simultaneously generating multiple modalities to increase information content while maintaining coherence within and across modalities. 5. Transference ( §2.7) aims to transfer knowledge between modalities, usually to help the target modality, which may be noisy or with limited resources. Transference is exemplified by (1) cross-modal transfer: adapting models to tasks involving the primary modality, (2) co-learning: transferring information from secondary to primary modalities by sharing representation spaces between both modalities, and (3) model induction: keeping individual unimodal models separate but transferring information across these models. 6. Quantification ( §2.8): The sixth and final challenge involves empirical and theoretical studies to better understand (1) the dimensions of heterogeneity in multimodal datasets and how they subsequently influence modeling and learning, (2) the presence and type of modality connections and interactions in multimodal datasets and captured by trained models, and (3) the learning and optimization challenges involved with heterogeneous data. Finally, we conclude this paper with a long-term perspective on multimodal learning by motivating open research questions identified by this taxonomy. This survey was also presented by the authors in a visual medium through tutorials at CVPR 2022 and NAACL 2022, as well as courses 11-777 Multimodal Machine Learning and 11-877 Advanced Topics in Multimodal Machine Learning at CMU. The reader is encouraged to refer to these public video recordings, additional readings, and discussion probes for more mathematical depth on certain topics, visual intuitions and explanations, and more open research questions in multimodal learning.\n\nThis paper is designed to complement other surveys that belong broadly to the study of multiple modalities or views: multi-view learning  [443, 578, 686]  is concerned with settings where different views (e.g., camera views) typically provide overlapping (redundant) information but not the other core challenges we cover, surveys on multimodal foundation models  [157, 185]  go into detail on tackling representation, fusion, and alignment using large-scale pretraining but do not cover other core challenges, and several application-oriented surveys in vision-language models  [628] , language and reinforcement learning  [394] , multimedia analysis  [34] , and multimodal humancomputer interaction  [277]  discuss specific multimodal challenges faced in these applications. This survey presents a telescoping overview suitable as a starting point for researchers who can then diver deeper into methodology or application-specific research areas.",
      "page_start": 36,
      "page_end": 38
    },
    {
      "section_name": "Key Modalities And Application Domains",
      "text": "In this subsection, we first contextualize our subsequent discussion of multimodal machine learning by listing some key modalities of interest, standard multimodal datasets and toolkits, and major applications of multimodal learning in the real world.\n\nAffective computing studies the perception of human affective states such as emotions, sentiment, and personalities from multimodal human communication: spoken language, facial expressions and gestures, body language, vocal expressions, and prosody  [484] . Some commonly studied tasks involve predicting sentiment  [557, 711] , emotions  [718] , humor  [225] , and sarcasm  [83]  from multimodal videos of social interactions.\n\nHealthcare: Machine learning can help integrate complementary medical signals from lab tests, imaging reports, patient-doctor conversations, and multi-omics data to assist doctors in the clinical process  [5, 21, 383] . Multimodal physiological signals recorded regularly from smartphones and wearable devices can also provide non-invasive health monitoring  [134, 189, 366] . Public datasets include MIMIC  [287]  with patient tabular data, medical reports, and medical sensor readings, question answering on pathology  [230]  and radiology  [329]  images, and multi-omics data integration  [611] .\n\nRobotics systems are often equipped with multiple sensors to aid in robust decision-making for real-world physical tasks such as grasping, cleaning, and delivery. These sensors can include vision (RGB and depth), force, and proprioception  [335] . These multi-sensor robots have been successfully applied in haptic  [459, 531]  and surgical robots  [4, 60] . More generally, language  [394]  and audio  [135]  have also emerged as useful signals for robot learning.\n\nInteractive agents in the virtual world can assist humans in multimedia web tasks and computer tasks  [183]  as well as in the social world through virtual agents  [472, 473] . These agents need to understand human commands and behaviors, process various forms of visual, tabular, and multimedia content, use external web tools and APIs, and interact in multi-step decision-making tasks. Webshop  [696]  and WebAreana  [736]  are recent environments testing the capabilities of AI agents in navigating image and text content to solve web tasks.\n\nMultimedia data spanning text, images, videos, audio, and music is abundant on the internet and has fueled a significant body of multimodal research  [34] , such as classification  [729] , retrieval  [505] , and recommendation  [423, 514, 733]  of multimedia content, image and video question answering  [11, 317, 339]  and captioning  [156, 641] ), multimedia and entertainment content description  [533]  (including movies  [32] , memes  [301, 538] , and cartoons  [236] ), and more recently in automatic creation of text  [727] , images  [512] , videos  [668] , music  [9] , and more.\n\nHuman-computer interaction has sought to endow computers with multimodal capabilities to provide more natural, powerful, and compelling interactive user experiences  [625] . These systems have leveraged speech, touch, vision, gestures, affective states  [463]  and affordable wearable and mobile sensors  [277, 457, 625] . Public datasets have enabled the study of multimodal user interfaces  [340, 645] , speech and gesture interactions  [166] , and human sensing  [89, 139, 527] .\n\nScience and environment: Deepening our knowledge of the natural sciences and physical environments can bring about impactful changes in scientific discovery, sustainability, and conservation. This requires processing modalities such as chemical molecules  [571] , protein structures  [726] , satellite images  [109, 694] , remote sensing  [243, 346] , wildlife movement  [389] , scientific diagrams and texts  [392] , and various physical sensors  [424] .\n\nEducation: AI can broaden access to educational content by digitizing lecture slides and videos, creating personalized tutors, and designing interactive learning curricula. It introduces challenges in processing recorded lecture slides and videos  [333] , and modeling student learning via asked questions, spoken feedback and non-verbal gestures  [87, 575, 680] .",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Foundational Principles In Multimodal Research",
      "text": "A modality refers to a way in which a natural phenomenon is perceived or expressed. For example, modalities include speech and audio recorded through microphones, images and videos captured The information present in different modalities will often show diverse qualities, structures, and representations. Dimensions of heterogeneity can be measured via differences in individual elements and their distribution, the structure of elements, as well as modality information, noise, and task relevance.\n\nvia cameras, and force and vibrations captured via haptic sensors. Modalities can be placed along a spectrum from raw to abstract: raw modalities are those more closely detected from a sensor, such as speech recordings from a microphone or images captured by a camera. Abstract modalities are those farther away from sensors, such as language extracted from speech recordings, objects detected from images, or even abstract concepts like sentiment intensity and object categories.\n\nMultimodal refers to situations where multiple modalities are involved. From a research perspective, multimodal entails the computational study of heterogeneous and interconnected modalities. Firstly, modalities are heterogeneous because the information present in different modalities will often show diverse qualities, structures, and representations. Secondly, these modalities are not independent entities but rather share connections due to complementary information. Thirdly, modalities interact in different ways when they are integrated for a task. We expand on these three foundational principles of multimodal research in the following subsections.",
      "page_start": 39,
      "page_end": 40
    },
    {
      "section_name": "Principle 1: Modalities Are Heterogeneous",
      "text": "The principle of heterogeneity reflects the observation that the information present in different modalities will often show diverse qualities, structures, and representations. Heterogeneity should be seen as a spectrum: two images from the same camera that capture the same view modulo camera wear and tear are closer to homogeneous, two different languages that capture the same meaning but from different language families are slightly heterogeneous, language and vision are even more heterogeneous, and so on. In this section, we present a non-exhaustive list of dimensions of heterogeneity (see Figure  2 .2 for an illustration). These dimensions are complementary and may overlap; each multimodal problem likely involves heterogeneity in multiple dimensions. 1. Element representation: Each modality is typically comprised of a set of elements -the most basic unit of data which cannot (or rather, the user chooses to not) be broken down into further units  [49, 358] . For example, typed text is recorded via a set of characters, videos are recorded via a set of frames, and graphs are recorded via a set of nodes and edges. What are the basic elements present in each modality, and how can we represent them? Formally, this dimension measures heterogeneity in the sample space or representation space of modality elements. 2. Distribution refers to the frequency and likelihood of modality elements. Elements typically follow a unique distribution, with words in a linguistic corpus following Zipf's Law  [744]  as an example. Distribution heterogeneity refers to the differences in frequencies and likelihoods of elements, such as different frequencies in recorded signals and the density of elements. 3. Structure: Natural data exhibits structure in the way individual elements are composed to form entire modalities  [71] . For example, images exhibit spatial structure across objects, language is hierarchically composed of words, and signals exhibit temporal structure across time. Structure heterogeneity refers to differences in this underlying structure. 4. Information measures the total information content present in each modality. Subsequently, information heterogeneity measures the differences in information content across modalities, which could be formally measured by information theoretic metrics  [536] . 5. Noise: Noise can be introduced at several levels across naturally occurring data and also during the data recording process. Natural data noise includes occlusions, imperfections in human-generated data (e.g., imperfect keyboard typing or unclear speech), or data ambiguity due to sensor failures  [367] . Noise heterogeneity measures differences in noise distributions across modalities, as well as differences in signal-to-noise ratio. 6. Relevance: Finally, each modality shows different relevance toward specific tasks and contexts -certain modalities may be more useful for certain tasks than others  [192] . Task relevance describes how modalities can be used for inference, while context relevance describes how modalities are contextualized with other modalities. It is useful to take these dimensions of heterogeneity into account when studying both unimodal and multimodal data. In the unimodal case, specialized encoders are typically designed to capture these unique characteristics in each modality  [71] . In the multimodal case, modeling heterogeneity is useful when learning representations and capturing alignment  [719] , and is a key subchallenge in quantifying multimodal models  [370] .",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Principle 2: Modalities Are Connected",
      "text": "Although modalities are heterogeneous, they are often connected due to shared complementary information. The presence of shared information is often in contrast to unique information that exists solely in a single modality  [663] . Modality connections describe the extent and dimensions to which information can be shared across modalities. When reasoning about the connections in multimodal data, it is helpful to think about both bottom-up (statistical) and top-down (semantic) approaches (see Figure  2 .3). From a statistical data-driven perspective, connections are identified from distributional patterns in multimodal data, while semantic approaches define connections based on our domain knowledge about how modalities share and contain unique information. 1. Statistical association exists when the values of one variable relate to the values of another.\n\nFor example, two elements may co-occur with each other, resulting in a higher frequency of both occurring at the same time. Statistically, this could lead to correlation -the degree to which elements are linearly related, or other non-linear associations. From a data-driven perspective, discovering which elements are associated with each other is important for modeling the joint distributions across modalities during multimodal representation and alignment  [606] . 2. Statistical dependence goes deeper than association and requires an understanding of the exact type of statistical dependency between two elements. For example, is there a causal dependency from one element to another, or an underlying confounder causing both elements to be present at the same time? Other forms of dependencies could be spatial or temporal: one element occurring above the other, or after the other. Typically, while statistical association can be estimated purely from data, understanding the nature of statistical dependence requires some knowledge of the elements and their underlying relationships  [445, 626] . 3. Semantic correspondence can be seen as the problem of ascertaining which elements in one modality share the same semantic meaning as elements in another modality  [456] . Identifying correspondences is fundamental in many problems related to language grounding  [86] , translation and retrieval  [486] , and cross-modal alignment  [589] . 4. Semantic relations: Finally, semantic relations generalize semantic correspondences: instead of modality elements sharing the same exact meaning, semantic relations include an attribute describing the exact nature of the relationship between two modality elements, such as semantic, logical, causal, or functional relations. Identifying these semantically related connections is important for higher-order reasoning  [49, 410] .",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Principle 3: Modalities Interact",
      "text": "Modality interactions study how modality elements interact to give rise to new information when integrated together for task inference. We note an important difference between modality connections and interactions: connections exist within multimodal data itself, whereas interactions only arise when modalities are integrated and processed together to bring a new response. In Figure  2 .4, we provide a high-level illustration of some dimensions of interactions that can exist. 1. Interaction information investigates the type of connected information that is involved in an interaction. When an interaction involves shared information common to both modalities, the interaction is redundant, while a non-redundant interaction is one that does not solely rely on shared information, and instead relies on different ratios of shared, unique, or possibly even synergistic information  [371, 663] . 2. Interaction mechanics are the functional operators involved when integrating modality elements for task inference. For example, interactions can be expressed as statistically additive, non-additive, and non-linear forms  [283] , as well as from a semantic perspective where two elements interact through a logical, causal, or temporal operation  [627] . 3. Interaction response studies how the inferred response changes in the presence of multiple modalities. For example, through sub-dividing redundant interactions, we can say that two modalities create an equivalence response if the multimodal response is the same as responses from either modality, or enhancement if the multimodal response displays higher confidence. On the other hand, non-redundant interactions such as modulation or emergence happen when there exist different multimodal versus unimodal responses  [469] .",
      "page_start": 42,
      "page_end": 43
    },
    {
      "section_name": "Core Technical Challenges",
      "text": "Building on these three core principles and our detailed review of recent work, we propose a new taxonomy to characterize the core technical challenges in multimodal research: representation, alignment, reasoning, generation, transference, and quantification. In Table  2 .1 we summarize our full taxonomy of these six core challenges, their subchallenges, categories of corresponding approaches, and recent examples in each category. In the following sections, we describe our new taxonomy in detail and also revisit the principles of heterogeneity, connections, and interactions to see how they pose research questions and inspire research in each of these six challenges.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Challenge 1: Representation",
      "text": "The first fundamental challenge is to learn representations that reflect cross-modal interactions between individual elements across different modalities. This challenge can be seen as learning a 'local' representation between elements, or a representation using holistic features. This section covers (1) representation fusion: integrating information from 2 or more modalities, effectively reducing the number of separate representations, (2) representation coordination: interchanging cross-modal information by keeping the same number of representations but improving multimodal contextualization, and (3) representation fission: creating a new decoupled set of representations, usually larger number than the input set, that reflects knowledge about internal structure such as data clustering or factorization (Figure  2 .5).",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "Subchallenge 1A: Representation Fusion",
      "text": "Representation fusion aims to learn a joint representation that models cross-modal interactions between individual elements of different modalities, effectively reducing the number of separate representations. We categorize these approaches into fusion with abstract modalities and fusion with raw modalities (Figure  2 .6). In fusion with abstract modalities, suitable unimodal encoders are first applied to capture a holistic representation of each element (or modality entirely), after which several building blocks for representation fusion are used to learn a joint representation. As a result, fusion happens at the abstract representation level. On the other hand, fusion with raw modalities entails representation fusion at very early stages with minimal preprocessing, perhaps even involving raw modalities themselves. Fusion with abstract modalities: We begin our treatment of representation fusion of abstract representations with additive and multiplicative interactions. These operators can be seen as differentiable building blocks combining information from two streams of data that can be flexibly inserted into almost any unimodal machine learning pipeline. Given unimodal data or features x 1 and x 2 , additive fusion can be seen as learning a new joint representation z mm = w 0 +w 1 x 1 +w 2 x 2 +ϵ, where w 1 and w 2 are the weights learned for additive fusion of x 1 and x 2 , w 0 the bias term, and ϵ the error term. If the joint representation z mm is directly taken as a prediction ŷ, then additive fusion resembles late or ensemble fusion ŷ = f 1 (x 1 ) + f 2 (x 2 ) with unimodal predictors f 1 and f 2  [180] . Otherwise, the additive representation z mm can also undergo subsequent unimodal or multimodal processing  [46] . Multiplicative interactions extend additive interactions to include a cross term w 3 (x 1 × x 2 ). These models have been used extensively in statistics, where it can be interpreted as a moderation effect of x 1 affecting the linear relationship between x 2 and y  [48] . Overall, purely additive interactions z mm = w 0 + w 1 x 1 + w 2 x 2 can be seen as a firstorder polynomial between input modalities x 1 and x 2 , combining additive and multiplicative z mm = w 0 + w 1 x 1 + w 2 x 2 + w 3 (x 1 × x 2 ) captures a second-order polynomial.\n\nTo further go beyond first and second-order interactions, tensors are specifically designed to explicitly capture higher-order interactions across modalities  [713] . Given unimodal data x 1 , x 2 ,",
      "page_start": 43,
      "page_end": 44
    },
    {
      "section_name": "Fusion Coordination Fission",
      "text": "# modalities > # representations # modalities = # representations # modalities < # representations",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Sub-Challenges:",
      "text": "Definition: Learning representations that reflect cross-modal interactions between individual elements, across different modalities tensors are defined as z mm = x 1 ⊗ x 2 where ⊗ denotes an outer product  [55, 182] . Tensor products of higher order represent polynomial interactions of higher order between elements  [245] . However, computing tensor products is expensive since their dimension scales exponentially with the number of modalities, so several efficient approximations based on low-rank decomposition have been proposed  [245, 388] . Finally, Multiplicative Interactions (MI) generalize additive and multiplicative operators to include learnable parameters that capture second-order interactions  [283] .\n\nIn its most general form, MI defines a bilinear product z mm = x 1 Wx 2 + x ⊺ 1 U + Vx 2 + b where W, U, Z, and b are trainable parameters.\n\nMultimodal gated units/attention units learn representations that dynamically change for every input  [88, 652] . Its general form can be written as z mm = x 1 ⊙ h(x 2 ), where h represents a function with sigmoid activation and ⊙ denotes element-wise product. h(x 2 ) is commonly referred to as 'attention weights' learned from x 2 to attend on x 1 . Recent work has explored more expressive forms of learning attention weights such as using Query-Key-Value mechanisms  [614] , fully-connected neural network layers  [32, 88] , or even hard gated units for sharper attention  [101] .\n\nFusion with raw modalities entails representation fusion at very early stages, perhaps even involving raw modalities themselves. These approaches typically bear resemblance to early fusion  [46] , which performs concatenation of input data before applying a prediction model (i.e., z mm = [x 1 , x 2 ]). Fusing at the raw modality level is more challenging since raw modalities are likely to exhibit more dimensions of heterogeneity. Nevertheless, Barnum et al.  [47]  demonstrated robustness benefits of fusion at early stages, while Gadzicki et al.  [184]  also found that complex early fusion can outperform abstract fusion. To account for the greater heterogeneity during complex early fusion, many approaches rely on generic encoders that are applicable to both  modalities, such as convolutional layers  [47, 184]  and Transformers  [370, 378] . However, do these complex non-additive fusion models actually learn non-additive interactions between modality elements? Not necessarily, according to Hessel and Lee  [235] . We cover these fundamental analysis questions and more in the quantification challenge ( §2.8).",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Subchallenge 1B: Representation Coordination",
      "text": "Representation coordination aims to learn multimodal contextualized representations that are coordinated through their interconnections (Figure  2 .7). In contrast to representation fusion, coordination keeps the same number of representations but improves multimodal contextualization. We start our discussion with strong coordination that enforces strong equivalence between modality elements, before moving on to partial coordination that captures more general connections such as correlation, order, hierarchies, or relationships beyond similarity.\n\nStrong coordination aims to bring semantically corresponding modalities close together in a coordinated space, thereby enforcing strong equivalence between modality elements. For example, these models would encourage the representation of the word 'dog' and an image of a dog to be close (i.e., semantically positive pairs), while the distance between the word 'dog' and an image of a car to be far apart (i.e., semantically negative pairs)  [181] . The coordination distance is typically cosine distance  [414]  or max-margin losses  [250] . Recent work has explored large-scale representation coordination by scaling up contrastive learning of image and text pairs  [498] , and also found that contrastive learning provably captures redundant information across the two views  [605, 609]  (but not non-redundant information). In addition to contrastive learning, several approaches instead learn a coordinated space by mapping corresponding data from one modality to another  [162] . For example, Socher et al.  [554]  maps image embeddings into word embedding spaces for zero-shot image classification. Similar ideas were used to learn coordinated representations between text, video, and audio  [483] , as well as between pretrained language models and image features  [590] .\n\nPartial coordination: Instead of capturing strong equivalences, partial coordination captures more general modality connections such as correlation, order, hierarchies, or relationships. Partially coordinated models enforce different types of constraints on the representation space beyond semantic similarity, and perhaps only on certain dimensions of the representation.\n\nCanonical correlation analysis (CCA) computes a linear projection that maximizes the correlation between two random variables while enforcing each dimension in a new representation to be orthogonal to each other  [600] . CCA models have been used extensively for cross-modal retrieval  [505]  audio-visual signal analysis  [522] , and emotion recognition  [439] . To increase the expressiveness of CCA, several nonlinear extensions have been proposed including Kernel CCA  [325] , Deep CCA  [27] , and CCA Autoencoders  [651] .\n\nOrdered and hierarchical spaces: Another example of representation coordination comes from order-embeddings of images and language  [635] , which aims to capture a partial order on the language and image embeddings to enforce a hierarchy in the coordinated space. A similar model using denotation graphs was also proposed by Young et al.  [703]  where denotation graphs are used to induce such a partial ordering hierarchy.\n\nRelationship coordination: In order to learn a coordinated space that captures semantic relationships between elements beyond correspondences, Zhang et al.  [728]  use structured representations of text and images to create multimodal concept taxonomies. Delaherche and Chetouani  [138]  learn coordinated representations capturing hierarchical relationships, while Alviar et al.  [20]  apply multiscale coordination of speech and music using partial correlation measures. Finally, Xu et al.  [679]  learn coordinated representations using a Cauchy loss to strengthen robustness to outliers.",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Subchallenge 1C: Representation Fission",
      "text": "Finally, representation fission aims to create a new decoupled set of representations (usually a larger number than the input representation set) that reflects knowledge about internal multimodal structure such as data clustering, independent factors of variation, or modality-specific information. In comparison with joint and coordinated representations, representation fission enables careful interpretation and fine-grained controllability. Depending on the granularity of decoupled factors, methods can be categorized into modality-level and fine-grained fission (Figure  2",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": ".8).",
      "text": "Modality-level fission aims to factorize into modality-specific information primarily in each modality and multimodal information redundant in both modalities  [249, 373, 615] . Disentangled representation learning aims to learn mutually independent latent variables that each explain a particular variation of the data  [57, 238] , and has been useful for modality-level fission by enforcing independence constraints on modality-specific and multimodal latent variables  [249, 615] . Tsai et al.  [615]  and Hsu and Glass  [249]  study factorized multimodal representations and demonstrate the importance of modality-specific and multimodal factors towards generation and prediction. Shi et al.  [545]  study modality-level fission in multimodal variational autoencoders using a mixture-of-experts layer, while Wu and Goodman  [669]  instead use a product-of-experts layer.  Post-hoc representation disentanglement is suitable when it is difficult to retrain a disentangled model, especially for large pretrained multimodal models. Empirical multimodally-additive function projection (EMAP)  [235]  is an approach for post-hoc disentanglement of the effects of unimodal (additive) contributions from cross-modal interactions in multimodal tasks, which works for arbitrary multimodal models and tasks. EMAP is also closely related to the use of Shapley values for feature disentanglement and interpretation  [417] , which can also be used for post-hoc representation disentanglement in general models.\n\nFine-grained fission: Beyond factorizing only into individual modality representations, finegrained fission attempts to further break multimodal data down into the individual subspaces covered by the modalities  [638] . Clustering approaches that group data based on semantic similarity  [402]  have been integrated with multimodal networks for end-to-end representation fission and prediction. For example, Hu et al.  [250]  combine k-means clustering in representations with unsupervised audiovisual learning. Chen et al.  [92]  combine k-means clustering with self-supervised contrastive learning on videos. Subspace clustering [1, 299], manifold learning  [354]  approximate graph Laplacians  [298] , conjugate mixture models  [297] , and dictionary learning  [306]  have also been integrated with multimodal models. Matrix factorization techniques have also seen several applications in multimodal fission for prediction  [16]  and cross-modal retrieval  [77] .",
      "page_start": 47,
      "page_end": 48
    },
    {
      "section_name": "Challenge 2: Alignment",
      "text": "A second challenge is to identify cross-modal connections and interactions between elements of multiple modalities. For example, when analyzing the speech and gestures of a human subject, how can we align specific gestures with spoken words or utterances? Alignment between modalities is challenging since it may depend on long-range dependencies, involves ambiguous segmentation (e.g., words or utterances), and could be either one-to-one, many-to-many, or not exist at all. This section covers recent work in multimodal alignment involving (1) discrete alignment: identifying connections between discrete elements across modalities, (2) continuous alignment: modeling alignment between continuous modality signals with ambiguous segmentation, and (3) contextualized representations: learning better multimodal representations by capturing cross-modal interactions between elements (Figure  2 .9).",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Subchallenge 2A: Discrete Alignment",
      "text": "The first subchallenge aims to identify connections between discrete elements of multiple modalities. We describe recent work in (1) local alignment to discover connections between a given matching pair of modality elements, and (2) global alignment where alignment must be performed globally to learn both the connections and matchings (Figure  2 .10).\n\nLocal alignment between connected elements is particularly suitable for multimodal tasks where there is clear segmentation into discrete elements such as words in text or object bounding boxes in images or videos (e.g., tasks such as visual coreference resolution  [315] , visual referring expression recognition  [120, 122] , and cross-modal retrieval  [181, 462, 486] ). When we have supervised data in the form of connected modality pairs, contrastive learning is a popular approach where the goal is to match representations of the same concept expressed in different modalities  [46] . Several objective functions for learning aligned spaces from varying quantities of paired  [80, 260]  and unpaired  [207]  data have been proposed. Many of the ideas that enforce strong  [181, 369]  or partial  [27, 635, 728]  representation coordination ( §2.3.2) are also applicable for local alignment. Several examples include aligning books with their corresponding movies/scripts  [741] , matching referring expressions to visual objects  [407] , and finding similarities between image regions and their descriptions  [254] . Methods for local alignment have also enabled the learning of shared semantic concepts not purely based on language but also on additional modalities such as vision  [260] , sound  [121, 554] , and multimedia  [741]  that are useful for downstream tasks.\n\nGlobal alignment: When the ground-truth modality pairings are not available, alignment must be performed globally between all elements across both modalities. Optimal transport (OT)-based approaches  [640]  (which belong to a broader set of matching algorithms) are a potential solution since they jointly optimize the coordination function and optimal coupling between modality elements by posing alignment as a divergence minimization problem. These approaches are useful for aligning multimodal representation spaces  [351, 492] . To alleviate computational issues, several recent advances have integrated them with neural networks  [99] , approximated optimal transport with entropy regularization  [660] , and formulated convex relaxations for efficient learning  [207] .",
      "page_start": 49,
      "page_end": 50
    },
    {
      "section_name": "Continuous Warping",
      "text": "",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Subchallenge 2B: Continuous Alignment",
      "text": "So far, one important assumption we have made is that modality elements are already segmented and discretized. While certain modalities display clear segmentation (e.g., words/phrases in a sentence or object regions in an image), there are many cases where the segmentation is not readily provided, such as in continuous signals (e.g, financial or medical time-series), spatiotemporal data (e.g., satellite or weather images), or data without clear semantic boundaries (e.g., MRI images). In these settings, methods based on warping and segmentation have been recently proposed:\n\nContinuous warping aims to align two sets of modality elements by representing them as continuous representation spaces and forming a bridge between these representation spaces, such as aligning continuous audio and video data  [187, 603, 604] . Adversarial training is a popular approach to warp one representation space into another. Initially used in domain adaptation  [54] , adversarial training learns a domain-invariant representation across domains where a domain classifier is unable to identify which domain a feature came from  [14] . These ideas have been extended to align multimodal spaces  [247, 252, 431] . Hsu et al.  [247]  use adversarial training to align images and medical reports, Hu et al.  [252]  design an adversarial network for cross-modal retrieval, and Munro and Damen  [431]  design both self-supervised alignment and adversarial alignment objectives for multimodal action recognition. Dynamic time warping (DTW)  [321]  segments and aligns multi-view time-series data by maximizing their similarity via time warping (inserting frames) such that they are aligned across time. For multimodal tasks, it is necessary to design similarity metrics between modalities  [28, 594] , such as combining DTW with CCA or other coordination functions  [612] .\n\nModality segmentation involves dividing high-dimensional data into elements with semantically meaningful boundaries. A common problem involves temporal segmentation, where the goal is to discover the temporal boundaries across sequential data. Several approaches for temporal segmentation include forced alignment, a popular approach to align discrete speech units with individual words in a transcript  [709] . Malmaud et al.  [404]  explore multimodal alignment using a factored hidden Markov model to align ASR transcripts to the ground truth. Clustering approaches have also been used to group continuous data based on semantic similarity  [402] . Clustering-based discretization has recently emerged as an important preprocessing step for generalizing language-based pretraining (with clear word/byte pair segmentation boundaries and discrete elements) to video or audio-based pretraining (without clear segmentation boundaries and continuous elements). By clustering raw video or audio features into a discrete set, approaches such as VideoBERT  [577]  perform masked pretraining on raw video and audio data. Similarly, approaches such as DALL.E  [503] , VQ-VAE  [630] , and CMCM  [384]  also utilize discretized intermediate layers obtained via vector quantization and showed benefits in modality alignment.",
      "page_start": 50,
      "page_end": 51
    },
    {
      "section_name": "Subchallenge 2C: Contextualized Representations",
      "text": "Finally, contextualized representation learning aims to model all modality connections and interactions to learn better representations. Contextualized representations have been used as an intermediate (often latent) step enabling better performance on a number of downstream tasks including speech recognition, machine translation, media description, and visual question-answering. We categorize work in contextualized representations into (1) joint undirected alignment, (2) cross-modal directed alignment, and (3) alignment with graph networks (Figure  2 .12).\n\nJoint undirected alignment aims to capture undirected connections across pairs of modalities, where the connections are symmetric in either direction. This is commonly referred to in the literature as unimodal, bimodal, trimodal interactions, and so on  [401] . Joint undirected alignment is typically captured by parameterizing models with alignment layers and training end-to-end for a multimodal task. These alignment layers can include attention weights  [88] , tensor products  [388, 713] , and multiplicative interactions  [283] . More recently, transformer models  [632]  have emerged as powerful encoders for sequential data by automatically aligning and capturing complementary features at different time steps. Building upon the initial text-based transformer model, multimodal transformers have been proposed that perform joint alignment using a full self-attention over modality elements concatenated across the sequence dimension (i.e., early fusion)  [348, 577] . As a result, all modality elements become jointly connected to all other modality elements similarly (i.e., modeling all connections using dot-product similarity kernels).\n\nCross-modal directed alignment relates elements from a source modality in a directed manner to a target modality, which can model asymmetric connections. For example, temporal attention models use alignment as a latent step to improve many sequence-based tasks  [677, 725] . These attention mechanisms are typically directed from the output to the input so that the resulting weights reflect a soft alignment distribution over the input. Multimodal transformers perform directed alignment using query-key-value attention mechanisms to attend from one modality's sequence to another, before repeating in a bidirectional manner. This results in two sets of asymmetric contextualized representations to account for the possibly asymmetric connections between modalities  [390, 589, 614] . These methods are useful for sequential data by automatically aligning and capturing complementary features at different time-steps  [614] . Large vision-language foundation models have emerged as powerful models capable of learning contextualized representations for multiple tasks involving natural language, images, video, and audio  [185, 370, 454, 498, 506] . These models typically build on top of pretrained language models  [497] , pretrained visual encoders  [154]  combined with an alignment layer. Alignment can be done via end-to-end training with multimodal transformers  [682]  (e.g., Flamingo  [18] , Open-Flamingo  [37] , Kosmos  [475] ), or keeping the language and vision parts frozen and only training a post-hoc alignment layer (e.g., MiniGPT-4  [737] , BLIP-2  [347] , InstructBLIP  [128] , LLaMA-Adapter V2  [186] ). Self-supervised pretraining has emerged as an effective way to train these architectures to learn general-purpose representations from larger-scale unlabeled multimodal data before transferring to specific downstream tasks via supervised fine-tuning  [155, 348, 737] . Pretraining objectives typically consist of unimodal language modeling  [497, 499] , image-to-text or text-to-image alignment  [232, 737] , and multimodal instruction tuning  [128, 385, 393] . We refer the reader to recent survey papers discussing these large vision-language models in more detail  [157, 185] .\n\nGraphical alignment generalizes the sequential pattern seen in undirected or directed alignment into arbitrary graph structures between elements. This has several benefits since it does not require all elements to be connected, and allows the user to choose different edge functions for different connections. Graph neural networks  [634]  can be used to recursively learn element representations contextualized with the elements in locally connected neighborhoods  [524, 634] , such as in MTAG  [688]  and F2F-CL  [662]  for multimodal and multi-speaker videos.",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Challenge 3: Reasoning",
      "text": "Reasoning is defined as combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and the problem structure. We categorize work towards multimodal reasoning into 4 subchallenges of structure modeling, intermediate concepts, inference paradigm, and external knowledge (Figure  2 .13). (1) Structure modeling involves defining or learning the relationships over which reasoning occurs, (2) intermediate concepts studies the parameterization of individual multimodal concepts in the reasoning process, (3) inference paradigm learns how increasingly abstract concepts are inferred from individual multimodal evidence, and (4) external knowledge aims to leverage large-scale databases in the study of structure, concepts, and inference.",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "Hierarchical Temporal",
      "text": "Interactive Discovery   2 ) temporal (i.e., organized across time), (  3 ) interactive (i.e., where the state changes depending on each step's decision), and (4) discovered when the latent structure is unknown and instead directly inferred from data and optimization.",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Subchallenge 3A: Structure Modeling",
      "text": "Structure modeling aims to capture the hierarchical relationship over which composition occurs, usually via a data structure parameterizing atoms, relations, and the reasoning process. Commonly used data structures include trees  [244] , graphs  [708] , or neural modules  [26] . We cover recent work in modeling latent hierarchical, temporal, and interactive structure, as well as structure discovery when the latent structure is unknown (Figure  2 .",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "14).",
      "text": "Hierarchical structure defines a system of organization where abstract concepts are defined as a function of less abstract ones. Hierarchical structure is present in many tasks involving language syntax, visual syntax, or higher-order reasoning. These approaches typically construct a graph based on predefined node and edge categories before using (heterogeneous variants of) graph neural networks to capture a representation of structure  [544] , such as using language syntactic structure to guide visual modules that discover specific information in images  [26, 120] . Graph-based reasoning approaches have been applied for visual commonsense reasoning  [380] , visual question answering  [520] , machine translation  [700] , recommendation systems  [593] , web image search  [648] , and social media analysis  [526] .\n\nTemporal structure extends the notion of compositionality to elements across time, which is necessary when modalities contain temporal information, such as in video, audio, or timeseries data. Explicit memory mechanisms have emerged as a popular choice to accumulate multimodal information across time so that long-range cross-modal interactions can be captured through storage and retrieval from memory. Rajagopalan et al.  [502]  explore various memory representations including multimodal fusion, coordination, and factorization. Insights from keyvalue memory  [677]  and attention-based memory  [714]  have also been successfully applied to applications including question answering, video captioning, emotion recognition, and sentiment analysis.\n\nInteractive structure extends the challenge of reasoning to interactive settings, where the state of the reasoning agent changes depending on the local decisions made at every step. Typically formalized by the sequential decision-making framework, the challenge lies in maximizing long-term cumulative reward despite only interacting with the environment through short-term actions  [583] . To tackle the challenges of interactive reasoning, the growing research field of multimodal reinforcement learning (RL) has emerged from the intersection of language understanding, embodiment in the visual world, deep reinforcement learning, and robotics. We refer the reader to the extensive survey paper by Luketina et al.  [394]  and the position paper by Bisk et al.  [62]  for a full review of this field. Luketina et al.  [394]  separate the literature into multimodal-conditional RL (in which multimodal interaction is necessitated by the problem formulation itself, such as instruction following  [88, 654] ) and language-assisted RL (in which multimodal data is optionally used to facilitate learning, such as reading instruction manuals  [437] ).\n\nStructure discovery: It may be challenging to define the structure of multimodal composition without some domain knowledge of the given task. As an alternative approach, recent work has also explored using differentiable strategies to automatically search for the structure in a fully data-driven manner. To do so, one first needs to define a candidate set of reasoning atoms and relationships, before using a 'meta' approach such as architecture search to automatically search for the ideal sequence of compositions for a given task  [479, 683] . These approaches can benefit from optimization tricks often used in the neural architecture search literature. Memory, Attention, and Composition (MAC) similarly search for a series of attention-based reasoning steps from data in an end-to-end approach  [266] . Finally, Hu et al.  [255]  extend the predefined reasoning structure obtained through language parsing in Andreas et al.  [26]  by instead using policy gradients to automatically optimize a compositional structure over a discrete set of neural modules.",
      "page_start": 53,
      "page_end": 54
    },
    {
      "section_name": "Subchallenge 3B: Intermediate Concepts",
      "text": "The second subchallenge studies how we can parameterize individual multimodal concepts within the reasoning process. While intermediate concepts are usually dense vector representations in standard neural architectures, there has also been substantial work towards interpretable attention maps, discrete symbols, and language as an intermediate medium for reasoning.\n\nAttention maps are a popular choice for intermediate concepts since they are, to a certain extent, human-interpretable, while retaining differentiability. For example, Andreas et al.  [26]  design individual modules such as 'attend', 'combine', 'count', and 'measure' that are each parametrized by attention operations on the input image for visual question answering. Xu et al.  [681]  explore both soft and hard attention mechanisms for reasoning in image captioning generation. Related work has also used attention maps through dual attention architectures  [434]  or stacked latent attention architectures  [169]  for multimodal reasoning. These are typically applied for problems involving complex reasoning steps such as CLEVR  [289]  or VQA  [730] .\n\nDiscrete symbols: A further level of discretization beyond attention maps involves using discrete symbols to represent intermediate concepts. Recent work in neuro-symbolic learning aims to integrate these discrete symbols as intermediate steps in multimodal reasoning in tasks such as visual question answering  [26, 406, 633]  or referring expression recognition  [120] . A core challenge in this approach lies in maintaining the differentiability of discrete symbols, which has been tackled via logic-based differentiable reasoning  [22, 532] .\n\nLanguage as a medium: Finally, perhaps the most human-understandable form of intermediate concepts is natural language (through discrete words or phrases) as a medium. Recently, Zeng et al.  [723]  explored using language as an intermediate medium to coordinate multiple separate pretrained models in a zero-shot manner. Several approaches also used language phrases obtained from external knowledge graphs to facilitate interpretable reasoning  [213, 740] . Hudson and Manning  [265]  designed a neural state machine to simulate the execution of a question being asked about an image, while using discrete words as intermediate concepts.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "Subchallenge 3C: Inference Paradigms",
      "text": "The third subchallenge in multimodal reasoning defines how increasingly abstract concepts are inferred from individual multimodal evidence. While advances in local representation fusion (such as additive, multiplicative, tensor-based, attention-based, and sequential fusion, see §2.3.1 for a full review) are also generally applicable here, the goal of reasoning is to be more interpretable in the inference process through domain knowledge about the multimodal problem. To that end, we cover recent directions in explicitly modeling the inference process via logical and causal operators as examples of recent trends in this direction.\n\nLogical inference: Logic-based differentiable reasoning has been widely used to represent knowledge in neural networks  [22, 532] . Many of these approaches use differentiable fuzzy logic  [631]  which provides a probabilistic interpretation of logical predicates, functions, and constants to ensure differentiability. These logical operators have been applied for visual question answering  [203]  and visual reasoning  [22] . Among the greatest benefits of logical reasoning lies in its ability to perform interpretable and compositional multi-step reasoning  [267] . Logical frameworks have also been useful for visual-textual entailment  [587]  and geometric numerical reasoning  [94] , fields where logical inductive biases are crucial toward strong performance.\n\nCausal inference extends the associational level of reasoning to interventional and counterfactual levels  [471] , which requires extensive knowledge of the world to imagine counterfactual effects. For example, Yi et al.  [699]  propose the CLEVRER benchmark focusing on four specific elements of reasoning on videos: descriptive (e.g., 'what color'), explanatory ('what's responsible for'), predictive ('what will happen next'), and counterfactual ('what if'). Beyond CLEVRER, recent work has also proposed Causal VQA  [8]  and Counterfactual VQA  [448]  to measure the robustness of VQA models under controlled interventions to the question as a step towards mitigating language bias in VQA models. Methods inspired by integrating causal reasoning capabilities into neural network models have also been shown to improve robustness and reduce biases  [650] .",
      "page_start": 55,
      "page_end": 56
    },
    {
      "section_name": "Subchallenge 3D: External Knowledge",
      "text": "The final subchallenge studies the derivation of knowledge in the study of defining composition and structure. Knowledge can refer to any data source that is complementary to the limited supervised training data that models typically see, which encapsulates larger banks of unlabeled internet data (e.g., textbooks, Wikipedia, videos), curated knowledge graphs and knowledge bases, and expert domain knowledge for specific tasks such as healthcare and robotics.\n\nMultimodal knowledge graphs extend classic work in language and symbolic knowledge graphs (e.g., Freebase  [66] , DBpedia  [35] , YAGO  [573] , WordNet  [420] ) to semantic networks containing multimodal concepts as nodes and multimodal relationships as edges  [739] . Multimodal knowledge graphs are important because they enable the grounding of structured information in the visual and physical world  [63, 706] . For example, Liu et al.  [387]  constructs multimodal knowledge graphs containing both numerical features and images for entities. Visual Genome is another example containing dense annotations of objects, attributes, and relationships in images and text  [317] . These multimodal knowledge bases have been shown to benefit visual question answering  [672, 740] , knowledge base completion  [481] , and image captioning  [415] . Gui et al.  [213]  integrates knowledge into vision-and-language transformers for automatic reasoning over both knowledge sources. Another promising approach is multimodal knowledge expan-Definition: Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure and coherence",
      "page_start": 55,
      "page_end": 56
    },
    {
      "section_name": "Sub-Challenges:",
      "text": "",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Summarization Translation Creation",
      "text": "Reduction Expansion Maintenance > Information:\n\n.15: How can we learn a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence? Generation involves (1) summarizing multimodal data to highlight the most salient parts, (2) translating from one modality to another while being consistent with modality connections, and (3) creating multiple modalities simultaneously while maintaining coherence.\n\nsion  [496, 673, 684]  using knowledge distillation to expand knowledge from unimodal data to multimodal settings. We refer the reader to a comprehensive survey by Zhu et al.  [739]  for additional references.\n\nMultimodal commonsense reasoning requires deeper real-world knowledge potentially spanning logical, causal, and temporal relationships between concepts. For example, elements of causal reasoning are required to answer the questions regarding images in VCR  [720]  and VisualCOMET  [467] , while other works have also introduced datasets with video and text inputs to test for temporal reasoning (e.g., MovieQA  [595] , MovieFIB  [403] , TVQA  [339] ). Benchmarks for multimodal commonsense typically require leveraging external knowledge from knowledge bases  [559]  or pretraining paradigms on large-scale datasets  [390, 721] .",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Challenge 4: Generation",
      "text": "The fourth challenge involves learning a generative process to produce raw modalities that reflect cross-modal interactions, structure, and coherence, through summarization, translation, and creation (Figure  9 .3). These three categories are distinguished based on the information change from input to output modalities, following categorizations in text generation  [141] . We will cover recent advances as well as the evaluation of generated content.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Subchallenge 4A: Summarization",
      "text": "Summarization aims to compress data to create an abstract that represents the most important or relevant information within the original content. Recent work has explored various input modalities to guide text summarization, such as images  [95] , video  [352] , and audio  [167, 282, 345] . Recent trends in multimodal summarization include extractive and abstractive approaches. Extractive approaches aim to filter words, phrases, and other unimodal elements from the input to create a summary  [96, 282, 345] . Beyond text as output, video summarization is the task of producing a compact version of the video (visual summary) by encapsulating the most informative parts  [516] . Li et al.  [345]  collected a dataset of news videos and articles paired with manually annotated summaries as a benchmark towards multimodal summarization. Finally, UzZaman et al.  [629]  aim to simplify complex sentences by extracting multimodal summaries for accessibility. On the other hand, abstractive approaches define a generative model to generate the summary at multiple levels of granularity  [95, 350] . Although most approaches only focus on generating a textual summary from multimodal data  [461] , several directions have also explored generating summarized images to supplement the generated textual summary  [95, 352] .",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Subchallenge 4B: Translation",
      "text": "Translation aims to map one modality to another while respecting semantic connections and information content  [641] . For example, generating a descriptive caption of an image can help improve the accessibility of visual content for blind people  [215] . Multimodal translation brings about new difficulties involving the generation of high-dimensional structured data as well as their evaluation. Recent approaches can be classified as exemplar-based, which are limited to retrieving from training instances to translate between modalities but guarantee fidelity  [171] , and generative models which can translate into arbitrary instances interpolating beyond the data but face challenges in quality, diversity, and evaluation  [310, 503, 623] . Despite these challenges, recent progress in large-scale generative models has yielded impressive results in textto-image  [503, 512] , text-to-video  [551] , audio-to-image  [281] , text-to-speech  [508] , speech-togesture  [13] , speaker-to-listener  [441] , language to pose  [12] , and speech and music generation  [9, 124, 452] .",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Subchallenge 4C: Creation",
      "text": "Creation aims to generate novel high-dimensional data (which could span text, images, audio, video, and other modalities) from small initial examples or latent conditional variables. This conditional decoding process is extremely challenging since it needs to be (1) conditional: preserve semantically meaningful mappings from the initial seed to a series of long-range parallel modalities, (2) synchronized: semantically coherent across modalities, (3) stochastic: capture many possible future generations given a particular state, and (4) auto-regressive across possibly long ranges. Many modalities have been considered as targets for creation. Language generation has been explored for a long time  [497] , and recent work has explored high-resolution speech and sound generation using neural networks  [452] . Photorealistic image generation has also recently become possible due to advances in large-scale generative modeling  [295] . Furthermore, there have been a number of attempts at generating abstract scenes  [588] , computer graphics  [418] , and talking heads  [738] . While there has been some progress toward video generation  [551] , complete synchronized generation of realistic video, text, and audio remains a challenge.\n\nFinally, one of the biggest challenges facing multimodal generation is difficulty in evaluating generated content, especially when there exist serious ethical issues when fake news  [56] , hate speech  [3, 193] , deepfakes  [222] , and lip-syncing videos  [584]  can be easily generated. While the ideal way to evaluate generated content is through user studies, it is time-consuming, costly, and can potentially introduce subjectivity bias  [195] . Several automatic proxy metrics have been proposed  [25, 98]  by none are universally robust across many generation tasks.",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Challenge 5: Transference",
      "text": "Transference aims to transfer knowledge between modalities and their representations, and is often used when there is a primary modality that we care about making predictions on but suffers from limited resources -a lack of annotated data, noisy inputs, or unreliable labels, and a secondary modality with more abundant or reliable data. How can knowledge learned from a secondary modality (e.g., predicted labels or representation) help a model trained on a primary modality?\n\nWe call this challenge transference, since the transfer of information from the secondary modality gives rise to new behaviors previously unseen in the primary modality. We identify three types of approaches: (1) cross-modal transfer, (2) multimodal co-learning, and (3) model induction (Figure  2 .16).",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Subchallenge 5A: Cross-Modal Transfer",
      "text": "In most settings, it may be easier to collect either labeled or unlabeled data in the secondary modality and train strong supervised or pretrained models. These models can then be conditioned or fine-tuned for a downstream task involving the primary modality. In other words, this line of research extends unimodal transfer and fine-tuning to cross-modal settings.\n\nTuning: Inspired by prior work in NLP involving prefix tuning  [357]  and prompt tuning  [342] , recent work has also studied the tuning of pretrained language models to condition on visual and other modalities. For example, Tsimpoukelli et al.  [623]  quickly conditions a pretrained, frozen language model on images for image captioning. Related work has also adapted prefix tuning for image captioning  [97] , multimodal fusion  [226] , and summarization  [707] . While prefix tuning is simple and efficient, it provides the user with only limited control over how information is transferred. Representation tuning goes a level deeper by modifying the inner representations of the language model via contextualization with other modalities. For example, Ziegler et al.  [742]  includes additional self-attention layers between language model layers and external modalities. Rahman et al.  [501]  design a shifting gate to adapt language model layers with audio and visual information.\n\nMultitask learning aims to use multiple large-scale tasks to improve performance as compared to learning on individual tasks. Several models such as Perceiver  [276] , MultiModel  [291] , ViT-BERT  [353] , and PolyViT  [378]  have explored the possibility of using the same unimodal encoder architecture for different inputs across unimodal tasks (i.e., language, image, video, or audioonly). The Transformer architecture has emerged as a popular choice due to its suitability for serialized inputs such as text (sequence of tokens)  [144] , images (sequence of patches)  [154] , video (sequence of images)  [577] , and other time-series data (sequence of timesteps)  [379] . There have also been several attempts to build a single model that works well on a suite of multimodal tasks, including both not limited to HighMMT  [370] , VATT  [15] , FLAVA  [552] , and Gato  [506] .\n\nTransfer learning: While more research has focused on transfer within the same modality with external information  [554, 676, 717] , Liang et al.  [369]  studies transfer to new modalities using small amounts of paired but unlabeled data. Lu et al.  [391]  found that Transformers pretrained on language transfer to other sequential modalities as well. Liang et al.  [370]  builds a single multimodal model capable of transferring to completely new modalities and tasks. Recently, there has also been a line of work investigating the transfer of pretrained language models for planning  [259] , interactive decision-making  [355] , and robotics  [70] .",
      "page_start": 58,
      "page_end": 59
    },
    {
      "section_name": "Subchallenge 5B: Multimodal Co-Learning",
      "text": "Multimodal co-learning aims to transfer information learned through secondary modalities to target tasks involving the primary modality by sharing intermediate representation spaces between both modalities. These approaches essentially result in a single joint model across all modalities.\n\nCo-learning via representation aims to learn a joint or coordinated representation space using both modalities as input. Typically, this involves adding secondary modalities during the training process, designing a suitable representation space, and investigating how the multimodal model transfers to the primary modality during testing. For example, DeViSE learns a coordinated space between image and text to improve image classification  [181] . Marino et al.  [409]  use knowledge graphs for image classification via a graph-based joint representation. Jia et al.  [285]  improve image classifiers with contrastive learning between images and noisy captions. Finally, Zadeh et al.  [717]  showed that implicit co-learning is also possible without explicit co-learning objectives.\n\nCo-learning via generation instead learns a translation model from the primary to secondary modality, resulting in enriched representations of the primary modality that can predict both the label and 'hallucinate' secondary modalities containing shared information. Classic examples in this category includes language modeling by mapping contextualized text embeddings into images  [590] , image classification by projecting image embeddings into word embeddings  [554] , and language sentiment analysis by translating language into video and audio  [483] .",
      "page_start": 59,
      "page_end": 60
    },
    {
      "section_name": "Subchallenge 5C: Model Induction",
      "text": "In contrast to co-learning, model induction approaches keep individual unimodal models across primary and secondary modalities separate but transfer information across them. There are two general ways of doing so. The first is co-training, where each unimodal model's predictions on their own modality are used to pseudo-label new unlabeled examples in the other modality, thereby enlarging the training set of the other modality  [65] . The second is co-regularization  [550, 564] , in which the predictions from separate unimodal classifiers are regularized to be similar, thereby encouraging both classifiers to share information (i.e., redundancy). Therefore, information is transferred across modalities through model predictions instead of shared representation spaces.\n\nMultimodal co-training extends co-training by jointly learning classifiers for multiple modalities  [239] . Guillaumin et al.  [214]  use a classifier on both image and text to pseudo-label unlabeled images before training a final classifier on both labeled and unlabeled images. Cheng et al.  [111]  performs semi-supervised multimodal learning using a diversity-preserving co-training algorithm. Finally, Dunnmon et al.  [159]  applies ideas from data programming to the problem of cross-modal weak supervision, where weak labels derived from a secondary modality (e.g., text) are used to train models over the primary modality (e.g., images).\n\nCo-regularization methods employs a regularizer that penalizes functions from either modality that disagree with each other. These methods are useful in controlling model complexity by preferring hypothesis classes with redundancy across the two modalities  [550] . Sridharan and Kakade  [564]  provide guarantees for these approaches using an information-theoretic framework. More recently, similar co-regularization approaches have also been applied for multimodal feature selection  [246] , semi-supervised multimodal learning  [693] , and video summarization  [428] .",
      "page_start": 59,
      "page_end": 60
    },
    {
      "section_name": "Challenge 6: Quantification",
      "text": "Quantification aims to provide a deeper empirical and theoretical study of multimodal models to gain insights and improve their robustness, interpretability, and reliability in real-world applications. We break down quantification into 3 sub-challenges: (1) quantifying the dimensions of heterogeneity and how they subsequently influence modeling and learning, (2) quantifying the presence and type of connections and interactions in multimodal datasets and trained models, and (3) characterizing the learning and optimization challenges involved when learning from heterogeneous data (Figure  2 .17).",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Subchallenge 6A: Dimensions Of Heterogeneity",
      "text": "This subchallenge aims to understand the dimensions of heterogeneity commonly encountered in multimodal research, and how they subsequently influence modeling and learning (Figure  2 .18).\n\nModality information: Understanding the information of modalities and their constituents is important for determining which parts contributed to subsequent modeling. Recent work can be categorized into (1) interpretable methods that explicitly model how each modality is used  [466, 616, 718]  or (2) post-hoc explanations of black-box models  [85, 205] . In the former, methods such as Concept Bottleneck Models  [311]  and fitting sparse linear layers  [667]  or decision trees  [642]  on top of deep feature representations have emerged as promising choices. In the latter, gradient-based visualizations  [205, 530, 549] ) and feature attributions (e.g., modality contribution  [192] , LIME  [510] , and Shapley values  [417] ) have been used to highlight regions of modality importance.\n\nModality biases are unintended correlations between input and outputs that could be introduced during data collection  [61, 67] , modeling  [194] , or during human annotation  [143] . Modality biases can lead to unexpectedly poor performance in the real world  [517] , or even more dangerously, potential for harm towards underrepresented groups  [231, 474] . For example, Goyal et al.  [206]  found unimodal biases in the language modality of VQA tasks, resulting in mistakes due to ignoring visual information  [10] . Subsequent work has developed carefully curated diagnostic benchmarks to mitigate data collection biases, like VQA 2.0  [206] , GQA  [267] , and NLVR2  [574] . Recent work has also found compounding social biases in multimodal systems  [114, 513, 565]  stemming from gender bias in both language and visual modalities  [73, 543] , which may cause danger when deployed  [474] .",
      "page_start": 60,
      "page_end": 61
    },
    {
      "section_name": "Modality Noise Topologies And Robustness:",
      "text": "The study of modality noise topologies aims to benchmark and improve how multimodal models perform in the presence of real-world data imperfections. Each modality has a unique noise topology, which determines the distribution of noise and imperfections that it commonly encounters. For example, images are susceptible to blurs and shifts, typed text is susceptible to typos following keyboard positions, and multimodal time-series data is susceptible to correlated imperfections across synchronized time steps. Liang et al.  [367]  collect a comprehensive set of targeted noisy distributions unique to each modality. In addition to natural noise topologies  [338, 399] , related work has also explored adversarial attacks  [149]  and distribution shifts  [176]  in multimodal systems. Finally, there have been recent efforts on incomplete multimodal learning  [370, 398, 649, 692]  to account for noisy or missing modalities, such as modality imputation using probabilistic models  [398] , autoencoders  [610] , translation models  [483] , low-rank approximations  [364] , or knowledge distillation  [649] , or training general models with a wide range of modalities so they can still operate on partial subsets  [370, 506] . However, they may run the risk of possible error compounding and require knowing which modalities are imperfect beforehand.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Subchallenge 6B: Modality Interconnections",
      "text": "Modality connections and interactions are an essential component of multimodal models, which has inspired an important line of work in visualizing and understanding the nature of modality interconnections in datasets and trained models. We divide recent work into quantifying (1) connections: how modalities are related and share commonality, and (2) interactions: how modality elements interact during inference (Figure  2",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": ".19).",
      "text": "Connections: Recent work has explored the quantification of modality connections through visualization tools on joint representation spaces  [271]  or attention maps  [7] . Perturbation-based analysis perturbs the input and observes changes in the output to understand internal connections  [374, 448] . Finally, specifically curated diagnostic datasets are also useful in understanding semantic connections: Winoground  [602]  probes vision and language models for visio-linguistic compositionality, and PaintSkills  [114]  measures the connections necessary for visual reasoning.\n\nInteractions: One common categorization of interactions involves redundancy, uniqueness, and synergy  [663] . Redundancy describes task-relevant information shared among features, uniqueness studies the task-relevant information present in only one of the features, and synergy investigates the emergence of new information when both features are present. From a statistical perspective, measures of redundancy include mutual information  [44, 65]  and contrastive learning estimators  [609, 617] . Other approaches have studied these measures in isolation, such as redundancy via distance between prediction logits using either feature  [411] , statistical distribution tests on input features  [36] , or via human annotations  [515] . From the semantic view, recent work in Causal VQA  [8]  and Counterfactual VQA  [448]  seek to understand the interactions captured by trained models by measuring their robustness under controlled semantic edits to the question or image. Finally, recent work has formalized definitions of non-additive interactions to quantify their presence in trained models  [563, 620, 685] . Parallel research such as EMAP  [235] , DIME  [396] , M2Lens  [655] , and MultiViz  [374]  take a more visual approach to visualize the interactions in real-world multimodal datasets and models through higher-order gradient activations of learned representations. Despite this, accurately visualizing multimodal information and interactions remains a challenge due to the brittleness of interpretation methods  [197] , difficulty in evaluation  [318] , and challenges in extending visualization methods to applications such as biomedical data integration, imaging, intelligent systems and user interfaces.",
      "page_start": 61,
      "page_end": 62
    },
    {
      "section_name": "Subchallenge 6C: Multimodal Learning Process",
      "text": "Finally, there is a need to characterize the learning and optimization challenges involved when learning from heterogeneous data. This section covers recent work in (1) generalization across modalities and tasks, (2) better optimization for balanced and efficient training, and (3) balancing the tradeoffs between performance, robustness, and complexity in real-world deployment (Figure  2 .20).\n\nGeneralization: With advances in sensing technologies, many real-world platforms such as cellphones, smart devices, self-driving cars, healthcare technologies, and robots now integrate a much larger number of sensors beyond the prototypical text, video, and audio modalities  [263] . Recent work has studied generalization across paired modality inputs  [369, 498]  and in unpaired scenarios where each task is defined over only a small subset of all modalities  [370, 391, 506] .\n\nOptimization challenges: Related work has also explored the optimization challenges of multimodal learning, where multimodal networks are often prone to overfitting due to increased capacity, and different modalities overfit and generalize at different rates so training them jointly with a single optimization strategy is sub-optimal  [652] . Subsequent work has studied why joint training of multimodal networks may be difficult and proposed methods to improve the optimization process via weighting approaches  [671] , adaptive learning  [261, 262] , or contrastive learning  [377] .\n\nModality Tradeoffs: In real-world deployment, a balance between performance, robustness, and complexity is often required. Therefore, one often needs to balance the utility of additional modalities with the additional complexity in data collection and modeling  [367]  as well as increased susceptibility to noise and imperfection in the additional modality  [483] . How can we formally quantify the utility and risks of each input modality, while balancing these tradeoffs for reliable real-world usage? There have been several attempts toward formalizing the semantics of a multimodal representation and how these benefits can transfer to downstream tasks  [358, 599, 617] , while information-theoretic arguments have also provided useful insights  [65, 564] .",
      "page_start": 62,
      "page_end": 63
    },
    {
      "section_name": "Chapter 3 Machine Learning Foundations Of Multimodal Interactions",
      "text": "A core challenge in machine learning lies in capturing the interactions between multiple input modalities. Learning different types of multimodal interactions is often quoted as motivation for many successful multimodal modeling paradigms, such as contrastive learning to capture redundancy  [307, 498] , modality-specific representations to retain unique information  [615] , as well as tensors and multiplicative interactions to learn higher-order interactions  [283, 364, 713] . However, several fundamental research questions remain: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? This paper aims to formalize these questions by proposing an approach to quantify the nature (i.e., which type) and degree (i.e., the amount) of modality interactions, a fundamental principle underpinning our understanding of multimodal datasets and models  [375] .\n\nBy bringing together two previously disjoint research fields of Partial Information Decomposition (PID) in information theory  [59, 210, 663]  and multimodal machine learning  [46, 375] , we provide precise definitions categorizing interactions into redundancy, uniqueness, and synergy. Redundancy quantifies information shared between modalities, uniqueness quantifies the information present in only one of the modalities, and synergy quantifies the emergence of new information not previously present in either modality. A key aspect of these four measures is that they not only quantify interactions between modalities, but also how they relate to a downstream task. Figure  3 .1 shows a depiction of these four measures, which we refer to as PID statistics. Leveraging insights from neural representation learning, we propose two new estimators for PID statistics that can scale to high-dimensional multimodal datasets and models. The first estimator is exact, based on convex optimization, and is able to scale to features with discrete support, while the second estimator is an approximation based on sampling, which enables us to handle features with large discrete or even continuous supports. We validate our estimation of PID in 2 ways: (1) on synthetic datasets where PID statistics are known from the nature of data generation, and (2) on real-world data where PID is compared with human annotation. Finally, we demonstrate that estimated PID statistics can help in multimodal applications involving: 1. Dataset quantification: We apply PID to quantify large-scale multimodal datasets, showing that these estimates match common intuition for interpretable modalities (e.g., language, vision, and audio) and yield new insights in other domains (e.g, healthcare, HCI, and robotics).",
      "page_start": 64,
      "page_end": 65
    },
    {
      "section_name": "Background And Related Work",
      "text": "Let X i and Y be sample spaces for features and labels. Define ∆ to be the set of joint distributions over (X 1 , X 2 , Y). We are concerned with features X 1 , X 2 (with support X i ) and labels Y (with support Y) drawn from some distribution p ∈ ∆. We denote the probability mass (or density) function by p(x 1 , x 2 , y), where omitted parameters imply marginalization. Key to our work is defining estimators that given p or samples {(x 1 , x 2 , y) ∶ X 1 × X 2 × Y} thereof (i.e., dataset or model predictions), estimates the amount of redundant, unique, and synergistic interactions.",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Partial Information Decomposition",
      "text": "Classical Information Theory Partial Information Decomposition  Figure 3 .1: PID decomposes I(X 1 , X 2 ; Y ) into redundancy R between X 1 and X 2 , uniqueness U 1 in X 1 and U 2 in X 2 , and synergy S in both X 1 and X 2 .\n\nInformation theory formalizes the amount of information that one variable provides about another  [536] . However, its extension to 3 variables is an open question  [190, 412, 597, 659] . In particular, the natural threeway mutual information I(X 1 ; X 2 ; Y ) = I(X 1 ; X 2 ) -I(X 1 ; X 2 |Y )  [412, 597]  can be both positive and negative, which makes it difficult to interpret. In response, Partial information decomposition (PID)  [663]  generalizes information theory to multiple variables by decomposing I p (X 1 , X 2 ; Y ), the total information 2 variables X 1 , X 2 provide about a task Y into 4 quantities (see Figure  3 .1): redundancy R between X 1 and X 2 , uniqueness U 1 in X 1 and U 2 in X 2 , and synergy S that only emerges when both X 1 and X 2 are present. We adopt the PID definition proposed by Bertschinger et al.  [59] :\n\nwhere\n\n} and the notation I p (⋅) and I q (⋅) disambiguates mutual information under p and q respectively. The key lies in optimizing q ∈ ∆ p to satisfy the marginals q(x i , y) = p(x i , y), but relaxing the coupling between x 1 and x 2 : q(x 1 , x 2 ) need not be equal to p(x 1 , x 2 ). The intuition behind this is that one should be able to infer redundancy and uniqueness given only access to p(x 1 , y) and p(x 2 , y), and therefore they should only depend on q ∈ ∆ p . Synergy is the only term that should depend on the coupling p(x 1 , x 2 ), and this is reflected in (6.2) depending on the full p distribution. This definition enjoys several useful properties in line with intuition, as we will see in comparison with related frameworks for interactions below  [59] .",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Related Frameworks For Feature Interactions",
      "text": "Information-theoretic definitions: Perhaps the first measure of redundancy in machine learning is co-training  [44, 65, 116] , where 2 variables are redundant if they are conditionally independent given the task: I(X 1 ; X 2 |Y ) = 0. As a result, redundancy can be measured by I(X 1 ; X 2 ; Y ). The same definition of redundancy is used in multi-view learning  [564, 606, 609]  which further define I(X 1 ; Y |X 2 ) and I(X 2 ; Y |X 1 ) as unique information in X 1 , X 2 . However, I(X 1 ; X 2 ; Y ) can be both positive and negative  [280] . PID resolves this by separating R and S such that R -S = I(X 1 ; X 2 ; Y ), identifying that prior measures confound redundancy and synergy. This crucially provides an explanation for the distinction between mediation, where one feature conveys the information already in another (i.e., R > S), versus moderation, where one feature affects the relationship of other features (i.e., S > R)  [48, 196] . Furthermore, if I(X 1 ; X 2 ; Y ) = 0 then existing frameworks are unable to distinguish between positive R and S canceling each other out. Statistical measures: Other approaches have studied interaction measures via statistical measures, such as redundancy via distance between prediction logits using either feature  [411] , statistical distribution tests on input features  [36, 704] , or via human annotations  [515] . However, it is unclear how to extend these definitions to uniqueness and synergy while remaining on the same standardized scale like PID provides. Also of interest are notions of redundant and synergistic interactions in human and animal communication  [175, 469, 470, 515] , which we aim to formalize.\n\nModel-based methods: Prior research has formalized definitions of non-additive interactions  [180]  to quantify their presence  [235, 563, 620, 621]  in trained models, or used Shapley values on trained features to measure interactions  [272] . Parallel research has also focused on qualitative visualizations of real-world multimodal datasets and models, such as DIME  [396] , M2Lens  [655] , and MultiViz  [374] .",
      "page_start": 66,
      "page_end": 67
    },
    {
      "section_name": "Scalable Estimators For Pid",
      "text": "PID as a framework for multimodality: Our core insight is that PID provides a formal framework to understand both the nature and degree of interactions involved when two features X 1 and X 2 are used for task Y . The nature of interactions is afforded by a precise decomposition into redundant, unique, and synergistic interactions, and the degree of interactions is afforded by a standardized unit of measure (bits). However, computing PID is a considerable challenge, since it involves optimization over ∆ p and estimating information-theoretic measures. Up to now, analytic approximations of these quantities were only possible for discrete and small support  [59, 210, 665]  or continuous but low-dimensional variables  [460, 494, 666] . Leveraging ideas in representation learning, Sections 3.2.1 and 3.2.2 are our first technical contributions enabling scalable estimation of PID for high-dimensional distributions. The first, CVX, is exact, based on convex optimization, and is able to scale to problems where |X i | and |Y| are around 100. The second, BATCH, is an approximation based on sampling, which enables us to handle large or even continuous supports for X i and Y . Applying these estimators in Section 3.3, we show that PID provides a path towards understanding the nature of interactions in datasets and those learned by different models, and principled approaches for model selection.",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Cvx: Dataset-Level Optimization",
      "text": "Our first estimator, CVX, directly compute PID from its definitions using convex programming. Crucially, Bertschinger et al.  [59]  show that the solution to the max-entropy optimization problem: q * = arg max q∈∆p H q (Y |X 1 , X 2 ) equivalently solves (3.1)-(6.2). When X i and Y are small and discrete, we can represent all valid distributions q(x 1 , x 2 , y) as a set of tensors\n\nThe problem then boils down to optimizing over valid tensors Q ∈ ∆ p that match the marginals p(x i , y).\n\nGiven a tensor Q representing q, our objective is the concave function H q (Y |X 1 , X 2 ). While Bertschinger et al.  [59]  report that direct optimization is numerically difficult as routines such as Mathematica's FINDMINIMUM do not exploit convexity, we overcome this by rewriting conditional entropy as a KL-divergence  [201] ,\n\nwhere q is an auxiliary product density of q(x 1 , x 2 ) ⋅ 1 |Y| enforced using linear constraints: q(x 1 , x 2 , y) = q(x 1 , x 2 )/|Y|. Finally, optimizing over Q ∈ ∆ p that match the marginals can also be enforced through linear constraints: the 3D-tensor Q summed over the second dimension gives q(x 1 , y) and summed over the first dimension gives q(x 2 , y), yielding the final optimization problem:\n\nThe KL-divergence objective is recognized as convex, allowing the use of conic solvers such as SCS  [451] , ECOS  [152] , and MOSEK  [30] . Plugging q * into (3.1)-(6.2) yields the desired PID.\n\nPre-processing via feature binning: In practice, X 1 and X 2 often take continuous rather than discrete values. Thus, Q is no longer a finite dimensional polytope. We work around this by histogramming each X i , thereby estimating the continuous joint density by discrete distributions with finite support. To make our discretization as data-independent as possible, we focus on a prespecified number of fixed-width bins (except for the first and last). For example, it is known that with a fixed number of samples, making the width of bins arbitrarily small will cause KL estimates to diverge. It is known that the number of bins should grow sub-linearly with the number of samples. For example, Rice  [511]  suggest setting the number of bins to be the cubed-root of number of samples. We propose BATCH, a scalable estimator for PID over high-dimensional continuous distributions. BATCH parameterizes q using a matrix A learned by neural networks such that mutual information objectives over q can be optimized via gradient-based approaches over minibatches. Marginal constraints q ∈ ∆ p are enforced through a variant of the Sinkhorn-Knopp algorithm on A.",
      "page_start": 67,
      "page_end": 68
    },
    {
      "section_name": "Batch: Batch-Level Amortization",
      "text": "We now present BATCH, our next estimator that is suitable for large datasets where X i is highdimensional and continuous (|Y| remains finite). To estimate PID given a sampled dataset D = {(x\n\n2 , y (j) )} of size n, we propose an end-to-end model parameterizing marginalmatching joint distributions in ∆ p and a training objective whose solution returns approximate PID values.\n\nSimplified algorithm sketch: Our goal, loosely speaking, is to optimize q ∈ ∆ p for objective (3.1) through an approximation using neural networks instead of exact optimization. We show an overview in Figure  3 .2. To explain our approach, we first describe (1) how we parameterize q using neural networks such that it can be learned via gradient-based approaches, (2) how we ensure the marginal constraints q ∈ ∆ p through a variant of the Sinkhorn-Knopp algorithm, and finally (3) how to scale this up over small subsampled batches from large multimodal datasets.\n\nParameterization using neural networks: The space of joint distributions ∆ is often too large to explicitly specify. To tackle this, we implicitly parameterize each distribution q ∈ ∆ using a neural network f ϕ that takes in batches of modalities X 1 ∈ X n 1 , X 2 ∈ X n 2 and the label Y ∈ Y n before returning a matrix A ∈ R n×n×|Y| representing an (unnormalized) joint distribution q, i.e., we want\n\nj], y) for each y ∈ Y. In practice, f ϕ is implemented via a pair of encoders f ϕ(1) and f ϕ(2) that learn modality representations, before an outer product to learn joint relationships A y = exp(f ϕ(1) (X 1 , y)f ϕ(2) (X 2 , y) ⊺ ) for each y, yielding the desired n × n × |Y| joint distribution. As a result, optimizing over q can be performed via optimizing over parameters ϕ.\n\nRespecting the marginal constraints: How do we make sure the q's learned by the network satisfies the marginal constraints (i.e., q ∈ ∆ p )? We use an unrolled version of Sinkhorn's algorithm  [127]  which projects A onto ∆ p by iteratively normalizing A's rows and columns to sum to 1 and rescaling to satisfy the marginals p(x i , y). However, p(x i , y) is not easy to estimate for high-dimensional continuous x i 's. In response, we first expand p(x i , y) into p(y|x i ) and p(x i ) using Bayes' rule. Since A was constructed by samples x i from the dataset, the rows and columns of A are already distributed according to p(x 1 ) and p(x 2 ) respectively. This means that it suffices to approximate p(y|x i ) with unimodal classifiers p(y|x i ) parameterized by neural networks and trained separately, before using Sinkhorn's algorithm to normalize each row to p(y|x 1 ) and each column to p(y|x 2 ). Objective: We choose the objective I q (X 1 ; X 2 ; Y ), which equivalently solves the optimization problems in the other PID terms  [59] . Given matrix A representing q(x 1 , x 2 , y), the objective can be computed in closed form through appropriate summation across dimensions in A to obtain q(x i ), q(x 1 , x 2 ), q(x i |y), and q(x 1 , x 2 |y) and plugging into I q(X 1 ; X 2 ; Y ) = I q(X 1 ; X 2 ) -I q(X 1 ; X 2 |Y ). We maximize I q(X 1 ; X 2 ; Y ) by updating parameters ϕ via gradient-based methods. Overall, each gradient step involves computing q = SINKHORN p(A), and updating ϕ to maximize (3.1) under q. Since Sinkhorn's algorithm is differentiable, gradients can be backpropagated end-to-end.\n\nApproximation with small subsampled batches: Finally, to scale this up to large multimodal datasets where the full q may be too large to store, we approximate q with small subsampled batches: for each gradient iteration t, the network f ϕ now takes in a batch of m ≪ n datapoints sampled from D and returns A ∈ R m×m×|Y| for the subsampled points. We perform Sinkhorn's algorithm on A and a gradient step on ϕ as above, as if D t was the full dataset (i.e., mini-batch gradient descent). While it is challenging to obtain full-batch gradients since computing the full A is intractable, we found our approach to work in practice for large m. Our approach can also be informally viewed as performing amortized optimization  [23]  by using ϕ to implicitly share information about the full batch using subsampled batches. Upon convergence of ϕ, we extract PID by plugging q into (3.1)-(6.2).\n\nImplementation details such as the network architecture of f , approximation of objective (3.1) via sampling from q, and estimation of I q({X 1 , X 2 }; Y ) from learned q are included in the full version of the paper  [371] .",
      "page_start": 68,
      "page_end": 68
    },
    {
      "section_name": "Evaluation And Applications Of Pid In Multimodal Learning",
      "text": "We design experiments to (1) understand PID on synthetic data, (2) quantify real-world multimodal benchmarks, (3) understand the interactions captured by multimodal models, (4) perform model selection across different model families, and (5) applications on novel real-world tasks.",
      "page_start": 69,
      "page_end": 69
    },
    {
      "section_name": "Validating Pid Estimates On Synthetic Data",
      "text": "Our first goal is to evaluate the accuracy of our proposed estimators with respect to the ground truth (if it can be computed) or human judgment (for cases where the ground truth cannot be readily obtained). We start with a suite of datasets spanning both synthetic and real-world distributions.\n\nExact 0.31 0 0 0.5 0.31 0 0 0.5 0 0 0 1 CVX 0.31 0 0 0.5 0.31 0 0 0.5 0 0 0 1 BATCH 0.31 0 0 0.5 0.31 0 0 0.5 0 0 0 1 Synthetic bitwise features: We sample from a binary bitwise distribution:\n\nEach bitwise operator's PID can be solved exactly when the x i 's and labels are discrete and low-dimensional  [59] . Compared to the ground truth in Bertschinger et al.  [59] , both our estimators exactly recover the correct PID values (Table  3 .1).\n\nGaussian Mixture Models (GMM): Consider a GMM, where X 1 , X 2 ∈ R and the label Y ∈ {-1, +1}, comprising two equally weighted standard multivariate Gaussians centered at ±µ, where µ ∈ R 2 , i.e., Y ∼ Bernoulli(1/2), (X 1 , X 2 )|Y = y ∼ N (y ⋅ µ, I). PID was estimated by sampling 1e6 points, histogramming them into 50 bins spanning [-5, +5] to give p, and then applying the CVX estimator. We term this PID-Cartesian. We also compute PID-Polar, which are PID computed using polar coordinates, (r, θ). We use a variant where the angle θ is given by the arctangent with principal values [0, π] and the length r ∈ R could be negative. θ specifies a line (through the origin), and r tells us where along the line the datapoint lies on.\n\nResults: We consider ||µ|| 2 ∈ {1.0, 2.0}, where for each ||µ|| 2 , we vary the angle ∠µ that µ makes with the horizontal axis. Our computed PID is presented in Figure  3 .3. Overall, we find that the PID matches what we expect from intuition. For Cartesian, unique information dominates when the angle goes to 0 or π/2 -if centroids share a coordinate, then observing that coordinate yields no information about y. Conversely, synergy and redundancy peak at π/4. Interestingly, synergy seems to be independent of ||µ|| 2 . For Polar, redundancy is 0. Furthermore, θ contains no unique information, since θ shows nothing about y unless we know r (in particular, its sign). When the angle goes to π/2, almost all information is unique in r. The distinctions between Cartesian and Polar highlight how different representations of data can exhibit wildly different PID values, even if total information is the same.\n\nSynthetic generative model: We begin with a set of latent vectors\n\nThe label y is generated as a function of (1) only z c , in which case we expect complete redundancy, (2) only z 1 or z 2 which represents complete uniqueness, (3) a combination of z 1 and z 2 representing complete synergy, or (4) arbitrary ratios of each of the above with z * i representing half of the dimensions from z i and therefore half of each interaction. In total, Table  3 .2 shows the 10 synthetic datasets we generated: 4 specialized datasets D I , I ∈ {R, U 1 , U 2 , S} where y only depends on one interaction, and 6 mixed datasets with varying interaction ratios. We also report the ground-truth interactions as defined by the label-generating process and the total capturable information using the bound in Feder and Merhav  [172] , which relates the accuracy of the best model on these datasets with the mutual information between the inputs to the label. Since the test\n\nR U 1 U 2 S CVX 0.16 0 0 0.05 0 0.16 0 0.05 0 0 0.17 0.05 0.07 0 0.01 0.14 0.04 0.01 0 0.07 BATCH 0.29 0.02 0.02 0 0 0.30 0 0 0 0 0.30 0 0.11 0.02 0.02 0.15 0.06 0.01 0.01 0.06 Truth 0.58 0 0 0 0 0.56 0 0 0 0 0.54 0 0 0 0 0.56 0.13 0 0 0.27\n\nR U 1 U 2 S CVX 0.04 0.06 0 0.07 0.07 0 0 0.12 0.1 0 0.01 0.07 0.03 0 0.04 0.05 0.1 0 0.04 0.05 BATCH 0.04 0.09 0 0.06 0.11 0.02 0.02 0.10 0.11 0.02 0.02 0.05 0.07 0 0.06 0 0.19 0 0.06 0 Truth 0 0.25 0 0.25 0.18 0 0 0.36 0.22 0 0 0.22 0.21 0 0.21 0 0.34 0 0.17 0 accuracies for Table  3 .2 datasets range from 67-75%, this corresponds to total MI of 0.42 -0.59 bits.\n\nResults: From Table  3 .2, both CVX and BATCH agree in relative PID values, correctly assigning the predominant interaction type and interactions with minimal presence consistent with the ground-truth based on data generation. For example, D R has the highest R value, and when the ratio of z 1 increases, U 1 increases from 0.01\n\n). We also note some interesting observations due to the random noise in label generation, such as the non-zero synergy measure of datasets such as D R , D U 1 , D U 2 whose labels do not depend on synergy.",
      "page_start": 70,
      "page_end": 70
    },
    {
      "section_name": "Quantifying Real-World Multimodal Benchmarks",
      "text": "We now apply these estimators to quantify the interactions in real-world multimodal datasets.\n\nReal-world multimodal data setup: We use a large collection of real-world datasets in MultiBench  [367]  which test multimodal fusion of different input signals (including images, video, audio, text, time-series, sets, and tables) for different tasks (predicting humor, sentiment, emotions, mortality rate, ICD-9 codes, image-captions, human activities, digits, and design interfaces). We also include experiments on question-answering (Visual Question Answering 2.0  [29, 206]  and CLEVR  [289] ) which test grounding of language into the visual domain. For the 4 datasets (top row of Table  3 .3) involving images and text where modality features are available and readily clustered, we apply the CVX estimator on top of discrete clusters. For the remaining 4 datasets (bottom row of Table  3 .3) with video, audio, and medical time-series modalities, clustering is not easy, so we use the end-to-end BATCH estimator.\n\nHuman judgment of interactions: Real-world multimodal datasets do not have reference PID values, and exact PID computation is impossible due to continuous data. We therefore use human judgment as a reference. We design a new annotation scheme where we show both modalities and the label and ask each annotator to annotate the degree of redundancy, uniqueness, and synergy on a scale of 0-5, alongside their confidence in their answers on a scale of 0-5. We give 50 datapoints from each dataset (except MIMIC and ENRICO which require specialized knowledge) to 3 annotators each. We show a sample user interface and annotation procedures in the full paper  [371] , and also provide an in-depth study of how humans annotate multimodal Table  3 .3: Estimating PID on real-world MultiBench  [367]  datasets. Many of the estimated interactions align well with human judgement as well as unimodal performance.\n\nR U 1 U 2 S BATCH 0.26 0.49 0.03 0.04 0.03 0.04 0.01 0.08 0.14 0.01 0.01 0.30 0.05 0.17 0 0.01 Human 0.32 0.20 0.15 0.15 0.04 0.05 0.03 0.04 0.13 0.17 0.04 0.16 ---interactions in a subsequent follow-up work  [372] .\n\nResults on multimodal fusion: From Table  3 .3, we find that different datasets do require different interactions. Some interesting observations: (1) all pairs of modalities on MUSTARD sarcasm detection show high synergy values, which aligns with intuition since sarcasm is often due to a contradiction between what is expressed in language and speech, (2) uniqueness values are strongly correlated with unimodal performance (e.g., modality 1 in AV-MNIST and MIMIC), (3) datasets with high synergy do indeed benefit from interaction modeling as also seen in prior work (e.g., MUSTARD, UR-FUNNY)  [83, 225] , and (4) conversely datasets with low synergy are those where unimodal performance is relatively strong (e.g., MIMIC)  [367] .\n\nResults on QA: We observe very high synergy values as shown in Table  3 .3 consistent with prior work studying how these datasets were balanced (e.g., VQA 2.0 having different images for the same question such that the answer can only be obtained through synergy)  [206]  and that models trained on these datasets require non-additive interactions  [235] . CLEVR has a higher proportion of synergy than VQA 2.0 (83% versus 75%): indeed, CLEVR is a more balanced dataset where the answer strictly depends on both the question and image with a lower likelihood of unimodal biases.\n\nComparisons with human judgment: For human judgment, we cannot ask humans to give a score in bits, so it is on a completely different scale (0-5 scale). To put them on the same scale, we normalize the human ratings such that the sum of human interactions is equal to the sum of PID estimates. The resulting comparisons are in Table  3 .3, and we find that the human-annotated interactions overall align with estimated PID: the highest values are the same for 4 datasets: both explain highest synergy on VQA and CLEVR, image (U 1 ) being the dominant modality in AV-MNIST, and language (U 1 ) being the dominant modality in MOSEI. Overall, the Krippendorff's alpha for inter-annotator agreement is high (0.72 for R, 0.68 for U 1 , 0.70 for U 2 , 0.72 for S) and the average confidence scores are also high (4.36/5 for R, 4.35/5 for U 1 , 4.27/5 for U 2 , 4.46/5 for S), indicating that the human-annotated results are reliable. For the remaining two datasets (UR-FUNNY and MUSTARD), estimated PID matches the second-highest human-annotated interaction. We believe this is because there is some annotator subjectivity in interpreting whether sentiment, humor, and sarcasm are present in language only (U 1 ) or when contextualizing both language and video (S), resulting in cases of low annotator agreement in U 1 and S: -0.14, -0.03 for UR-FUNNY and -0.08, -0.04 for MUSTARD.\n\nComparisons with other interaction measures: Our framework allows for easy general- ization to other interaction definitions: we also implemented 3 information theoretic measures I-min  [663] , WMS  [91] , and CI  [447] . These results are included in the full paper  [371] , where we explain the limitations of these methods as compared to PID, such as over-and under-estimation, and potential negative estimation  [210] . These are critical problems with the application of information theory for shared I(X 1 ; X 2 ; Y ) and unique information I(X 1 ; Y |X 2 ), I(X 2 ; Y |X 1 ) often quoted in the co-training  [44, 65]  and multi-view learning  [564, 606, 609]  literature. We also tried 3 non-info theory measures: Shapley values  [395] , Integrated gradients (IG)  [580] , and CCA  [27] , which are based on quantifying interactions captured by a multimodal model. Our work is fundamentally different in that interactions are properties of data before training any models.",
      "page_start": 71,
      "page_end": 71
    },
    {
      "section_name": "Quantifying Multimodal Model Predictions",
      "text": "We now shift our focus to quantifying multimodal models. Do different multimodal models learn different interactions? A better understanding of the types of interactions that our current models struggle to capture can provide new insights into improving these models.\n\nSetup: For each dataset, we train a suite of models on the train set D train and apply it to the validation set D val , yielding a predicted dataset D pred = {(x 1 , x 2 , ŷ) ∈ D val }. Running PID on D pred summarizes the interactions that the model captures. We categorize and implement a comprehensive suite of models (spanning representation fusion at different feature levels, types of interaction inductive biases, and training objectives) that have been previously motivated to capture redundant, unique, and synergistic interactions.\n\nResults: We show results in Table  3 .4 and highlight the following observations: General observations: We first observe that model PID values are consistently higher than dataset PID. The sum of model PID is also a good indicator of test performance, which agrees with their formal definition since their sum is equal to I({X 1 , X 2 }; Y ), the total task-relevant information.\n\nOn redundancy: Several methods succeed in capturing redundancy, with an overall average of R = 0.41 ± 0.11 and accuracy of 73.0 ± 2.0% on redundancy-specialized datasets. Additive, agreement, and alignment-based methods are particularly strong, and we do expect them to capture redundant shared information  [147, 498] . Methods based on tensor fusion (synergybased), including lower-order interactions, and adding reconstruction objectives (unique-based) also capture redundancy. We find high correlation (ρ = 0.8) between the performance drop when X i is missing and the model's U i value: high U i coincides with large performance drops (red), but low U i can also lead to performance drops. The latter can be further explained by large S so X i is necessary (green).\n\nOn uniqueness: Uniqueness is harder to capture than redundancy, with an average of U = 0.37 ± 0.14. Redundancybased methods like additive and agreement do poorly on uniqueness, while those designed for uniqueness (lowerorder interactions  [713]  and modality reconstruction objectives  [615] ) do well, with on average U = 0.55 and 73.0% accuracy on uniqueness datasets.\n\nOn synergy: Synergy is the hardest to capture, with an average score of only S = 0.21 ± 0.10. Some of the strong methods are tensor fusion  [182] , tensors with lower-order interactions  [713] , modality reconstruction  [615] , and multimodal transformer  [682] , which achieve around S = 0.30, acc = 73.0%. Additive, agreement, and element-wise interactions do not seem to capture synergy well.\n\nOn robustness: Finally, we also show connections between PID and model performance in the presence of missing modalities. We find high correlation (ρ = 0.8) between the performance drop when X i is missing and the model's U i value. Inspecting Figure  3 .4, we find that the implication only holds in one direction: high U i coincides with large performance drops (in red), but low U i can also lead to performance drops (in green). The latter can be further explained by the presence of large S values: when X i is missing, synergy can no longer be learned which affects performance. For the subset of points when U i ≤ 0.05, the correlation between S and performance drop is ρ = 0.73 (in contrast, the correlation for R is ρ = 0.01).  Results: Our key finding is that PID agreement scores α(f, D) correlate (ρ = 0.81) with model accuracy across all 10 synthetic datasets as illustrated in Figure  3 .5. This shows that PID agreement can be a useful proxy for model performance. For the specialized datasets, we find that the correlation between α I and D I is 0.96 for R, 0.86 for U , and 0.91 for S, and negatively correlated with other specialized datasets. For mixed datasets with roughly equal ratios of each interaction, the measures that correlate most with performance are α R (ρ = 0.82) and α S (ρ = 0.89); datasets with relatively higher redundancy see ρ = 0.89 for α R ; those with higher uniqueness have α U 1 and α U 2 correlate ρ = 0.92 and ρ = 0.85; those with higher synergy increases the correlation of α S to ρ = 0.97.",
      "page_start": 73,
      "page_end": 73
    },
    {
      "section_name": "Pid Agreement And Model Selection",
      "text": "Using these observations, our final experiment is model selection: can we choose the most appropriate model to tackle the interactions required for a dataset?\n\nSetup: Given a new dataset D, we first compute its difference in normalized PID values with respect to D ′ among our suite of 10 synthetic datasets, s(D, D ′ ) = ∑ I∈{R,U 1 ,U 2 ,S} | ÎD -ÎD ′ |, to rank the dataset D * with the most similar interactions, and return the top-3 performing models on D * . In other words, we select models that best capture interactions that are of similar nature and degree as those in D. We emphasize that even though we restrict dataset and model search to synthetic datasets, we evaluate model selection on real-world datasets and find that it generalizes to the real world.\n\nResults: We test our selected models on 5 new synthetic datasets with different PID ratios and 6 real-world datasets, summarizing results in Table  3 .5. We find that the top 3 chosen models achieve 95% -100% of the best-performing model accuracy, and > 98.5% for all datasets except 95.2% on MUSTARD. For example, UR-FUNNY and MUSTARD have the highest synergy (S = 0.13, S = 0.3) and indeed transformers and higher-order interactions are helpful (MULT: 65%, MI: 61%, TENSOR: 60%). ENRICO has the highest R = 0.73 and U 2 = 0.53, and methods for redundant and unique interactions perform best (LOWER: 52%, ALIGN: 52%, AGREE: 51%). MIMIC has the highest U 1 = 0.17, and unimodal models are mostly sufficient  [367] .",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Real-World Applications",
      "text": "Finally, we apply PID to 3 real-world case studies: pathology, mental health, and robotic perception.\n\nCase Study 1: Computational pathology. Cancer prognostication is a challenging task in anatomic pathology that requires integration of whole-slide imaging (WSI) and molecular features for patient stratification  [92, 383, 425] . We use The Cancer Genome Atlas (TCGA), a large public data consortium of paired WSI, molecular, and survival information  [608, 661] , including modalities: (1) pre-extracted histology image features from diagnostic WSIs and (2) bulk gene mutation status, copy number variation, and RNA-Seq abundance values. We evaluate on two cancer datasets in TCGA, lower-grade glioma (LGG  [440] , n = 479) and pancreatic adenocarcinoma (PAAD  [504] , n = 209).\n\nResults: In TCGA-LGG, most PID measures were near-zero except U 2 = 0.06 for genomic features, which indicates that genomics is the only modality containing task-relevant information. This conclusion corroborates with the high performance of unimodal-genomic and multimodal models in Chen et al.  [102] , while unimodal-pathology performance was low. In TCGA-PAAD, the uniqueness in pathology and genomic features was less than synergy (U 1 = 0.06, and U 2 = 0.08 and S = 0.15), which also match the improvement of using multimodal models that capture synergy.\n\nCase Study 2: Mental health. Suicide is the second leading cause of death among adolescents  [84] . Intensive monitoring of behaviors via adolescents' frequent use of smartphones may shed new light on the early risk of suicidal ideations  [200, 433] , since smartphones provide rich behavioral markers  [366] . We used a dataset, MAPS, of mobile behaviors from high-risk consenting adolescent populations (approved by IRB). Passive sensing data is collected from each participant's smartphone across 6 months. The modalities include (1) text entered by the user represented as a bag of top 1000 words, (2) keystrokes that record the exact timing and duration of each typed character, and (3) mobile applications used per day as a bag of 137 apps. Every morning, users self-report their daily mood, which we discretized into -1, 0, +1. In total, MAPS has 844 samples from 17 participants.\n\nResults: We first experiment with MAPS T,K using text and keystroke features. PID measures show that MAPS T,K has high synergy (0.40), some redundancy (0.12), and low uniqueness (0.04). We found the purely synergistic dataset D S has the most similar interactions and the suggested models LOWER, REC, and TENSOR that work best on D S were indeed the top 3 best-performing models on MAPS T,K , indicating that model selection is effective. Model selection also retrieves the best-performing model on MAPS T,A using text and app usage features.\n\nCase Study 3: Robotic Perception. MuJoCo PUSH  [334]  is a contact-rich planar pushing task in MuJoCo  [607] , where a 7-DoF Panda Franka robot is pushing a circular puck with its end-effector in simulation. The dataset consists of 1000 trajectories with 250 steps sampled at 10Hertz. The multimodal inputs are gray-scaled images from an RGB camera, force and binary contact information from a force/torque sensor, and the 3D position of the robot end-effector. We estimate the 2D position of the unknown object on a table surface while the robot intermittently interacts with it.\n\nResults: We find that BATCH predicts U 1 = 1.79 as the highest PID value, which aligns with our observation that image is the best unimodal predictor. Comparing both estimators, CVX underestimates U 1 and R since the high-dimensional time-series modality cannot be easily described by clusters without losing information. In addition, both estimators predict a low U 2 value but attribute high R, implying that a multimodal model with higher-order interactions would not be much better than unimodal models. Indeed, we observe no difference in performance between these two.",
      "page_start": 75,
      "page_end": 75
    },
    {
      "section_name": "Conclusion",
      "text": "Our work aims to quantify the nature and degree of feature interactions by proposing scalable estimators for redundancy, uniqueness, and synergy suitable for high-dimensional heterogeneous datasets. Through comprehensive experiments and real-world applications, we demonstrate the utility of our proposed framework in dataset quantification, model quantification, and model selection. We are aware of some potential limitations: 1. These estimators only approximate real interactions due to cluster preprocessing or unimodal models, which naturally introduce optimization and generalization errors. We expect progress in density estimators, generative models, and unimodal classifiers to address these problems. 2. It is harder to quantify interactions for certain datasets, such as ENRICO which displays all interactions which makes it difficult to distinguish between R and S or U and S. 3. Finally, there exist challenges in quantifying interactions since the data generation process is never known for real-world datasets, so we have to resort to human judgment, other automatic measures, and downstream tasks such as estimating model performance and model selection.\n\nFuture work can leverage PID for targeted dataset creation, representation learning optimized for PID values, and applications of information theory to higher-dimensional data. More broadly, there are several exciting directions in investigating more applications of multivariate information theory in modeling feature interactions, predicting multimodal performance, and other tasks involving feature interactions such as privacy-preserving and fair representation learning from high-dimensional data  [161, 219] . Being able to provide guarantees for fairness and privacypreserving learning can be particularly impactful.",
      "page_start": 76,
      "page_end": 77
    },
    {
      "section_name": "Chapter 4 Factorized Learning Of Multimodal Interactions 4.1 Introduction",
      "text": "Using the mathematical foundation of multimodal interactions that we just presented, we now seek to learn representations from multimodal data that are suitable in capturing each of these interactions. Learning representations from different modalities is a central paradigm in machine learning  [375] . Today, a popular learning method is to first pre-train general representations on unlabeled multimodal data before fine-tuning on task-specific labels  [72, 370, 375, 390] . These current multimodal pre-training approaches have largely been inherited from prior work in multi-view learning  [103, 453]  that exploit a critical assumption of multi-view redundancy: the property that shared information between modalities is almost exactly what is relevant for downstream tasks  [564, 609, 617] . When this assumption holds, approaches based on contrastive pre-training to capture shared information  [103, 300, 498, 606] , followed by fine-tuning to keep task-relevant shared information  [617] , have seen successful applications in learning from images and captions  [498] , video and audio  [31] , speech and transcribed text  [453] , and instructions and actions  [168] . However, our paper studies two fundamental limitations in the application of contrastive learning (CL) to learn multimodal interactions in real-world settings 1. Low shared information relevant to tasks: There exists a wide range of multimodal tasks involving small amounts of shared information, such as between cartoon images and figurative captions (i.e., not literal but metaphoric or idiomatic descriptions of the images  [410, 701] ). In these situations, standard multimodal CL will only receive a small percentage of information from the learned representations and struggle to learn the desired task-relevant information. 2. High unique information relevant to tasks: Many real-world modalities can provide unique information not present in other modalities. Examples include healthcare with medical sensors or robotics with force sensors  [367, 371] . Standard CL will discard task-relevant unique information, leading to poor downstream performance. We refer the reader to Figure  9 .1 for a visual depiction and experimental results showing the performance drop of CL in these two settings of low shared information and high unique information.\n\nIn light of these limitations, how can we design suitable multimodal learning objectives that\n\nas task-relevant unique information. Right: On controllable datasets with varying ratios of S, U 1 , and U 2 , standard CL captures S but struggles when there is more U 1 and U 2 . Our FACTORCL approach maintains best performance, whereas SimCLR  [103]  and SupCon  [300]  see performance drops as unique information increases, and Cross+Self  [258, 278, 337, 710]  recovers in fully unique settings but suffers at other ratios.\n\nwork beyond multi-view redundancy? In this paper, starting from the first principles in information theory, we provide formal definitions of shared and unique information via conditional mutual information and propose an approach, FACTORIZED CONTRASTIVE LEARNING (FACTORCL for short), to learn these multimodal representations beyond multi-view redundancy using three key ideas. The first idea is to explicitly factorize shared and unique representations. The second idea is to capture task-relevant information via maximizing lower bounds on MI and remove task-irrelevant information via minimizing upper bounds on MI, resulting in representations with sufficient and necessary information content. Finally, a notion of task relevance without explicit labels in the self-supervised setting is achieved by leveraging multimodal augmentations.\n\nExperimentally, we evaluate the effectiveness of FACTORCL on a suite of synthetic datasets and large-scale real-world multimodal benchmarks involving images and figurative language  [701] , prediction of human sentiment  [711] , emotions  [718] , humor  [225] , and sarcasm  [83] , as well as patient disease and mortality prediction from health indicators and sensor readings  [286] , achieving new state-of-the-art performance on six datasets. Overall, we summarize our key technical contributions here:\n\n1. A new analysis of contrastive learning performance showing that standard multimodal CL fails to capture task-relevant unique information under low shared or high unique information cases.",
      "page_start": 78,
      "page_end": 79
    },
    {
      "section_name": "A New Contrastive Learning Algorithm Called Factorcl:",
      "text": "(a) FACTORCL factorizes task-relevant information into shared and unique information, expanding contrastive learning to better handle low shared or high unique information. (b) FACTORCL optimizes shared and unique information separately, by removing taskirrelevant information via MI upper bounds and capturing task-relevant information via lower bounds, yielding optimal task-relevant representations. (c) FACTORCL leverages multimodal augmentations to approximate task-relevant information, enabling self-supervised learning from our proposed FACTORCL.",
      "page_start": 79,
      "page_end": 79
    },
    {
      "section_name": "Analysis Of Multi-View Contrastive Learning",
      "text": "We begin by formalizing definitions of four types of information: shared, unique, task-relevant, and task-irrelevant information in multimodal data. To formalize the learning setting, we assume there exist two modalities expressed as random variables X 1 and X 2 with outcomes x 1 and x 2 , and a task with the random variable Y and outcome y. We denote X -i as the other modality where appropriate.\n\nShared and unique information: We formalize shared and unique information by decomposing the total multimodal information I(X 1 , X 2 ; Y ) into three conditional mutual information (MI) terms:\n\nwhere I(X 1 , X 2 ; Y ) = ∫ p(x 1 , x 2 , y) log p(x 1 ,x 2 ,y) p(x 1 ,x 2 )p(y) dx 1 dx 2 dy is the total MI between the joint random variable X 1 , X 2 and the task Y , S = I(X 1 ; X\n\np(x 1 |y)p(x 2 |y) dx 1 dx 2 dy is the task-irrelevant shared information, and\n\nLimitations of CL: Current approaches for CL maximize mutual information I(X 1 ; X 2 ) (and subsequently task-relevant shared information I(X 1 ; X 2 ; Y ) during supervised fine-tuning), without modeling unique information. These methods generally learn a pair of representations  [609, 617] ,\n\nFor example, Z 1 could encode images X 1 and Z 2 encodes text X 2 via maximizing a lower bound on I(X 1 ; X 2 ) using the NCE objective  [453] . The NCE objective falls into a broader class of contrastive learning methods  [103, 107, 229, 300, 498]  that model the ratio between joint densities p(x 1 , x 2 ) and product of marginal densities p(x 1 )p(x 2 ) using positive and negative samples  [444, 458, 488, 622, 670]  or probabilistic classifiers  [430, 618] . It has been shown that contrastive learning works well under the assumption of multi-view redundancy  [39, 240, 564, 605] :\n\nIn other words, the task-relevant information in data is mostly shared across both views and the unique information is at most a small ϵ. From a representation perspective, Tian et al.  [606]  further introduces the assumption that the optimal representation is minimal and sufficient, where all learned task-relevant information is shared information: I(Z 1 ; Y |X 2 ) = I(Z 2 ; Y |X 1 ) = 0. While the multi-view redundancy is certainly true for particular types of multimodal distributions, it crucially ignores settings that display multi-view non-redundancy and unique information can be important, such as when health indicators, medical sensors, and robotic visual or force sensors each provide unique information not present in other modalities  [367, 371] . Definition 2. (Multi-view non-redundancy) ∃ϵ > 0 such that I(X 1 ; Y |X 2 ) > ϵ or I(X 2 ; Y |X 1 ) > ϵ.\n\nUnder multi-view non-redundancy, we show that standard CL only receives a weak training signal since it can only maximize a lower bound on shared information I(X 1 ; X 2 ), and struggles to learn task-relevant unique information. We formalize this intuition with the following statement: Theorem 1. (Suboptimality of standard CL) When there is multi-view non-redundancy as in Definition 2, given optimal representations {Z 1 , Z 2 } that satisfy Eq.(4.2 and I(Z 1 ; Y |X 2 ) = I(Z 2 ; Y |X 1 ) = 0  [606] , we have that\n\nCorrespondingly, the Bayes error rate\n\na downstream task Y is given by:\n\nWe include proofs and a detailed discussion of the assumptions in the full paper  [373] . Based on Eq.(4.3), I(Z 1 , Z 2 ; Y ) decreases with higher task-relevant unique information I(X 1 ; Y |X 2 ) and I(X 2 ; Y |X 1 ); we call this the difference I(X 1 , X 2 ; Y ) -I(Z 1 , Z 2 ; Y ) the uniqueness gap. The uniqueness gap measures the loss in task-relevant information between the input and encoded representation: as task-relevant unique information grows, the uniqueness gap increases. In addition, I(Z 1 , Z 2 ; Y ) also drops with lower I(X 1 ; X 2 ) (i.e., two modalities sharing little information to begin with), or with higher I(X 1 ; X 2 |Y ) (i.e., when the shared information is mostly task-irrelevant). Similarly, in Eq.(4.5), the Bayes error rate of using {Z 1 , Z 2 } for prediction is directly related to the task-relevant information in {Z 1 , Z 2 }: error on the downstream task increases with higher unique information and lower shared information.",
      "page_start": 80,
      "page_end": 81
    },
    {
      "section_name": "Factorized Contrastive Learning",
      "text": "We now present a suite of new CL objectives that alleviate the challenges above and work at all ranges of shared and unique information. At a high level, we aim to learn a set of factorized representations Z S 1 , Z S 2 , Z U 1 , Z U 2 representing task-relevant information in X 1 shared with X 2 , in X 2 shared with X 1 , unique to X 1 , and unique to X 2 respectively. As common in practice  [498, 606] , we define neural networks f θ with trainable parameters θ to extract representations from inputs X 1 and X 2 . Learning these parameters requires optimizing differentiable and scalable training objectives to capture task-relevant shared and unique information (see overview in Figure  4 .2):\n\nwhere I(Z 1 ; X 2 ; Y ) = I(Z 1 ; X 2 ) -I(Z 1 ; X 2 |Y ) is the shared information and I(Z 2 ; X 1 ; Y ) = I(Z 2 ; X 2 )-I(Z 2 ; X 1 |Y ) is the unique information. One important characteristic of our framework is that when unique information is zero: I(X 1 ; Y |X 2 ) = 0 and I(X 2 ; Y |X 1 ) = 0, or all shared information is task-relevant: I(X 1 ; X 2 ; Y ) = I(X 1 ; X 2 ), our framework recovers standard CL as in Eq.(4.2). However, as we have previously indicated and will show empirically, these assumptions can easily be violated, and our framework enlarges Eq.(4.2) to cases where unique information is present. The learned Zs can then be used as input to a linear classifier and fine-tuned to predict the label for multimodal classification or retrieval tasks. However, the shared and unique MI terms above are often intractable in practice. In the next section, we will build up our method step by step, eventually showing that each term in Eqs.(4.6-4.7) can be approximated as follows:\n\nwhere I NCE and I NCE-CLUB are scalable contrastive estimators (Section 4.3.1) and X ′ 1 , X ′ 2 are suitable data augmentations (Section 4.3.2) on each modality. Overall, these equations can be interpreted as both positive and negative signals to learn representations for S and U . For shared information S, the estimator maximizes task-relevant shared information via I NCE (X 1 ; X 2 ) while removing task-irrelevant shared information via a novel upper bound\n\n). In the following sections, we derive this final objective step-by-step: (1) approximating the MI objectives in S and U with CL estimators, (2) relaxing the dependence on labels Y with self-supervised data augmentations, finally (3) discussing overall training and implementation details of end-to-end self-supervised learning.",
      "page_start": 81,
      "page_end": 82
    },
    {
      "section_name": "Supervised Factorcl With Shared And Unique Information",
      "text": "To capture shared and unique information via an objective function, we will need to maximize lower bounds for all terms with a positive sign in Eq.(4.8) and (4.9) (I (X 1 ; X 2 ) , I (X i ; Y ) , I (X 1 ; X 2 |Y )) and minimize upper bounds for all terms with a negative sign (I (X 1 ; X 2 ) , I (X 1 ; X 2 |Y )). Our first theorem derives general lower and upper bounds for MI terms as variants of contrastive estimation: Theorem 2. (Contrastive estimators for I(X 1 ; X 2 )) Defining the NCE and NCE-CLUB estimators,\n\n] (4.10)\n\nwhere f * (x 1 , x 2 ) is the optimal critic from I NCE plugged into the I CLUB objective  [110] . We call the proposed plug-in objective Eq.(4.11) I NCE-CLUB , and obtain lower and upper bounds on I(X 1 ; X 2 ):\n\n(4.12) Proof. The lower bound I NCE (X 1 ; X 2 ) ≤ I(X 1 ; X 2 ) follows from Oord et al.  [453] : optimizing the objective leads to an optimal critic  [488]  f * = log p(x 1 |x 2 ) + c(x 1 ), with a deterministic function c(⋅). Plugging optimal critic f * into I NCE-CLUB (X 1 ; X 2 ) cancels out the c(x 1 ) term and yields I NCE-CLUB (X 1 ; X 2 ) and I(X 1 ; X 2 ) ≤ I NCE-CLUB . We include a detailed proof in the full paper  [373] .\n\nI NCE-CLUB (X 1 ; X 2 ) gives a desired upper bound of I(X 1 ; X 2 ) \"for free\" while avoiding separately optimizing lower bound and upper bounds. In Figure  4 .3, we show these two bounds in practice across two Gaussian distributions X 1 and X 2 with varying amounts of MI I(X 1 ; X 2 ). We use the second formulation of I CLUB  [110] , which assumes p(x 1 |x 2 ) to be unknown. Our upper bound is empirically tighter (see Figure  4 .3) and comes for \"free\" via jointly maximizing the lower bound I NCE . These lower and upper bounds can be seen as new contrastive objectives over positive and negative (x 1 , x 2 ) pairs, enabling a close integration with existing pre-training paradigms. Finally, we can similarly obtain bounds for the conditional MI\n\nThese two bounds result in conditional CL objectives  [397, 613, 619]  -they differ critically from standard CL methods since they capture task-irrelevant shared information that remains",
      "page_start": 82,
      "page_end": 83
    },
    {
      "section_name": "Gaussian, Dim=50",
      "text": "Gaussian, dim=100 Gaussian, dim=200   [453]  and our proposed upper bound I NCE-CLUB on sample distributions with changing mutual information: our upper bound is tighter, more accurate, and more stable than I CLUB upper bound  [110] , and also comes for 'free' via jointly estimating both lower and upper bounds simultaneously. We find that as dimension increases, the I CLUB estimator collapses to zero and no longer tracks true MI.\n\nbetween X 1 and X 2 after observing Y . This task-irrelevant shared information is removed by minimizing its upper bound. Note that f (x 1 , x 2 , y) here denotes a different function from f (x 1 , x 2 ) in Eq.(4.10), as the general forms are different (taking in x 1 , x 2 versus x 1 , x 2 , y). f (x 1 , x 2 , y) can be implemented in different ways, e.g., g([x 1 , y]) T h(x 2 ) where g(), h() are trainable encoders and [x 1 , y] denotes concatenation  [562] .",
      "page_start": 84,
      "page_end": 84
    },
    {
      "section_name": "Self-Supervised Factorcl Via Multimodal Augmentations",
      "text": "The derivations above bring about supervised CL objectives with access to Y  [300] . For unsupervised CL  [453, 606] , we derive similar objectives without access to Y by leveraging semantic augmentations on each modality. Denote X ′ as some augmentation of X (e.g., rotating, shifting, or cropping). Under the optimal augmentation assumption from Tian et al.  [606]  (restated below), replacing Y with X ′ in our formulations enables learning of task-relevant information without access to labels: Definition 3. (Optimal unimodal augmentation) [606] X ′ 1 is an optimal unimodal augmentation for X 1 when I(X; X ′ ) = I(X; Y ), which implies that the only information shared between X and X ′ is task-relevant with no irrelevant noise.\n\nThis assumption is satisfied when all information shared between X and X ′ is task-relevant, which implies that the augmentation keeps task-relevant information constant while changing task-irrelevant information. In the case of image classification, task-relevant information is the object in the picture, while task-irrelevant information is the background. By performing two separate unimodal augmentations giving X ′ 1 and X ′ 2 , we can substitute contrastive estimators in Eqs.(4.13) and (4.14), by replacing I(X i ; Y ) terms with I(X i ; X ′ i ) and replacing I(\n\nThe objectives can be seen as conditional contrastive learning on augmentations (X ′ 1 , X ′ 2 ). Here again f (x 1 , x 2 , x ′ 1 , x ′ 2 ) is different from the critics in Eqs.(4.13 because of the different general forms. We implement f () here as g([\n\n) where g(), h() are trainable encoders specific for each modality and [x 1 , x ′ 1 ] denotes concatenation. This concatenation is justified by the CMI estimators in Sordoni et al.  [562] , who show that concatenating the conditioning variable with the input in the critic f (x 1 , x 2 , x ′ 1 , x ′ 2 ) yields a Conditional InfoNCE estimator (Eq.(4.15)) that is a lower bound for CMI. However, the exact Conditional InfoNCE estimator learns a different conditional distribution p(x 1 , x 2 |x ′ 1 , x ′ 2 ) for each augmented pair x ′ 1 , x ′ 2 , which can be prohibitively expensive. We could approximate this by creating multiple augmentations of a single paired x 1 , x 2 . Our code uses one augmented pair x ′ 1 , x ′ 2 for each x 1 , x 2 but could be extended to multiple pairs, and we find this simple approach yields consistent CMI lower and upper bounds that are empirically comparable to existing CMI estimators  [430, 562] . We include full comparisons and implementation details in the full paper  [373] .   [701]  dataset. After augmenting text modality X 1 independently (same for both augmentation types), we illustrate their differences for image augmentation: unique augmentation on images should avoid removing information referred to by X 1 (the text). The text mentions that the car is fast so unique augmentation for images should not remove the highway pixels of the image which can suggest the car is fast.\n\nAlthough we find this method to work well in practice, a more careful analysis reveals that 2 separate unimodal augmentations X ′ 1 and X ′ 2 each satisfying I(X i ;\n\nneeded for the substitution in Eqs.(4.15) and (4.16) to hold with equality. To satisfy this property exactly, we define optimal multimodal augmentations: Definition 4. (Optimal multimodal augmentation) X ′ 1 and X ′ 2 are optimal multimodal augmentation for X 1 and X 2 when I(X 1 , X 2 ; X ′ 1 , X ′ 2 ) = I(X 1 , X 2 ; Y ), which implies that the only information shared between X 1 , X 2 and X ′ 1 , X ′ 2 is taskrelevant with no irrelevant noise.\n\nWe satisfy I(X 1 , X 2 ; X ′ 1 , X ′ 2 ) = I(X 1 , X 2 ; Y ) using two steps:\n\nUnique aug:\n\nWe call the second step unique augmentation: after observing X 1 , we create augmented X ′ 2 from X 2 to keep task-relevant information not already in X 1 . To empirically satisfy optimal multimodal augmentations, we avoid augmentations in one modality that will remove or strongly destroy information shared with the other modality. For example, in image captioning, we should avoid image augmentations such as cropping that destroy information\n\nfrom the caption (e.g., cropping object parts referred to by the caption), and instead, only augment images via flipping or color jittering which retains all caption information. Figure  4 .4 shows an example of unique augmentation that satisfies these conditions. In our experiments, we will show that our augmentations consistently perform better than standard augmentations (Table  4 .3), suggesting that approximately satisfying Eqs.(4.17) and (4.18) can be empirically sufficient, which is simple and straightforward to implement on real-world datasets.",
      "page_start": 84,
      "page_end": 85
    },
    {
      "section_name": "Overall Method And Implementation",
      "text": "The final algorithm sketch is in Algorithm 2, which we compare against standard CL in Algorithm 1. It can be shown that FACTORCL learns all the task-relevant information from both modalities:\n\n), suggesting that FACTORCL learns both shared and unique task-relevant information.\n\nWe include the full proof in the full paper  [373] . In practice, while we do not expect perfect estimation of MI quantities and maximization with respect to MI objectives, we show that our method still improves empirical performance on several real-world datasets.\n\nComplexity: Compared to heuristic combinations of cross-modal and single-modality CL  [258, 278, 337, 535, 647, 689, 710] , our approach does not significantly increase complexity: (1) upper bounds on MI can be estimated \"for free\" by directly plugging in the optimal critic from I NCE , (2) removal of task-irrelevant information via I(X 1 ; X 2 |X ′ 1 , X ′ 2 ) shares encoders with I NCE , and (3) separate unimodal augmentations perform empirically well.",
      "page_start": 86,
      "page_end": 86
    },
    {
      "section_name": "Experiments",
      "text": "We run comprehensive experiments on a suite of synthetic and large-scale real-world datasets with varying requirements of shared and unique task-relevant information, comparing our FACTORCL method to key baselines: 1. SimCLR  [103] : the straightforward method of cross-modal (X 1 , X 2 ) contrastive learning.\n\n2. Cross+Self  [258, 278, 337, 535, 689, 710] : captures a range of methods combining crossmodal (X 1 , X 2 ) CL with additional unimodal (X i , X ′ i ) CL objectives. This category also includes other ways of preserving unique information, such as through (variational) autoencoder reconstructions  [647] . 3. Cross+Self+Fact  [690, 710] : A factorized extension of Cross+Self, which is approximately done in prior work that adds separate (typically pre-trained) unimodal encoders for each modality. 4. SupCon  [300] , which learns I(X 1 ; X 2 |Y ) using CL conditioned on Y from labeled data. We also carefully ablate each component of our method and investigate factors, including training data size and choice of augmentations. The intermediate ablations that emerge include: 1. FACTORCL-SUP: The supervised CL version which uses labels Y in Eqs.(4.13) and (4.14).",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Factorcl-Ssl:",
      "text": "The fully self-supervised version of our approach replacing Y with multimodal augmentations X ′ 1 and X ′ 2 to approximate the task. 3. OurCL-SUP: FACTORCL-SUP but removing the factorization so only two features Z 1 is optimized for both I(X 1 ; X 2 ; Y ) and I(X 1 ; Y |X 2 ), Z 2 optimized for both I(X 1 ; X 2 ; Y ) and I(X 2 ; Y |X 1 ). 4. OurCL-SSL: FACTORCL-SSL but also removing the factorization in the self-supervised setting. The formulation of each ablation and implementation can be found in the full paper  [373] .",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Controlled Experiments On Synthetic Datasets",
      "text": "Synthetic data generation: We begin by generating data with controllable ratios of task-relevant shared and unique information. Starting with a set of latent vectors w 1 , w 2 , w s ∼ N (0 d , Σ 2 d ), d = 50 representing information unique to X 1 , X 2 and common to both respectively, the concatenated vector [w 1 , w s ] is transformed into high-dimensional x 1 using a fixed transformation T 1 and likewise [w 2 , w s ] to x 2 via T 2 . The label y is generated as a function (with nonlinearity and noise) of varying ratios of w s , w 1 , and w 2 to represent shared and unique task-relevant information.\n\nResults: In Figure  9 .1, we show our main result on synthetic data comparing FACTORCL with existing CL baselines. FACTORCL consistently maintains the best performance, whereas SimCLR  [103]  and SupCon  [300]  see performance drops as unique information increases. Cross+Self  [258, 278, 337, 710]  recovers in fully unique settings (x-axis= 1.0) but suffers at other ratios.",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Representation Probing Information:",
      "text": "We run a probing experiment to compute how well different contrastive representations capture shared and unique information. In Table  4 .1, for the Z i 's learned by each method, we approximately compute I(Z i ; w 1 ), I(Z i ; w 2 ), and I(Z i ; w s ) with respect to ground truth generative variables w s , w 1 , and w 2 . As expected, existing methods such as SimCLR capture smaller amounts of unique information (roughly 4 bits in I(Z i ; w 1 ) and I(Z i ; w 2 )), focusing instead on learning I(Z i ; w s ) (12 bits). Cross+self captures slightly larger I(Z i ; w 2 ) = 4.26, and SupCon with labeled data captures up to 5 bits of unique information. Our FACTORCL approach captures 7 bits of unique information and maintains 10 bits of shared information, with total information captured higher than the other approaches. Furthermore, {Z S 1 , Z S 2 } capture more information about w s , Z U 1 about w 1 , and Z U 2 about w 2 , indicating that factorization in our approach is successful.",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Self-Supervised Learning With Low Redundancy And High Uniqueness",
      "text": "Multimodal fusion datasets: We use a large collection of real-world datasets provided in MultiBench  [367] , where we expect varying ratios of shared and unique information important for the task, to compare FACTORCL with other CL baselines: 1. MIMIC  [286] : mortality and disease prediction from 36, 212 medical records (tabular patient data and medical time-series sensors from ICU). 2. MOSEI  [718] : multimodal sentiment and emotion benchmark with 23, 000 monologue videos. 3. MOSI  [711] : multimodal sentiment analysis from 2, 199 YouTube videos. 4. UR-FUNNY  [225] : a dataset of humor detection from more than 16, 000 TED talk videos. 5. MUSTARD  [83] : a corpus of 690 videos for research in sarcasm detection from TV shows. 6. IRFL  [701] : 6, 697 matching images and figurative captions (rather than literal captions). Together, these datasets cover seven different modalities from the healthcare, affective computing, and multimedia research areas and total more than 84, 000 data points. For MIMIC with tabular and medical sensor inputs, we train self-supervised CL models on top of raw modality inputs. For IRFL with image and caption inputs, we start with a pretrained CLIP model  [498]  and perform continued pre-training to update CLIP weights with our FACTORCL objectives, before linear classifier testing. For the remaining four video datasets, we train self-supervised CL models starting from standard pre-extracted text, video, and audio features  [367] . Please refer to the full paper  [373]  for experimental details. We release our code and models at https: //github.com/pliang279/FactorCL.\n\nMultimodal fusion results: From Table  4 .2, FACTORCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and selfsupervised settings, particularly on MUSTARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). In Table  4 .3, we also show that FACTORCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP  [498]  as well as continued pre-training baselines on top of CLIP.\n\nModeling ablations: In Table  4 .2, we also carefully ablate each component in our method and indicate either existing baselines or newly-run ablation models. 1. Factorized representations: In comparing FACTORCL-SSL with OurCL-SSL, and also FAC-   [367]  datasets with varying shared and unique information: FACTORCL achieves strong results vs self-supervised (top 5 rows) and supervised (bottom 3 rows) baselines that do not have unique representations, factorization, upper-bounds to remove irrelevant information, and multimodal augmentations. TORCL-SUP with OurCL-SUP, we find that factorization is critical: without it, performance drops on average 6.1%, with performance drop as high as 8.6% for MIMIC. 2. Information removal via upper bound: By comparing FACTORCL with SimCLR, Cross+Self, and Cross+Self+Fact, and SupCon that only seek to capture task-relevant information via contrastive lower bounds on MI, we find that separately modeling the task-relevant information (to be captured) and task-irrelevant information (to be removed) is helpful. Without removing task-irrelevant information via the upper-bound objective, performance drops on average 13.6%, with performance drops as high as 23.5% for the MOSI dataset. We also found that training was more difficult without this objective, which is expected due to overwhelming superfluous information from the dataset  [718] . 3. Multimodal augmentations: Finally, we investigate the differences between separate unimodal augmentations (FACTORCL-IndAug in Table  4 .3) versus a joint multimodal augmentation (FACTORCL-SSL) on the IRFL dataset. We choose this dataset since its images and captions are the easiest to visualize (see Figure  4 .4 for augmentations from both strategies). In the self-supervised setting, we find that multimodal augmentations achieve 95% performance, higher than the 92% for separate unimodal augmentations, and both outperform baselines SimCLR and Cross+Self. Ablations on S, U 1 and U 2 : In Table  4 .4, we also test FAC-TORCL when training linear classifiers on top of only shared {Z S 1 , Z S 2 } and unique Z U 1 , Z U 2 separately. We call these models FACTORCL-S, FACTORCL-U 1 , and FACTORCL-U 2 . Immediately, we observe that performance drops as compared to the full FACTORCL model, indicating that both shared and unique information are critical in real-world multimodal tasks. As expected, the best-performing submodel is the one that captures the region with the largest amount of task-relevant information: MOSEI and MOSI are known to include a lot of redundancy and unique information since language is very important for detecting sentiment  [718] , so FACTORCL-S and FACTORCL-U 2 perform best. For sarcasm detection on MUSTARD, video information is most important with FACTORCL-U 1 performing best (59.4%), and ablation models are also the furthest away from full multimodal performance (69.9%). This is aligned with intuition where sarcasm is expressed through tone of voice and visual gestures (high U 1 ), as well as from contradictions between language and video (higher multimodal performance).",
      "page_start": 88,
      "page_end": 90
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 90,
      "page_end": 90
    },
    {
      "section_name": "Related Work",
      "text": "Contrastive learning is a successful self-supervised learning paradigm for computer vision  [82, 103, 106, 211, 229, 453] , natural language  [188, 416, 438] , speech  [42, 453, 528] , and multimodal tasks  [15, 285, 498] . Its foundational underpinnings are inspired by work in multiview information theory  [173, 300, 564, 606, 617]  studying the shared information between two views and whether they are necessary or sufficient in predicting the label. Recently, Wang et al.  [647]  and Kahana and Hoshen  [290]  discuss the limitations of assuming multiview redundancy and propose autoencoder reconstruction or unimodal contrastive learning to retain unique information, which resembles the Cross+self baselines in our experiments. We refer the reader to Shwartz-Ziv and LeCun  [548]  for a comprehensive review on multiview and contrastive learning. Our work also relates to conditional contrastive learning  [112, 397, 619, 697] , where positive or negative pairs are supposed to sample from conditional distributions. Multimodal contrastive learning aims to align related data from different modalities, typically provided as positive pairs. This could be done via optimizing a contrastive objective for inter-modality pairs  [15, 17, 285, 498] , or both intra-and inter-modality data pairs  [258, 278, 303, 337, 710] . Our work also relates to factorized representation learning, which primarily studies how to capture modality-specific information primarily in each modality and multimodal information redundant in both modalities  [249, 615] . Prior work has used disentangled latent variable models  [57, 238, 249, 615] , mixture-of-experts  [545] , or product-of-experts  [669]  layer to explain factors in multimodal data.\n\nInformation theory  [125, 536]  has been used to study several phenomena in multimodal learning, including co-learning  [500, 717]  and multi-view learning  [261, 617] . Due to its theoretical importance, several lower and upper bounds have been proposed for practical estimation  [453, 458, 488, 670] . We build on the CLUB upper bound  [110]  to create a more accurate and stable bound. Our characterizations of shared and unique information are also related to partial information decomposition  [663] , co-information  [51, 636] , interaction information  [412] , and cross-domain disentanglement  [269]  research.",
      "page_start": 90,
      "page_end": 91
    },
    {
      "section_name": "Conclusion",
      "text": "This paper studied how standard CL methods suffer when task-relevant information lies in regions unique to each modality, which is extremely common in real-world applications such as sensor placement, medical testing, and multimodal interaction. In response, we proposed FACTORCL, a new method expanding CL techniques through the use of factorized representations, removing task-irrelevant information via upper bounds on MI, and multimodal data augmentations suitable for approximating the unobserved task. Based on FACTORCL's strong performance, there are several exciting directions in extending these ideas for masked and non-contrastive pre-training.",
      "page_start": 90,
      "page_end": 91
    },
    {
      "section_name": "Chapter 5",
      "text": "Quantifying Multimodal Interactions in Trained Models",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Introduction",
      "text": "Using our foundation of multimodal interactions, we now present our work in model quantification: visualizing and understanding the internal modeling of multimodal interactions in trained models. As multimodal models are increasingly deployed in real-world applications, it has become increasingly important to quantify and understand their internal mechanics  [205, 375, 466]  as a step towards accurately benchmarking their limitations for more reliable deployment  [231, 274] . However, modern multimodal models are typically black-box neural networks, such as pretrained transformers  [348, 390] , which makes understanding what interactions they learn difficult.\n\nAs a step in interpreting multimodal models, this paper introduces an analysis and visualization method called MULTIVIZ (see Figure  5 .1). To tackle the challenges of visualizing model behavior, we scaffold the problem of interpretability into 4 stages: (1) unimodal importance: identifying the contributions of each modality towards downstream modeling and prediction, (2) cross-modal interactions: uncovering the various ways in which different modalities can relate with each other and the types of new information possibly discovered as a result of these relationships, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction for a given task. In addition to including current approaches for unimodal importance  [205, 417, 509]  and cross-modal interactions  [235, 396] , we additionally propose new methods for interpreting cross-modal interactions, multimodal representations, and prediction to complete these stages in MULTIVIZ. By viewing multimodal interpretability through the lens of these 4 stages, MULTIVIZ contributes a modular and human-in-the-loop visualization toolkit for the community to visualize popular multimodal datasets and models as well as compare with other interpretation perspectives, and for stakeholders to understand multimodal models in their research domains.\n\nMULTIVIZ is designed to support many modality inputs while also operating on diverse modalities, models, tasks, and research areas. Through experiments on 6 real-world multimodal tasks (spanning fusion, retrieval, and question-answering), 6 modalities, and 8 models, we show that MULTIVIZ helps users gain a deeper understanding of model behavior as measured via a Proposed Work 1: Interpreting Internal Mechanics",
      "page_start": 92,
      "page_end": 93
    },
    {
      "section_name": "Cross-Modal Interactions \"Color\"",
      "text": "What color is the building?",
      "page_start": 93,
      "page_end": 93
    },
    {
      "section_name": "Local Analysis Of Given Datapoint",
      "text": "",
      "page_start": 93,
      "page_end": 93
    },
    {
      "section_name": "Global Analysis By Retrieving Similar Datapoints",
      "text": "What color is the Salisbury Rd sign?\n\nWhat color are the checkers on the wall?",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Multimodal Representations",
      "text": "What color is the building?   4 ) multimodal prediction studies how these features are composed to make a prediction. Right: We visualize multimodal representations through local and global analysis. Given an input datapoint, local analysis visualizes the unimodal and cross-modal interactions that activate a feature. Global analysis informs the user of similar datapoints that also maximally activate that feature, and is useful in assigning human-interpretable concepts to features by looking at similarly activated input regions (e.g., the concept of color). proxy task of model simulation. We further demonstrate that MULTIVIZ helps human users assign interpretable language concepts to previously uninterpretable features and perform error analysis on model misclassifications. Finally, using takeaways from error analysis, we present a case study of human-in-the-loop model debugging. Overall, MULTIVIZ provides a practical toolkit for interpreting multimodal models for human understanding and debugging. MULTIVIZ datasets, models, and code are at https://github.com/pliang279/MultiViz.",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "Multiviz: Visualizing & Understanding Multimodal Models",
      "text": "This section presents MULTIVIZ, our proposed analysis framework for analyzing the behavior of multimodal models. As a general setup, we assume multimodal datasets take the form\n\n1 , ..., x\n\n(1)\n\n2 , ..., y) n i=1 }, with boldface x denoting the entire modality, each x 1 , x 2 indicating modality atoms (i.e., fine-grained sub-parts of modalities that we would like to analyze, such as individual words in a sentence, object regions in an image, or time-steps in time-series data), and y denoting the label. These datasets enable us to train a multimodal model ŷ = f (x 1 , x 2 ; θ) which we are interested in visualizing.\n\nModern parameterizations of multimodal models f are typically black-box neural networks, such as multimodal transformers  [232, 614]  and pretrained models  [348, 390] . How can we visualize and understand the internal modeling of multimodal information and interactions in these models? Having an accurate understanding of their decision-making process would enable us to benchmark their opportunities and limitations for more reliable real-world deployment. However, interpreting f is difficult. In many multimodal problems, it is useful to first scaffold the problem Three small dogs, two white and one black and white, on a sidewalk.",
      "page_start": 93,
      "page_end": 93
    },
    {
      "section_name": "Cmu-Mosei",
      "text": "Why am I spending my money watching this? (sigh) I think I was more sad....",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Figure 5.2:",
      "text": "Examples of cross-modal interactions discovered by our proposed second-order gradient approach: first taking a gradient of model f with respect to an input word (e.g., x 1 = birds), before taking a second-order gradient with respect to all image pixels (highlighted in green) or bounding boxes (in red boxes) x 2 indeed results in all birds in the image being highlighted.\n\nof interpreting f into several intermediate stages from low-level unimodal inputs to high-level predictions, spanning unimodal importance, cross-modal interactions, multimodal representations, and multimodal prediction. Each of these stages provides complementary information on the decision-making process (see Figure  5 .1). We now describe each step in detail and propose methods to analyze each step.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Unimodal Importance (U)",
      "text": "Unimodal importance aims to understand the contributions of each modality towards modeling and prediction. It builds upon ideas of gradients  [40, 165, 549]  and feature attributions (e.g., LIME  [509] , Shapley values  [417] ). We implement unimodal feature attribution methods as a module UNI(f θ , y, x) taking in a trained model f θ , an output/feature y which analysis is performed with respect to, and the modality of interest x. UNI returns importance weights across atoms x of modality x.",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Cross-Modal Interactions (C)",
      "text": "Cross-modal interactions describe various ways in which atoms from different modalities can relate with each other and the types of new information possibly discovered as a result of these relationships. Recent work  [235, 396]  has formalized a definition of cross-modal interactions by building upon literature in statistical non-additive interactions: Definition 1 (Statistical Non-Additive Interaction  [180, 563, 620, 621] ). A function f learns a feature interaction I between 2 unimodal atoms x 1 and x 2 if and only if f cannot be decomposed into a sum of unimodal subfunctions\n\nThis definition of non-additive interactions is general enough to include different ways that interactions can happen, including multiplicative interactions from complementary views of the data (i.e., an interaction term x 1 Wx 2  [283] ), or cooperative interactions from equivalent views (i.e., an interaction term majority(f (x 1 ), f (x 2 ))  [146] ). Using this definition, MULTIVIZ first includes two recently proposed methods for understanding cross-modal interactions: EMAP  [235]  decomposes f (x 1 , x 2 ) = g 1 (x 1 )+g 2 (x 2 )+g 12 (x 1 , x 2 ) into strictly unimodal representations g 1 , g 2 , and cross-modal representation\n\nto quantify the degree of global cross-modal interactions across an entire dataset. DIME  [396]  further extends EMAP using feature visualization on each disentangled representation locally (per datapoint). However, these approaches require approximating expectations over modality subsets, which may not scale beyond 2 modalities. To fill this gap, we propose an efficient approach for visualizing these cross-modal interactions by observing that the following gradient definition directly follows from Definition 1: Definition 2 (Gradient definition of statistical non-additive interaction). A function f exhibits non-additive interactions among 2 unimodal atoms x 1 and\n\nTaking a second-order gradient of f zeros out the unimodal terms g 1 (x 1 ) and g 2 (x 2 ) and isolates the interaction g 12 (x 1 , x 2 ). Theoretically, second-order gradients are necessary and sufficient to recover cross-modal interactions: purely additive models will have strictly 0 secondorder gradients so\n\n= 0, and any non-linear interaction term g 12 (x 1 , x 2 ) has non-zero second-order gradients since g cannot be a constant or unimodal function, so\n\n2 > 0. Definition 2 inspires us to extend first-order gradient and perturbation-based approaches  [221, 509, 702]  to the second order. Our implementation first computes a gradient of f with respect to a modality atom which the user is interested in querying cross-modal interactions for (e.g., x 1 = birds), which results in a vector ∇ 1 = ∂f ∂x 1 of the same dimension as x 1 (i.e., token embedding dimension). We aggregate the vector components of ∇ 1 via summation to produce a single scalar ∥∇ 1 ∥, before taking a second-order gradient with respect to all atoms of the second modality x 2 ∈ x 2 (e.g., all image pixels), which results in a vector\n\n] of the same dimension as x 2 (i.e., total number of pixels). Each scalar entry in ∇ 12 highlights atoms x 2 that have non-linear interactions with the original atom x 1 , and we choose the x 2 's with the largest magnitude of interactions with x 1 (i.e., which highlights the birds in the image, see Figure  5 .2 for examples on real datasets). We implement a general module CM(f θ , y, x 1 , x 2 ) for cross-modal visualizations, taking in a trained model f θ , an output/feature y, the first modality's atom of interest x 1 , and the entire second modality of interest x 2 , before returning importance weights across atoms x 2 of modality x 2 .",
      "page_start": 94,
      "page_end": 94
    },
    {
      "section_name": "Multimodal Representations",
      "text": "Given these highlighted unimodal and cross-modal interactions at the input level, the next stage aims to understand how these interactions are represented at the feature representation level. Specifically, given a trained multimodal model f , define the matrix M z ∈ R N ×d as the penultimate layer of f representing (uninterpretable) deep feature representations implicitly containing information from both unimodal and cross-modal interactions. For the ith datapoint, z = M z (i) collects a set of individual feature representations z 1 , z 2 , ..., z d ∈ R. We aim to interpret these feature representations through both local and global analysis (see Figure  5 .1 (right) for an example):\n\nLocal representation analysis (R ℓ ) informs the user on parts of the original datapoint that activate feature z j . To do so, we run unimodal and cross-modal visualization methods with respect to feature z j (i.e., UNI(f θ , z j , x), CM(f θ , z j , x 1 , x 2 )) in order to explain the input unimodal and cross-modal interactions represented in feature z j . Local analysis is useful in explaining model predictions on the original datapoint by studying the input regions activating feature z j .",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "Unimodal Image Importance",
      "text": "",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "Unimodal Text Importance",
      "text": "",
      "page_start": 95,
      "page_end": 95
    },
    {
      "section_name": "Cross-Modal Text -> Image Interaction",
      "text": "Cross-modal image -> text interaction Global representation analysis (R g ) provides the user with the top k datapoints D k (z j ) = {(x 1 , x 2 , y) k i=(1) } that also maximally activate feature z j . By further unimodal and cross-modal visualizations on datapoints in D k (z j ), global analysis is especially useful in helping humans assign interpretable language concepts to each feature by looking at similarly activated input regions across datapoints (e.g., the concept of color in Figure  5 .1, right). Global analysis can also help to find related datapoints the model also struggles with for error analysis.",
      "page_start": 96,
      "page_end": 96
    },
    {
      "section_name": "Overview",
      "text": "",
      "page_start": 96,
      "page_end": 96
    },
    {
      "section_name": "Multimodal Prediction (P)",
      "text": "Finally, the prediction step takes the set of feature representations z 1 , z 2 , ..., z d and composes them to form higher-level abstract concepts suitable for a task. We approximate the prediction process with a linear combination of penultimate layer features by integrating a sparse linear prediction model with neural network features  [667] . Given the penultimate layer M z ∈ R N ×d , we fit a linear model E (Y |X = x) = M ⊺ z β (bias β 0 omitted for simplicity) and solve for sparsity using:\n\n(5.1)\n\nThe resulting understanding starts from the set of learned weights with the highest non-zero coefficients β top = {β (1) , β (2) , ...} and corresponding ranked features z top = {z (1) , z (2) , ...}. β top tells the user how features z top are composed to make a prediction, and z top can then be visualized with respect to unimodal and cross-modal interactions using the representation stage (Section 5.2.3).",
      "page_start": 96,
      "page_end": 96
    },
    {
      "section_name": "Putting Everything Together",
      "text": "We summarize these proposed approaches for understanding each step of the multimodal process and show the overall MULTIVIZ user interface in Figure  5 .3. This interactive API enables users to choose multimodal datasets and models and be presented with a set of visualizations at each stage, with an overview page for general unimodal importance, cross-modal interactions, and prediction weights, as well as a feature page for local and global analysis of user-selected features (see full paper  [374]  for details).",
      "page_start": 96,
      "page_end": 96
    },
    {
      "section_name": "Experiments",
      "text": "Our experiments are designed to verify the usefulness and complementarity of the 4 MULTIVIZ stages. We start with a model simulation experiment to test the utility of each stage towards overall model understanding (Section 5.3.1). We then dive deeper into the individual stages by testing how well MULTIVIZ enables representation interpretation (Section 5.3.2) and error analysis (Section 5.3.3), before presenting a case study of model debugging from error analysis insights (Section 5.3.4). We showcase the following selected experiments and defer results on other datasets to the full paper  [374] .\n\nSetup: We use a large suite of datasets from MultiBench  [367]  which span real-world fusion  [32, 287, 718] , retrieval  [486] , and QA  [206, 289]  tasks. For each dataset, we test a corresponding state-of-the-art model: MULT  [614] , LRTF  [388] , LF  [46] , VILT  [307] , CLIP  [498] , CNN-LSTM-SA  [289] , MDETR  [292] , and LXMERT  [589] . These cover models both pretrained and trained from scratch. We summarize all 6 datasets and 8 models tested in Table  5 .1, and provide more details in the full paper  [374] .",
      "page_start": 97,
      "page_end": 97
    },
    {
      "section_name": "Model Simulation",
      "text": "We first design a model simulation experiment to determine if MULTIVIZ helps users of multimodal models gain a deeper understanding of model behavior. If MULTIVIZ indeed generates human-understandable explanations, humans should be able to accurately simulate model predictions given these explanations only, as measured by correctness with respect to actual model predictions and annotator agreement (Krippendorff's alpha  [316] ). To investigate the utility of each stage in MULTIVIZ, we design a human study to see how accurately 21 humans users (3 users for each of the following 7 local ablation settings) can simulate model predictions:\n\n(1) U: Users are only shown the unimodal importance (U) of each modality towards label y.\n\n(2) U + C: Users are also shown cross-modal interactions (C) highlighted towards label y.\n\n(3) U + C + R ℓ : Users are also shown local analysis (R ℓ ) of unimodal and cross-modal interactions of top features z top = {z (1) , z (2) , ...} maximally activating label y.\n\n(4) U + C + R ℓ + R g : Users are additionally shown global analysis (R g ) through similar datapoints that also maximally activate top features z top for label y.\n\n(5) MULTIVIZ (U + C + R ℓ + R g + P): The entire MULTIVIZ method by further including visualizations of the final prediction (P) stage: sorting top ranked feature neurons z top = {z (1) , z (2) , ...} with respect to their coefficients β top = {β (1) , β (2) , ...} and showing these coefficients to the user. Using 20 datapoints per setting, these experiments with 15 users on 3 datasets and 3 models involve 35 total hours of users interacting with MULTIVIZ, which is a significantly larger-scale study of model simulation compared to prior work  [7, 396, 655] .\n\nQuantitative results: We show these results in Table  5 .2 and find that having access to all stages in MULTIVIZ leads to significantly highest accuracy of model simulation on VQA 2.0, along with lowest variance and most consistent agreement between annotators. On fusion tasks with MM-IMDB and CMU-MOSEI, we also find that including each visualization stage consistently leads to higher correctness and agreement, despite the fact that fusion models may not require cross-modal interactions to solve the task  [235] . More importantly, humans are able to simulate model predictions, regardless of whether the model made the correct prediction or not.\n\nTo test additional intermediate ablations, we conducted user studies on (6) R ℓ + P (local analysis on final-layer features along with their prediction weights) and (7) R g + P (global analysis on final-layer features along with their prediction weights), to ablate the effect of overall analysis (U and C) and feature analysis (R ℓ or R g in isolation). R ℓ + P results in an accuracy of 51.7 ± 12.6 with 0.40 agreement, while R g + P gives 71.7 ± 7.6 with 0.53 agreement. Indeed, these underperform as compared to including overall analysis (U and C) and feature analysis (R ℓ + R g ).\n\nFinally, we also scaled to 100 datapoints on VQA 2.0, representing upwards of 10 hours of user interaction (for the full MULTIVIZ setting), and obtain an overall correctness of 80%, reliably within the range of model simulation using 20 points (81.7 ± 2.9). Therefore, the sample size of 20 points that makes all experiments feasible is still a reliable sample.\n\nWe also conducted qualitative interviews to determine what users found useful in MULTIVIZ:\n\n(1) Users reported that they found local and global representation analysis particularly useful: global analysis with other datapoints that also maximally activate feature representations were important for identifying similar concepts and assigning them to multimodal features.\n\n(2) Between Overview (U + C) and Feature (R ℓ + R g + P) visualizations, users found Feature visualizations more useful in 31.7%, 61.7%, and 80.0% of the time under settings (3), (4), and (  5 ) respectively, and found Overview more useful in the remaining points. This means that for each stage, there exists a significant fraction of data points where that stage is most needed.\n\nTable  5 .3: Left: Across 15 human users (5 users for each of the following 3 settings), we find that users are able to consistently assign concepts to previously uninterpretable multimodal features using both local and global representation analysis. Right: Across 10 human users (5 users for each of the following 2 settings), we find that users are also able to categorize model errors into one of 3 stages they occur in when given full MULTIVIZ visualizations.   (3) While it may be possible to determine the prediction of the model with a subset of stages, having more stages that confirm the same prediction makes them a lot more confident about their prediction, which is quantitatively substantiated by the higher accuracy, lower variance, and higher agreement in human predictions. We also include additional experiments in the full paper  [374] .",
      "page_start": 97,
      "page_end": 97
    },
    {
      "section_name": "Representation Interpretation",
      "text": "We now take a deeper look to check that MULTIVIZ generates accurate explanations of multimodal representations. Using local and global representation visualizations, can humans consistently assign interpretable concepts in natural language to previously uninterpretable features? We study this question by tasking 15 human users (5 users for each of the following 3 settings) to assign concepts to each feature z when given access to visualizations of (1) R ℓ (local analysis of unimodal and cross-modal interactions in z), (2) R ℓ + R g (no viz) (including global analysis through similar datapoints that also maximally activate feature z), and (3) R ℓ + R g (adding highlighted unimodal and cross-modal interactions of global datapoints). Using 20 datapoints per setting, these experiments with 15 users involve roughly 10 total hours of users interacting with MULTIVIZ.\n\nWhat color is the streak? Pred: white. Correct: red What is the chair made of? Pred: plastic. Correct: leather",
      "page_start": 99,
      "page_end": 99
    },
    {
      "section_name": "Prediction Errors Unimodal Perception Errors Cross-Modal Interaction Errors",
      "text": "From visualizing unimodal importance, the model fails to detect the streak.  Quantitative results: Since there are no ground-truth labels for feature concepts, we rely on annotator confidence (1-5 scale) and annotator agreement  [316]  as a proxy for accuracy. From Table  5 .3 (left), we find that having access to both local and global visualizations are crucial towards interpreting multimodal features, as measured by higher confidence with low variance in confidence, as well as higher agreement among users.",
      "page_start": 100,
      "page_end": 100
    },
    {
      "section_name": "From Visualizing Crossmodal Interactions, The Model Detects The Chair Accurately, But Misclassifies Its Material At The Reasoning Level. From Visualizing Crossmodal Interactions, The Model Fails To Capture The Interaction Between 'Creamy' And The Image.",
      "text": "Qualitative interviews:\n\nWe show examples of human-assigned concepts in Figure  5 .4. Note that the 3 images in each box of Figure  5 .4 (even without feature highlighting) does constitute a visualization generated by MULTIVIZ, as they belong to data instances that maximize the value of the feature neuron (i.e. R g in stage 3 multimodal representations). Without MULTIVIZ, it would not be possible to perform feature interpretation without combing through the entire dataset. Participants also noted that feature visualizations make the decision a lot more confident if its highlights match the concept. Taking as example Figure  5 .4 top left, the visualizations serve to highlight what the model's feature neuron is learning (i.e., highlighting the person holding sports equipment), rather than what category of datapoint it is. If the visualization was different, such as highlighting the ground, then users would have to conclude that the feature neuron is capturing 'outdoor ground' rather than 'sports equipment'. Similarly, for text highlights (Figure  5 .4 top right), without using MULTIVIZ to highlight 'counter', 'countertop', and 'wall', along with the image crossmodal interactions corresponding to these entities, one would not be able to deduce that the feature asks about material -it could also represent 'what' questions, or 'household objects', and so on. Therefore, these conclusions can only be reliably deduced with all MultiViz stages.",
      "page_start": 101,
      "page_end": 101
    },
    {
      "section_name": "Error Analysis",
      "text": "We examine a case study of error analysis on trained models. We task 10 human users (5 users for each of the following 2 settings) to use MULTIVIZ and highlight the errors that a model exhibits by categorizing these errors into one of 3 stages: failures in (1) unimodal perception, (2) capturing cross-modal interaction, and (3) prediction with perceived unimodal and cross-modal information. Again, we rely on annotator confidence (1-5 scale) and agreement due to lack of ground-truth error categorization, and compare (1) MULTIVIZ with (2) No viz, a baseline that does not provide any model visualizations to the user. Using 20 datapoints per setting, these experiments with 10 users on 2 datasets and 2 models involve roughly 15 total hours of users interacting with MULTIVIZ. From Table  5 .3 (right), we find that MULTIVIZ enables humans to consistently categorize model errors into one of 3 stages. We show examples that human annotators classified into unimodal perception, cross-modal interaction, and prediction errors in Figure  5 .5.",
      "page_start": 100,
      "page_end": 100
    },
    {
      "section_name": "A Case Study In Model Debugging",
      "text": "Following error analysis, we take a deeper investigation into one of the errors on a pretrained LXMERT model fine-tuned on VQA 2.0. Specifically, we first found the top 5 penultimate-layer neurons that are most activated on erroneous datapoints. Inspecting these neurons carefully through MULTIVIZ local and global representation analysis, human annotators found that 2 of the 5 neurons were consistently related to questions asking about color, which highlighted the model's failure to identify color correctly (especially blue). The model has an accuracy of only 5.5% amongst all blue-related points (i.e., either have blue as correct answer or predicted answer), and these failures account for 8.8% of all model errors. We show examples of such datapoints and their MULTIVIZ visualizations in Figure  5 .6. Observe that the model is often able to capture unimodal and cross-modal interactions perfectly, but fails to identify color at prediction.\n\nCurious as to the source of this error, we looked deeper into the source code for the entire pipeline of LXMERT, including that of its image encoder, Faster R-CNN  [507]  1 . We in fact uncovered a bug in data preprocessing for Faster R-CNN in the popular Hugging Face repository that swapped the image data storage format from RGB to BGR formats responsible for these errors. This presents a concrete use case of MULTIVIZ: through visualizing each stage, we were able to (1) isolate the source of the bug (at prediction and not unimodal perception or cross-modal interactions), and (2) use representation analysis to localize the bug to the specific color concept. In our full paper  [374] , we further detail our initial attempt at tackling this error by using MULTIVIZ analysis to select additional targeted datapoints in an active learning scenario, which proved to be much more effective (higher improvement with fewer data) as compared to baselines that add data randomly or via uncertainty sampling  [344] , which may be of independent interest.",
      "page_start": 101,
      "page_end": 102
    },
    {
      "section_name": "Additional Experiments And Takeaways Messages",
      "text": "New models: We included results on VILT  [307] , CLIP  [498] , and MDETR  [292]  in the full paper  [374] , showing that MULTIVIZ is a general approach that can be quickly applied to new models. We also study the correlation between performance and cross-modal interactions across several older and recent models, and find that the ability to capture cross-modal alignment, as judged by MULTIVIZ, correlates strongly with final task performance.\n\nSanity checks: In our full paper  [374] , we show that MULTIVIZ passes the data randomization and model randomization sanity checks for interpretation approaches  [6] .\n\nIntermediate-layer features: Finally, we show that MULTIVIZ can be extended to visualize any intermediate layer, not just the final layer of multimodal models. We showcase a few examples of R ℓ and R g on intermediate-layer neurons and discuss several tradeoffs: while they reveal new visualization opportunities, they run the risk of overwhelming the user with the number of images they have to see multiplied by d L (d: dimension of each layer, L: number of layers).",
      "page_start": 103,
      "page_end": 103
    },
    {
      "section_name": "Related Work",
      "text": "Interpretable ML aims to further our understanding and trust of ML models, enable model debugging, and use these insights for joint decision-making between stakeholders and AI  [104, 198] . Interpretable ML is a critical area of research straddling machine learning  [6] , language  [598] , vision  [549] , and HCI  [117] . We categorize related work in interpreting multimodal models into:\n\nUnimodal importance: Several approaches have focused on building interpretable components for unimodal importance through soft  [466]  and hard attention mechanisms  [101] . When aiming to explain black-box multimodal models, related work rely primarily on gradient-based visualizations  [40, 165, 549]  and feature attributions (e.g., LIME  [509] , Shapley values  [417] ) to highlight regions of the image which the model attends to.\n\nCross-modal interactions: Recent work investigates the activation patterns of pretrained transformers  [79, 349] , performs diagnostic experiments through specially curated inputs  [177, 320, 464, 602] , or trains auxiliary explanation modules  [293, 466] . Particularly related to our work is EMAP  [235]  for disentangling the effects of unimodal (additive) contributions from cross-modal interactions in multimodal tasks, as well as M2Lens  [655] , an interactive visual analytics system to visualize multimodal models for sentiment analysis through both unimodal and cross-modal contributions.\n\nMultimodal representation and prediction: Existing approaches have used language syntax (e.g., the question in VQA) for compositionality into higher-level features  [22, 26, 633] . Similarly, logical statements have been integrated with neural networks for interpretable logical reasoning  [203, 587] . However, these are typically restricted to certain modalities or tasks. Finally, visualizations have also uncovered several biases in models and datasets (e.g., unimodal biases in VQA questions  [24, 75]  or gender biases in image captioning  [231] ). We believe that MULTIVIZ will enable the identification of biases across a wider range of modalities and tasks.",
      "page_start": 102,
      "page_end": 103
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes MULTIVIZ for analyzing and visualizing multimodal models. MULTIVIZ scaffolds the interpretation problem into unimodal importance, cross-modal interactions, multimodal representations, and multimodal prediction, before providing existing and newly proposed analysis tools in each stage. MULTIVIZ is designed to be modular (encompassing existing analysis tools and encouraging research towards understudied stages), general (supporting diverse modalities, models, and tasks), and human-in-the-loop (providing a visualization tool for human model interpretation, error analysis, and debugging), qualities which we strive to upkeep by ensuring its public access and regular updates from community feedback.",
      "page_start": 103,
      "page_end": 103
    },
    {
      "section_name": "Chapter 6",
      "text": "Estimating Multimodal Performance and Modality Selection",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Introduction",
      "text": "To conclude the first part of this thesis, we provide a guideline for researchers to decide which modalities to collect that will lead to improved multimodal performance  [376] . Specifically, we study how to quantify interactions in a semi-supervised setting where there is only unlabeled multimodal data D M = {(x 1 , x 2 )} and some labeled unimodal data D i = {(x i , y)} collected separately for each modality. This multimodal semi-supervised paradigm is reminiscent of many real-world settings with separate unimodal datasets like visual recognition  [140]  and text classification  [643] , as well as naturally co-occurring multimodal data (e.g., news images and captions or video and audio), but when labeling them is time-consuming  [247, 250]  or impossible due to partially observed modalities  [370]  or privacy concerns  [90] . We want to understand how the modalities can share, exchange, and create information to inform practitioners whether it is worth collecting multimodal data and trying multimodal models  [283, 371, 713] .\n\nUsing a precise information-theoretic definition of interactions  [59] , our key contributions are the derivations of lower and upper bounds to quantify multimodal interactions in this semisupervised setting with only D i and D M . We propose two lower bounds: the first relates interactions with the amount of shared information between modalities, and the second is based on the disagreement of classifiers trained separately on each modality. Finally, we propose an upper bound through connections to approximate algorithms for min-entropy couplings  [118] . To validate our bounds, we experiment on both synthetic and large real-world datasets with varying amounts of interactions. In addition, these theoretical results naturally yield new guarantees regarding the performance of multimodal models. By analyzing the relationship between interaction estimates and downstream task performance assuming optimal multimodal classifiers are trained on labeled multimodal data, we can closely predict multimodal model performance, before even training the model itself. These performance estimates also help develop new guidelines for deciding when to collect additional modality data and select the appropriate multimodal fusion models. We believe these results shed light on the intriguing connections between multimodal interactions, modality disagreement, and model performance, and release our code and models at https://github.com/pliang279/PID.",
      "page_start": 104,
      "page_end": 105
    },
    {
      "section_name": "Related Work And Technical Background 6.2.1 Semi-Supervised Multimodal Learning",
      "text": "Let X i and Y be finite sample spaces for features and labels. Define ∆ to be the set of joint distributions over (X 1 , X 2 , Y). We are concerned with features X 1 , X 2 (with support X i ) and labels Y (with support Y) drawn from some distribution p ∈ ∆. We denote the probability mass function by p(x 1 , x 2 , y), where omitted parameters imply marginalization. Many realworld applications such as multimedia and healthcare naturally exhibit multimodal data (e.g., images and captions, video and audio, multimodal medical readings) which are difficult to label  [370, 498, 552, 704, 722] . As such, rather than the full distribution from p, we only have partial datasets:\n\nand D M follow the pairwise marginals p(x 1 , y), p(x 2 , y) and p(x 1 , x 2 ). We define\n\n} as the set of joint distributions which agree with the labeled unimodal data D 1 and D 2 , and ∆ p 1,2,12 = {r ∈ ∆ ∶ r(x 1 , x 2 ) = p(x 1 , x 2 ), r(x i , y) = p(x i , y)} as the set of joint distributions which agree with all D 1 , D 2 and D M .",
      "page_start": 106,
      "page_end": 106
    },
    {
      "section_name": "Multimodal Interactions And Information Theory",
      "text": "The study of multimodal interactions aims to quantify the information shared between both modalities, in each modality alone, and how modalities can combine to form new information not present in either modality, eventually using these insights to design machine learning models to capture interactions from large-scale multimodal datasets  [375] . Existing literature has primarily studied the interactions captured by trained models, such as using Shapley values  [272]  and Integrated gradients  [374, 580, 620]  to measure the importance a model assigns to each modality, or approximating trained models with additive or non-additive functions to determine what functions are best suited to capture interactions  [180, 235, 563] . However, these measure interactions captured by a trained model -our work is fundamentally different in that interactions are properties of data. Quantifying the interactions in data, independent of trained models, allows us to characterize datasets, predict model performance, and perform model selection, prior to choosing and training a model altogether. Prior work in understanding data interactions to design multimodal models is often driven by intuition, such as using contrastive learning  [487, 498, 609] , correlation analysis  [27] , and agreement  [147]  for shared information (e.g., images and descriptive captions), or using tensors and multiplicative interactions  [283, 713]  for higher-order interactions (e.g., in expressions of sarcasm from speech and gestures).\n\nTo fill the gap in data quantification, information theory has emerged as a theoretical foundation since it naturally formalizes information and its sharing as statistical properties of data distributions. Information theory studies the information that one random variable (X 1 ) provides about another (X 2 ), as quantified by Shannon's mutual information (MI) and conditional MI:\n\ndxdy.\n\nI(X 1 ; X 2 ) measures the amount of information (in bits) obtained about X 1 by observing X 2 , and by extension, I(X 1 ; X 2 |Y ) is the expected value of MI given the value of a third (e.g., task Y ).\n\nTo generalize information theory for multimodal interactions, Partial information decomposition (PID)  [663]  decomposes the total information that two modalities X 1 , X 2 provide about a task Y into 4 quantities:\n\nis the MI between the joint random variable (X 1 , X 2 ) and Y . These 4 quantities are: redundancy R for the task-relevant information shared between X 1 and X 2 , uniqueness U 1 and U 2 for the information present in only X 1 or X 2 respectively, and synergy S for the emergence of new information only when both X 1 and X 2 are present  [59, 210] : Definition 5. (Multimodal interactions) Given X 1 , X 2 , and a target Y , we define their redundant (R), unique (U 1 and U 2 ), and synergistic (S) interactions as:\n\nI q (X 2 ; Y |X 1 ), (6.1)\n\nwhere the notation I p (⋅) and I q (⋅) disambiguates mutual information (MI) under p and q respectively.\n\nis a multivariate extension of information theory  [51, 412] . Most importantly, R, U 1 , and U 2 can be computed exactly using convex programming over distributions q ∈ ∆ p 1,2 with access only to the marginals p(x 1 , y) and p(x 2 , y) by solving a convex optimization problem with linear marginal-matching constraints q * = arg max q∈∆p 1,2 H q (Y |X 1 , X 2 )  [59, 371] . This gives us an elegant interpretation that we need only labeled unimodal data in each feature from D 1 and D 2 to estimate redundant and unique interactions. Unfortunately, S is impossible to compute via equation (6.2) when we do not have access to the full joint distribution p, since the first term I p ({X 1 , X 2 }; Y ) is unknown.\n\nIt is worth noting that other valid information-theoretic definitions of multimodal interactions also exist, but are known to suffer from issues regarding over-and under-estimation, and may even be negative; these are critical problems with the application of information theory for shared I(X 1 ; X 2 ; Y ) and unique information I(X 1 ; Y |X 2 ), I(X 2 ; Y |X 1 ) often quoted in the co-training  [44, 65]  and multi-view learning  [564, 606, 609, 617]  literature. We refer the reader to Griffith and Koch  [210]  for a full discussion. We choose the one in Definition 5 above since it fulfills several desirable properties, but our results can be extended to other definitions as well.",
      "page_start": 105,
      "page_end": 106
    },
    {
      "section_name": "Estimating Semi-Supervised Multimodal Interactions",
      "text": "Our goal is to estimate multimodal interactions R, U 1 , U 2 , and S assuming access to only semisupervised multimodal data D 1 , D 2 , and D M . Our first insight is that while S cannot be computed exactly, R, U 1 , and U 2 can be computed from equation 6.1 with access to only semi-supervised data. Therefore, studying the relationships between S and other multimodal interactions is key to its estimation. Using these relationships, we will then derive lower and upper bounds for synergy in the form S ≤ S ≤ S. Crucially, S and S depend only on D 1 , D 2 , and D M .",
      "page_start": 106,
      "page_end": 106
    },
    {
      "section_name": "Understanding Relationships Between Interactions",
      "text": "We start by identifying two important relationships, between S and R, and between S and U .\n\nSynergy and redundancy Our first relationship stems from the case when two modalities contain shared information about the task. In studying these situations, a driving force for estimating S is the amount of shared information I(X 1 ; X 2 ) between modalities, with the intuition that more shared information naturally leads to redundancy which gives less opportunity for new synergistic interactions. Mathematically, we formalize this by relating S to R,\n\nimplying that synergy exists when there is high redundancy and low (or even negative) three-way MI I p (X 1 ; X 2 ; Y ). By comparing the difference in X 1 , X 2 dependence with and without the task (i.e., I p (X 1 ; X 2 ) vs I p (X 1 ; X 2 |Y )), 2 cases naturally emerge (see left side of Figure  6 .1):\n\n1. S > R: When both modalities do not share a lot of information as measured by low I(X 1 ; X 2 ), but conditioning on Y increases their dependence: I(X 1 ; X 2 |Y ) > I(X 1 ; X 2 ), then there is synergy between modalities when combining them for task Y . This setting is reminiscent of common cause structures. Examples of these distributions in the real world are multimodal question answering, where the image and question are less dependent (some questions like 'what is the color of the car' or 'how many people are there' can be asked for many images), but the answer (e.g., 'blue car') connects the two modalities, resulting in dependence given the label. As expected, S = 4.92, R = 0.79 for the VQA 2.0 dataset  [206] . 2. R > S: Both modalities share a lot of information but conditioning on Y reduces their dependence: I(X 1 ; X 2 ) > I(X 1 ; X 2 |Y ), which results in more redundant than synergistic information. This setting is reminiscent of common effect structures. A real-world example is in detecting sentiment from multimodal videos, where text and video are highly dependent since they are emitted by the same speaker, but the sentiment label explains away some of the dependencies between both modalities. Indeed, for multimodal sentiment analysis from text, video, and audio of monologue videos on MOSEI  [718] , R = 0.26 and S = 0.04.",
      "page_start": 107,
      "page_end": 108
    },
    {
      "section_name": "Synergy And Uniqueness",
      "text": "The second relationship arises when two modalities contain disagreeing information about the task, and synergy arises due to this disagreement in information.\n\nTo illustrate this, suppose y 1 = arg max y p(y|x 1 ) is the most likely prediction from the first modality, y 2 = arg max y p(y|x 2 ) for the second modality, and y = arg max y p(y|x 1 , x 2 ) is the true multimodal prediction. There are again 2 cases (see right side of Figure  6 .1): 1. U > S: Multimodal prediction y = arg max y p(y|x 1 , x 2 ) is the same as one of the unimodal predictions (e.g., y = y 2 ), in which case unique information in modality 2 leads to the outcome and there is no synergy. A real-world dataset is MIMIC involving mortality and disease prediction from tabular patient data and time-series medical sensors  [286]  which primarily shows unique information in the tabular modality. The disagreement on MIMIC is high at 0.13, but since disagreement is due to a lot of unique information, there is less synergy S = 0.01. 2. S > U: Multimodal prediction y is different from both y 1 and y 2 , then both modalities interact synergistically to give rise to a final outcome different from both disagreeing unimodal predictions. This type of joint distribution is indicative of real-world expressions of sarcasm from language and speech -the presence of sarcasm is typically detected due to a contradiction between what is expressed in language and speech, as we observe from the experiments on MUSTARD  [83]  where S = 0.44 and disagreement = 0.12 are both large.",
      "page_start": 107,
      "page_end": 108
    },
    {
      "section_name": "Lower And Upper Bounds On Synergy",
      "text": "Given these relationships between synergy and other interactions, we now derive bounds on S. We present two lower bounds S R and S U , which are based on redundancy and uniqueness, as well as an upper bound S. We also describe the computational complexity for computing each bound.\n\nRemark on high dimensional, continuous modalities. Our theoretical results are concerned with finite spaces for features and labels. However, this may be restrictive when working with real-world datasets (e.g., images, video, text) which are often continuous and/or high-dimensional. In such situations, we preprocess by performing discretization of each modality via clustering to estimate p(x 1 , y), p(x 2 , y), p(x 1 , x 2 ), each with a small, finite support. These are subsequently used for the computation of S R , S U and S. Discretization is a common way to approximate information theoretic quantities like mutual information  [130, 371]  and for learning representations over high-dimensional modalities  [453] .\n\nLower bound using redundancy Our first lower bound uses the relationship between synergy, redundancy, and dependence in equation 6.3. In semi-supervised settings, we can compute R exactly from p(x 1 , y), p(x 2 , y), as well as the shared information I(X 1 ; X 2 ) from p(x 1 , x 2 ). However, I p (X 1 ; X 2 |Y ) cannot be computed without access to the full distribution p. In Theorem 4, we obtain a lower bound on I p (X 1 ; X 2 |Y ), resulting in a lower bound S R for synergy. Theorem 4. (Lower-bound on synergy via redundancy) We relate S to modality dependence\n\nWe include a proof in the full paper  [376] . This bound compares S to R via the difference of their dependence I p (X 1 ; X 2 ) and their dependence given the task I p (X 1 ; X 2 |Y ). Since the full distribution p is not available to compute I p (X 1 ; X 2 |Y ), we prove a lower bound using conditional MI computed with respect to a set of auxiliary distributions r ∈ ∆ p 1,2,12 that are close to p, as measured by matching both unimodal marginals r(x i , y) = p(x i , y) and modality marginals r(x 1 , x 2 ) = p(x 1 , x 2 ). If conditioning on the task increases the dependence and I r (X 1 ; X 2 |Y ) is large relative to I p (X 1 ; X 2 ) then we obtain a larger value of S R , otherwise if conditioning on the task decreases the dependence and I r (X 1 ; X 2 |Y ) is small relative to I p (X 1 ; X 2 ) then we obtain a smaller value of S R .\n\nComputational complexity. R and min r∈∆p 1,2,12 I r (X 1 ; X 2 |Y ) are convex optimization problems solvable in polynomial time with off-the-shelf solvers. I p (X 1 ; X 2 ) can be computed directly.\n\nLower bound using uniqueness Our second bound formalizes the relationship between disagreement, uniqueness, and synergy. The key insight is that while labeled multimodal data is unavailable, the output of unimodal classifiers may be compared against each other. Consider unimodal classifiers f i ∶ X i → Y and multimodal classifiers f M ∶ X 1 × X 2 → Y. Define modality disagreement as: Definition 6. (Modality disagreement) Given X 1 , X 2 , and a target Y , as well as unimodal classifiers f 1 and f 2 , we define modality disagreement as\n\nis a distance function in label space scoring the disagreement of f 1 and f 2 's predictions.\n\nConnecting modality disagreement and synergy via Theorem 5 yields a lower bound S U : Theorem 5. (Lower-bound on synergy via uniqueness, informal) We can relate synergy S and uniqueness U to modality disagreement α(f 1 , f 2 ) of optimal unimodal classifiers f 1 , f 2 as follows:\n\nfor some constant c depending on the label dimension |Y| and choice of label distance function d. Theorem 5 implies that if there is substantial disagreement α(f 1 , f 2 ) between unimodal classifiers, it must be due to the presence of unique or synergistic information. If uniqueness is small, then disagreement must be accounted for by synergy, thereby yielding a lower bound S U . Note that the optimality of unimodal classifiers is important: poorly trained unimodal classifiers could show high disagreement but would be uninformative about true interactions. We include the formal version of the theorem based on Bayes' optimality and a proof in the full paper  [376] .\n\nComputational complexity. Lower bound S U can also be computed efficiently by estimating p(y|x 1 ) and p(y|x 2 ) over modality clusters or training unimodal classifiers f θ (y|x 1 ) and f θ (y|x 2 ). U 1 and U 2 can be computed using a convex solver in polynomial time.\n\nHence, the relationships between S, R, and U yield two lower bounds S R and S U . Note that these bounds always hold, so we could take S = max{S R , S U }.\n\nUpper bound on synergy By definition, S = I p ({X 1 , X 2 }; Y ) -R -U 1 -U 2 . However, I p ({X 1 , X 2 }; Y ) cannot be computed exactly without the full distribution p. Using the same idea as lower bound 1, we upper bound synergy by considering the worst-case maximum I r ({X 1 , X 2 }; Y ) computed over a set of auxiliary distributions r ∈ ∆ p 1,2,12 that match both unimodal marginals r(x i , y) = p(x i , y) and modality marginals r(x 1 , x 2 ) = p(x 1 , x 2 ):\n\nwhere the second line follows from the definition of ∆ p 1,2,12 . While the first two terms are easy to compute, the third may be difficult, as shown in the following theorem: Theorem 6. Solving r * = arg min r∈∆p 1,2,12 H r (X 1 , X 2 , Y ) is NP-hard, even for a fixed |Y| ≥ 4.\n\nTheorem 6 suggests we cannot tractably find a joint distribution which tightly upper bounds synergy when the feature spaces are large. Fortunately, a relaxation of r ∈ ∆ p 1,2,12 to r ∈ ∆ p 12,y , where r(x 1 , x 2 ) = p(x 1 , x 2 ) and r(y) = p(y), recovers the classic min-entropy coupling problem over (X 1 , X 2 ) and Y , which is still NP-hard but admits good approximations  [118, 119, 123, 309] . Our final upper bound S is:\n\nProofs of Theorem 6, 7, and detailed approximation algorithms for min-entropy couplings are included in the full paper  [376] .\n\nComputational complexity. The upper bound S can be computed efficiently since solving the variant of the min-entropy problem in Theorem 7 admits approximations that can be computed in time\n\nAll other entropy and R, U 1 , U 2 terms are easy to compute (or have been computed via convex optimization from the lower bounds).\n\nPractically, calculating all three bounds is extremely simple, with just a few lines of code. The computation takes < 1 minute and < 180 MB memory space on average for our large datasets (1, 000-20, 000 datapoints), more efficient than training even the smallest multimodal prediction model which takes at least 3x time and 15x memory. As a result, these bounds scale to large and high-dimensional multimodal datasets found in the real world, which we verify in the following experiments.",
      "page_start": 108,
      "page_end": 108
    },
    {
      "section_name": "Experiments",
      "text": "We design comprehensive experiments to validate these estimated bounds and relationships between different multimodal interactions. Using these results, we describe applications in estimating optimal multimodal performance before training the model itself, which can be used to guide data collection and select appropriate multimodal models for various tasks.",
      "page_start": 110,
      "page_end": 110
    },
    {
      "section_name": "Verifying Interaction Estimation In Semi-Supervised Learning",
      "text": "Synthetic bitwise datasets Let X 1 = X 2 = Y = {0, 1}. We generate joint distributions ∆ by sampling 100, 000 vectors from the 8-dim probability simplex and assigning them to p(x 1 , x 2 , y). Large real-world multimodal datasets We use a collection of 10 real-world datasets from MultiBench  [367]  which add up to a size of more than 700, 000 datapoints. 1. MOSI: 2, 199 videos for sentiment analysis  [711]  and screenshots  [340] . 7. IRFL: 6, 697 images and figurative captions (e.g, 'the car is as fast as a cheetah' describing an image with a fast car in it)  [701] . 8. NYCaps: 1, 820 New York Yimes cartoon images and humorous captions describing these images  [236] . 9. VQA: 614, 000 questions and answers about natural images  [29] . 10. ScienceQA: 21, 000 questions and answers about science problems with scientific diagrams  [392] . These high-dimensional and continuous modalities require approximating disagreement and mutual information: we train unimodal classifiers fθ (y|x 1 ) and fθ (y|x 2 ) to estimate disagreement, and we cluster modality features to approximate continuous modalities by discrete distributions with finite support to compute the lower and upper bounds. We summarize the following regarding the validity of each bound:\n\n1. Overall trends For the 100, 000 bitwise distributions, we compute S, the true value of synergy assuming oracle knowledge of the full multimodal distribution, and compute S R -S, S U -S, and S -S for each point. Plotting these points as a histogram in Figure  6 .2, we find that the two lower bounds track synergy from below (S R -S and S U -S approaching 0 from below), and the upper bound tracks synergy from above (S -S approaching 0 from above). The two lower bounds are quite tight, as we see that for many points S R -S and S U -S are approaching close to 0, with an average gap of 0.18. S U seems to be tighter empirically than S R : for half the points, S U is within 0.14 and S R is within 0.2 of S. For the upper bound, there is an average gap of 0.62. However, it performs especially well on high synergy data: when S > 0.6, the average gap is 0.24, with more than half of the points within 0.25 of S.\n\nOn real-world MultiBench datasets, we show the estimated bounds and actual S computed assuming knowledge of full p in Table  6 .1. The lower and upper bounds track true S: as estimated S R and S U increases from MOSEI to UR-FUNNY to MOSI to MUSTARD, true S also increases. For datasets like MIMIC with disagreement but high uniqueness, S U can be negative, but we can rely on S R to give a tight estimate on low synergy. Unfortunately, our bounds do not track synergy well on ENRICO. We believe this is because ENRICO displays all interactions: x 1 x 2 y p 0 0 0 0 0 0 1 0.05 0 1 0 0.03 0 1 1 0.28 1 0 0 0.53 1 0 1 0.03 1 1 0 0.01 1 1 1 0.06 x 1 x 2 y p 0 0 0 0.25 0 1 0 0.25 1 0 1 0.25 1 1 1 0.25\n\nx 1 x 2 y p 0 0 0 0.5 1 1 1 0.5\n\n0.53, S = 0.34, which makes it difficult to distinguish between R and S using S R or U and S using S U since no interaction dominates over others, and S is also quite loose. Given these general observations, we now carefully analyze the relationships between redundancy, uniqueness, and synergy.",
      "page_start": 111,
      "page_end": 111
    },
    {
      "section_name": "Guidelines",
      "text": "We provide a guideline to decide whether a lower or upper bound on synergy can be considered 'close enough'. It is close enough if the maximum interaction can be consistently estimated -often the exact value of synergy is not the most important (e.g, whether S is 0.5 or 0.6) but rather synergy relative to other interactions (e.g., if we estimate S ∈ [0.2, 0.5], and exactly compute R = U 1 = U 2 = 0.1, then we know for sure that S is the most important interaction and can collect data or design models based on that). We find that our bounds accurately identify the same highest interaction on all 10 real-world datasets as the true synergy does. Furthermore, we observed that the estimated synergy correlates very well with true synergy: as high as 1.05 on ENRICO (true S = 1.02) and as low as 0.21 on MIMIC (true S = 0.02).\n\n3. The relationship between S and R In Table  6 .2b we show the classic AGREEMENT XOR distribution where X 1 and X 2 are independent, but Y = 1 sets X 1 ≠ X 2 to increase their dependence. I(X 1 ; X 2 ; Y ) is negative, and S R = 1 ≤ 1 = S is tight. On the other hand, Table  6 .2d is an extreme example where the probability mass is distributed uniformly only when y = x 1 = x 2 and 0 elsewhere. As a result, X 1 is always equal to X 2 (perfect dependence), and yet Y perfectly explains away the dependence between X 1 and X 2 so I(X 1 ; X 2 |Y ) = 0: S R = 0 ≤ 0 = S. A real-world example is multimodal sentiment analysis from text, video, and audio on MOSEI, R = 0.26 and S = 0.03, and as expected the lower bound is small S R = 0 ≤ 0.03 = S (Table  6 .1).\n\n4. The relationship between S and U In Table  6 .2a we show an example called DISAGREE-MENT XOR. There is maximum disagreement between p(y|x 1 ) and p(y|x 2 ): the likelihood for y is high when y is the opposite bit as x 1 , but reversed for x 2 . Given both x 1 and x 2 : y takes a 'disagreement' XOR of the individual marginals, i.e. p(y|x 1 , x 2 ) = arg max y p(y|x 1 ) XOR arg max y p(y|x 2 ), which indicates synergy (note that an exact XOR would imply perfect agreement and high synergy). The actual disagreement is 0.15, S is 0.16, and U is 0.02, indicating a very strong lower bound S U = 0.14 ≤ 0.16 = S. A real-world equivalent dataset is MUSTARD, where the presence of sarcasm is often due to a contradiction between what is expressed in language and speech, so disagreement α = 0.12 is the highest out of all the video datasets, giving a lower bound S U = 0.11 ≤ 0.44 = S. The lower bound is low when all disagreement is explained by uniqueness (e.g., y = x 1 , Table  6 .2c), which results in S U = 0 ≤ 0 = S (α and U cancel each other out). A real-world equivalent is MIMIC: from Table  6 .1, disagreement is high α = 0.13 due to unique information U 1 = 0.25, so the lower bound informs us about the lack of synergy S U = -0.12 ≤ 0.02 = S. Finally, the lower bound is loose when there is synergy without disagreement, such as AGREEMENT XOR (y = x 1 XOR x 2 , Table  6 .2b) where the marginals p(y|x i ) are both uniform, but there is full synergy: S U = 0 ≤ 1 = S. Real-world datasets include UR-FUNNY where there is low disagreement in predicting humor α = 0.03, and relatively high synergy S = 0.18, which results in a loose lower bound S U = 0.01 ≤ 0.18 = S.",
      "page_start": 112,
      "page_end": 113
    },
    {
      "section_name": "On Upper Bounds For Synergy",
      "text": "The upper bound for MUSTARD is close to real synergy, S = 0.79 ≥ 0.44 = S. On MIMIC, the upper bound is the lowest S = 0.41, matching the lowest S = 0.02. Some of the other examples in Table  6 .1 show weaker bounds. This could be because (i) there exists high synergy distributions that match D i and D M , but these are rare in the real world, or (ii) our approximation used in Theorem 7 is loose. We leave these as directions for future work.",
      "page_start": 113,
      "page_end": 113
    },
    {
      "section_name": "Additional Results",
      "text": "In the full paper  [376] , we also study the effect of imperfect unimodal predictors and disagreement measurements on our derived bounds, by perturbing the label by various noise levels (from no noise to very noisy) and examining the changes in estimated upper and lower bounds. We found these bounds are quite robust to label noise, still giving close trends of S. We also include more discussions studying the relationships between various interactions, and how the relationship between disagreement and synergy can inspire new self-supervised learning methods.",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Implications Towards Performance, Data Collection, Model Selection",
      "text": "Now that we have validated the accuracy of these bounds, we apply them to estimate multimodal performance in semi-supervised settings. This serves as a strong signal for deciding (1) whether to collect paired and labeled data from a second modality, and (  2 ) what type of multimodal fusion method should be used. To estimate performance given D 1 , D 2 , and D M , we first compute our lower and upper bounds S and S. Combined with the exact computation of R and U , we obtain the total information I p ({X 1 , X 2 }; Y ), and combine a result from Feder and Merhav  [172]  with Fano's inequality  [170]  to yield tight bounds of performance as a function of total information. Theorem 8.\n\n] denote the accuracy of the Bayes' optimal multimodal model f * M (i.e., P acc (f * M ) ≥ P acc (f ′ M ) for all f ′ M ∈ F M ). We have that\n\nand we can plug in\n\nS to obtain lower P acc (f * M ) and upper P acc (f * M ) bounds on optimal multimodal performance. We show the proof in the full paper  [376] . Finally, we summarize estimated multimodal performance as the average PM = (P acc (f * M ) + P acc (f * M ))/2. A high PM suggests the presence of important joint information from both modalities (not present in each) which could boost accuracy, so it is worthwhile to collect the full distribution p and explore multimodal fusion.\n\nSetup For each MultiBench dataset, we implement a suite of unimodal and multimodel models spanning simple and complex fusion. Unimodal models are trained and evaluated separately on each modality. Simple fusion includes ensembling by taking an additive or majority vote between unimodal models  [228] . Complex fusion is designed to learn higher-order interactions as exemplified by bilinear pooling  [182] , multiplicative interactions  [283] , tensor fusion  [713] , and cross-modal self-attention  [614] . See our full paper  [376]  for models and training details. We include unimodal, simple and complex multimodal performance, as well as estimated lower and upper bounds on performance in Table  6 .3.",
      "page_start": 114,
      "page_end": 115
    },
    {
      "section_name": "Rq1: Estimating Multimodal Fusion Performance How Well Could My Multimodal Model Perform?",
      "text": "We find that estimating interactions enables us to closely predict multimodal model performance, before even training a model. For example, on MOSEI, we estimate the performance to be 52% based on the lower bound and 107% based on the upper bound, for an average of 80% which is very close to true model performance ranging from 82% for the best unimodal model, and 85% -88% for various multimodal model. Estimated performances for ENRICO, UR-FUNNY, and MOSI are 68%, 90%, 96%, which track true performances 51%, 77%, 86%. RQ2: Data collection Should I collect multimodal data? We compare estimated performance PM with the actual difference between unimodal and best multimodal performance in Figure  6 .3 (left). Higher estimated PM correlates with a larger gain from unimodal to multimodal (correlation ρ = 0.21 and rises to 0.53 if ignoring the outlier in MIMIC). MUSTARD and ENRICO show the most opportunity for multimodal modeling. Therefore, a rough guideline is that if the estimated multimodal performance based on semi-supervised data is higher, then collecting the full labeled multimodal data is worth it.",
      "page_start": 114,
      "page_end": 114
    },
    {
      "section_name": "Rq3: Model Selection What Model Should I Choose For Multimodal Fusion?",
      "text": "We note strong relationships between estimated performance and the performance of different fusion methods. From Table  6 .3, synergistic datasets like MUSTARD and ENRICO show best multimodal performance only slightly above our estimated lower bound, indicating that there is a lot of room for improvement in better fusion methods. Indeed, more complex fusion methods such as multimodal transformer designed to capture synergy is the best on MUSTARD which matches its high synergy (72% accuracy). For datasets with less synergy like MOSEI and MIMIC, the best multimodal performance is much higher than the estimated lower bound, indicating that existing fusion methods may already be quite optimal. Indeed, simpler fusion methods such as feature alignment, designed to capture redudnancy, are the best on MOSEI which matches its high redundancy (80% accuracy). Figure  6 .3 (right) shows a visual comparison, where plotting the performance gap between complex and simple fusion methods against estimated performance PM shows a correlation coefficient of 0.77. We again observe positive trends between higher estimated performance and improvements with complex fusion, with large gains on MUSTARD and ENRICO. We expect new methods to further improve the state-of-the-art on these datasets due to their generally high interaction values and low multimodal performance relative to estimated lower bound P acc (f * M ). Therefore, a rough guideline is that if the estimated multimodal performance based on semisupervised data is higher, then there is more potential for improvement by trying more complex multimodal fusion strategies.",
      "page_start": 115,
      "page_end": 115
    },
    {
      "section_name": "Conclusion And Broader Impacts",
      "text": "We proposed estimators of multimodal interactions when observing only labeled unimodal data and some unlabeled multimodal data, a general semi-supervised setting that encompasses many real-world constraints involving partially observable modalities, limited labels, and privacy con-cerns. Our key results draw new connections between multimodal interactions, the disagreement of unimodal classifiers, and min-entropy couplings, which yield new insights for estimating multimodal model performance, data analysis, and model selection. We are aware of some potential limitations: 1. These estimators only approximate real interactions due to cluster preprocessing or unimodal models, which naturally introduce optimization and generalization errors. We expect progress in density estimators, generative models, and unimodal classifiers to address these problems. 2. It is harder to quantify interactions for certain datasets, such as ENRICO which displays all interactions which makes it difficult to distinguish between R and S or U and S. 3. Finally, there exist challenges in quantifying interactions since the data generation process is never known for real-world datasets, so we have to resort to human judgment, other automatic measures, and downstream tasks such as estimating model performance and model selection.\n\nFuture work should investigate more applications of multivariate information theory in designing self-supervised models, predicting multimodal performance, and other tasks involving feature interactions such as privacy-preserving and fair representation learning from high-dimensional data  [161, 219] . Being able to provide guarantees for fairness and privacy-preserving learning, especially for semi-supervised pretraining datasets, can be particularly impactful.",
      "page_start": 115,
      "page_end": 116
    },
    {
      "section_name": "Introduction",
      "text": "Current multimodal research has led to impressive advances in benchmarking and modeling for specific domains such as language and vision  [11, 360, 381, 503] . However, other domains, modalities, and tasks are relatively understudied. The future will lie in multisensory foundation models that are grounded in the world: being able to simultaneously process a large number of modalities beyond language, to vision, audio  [11, 360, 381, 503] , and leveraging advances in sensing technologies such as cellphones  [366] , wearable devices  [218] , autonomous vehicles  [698] , healthcare technologies  [287] , and robots  [53, 304]  that give a wealth of sensor data about the world.\n\nMULTIBENCH: In order to accelerate research in building general-purpose multimodal foundation models, this chapter describes MULTIBENCH (Figure  7 .1), a systematic and unified large-scale benchmark that brings us closer to the requirements of real-world multimodal applications. MULTIBENCH is designed to comprehensively evaluate generalization across domains and modalities. To that end, MULTIBENCH contains a diverse set of 28 datasets spanning 14 modalities and testing for more than 30 prediction tasks across 6 distinct research areas and 5 technical challenges of multimodal machine learning. These research areas include important tasks understudied from a multimodal learning perspective, such as healthcare, finance, and HCI. Building upon extensive data-collection efforts by domain experts, we worked with them to adapt datasets that reflect real-world relevance, present unique challenges to multimodal learning, and enable opportunities in algorithm design and evaluation.\n\nTogether, MULTIBENCH unifies efforts across separate research areas in multimodal learning to enable quick and accurate benchmarking across a wide range of datasets and metrics. To help the community accurately compare performance and ensure reproducibility, MULTIBENCH includes an end-to-end pipeline including data preprocessing, dataset splits, multimodal algorithms, evaluation metrics, and cross-validation protocols. This includes an implementation of 20 core multimodal approaches spanning innovations in fusion paradigms, optimization objectives, and training approaches in a standard public toolkit called MULTIZOO. We perform a systematic eval- uation and show that directly applying these methods can improve the state-of-the-art performance on 9 out of the 15 datasets. Therefore, MULTIBENCH presents a step towards unifying disjoint efforts in multimodal research and paves a way towards a deeper understanding of multimodal models. Most importantly, our public zoo of multimodal benchmarks and models will ensure ease of use, accessibility, and reproducibility. Finally, we outline our plans to ensure the continual availability, maintenance, and expansion of MULTIBENCH, including using it as a theme for future workshops and competitions and to support the multimodal learning courses taught around the world.",
      "page_start": 117,
      "page_end": 118
    },
    {
      "section_name": "Multibench: The Multiscale Multimodal Benchmark",
      "text": "Background: We define a modality as a single particular mode in which a signal is expressed or experienced. Multiple modalities then refer to a combination of multiple heterogeneous signals  [46] . The first version of MULTIBENCH focuses on benchmarking algorithms for multimodal fusion, where the main challenge is to join information from two or more modalities to perform a prediction (e.g., classification, regression). Classic examples for multimodal fusion include audio-visual speech recognition where visual lip motion is fused with speech signals to predict spoken words  [160] . Multimodal fusion can be contrasted with multimodal translation where the goal is to generate a new and different modality  [641] , grounding and question answering where one modality is used to query information in another (e.g., visual question answering  [11] ), and unsupervised or self-supervised multimodal representation learning  [390, 572] . We plan future versions of MULTIBENCH to study these important topics in multimodal research.\n\nEach of the following 15 datasets in MULTIBENCH contributes a unique perspective to the various technical challenges in multimodal learning involving learning and aligning complemen-Table  7 .1: MULTIBENCH provides a comprehensive suite of 28 multimodal datasets to benchmark current and proposed approaches in multimodal machine learning. It covers a diverse range of technical challenges, research areas, dataset sizes, input modalities (in the form of a: audio, e: embodied environment, f : force sensor, g: graph, i: image ℓ: language, o: optical flow, p: proprioception sensor, π: policy/action, q: question (for question-answering tasks), s: set, t: time-series, ta: tabular, v: video), and prediction tasks. We provide a standardized data loader for datasets in MULTIBENCH, along with a set of state-of-the-art multimodal models.   [317, 409]  {i, g} → i 100, 000 knowledge graph → image",
      "page_start": 118,
      "page_end": 119
    },
    {
      "section_name": "Multibench Datasets",
      "text": "",
      "page_start": 119,
      "page_end": 119
    },
    {
      "section_name": "Multibench Data Loader",
      "text": "",
      "page_start": 119,
      "page_end": 119
    },
    {
      "section_name": "Multizoo Model",
      "text": "",
      "page_start": 119,
      "page_end": 119
    },
    {
      "section_name": "Multibench Evaluator",
      "text": "MultiBench leaderboard  Figure 7 .2: MULTIBENCH provides a standardized machine learning pipeline across data processing, data loading, multimodal models, evaluation metrics, and a public leaderboard to encourage future research in multimodal representation learning. MULTIBENCH aims to present a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility.\n\ntary information, scalability to a large number of modalities, and robustness to realistic real-world imperfections.\n\nMULTIBENCH provides a standardized machine learning pipeline that starts from data loading to running multimodal models, providing evaluation metrics, and a public leaderboard to encourage future research in multimodal representation learning (see Figure  7 .2). Table  7 .1 shows an overview of these datasets. We provide a brief overview of the research areas, modalities, and tasks for each of these datasets.",
      "page_start": 120,
      "page_end": 120
    },
    {
      "section_name": "Research Areas",
      "text": "Affective computing studies the perception of human affective states (emotions, sentiment, and personalities) from our display of multimodal signals spanning language (spoken words), visual (facial expressions, gestures), and acoustic (prosody, speech tone)  [484] . It has impacts towards building emotionally intelligent computers, human behavior analysis, and AI-assisted education.\n\nHealthcare: Modern medical decision-making often involves integrating complementary information and signals from several sources such as lab tests, imaging reports, and patient-doctor conversations. Multimodal models can help doctors make sense of high-dimensional data and assist them in the diagnosis process  [21] .\n\nRobotics: Modern robot systems are equipped with multiple sensors to aid in their decisionmaking. Some systems also have a large number of heterogeneous sensors deployed in the real world with realistic noise and imperfections. These present scalability and robustness challenges for multimodal machine learning.\n\nFinance: The field of machine learning for finance studies the use of algorithms to make better automatic trading decisions through historical data, news and document understanding, social media analytics, and other multimodal signals. This field presents challenges in timeseries analysis on high-frequency multimodal signals, a dynamic and large number of possible modalities, as well as robustness and compute efficiency for real-world deployment.\n\nHuman Computer Interaction (HCI) studies the design of computer technology and interactive interfaces between humans and computers  [151] . Many real-world human-centric problems involve multimodal inputs such as language, visual, and audio interfaces. Designing multimodal models that actively interact with humans further necessitates guarantees on their fairness and robustness in real-world scenarios.\n\nMultimedia: A significant body of research in multimodal learning has been fueled by the large availability of multimedia data (language, image, video, and audio) on the internet. Multimedia research is exemplified by the research tasks of media description, multimodal question answering, and cross-modal retrieval.\n\nSimulated environments: Finally, simulated interactive environments such as Atari games  [52] , Minecraft  [216] , and NetHack  [323]  present scalable opportunities for research in reinforcement learning while also enabling rich programming of multimodal environments involving text  [394] , audio  [135] , and video  [88] . By way of their flexible design, these environments can often provide richer interactions between text and embodied environments, more difficult planning and exploration challenges, and procedurally generated tasks of increasing difficulty.",
      "page_start": 120,
      "page_end": 121
    },
    {
      "section_name": "Fusion Datasets",
      "text": "In multimodal fusion, the main challenge is to join information from two or more modalities to perform a prediction. Classic examples include audio-visual speech recognition where visual lip motion is fused with speech signals to predict spoken words  [160] . Information coming from different modalities have varying predictive power by themselves and also when complemented by each other (i.e., higher-order interactions). In order to capture higher-order interactions, there is also a need to identify the relations between granular units from two or more different modalities (i.e., alignment). When dealing with temporal data, it also requires capturing possible longrange dependencies across time (i.e., temporal alignment). MULTIBENCH contains the following datasets for multimodal fusion spanning several research areas:\n\nAffective computing: MULTIBENCH contains 4 datasets involving fusing language, video, and audio time-series data to predict sentiment (CMU-MOSI  [711] ), emotions (CMU-MOSEI  [718] ), humor (UR-FUNNY  [225] ), and sarcasm (MUSTARD  [83] ). Complementary information may occurs at different moments, requiring models to address the multimodal challenges of grounding and alignment.\n\nHealthcare: MULTIBENCH includes the large-scale MIMIC dataset  [287]  which records ICU patient data including time-series data measured every hour and other demographic variables (e.g., age, gender, ethnicity in the form of tabular numerical data). These are used to predict the disease ICD-9 code and mortality rate. MIMIC poses unique challenges in integrating time-varying and static modalities, reinforcing the need of aligning multimodal information at correct granularities. Extending MIMIC, we also include the MIMIC-CXR  [288]  datasets of de-identified publicly available chest radiographs and free-text reports Robotics: We include MUJOCO PUSH  [334]  and VISION&TOUCH  [335]  which record the manipulation of simulated and real robotic arms equipped with visual (RGB and depth), force, and proprioception sensors. In MUJOCO PUSH, the goal is to predict the pose of the object being pushed by the robot end-effector. In VISION&TOUCH, the goal is to predict action-conditional learning objectives that capture forward dynamics of contact prediction and robot end-effector pose. Robustness is important due to the risk of real-world sensor failures  [336] .\n\nFinance: We gathered historical stock data from the internet to create our own dataset for financial time-series prediction across 3 groups of correlated stocks: STOCKS-F&B, STOCKS-HEALTH, and STOCKS-TECH. Within each group, the previous stock prices of a set of stocks are used as multimodal time-series inputs to predict the price and volatility of a related stock (e.g., using Apple, Google, and Microsoft data to predict future Microsoft prices). Multimodal stock prediction  [521]  presents scalability issues due to a large number of modalities (18/63/100 vs 2/3 in most datasets), as well as robustness challenges arising from real-world data with an inherently low signal-to-noise ratio.\n\nHCI: We use the ENRICO (Enhanced Rico) dataset  [137, 340]  of Android app screens (consisting of an image as well as a set of apps and their locations) categorized by their design motifs and collected for data-driven design applications such as design search, user interface (UI) layout generation, UI code generation, and user interaction modeling.\n\nMultimedia: MULTIBENCH includes 4 popular large-scale multimedia datasets with varying sizes and levels of difficulty: (1) the hateful memes challenge  [301]  as a core challenge in multimedia to ensure safer learning from ubiquitous text and images from the internet, (2) AV-MNIST  [639]  is assembled from images of handwritten digits  [332]  and audio samples of spoken digits  [341] , (3) MM-IMDB  [32]  uses movie titles, metadata, and movie posters to perform multi-label classification of movie genres, and (4) KINETICS  [296]  contains video, audio, and optical flow of 306, 245 video clips annotated for 400 human actions.",
      "page_start": 121,
      "page_end": 121
    },
    {
      "section_name": "Question Answering Datasets",
      "text": "Within the domain of language and vision, there has been growing interest in language-based question answering (i.e., \"query\" modality) of entities in the visual, video, or embodied domain (i.e., \"queried\" modality). Datasets such as Visual Question Answering  [11] , Social IQ  [716] , and Embodied Question Answering  [131]  have been proposed to benchmark the performance of multimodal models in these settings. A core challenge lies in aligning words asked in the question with entities in the queried modalities, which typically take the form of visual entities in images or videos (i.e., alignment). MULTIBENCH contains the following datasets for multimodal question answering spanning several research areas:\n\nAffective computing: SOCIAL IQ  [716]  is an unconstrained benchmark specifically designed to train and evaluate socially intelligent AI through a rich source of open-ended questions and answers. It contains 1, 250 videos of natural social situations, 7, 500 questions and 52, 500 correct and incorrect answers Multimedia: CLEVR  [289]  is a diagnostic dataset for studying the ability of VQA systems to perform visual reasoning. It contains 100, 000 rendered images and about 853, 000 unique automatically generated questions that test visual reasoning abilities such as counting, comparing, logical reasoning, and storing information in memory. VQA 2.0  [206]  is a balanced version of the popular VQA  [11]  dataset by collecting complementary images such that every question is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. The reduces the occurrence of spurious correlations in the dataset and enables training of more robust models.",
      "page_start": 122,
      "page_end": 122
    },
    {
      "section_name": "Retrieval Datasets",
      "text": "Another area of great interest lies in cross-modal retrieval  [369, 733] , where the goal is to retrieve semantically similar data from a new modality using a modality as a query (e.g., given a phrase, retrieve the closest image describing that phrase). The core challenge is to perform alignment of representations across both modalities. MULTIBENCH contains the following datasets for multimodal retrieval and grounding:\n\nMultimedia: CIFAR-ESC  [369]  is an image-audio retrieval dataset constructed by combining CIFAR-100, CIFAR-10  [319] , and ESC-50  [485]  into 17 shared classes using concept ontologies from WordNet  [420] . CLOTHO  [156]  is a dataset for audio captioning with 4981 audio samples of 15 to 30 seconds duration and 24, 905 captions of 8 to 20 words length. YUMMLY-28K  [421]  contains parallel text descriptions and images of recipes with 27, 638 recipes in total. Each recipe contains one recipe image, the ingredients, the cuisine and the course information. FLICKR-30K  [486]  contains 32, 000 images collected from Flickr, together with 5 reference sentences provided by human annotators enabling the tasks of text-to-image reference resolution, localizing textual entity mentions in an image, and bidirectional image-caption retrieval.",
      "page_start": 122,
      "page_end": 123
    },
    {
      "section_name": "Reinforcement Learning Environments",
      "text": "Learning from multiple modalities in an interactive setting is an area of interest towards building more intelligent embodied agents that can perceive the visual world, language instructions, auditory feedback, and other sensor modalities  [394] . Recent work has also explored audio as a modality in an agent's multisensory interaction with the world  [135] . These multimodal problems are fundamentally different from those that are concerned with prediction tasks. Alongside the core challenges in learning complementary information and aligning entities in language instructions to those in the visual environment, there also lies the core challenge of learning actionable representations that link to the set of actions that can be taken and their associated longterm rewards  [394] . MULTIBENCH contains the following datasets for multimodal reinforcement learning in both real-world and simulated environments:\n\nSimulated environments: We choose the RTFM  [734]  (Reading to Fight Monsters) simulated text and visual environment. RTFM requires an agent to jointly reason over a language goal, a document that specifies environment dynamics, and environment observations. It can also be procedurally generated for increasing difficult interactions between environment dynamics and natural language. RTFM is also part of the larger SILG benchmark  [735]  of 5 similar diverse grounded language learning environments under a common interface, so it enables generalization to these other environments as well.",
      "page_start": 123,
      "page_end": 124
    },
    {
      "section_name": "Co-Learning Datasets",
      "text": "Co-learning aims to transfer knowledge between modalities and their representations. Exemplified by algorithms of fine-tuning, co-training, and contrastive learning, how can knowledge learned from an additional secondary modality (e.g., predicted labels or representation) help a computational model trained on a primary modality? This challenge is particularly relevant when the primary modality has limited resources such as lack of annotated data, noisy input, and unreliable labels.\n\nAffective computing: In affective computing, we investigate transferring information from CMU-MOSI to SST, as well as the larger CMU-MOSEI to SST  [717] . The former 2 are multimodal (language + vision + audio) datasets annotated for sentiment, while SST is a languageonly sentiment analysis dataset. Multimedia: In multimedia, we transfer information from GLOVE word embeddings for CIFAR10 image classification  [554] . We also transfer information from knowledge graphs to image classification by providing the Visual Genome dataset  [317, 409] .",
      "page_start": 123,
      "page_end": 123
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "MULTIBENCH provides standardized evaluation using metrics designed for each dataset, ranging from MSE and MAE for regression to accuracy, micro & macro F1-score, and AUPRC for classification on each dataset. To assess for generalization, we compute the variance of a particular model's performance across all datasets in MULTIBENCH on which it is tested. We split these results on multiple datasets into in-domain datasets and out-domain datasets. In-domain datasets refer to model performance on datasets that it was initially proposed and tested on, while outdomain datasets refer to model performance on the remaining datasets. Comparing out-domain vs in-domain performance, as well as variance in performance across datasets as a whole, allow us to summarize the generalization statistics of each multimodal model.",
      "page_start": 124,
      "page_end": 124
    },
    {
      "section_name": "Multizoo: A Zoo Of Multimodal Algorithms",
      "text": "To complement MULTIBENCH, we release a comprehensive toolkit, MULTIZOO, as starter code for multimodal algorithms which implements 20 methods spanning different methodological innovations in (1) data preprocessing, (2) fusion paradigms, (3) optimization objectives, and (4) training procedures (see Figure  7 .3). To introduce these algorithms, we use the simple setting with 2 modalities for notational convenience. We use x 1 , x 2 for input modalities, z 1 , z 2 for unimodal representations, z mm for the multimodal representation, and ŷ for the predicted label.",
      "page_start": 124,
      "page_end": 124
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Temporal alignment  [101]  has been shown to help tackle the multimodal alignment problem for time-series data. This approach assumes a temporal granularity of the modalities (e.g., at the level of words for text) and aligns information from the remaining modalities to the same granularity. We call this approach WORDALIGN  [101]  for temporal data where text is one of the modalities.",
      "page_start": 125,
      "page_end": 125
    },
    {
      "section_name": "Fusion Paradigms",
      "text": "Early and late fusion have been the de-facto first-approach when tackling new multimodal problems. Early fusion performs concatenation at the input data level before using a suitable prediction model (i.e., z mm = [x 1 , x 2 ]) and late fusion applies suitable unimodal models to each modality to obtain their feature representations, concatenates these features, and defines a classifier to the label (i.e., z mm = [z 1 , z 2 ])  [46] . MULTIZOO includes their implementations denoted as EF and LF respectively.\n\nTensors are specifically designed to tackle the multimodal complementarity challenge by explicitly capturing higher-order interactions across modalities  [713] . Given unimodal representations z 1 , z 2 , a multimodal tensor representation is defined as\n\n] where ⊗ denotes an outer product. However, computing tensor products is expensive since their dimension scales exponentially with the number of modalities. Several efficient variants have been proposed to approximate expensive full tensor products with cheaper variants while maintaining performance  [245, 364, 388] . MULTIZOO includes Tensor Fusion (TF)  [713]  as well as approximate Low-rank Tensor Fusion (LRTF)  [388] . As future work, we also plan to include more expressive higher-order tensor fusion methods  [245] .\n\nMultiplicative Interactions (MI) further generalize tensor products to include learnable parameters that capture the interactions between streams of information  [284] . In its most general form, MI defines a bilinear product z mm = z 1 Wz 2 + z ⊺ 1 U + Vz 2 + b where W, U, Z, and b are trainable parameters. By appropriately constraining the rank and structure of these parameters, MI recovers HyperNetworks  [217]  (unconstrained parameters resulting in a matrix output), Featurewise linear modulation (FiLM)  [477, 734]  (diagonal parameters resulting in vector output), and Sigmoid units  [133]  (scalar parameters resulting in scalar output). MULTIZOO includes all 3 as MI-MATRIX, MI-VECTOR, and MI-SCALAR respectively.\n\nWe also referred to the implementation of Feature-wise linear modulation (FiLM)  [477]  and added it as a module in MULTIBENCH, which we call FILM. While MI-VECTOR (i.e., diagonal parameters in a MI layer which results in a vector output) corresponds to the most basic implementation of FILM, the original FILM layer uses multiple non-linear layers instead of a single linear transformation in MI-VECTOR which has been shown to improve performance  [477] .\n\nMultimodal gated units are prevalent in learning combinations of two representations that dynamically change for every input  [88, 652, 681] . Its general form can be written as z mm = z 1 ⊙ h(z 2 ), where h represents a function with sigmoid activation and ⊙ denotes the element-wise product. The output h(z 2 ) is commonly referred to as \"attention weights\" learned from z 2 used to attend on z 1 . We implement the Query-Key-Value mechanism as NL GATE as proposed in  [652] . This attention mechanism is conceptually similar to the MI-VECTOR case above but recent work has explored more expressive forms of h such as using a Query-Key-Value mechanism  [652]  or several fully-connected layers  [88]  rather than a linear transformation in MI-VECTOR.\n\nMultimodal transformers are useful in tackling the challenge of multimodal alignment and complementarity. Transformer models  [632]  have been shown to be useful for temporal multimodal data by automatically aligning and capturing complementary features at different time-steps  [614, 695] . We include the Multimodal Transformer (MULT)  [614]  which uses a Crossmodal Transformer block that uses z 1 to attend to z 2 (and vice-versa), before concatenating both representations to obtain\n\nTo extend this to 3 modalities, the crossmodal transformer block is repeated across all 3 sets of modality pairs (i.e., z mm = [z 1→2 , z 2→1 , z 1→3 , z 3→1 , z 2→3 , z 3→2 ]). While this is still computationally feasible for 3 modalities such as the language, video, and audio datasets that MULT was originally designed for, this quickly becomes intractable for problems involving more than 3 modalities. To adapt MULT for the financial prediction task involving more than 10 modalities, we cluster all modalities into 3 groups based on similarities in their data and perform early fusion on the data within each cluster before applying MULT only on the 3 clusters of modalities. While MULT is a strong model based on performance, it may pose scalability issues since the number of cross-modal attention blocks grows quadratically with the number of modalities.\n\nArchitecture search: Finally, instead of hand-designing multimodal architectures, several approaches define a set of atomic neural operations (e.g., linear transformation, activation, attention, etc.) and use architecture search to automatically learn the best order of these operations for a given multimodal task  [479, 683] . We focus on the more general approach, MFAS  [479] , designed for language and vision datasets. While this approach is categorized under innovations in model architecture (since it primarily targets better architectures for multimodal fusion), its code in the MULTIZOO toolkit is implemented under training structures, since architecture search requires an outer loop to learn model architectures over multiple inner supervised learning loops that train an individual model architecture. Therefore, we are unable to integrate MFAS directly with the basic supervised learning training loops like we do for the other fusion paradigms described above.",
      "page_start": 125,
      "page_end": 126
    },
    {
      "section_name": "Optimization Objectives",
      "text": "In addition to the standard supervised losses (e.g., cross entropy for classification, MSE/MAE for regression), several proposed methods have proposed new objective functions based on:\n\nPrediction-level alignment: There has been extensive research in defining objective functions to tackle the challenge of multimodal alignment: capturing a representation space where semantically similar concepts from different modalities are close together. While primarily useful for cross-modal retrieval  [369, 733] , recent work has also shown its utility in learning representations for prediction  [39, 126, 335, 605] . These alignment objectives have been applied at both prediction and feature levels. In the former, we implement Canonical Correlation Analysis (CCA)  [27, 651] , which computes L CCA = corr (g 1 (z 1 ), g 2 (z 2 )) where g 1 , g 2 are auxiliary classifiers mapping each unimodal representation to the label. This method corresponds to prediction-level alignment since they aim to learn representations of each modality that agree on the label, as measured by the correlation of label predictions made by each modality across a batch of samples. We refer to the paper that most closely implements CCA-based alignment for multimodal data (specifically directly testing on the CMU-MOSI dataset)  [579] .\n\nFeature-level alignment: In the latter, contrastive learning has emerged as a popular approach that brings similar concepts close in feature space and different concepts far away  [126, 335, 605] . MULTIZOO includes REFNET  [518]  which includes a self-supervised contrastive loss between unimodal representations z 1 , z 2 and the multimodal representation z mm , i.e., L contrast = Algorithm 3 PyTorch code integrating MULTIBENCH datasets and MULTIZOO models. 1cos(z mm , g 1 (z 1 )) + 1cos(z mm , g 2 (z 2 )) where g 1 , g 2 is an auxiliary layer mapping each modality's representation into the joint multimodal space. The intuition here is that the unimodal representations z 1 , z 2 and the multimodal representation z mm should be aligned in the multimodal feature space as measured by cosine similarity. While the original REFNET method does not use negative samples, closely related work in multi-view contrastive learning has extended this idea to use negative samples which is more closely in line with recent work in contrastive learning  [605] .\n\nReconstruction objectives: Methods based on generative-discriminative models (e.g., VAEs) include an objective to reconstruct the input (or some part of the input)  [335, 615] . These have been shown to better preserve task-relevant information learned in the representation, especially in settings with sparse supervised signals such as robotics  [335]  and long videos  [615] . We include the Multimodal Factorized Model (MFM)  [615]  which is a general approach that learns a representation z mm that can reconstruct input data x 1 , x 2 while also predicting the label. The multimodal representation is a concatenation of factorized representations z 1 , z 2 , ..., z M , and z y .\n\nSince MFM optimizes a variational lower-bound to the log likelihood, the overall objective consists of 3 terms -generative, discriminative, and prior regularization:\n\nwhere f i 's are encoders from each modality to representations, f mm is a multimodal encoder to the joint representation z y , g i 's are decoders from latent representations back into input data, and g y is a classification head to the label. The final MMD term is a regularizer to bring the representations close to a unit Gaussian prior. The multimodal encoder f mm in MFM can be instantiated with any multimodal model (e.g., learning z y via tensors and adding a term to reconstruct input data). We use the public implementation in https://github.com/pliang279/factorized, which uses a temporal attention model as f mm for multimodal time-series data. For the remaining experiments we replace f mm with a simple late fusion but also run some experiments with multimodal methods that are state-of-the-art in each domain.\n\nImproving robustness: These approaches modify the objective function to account for robustness to noisy  [364]  or missing  [336, 398, 483]  modalities. MULTIZOO includes MCTN  [483]  which uses cycle-consistent translation to predict the noisy/missing modality from present ones.\n\nThe key insight is that a joint representation between modalities x 1 and x 2 can be learned by using x 1 to predict x 2 , in a vein similar to machine translation or image/text style transfer. MCTN defines a cyclic translation path x 1 → z mm → x2 → z mm → x1 and adds additional reconstruction losses L rec = ∥x 1 -x1 ∥ 2 + ∥x 2 -x2 ∥ 2 on top of the supervised learning loss. The representations z mm learned via translation are then used to predict the label. Surprisingly, the model needs to take in only x 1 at test time and is therefore robust to all levels of noise or missingness in x 2 .",
      "page_start": 126,
      "page_end": 127
    },
    {
      "section_name": "Training Procedures",
      "text": "Improving generalization: Recent work has found that directly training a multimodal model with all modalities using supervised learning is sub-optimal since different modalities overfit and generalize at different rates. MULTIZOO includes an approach to solve this, called Gradient Blending (GRADBLEND), that computes generalization statistics for each modality to determine their weights during multimodal fusion  [652] . We also include a similar work, Regularization by Maximizing Functional Entropies (RMFE), which uses functional entropy to balance the contribution of each modality to the classification result  [191] .",
      "page_start": 128,
      "page_end": 128
    },
    {
      "section_name": "Putting Everything Together",
      "text": "In Algorithm 3, we show a sample code snippet in Python that loads a dataset from MULTIBENCH, defines the unimodal and multimodal architectures, optimization objective, and training procedures, before running the evaluation protocol. Our MULTIZOO toolkit is easy to use and trains entire multimodal models in less than 10 lines of code. By standardizing the implementation of each module and disentangling the individual effects of models, optimizations, and training, MULTIZOO ensures both accessibility and reproducibility of its algorithms.",
      "page_start": 128,
      "page_end": 128
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "Setup: Using MULTIBENCH, we load each of the datasets and test the multimodal approaches in MULTIZOO. We only vary the contributed method of interest and keep all other possibly confounding factors constant (i.e., using the exact same training loop when testing a new multimodal fusion paradigm), a practice unfortunately not consistent in previous work. Our code is available at https://github.com/pliang279/MultiBench. MULTIBENCH allows for careful analysis of multimodal models and we summarize the main take-away messages below.",
      "page_start": 128,
      "page_end": 128
    },
    {
      "section_name": "Benefits Of Standardization",
      "text": "From Table  7 .2, simply applying methods in a research different area achieves state-of-the-art performance on 9 out of the 15 fusion tasks. We find that this is especially true for domains and modalities that have been relatively less studied in multimodal research (i.e., healthcare, finance, HCI). Performance gains can be obtained using multimodal methods outside of that research area. Therefore, this motivates the benefits of standardizing and unifying areas of research in multimodal machine learning. We believe that the ever-expanding diversity of datasets in MULTIBENCH can greatly accelerate research in multimodal learning.",
      "page_start": 129,
      "page_end": 129
    },
    {
      "section_name": "Generalization Across Domains And Modalities",
      "text": "MULTIBENCH offers an opportunity to analyze algorithmic developments across a large suite of modalities, domains, and tasks. We illustrate these observations through 2 summary plots of the generalization performance of multimodal models. Firstly, in Figure  7 .4, we plot the performance of each multimodal method across all datasets that it is tested on, using the color red to indicate performance on datasets that it was initially proposed and tested on (which we label as in-domain), and blue to indicate its performance on the remaining datasets (which we label as out-domain). Secondly, in Figure  7 .5, we color-code the performance on each dataset depending on which research area the dataset belongs to (one of the 6 research areas covered in MULTIBENCH). We summarize several observations regarding generalization across modalities and tasks: 1. Many multimodal methods still do not generalize across domains and datasets. For examples, MFAS  [479]  works well on domains it was designed for (AV-MNIST and MM-IMDB in the multimedia domain), but does not generalize to other domains such as healthcare (MIMIC). Similarly, the method designed for robustness, MCTN  [483] , does not generalize to datasets within the affective computing domain (UR-FUNNY and MUSTARD). Finally, GRADBLEND  [652] , an approach specifically designed to improve generalization in multimodal learning and tested on video and audio datasets (e.g., Kinetics), does not perform well on other datasets. Therefore, there still does not exist a one-size-fits-all model, especially on understudied modalities and tasks.",
      "page_start": 129,
      "page_end": 130
    },
    {
      "section_name": "Ef Lf Tf Lrtf Mi Mult Mfas Cca Refnet Mfm Mvae Gradblend",
      "text": "Performance in-domain datasets out-domain datasets In-domain refers to the performance on datasets that the method was previously proposed for and out-domain shows performance on the remaining datasets. We find that many methods show strongest performance on in-domain datasets which drop when tested on different domains, modalities, and tasks.\n\nIn general, we also observe high variance in the performance of multimodal methods across datasets in MULTIBENCH, which suggest open questions in building more generalizable models.",
      "page_start": 131,
      "page_end": 131
    },
    {
      "section_name": "Ef Lf Tf Lrtf Mi Mult Mfas Cca Refnet Mfm Mvae Gradblend",
      "text": "Performance Affect Healthcare Robotics Finance HCI Multimedia 2. From Figure  7 .4, many methods show strongest performance on in-domain datasets, and their performance drops when tested on different domains, modalities, and tasks. MULT performs extremely well on the affect recognition datasets it was designed for but struggles on other multimodal time-series in the finance and robotics domains. On the other hand, MFM shows an impressive performance in generalizing to new domains, although its in-domain performance has been exceeded by several other methods. 3. From Figure  7 .4, there is high variance in multimodal performance across datasets in MULTIBENCH, which suggest open questions in building more generalizable models. We find that LF is quite stable and always achieves above-average performance. 4. There are methods that are quite generalizable -typically general modality-agnostic methods such as LF. While simple, it is a strong method that balances simplicity, performance, and low complexity. However, it does not achieve the best performance on any dataset, which suggests that it is a good starting point but perhaps not the best eventual method. 5. From Figure  7 .5, we find that performance also varies significantly across research areas. 6. Several methods such as MFAS and CCA are designed for only 2 modalities (usually image and text), and TF and MI do not scale efficiently beyond 2/3 modalities. Therefore, we were unable to directly adapt these approaches to other datasets. We encourage the community to generalize these approaches across datasets and modalities on MULTIBENCH.    7.7:  Tradeoff between performance and robustness. Size of circles shows variance in robustness across datasets. We show the line of best linear fit in dotted blue. While better performing methods show better relative robustness (a), some suffer in effective robustness since performance drops off faster (b). Few models currently achieve both relative and effective robustness, suggesting directions for future research.",
      "page_start": 132,
      "page_end": 132
    },
    {
      "section_name": "Robustness",
      "text": "",
      "page_start": 132,
      "page_end": 132
    },
    {
      "section_name": "Tradeoffs Between Performance And Complexity",
      "text": "In Figure  7 .6(a), we summarize the performance of all methods in terms of performance and complexity. We find a strong tradeoff between these two desiderata: simple fusion techniques (e.g., LF) are actually appealing choices which score high on both metrics, especially when compared to complex (but slightly better performing) methods such as architecture search (MFAS) or Multimodal Transformers (MULT). While LF is the easiest to adapt to new datasets and domains, we encountered difficulties in adapting several possibly well-performing methods (such as MFAS or MULT) to new datasets and domains. Therefore, while their average performance is only slightly better than LF on all datasets (see Figure  7 .6(a)), they perform much better on wellstudied datasets (see Figure  7 .6(b)). We hope that the release of FACTORCL will greatly accelerate research in adapting complex methods on new datasets.",
      "page_start": 133,
      "page_end": 133
    },
    {
      "section_name": "Tradeoffs Between Performance And Robustness",
      "text": "In Figure  7 .7, we plot a similar tradeoff plot between accuracy and (relative & effective) robustness. As a reminder, relative robustness directly measures accuracy under imperfections while effective robustness measures the rate at which accuracy drops after equalizing for initial accuracy on clean test data. We observe a positive correlation between performance and relative robustness (see Figure  7 .7(a)), implying that models starting off with higher accuracy tend to stay above other models on the performance-imperfection curve. However, we observe a negative best fit between performance and effective robustness (see Figure  7 .7(b)) because several well-performing methods such as MULT, CCA, and MVAE tend to drop off faster after equalizing for initial accuracy on clean test data. Furthermore, very few models currently achieve both positive relative and effective robustness, which is a crucial area for future multimodal research.",
      "page_start": 134,
      "page_end": 134
    },
    {
      "section_name": "Related Work",
      "text": "We review related work on standardizing datasets and methods in multimodal learning.\n\nComparisons with related benchmarks: To the best of our knowledge, MULTIBENCH is the first multimodal benchmark with such a large number of datasets, modalities, and tasks. Most previous multimodal benchmarks have focused on a single research area such as within affective computing  [199] , human multimodal language  [360] , language and vision-based question answering  [174, 537] , text classification with external multimodal information  [212] , and multimodal learning for education  [227] . MULTIBENCH is specifically designed to go beyond the commonly studied language, vision, and audio modalities to encourage the research community to explore relatively understudied modalities (e.g., tabular data, time-series, sensors, graph and set data) and build general multimodal methods that can handle a diverse set of modalities.\n\nOur work is also inspired by recent progress in better evaluation benchmarks for a suite of important tasks in ML such as language representation learning  [643, 644] , long-range sequence modeling  [596] , multilingual representation learning  [251] , graph representation learning  [256] , and robustness to distribution shift  [312] . These well-crafted benchmarks have accelerated progress in new algorithms, evaluation, and analysis in their respective research areas.\n\nStandardizing multimodal learning: There have also been several attempts to build a single model that works well on a suite of multimodal tasks  [348, 390, 572] . However, these are limited to the language and vision space, and multimodal training is highly tailored for text and images. Transformer architectures have emerged as a popular choice due to their suitability for both language and image data  [108, 253]  and a recent public toolkit was released for incorporating multimodal data on top of text-based Transformers for prediction tasks  [212] . By going beyond Transformers and text data, MULTIBENCH opens the door to important research questions involving a much more diverse set of modalities and tasks while holistically evaluating performance, complexity, and robustness.\n\nAnalysis of multimodal representations: Recent work has carefully analyzed and challenged long-standing assumptions in multimodal learning. They have shown that certain models do not actually learn cross-modal interactions but rather rely on ensembles of unimodal statistics  [235]  and that certain datasets and models are biased to the most dominant modality  [75, 206] , sometimes ignoring others completely  [10] . These observations are currently only conducted on specific datasets and models without testing their generalization to others, a shortcoming we hope to solve using MULTIBENCH which enables scalable analysis over modalities, tasks, and models.",
      "page_start": 133,
      "page_end": 133
    },
    {
      "section_name": "Conclusion",
      "text": "Limitations: While MULTIBENCH can help to accelerate research in multimodal ML, we are aware of the following possible limitations:\n\n1. Tradeoffs between generality and specificity: While it is desirable to build models that work across modalities and tasks, there is undoubtedly merit in building modality and task-specific models that can often utilize domain knowledge to improve performance and interpretability (e.g., see neuro-symbolic VQA  [633] , or syntax models for the language modality  [120] ). By easing access to data, models, and evaluation, we hope that MULTIBENCH will challenge researchers to design interpretable models leveraging domain knowledge for many multimodal tasks. It remains an open question to define \"interpretability\" for other modalities beyond image and text, a question we hope MULTIBENCH will drive research in.\n\n2. Scale of datasets, models, and metrics: We plan for MULTIBENCH to be a continuouslygrowing community effort with regular maintenance and expansion. While MULTIBENCH currently does not include several important research areas outside of multimodal fusion (e.g., question answering  [11, 223] , retrieval  [733] , grounding  [121] , and reinforcement learning  [394] ), and is also limited by the models and metrics it supports, we have plans to expand MULTIBENCH towards a wider scale of datasets, models, and metrics.\n\nProjected expansions of MULTIBENCH: In this subsection, we describe concrete ongoing and future work towards expanding MULTIBENCH:\n\n1. Other multimodal research problems: We are genuinely committed to building a community around these resources and continue improving it over time. While we chose to focus on multimodal fusion by design for this first version to have a more coherent way to standardize and evaluate methods across datasets, we acknowledge the breadth of multimodal learning and are looking forward to expanding it in other directions in collaboration with domain experts. We have already included 2 datasets in captioning (and more generally for non-language outputs, retrieval): (1) Yummly-28K of paired videos and text descriptions of food recipes  [421] , and (2) Clotho dataset for audio-captioning  [156]  as well as a language-guided RL environment Read to Fight Monsters (RTFM)  [734]  and are also working towards more datasets in QA, retrieval, and multimodal RL.\n\nTo help in scalable expansion, we plan for an open call to the community for suggestions and feedback about domains, datasets, and metrics. As a step in this direction, we have concrete plans to use MULTIBENCH as a theme for future workshops and competitions (building on top of the multimodal workshops we have been organizing at NAACL 2021, ACL 2020, and ACL 2019, and in multimodal learning courses (starting with the course taught annually at CMU). Since MULTIBENCH is public and will be regularly maintained, the existing benchmark, code, evaluation, and experimental protocols can greatly accelerate any dataset and modeling innovations added in the future. In our public GitHub, we have included a section on contributing through task proposals or additions of datasets and algorithms. The authors will regularly monitor new proposals through this channel.\n\n2. New evaluation metrics: We also plan to include evaluation for distribution shift, uncertainty estimation, tests for fairness and social biases, as well as labels/metrics for interpretable multimodal learning. In the latter, we plan to include the EMAP score  [235]  as an interpretability metric assessing whether cross-modal interactions improve performance.\n\n3. Multimodal transfer learning and co-learning: Can training data in one dataset help learning on other datasets? MULTIBENCH enables easy experimentation of such research questions: our initial experiments on transfer learning found that pre-training on larger datasets in the same domain can improve performance on smaller datasets when fine-tuned on a smaller dataset: performance on the smaller CMU-MOSI dataset improved from 75.2 to 75.8 using the same late fusion model with transfer learning from the larger UR-FUNNY and CMU-MOSEI datasets. Furthermore, recent work has shown that multimodal training can help improve unimodal performance as well  [554, 676, 717] . While previous experiments were on a small scale and limited to a single domain, we plan to expand significantly on this phenomenon (multimodal co-learning) in future versions of MULTIBENCH.\n\n4. Multitask learning across modalities: Multitask learning across multimodal tasks with a shared set of input modalities is a promising direction that can enable statistical strength sharing across datasets and efficiency in training a single model. Using MULTIBENCH, we also ran an extra experiment on multi-dataset multitask learning. We used the 4 datasets in the affective computing domain and trained a single model across all 4 of them with adjustable input embedding layers if the input features were different and separate classification heads for each dataset's task. We found promising initial results with performance on the largest CMU-MOSEI dataset improving from 79.2 to 80.9 for a late fusion model and from 82.1 to 82.9 using a multimodal transformer model, although performance on the smaller CMU-MOSI dataset decreased from 75.2 to 70.8. We believe that these potential future studies in multitask and transfer learning are strengths of MULTIBENCH since it shows the potential of interesting experiments and usage.\n\nIn conclusion, we present MULTIBENCH, a large-scale benchmark unifying previously disjoint efforts in multimodal research with a focus on ease of use, accessibility, and reproducibility, thereby paving the way towards a deeper understanding of multimodal models. Through its unprecedented range of research areas, datasets, modalities, tasks, and evaluation metrics, MULTI-BENCH highlights several future directions in building more generalizable, lightweight, and robust multimodal models.",
      "page_start": 133,
      "page_end": 133
    },
    {
      "section_name": "Chapter 8",
      "text": "Neural Architectures for Multisensory Foundation Models",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Introduction",
      "text": "To build general multisensory foundation models that work across the diverse modalities and tasks in MULTIBENCH, this chapter of the thesis presents two architectures that are broadly generalizable across diverse modalities. The large number of heterogeneous modalities creates challenges in building multisensory foundation models. For example, the healthcare domain typically collects tabular data and high-frequency sensors  [287] , and it remains an open question how to best combine large language models with tabular data and sensors  [547] . To tackle the heterogeneity across many different modalities, we treat modalities in their most general form as sequences of elements, and study how to learn interactions between multiple elements across modalities. As motivated in the first part of the thesis, these local interactions between two elements can be redundant, unique, and synergistic: redundancy quantifies information shared between modalities, uniqueness quantifies the information present in only one of the modalities, and synergy quantifies the emergence of new information not previously present in either modality.\n\nTreating modalities as sequences of elements now introduces a new challenge due to asynchrony in time: for example, the simultaneous co-occurrence between a smile and a positive word, or the delayed occurrence of laughter after the end of a sentence. Modeling these interactions lie at the heart of analyzing human communication, audio-video data, sensor fusion, and medical modalities. We now present two approaches to learn interactions from heterogeneous modality elements across sequences: the cross-modal attention  [101, 359]  and multimodal transformer  [614]  architectures.\n\nThe first architecture is called RECURRENT MULTISTAGE FUSION NETWORK, or RMFN for short. This method automatically decomposes the multimodal fusion problem into multiple recursive stages across the sequence. At each stage, a subset of multimodal signals is highlighted and fused with previous fusion representations (see Figure  9 .1). This divide-and-conquer approach decreases the burden on each fusion stage, allowing each stage to be performed in a more specialized and effective way. This is in contrast with conventional fusion approaches which usually model interactions over multimodal sequences altogether in one iteration (e.g., early or late Finally, the third stage selects the shrugging and speech elongation behaviors that reflect ambivalence and when fused with previous stages is interpreted as a representation for the disappointed emotion.\n\nfusion  [45] ). In RMFN, multimodal interactions are modeled by integrating our new multistage fusion process with a system of recurrent neural networks. Overall, RMFN recursively models all forms of redundant, unique, and synergistic multimodal interactions across the sequence and is differentiable end-to-end.\n\nThe second architecture we propose is the MULTIMODAL TRANSFORMER (MULT), an end-toend model that extends the standard Transformer network  [632]  to learn representations directly from unaligned multimodal sequences. At the heart of MULT is the crossmodal attention module, which learns multimodal interactions between all elements in the first modality with all elements in the second modality. As a result, all multimodal interactions across the entire sequence are learned simultaneously, and can be parallelized efficiently over GPUs as compared to the first recurrent fusion approach. This makes MULT extremely scalable and effective, especially in settings where modality elements are asynchronous and where obtaining alignment information is difficult (e.g., by forced word-aligning before training  [483, 718] , see Figure  8 .2 for a comparison).\n\nWe evaluate RMFN and MULT on three different tasks related to human multimodal language: sentiment analysis, emotion recognition, and speaker traits recognition across three public multimodal datasets. RMFN achieves state-of-the-art performance in all three tasks. Through a comprehensive set of ablation experiments and visualizations, we demonstrate the advantages of explicitly defining multiple recursive stages for multimodal fusion.",
      "page_start": 136,
      "page_end": 138
    },
    {
      "section_name": "Related Work",
      "text": "Previous approaches in human multimodal language modeling can be categorized as follows:\n\nNon-temporal Models: These models simplify the problem by using feature-summarizing temporal observations  [491] . Each modality is represented by averaging temporal information through time, as shown for language-based sentiment analysis  [105, 273]  and multimodal sentiment analysis  [2, 427, 449, 712] . Conventional supervised learning methods are utilized to discover intra-modal and cross-modal interactions without specific model design  [489, 646] . These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics  [678] .\n\nMultimodal Temporal Graphical Models: The application of graphical models in sequence modeling has been an important research problem. Hidden Markov Models (HMMs)  [50] , Conditional Random Fields (CRFs)  [324] , and Hidden Conditional Random Fields (HCRFs)  [495]  were shown to work well on modeling sequential data from the language  [264, 400, 422]  and acoustic  [709]  modalities. These temporal graphical models have also been extended for modeling multimodal data. Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views  [560] , multi-layered CRFs with latent variables to learn hidden spatio-temporal dynamics from multi-view data  [560] , and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations  [561] .\n\nMultimodal Temporal Neural Networks: More recently, with the advent of deep learning, Recurrent Neural Networks  [164, 279]  have been used extensively for language and speech based sequence modeling  [558, 743] , sentiment analysis  [78, 153, 202, 555] , and emotion recognition  [58, 220, 326] . Long-short Term Memory (LSTM) networks  [242]  have also been extended for multimodal settings  [502]  and by learning binary gating mechanisms to remove noisy modalities  [101] . Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions. These use Bayesian ranking algorithms  [234]  to model both personindependent and person-dependent features  [361] , generative-discriminative objectives to learn either joint  [482]  or factorized multimodal representations  [615] , external memory mechanisms to synchronize multimodal data  [714] , or low-rank tensors to approximate expensive tensor products  [388] . All these methods assume that cross-modal interactions should be discovered all at once rather than across multiple stages, where each stage solves a simpler fusion problem. Our empirical evaluations show the advantages of the multistage fusion approach.\n\nTransformer Network: The Transformer network  [632]  was first introduced for neural machine translation, where the encoder and decoder side each leverages a self-attention  [382, 465, 632]  transformer. After each layer of self-attention, the encoder and decoder are connected by an additional decoder sublayer where the decoder attends to each element of the source text for each element of the target text. In addition to translation, transformer networks have also been successfully applied to other tasks, including language modeling  [41, 129] , semantic role labeling  [569] , word sense disambiguation  [591] , learning sentence representations  [144] , and video activity recognition  [653] .",
      "page_start": 138,
      "page_end": 139
    },
    {
      "section_name": "Recurrent Multistage Fusion Network",
      "text": "We first describe the RECURRENT MULTISTAGE FUSION NETWORK (RMFN for short) for multimodal language analysis (Figure  9 .2). Given a set of modalities {l(anguage), v(isual), a(coustic)}, the signal from each modality m ∈ {l, v, a} is represented as a temporal sequence X m = {x m 1 , x m 2 , x m 3 , ⋯, x m T }, where x m t is the input at time t. Each sequence X m is modeled with an intra-modal recurrent neural network (see subsection 8.3.3 for details). At time t, each intra-modal recurrent network will output a unimodal representation h m t . The Multistage Fusion Process uses a recursive approach to fuse all unimodal representations h m t into a cross-modal representation z t which is then fed back into each intra-modal recurrent network.",
      "page_start": 139,
      "page_end": 139
    },
    {
      "section_name": "Multistage Fusion Process",
      "text": "The Multistage Fusion Process (MFP) is a modular neural approach that performs multistage fusion to model cross-modal interactions. Multistage fusion is a divide-and-conquer approach which decreases the burden on each stage of multimodal fusion, allowing each stage to be performed in a more specialized and effective way. The MFP has three main modules: HIGHLIGHT, FUSE and SUMMARIZE. Two modules are repeated at each stage: HIGHLIGHT and FUSE. The HIGHLIGHT module identifies a subset of multimodal signals from [h l t , h v t , h a t ] that will be used for that stage of fusion. The FUSE module then performs two subtasks simultaneously: a local fusion of the highlighted features and integration with representations from previous stages. Both HIGHLIGHT and FUSE modules are realized using memory-based neural networks which enable coherence between stages and storage of previously modeled cross-modal interactions. As a final step, the SUMMARIZE module takes the multimodal representation of the final stage and translates it into a cross-modal representation z t .",
      "page_start": 139,
      "page_end": 139
    },
    {
      "section_name": "Recurrent Multistage Fusion Network",
      "text": "Figure  9 .1 shows an illustrative example for multistage fusion. The HIGHLIGHT module selects \"neutral words\" and \"frowning\" expression for the first stage. The local and integrated fusion at this stage creates a representation reflecting negative emotion. For stage 2, the HIGHLIGHT module identifies the acoustic feature \"loud voice\". The local fusion at this stage interprets it as an expression of emphasis and is fused with the previous fusion results to represent a strong negative emotion. Finally, the highlighted features of \"shrug\" and \"speech elongation\" are selected and are locally interpreted as \"ambivalence\". The integration with previous stages then gives a representation closer to \"disappointed\".",
      "page_start": 140,
      "page_end": 140
    },
    {
      "section_name": "Module Descriptions",
      "text": "In this section, we present the details of the three multistage fusion modules: HIGHLIGHT, FUSE and SUMMARIZE. Multistage fusion begins with the concatenation of intra-modal network outputs h t = ⊕ m∈M h m t . We use superscript [k] to denote the indices of each stage k = 1, ⋯, K during K total stages of multistage fusion. Let Θ denote the neural network parameters across all modules.\n\nHIGHLIGHT: At each stage k, a subset of the multimodal signals represented in h t will be automatically highlighted for fusion. Formally, this module is defined by the process function f H :\n\nwhere at stage k, a\n\n[k] t is a set of attention weights which are inferred based on the previously assigned attention weights a . As a result, the highlights at a specific stage k will be dependent on previous highlights. To fully encapsulate these dependencies, the attention assignment process is performed in a recurrent manner using a LSTM which we call the HIGHLIGHT LSTM. The initial HIGHLIGHT LSTM memory at stage 0, c HIGHLIGHT[0] t , is initialized using a network M that maps h t into LSTM memory space:\n\nThis allows the memory mechanism of the HIGHLIGHT LSTM to dynamically adjust to the intra-modal representations h t . The output of the HIGHLIGHT LSTM h\n\nis softmax activated to produce attention weights a\n\n[k] t at every stage k of the multistage fusion process:\n\nt is fed as input into the HIGHLIGHT LSTM at stage k + 1. Therefore, the HIGHLIGHT LSTM functions as a decoder LSTM  [115, 582]  in order to capture the dependencies on previous attention assignments. Highlighting is performed by element-wise multiplying the attention weights a\n\n[k] t with the concatenated intra-modal representations h t :\n\nwhere ⊙ denotes the Hadamard product and h[k] t are the attended multimodal signals that will be used for the fusion at stage k.\n\nFUSE: The highlighted multimodal signals are simultaneously fused in a local fusion and then integrated with fusion representations from previous stages. Formally, this module is defined by the process function f F : s\n\nwhere s\n\n[k]\n\nt denotes the integrated fusion representations at stage k. We employ a FUSE LSTM to simultaneously perform the local fusion and the integration with previous fusion representations. The FUSE LSTM input gate enables a local fusion while the FUSE LSTM forget and output gates enable integration with previous fusion results. The initial FUSE LSTM memory at stage 0, c\n\n, is initialized using random orthogonal matrices  [33, 330] . SUMMARIZE: After completing K recursive stages of HIGHLIGHT and FUSE, the SUMMARIZE operation generates a cross-modal representation using all final fusion representations s\n\n[1∶K] t . Formally, this operation is defined as:\n\nwhere z t is the final output of the multistage fusion process and represents all cross-modal interactions discovered at time t. The summarized cross-modal representation is then fed into the intra-modal recurrent networks as described in the subsection 8.3.3.",
      "page_start": 140,
      "page_end": 141
    },
    {
      "section_name": "System Of Long Short-Term Hybrid Memories",
      "text": "To integrate the cross-modal representations z t with the temporal intra-modal representations, we employ a system of Long Short-term Hybrid Memories (LSTHMs)  [715] . The LSTHM extends the LSTM formulation to include the cross-modal representation z t in a hybrid memory component:\n\n)\n\nwhere σ is the (hard-)sigmoid activation function, tanh is the tangent hyperbolic activation function, ⊙ denotes the Hadamard product. i, f and o are the input, forget and output gates respectively. cm t+1 is the proposed update to the hybrid memory c m t at time t + 1 and h m t is the time distributed output of each modality. The cross-modal representation z t is modeled by the Multistage Fusion Process as discussed in subsection 8.3.2. The hybrid memory c m t contains both intra-modal interactions from individual modalities x m t as well as the cross-modal interactions captured in z t .",
      "page_start": 142,
      "page_end": 142
    },
    {
      "section_name": "Optimization",
      "text": "The multimodal prediction task is performed using a final representation E which integrate (1) the last outputs from the LSTHMs and (2) the last cross-modal representation z T . Formally, E is defined as:\n\nwhere ⊕ denotes vector concatenation. E can then be used as a multimodal representation for supervised or unsupervised analysis of multimodal language. It summarizes all modeled intra-modal and cross-modal representations from the multimodal sequences. RMFN is differentiable end-to-end which allows the network parameters Θ to be learned using gradient descent approaches.",
      "page_start": 142,
      "page_end": 142
    },
    {
      "section_name": "Multimodal Transformer",
      "text": "We next describe our second proposed architecture, the MULTIMODAL TRANSFORMER (MULT) (Figure  9 .6) for modeling unaligned multimodal language sequences. At the high level, MULT merges multimodal time-series via a feed-forward fusion process from multiple directional pairwise crossmodal transformers. Specifically, each crossmodal transformer (introduced in Section 8.4.2) serves to repeatedly reinforce a target modality with the low-level features from another source modality by learning the attention across the two modalities' features. A MULT architecture hence models all pairs of modalities with such crossmodal transformers, followed by sequence models (e.g., self-attention transformer) that predicts using the fused features.\n\nThe core of our proposed model is crossmodal attention module, which we first introduce in Section 8.4.1. Then, in Section 8.4.2 and 8.4.3, we present in details the various ingredients of the MULT architecture (see Figure  9 .6) and discuss the difference between crossmodal attention and classical multimodal alignment.",
      "page_start": 142,
      "page_end": 142
    },
    {
      "section_name": "Crossmodal Attention",
      "text": "We consider two modalities α and β, with two (potentially non-aligned) sequences from each of them denoted X α ∈ R Tα×dα and X β ∈ R T β ×d β , respectively. For the rest of the paper, T (⋅) and d (⋅) are used to represent sequence length and feature dimension, respectively. Inspired by the decoder transformer in NMT  [632]  that translates one language to another, we hypothesize a good way to fuse crossmodal information is providing a latent adaptation across modalities; i.e., β to α. Note that the modalities consider in our paper may span very different domains such as facial attributes and spoken words.\n\nWe define the Querys as\n\nNote that Y α has the same length as Q α (i.e., T α ), but is meanwhile represented in the feature space of V β . Specifically, the scaled (by",
      "page_start": 143,
      "page_end": 143
    },
    {
      "section_name": "Modality ↵ Modality",
      "text": "(a) Crossmodal attention CM β→α (X α , X β ) between sequences X α , X β from distinct modalities.",
      "page_start": 144,
      "page_end": 144
    },
    {
      "section_name": "Multi-Head",
      "text": "",
      "page_start": 144,
      "page_end": 144
    },
    {
      "section_name": "Positionwise",
      "text": "Feed-forward ⇥D Layers Layer 0  softmax (⋅) ∈ R Tα×T β , whose (i, j)-th entry measures the attention given by the i-th time step of modality α to the j-th time step of modality β. Hence, the i-th time step of Y α is a weighted summary of V β , with the weight determined by i-th row in softmax(⋅). We call Equation eq (8.14) a single-head crossmodal attention, which is illustrated in Figure  8 .5a. Following prior works on transformers  [100, 129, 144, 632] , we add a residual connection to the crossmodal attention computation. Then, another positionwise feed-forward sublayer is injected to complete a crossmodal attention block (see Figure  8 .5b). Each crossmodal attention block adapts directly from the low-level feature sequence (i.e., Z\n\n[0] β in Figure  8 .5b) and does not rely on self-attention, which makes it different from the NMT encoder-decoder architecture  [541, 632]  (i.e., taking intermediate-level features). We argue that performing adaptation from low-level feature benefits our model to preserve the low-level information for each modality.",
      "page_start": 145,
      "page_end": 145
    },
    {
      "section_name": "Overall Architecture",
      "text": "Three major modalities are typically involved in multimodal language sequences: language (L), video (V ), and audio (A) modalities. We denote with X {L,V,A} ∈ R T {L,V,A} ×d {L,V,A} the input feature sequences (and the dimensions thereof) from these 3 modalities. With these notations, in this subsection, we describe in greater details the components of Multimodal Transformer and how crossmodal attention modules are applied.\n\nTemporal Convolutions. To ensure that each element of the input sequences has sufficient awareness of its neighborhood elements, we pass the input sequences through a 1D temporal convolutional layer:\n\nwhere k {L,V,A} are the sizes of the convolutional kernels for modalities {L, V, A}, and d is a common dimension. The convolved sequences are expected to contain the local structure of the sequence, which is important since the sequences are collected at different sampling rates. Moreover, since the temporal convolutions project the features of different modalities to the same dimension d, the dot-products are admittable in the crossmodal attention module.\n\nPositional Embedding. To enable the sequences to carry temporal information, following prior work  [632] , we augment positional embedding (PE) to X{L,V,A} :\n\n{L,V,A} = X{L,V,A} + PE(T {L,V,A} , d)  (8.16)  where PE(T {L,V,A} , d) ∈ R T {L,V,A} ×d computes the (fixed) embeddings for each position index, and\n\n{L,V,A} are the resulting low-level position-aware features for different modalities. We leave more details of the positional embedding to the full paper  [614] .\n\nCrossmodal Transformers. Based on the crossmodal attention blocks, we design the crossmodal transformer that enables one modality for receiving information from another modality. In the following, we use the example for passing vision (V ) information to language (L), which is denoted by \"V → L\". We fix all the dimensions (d {α,β,k,v} ) for each crossmodal attention block as d.\n\nEach crossmodal transformer consists of D layers of crossmodal attention blocks (see Figure  8 .5b). Formally, a crossmodal transformer computes feed-forwardly for i = 1, . . . , D layers:\n\nwhere f θ is a positionwise feed-forward sublayer parametrized by θ, and CM\n\n[i],mul\n\nV →L means a multihead (see prior work  [632]  for more details) version of CM V →L at layer i (note: d should be divisible by the number of heads). LN means layer normalization  [38] .\n\nIn this process, each modality keeps updating its sequence via low-level external information from the multi-head crossmodal attention module. At every level of the crossmodal attention block, the low-level signals from source modality are transformed to a different set of Key/Value pairs to interact with the target modality. Empirically, we find that the crossmodal transformer learns to correlate meaningful elements across modalities. The eventual MULT is based on modeling every pair of crossmodal interactions. Therefore, with 3 modalities (i.e., L, V, A) in consideration, we have 6 crossmodal transformers in total (see Figure  9 .6).\n\nSelf-Attention Transformers and Prediction. As a final step, we concatenate the outputs from the crossmodal transformers that share the same target modality to yield Z {L,V,A} ∈ R T {L,V,A} ×2d . For example,\n\nA→L ]. Each of them is then passed through a sequence model to collect temporal information to make predictions. We choose the self-attention transformer  [632] . Eventually, the last elements of the sequences models are extracted to pass through fully-connected layers to make predictions.",
      "page_start": 144,
      "page_end": 145
    },
    {
      "section_name": "Discussion About Attention & Alignment",
      "text": "When modeling unaligned multimodal language sequences, MULT relies on crossmodal attention blocks to merge signals across modalities. While the multimodal sequences were (manually) aligned to the same length in prior works before training  [359, 483, 615, 656, 718] , we note that MULT looks at the non-alignment issue through a completely different lens. Specifically, for MULT, the correlations between elements of multiple modalities are purely based on attention. In other words, MULT does not handle modality non-alignment by (simply) aligning them; instead, the crossmodal attention encourages the model to directly attend to elements in other modalities where strong signals or relevant information is present. As a result, MULT can capture long-range crossmodal contingencies in a way that conventional alignment could not easily reveal. Classical crossmodal alignment, on the other hand, can be expressed as a special (step diagonal) crossmodal attention matrix (i.e., monotonic attention  [705] ). We illustrate their differences in Figure  8 .6.",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Experimental Setup",
      "text": "To evaluate the performance and generalization of RMFN and MULT, three domains of human multimodal language were selected: multimodal sentiment analysis, emotion recognition, and speaker traits recognition. Our goal is to compare MULT with prior competitive approaches on both word-aligned (by word, which almost all prior works employ) and unaligned (which is more challenging, and which MULT is generically designed for) multimodal language sequences.",
      "page_start": 146,
      "page_end": 146
    },
    {
      "section_name": "Datasets",
      "text": "All datasets consist of monologue videos. The speaker's intentions are conveyed through three modalities: language, visual and acoustic.\n\nMultimodal Sentiment Analysis involves analyzing speaker sentiment based on video content. Multimodal sentiment analysis extends conventional language-based sentiment analysis to a multimodal setup where both verbal and non-verbal signals contribute to the expression of sentiment. We use CMU-MOSI  [712]  which consists of 2199 opinion segments from online videos each annotated with sentiment in the range  [-3,3] .\n\nMultimodal Emotion Recognition involves identifying speaker emotions based on both verbal and nonverbal behaviors. We perform experiments on the IEMOCAP dataset  [74]  which consists of 7318 segments of recorded dyadic dialogues annotated for the presence of human emotions happiness, sadness, anger and neutral.\n\nMultimodal Speaker Traits Recognition involves recognizing speaker traits based on multimodal communicative behaviors. POM  [468]  contains 903 movie review videos each annotated for 12 speaker traits: confident (con), passionate (pas), voice pleasant (voi), credible (cre), vivid (viv), expertise (exp), reserved (res), trusting (tru), relaxed (rel), thorough (tho), nervous (ner), persuasive (per) and humorous (hum).\n\nEach task consists of a word-aligned (processed in the same way as in prior works) and an unaligned version. For both versions, the multimodal features are extracted from the textual (GloVe word embeddings  [476] ), visual (Facet  [270] ), and acoustic (COVAREP  [136] ) data modalities. A more detailed introduction to the features is included in the full paper  [359] .\n\nFor the word-aligned version, following  [483, 615, 714] , we first use P2FA  [709]  to obtain the aligned timesteps (segmented w.r.t. words) for audio and vision streams, and we then perform averaging on the audio and vision features within these time ranges. All sequences in the wordaligned case have length 50. The process remains the same across all the datasets. On the other hand, for the unaligned version, we keep the original audio and visual features as extracted, without any word-segmented alignment or manual subsampling. As a result, the lengths of each modality vary significantly, where audio and vision sequences may contain up to > 1, 000 time steps. We elaborate on the three tasks below.",
      "page_start": 146,
      "page_end": 147
    },
    {
      "section_name": "Multimodal Features And Alignment",
      "text": "GloVe word embeddings  [476] , Facet  [270]  and COVAREP  [136]  are extracted for the language, visual and acoustic modalities respectively 1  . Forced alignment is performed using P2FA  [709]  to obtain the exact utterance times of each word. We obtain the aligned video and audio features by computing the expectation of their modality feature values over each word utterance time interval  [615] .",
      "page_start": 148,
      "page_end": 148
    },
    {
      "section_name": "Baseline Models",
      "text": "We compare to the following models for multimodal machine learning: MFN  [714]  synchronizes multimodal sequences using a multi-view gated memory. It is the current state of the art on CMU-MOSI and POM. MARN  [715]  models intra-modal and cross-modal interactions using multiple attention coefficients and hybrid LSTM memory components. GME-LSTM(A)  [101]  learns binary gating mechanisms to remove noisy modalities that are contradictory or redundant for prediction. TFN  [713]  models unimodal, bimodal and trimodal interactions using tensor products. BC-LSTM  [491]  performs context-dependent sentiment analysis and emotion recognition, currently state of the art on IEMOCAP. EF-LSTM concatenates the multimodal inputs and uses that as input to a single LSTM  [241] . We also implement the Stacked, (EF-SLSTM)  [208]  Bidirectional (EF-BLSTM)  [529]  and Stacked Bidirectional (EF-SBLSTM) LSTMs.   current image with a memorized or aggregated representation of the speaker's face. Our proposed multistage fusion approach can easily be extended to memory-based fusion methods.\n\nUnaligned Experiments. Next, we evaluate MULT on the same set of datasets in the unaligned setting. Note that MULT can be directly applied to unaligned multimodal stream, while the baseline models (except for LF-LSTM) require the need of additional alignment module (e.g., CTC module).\n\nThe results are shown in the bottom part of Table  8 .1, 8.2, and 8.3. On the three benchmark datasets, MULT improves upon the prior methods (some with CTC) by 10%-15% on most attributes. Empirically, we find that MULT converges faster to better results at training when compared to other competitive approaches (see Figure  8 .7). In addition, while we note that in general there is a performance drop on all models when we shift from the word-aligned to unaligned multimodal time-series, the impact MULT takes is much smaller than the other approaches. We hypothesize such performance drop occurs because the asynchronous (and much  Q1: To study the effectiveness of the multistage fusion process, we test the baseline RMFN-R1 which performs fusion in only one stage instead of across multiple stages. This model makes the strong assumption that all cross-modal interactions can be modeled during only one stage. From Table  8 .5, RMFN-R1 underperforms as compared to RMFN which performs multistage fusion.\n\nQ2: We test baselines RMFN-RK which perform K stages of fusion. From Table  8 .5, we observe that increasing the number of stages K increases the model's capability to model cross-modal interactions up to a certain point (K = 3) in our experiments. Further increases led to decreases in performance and we hypothesize this is due to overfitting on the dataset.\n\nQ3: To compare multistage against independent modeling of cross-modal interactions, we pay close attention to the performance comparison with respect to MARN which models multiple cross-modal interactions all at once (see  Table 8.6) . RMFN shows improved performance, indicating that multistage fusion is both effective and efficient for human multimodal language modeling. Q4: RMFN (no MFP) represents a system of LSTHMs without the integration of z t from the MFP to model cross-modal interactions. From Table  8 .6, RMFN (no MFP) is outperformed by RMFN, confirming that modeling cross-modal interactions is crucial in analyzing human multimodal language.\n\nQ5: RMFN (no HIGHLIGHT) removes the HIGHLIGHT module from MFP during multistage fusion. From Table  8 .6, RMFN (no HIGHLIGHT) underperforms, indicating that highlighting multimodal representations using attention weights are important for modeling cross-modal interactions.\n\nVisualizations of learned fusion patterns: Using an attention assignment mechanism during the HIGHLIGHT process gives more interpretability to the model since it allows us to visualize the attended multimodal signals at each stage and time step (see Figure  8 .9). Using RMFN trained on the CMU-MOSI dataset, we plot the attention weights across the multistage fusion process for three videos in CMU-MOSI. Based on these visualizations we first draw the following general observations on multistage fusion:\n\nAcross stages: Attention weights change their behaviors across the multiple stages of fusion. Some features are highlighted by earlier stages while other features are used in later stages. This supports our hypothesis that RMFN learns to specialize in different stages of the fusion process.\n\nAcross time: Attention weights vary over time and adapt to the multimodal inputs. We observe that the attention weights are similar if the input contains no new information. As soon as new multimodal information comes in, the highlighting mechanism in RMFN adapts to these new inputs.\n\nPriors: Based on the distribution of attention weights, we observe that the language and acoustic modalities seem the most commonly highlighted. This represents a prior over the expression of sentiment in human multimodal language and is closely related to the strong connections between language and speech in human communication  [322] .\n\nInactivity: Some attention coefficients are not active (always orange) throughout time. We hypothesize that these corresponding dimensions carry only intra-modal dynamics and are not involved in the formation of cross-modal interactions.\n\nIn addition to the general observations above, Figure  8 .9 shows three examples where multistage fusion learns cross-modal representations across three different scenarios.\n\nSynchronized Interactions: In Figure  8 .9(a), the language features are highlighted corresponding to the utterance of the word \"fun\" that is highly indicative of sentiment (t = 5). This sudden change is also accompanied by a synchronized highlighting of the acoustic features. We also notice that the highlighting of the acoustic features lasts longer across the 3 stages since it may take multiple stages to interpret all the new acoustic behaviors (elongated tone of voice and phonological emphasis).\n\nAsynchronous Trimodal Interactions: In Figure  8 .9(b), the language modality displays ambiguous sentiment: \"delivers a lot of intensity\" can be inferred as both positive or negative. We observe that the circled attention units in the visual and acoustic features correspond to the asynchronous presence of a smile (t = 2 ∶ 5) and phonological emphasis (t = 3) respectively. These nonverbal behaviors resolve ambiguity in language and result in an overall display of positive sentiment. We further note the coupling of attention weights that highlight the language, visual and acoustic features across stages (t = 3 ∶ 5), further emphasizing the coordination of all three modalities during multistage fusion despite their asynchronous occurrences.\n\nBimodal Interactions: In Figure  8 .9(c), the language modality is better interpreted in the context of acoustic behaviors. The disappointed tone and soft voice provide the nonverbal information useful for sentiment inference. This example highlights the bimodal interactions (t = 4 ∶ 7) in alternating stages: the acoustic features are highlighted more in earlier stages while the language features are highlighted increasingly in later stages.",
      "page_start": 147,
      "page_end": 153
    },
    {
      "section_name": "Deeper Analysis Of Mult",
      "text": "Ablation studies: To further study the influence of the individual components in MULT, we perform comprehensive ablation analysis using the unaligned version of CMU-MOSEI. The results are shown in Table  8 .7.\n\nFirst, we consider the performance for only using unimodal transformers (i.e., language, audio or vision only). We find that the language transformer outperforms the other two by a large margin. For example, for the Acc h 2 metric, the model improves from 65.6 to 77.4 when comparing audio only to language only unimodal transformer. This fact aligns with the observations in prior work  [483] , where the authors found that a good language network could already achieve good performance at inference time.\n\nSecond, we consider 1) a late-fusion transformer that feature-wise concatenates the last elements of three self-attention transformers; and 2) an early-fusion self-attention transformer that takes in a temporal concatenation of three asynchronous sequences [ XL , XV , XA ] ∈ R (T L +T V +T A )×dq\n\nAs shown in Table  8 .7, we find crossmodal attention modules consistently improve over the late-and early-fusion transformer models in most metrics on unaligned CMU-MOSEI. In particular, among the three crossmodal transformers, the one where language(L) is the target modality works best. We also additionally study the effect of adapting intermediate-level instead of the low-level features from source modality in crossmodal attention blocks (similar to the NMT encoder-decoder architecture but without self-attention; see Section 8.4.1). While MULT leveraging intermediate-level features still outperform models in other ablative settings, we empirically find adapting from low-level features works best. The ablations suggest that crossmodal attention concretely benefits MULT with better representation learning.\n\nQualitative analysis of learned cross-modal attention: To understand how crossmodal attention works while modeling unaligned multimodal data, we empirically inspect what kind of signals MULT picks up by visualizing the attention activations. Figure  8 .8 shows an example of a section of the crossmodal attention matrix on layer 3 of the V → L network of MULT (the original matrix has dimension T L × T V ; the figure shows the attention corresponding to approximately a 6-sec short window of that matrix). We find that crossmodal attention has learned to attend to meaningful signals across the two modalities. For example, stronger attention is given to the intersection of words that tend to suggest emotions (e.g., \"movie\", \"disappointing\") and drastic facial expression changes in the video (start and end of the above vision sequence). This observation advocates one of the aforementioned advantage of MULT over conventional alignment (see Section 8.4.3): crossmodal attention enables MULT to directly capture potentially long-range signals, including those off-diagonals on the attention matrix.",
      "page_start": 153,
      "page_end": 154
    },
    {
      "section_name": "Conclusion",
      "text": "This chapter proposed the RECURRENT MULTISTAGE FUSION NETWORK (RMFN) and Multimodal Transformer (MULT) architectures or analyzing human multimodal language. RMFN which recursively decomposes the multimodal fusion problem into multiple stages, each focused on learning interactions from a subset of attended multimodal signals. MULT uses the crossmodal attention module to learn multimodal interactions between all elements in the first modality with all elements in the second modality. As a result, all multimodal interactions across the entire sequence are learned simultaneously, and can be parallelized efficiently over GPUs.\n\nBoth methods show strong results on multiple datasets with multimodal temporal data (e.g., human communication), displaying capabilities to capture long-range multimodal interactions, handling unaligned multimodal data, and learning redundant, unique, and synergistic interactions.",
      "page_start": 155,
      "page_end": 155
    },
    {
      "section_name": "Chapter 9",
      "text": "Training High-modality Foundation Models  Finally, using MULTIBENCH, we scale multimodal transformers to the high-modality setting where there are a large number of modalities partially observed for different tasks  [370] .\n\nWhile there have been impressive advances in modeling language, vision, and audio  [11, 503] , advances in sensing technologies have resulted in many real-world platforms such as cellphones, smart devices, selfdriving cars, healthcare technologies, and robots now integrating a much larger number of sensors such as time-series, proprioception, sets, tables, and high-frequency sensors  [53, 179, 335, 340, 366, 698] . This new setting of high-modality learning involves learning representations over many diverse modality inputs. As more modalities are introduced, adding new model parameters for every new modality or task  [283, 390, 614]  becomes prohibitively expensive and not scalable  [375] . A critical technical challenge for efficient high-modality learning, therefore, is heterogeneity quantification: how can we measure which modalities encode similar information and similar interactions in order to permit parameter sharing with previous modalities (see Figure  9 .1)? For example, how can one determine whether the same modality encoder can be shared when processing language and speech, or that the same fusion network can be shared when fusing human speech and gestures as well as robot visual and force sensors?\n\nIn this paper, we propose a principled approach for heterogeneity quantification via modality information transfer, an approach that measures the amount of transferable information from one modality to another. Our first proposed metric, (1) modality heterogeneity studies how similar 2 modalities {X 1 , X 2 } are by measuring how much usable information can be transferred from X 1 to X 2 , and our second metric, (2) interaction heterogeneity studies how similarly 2 modality pairs {X 1 , X 2 }, {X 3 , X 4 } interact by measuring how much usable interaction information can be transferred from {X 1 , X 2 } to {X 3 , X 4 }. We show the importance of these 2 proposed metrics in high-modality scenarios as a way to automatically prioritize the fusion of modalities that contain unique information or unique interactions, and otherwise sharing parameters across similar modalities displaying similar information or interactions.\n\nOperationalizing these ideas on a suite of 10 modalities, 15 prediction tasks, and 5 research areas, we show how to train a single model, HIGHMMT, that (1) improves the tradeoff between performance and efficiency over task-specific state-of-the-art models  [283, 367] , and general multimodal models with full parameter sharing  [15, 253, 276, 506] , (2) enables cross-modal transfer by pretraining on source tasks before transferring to new target modalities and tasks, and (3) is especially beneficial for low-resource scenarios (less training data and partially-observable modalities). Beyond these empirical results, we believe that our insights on quantifying heterogeneity and information sharing in multimodal models are independently useful for future work.",
      "page_start": 156,
      "page_end": 156
    },
    {
      "section_name": "High-Modality Multimodal Transformer",
      "text": "In this section, we describe our overall approach for high-modality representation learning (see Figure  9 .2). In §9.2.1, we formalize modality and interaction heterogeneity to understand whether modalities should be processed similarly or differently. Using these insights, §9.2.2 describes our proposed HIGHMMT model with dynamic parameter sharing based on heterogeneity measurements.",
      "page_start": 157,
      "page_end": 157
    },
    {
      "section_name": "Measuring Heterogeneity Via Modality Information Transfer",
      "text": "We begin our motivation by formalizing two important sources of heterogeneity in multimodal tasks. Firstly, modality heterogeneity occurs because the information present in different modalities often shows diverse qualities, structures, and representations. Secondly, interaction heterogeneity occurs because different modalities interact differently to give rise to new information when used for task inference. Formalizing and measuring these two sources of heterogeneity results in actionable insights for building multimodal models: measuring modality heterogeneity enables us to answer: should I use the same unimodal model to encode X 1 and X 2 ? Measuring interaction heterogeneity enables us to answer: should I use the same fusion model to fuse {X 1 , X 2 } and {X 3 , X 4 }? We will formalize heterogeneity via modality transfer, an approach that measures the amount of transferable information from one modality to another.\n\nEstimating modality heterogeneity via unimodal information transfer. We propose to measure heterogeneity between modalities X 1 and X 2 via unimodal transfer. Given a task Y defined over X 1 and X 2 , how well does an unimodal model trained on the task (X 1 ; Y ) transfer to (X 2 ; Y )? We choose model transfer as our focus of heterogeneity since it is captured at the level of features extracted via representation learning, rather than at the data-level. Even though the input data may be very different (e.g., images from different cameras or paraphrased sentences), effective feature extractors may be able to learn similar representations from them. Furthermore, it directly models task-relevance: the degree of heterogeneity depends on the end task, which enables using these heterogeneity measures subsequently for end-task optimization.\n\nWe formalize unimodal transfer as the difference in performance between unimodal models trained on X 1 before transfer to X 2 , versus those trained directly on X 2 . Specifically, we represent an unimodal model using modality X 2 with parameters θ as ŷ = f (y|x 2 ; θ). For a suitably chosen loss function ℓ(ŷ, y), define the loss of a model as E p(x 2 ,y) ℓ(f (y|x 2 ; θ), y) which measures the expected error over the joint distribution p(x 2 , y). To measure transfer, we train 2 models to obtain an approximation of task performance: the first randomly initialized and trained on the target task giving loss L * 2 ,\n\nL * 2 = min θ E p(x 2 ,y) ℓ(f (y|x 2 ; θ), y), (9.1) and the second using initialization from model parameters θ 1 trained on the source task (X 1 ; Y ) before fine-training on the target task giving loss L * 1→2 .\n\nwhere θ ← θ 1 denotes parameter initialization with θ 1 . Intuitively, L * 2 measures the (baseline) task-relevant information in X 2 , while L * 1→2 measures the task-relevant information transferable from X 1 to X 2 . The differences between these 2 losses,\n\ntherefore measures the difficulty of transferring a model trained on the source task (X 1 ; Y ) to a target task (X 2 ; Y ). Note that computing T (X 1 → X 2 ; Y ) only requires the training or fine-tuning of 2 models across the source and target modalities, which is efficient. In practice, the expectations over p(x 1 , y) and p(x 2 , y) are approximated using empirical samples from the training set (for model fine-tuning) and validation dataset (for final evaluation of performance). What are some properties of T (X 1 → X 2 ; Y )? For very different modalities X 1 and X 2 , we typically expect a source task (X 1 , Y ) to contain less usable information for a target task (X 2 ; Y ), which would imply that L * 1→2 ≥ L * 2 and therefore T (X 1 → X 2 ; Y ) ≥ 0 (i.e., positive distance). This is consistent with work demonstrating negative transfer across different modalities  [367, 369, 624, 658] . Under these scenarios, the larger the positive magnitude of T (X 1 → X 2 ; Y ), the more different modalities X 1 and X 2 are in the context of task Y (more difficult to transfer). However, there can also be cases of zero or even positive transfer (i.e., T (X 1 → X 2 ; Y ) ≤ 0), even in the surprising case of very different modalities  [391] . These cases reinforce the benefits of feature-based approaches to measure heterogeneity: while the raw modalities themselves seem very different, they can still be processed by similar models resulting in positive transfer, and should be assigned a difference of 0. Our final heterogeneity measure d(X 1 ; X 2 ) aggregates the non-negative value (to account for positive transfer) of transfer difficulty statistics across both transfer directions X 1 → X 2 and X 2 → X 1 :\n\nwhere x ≥0 = max(x, 0). Under certain assumptions on the modalities and tasks, our modality heterogeneity measure d(X 1 ; X 2 ) is a metric: it satisfies non-negativity: d(X 1 ; X 2 ) ≥ 0, with d(X 1 ; X 2 ) = 0 when X 1 = X 2 , and symmetry: d(X 1 ; X 2 ) = d(X 2 ; X 1 ), positivity, X 1 ≠ X 2 implies that d(X 1 ; X 2 ) > 0, and a relaxed version of the triangle inequality: d(X 1 ; X 3 ) ≤ d(X 1 ; X 2 ) + d(X 2 ; X 3 ). However, in the most general case, there may be settings where positivity and the triangle inequality are not satisfied since the exact dynamics of transfer learning is still not well understood for general deep networks: positive transfer can happen (which would imply cases of X 1 ≠ X 2 but d(X 1 ; X 2 ) = 0), and in practice, the relaxed triangle inequality is satisfied 96% of the time from a real heterogeneity matrix in Figure  9 .5.\n\nEstimating interaction heterogeneity via crossmodal information transfer. We are also interested in interaction heterogeneity: specifically, how differently should I fuse modalities {X 1 , X 2 } versus {X 3 , X 4 }? We therefore extend to crossmodal transfer by comparing the difference in performance between a multimodal model pretrained on (X 1 , X 2 ; Y ) before transfer to (X 3 , X 4 ; Y ), versus those trained directly on the target task (X 3 , X 4 ; Y ). In other words, we measure the difference in loss between\n\nand direct training\n\nThe distance d(X 1 , X 2 ; X 3 , X 4 ) after aggregation over tasks and transfer directions estimates the interaction heterogeneity between {X 1 , X 2 } and {X 3 , X 4 }.\n\nModality and interaction heterogeneity matrix. Finally, we construct a modality heterogeneity matrix M U (i, j) = d(X i ; X j ) and an interaction heterogeneity matrix (technically 4D-tensor) M C (i, j, k, ℓ) = d(X i , X j ; X k , X ℓ ). Determining parameter groupings to balance both total performance and parameter efficiency can be solved via agglomerative hierarchical clustering where modalities are nodes and heterogeneity measurements are edges. The number of clusters k is treated as a hyperparameter dependent on the parameter budget (see clustering examples in §9.3.1). Clustering on the modality heterogeneity matrix M U results in a grouping of modalities based on similarity (e.g., U 1 = {X 1 , X 2 , X 4 }, U 2 = {X 3 }, U 3 = {X 5 }), and likewise for the crossmodal matrix M C (e.g.,\n\n}}, and so on.\n\nComputational complexity. In a high-modality setting, suppose we are given a suite of modalities and tasks of the form {(X 1 , X 2 , Y 1 ), (X 1 , X 3 , X 4 , Y 2 ), ...} and so on, where there are a total of M unique modality and task pairs {(X 1 , Y 1 ), (X 2 , Y 1 ), (X 1 , Y 2 ), (X 3 , Y 2 ), (X 4 , Y 2 ), ...}. In practice, the number of unique (pairwise) interaction and task pairs {(X 1 , X 2 , Y 1 ), (X 1 , X 3 , Y 2 ), ...} is also O(M ), since the maximum number of modalities jointly observed for a task is never above a constant (at most 4 in all real-world datasets, and often 2 or 3). As an example in Figure  9 .5, our experiments involve M = 10 modality and task pairs (across 4 tasks defined on 2, 2, 3 and 3 modalities respectively), and 8 = (  2 2 ) + (  2 2\n\n) interaction and task pairs. The modality heterogeneity matrix for M unique modality and task pairs has M (M -1)/2 unique entries after removing the upper triangular portion due to symmetry and diagonal entries since d(X i , X i ) = 0. Computing these M (M -1)/2 entries exactly requires one to first train M unimodal models (to estimate the M L * m terms) before fine-tuning M (M -1) transfer models (to estimate the M (M -1) L * m→n terms), for a total of M 2 pre-trained and fine-tuned models. The interaction heterogeneity matrix also requires O(M 2 ) models for exact computation. However, we find that a key approximation can be made in practice: the heterogeneity matrices are highly structured due to distances approximately satisfying the triangle inequality, which implies that we do not need to compute all entries and instead rely on low-rank reconstruction from partial entries in practice. In our experiments, even using a low-rank approximation of r = 3 is sufficient to approximate the entire matrix. This suggests that we do not need to exhaustively measure unimodal and interaction transfer between all modality pairs to enjoy the benefits of our proposed approach. Instead, running a random sample of O(M ) pairs of heterogeneity values, and imputing the rest of the heterogeneity matrix, is sufficient in practice. Please see an example heterogeneity quantification for real-world datasets in §9.3.1.",
      "page_start": 157,
      "page_end": 157
    },
    {
      "section_name": "Capturing Heterogeneity And Homogeneity In Highmmt",
      "text": "Using these insights, we now describe our architecture for a general model HIGHMMT suitable for high-modality representation across many modalities and tasks (see Figure  9 .3). Training the HIGHMMT model consists of 2 main steps (see Figure  9 .4): (1) homogeneous pre-training of a fully shared model across all modalities, before (2) heterogeneity-aware fine-tuning to respect modality and interaction heterogeneity.\n\nHomogeneous pre-training. We first design a homogeneous multimodal model fully shared across all modalities and tasks with the following key components (see Figure  9 . as is already done for sequential data such as text, audio, and time series, and recently adapted for image patches too  [154] . For tables, sets, and graphs we treat each element in the table/set/graph as an element in the sequence. The end result is a standardized input data X m of dimension t m × d m , where t m is a modality and task-specific input sequence length, and d m is a modality and task-specific input dimension.\n\n2. Modality-specific embedding and positional encoding. For each distinct modality m ∈ M (which may appear across multiple tasks), we define a one-hot modality embedding e m ∈ R |M | , where |M | is the total number of distinct modalities, to identify common modalities across different tasks for information sharing. We also introduce Fourier feature positional encodings p m ∈ R tm×dpm , where d pm is the positional encoding dimension, to capture positional information across each modality. For multimodal tasks where a common dimension is shared across time (e.g., videos/time series), we apply a common positional encoding to capture the common time dimension (i.e., the first image frame occurs at the same time as the first word and first audio segment). Finally, the processed modality m is given by concatenating X m = X m ⊕ e m ⊕ p m ⊕ 0 m (i.e., the input sequence, modality embedding, positional encodings and zero-padding) into a standard dimension t m × d all . d all = max m∈M (d m + |M | + d pm ) where d m is the channel size of modality m, d pm is the positional encoding size of modality m, and |M | is the modality encoding size (i.e., the total number of involved modalities).\n\n3. Shared unimodal networks. Now that we have standardized all modalities into a common format, we design a general unimodal encoder with parameters U via a Transformer-based Perceiver block  [276] . Our model recursively trains a latent array Z m of shape d LN × d LS (where d LN is the sequence length/number of latent vectors and d LS is the latent dimension) that is random initialized as Z m for input to the next layer:\n\nwith trainable cross-attention parameters and keys and values K, V = Z 2 to learn attention from Z 2 to Z 1 , and a separate block to capture the attention in the opposite direction.\n\nand vice-versa for Z 1→2 , with parameters\n\n. This step enables one modality's elements to discover bidirectional interactions with another, resulting in a final multimodal representation Z mm = [Z 1→2 , Z 2→1 ] of shape d LS ×2d k . For each layer, we first perform cross-attention followed by self-attention and feed-forward functions.\n\nFor tasks with more than 2 modalities, a Crossmodal Transformer block is applied for each pair of modalities before concatenating all representations. 5. Task-specific classifier and multitask pre-training. Finally, on top of Z mm , we use a separate linear classification layer per task. To enable information sharing across modalities and tasks, homogeneous pre-training is performed across a diverse set of datasets in a multitask manner by optimizing a weighted sum of losses over tasks. The result is a single set of shared unimodal parameters U * that encodes all modalities, and a single set of shared crossmodal parameters C * that captures all pairwise interactions between modality pairs, along with all modality-specific embeddings E * and task-specific classifiers T * .\n\nHeterogeneity-aware fine-tuning. Finally, we account for heterogeneity by grouping unimodal parameters based on modalities that we know to be similar from §9.2.1 (e.g., setting\n\n}), and likewise for the crossmodal parameters (e.g.,   The modality embeddings E * and task classifiers T * are jointly fine-tuned as well. Fine-tuning is also performed in a multitask manner by optimizing a weighted sum of supervised losses across all modalities and tasks.",
      "page_start": 160,
      "page_end": 160
    },
    {
      "section_name": "Experiments",
      "text": "Setup: In this section, we design experiments to analyze the multitask, transfer, and generalization capabilities of HIGHMMT. We use a large collection of multimodal datasets provided in MultiBench  [367]  spanning 10 modalities, 15 prediction tasks, and 5 research areas. We trained 3 multitask models across combinations of these datasets (see Table  9 .1 for details). Overall, the total size of datasets involved in our experiments exceeds 370, 000 and covers diverse modalities such as images, video, audio, text, time-series, robotics sensors, sets, and tables, prediction tasks spanning the image-caption matching, robot pose, object pose, robot contact, design interfaces, digits, humor, sentiment, emotions, mortality rate, and ICD-9 codes from the research areas of affective computing, healthcare, multimedia, robotics, and HCI.",
      "page_start": 163,
      "page_end": 163
    },
    {
      "section_name": "Modality Heterogeneity Matrix",
      "text": "Interaction heterogeneity matrix  Figure 9 .5: Modality and interaction heterogeneity matrices color coded by distances, with green showing smaller distances and dark red larger distances. We find clear task outliers (AV-MNIST has high difficulty transferring to others), and that there is generally more interaction heterogeneity than unimodal heterogeneity. Otherwise, the same modality and modality pairs across different tasks are generally similar to each other.",
      "page_start": 164,
      "page_end": 164
    },
    {
      "section_name": "Heterogeneity Measurements And Parameter Groups",
      "text": "We begin with a study of the heterogeneity matrices in Figure  9 .5 and the resulting parameter groups.\n\nModality heterogeneity: We first notice that the modalities from AV-MNIST only transfer well to each other and has high difficulty transferring to other modalities from the other datasets. The same modality across different tasks is generally similar to each other (e.g., text between UR-FUNNY and MOSEI, audio between UR-FUNNY and MOSEI). The text modality in UR-FUNNY seems to be close to most other modalities, and likewise for the tabular modality in MIMIC. It is also worth noting that the video and audio modalities are not the most informative in MOSEI, and predictions are dominated by language  [713] , which may explain their general homogeneity with respect to other modalities.\n\nInteraction heterogeneity: There is generally more interaction heterogeneity than unimodal, implying that the interactions between modality pairs tend to be more unique. Again, we notice the general poor transfer from the modality pair (image+audio) in AV-MNIST to other pairs, and the general strong transfer from (audio+text) in UR-FUNNY to the rest, which shows a relationship between modality and interaction heterogeneity. We also find that the same modality pairs (video+text) and (video+audio) shows crossmodal similarity across both datasets they appear in: MOSEI and UR-FUNNY. Finally, while the triplet of crossmodal pairs in MOSEI are quite different from each other, those in UR-FUNNY are more similar.\n\nUsing these measurements, we show the final groups of parameters obtained after clustering the matrices for different values of Finally, we observe the low-rank nature of the heterogeneity matrices due to symmetry and approximate triangle inequality, such that even using a low-rank approximation of r = 3 is sufficient to approximate the entire matrix. This suggests that we do not need to exhaustively measure unimodal and interaction transfer between all modality pairs to enjoy the benefits of our proposed approach.",
      "page_start": 164,
      "page_end": 165
    },
    {
      "section_name": "Qualitative Results",
      "text": "We now present our results on the multitask, transfer, and generalization capabilities of HIGHMMT using performance and efficiency metrics. Henceforth, we will refer to the following models:\n\n(1) HIGHMMT share none refers to individual copies of HIGHMMT models, one for each task.\n\n(2) HIGHMMT share all refers to one single HIGHMMT model fully shared across all modalities and tasks.\n\n(3) HIGHMMT refers to the full heterogeneity-aware HIGHMMT model across all modalities and tasks with learned parameter groupings based on heterogeneity measurements. All task-specific model combinations (>10,000) ) combinations of task-specific models across multiple datasets  [367] . The x-axis denotes (inverted) total parameters and y-axis denotes performance scaled to a 0 -1 range before averaging across datasets.\n\nMultitask performance and efficiency. In Figure  9 .6, we summarize the overall tradeoff between performance and efficiency using existing taskspecific models and variants of HIGHMMT. The blue dots represent all possible combinations of taskspecific models across multiple datasets (summarized in MultiBench  [367] , > 10 5 total combinations) with their overall performance (scaled to a 0 -1 range before averaging across datasets) and overall efficiency (inverted total number of parameters). The red dots represent the state-of-the-art Pareto front: points that are not strictly dominated in both performance and efficiency. In light green, separate single-task HIGH-MMT models (share none) already improve parameter efficiency as compared to standard Multimodal Transformers  [390, 614] . In dark green is HIGH-MMT (share all) trained in a homogeneous multitask manner (i.e., with full parameter sharing across unimodal and multimodal layers within and across tasks), which further pushes forward the Pareto front by improving both performance and efficiency. Finally, in orange, HIGHMMT with heterogeneity-aware finetuning achieves significantly better tradeoffs between performance and efficiency, with efficiency and consistently high performance across multiple modalities and tasks. Table  9 .3: Cross-modal few-shot transfer to new modalities and tasks. We train multitask HIGHMMT on 1/2/3 datasets and find that it generalizes few-shot to new modalities and tasks on the 4th dataset, with improved performance over single-task training on the 4th dataset. Cross-modal transfer improves with more pretraining tasks and works best on the smallest target tasks (UR-FUNNY). The suite of HIGHMMT models is obtained by tuning k, the total number of unimodal and crossmodal parameter groups (i.e., the number of clusters when clustering heterogeneity matrices). k can be seen as a hyper-parameter depending on the computational budget, with smaller k implying more parameter sharing on lower budgets and vice-versa. In Table  9 .2, we show the effect of k on average performance and total parameters. We test k in the range {2, where |U|, |C| denote the number of unimodal and crossmodal parameter groups. We see a controllable tradeoff: starting with a fully shared model and increasing the number of parameter groups, we also see steadily improving performance approaching task-specific state-of-the-art models. Overall, optimizing for performance results in a model as strong as current state-of-the-art models while using 8× fewer total parameters. Optimizing for efficiency results in a model that reaches within 96% of current state-of-the-art performance but using 30× fewer total parameters (mean and deviation over 10 runs).\n\nPositive transfer to new modalities and tasks. HIGHMMT also offers opportunities to study whether we can transfer knowledge between completely different modalities and tasks. Starting with the collection of 4 datasets in the order MOSEI, AV-MNIST, MIMIC, and UR-FUNNY ranked by largest dataset size (total of datapoints and memory storage per datapoint), we pre-train a fully-shared HIGHMMT model on 1/2/3 of the 4 tasks before fine-tuning on the fourth task only (e.g., train on MOSEI and transfer to UR-FUNNY, on MOSEI+AV-MNIST then transfer to UR-FUNNY, and on MOSEI+AV-MNIST+MIMIC then transfer to UR-FUNNY, and likewise for transfer to the other 3 datasets.\n\nFrom Table  9 .3, we found that on all four combinations of multitask pretraining and finetuning, weights learned from other multimodal tasks generalize well to new modalities and tasks, improving performance over single target-task training (mean and standard deviation over 10 runs). When we increase the number of pretraining datasets, we observe a consistent Table  9 .4: HIGHMMT achieves strong performance on overall performance and efficiency (mean and deviation over 10 runs), sometimes even beating (shown in bold) the task-specific state-of-the-art, especially on the relatively understudied modalities (time-series, robotics sensors, and sets) from the robotics (PUSH, V&T) HCI (ENRICO), and healthcare (MIMIC) research areas, while using 10× fewer parameters due to parameter sharing and multitask learning. SOTA captures the max performance and parameters of more than 20 task-specific multimodal models: [1] GRADBLEND  [652] ,  [2]  LF-LSTM  [148] ,  [3]  LF  [184] ,  [4]  MULT  [614] ,  [5]  MFAS  [479] ,  [6]  MFM  [687] , and  [7]  LRTF  [724] . improvement in fine-tuned target task performance. There is an inverse correlation between target task size and performance improvement: the smallest dataset, UR-FUNNY, benefited the most (+2.4%) from transfer learning from 0 to 3 multitask datasets. This implies that our multimodal pretraining-fine-tuning paradigm is useful for low-resource target modalities and tasks. Finally, we compare transfer learning performance across different levels of partial observability. While one would expect the transfer to MIMIC to be the hardest due to its modality set {time-series, table} being completely disjoint from the remaining 3 datasets, we still observe a +0.8% gain as compared to single-task training. Therefore, HIGHMMT can generalize to new modalities and tasks. Unsurprisingly, for datasets with more overlap (e.g., UR-FUNNY with complete overlap in {text, video, audio} with respect to pretraining), we find larger improvements using transfer learning over single-task models (+2.4%).",
      "page_start": 165,
      "page_end": 167
    },
    {
      "section_name": "Model",
      "text": "Comparison with task-specific state-of-the-art. In Table  9 .4, we compare multitask performance and efficiency with task-specific state-of-the-art models. We achieve performance within the range of published models (and usually close to the individual task-specific state-of-the-art) in MultiBench, which tallies more than 20 recent multimodal models in each task's literature  [367] . In fact, HIGHMMT even sets new state-of-the-art results on several datasets, especially on the relatively understudied modalities (time-series, force and proprioception sensors, and sets) from the robotics (PUSH, V&T) and HCI (ENRICO) research areas. On top of strong performance, the main benefit lies in using fewer total parameters as compared to separate task-specific models -more than 10× reduction. Since this reduction grows with the number of tasks, our approach is scalable to high-modality scenarios.\n\nPartial-observability. Observe HIGHMMT performance on partially-observable modality subsets (i.e., target task involving modalities not present in the other tasks): from Table  9 .4, we find that the model performs well on the MIMIC dataset despite its modality set {time-series, table} being completely disjoint from the remaining 3 datasets -we obtain similar performance across both multitask and single-task models (68.2 ± 0.3% vs 68.9 ± 0.5%). We find that HIGHMMT multitask also works on ENRICO dataset in HCI (52.7±0.6% multitask vs 51.0±1.4% single-task) despite it having completely disjoint modality inputs.\n\nMultitask fusion and retrieval. We perform multitask training over multimodal fusion in 57.9 ± 0.3 61.9 ± 2.1 63.0 ± 0.9 59.5 ± 1.4 60.6 ± 0.7 -w/o crossmodal  [506]  63.8 ± 1.0 79.5 ± 0.5 67.9 ± 0. AV-MNIST and retrieval in CIFAR-ESC. While fusion emphasizes information integration, retrieval focuses on aligning corresponding elements expressed through different views of the data  [375] . Even across these vastly different prediction tasks, we find that multitask training (60.5% retrieval accuracy) improves upon single-task training (58.8%). Not only have the unimodal networks simultaneously processed different modalities, but the crossmodal network has captured correspondences useful for both fusion and retrieval.",
      "page_start": 167,
      "page_end": 168
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this subsection, we carefully ablate the model architectures, parameter sharing, and training decisions.\n\nArchitectural ablations. We first analyze each architectural component of HIGHMMT: (1) w/o embeddings removes the only modality-specific component in the model -the modality embeddings. We set embeddings for all modalities to be the same to test whether a modalityspecific component is necessary to capture heterogeneity across input data sources, (2) w/o unimodal removes the unimodal encoder and directly applies the cross-attention layer, and w/o crossmodal replaces the crossmodal layer with a concatenation of unimodal features and a linear classification layer. The latter resembles the most direct multimodal extension of existing work in shared unimodal encoders like Perceiver  [276] , MultiModel  [291] , ViT-BERT  [353]  or PolyViT  [378] . From Table  9 .5, removing any of the 3 components in HIGHMMT results in worse performance. The unimodal encoder is particularly important.\n\nParam sharing ablations. We further ablate with respect to possible parameter sharing settings in HIGHMMT: (1) share none uses separate unimodal and multimodal layers reminiscent of typical single-task multimodal transformers  [232, 390, 614] , (2-3) share unimodal (crossmodal) only shares the unimodal (crossmodal) layer during multitask training, (4) share all shares all parameters without accounting for possible heterogeneity  [506] , (5) random difference determines k parameter groups randomly rather than via heterogeneity measurements, (6) feature difference uses feature-level divergences on jointly trained unimodal encoders (i.e., ∥U (X 1 ) -U (X 2 )∥ 2  2 ) rather than transfer performance to measure heterogeneity as is commonly done in transfer learning and domain adaptation  [132, 576] . From Table  9 .5, our proposed heterogeneity-aware parameter grouping results in the best overall performance as compared to fully shared, fully separate, or parameter grouping informed by other heterogeneity measures such as random or feature distance.\n\nTraining ablations. Finally, we explore w/o homogeneous pretraining: directly learning a model with parameter groups as selected by our approach as opposed to performing homogeneous pre-training before fine-tuning them into parameter groups. From Table  9 .5, we find that this ablation underperforms -training parameter groups from scratch overfits to smaller datasets which hurts overall performance.",
      "page_start": 168,
      "page_end": 169
    },
    {
      "section_name": "Understanding Homogeneity And Heterogeneity In Highmmt",
      "text": "We now take a deeper empirical analysis to better understand HIGHMMT, through parameter overlap and interference experiments.\n\nParameter overlap. Starting with a trained multitask HIGHMMT, we use a gradient-based method  [221]  to determine how much each parameter is involved in a specific task. For each task T and parameter θ ∈ Θ in multitask model M Θ , we compute the involvement I T (θ) = E (x,y)∈T |∇ θ M Θ (y|x)| where M Θ (y|x) is the predicted probability of correct target y by M Θ given x as input. In other words, this measures the absolute gradient with respect to θ when predicting y given x in task T . A higher absolute gradient implies \"activated\" neurons and vice-versa for gradients closer to 0. This enables us to compute the extent a parameter θ is involved for each task. The number of tasks a given parameter θ is involved in can then be approximated by thresholding and summing up n(θ) = ∑ T (1{I T (θ) > ϵ max(I 1 (θ), I 2 (θ), I 3 (θ), I 4 (θ)}) which returns an integer from 1 to 4. We chose a threshold ϵ such that parameters are classified as active about half the time on average, which occurs at ϵ = 0.2. Since we are interested in the level of parameter overlap in the shared unimodal encoder and multimodal layer, we set θ as these 2 modules and report results in Table  9 .6. There is evidence of significant parameter overlap across unimodal encoders: more than 92% of neurons are involved in at least 3 of the 4 tasks. On the other hand, there is not nearly as much parameter overlap in the multimodal layer: only 10% of neurons are involved in 3 or 4 tasks. Hence, it seems like the unimodal encoders learn taskagnostic representations, but the subsequent multimodal layers (closer to task-specific classifiers) capture more task-specific information. This also reinforces our observation in §9.3.1 that there is generally more interaction heterogeneity than modality heterogeneity, which suggests using fewer unimodal parameter groups and more crossmodal parameter groups.\n\nParameter interference. Another empirical proof for parameter sharing in multitask models is the phenomenon of parameter interference: to what extent do parameters interfere with each other across tasks? We perform an experiment to investigate parameter interference: we pick one task and flip the labels in its training set, train the multitask model on the modified training set, and see how the incorrectly labeled task affects performance on other tasks. This experiment provides evidence of information sharing: if the multitask model does not share information (i.e., the model learns independent subspaces for each task), then one would not observe negative interference from one noisy dataset. We study negative interference under From Table  9 .7, certain tasks are more affected by negative interference (e.g., AV-MNIST), while some tasks are not influenced as much (e.g., UR-FUNNY). Again, this reflects our heterogeneity measurements in §9.3.1, where AV-MNIST displays high heterogeneity. Furthermore, performance drops due to training the unimodal encoders are the most significant, which corroborates with our parameter overlap and heterogeneity analysis that unimodal encoders contain more entangled parameters which are more sensitive to task changes. On the other hand, multimodal layers contain more disentangled parameters, which results in higher heterogeneity measurements and needs more separate parameter groups.",
      "page_start": 169,
      "page_end": 170
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Transformers have emerged as strong models for representation learning. Building upon the Transformer  [632] , multimodal extensions use either full self-attention over modalities concatenated across the sequence dimension  [108, 348, 572, 577]  or a cross-modal attention layer  [390, 589, 614] , and are useful for sequential data by automatically aligning and capturing complementary features at different time-steps  [337, 614, 695] . Self-supervised multimodal pretraining has emerged as an effective way to train these architectures, with the aim of learning representations from large-scale unlabeled multimodal data before transferring to downstream tasks via fine-tuning  [348, 390, 572] . These pretraining objectives typically consist of unimodal masked prediction, crossmodal masked prediction, and multimodal alignment prediction  [232] .\n\nUnified encoder for unimodal learning. Several works such as Perceiver  [275, 276] , Multi-Model  [291] , ViT-BERT  [353] , and PolyViT  [378]  have explored the possibility of using the same architecture for different inputs on unimodal tasks (i.e., language, image, video, or audio-only). The Transformer architecture has emerged as a popular choice due to its suitability for serialized inputs such as text  [144] , images  [154] , video  [577] , and time-series data  [379] , a phenomenon further observed by Lu et al.  [391]  where a single Transformer pretrained on text transfers to sequence modeling and image classification. While these serve as building blocks in our model, our focus is on a general-purpose multimodal model for multitask and transfer learning across different subsets of modalities rather than unimodal tasks.\n\nMultimodal multitask and transfer learning. There have also been several attempts to build a single model that works well on a suite of multimodal tasks  [113, 348, 390, 506, 572] . For example, UniT  [253] , VLBERT  [572] , ViLBERT  [390] , and VL-T5  [113]  are all unifying models for vision-and-language tasks. VATT  [15]  jointly trains a shared model on video, audio, and text data to perform audio-only, video-only, and image-text retrieval tasks. FLAVA  [552]  found that pretraining a shared model with unpaired images, unpaired text, and image-text pairs results in strong performance on image-only, text-only, and image-text multimodal tasks, while Reed et al.  [506]  scales up a single Transformer model for image, text, and decision-making tasks. However, all of these train a single model for all tasks, without investigating how heterogeneity can necessitate partial parameter sharing. On the transfer side, while more research has focused on transfer within the same modality with external information  [158, 554, 676, 717] , Liang et al.  [369]  is the only work that studies transfer to completely new modalities. However, they require paired data collection and modality-specific modeling. Our work goes beyond the commonly studied language, vision, and audio modalities to relatively understudied ones (e.g., tabular data, time-series, sensors, graphs, and set data). Furthermore, we show the possibility of generalizing to new modality subsets. Finally, our work also complements studies of transfer learning in a single modality  [567, 674, 719] , where insights from task heterogeneity have informed multitask approaches, as well as multisensor fusion in various domains such as healthcare  [429]  and robotics  [585, 592] .",
      "page_start": 170,
      "page_end": 171
    },
    {
      "section_name": "Conclusion",
      "text": "We propose an information transfer approach for estimating modality and interaction heterogeneity, a key component towards automatically determining which modalities should be processed and fused jointly for efficient representation learning in high-modality scenarios. Our resulting model, HIGHMMT dynamically determines the optimal parameter groupings balancing total performance and parameter efficiency, simultaneously achieves strong results on modalities (text, image, video, audio, time-series, sensors, tables, and sets) and tasks from different research areas, and transfers to new modalities and tasks during fine-tuning. We release our code and benchmarks which we hope will present a unified platform for subsequent analysis.",
      "page_start": 171,
      "page_end": 171
    },
    {
      "section_name": "Chapter 10 Conclusion",
      "text": "In this thesis, we advanced the foundations of multimodal machine learning by highlighting its key principles and core challenges. In the bulk of the thesis, we outlined our progress towards understanding the foundations of multimodal interactions and new modeling methods for generalizable representation learning across many input modalities and tasks. This concluding chapter provides a summary of the main contributions, discusses potential limitations, and outlines future research directions in multimodal artificial intelligence.",
      "page_start": 172,
      "page_end": 172
    },
    {
      "section_name": "Summary Of Thesis Contributions",
      "text": "Multimodal artificial intelligence is one of the most exciting subareas of artificial intelligence research today, and has the potential to make major impacts in autonomous agents with digital, physical, and social capabilities. This thesis aims to pave a foundation for multimodal artificial intelligence so that future students and researchers are able to better understand the breadth and depth of multimodal research today, are equipped with the scientific fundamentals required to perform cutting-edge research in this field, and are up-to-date with practical methods for machine learning from real-world multimodal datasets.\n\nTo summarize the contributions of this thesis, we began (in Section 2) by outlining the theoretical and computational foundations of multimodal machine learning by synthesizing a broad range of theoretical frameworks and application domains from both historical and recent perspectives. This foundation involves three key principles of modality heterogeneity, connections, and interactions often present in multimodal problems which brings unique challenges to machine learning, which we outline through a taxonomy of six core challenges: representation, alignment, reasoning, generation, transference, and quantification. This taxonomy enables researchers to navigate the breadth of recent technical achievements and enables us to identify key open problems for future research.\n\nIn this first major part of this thesis, we build a foundation for multimodal interactions: the basic principle of how modalities combine to give rise to new information for a task. Section 3 presented an information-theoretic framework formalizing how modalities interact with each other to give rise to new information for a task, which can be decomposed into redundancy, uniqueness, and synergy  [371] . Using this theoretical framework, we proposed two practical estimators to quantify the interactions in real-world datasets. Quantifying the types of interactions a multimodal task requires enables researchers to understand their data and choose the right model to learn interactions in a principled way. Using this foundation of multimodal interactions, we design new self-supervised approaches to learn these interactions  [373]  (Section 4), visualization tools for practitioners to analyze whether their model has succeeded in learning  [374]  (Section 5), and new guidelines for practitioners to decide which modality to collect for maximum increase in performance  [376]  (Section 6).\n\nIn the second major part of this thesis, we design practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities such as videos, physical sensors, and medical data. Section 7 introduced MULTIBENCH, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas enabling research towards multimodal foundation models  [367] . Section 8 presented the cross-modal attention  [101, 359]  and multimodal transformers  [614]  architectures that are suitable for learning the interactions across many elements in modality sequences such as text, videos, time-series, and sensors. Finally, Section 9 showed how we can scale these architectures on MULTIBENCH to create general-purpose multimodal multitask models across a variety of tasks, including collaborating with practitioners to apply these models for real-world impact on affective computing, mental health, and cancer prognosis.\n\nTogether, our contributions deliver fundamental methodological and practical insights in multimodal learning, presenting approaches that are principled and explainable to practitioners while also capturing the benefits of scale across many modalities and tasks. Some of the work done during the PhD but not included in this thesis also paves a way towards improving the robustness, safety, and efficiency of multimodal models for real-world deployment.",
      "page_start": 172,
      "page_end": 173
    },
    {
      "section_name": "Limitations And Future Directions",
      "text": "Finally, we conclude this thesis by identifying the following future research challenges in multimodal artificial intelligence:\n\nRepresentation: Learning multimodal representations is the cornerstone of multimodal machine learning. There has been substantial progress towards increasingly expressive and performant multimodal representations. However, there remain key challenges in their theoretical understanding and generalization beyond image and text.\n\nTheoretical and empirical frameworks: How can we formally define the three core principles of heterogeneity, connections, and interactions? Can we quantify their presence in multimodal datasets and models, and understand whether current multimodal representation learning methods are suitable for learning different interactions? Answering these fundamental questions will lead to a better understanding of the capabilities and limitations of current multimodal representations, and inspire the development of new methods in a principled manner.\n\nBeyond additive and multiplicative cross-modal interactions: While recent work has been successful at modeling multiplicative interactions of increasing order, how can we capture causal, logical, and temporal connections and interactions? What is the right type of data and domain knowledge necessary to model these relationships? Modeling these interactions in a principled manner could lead to systems that are more robust, compositional, and explainable than those based fully on neural networks. Tabular, sensors, and time-series: Existing work has shown success in learning image, text, and audio-visual representations. However, tabular and time-series data are prevalent in many realworld applications such as healthcare and autonomous vehicles. How can we learn multimodal interactions between the best encoders for tabular and sensor data, which may not be based on deep learning (e.g., decision trees, time-series analysis), and neural network representations that are state-of-the-art for the text and image modalities?\n\nBrain and multimodal perception. There are many core insights regarding multimodal processing to be gained from human cognition, including the brain's multimodal properties  [314]  and mental imagery  [435] . How does the human brain represent different modalities, how is multisensory integration performed, and how can these insights inform multimodal learning? In the other direction, what are opportunities in processing high-resolution brain signals such as fMRI and MEG/EEG, and how can multimodal learning help in the future analysis of data collected in neuroscience?\n\nAlignment: There remain important challenges in aligning modality elements when these elements are extremely fine-grained in nature and exhibit long-range patterns across time.\n\nMemory and long-term interactions. Many current multimodal benchmarks only have a short temporal dimension, which has limited the demand for models that can accurately process long-range sequences and learn long-range interactions. Capturing long-term interactions presents challenges since it is difficult to semantically relate information when they occur very far apart in time or space and raises complexity issues. How can we design models (perhaps with memory mechanisms) to ensure that these long-term cross-modal interactions are captured?\n\nReasoning: Today's multimodal systems, especially those based on deep learning or large language models, are still not capable of robust and complex reasoning. We outline two challenges in compositional and interactive reasoning.\n\nMultimodal compositionality. How can we understand the reasoning process of trained models, especially regarding how they combine information from modality elements? This challenge of compositional generalization is difficult since many compositions of elements are typically not present during training, and the possible number of compositions increases exponentially with the number of elements  [602] . How can we best test for compositionality, and what reasoning approaches can enable compositional generalization?\n\nMultimodal embodiment and interaction. Most of today's multimodal systems are trained to make predictions without the capability to take actions in the world. The next generation of these systems will be those that can plan actions, imagine the effect these actions will have on the world, and choose the right sequence of actions over a long period of time to solve complex tasks. We have begun to build these interactive multimodal agents for the virtual world, such as processing multimedia web data to help humans with web tasks like online shopping, travel bookings, and content management. Building multisensory robotic systems that can actions in the real world, while respecting safety and robustness, is another long-term future direction.\n\nGeneration: The incredible advances of generative AI have inspired many future directions in generating multimedia content.\n\nMultimodal creation. Synchronized creation of realistic video, text, and audio remains a challenge. These systems can be applied for entertainment, such as generating music videos, virtual avatar characters, virtual humans, and more. It is also likely that better multimodal generative models of the world can serve as world models to train planning and sequential decision making agents.\n\nReal-world ethical concerns. However, the recent success in generation has brought ethical concerns regarding their use. For example, large-scale pretrained language models can generate text denigrating to particular social groups  [543] , toxic speech  [193] , and sensitive pretraining data  [81] . Future work should study how these risks are potentially amplified or reduced when the dataset is multimodal, and whether there are ethical issues specific to multimodal generation.\n\nTransference: Advances in foundation models have also enabled increasingly general-purpose models that can transfer information and knowledge across a wide range of modalities and tasks. This opens up new directions in high-modality learning.\n\nHigh-modality learning aims to learn representations from an especially large number of heterogeneous data sources, which is a common feature of many real-world multimodal systems such as self-driving cars and IoT  [263] . More modalities introduce more dimensions of heterogeneity, incur complexity challenges in unimodal and multimodal processing, and require dealing with non-parallel data (i.e., not all modalities are present at the same time).\n\nQuantification: Finally, we highlight several important lines of future work in quantifying and understanding key design decisions in the multimodal learning process.\n\nModality utility, tradeoffs, and selection. How can we formalize why modalities can be useful or potentially harmful for a task? There are also challenges in quantifying modality and social biases and robustness to imperfect, noisy, and out-of-distribution modalities. Future work should come up with formal guidelines to compare these tradeoffs and select the optimal set of modalities balancing performance with these other potential concerns, which can help practitioners decide the right modalities to work with.\n\nExplainability and interpretability. Before models can be safely used by real-world stakeholders in domains such as medicine, autonomous systems, and user interfaces, we need to understand how to interpret their inner workings. How can we evaluate whether these phenomena are accurately interpreted? These challenges are exacerbated for relatively understudied modalities beyond language and vision, where the modalities themselves are not easy to visualize. Finally, how can we tailor these explanations, possibly in a human-in-the-loop manner, to inform real-world decision-making?\n\nIn conclusion, we believe that this thesis can lay the theoretical and practical foundations for multimodal machine learning and inspire future work towards these open problems.",
      "page_start": 173,
      "page_end": 175
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 1: This thesis is designed to advance the theoretical and computational foundations of multimodal",
      "page": 25
    },
    {
      "caption": "Figure 1: 2: I have also pursued the following directions during my Ph.D. studies: (1) new machine",
      "page": 31
    },
    {
      "caption": "Figure 2: 1). These core multimodal challenges are understudied in",
      "page": 36
    },
    {
      "caption": "Figure 2: 1: Core research challenges in multimodal learning: Every multimodal problem typically requires",
      "page": 37
    },
    {
      "caption": "Figure 2: 2: The information present in different modalities will often show diverse qualities, structures,",
      "page": 40
    },
    {
      "caption": "Figure 2: 2 for an illustration). These dimensions are complementary and",
      "page": 40
    },
    {
      "caption": "Figure 2: 3). From a statistical data-driven perspective, connections are identified",
      "page": 41
    },
    {
      "caption": "Figure 2: 3: Modality connections describe how modalities are related and share commonalities, such as",
      "page": 42
    },
    {
      "caption": "Figure 2: 4, we provide a high-level illustration of some dimensions of interactions that can exist.",
      "page": 42
    },
    {
      "caption": "Figure 2: 4: Several dimensions of modality interactions: (1) Interaction information studies whether",
      "page": 43
    },
    {
      "caption": "Figure 2: 6). In fusion with abstract modalities, suitable unimodal encoders",
      "page": 43
    },
    {
      "caption": "Figure 2: 5: Challenge 1 aims to learn representations that reflect cross-modal interactions between",
      "page": 45
    },
    {
      "caption": "Figure 2: 6: We categorize representation fusion approaches into (1) fusion with abstract modalities,",
      "page": 45
    },
    {
      "caption": "Figure 2: 7: There is a spectrum of representation coordination functions: strong coordination aims to",
      "page": 46
    },
    {
      "caption": "Figure 2: 7). In contrast to representation fusion, coor-",
      "page": 46
    },
    {
      "caption": "Figure 2: 8: Representation fission creates a larger set of decoupled representations that reflects knowledge",
      "page": 47
    },
    {
      "caption": "Figure 2: 9: Alignment aims to identify cross-modal connections and interactions between modality",
      "page": 48
    },
    {
      "caption": "Figure 2: 10: Discrete alignment identifies connections between discrete elements, spanning (1) local",
      "page": 49
    },
    {
      "caption": "Figure 2: 11: Continuous alignment tackles the difficulty of aligning continuous signals where element",
      "page": 50
    },
    {
      "caption": "Figure 2: 12: Contextualized representation learning aims to model modality connections to learn better",
      "page": 51
    },
    {
      "caption": "Figure 2: 13: Reasoning aims to combine knowledge, usually through multiple inferential steps, exploiting",
      "page": 52
    },
    {
      "caption": "Figure 2: 13). (1) Structure modeling involves defining or learning the",
      "page": 52
    },
    {
      "caption": "Figure 2: 14: Structure modeling aims to define the relationship over which composition occurs, which",
      "page": 53
    },
    {
      "caption": "Figure 2: 15: How can we learn a generative process to produce raw modalities that reflect cross-modal",
      "page": 56
    },
    {
      "caption": "Figure 9: 3). These three categories are distinguished based on the information change",
      "page": 56
    },
    {
      "caption": "Figure 2: 16: Transference studies the transfer of knowledge between modalities, usually to help a noisy",
      "page": 58
    },
    {
      "caption": "Figure 2: 17: Quantification: what are the empirical and theoretical studies we can design to better",
      "page": 60
    },
    {
      "caption": "Figure 2: 18: The subchallenge of heterogeneity quantification aims to understand the dimensions of",
      "page": 61
    },
    {
      "caption": "Figure 2: 19: Quantifying modality interconnections studies (1) connections: can we discover what",
      "page": 62
    },
    {
      "caption": "Figure 2: 20: Studying the multimodal learning process involves understanding (1) generalization across",
      "page": 63
    },
    {
      "caption": "Figure 3: 1 shows a depiction of these four measures, which we refer to as PID statistics.",
      "page": 64
    },
    {
      "caption": "Figure 3: 1: PID decomposes I(X1,X2;Y ) into redun-",
      "page": 65
    },
    {
      "caption": "Figure 3: 1): redundancy R between X1 and X2, uniqueness U1 in X1",
      "page": 65
    },
    {
      "caption": "Figure 3: 2: We propose BATCH, a scalable estimator for PID over high-dimensional continuous distribu-",
      "page": 68
    },
    {
      "caption": "Figure 3: 2. To explain our approach, we first describe (1) how we parameterize",
      "page": 68
    },
    {
      "caption": "Figure 3: 3: Left to right: (a) Contour plots of the GMM’s density for ∣∣µ∣∣2 = 2.0. Red line denotes the",
      "page": 69
    },
    {
      "caption": "Figure 3: 3. Overall, we",
      "page": 70
    },
    {
      "caption": "Figure 3: 4: We find high correla-",
      "page": 74
    },
    {
      "caption": "Figure 3: 4, we find that the implication only",
      "page": 74
    },
    {
      "caption": "Figure 3: 5: PID agreement α(f,D)",
      "page": 74
    },
    {
      "caption": "Figure 3: 5. This shows that PID",
      "page": 75
    },
    {
      "caption": "Figure 9: 1 for a visual depiction and experimental results showing the per-",
      "page": 78
    },
    {
      "caption": "Figure 4: 1: Left: We define S = I(X1;X2;Y ) as task-relevant shared information and U1 = I(X1;Y ∣X2),",
      "page": 79
    },
    {
      "caption": "Figure 4: 2: FACTORCL: We propose a self-supervised CL method to learn factorized representations",
      "page": 83
    },
    {
      "caption": "Figure 4: 3, we show these two bounds",
      "page": 83
    },
    {
      "caption": "Figure 4: 3) and comes for “free” via jointly maximizing",
      "page": 83
    },
    {
      "caption": "Figure 4: 3: Estimated INCE lower bound [453] and our proposed upper bound INCE-CLUB on sample",
      "page": 84
    },
    {
      "caption": "Figure 4: 4: Standard vs. unique augmentations",
      "page": 85
    },
    {
      "caption": "Figure 4: 4 shows an example of unique augmentation that satisfies these",
      "page": 86
    },
    {
      "caption": "Figure 9: 1, we show our main result on synthetic data comparing FACTORCL",
      "page": 87
    },
    {
      "caption": "Figure 4: 4 for augmentations from both strategies). In",
      "page": 89
    },
    {
      "caption": "Figure 5: 1). To tackle the challenges of visualizing model behavior,",
      "page": 92
    },
    {
      "caption": "Figure 5: 1: Left: We scaffold the problem of multimodal interpretability and propose MULTIVIZ,",
      "page": 93
    },
    {
      "caption": "Figure 5: 2: Examples of cross-modal interactions discovered by our proposed second-order gradient",
      "page": 94
    },
    {
      "caption": "Figure 5: 1). We now describe each step in detail and propose",
      "page": 94
    },
    {
      "caption": "Figure 5: 1 (right) for an example):",
      "page": 95
    },
    {
      "caption": "Figure 5: 3: MULTIVIZ provides an interactive visualization API across multimodal datasets and models.",
      "page": 96
    },
    {
      "caption": "Figure 5: 1, right). Global analysis can also",
      "page": 96
    },
    {
      "caption": "Figure 5: 3. This interactive API enables users to",
      "page": 96
    },
    {
      "caption": "Figure 5: 4: Examples of human-annotated concepts using MULTIVIZ on feature representations. We find",
      "page": 99
    },
    {
      "caption": "Figure 5: 5: Examples of human-annotated error analysis using MULTIVIZ on multimodal models. Using",
      "page": 100
    },
    {
      "caption": "Figure 5: 4 (even without feature highlighting) does constitute a",
      "page": 100
    },
    {
      "caption": "Figure 5: 4 top left, the visualizations serve to",
      "page": 100
    },
    {
      "caption": "Figure 5: 6: A case study on model debugging: we task 3 human users to use MULTIVIZ visualizations",
      "page": 101
    },
    {
      "caption": "Figure 5: 6. Observe that the model is often able to capture",
      "page": 101
    },
    {
      "caption": "Figure 6: 1: We study the relationships between (left) synergy and redundancy as a result of the task Y",
      "page": 108
    },
    {
      "caption": "Figure 6: 2, we find that",
      "page": 111
    },
    {
      "caption": "Figure 6: 3: Datasets with higher estimated multimodal per-",
      "page": 115
    },
    {
      "caption": "Figure 6: 3 (left). Higher",
      "page": 115
    },
    {
      "caption": "Figure 6: 3 (right) shows a visual comparison, where plotting the performance gap between",
      "page": 115
    },
    {
      "caption": "Figure 7: 1), a systematic and unified",
      "page": 117
    },
    {
      "caption": "Figure 7: 1: MULTIBENCH contains a diverse set of 28 datasets spanning 14 modalities and testing for",
      "page": 118
    },
    {
      "caption": "Figure 7: 2: MULTIBENCH provides a standardized machine learning pipeline across data processing, data",
      "page": 120
    },
    {
      "caption": "Figure 7: 2). Table 7.1 shows an",
      "page": 120
    },
    {
      "caption": "Figure 7: 3: MULTIZOO provides a standardized implementation of a suite of multimodal methods",
      "page": 124
    },
    {
      "caption": "Figure 7: 3). To introduce these algorithms, we use the simple setting with",
      "page": 124
    },
    {
      "caption": "Figure 7: 4, we plot the performance",
      "page": 129
    },
    {
      "caption": "Figure 7: 5, we color-code the performance on each dataset depending on which",
      "page": 129
    },
    {
      "caption": "Figure 7: 4: Relative performance of each model across in-domain (red dots) and out-domain datasets (blue",
      "page": 130
    },
    {
      "caption": "Figure 7: 5: Relative performance of each model across different domains. We find that the performance of",
      "page": 130
    },
    {
      "caption": "Figure 7: 6: Tradeoff between performance and complexity. Size of circles shows variance in perfor-",
      "page": 131
    },
    {
      "caption": "Figure 7: 4, many methods show strongest performance on in-domain datasets, and",
      "page": 131
    },
    {
      "caption": "Figure 7: 4, there is high variance in multimodal performance across datasets in",
      "page": 131
    },
    {
      "caption": "Figure 7: 5, we find that performance also varies significantly across research areas.",
      "page": 131
    },
    {
      "caption": "Figure 7: 7: Tradeoff between performance and robustness. Size of circles shows variance in robustness",
      "page": 132
    },
    {
      "caption": "Figure 7: 6(a), we summarize the performance of all methods in terms of performance and",
      "page": 132
    },
    {
      "caption": "Figure 7: 6(a)), they perform much better on well-",
      "page": 132
    },
    {
      "caption": "Figure 7: 6(b)). We hope that the release of FACTORCL will greatly accelerate",
      "page": 132
    },
    {
      "caption": "Figure 7: 7, we plot a similar tradeoff plot between accuracy and (relative & effective) robustness.",
      "page": 132
    },
    {
      "caption": "Figure 7: 7(a)), implying that models starting off with higher accuracy tend to stay above",
      "page": 132
    },
    {
      "caption": "Figure 7: 7(b)) because several well-performing",
      "page": 132
    },
    {
      "caption": "Figure 9: 1). This divide-and-conquer approach",
      "page": 136
    },
    {
      "caption": "Figure 8: 1: An illustrative example for Recurrent Multistage Fusion. At each recursive stage, a subset of",
      "page": 137
    },
    {
      "caption": "Figure 8: 2: Example video clip from movie reviews. [Top]: Illustration of word-level alignment where",
      "page": 138
    },
    {
      "caption": "Figure 8: 2 for a comparison).",
      "page": 138
    },
    {
      "caption": "Figure 9: 2). Given a set of modalities {l(anguage),v(isual),a(coustic)},",
      "page": 139
    },
    {
      "caption": "Figure 8: 3: The RECURRENT MULTISTAGE FUSION NETWORK for multimodal language analysis. The",
      "page": 140
    },
    {
      "caption": "Figure 9: 1 shows an illustrative example for multistage fusion. The HIGHLIGHT module se-",
      "page": 140
    },
    {
      "caption": "Figure 9: 6) for modeling unaligned multimodal language sequences. At the high level, MULT",
      "page": 142
    },
    {
      "caption": "Figure 8: 4: Overall architecture for MULT on modalities (L,V,A). The crossmodal transformers, which",
      "page": 143
    },
    {
      "caption": "Figure 9: 6) and discuss the difference between crossmodal attention and",
      "page": 143
    },
    {
      "caption": "Figure 8: 5: Architectural elements of a crossmodal transformer between two time-series from modality α",
      "page": 144
    },
    {
      "caption": "Figure 8: 5b). Each crossmodal attention",
      "page": 144
    },
    {
      "caption": "Figure 8: 5b) and does not",
      "page": 144
    },
    {
      "caption": "Figure 8: 5b). Formally, a crossmodal transformer computes feed-forwardly for i = 1,...,D layers:",
      "page": 145
    },
    {
      "caption": "Figure 8: 6: An example of visualizing alignment using attention matrix from modality β to α. Multimodal",
      "page": 146
    },
    {
      "caption": "Figure 8: 7: Validation set convergence of MULT when compared to other baselines on the unaligned",
      "page": 150
    },
    {
      "caption": "Figure 8: 8: Visualization of sample crossmodal attention weights from layer 3 of [V →L] crossmodal",
      "page": 150
    },
    {
      "caption": "Figure 8: 7). In addition, while we note that",
      "page": 150
    },
    {
      "caption": "Figure 8: 9: Visualization of learned attention weights across stages 1,2 and 3 of the multistage fusion",
      "page": 152
    },
    {
      "caption": "Figure 8: 9). Using RMFN trained",
      "page": 152
    },
    {
      "caption": "Figure 8: 9 shows three examples where multi-",
      "page": 153
    },
    {
      "caption": "Figure 8: 9(a), the language features are highlighted corre-",
      "page": 153
    },
    {
      "caption": "Figure 8: 9(b), the language modality displays",
      "page": 153
    },
    {
      "caption": "Figure 8: 9(c), the language modality is better interpreted in the",
      "page": 153
    },
    {
      "caption": "Figure 8: 8 shows an example of a",
      "page": 154
    },
    {
      "caption": "Figure 9: 1: Heterogeneity quantification: Efficiently learning",
      "page": 156
    },
    {
      "caption": "Figure 9: 2). In §9.2.1, we formalize modality and interaction heterogeneity to understand",
      "page": 157
    },
    {
      "caption": "Figure 9: 2: HIGHMMT workflow: (1) We estimate modality and interaction heterogeneity via modality",
      "page": 158
    },
    {
      "caption": "Figure 9: 3). Training the",
      "page": 160
    },
    {
      "caption": "Figure 9: 4): (1) homogeneous pre-training of a",
      "page": 160
    },
    {
      "caption": "Figure 9: 3: HIGHMMT architecture: Given arbitrary modalities, (1) the inputs are standardized into a",
      "page": 161
    },
    {
      "caption": "Figure 9: 4: HIGHMMT training involves 2 steps: (1) homogeneous pre-training of a fully shared model",
      "page": 163
    },
    {
      "caption": "Figure 9: 4, these parameter groups",
      "page": 163
    },
    {
      "caption": "Figure 9: 5: Modality and interaction heterogeneity matrices color coded by distances, with green showing",
      "page": 164
    },
    {
      "caption": "Figure 9: 5 and the resulting parameter",
      "page": 164
    },
    {
      "caption": "Figure 9: 6: Overall tradeoff. HIGHMMT",
      "page": 165
    },
    {
      "caption": "Figure 9: 6, we summarize the overall tradeoff be-",
      "page": 165
    }
  ],
  "tables": [
    {
      "caption": "Table 3: 1: Results on estimating PID on synthetic bit-",
      "data": [
        {
          "Task": "PID",
          "OR": "R\nS\nU1 U2",
          "AND": "R\nS\nU1 U2",
          "XOR": "R U1 U2 S"
        },
        {
          "Task": "Exact\nCVX\nBATCH 0.31",
          "OR": "0.31\n0\n0\n0.31\n0\n0\n0\n0",
          "AND": "0.5 0.31\n0\n0\n0.5\n0.5 0.31\n0\n0\n0.5\n0.5 0.31\n0\n0\n0.5",
          "XOR": "0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1"
        }
      ],
      "page": 70
    },
    {
      "caption": "Table 3: 2, both CVX and BATCH agree in relative PID values, correctly",
      "data": [
        {
          "Task": "PID",
          "DR": "R\nS\nU1\nU2",
          "DU1": "S\nR U1\nU2",
          "DU2": "S\nR U1\nU2",
          "DS": "R\nS\nU1\nU2",
          "y = f (z∗\nc )\n1 , z∗\n2 , z∗": "R\nS\nU1\nU2"
        },
        {
          "Task": "CVX\nBATCH 0.29 0.02 0.02\nTruth",
          "DR": "0.16\n0\n0\n0\n0.58\n0\n0\n0",
          "DU1": "0.05 0 0.16\n0\n0.05 0\n0 0.30\n0\n0\n0\n0.56\n0\n0",
          "DU2": "0\n0.17 0.05 0.07\n0\n0\n0.30\n0\n0\n0\n0.54\n0",
          "DS": "0\n0.01 0.14 0.04 0.01\n0\n0\n0\n0.56",
          "y = f (z∗\nc )\n1 , z∗\n2 , z∗": "0\n0.07\n0.11 0.02 0.02 0.15 0.06 0.01 0.01 0.06\n0.13\n0\n0\n0.27"
        }
      ],
      "page": 71
    },
    {
      "caption": "Table 3: 2, both CVX and BATCH agree in relative PID values, correctly",
      "data": [
        {
          "Task": "PID",
          "y = f (z1, z∗\nc )\n2 , z∗": "R\nS\nU1\nU2",
          "y = f (z1, z2, z∗\nc )": "R\nS\nU1\nU2",
          "y = f (z∗\n1 , z∗\n2 , zc)": "R\nS\nU1\nU2",
          "y = f (z∗\nc )\n2 , z∗": "R\nS\nU1\nU2",
          "y = f (z∗\n2 , zc)": "R\nS\nU1 U2"
        },
        {
          "Task": "CVX\nBATCH 0.04 0.09\nTruth",
          "y = f (z1, z∗\nc )\n2 , z∗": "0.04 0.06\n0\n0.07 0.07\n0\n0\n0.25\n0\n0.25",
          "y = f (z1, z2, z∗\nc )": "0\n0\n0.12\n0.06 0.11 0.02 0.02 0.10 0.11 0.02 0.02 0.05 0.07\n0.18\n0\n0\n0.36",
          "y = f (z∗\n1 , z∗\n2 , zc)": "0.1\n0\n0.01 0.07 0.03\n0.22\n0\n0\n0.22",
          "y = f (z∗\nc )\n2 , z∗": "0\n0.04 0.05\n0\n0.06\n0\n0.21\n0\n0.21\n0",
          "y = f (z∗\n2 , zc)": "0.1\n0\n0.04 0.05\n0.19\n0\n0.06\n0\n0.34\n0\n0.17\n0"
        }
      ],
      "page": 71
    },
    {
      "caption": "Table 3: 4: Average interactions (R/U/S) learned by models alongside their average performance on",
      "data": [
        {
          "Model": "R\nAcc(DR)",
          "EF\nADDITIVE AGREE ALIGN ELEM TENSOR MI MULT LOWER REC\nAVERAGE": "0.35\n0.48\n0.44\n0.47\n0.27\n0.55\n0.20\n0.40\n0.47\n0.53 0.41 ± 0.11\n0.71\n0.74\n0.73\n0.74\n0.70\n0.75\n0.67\n0.73\n0.74\n0.75\n0.73 ± 0.02"
        },
        {
          "Model": "U\nAcc(DU )",
          "EF\nADDITIVE AGREE ALIGN ELEM TENSOR MI MULT LOWER REC\nAVERAGE": "0.29\n0.31\n0.19\n0.44\n0.20\n0.52\n0.18\n0.45\n0.55\n0.55 0.37 ± 0.14\n0.66\n0.55\n0.60\n0.73\n0.66\n0.73\n0.66\n0.72\n0.73\n0.73\n0.68 ± 0.06"
        },
        {
          "Model": "S\nAcc(DS)",
          "EF\nADDITIVE AGREE ALIGN ELEM TENSOR MI MULT LOWER REC\nAVERAGE": "0.13\n0.09\n0.08\n0.29\n0.14\n0.33\n0.12\n0.29\n0.31\n0.32 0.21 ± 0.10\n0.56\n0.66\n0.63\n0.72\n0.66\n0.74\n0.65\n0.72\n0.73\n0.74\n0.68 ± 0.06"
        }
      ],
      "page": 73
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nRepresentations": "I(Z; w1)\nI(Z; w2)\nI(Z; ws)",
          "SimCLR\nZ1\nZ2": "4.45\n0.16\n0.17\n3.92\n12.61\n12.06",
          "Cross+self\nZ1\nZ2": "4.39\n0.14\n0.13\n4.26\n11.30\n11.47",
          "SupCon\nZ1\nZ2": "5.17\n0.19\n0.23\n5.17\n7.48\n7.17",
          "FACTORCL\nZU1\nZU2\nZS1\nZS2": "7.83\n0.03\n6.25\n0.04\n7.17\n0.06\n0.05\n5.79\n9.47\n9.89\n10.13\n9.40"
        }
      ],
      "page": 87
    },
    {
      "caption": "Table 4: 4, we also test FAC-",
      "data": [
        {
          "Model": "SimCLR [103]\nCross+Self [647]\nCross+Self+Fact [710]\nOurCL-SSL\nFACTORCL-SSL",
          "(X1; X2) (Xi; X ′\ni) (X1; X2∣Y ) (X ′′": "✓\n✗\n✗\n✗\n✗\n✓\n✓\n✗\n✗\n✗\n✓\n✓\n✗\n✗\n✓\n✓\n✓\n✓\n✓\n✗\n✓\n✓\n✓\n✓\n✓",
          "2 ) Fact MIMIC MOSEI MOSI UR-FUNNY MUSTARD": "66.67% 71.03% 46.21%\n50.09%\n53.48%\n65.20% 71.04% 46.92%\n56.52%\n53.91%\n65.49% 71.07% 52.37%\n59.91%\n53.91%\n65.22% 71.16% 48.98%\n58.79%\n53.98%\n60.50%\n67.34% 74.88% 52.91%\n55.80%"
        },
        {
          "Model": "SupCon [300]\nOurCL-SUP\nFACTORCL-SUP",
          "(X1; X2) (Xi; X ′\ni) (X1; X2∣Y ) (X ′′": "✗\n✗\n✓\n✗\n✗\n✓\n✓\n✓\n✗\n✗\n✓\n✓\n✓\n✗\n✓",
          "2 ) Fact MIMIC MOSEI MOSI UR-FUNNY MUSTARD": "67.37% 72.71% 47.23%\n50.98%\n52.75%\n68.16% 71.15% 65.32%\n58.32%\n65.05%\n76.79% 77.34% 70.69%\n63.52%\n69.86%"
        }
      ],
      "page": 89
    },
    {
      "caption": "Table 5: 2 and find that having access to",
      "data": [
        {
          "Research area\nDataset\nModel": "Metric",
          "QA\nVQA 2.0\nLXMERT": "Correctness\nAgreement",
          "Fusion\nMM-IMDB\nLRTF": "Correctness\nAgreement",
          "Fusion\nCMU-MOSEI\nMULT": "Correctness\nAgreement"
        },
        {
          "Research area\nDataset\nModel": "U\nU + C\nU + C + Rℓ\nU + C + Rℓ + Rg\nMULTIVIZ",
          "QA\nVQA 2.0\nLXMERT": "55.0 ± 0.0\n0.39\n65.0 ± 5.0\n0.50\n61.7 ± 7.6\n0.57\n71.7 ± 15.3\n0.61\n81.7 ± 2.9\n0.86",
          "Fusion\nMM-IMDB\nLRTF": "50.0 ± 13.2\n0.34\n53.7 ± 7.6\n0.51\n56.7 ± 7.6\n0.59\n61.7 ± 7.6\n0.43\n65.0 ± 5.0\n0.60",
          "Fusion\nCMU-MOSEI\nMULT": "71.7 ± 17.6\n0.39\n76.7 ± 10.4\n0.45\n78.3 ± 2.9\n0.42\n100.0 ± 0.0\n1.00\n100.0 ± 0.0\n1.00"
        }
      ],
      "page": 98
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Research area\nDataset\nModel": "Metric",
          "QA\nCLEVR\nCNN-LSTM-SA": "Confidence\nAgree.",
          "QA\nVQA 2.0\nLXMERT": "Confidence\nAgree."
        },
        {
          "Research area\nDataset\nModel": "No viz\nMULTIVIZ",
          "QA\nCLEVR\nCNN-LSTM-SA": "2.72 ± 0.15\n0.05\n4.12 ± 0.45\n0.67",
          "QA\nVQA 2.0\nLXMERT": "2.15 ± 0.70\n0.14\n4.21 ± 0.62\n0.60"
        }
      ],
      "page": 99
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Challenge": "Fusion",
          "Research Area Size": "Affect",
          "Dataset\nModalities\n# Samples\nPrediction task": "S\nMUSTARD [83]\nsarcasm\n{ℓ, v, a} → y\n690\nM\nCMU-MOSI [711]\nsentiment\n{ℓ, v, a} → y\n2, 199\nL\nUR-FUNNY [225]\nhumor\n{ℓ, v, a} → y\n16, 514\nL\nCMU-MOSEI [718]\nsentiment, emotions\n{ℓ, v, a} → y\n22, 777"
        },
        {
          "Challenge": "",
          "Research Area Size": "Healthcare",
          "Dataset\nModalities\n# Samples\nPrediction task": "L\nMIMIC [287]\n{t, ta} → y\n36, 212\nmortality, ICD-9 codes\nL\nMIMIC-CXR [288]\n{ℓ, i} → y\n377, 110\nmortality, ICD-9 codes"
        },
        {
          "Challenge": "",
          "Research Area Size": "Robotics",
          "Dataset\nModalities\n# Samples\nPrediction task": "M\nMUJOCO PUSH [334]\nobject pose\n{i, f, p} → y\n37, 990\nL\nVISION&TOUCH [335]\ncontact, robot pose\n{i, f, p} → y\n147, 000"
        },
        {
          "Challenge": "",
          "Research Area Size": "Finance",
          "Dataset\nModalities\n# Samples\nPrediction task": "M\nSTOCKS-F&B\nstock price, volatility\n{t × 18} → y\n5, 218\nM\nSTOCKS-HEALTH\nstock price, volatility\n{t × 63} → y\n5, 218\nM\nSTOCKS-TECH\nstock price, volatility\n{t × 100} → y\n5, 218"
        },
        {
          "Challenge": "",
          "Research Area Size": "HCI",
          "Dataset\nModalities\n# Samples\nPrediction task": "S\nENRICO [340]\ndesign interface\n{i, s} → y\n1, 460"
        },
        {
          "Challenge": "",
          "Research Area Size": "Multimedia",
          "Dataset\nModalities\n# Samples\nPrediction task": "M\nHATEFUL MEMES [301]\nhate speech\n{ℓ, i} → y\n10, 000\nM\nMM-IMDB [32]\nmovie genre\n{ℓ, i} → y\n25, 959\nM\nAV-MNIST [639]\ndigit\n{i, a} → y\n70, 000\nL\nKINETICS400 [296]\nhuman action\n{v, a, o} → y\n306, 245"
        }
      ],
      "page": 119
    },
    {
      "caption": "Table 7: 2: Standardizing methods and datasets enables quick application of methods from different",
      "data": [
        {
          "Dataset": "Unimodal",
          "MUSTARD ↑ CMU-MOSI ↑ UR-FUNNY ↑ CMU-MOSEI ↑ MIMIC ↑": "74.2 ± 0.5"
        },
        {
          "Dataset": "In-domain\nOut-domain\nImprovement",
          "MUSTARD ↑ CMU-MOSI ↑ UR-FUNNY ↑ CMU-MOSEI ↑ MIMIC ↑": "83.0 ± 0.1\n75.5 ± 0.5\n-"
        }
      ],
      "page": 129
    },
    {
      "caption": "Table 7: 2: Standardizing methods and datasets enables quick application of methods from different",
      "data": [
        {
          "Dataset": "Unimodal",
          "MUJOCO PUSH ↓": "0.334 ± 0.034",
          "V&T EE ↓": "0.202 ± 0.022",
          "STOCKS-F&B ↓ STOCKS-HEALTH ↓ STOCKS-TECH ↓": "0.541 ± 0.010"
        },
        {
          "Dataset": "In-domain\nOut-domain\nImprovement",
          "MUJOCO PUSH ↓": "0.290 ± 0.018\n0.402 ± 0.026\n-",
          "V&T EE ↓": "0.258 ± 0.011\n0.185 ± 0.011\n8.4%",
          "STOCKS-F&B ↓ STOCKS-HEALTH ↓ STOCKS-TECH ↓": "0.541 ± 0.010\n0.526 ± 0.017\n2.8%"
        }
      ],
      "page": 129
    },
    {
      "caption": "Table 7: 2: Standardizing methods and datasets enables quick application of methods from different",
      "data": [
        {
          "Dataset": "Unimodal",
          "ENRICO ↑ MM-IMDB ↑ AV-MNIST ↑ KINETICS-S ↑ KINETICS-L ↑": "45.6 ± 4.5"
        },
        {
          "Dataset": "In-domain\nOut-domain\nImprovement",
          "ENRICO ↑ MM-IMDB ↑ AV-MNIST ↑ KINETICS-S ↑ KINETICS-L ↑": "49.8 ± 1.7\n50.2 ± 0.9\n0.8%"
        }
      ],
      "page": 129
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-": "0.4"
        },
        {
          "-": "20"
        },
        {
          "-": "4.1"
        },
        {
          "-": "0"
        },
        {
          "-": "0"
        },
        {
          "-": "2.5"
        },
        {
          "-": "0"
        },
        {
          "-": "0.2"
        },
        {
          "-": "0"
        }
      ],
      "page": 164
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-": "23"
        },
        {
          "-": "0.8"
        },
        {
          "-": "0.4"
        },
        {
          "-": "0.4"
        },
        {
          "-": "0.4"
        },
        {
          "-": "0.4"
        },
        {
          "-": "0.5"
        }
      ],
      "page": 164
    },
    {
      "caption": "Table 9: 5, removing any of the 3 components in HIGHMMT results in",
      "data": [
        {
          "Model": "HIGHMMT",
          "UR-FUNNY ↑ MOSEI ↑ MIMIC ↑ AV-MNIST ↑": "66.2 ± 0.4\n80.2 ± 0.2 68.2 ± 0.3\n71.1 ± 0.2",
          "Ave ↑": "71.4 ± 0.3"
        },
        {
          "Model": "- w/o embeddings\n- w/o unimodal\n- w/o crossmodal [506]",
          "UR-FUNNY ↑ MOSEI ↑ MIMIC ↑ AV-MNIST ↑": "63.0 ± 1.2\n79.0 ± 0.7\n67.1 ± 1.2\n70.3 ± 0.7\n57.9 ± 0.3\n61.9 ± 2.1\n63.0 ± 0.9\n59.5 ± 1.4\n63.8 ± 1.0\n79.5 ± 0.5 67.9 ± 0.4\n70.4 ± 0.5",
          "Ave ↑": "69.8 ± 0.3\n60.6 ± 0.7\n70.4 ± 0.5"
        },
        {
          "Model": "- share none [367]\n- share unimodal [506]\n- share crossmodal [15]\n- share all [552]\n- random difference\n- feature difference [576]",
          "UR-FUNNY ↑ MOSEI ↑ MIMIC ↑ AV-MNIST ↑": "63.7 ± 0.7\n79.4 ± 0.4\n67.7 ± 0.7\n70.4 ± 0.1\n62.5 ± 1.3\n79.0 ± 1.1\n63.4 ± 1.4\n70.1 ± 0.7\n63.0 ± 1.1\n79.5 ± 0.3\n64.3 ± 0.3\n70.1 ± 0.9\n63.1 ± 0.7\n79.2 ± 0.3\n63.7 ± 1.6\n68.6 ± 0.6\n62.9 ± 0.9\n79.5 ± 0.6\n67.6 ± 0.3\n70.4 ± 0.2\n64.0 ± 1.0\n79.4 ± 0.3 67.9 ± 0.3\n70.1 ± 0.4",
          "Ave ↑": "70.2 ± 0.3\n68.8 ± 0.8\n69.2 ± 0.3\n68.7 ± 0.5\n70.1 ± 0.3\n70.4 ± 0.2"
        },
        {
          "Model": "Training ablations - w/o homogeneous pretraining",
          "UR-FUNNY ↑ MOSEI ↑ MIMIC ↑ AV-MNIST ↑": "61.2 ± 0.1\n78.5 ± 0.1\n64.8 ± 0.1\n71.1 ± 0.2",
          "Ave ↑": "69.9 ± 0.1"
        }
      ],
      "page": 168
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Personality Traits Task Con Pas Voi Cre Viv Exp Res Rel Tho Ner Per Hum Metric Acc7",
      "authors": [
        "Pom Dataset",
        "Speaker"
      ],
      "venue": "Acc7 ↑ Acc7 ↑ Acc7 ↑ Acc7 ↑ Acc7 ↑ Acc5 ↑ Acc5 ↑ Acc5 ↑ Acc5 ↑ Acc7 ↑ Acc"
    },
    {
      "citation_id": "2",
      "title": "results of our proposed approaches and previous baselines on the word-aligned task. With similar model sizes (around 200K parameters), MULT outperforms the other competitive approaches on different metrics on all tasks, with the exception of the \"sad\" class results on IEMOCAP. We also observe that RMFN does not improve results on IEMOCAP neutral emotion and the model outperforming RMFN is a memory-based fusion baseline [714]. We believe that this is because neutral expressions are quite idiosyncratic. Some people may always look angry given their facial configuration (e.g., natural eyebrow raises of actor Jack Nicholson). In these situations, it becomes useful to compare the [1] Mahdi Abavisani and Vishal M Patel. Deep multimodal subspace clustering networks",
      "year": "2018",
      "venue": "We first evaluate RMFN and MULT on the word-aligned sequencesthe \"home turf\" of prior approaches modeling human multimodal language"
    },
    {
      "citation_id": "3",
      "title": "Multimodal sentiment analysis using deep neural networks",
      "authors": [
        "Harika Abburi",
        "Rajendra Prasath",
        "Manish Shrivastava",
        "Suryakanth Gangashetty"
      ],
      "year": "2016",
      "venue": "International Conference on Mining Intelligence and Knowledge Exploration"
    },
    {
      "citation_id": "4",
      "title": "Persistent anti-muslim bias in large language models",
      "authors": [
        "Abubakar Abid",
        "Maheen Farooqi",
        "James Zou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "5",
      "title": "Multi-modal haptic feedback for grip force reduction in robotic surgery",
      "authors": [
        "Ahmad Abiri",
        "Jake Pensa",
        "Anna Tao",
        "Ji Ma",
        "Yen-Yi Juo",
        "J Syed",
        "Askari"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "6",
      "title": "Multimodal biomedical ai",
      "authors": [
        "Guido Julián N Acosta",
        "Pranav Falcone",
        "Eric Rajpurkar",
        "Topol"
      ],
      "year": "2022",
      "venue": "Nature Medicine"
    },
    {
      "citation_id": "7",
      "title": "Sanity checks for saliency maps",
      "authors": [
        "Julius Adebayo",
        "Justin Gilmer",
        "Michael Muelly",
        "Ian Goodfellow",
        "Moritz Hardt",
        "Been Kim"
      ],
      "year": "2018",
      "venue": "Sanity checks for saliency maps"
    },
    {
      "citation_id": "8",
      "title": "Vl-interpret: An interactive visualization tool for interpreting vision-language transformers",
      "authors": [
        "Estelle Aflalo",
        "Meng Du",
        "Shao-Yen",
        "Yongfei Tseng",
        "Chenfei Liu",
        "Nan Wu",
        "Vasudev Duan",
        "Lal"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing",
      "authors": [
        "Vedika Agarwal",
        "Rakshith Shetty",
        "Mario Fritz"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Caillon"
      ],
      "year": "2023",
      "venue": "Generating music from text",
      "arxiv": "arXiv:2301.11325"
    },
    {
      "citation_id": "11",
      "title": "Analyzing the behavior of visual question answering models",
      "authors": [
        "Aishwarya Agrawal",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2016",
      "venue": "EMNLP"
    },
    {
      "citation_id": "12",
      "title": "VQA: Visual question answering",
      "authors": [
        "Aishwarya Agrawal",
        "Jiasen Lu",
        "Stanislaw Antol",
        "Margaret Mitchell",
        "C Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2017",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Language2pose: Natural language grounded pose forecasting",
      "authors": [
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "DV"
    },
    {
      "citation_id": "14",
      "title": "Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach",
      "authors": [
        "Chaitanya Ahuja",
        "Dong Lee",
        "Yukiko Nakano",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "15",
      "title": "Domainadversarial neural networks",
      "authors": [
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "François Laviolette",
        "Mario Marchand"
      ],
      "year": "2014",
      "venue": "Domainadversarial neural networks",
      "arxiv": "arXiv:1412.4446"
    },
    {
      "citation_id": "16",
      "title": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text",
      "authors": [
        "Hassan Akbari",
        "Liangzhe Yuan",
        "Rui Qian"
      ],
      "year": "2021",
      "venue": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text",
      "arxiv": "arXiv:2104.11178"
    },
    {
      "citation_id": "17",
      "title": "A probabilistic framework to incorporate mixed-data type features: Matrix factorization with multimodal side information",
      "authors": [
        "Mehmet Aktukmak",
        "Yasin Yilmaz",
        "Ismail Uysal"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Self-supervised multimodal versatile networks",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Adria Recasens",
        "Rosalia Schneider",
        "Relja Arandjelović",
        "Jason Ramapuram",
        "Jeffrey De Fauw",
        "Lucas Smaira",
        "Sander Dieleman",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "19",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson"
      ],
      "year": "2022",
      "venue": "Flamingo: a visual language model for few-shot learning",
      "arxiv": "arXiv:2204.14198"
    },
    {
      "citation_id": "20",
      "title": "Publicly available clinical BERT embeddings",
      "authors": [
        "Emily Alsentzer",
        "John Murphy",
        "William Boag",
        "Wei-Hung Weng",
        "Di Jindi",
        "Tristan Naumann",
        "Matthew Mc-Dermott"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
      "doi": "10.18653/v1/W19-1909"
    },
    {
      "citation_id": "21",
      "title": "Multimodal coordination of sound and movement in music and speech",
      "authors": [
        "Camila Alviar",
        "Rick Dale",
        "Akeiylah Dewitt",
        "Christopher Kello"
      ],
      "year": "2020",
      "venue": "Discourse Processes"
    },
    {
      "citation_id": "22",
      "title": "Overview of artificial intelligence in medicine",
      "authors": [
        "Paras Malik Amisha",
        "Monika Pathania",
        "Vyas Kumar"
      ],
      "year": "2019",
      "venue": "Journal of family medicine and primary care"
    },
    {
      "citation_id": "23",
      "title": "Neuro-symbolic visual reasoning: Disentangling visual from reasoning",
      "authors": [
        "Saeed Amizadeh",
        "Hamid Palangi",
        "Alex Polozov",
        "Yichen Huang",
        "Kazuhito Koishida"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "24",
      "title": "Tutorial on amortized optimization for learning to optimize over continuous domains",
      "authors": [
        "Brandon Amos"
      ],
      "year": "2022",
      "venue": "Tutorial on amortized optimization for learning to optimize over continuous domains",
      "arxiv": "arXiv:2202.00665"
    },
    {
      "citation_id": "25",
      "title": "Blindfold baselines for embodied qa",
      "authors": [
        "Ankesh Anand",
        "Eugene Belilovsky",
        "Kyle Kastner",
        "Hugo Larochelle",
        "Aaron Courville"
      ],
      "year": "2018",
      "venue": "Blindfold baselines for embodied qa",
      "arxiv": "arXiv:1811.05013"
    },
    {
      "citation_id": "26",
      "title": "Spice: Semantic propositional image caption evaluation",
      "authors": [
        "Peter Anderson",
        "Basura Fernando",
        "Mark Johnson",
        "Stephen Gould"
      ],
      "year": "2016",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "27",
      "title": "Neural module networks",
      "authors": [
        "Jacob Andreas",
        "Marcus Rohrbach",
        "Trevor Darrell",
        "Dan Klein"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "28",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "ICML"
    },
    {
      "citation_id": "29",
      "title": "Audio-to-text alignment for speech recognition with very limited resources",
      "authors": [
        "Xavier Anguera",
        "Jordi Luque",
        "Ciro Gracia"
      ],
      "year": "2014",
      "venue": "Fifteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "30",
      "title": "Vqa: Visual question answering",
      "authors": [
        "Stanislaw Antol",
        "Aishwarya Agrawal",
        "Jiasen Lu",
        "Margaret Mitchell",
        "Dhruv Batra",
        "C Lawrence Zitnick",
        "Devi Parikh"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "31",
      "title": "MOSEK Optimizer API for Python 10",
      "authors": [
        "Mosek Aps"
      ],
      "year": "2022",
      "venue": "MOSEK Optimizer API for Python 10"
    },
    {
      "citation_id": "32",
      "title": "Look, listen and learn",
      "authors": [
        "Relja Arandjelovic",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "33",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gómez",
        "Fabio González"
      ],
      "year": "2017",
      "venue": "Gated multimodal units for information fusion",
      "arxiv": "arXiv:1702.01992"
    },
    {
      "citation_id": "34",
      "title": "Unitary evolution recurrent neural networks",
      "authors": [
        "Martín Arjovsky",
        "Amar Shah",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Unitary evolution recurrent neural networks"
    },
    {
      "citation_id": "35",
      "title": "Multimodal fusion for multimedia analysis: a survey",
      "authors": [
        "M Pradeep K Atrey",
        "Abdulmotaleb Anwar Hossain",
        "Mohan Saddik",
        "Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia systems"
    },
    {
      "citation_id": "36",
      "title": "Dbpedia: A nucleus for a web of open data",
      "authors": [
        "Sören Auer",
        "Christian Bizer",
        "Georgi Kobilarov",
        "Jens Lehmann",
        "Richard Cyganiak",
        "Zachary Ives"
      ],
      "year": "2007",
      "venue": "The semantic web"
    },
    {
      "citation_id": "37",
      "title": "Comparison of redundancy and relevance measures for feature selection in tissue classification of ct images",
      "authors": [
        "Benjamin Auffarth",
        "Maite López",
        "Jesús Cerquides"
      ],
      "year": "2010",
      "venue": "Industrial conference on data mining"
    },
    {
      "citation_id": "38",
      "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
      "authors": [
        "Anas Awadalla",
        "Irena Gao",
        "Josh Gardner",
        "Jack Hessel",
        "Yusuf Hanafy"
      ],
      "year": "2023",
      "venue": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
      "arxiv": "arXiv:2308.01390"
    },
    {
      "citation_id": "39",
      "title": "",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "40",
      "title": "Learning representations by maximizing mutual information across views",
      "authors": [
        "Philip Bachman",
        "Devon Hjelm",
        "William Buchwalter"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "How to explain individual classification decisions",
      "authors": [
        "David Baehrens",
        "Timon Schroeter",
        "Stefan Harmeling",
        "Motoaki Kawanabe",
        "Katja Hansen",
        "Klaus-Robert Müller"
      ],
      "year": "2010",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "42",
      "title": "Adaptive input representations for neural language modeling",
      "authors": [
        "Alexei Baevski",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "43",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Data and algorithmic bias in the web",
      "authors": [
        "Ricardo Baeza-Yates"
      ],
      "year": "2016",
      "venue": "Proceedings of the 8th ACM Conference on Web Science"
    },
    {
      "citation_id": "45",
      "title": "Co-training and expansion: Towards bridging theory and practice",
      "authors": [
        "Maria-Florina Balcan",
        "Avrim Blum",
        "Ke Yang"
      ],
      "year": "2004",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "46",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Multimodal machine learning: A survey and taxonomy",
      "arxiv": "arXiv:1705.09406"
    },
    {
      "citation_id": "47",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "48",
      "title": "On the benefits of early fusion in multimodal representation learning",
      "authors": [
        "George Barnum",
        "J Sabera",
        "Yisong Talukder",
        "Yue"
      ],
      "year": "2020",
      "venue": "NeurIPS 2020 Workshop SVRHM"
    },
    {
      "citation_id": "49",
      "title": "The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations",
      "authors": [
        "M Reuben",
        "David Baron",
        "Kenny"
      ],
      "year": "1986",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "50",
      "title": "Image-music-text",
      "authors": [
        "Roland Barthes"
      ],
      "year": "1977",
      "venue": "Image-music-text"
    },
    {
      "citation_id": "51",
      "title": "Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics",
      "authors": [
        "E Leonard",
        "Ted Baum",
        "Petrie"
      ],
      "year": "1966",
      "venue": "Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics"
    },
    {
      "citation_id": "52",
      "title": "The co-information lattice",
      "authors": [
        "J Anthony",
        "Bell"
      ],
      "year": "2003",
      "venue": "Proceedings of the fifth international workshop on independent component analysis and blind signal separation: ICA"
    },
    {
      "citation_id": "53",
      "title": "The arcade learning environment: An evaluation platform for general agents",
      "authors": [
        "Yavar Marc G Bellemare",
        "Joel Naddaf",
        "Michael Veness",
        "Bowling"
      ],
      "year": "2013",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "54",
      "title": "Social robots for education: A review",
      "authors": [
        "Tony Belpaeme",
        "James Kennedy",
        "Aditi Ramachandran",
        "Brian Scassellati",
        "Fumihide Tanaka"
      ],
      "year": "2018",
      "venue": "Science robotics"
    },
    {
      "citation_id": "55",
      "title": "Analysis of representations for domain adaptation",
      "authors": [
        "Shai Ben-David",
        "John Blitzer",
        "Koby Crammer",
        "Fernando Pereira"
      ],
      "year": "2006",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "56",
      "title": "Mutan: Multimodal tucker fusion for visual question answering",
      "authors": [
        "Hedi Ben-Younes",
        "Rémi Cadene",
        "Matthieu Cord",
        "Nicolas Thome"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "57",
      "title": "On the dangers of stochastic parrots: Can language models be too big?",
      "authors": [
        "Emily Bender",
        "Timnit Gebru",
        "Angelina Mcmillan-Major",
        "Shmargaret Shmitchell"
      ],
      "year": "2021",
      "venue": "FaaCT"
    },
    {
      "citation_id": "58",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "TPAMI"
    },
    {
      "citation_id": "59",
      "title": "Real-time speech emotion and sentiment recognition for interactive dialogue systems",
      "authors": [
        "Dario Bertero",
        "Farhad Bin Siddique",
        "Chien-Sheng Wu",
        "Yan Wan",
        "Ricky Chan",
        "Pascale Fung"
      ],
      "venue": "Real-time speech emotion and sentiment recognition for interactive dialogue systems"
    },
    {
      "citation_id": "60",
      "title": "Quantifying unique informa-tion",
      "authors": [
        "Nils Bertschinger",
        "Johannes Rauh",
        "Eckehard Olbrich",
        "Jürgen Jost",
        "Nihat Ay"
      ],
      "year": "2014",
      "venue": "Entropy"
    },
    {
      "citation_id": "61",
      "title": "Application of haptic feedback to robotic surgery",
      "authors": [
        "Brian Bethea",
        "Allison Okamura",
        "Masaya Kitagawa",
        "Torin Fitton",
        "Stephen Cattaneo",
        "William Vincent L Gott",
        "David Baumgartner",
        "Yuh"
      ],
      "year": "2004",
      "venue": "Journal of Laparoendoscopic & Advanced Surgical Techniques"
    },
    {
      "citation_id": "62",
      "title": "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
      "authors": [
        "Abeba Birhane",
        "Uday Vinay",
        "Emmanuel Prabhu",
        "Kahembwe"
      ],
      "year": "2021",
      "venue": "Multimodal datasets: misogyny, pornography, and malignant stereotypes",
      "arxiv": "arXiv:2110.01963"
    },
    {
      "citation_id": "63",
      "title": "Experience grounds language",
      "authors": [
        "Yonatan Bisk",
        "Ari Holtzman",
        "Jesse Thomason",
        "Jacob Andreas",
        "Yoshua Bengio",
        "Joyce Chai",
        "Mirella Lapata",
        "Angeliki Lazaridou",
        "Jonathan May",
        "Aleksandr Nisnevich"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "64",
      "title": "Piqa: Reasoning about physical commonsense in natural language",
      "authors": [
        "Yonatan Bisk",
        "Rowan Zellers",
        "Jianfeng Gao",
        "Yejin Choi"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "65",
      "title": "Language (technology) is power: A critical survey of \"bias\" in nlp",
      "authors": [
        "Lin Su",
        "Solon Blodgett",
        "Hal Barocas",
        "Iii Daumé",
        "Hanna Wallach"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "66",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "Avrim Blum",
        "Tom Mitchell"
      ],
      "year": "1998",
      "venue": "COLT"
    },
    {
      "citation_id": "67",
      "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
      "authors": [
        "Kurt Bollacker",
        "Colin Evans",
        "Praveen Paritosh",
        "Tim Sturge",
        "Jamie Taylor"
      ],
      "year": "2008",
      "venue": "ACM SIGMOD"
    },
    {
      "citation_id": "68",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
      ],
      "year": "2016",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "69",
      "title": "Towards federated learning at scale: System design",
      "authors": [
        "Keith Bonawitz",
        "Hubert Eichner",
        "Wolfgang Grieskamp",
        "Dzmitry Huba",
        "Alex Ingerman",
        "Vladimir Ivanov",
        "Chloé Kiddon",
        "Jakub Konecný",
        "Stefano Mazzocchi",
        "H Mcmahan",
        "Timon Van Overveldt",
        "David Petrou",
        "Daniel Ramage",
        "Jason Roselander"
      ],
      "year": "2019",
      "venue": "Towards federated learning at scale: System design"
    },
    {
      "citation_id": "70",
      "title": "Home: a household multimodal environment",
      "authors": [
        "Simon Brodeur",
        "Ethan Perez",
        "Ankesh Anand",
        "Florian Golemo",
        "Luca Celotti"
      ],
      "year": "2017",
      "venue": "NIPS 2017's Visually-Grounded Interaction and Language Workshop"
    },
    {
      "citation_id": "71",
      "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "authors": [
        "Anthony Brohan",
        "Noah Brown",
        "Justice Carbajal",
        "Yevgen Chebotar",
        "Xi Chen",
        "Krzysztof Choromanski",
        "Tianli Ding",
        "Danny Driess",
        "Avinava Dubey",
        "Chelsea Finn"
      ],
      "year": "2023",
      "venue": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "arxiv": "arXiv:2307.15818"
    },
    {
      "citation_id": "72",
      "title": "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges",
      "authors": [
        "Joan Michael M Bronstein",
        "Taco Bruna",
        "Petar Cohen",
        "Veličković"
      ],
      "year": "2021",
      "venue": "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges",
      "arxiv": "arXiv:2104.13478"
    },
    {
      "citation_id": "73",
      "title": "Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts",
      "authors": [
        "Emanuele Bugliarello",
        "Ryan Cotterell",
        "Naoaki Okazaki",
        "Desmond Elliott"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "74",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "Joy Buolamwini",
        "Timnit Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "75",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "76",
      "title": "Reducing unimodal biases for visual question answering",
      "authors": [
        "Remi Cadene",
        "Corentin Dancette",
        "Matthieu Cord",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "77",
      "title": "A survey on multimodal data-driven smart healthcare systems: approaches and applications",
      "authors": [
        "Qiong Cai",
        "Hao Wang",
        "Zhenmin Li",
        "Xiao Liu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "78",
      "title": "Online matrix factorization for multimodal image retrieval",
      "authors": [
        "C Juan",
        "Fabio Caicedo",
        "González"
      ],
      "year": "2012",
      "venue": "Iberoamerican Congress on Pattern Recognition"
    },
    {
      "citation_id": "79",
      "title": "Affective computing and sentiment analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2016.31"
    },
    {
      "citation_id": "80",
      "title": "Behind the scene: Revealing the secrets of pre-trained vision-and-language models",
      "authors": [
        "Jize Cao",
        "Zhe Gan",
        "Yu Cheng",
        "Licheng Yu",
        "Yen-Chun Chen",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "81",
      "title": "Transitive hashing network for heterogeneous multimedia retrieval",
      "authors": [
        "Zhangjie Cao",
        "Mingsheng Long",
        "Jianmin Wang",
        "Qiang Yang"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "82",
      "title": "Extracting training data from large language models",
      "authors": [
        "Nicholas Carlini",
        "Florian Tramer",
        "Eric Wallace",
        "Matthew Jagielski",
        "Ariel Herbert-Voss"
      ],
      "year": "2021",
      "venue": "USENIX Security"
    },
    {
      "citation_id": "83",
      "title": "Unsupervised learning of visual features by contrasting cluster assignments",
      "authors": [
        "Mathilde Caron",
        "Ishan Misra",
        "Julien Mairal",
        "Priya Goyal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "84",
      "title": "Towards multimodal sarcasm detection (an _obviously_ perfect paper)",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Verónica Pérez-Rosas",
        "Roger Zimmermann",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "85",
      "title": "Suicide Facts at a Glance",
      "authors": [
        "Cdc"
      ],
      "year": "2015",
      "venue": "Suicide Facts at a Glance"
    },
    {
      "citation_id": "86",
      "title": "Do explanations make vqa models more predictable to a human?",
      "authors": [
        "Arjun Chandrasekaran",
        "Viraj Prabhu",
        "Deshraj Yadav",
        "Prithvijit Chattopadhyay",
        "Devi Parikh"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "87",
      "title": "Grounding 'grounding'in nlp",
      "authors": [
        "Raghavi Khyathi",
        "Yonatan Chandu",
        "Alan Bisk",
        "Black"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "88",
      "title": "A review on data fusion in multimodal learning analytics and educational data mining",
      "authors": [
        "Wilson Chango",
        "Juan Lara",
        "Rebeca Cerezo",
        "Cristobal Romero"
      ],
      "year": "2022",
      "venue": "A review on data fusion in multimodal learning analytics and educational data mining"
    },
    {
      "citation_id": "89",
      "title": "Gated-attention architectures for task-oriented language grounding",
      "authors": [
        "Devendra Singh Chaplot",
        "Kanthashree Sathyendra",
        "Rama Kumar Pasumarthi",
        "Dheeraj Rajagopal",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "90",
      "title": "Human4d: A human-centric multimodal dataset for motions and immersive media",
      "authors": [
        "Anargyros Chatzitofis",
        "Leonidas Saroglou",
        "Prodromos Boutis",
        "Petros Drakoulis",
        "Nikolaos Zioulis"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "91",
      "title": "Multimodal federated learning: A survey",
      "authors": [
        "Liwei Che",
        "Jiaqi Wang",
        "Yao Zhou",
        "Fenglong Ma"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "92",
      "title": "Group redundancy measures reveal redundancy reduction in the auditory pathway",
      "authors": [
        "Gal Chechik",
        "M Amir Globerson",
        "E Anderson",
        "Israel Young",
        "Naftali Nelken",
        "Tishby"
      ],
      "year": "2001",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "93",
      "title": "Multimodal clustering networks for self-supervised learning from unlabeled videos",
      "authors": [
        "Brian Chen",
        "Andrew Rouditchenko",
        "Kevin Duarte",
        "Hilde Kuehne",
        "Samuel Thomas",
        "Angie Boggust"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "94",
      "title": "Soundspaces: Audio-visual navigation in 3d environments",
      "authors": [
        "Changan Chen",
        "Unnat Jain",
        "Carl Schissler",
        "Sebastia Vicenc",
        "Amengual Gari",
        "Ziad Al-Halah",
        "Krishna Vamsi",
        "Philip Ithapu",
        "Kristen Robinson",
        "Grauman"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "95",
      "title": "Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning",
      "authors": [
        "Jiaqi Chen",
        "Jianheng Tang",
        "Jinghui Qin",
        "Xiaodan Liang",
        "Lingbo Liu",
        "Eric Xing",
        "Liang Lin"
      ],
      "year": "2021",
      "venue": "ACL-IJCNLP Findings"
    },
    {
      "citation_id": "96",
      "title": "Abstractive text-image summarization using multi-modal attentional hierarchical rnn",
      "authors": [
        "Jingqiang Chen",
        "Hai Zhuge"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "97",
      "title": "Extractive text-image summarization using multi-modal rnn",
      "authors": [
        "Jingqiang Chen",
        "Hai Zhuge"
      ],
      "year": "2018",
      "venue": "2018 14th International Conference on Semantics, Knowledge and Grids (SKG)"
    },
    {
      "citation_id": "98",
      "title": "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning",
      "authors": [
        "Jun Chen",
        "Han Guo",
        "Kai Yi",
        "Boyang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2021",
      "venue": "Visualgpt: Data-efficient adaptation of pretrained language models for image captioning",
      "arxiv": "arXiv:2102.10407"
    },
    {
      "citation_id": "99",
      "title": "What comprises a good talking-head video generation?",
      "authors": [
        "Lele Chen",
        "Guofeng Cui",
        "Ziyi Kou",
        "Haitian Zheng",
        "Chenliang Xu"
      ],
      "year": "2020",
      "venue": "A survey and benchmark",
      "arxiv": "arXiv:2005.03201"
    },
    {
      "citation_id": "100",
      "title": "Graph optimal transport for cross-domain alignment",
      "authors": [
        "Liqun Chen",
        "Zhe Gan",
        "Yu Cheng",
        "Linjie Li",
        "Lawrence Carin",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "101",
      "title": "The best of both worlds: Combining recent advances in neural machine translation",
      "authors": [
        "Mia Xu",
        "Orhan Firat",
        "Ankur Bapna",
        "Melvin Johnson",
        "Wolfgang Macherey",
        "George Foster",
        "Llion Jones",
        "Mike Schuster",
        "Noam Shazeer",
        "Niki Parmar",
        "Ashish Vaswani",
        "Jakob Uszkoreit",
        "Lukasz Kaiser",
        "Zhifeng Chen",
        "Yonghui Wu",
        "Macduff Hughes"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "102",
      "title": "Multimodal sentiment analysis with word-level fusion and reinforcement learning",
      "authors": [
        "Minghai Chen",
        "Sen Wang",
        "Paul Liang",
        "Tadas Baltrušaitis",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "103",
      "title": "Pan-cancer integrative histology-genomic analysis via multimodal deep learning",
      "authors": [
        "Ming Richard J Chen",
        "Lu",
        "Tiffany Drew Fk Williamson",
        "Jana Chen",
        "Zahra Lipkova",
        "Muhammad Noor",
        "Maha Shaban",
        "Mane Shady",
        "Bumjin Williams",
        "Joo"
      ],
      "year": "2022",
      "venue": "Cancer Cell"
    },
    {
      "citation_id": "104",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "105",
      "title": "Interpretable machine learning: Moving from mythos to diagnostics",
      "authors": [
        "Valerie Chen",
        "Jeffrey Li",
        "Joon Sik Kim",
        "Gregory Plumb",
        "Ameet Talwalkar"
      ],
      "year": "2022",
      "venue": "Queue"
    },
    {
      "citation_id": "106",
      "title": "Adversarial deep averaging networks for cross-lingual sentiment classification",
      "authors": [
        "Xilun Chen",
        "Ben Athiwaratkun",
        "Yu Sun",
        "Kilian Weinberger",
        "Claire Cardie"
      ],
      "year": "2016",
      "venue": "Adversarial deep averaging networks for cross-lingual sentiment classification"
    },
    {
      "citation_id": "107",
      "title": "Exploring simple siamese representation learning",
      "authors": [
        "Xinlei Chen",
        "Kaiming He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "108",
      "title": "An empirical study of training self-supervised vision transformers",
      "authors": [
        "Xinlei Chen",
        "Saining Xie",
        "Kaiming He"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "109",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "110",
      "title": "Remote sensing image scene classification: Benchmark and state of the art",
      "authors": [
        "Gong Cheng",
        "Junwei Han",
        "Xiaoqiang Lu"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "111",
      "title": "Club: A contrastive log-ratio upper bound of mutual information",
      "authors": [
        "Pengyu Cheng",
        "Weituo Hao",
        "Shuyang Dai",
        "Jiachang Liu",
        "Zhe Gan",
        "Lawrence Carin"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "112",
      "title": "Semi-supervised multimodal deep learning for rgb-d object recognition",
      "authors": [
        "Yanhua Cheng",
        "Xin Zhao",
        "Rui Cai",
        "Zhiwei Li",
        "Kaiqi Huang",
        "Yong Rui"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "113",
      "title": "Conditional supervised contrastive learning for fair text classification",
      "authors": [
        "Jianfeng Chi",
        "William Shand",
        "Yaodong Yu",
        "Kai-Wei Chang",
        "Han Zhao",
        "Yuan Tian"
      ],
      "year": "2022",
      "venue": "Conditional supervised contrastive learning for fair text classification",
      "arxiv": "arXiv:2205.11485"
    },
    {
      "citation_id": "114",
      "title": "Unifying vision-and-language tasks via text generation",
      "authors": [
        "Jaemin Cho",
        "Jie Lei",
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "115",
      "title": "Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers",
      "authors": [
        "Jaemin Cho",
        "Abhay Zala",
        "Mohit Bansal"
      ],
      "year": "2022",
      "venue": "Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers",
      "arxiv": "arXiv:2202.04053"
    },
    {
      "citation_id": "116",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merrienboer",
        "Çaglar Gülçehre",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using RNN encoder-decoder for statistical machine translation"
    },
    {
      "citation_id": "117",
      "title": "Multi-view learning in the presence of view disagreement",
      "authors": [
        "Mario Christoudias",
        "Raquel Urtasun",
        "Trevor Darrell"
      ],
      "year": "2008",
      "venue": "Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence"
    },
    {
      "citation_id": "118",
      "title": "Interpretation and trust: Designing model-driven visualizations for text analysis",
      "authors": [
        "Jason Chuang",
        "Daniel Ramage",
        "Christopher Manning",
        "Jeffrey Heer"
      ],
      "year": "2012",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "119",
      "title": "Supermodularity and subadditivity properties of the entropy on the majorization lattice",
      "authors": [
        "Ferdinando Cicalese",
        "Ugo Vaccaro"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "120",
      "title": "How to find a joint probability distribution of minimum entropy (almost) given the marginals",
      "authors": [
        "Ferdinando Cicalese",
        "Luisa Gargano",
        "Ugo Vaccaro"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Information Theory (ISIT)"
    },
    {
      "citation_id": "121",
      "title": "Using syntax to ground referring expressions in natural images",
      "authors": [
        "Volkan Cirik",
        "Taylor Berg-Kirkpatrick",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "122",
      "title": "Visual referring expression recognition: What do systems actually learn?",
      "authors": [
        "Volkan Cirik",
        "Louis-Philippe Morency",
        "Taylor Berg-Kirkpatrick"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "123",
      "title": "A referring expression recognition dataset in 360 images",
      "authors": [
        "Volkan Cirik",
        "Taylor Berg-Kirkpatrick",
        "L-P Morency"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "124",
      "title": "Minimum-entropy coupling approximation guarantees beyond the majorization barrier",
      "authors": [
        "Spencer Compton",
        "Dmitriy Katz",
        "Benjamin Qi",
        "Kristjan Greenewald",
        "Murat Kocaoglu"
      ],
      "year": "2023",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "125",
      "title": "Simple and controllable music generation",
      "authors": [
        "Jade Copet",
        "Felix Kreuk",
        "Itai Gat",
        "Tal Remez",
        "David Kant",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Alexandre Défossez"
      ],
      "year": "2023",
      "venue": "Simple and controllable music generation",
      "arxiv": "arXiv:2306.05284"
    },
    {
      "citation_id": "126",
      "title": "Information theory and statistics",
      "authors": [
        "M Thomas",
        "Joy Cover",
        "Thomas"
      ],
      "year": "1991",
      "venue": "Elements of information theory"
    },
    {
      "citation_id": "127",
      "title": "Unsupervised natural language inference via decoupled multimodal contrastive learning",
      "authors": [
        "Wanyun Cui",
        "Guangyu Zheng",
        "Wei Wang"
      ],
      "year": "2020",
      "venue": "Unsupervised natural language inference via decoupled multimodal contrastive learning"
    },
    {
      "citation_id": "128",
      "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
      "authors": [
        "Marco Cuturi"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "129",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Meng",
        "Huat Tiong",
        "Junqi Zhao",
        "Weisheng Wang"
      ],
      "year": "2023",
      "venue": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "arxiv": "arXiv:2305.06500"
    },
    {
      "citation_id": "130",
      "title": "Transformer-xl: Language modeling with longer-term dependency",
      "authors": [
        "Zihang Dai",
        "Zhilin Yang",
        "Yiming Yang",
        "William Cohen",
        "Jaime Carbonell",
        "Ruslan Quoc V Le",
        "Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Transformer-xl: Language modeling with longer-term dependency"
    },
    {
      "citation_id": "131",
      "title": "Estimation of the information by an adaptive partitioning of the observation space",
      "authors": [
        "A Georges",
        "Igor Darbellay",
        "Vajda"
      ],
      "year": "1999",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "132",
      "title": "Embodied question answering",
      "authors": [
        "Abhishek Das",
        "Samyak Datta",
        "Georgia Gkioxari",
        "Stefan Lee",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "133",
      "title": "Frustratingly easy domain adaptation",
      "authors": [
        "Hal Daumé"
      ],
      "year": "2007",
      "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics"
    },
    {
      "citation_id": "134",
      "title": "Language modeling with gated convolutional networks",
      "authors": [
        "Angela Yann N Dauphin",
        "Michael Fan",
        "David Auli",
        "Grangier"
      ],
      "year": "2017",
      "venue": "ICML"
    },
    {
      "citation_id": "135",
      "title": "Multimodal wearable sensing for fine-grained activity recognition in healthcare",
      "authors": [
        "Debraj De",
        "Pratool Bharti",
        "K Sajal",
        "Sriram Das",
        "Chellappan"
      ],
      "year": "2015",
      "venue": "IEEE Internet Computing"
    },
    {
      "citation_id": "136",
      "title": "See, hear, explore: Curiosity via audio-visual association",
      "authors": [
        "Victoria Dean",
        "Shubham Tulsiani",
        "Abhinav Gupta"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "137",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "138",
      "title": "Rico: A mobile app dataset for building data-driven design applications",
      "authors": [
        "Biplab Deka",
        "Zifeng Huang",
        "Chad Franzen",
        "Joshua Hibschman",
        "Daniel Afergan",
        "Yang Li",
        "Jeffrey Nichols",
        "Ranjitha Kumar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "139",
      "title": "Multimodal coordination: exploring relevant features and measures",
      "authors": [
        "Emilie Delaherche",
        "Mohamed Chetouani"
      ],
      "year": "2010",
      "venue": "Proceedings of the 2nd international workshop on Social signal processing"
    },
    {
      "citation_id": "140",
      "title": "Actionsense: A multimodal dataset and recording framework for human activities using wearable sensors in a kitchen environment",
      "authors": [
        "Joseph Delpreto",
        "Chao Liu",
        "Yiyue Luo"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "141",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "142",
      "title": "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
      "authors": [
        "Mingkai Deng",
        "Bowen Tan",
        "Zhengzhong Liu",
        "Eric Xing",
        "Zhiting Hu"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "143",
      "title": "Stochastic video generation with a learned prior",
      "authors": [
        "Emily Denton",
        "Rob Fergus"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "144",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Lori Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "145",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    },
    {
      "citation_id": "146",
      "title": "Emotion recognition using phog and lpq features",
      "authors": [
        "Abhinav Dhall",
        "Akshay Asthana",
        "Roland Goecke",
        "Tom Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "147",
      "title": "Cooperative learning for multi-view analysis",
      "authors": [
        "Daisy Yi",
        "Robert Tibshirani"
      ],
      "year": "2021",
      "venue": "Cooperative learning for multi-view analysis",
      "arxiv": "arXiv:2112.12337"
    },
    {
      "citation_id": "148",
      "title": "Cooperative learning for multiview analysis",
      "authors": [
        "Daisy Yi Ding",
        "Shuangning Li",
        "Balasubramanian Narasimhan",
        "Robert Tibshirani"
      ],
      "year": "2022",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "149",
      "title": "A multimodal fusion method for sarcasm detection based on late fusion",
      "authors": [
        "Ning Ding",
        "Long Sheng-Wei Tian",
        "Yu"
      ],
      "year": "2022",
      "venue": "A multimodal fusion method for sarcasm detection based on late fusion"
    },
    {
      "citation_id": "150",
      "title": "Multimodal safety-critical scenarios generation for decision-making algorithms evaluation",
      "authors": [
        "Wenhao Ding",
        "Baiming Chen",
        "Bo Li",
        "Kim Ji Eun",
        "Ding Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "151",
      "title": "Latent low-rank transfer subspace learning for missing modality recognition",
      "authors": [
        "Zhengming Ding",
        "Ming Shao",
        "Yun Fu"
      ],
      "year": "2014",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "152",
      "title": "Human-computer interaction. Harlow ua",
      "authors": [
        "Alan Dix",
        "Janet Finlay",
        "Gregory Abowd",
        "Russell Beale"
      ],
      "year": "2000",
      "venue": "Human-computer interaction. Harlow ua"
    },
    {
      "citation_id": "153",
      "title": "Ecos: An socp solver for embedded systems",
      "authors": [
        "Alexander Domahidi",
        "Eric Chu",
        "Stephen Boyd"
      ],
      "year": "2013",
      "venue": "2013 European Control Conference (ECC)"
    },
    {
      "citation_id": "154",
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "authors": [
        "Cícero Nogueira",
        "Dos Santos",
        "Maira Gatti"
      ],
      "year": "2014",
      "venue": "COLING"
    },
    {
      "citation_id": "155",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "156",
      "title": "Palm-e: An embodied multimodal language model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Wahid"
      ],
      "year": "2023",
      "venue": "Palm-e: An embodied multimodal language model",
      "arxiv": "arXiv:2303.03378"
    },
    {
      "citation_id": "157",
      "title": "Clotho: An audio captioning dataset",
      "authors": [
        "Konstantinos Drossos",
        "Samuel Lipping",
        "Tuomas Virtanen"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "158",
      "title": "A survey of vision-language pre-trained models",
      "authors": [
        "Yifan Du",
        "Zikang Liu",
        "Junyi Li",
        "Wayne Zhao"
      ],
      "year": "2022",
      "venue": "A survey of vision-language pre-trained models",
      "arxiv": "arXiv:2202.10936"
    },
    {
      "citation_id": "159",
      "title": "Cross-modal data programming enables rapid medical machine learning",
      "authors": [
        "Jared Dunnmon",
        "Alexander Ratner",
        "Nishith Khandwala",
        "Khaled Saab",
        "Matthew Markert",
        "Hersh Sagreiya",
        "Roger Goldman",
        "Christopher Lee-Messer",
        "Matthew Lungren",
        "Daniel Rubin",
        "Christopher Ré"
      ],
      "year": "2019",
      "venue": "Cross-modal data programming enables rapid medical machine learning"
    },
    {
      "citation_id": "160",
      "title": "Cross-modal data programming enables rapid medical machine learning",
      "authors": [
        "Jared Dunnmon",
        "Alexander Ratner",
        "Khaled Saab",
        "Nishith Khandwala",
        "Matthew Markert",
        "Hersh Sagreiya",
        "Roger Goldman"
      ],
      "year": "2020",
      "venue": "Patterns"
    },
    {
      "citation_id": "161",
      "title": "Audio-visual speech modeling for continuous speech recognition",
      "authors": [
        "Stéphane Dupont",
        "Juergen Luettin"
      ],
      "year": "2000",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "162",
      "title": "An informationtheoretic quantification of discrimination with exempt features",
      "authors": [
        "Sanghamitra Dutta",
        "Praveen Venkatesh",
        "Piotr Mardziel",
        "Anupam Datta",
        "Pulkit Grover"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "163",
      "title": "Notes on noise contrastive estimation and negative sampling",
      "authors": [
        "Chris Dyer"
      ],
      "year": "2014",
      "venue": "Notes on noise contrastive estimation and negative sampling",
      "arxiv": "arXiv:1410.8251"
    },
    {
      "citation_id": "164",
      "title": "Medial orbitofrontal cortex codes relative rather than absolute value of financial rewards in humans",
      "authors": [
        "R Elliott",
        "Z Agnew",
        "J Deakin"
      ],
      "venue": "European Journal of Neuroscience",
      "doi": "10.1111/j.1460-9568.2008.06202.x"
    },
    {
      "citation_id": "165",
      "title": "Finding structure in time",
      "authors": [
        "Jeffrey L Elman"
      ],
      "year": "1990",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "166",
      "title": "Visualizing higher-layer features of a deep network",
      "authors": [
        "Yoshua Dumitru Erhan",
        "Aaron Bengio",
        "Pascal Courville",
        "Vincent"
      ],
      "year": "2009",
      "venue": "University of Montreal"
    },
    {
      "citation_id": "167",
      "title": "Multi-modal gesture recognition challenge 2013: Dataset and results",
      "authors": [
        "Sergio Escalera",
        "Jordi Gonzàlez",
        "Xavier Baró",
        "Miguel Reyes",
        "Oscar Lopes",
        "Isabelle Guyon",
        "Athitsos Vassilis",
        "Hugo Escalante"
      ],
      "year": "2013",
      "venue": "ICMI"
    },
    {
      "citation_id": "168",
      "title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention",
      "authors": [
        "Georgios Evangelopoulos",
        "Athanasia Zlatintsi",
        "Alexandros Potamianos"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "169",
      "title": "Contrastive learning as goalconditioned reinforcement learning",
      "authors": [
        "Benjamin Eysenbach",
        "Tianjun Zhang",
        "Sergey Levine",
        "Russ Salakhutdinov"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "170",
      "title": "Stacked latent attention for multimodal reasoning",
      "authors": [
        "Haoqi Fan",
        "Jiatong Zhou"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "171",
      "title": "Transmission of information: a statistical theory of communications",
      "authors": [
        "M Robert",
        "Fano"
      ],
      "year": "1968",
      "venue": "Transmission of information: a statistical theory of communications"
    },
    {
      "citation_id": "172",
      "title": "Every picture tells a story: Generating sentences from images",
      "authors": [
        "Ali Farhadi",
        "Mohsen Hejrati",
        "Mohammad Sadeghi",
        "Peter Young",
        "Cyrus Rashtchian",
        "Julia Hockenmaier",
        "David Forsyth"
      ],
      "year": "2010",
      "venue": "ECCV"
    },
    {
      "citation_id": "173",
      "title": "Relations between entropy and error probability",
      "authors": [
        "Meir Feder",
        "Neri Merhav"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on Information theory"
    },
    {
      "citation_id": "174",
      "title": "Learning robust representations via multi-view information bottleneck",
      "authors": [
        "Marco Federici",
        "Anjan Dutta",
        "Patrick Forré",
        "Nate Kushman",
        "Zeynep Akata"
      ],
      "year": "2020",
      "venue": "Learning robust representations via multi-view information bottleneck",
      "arxiv": "arXiv:2002.07017"
    },
    {
      "citation_id": "175",
      "title": "A survey of current datasets for vision and language research",
      "authors": [
        "Francis Ferraro",
        "Nasrin Mostafazadeh",
        "Ting-Hao Huang",
        "Lucy Vanderwende",
        "Jacob Devlin",
        "Michel Galley",
        "Margaret Mitchell"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D15-1021"
    },
    {
      "citation_id": "176",
      "title": "The development of infant discrimination of affect in multimodal and unimodal stimulation: The role of intersensory redundancy",
      "authors": [
        "Ross Flom",
        "Lorraine Bahrick"
      ],
      "year": "2007",
      "venue": "Developmental psychology"
    },
    {
      "citation_id": "177",
      "title": "Towards reliable multimodal stress detection under distribution shift",
      "authors": [
        "Andreas Foltyn",
        "Jessica Deuschel"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "178",
      "title": "Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers",
      "authors": [
        "Stella Frank",
        "Emanuele Bugliarello",
        "Desmond Elliott"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "179",
      "title": "Risk factors for suicidal thoughts and behaviors: a meta-analysis of 50 years of research",
      "authors": [
        "Jessica Joseph C Franklin",
        "Kathryn Ribeiro",
        "Kate Fox",
        "Evan Bentley",
        "Xieyining Kleiman",
        "Katherine Huang",
        "Adam Musacchio",
        "Bernard Jaroszewski",
        "Matthew Chang",
        "Nock"
      ],
      "year": "2017",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "180",
      "title": "On the classification of emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare applications",
      "authors": [
        "Christos Frantzidis",
        "Charalampos Bratsas",
        "Manousos Klados",
        "Evdokimos Konstantinidis",
        "Chrysa Lithari",
        "Ana Vivas",
        "Christos Papadelis",
        "Eleni Kaldoudi",
        "Costas Pappas",
        "Panagiotis Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "181",
      "title": "Predictive learning via rule ensembles",
      "authors": [
        "H Jerome",
        "Bogdan Friedman",
        "Popescu"
      ],
      "year": "2008",
      "venue": "The annals of applied statistics"
    },
    {
      "citation_id": "182",
      "title": "Devise: A deep visual-semantic embedding model",
      "authors": [
        "Andrea Frome",
        "Greg Corrado",
        "Jon Shlens",
        "Samy Bengio",
        "Jeff Dean",
        "Marc'aurelio Ranzato",
        "Tomas Mikolov"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "183",
      "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "authors": [
        "Akira Fukui",
        "Dong Park",
        "Daylen Yang",
        "Anna Rohrbach",
        "Trevor Darrell",
        "Marcus Rohrbach"
      ],
      "year": "2016",
      "venue": "EMNLP"
    },
    {
      "citation_id": "184",
      "title": "Multimodal web navigation with instruction-finetuned foundation models",
      "authors": [
        "Hiroki Furuta",
        "Ofir Nachum",
        "Kuang-Huei Lee",
        "Yutaka Matsuo",
        "Shixiang Shane Gu",
        "Izzeddin Gur"
      ],
      "year": "2023",
      "venue": "Multimodal web navigation with instruction-finetuned foundation models",
      "arxiv": "arXiv:2305.11854"
    },
    {
      "citation_id": "185",
      "title": "Early vs late fusion in multimodal convolutional neural networks",
      "authors": [
        "Konrad Gadzicki",
        "Razieh Khamsehashari",
        "Christoph Zetzsche"
      ],
      "year": "2020",
      "venue": "2020 IEEE 23rd International Conference on Information Fusion (FUSION)"
    },
    {
      "citation_id": "186",
      "title": "Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision",
      "authors": [
        "Zhe Gan",
        "Linjie Li",
        "Chunyuan Li",
        "Lijuan Wang",
        "Zicheng Liu",
        "Jianfeng Gao"
      ],
      "year": "2022",
      "venue": "Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision"
    },
    {
      "citation_id": "187",
      "title": "Llama-adapter v2: Parameter-efficient visual instruction model",
      "authors": [
        "Peng Gao",
        "Jiaming Han",
        "Renrui Zhang",
        "Ziyi Lin",
        "Shijie Geng",
        "Aojun Zhou",
        "Wei Zhang"
      ],
      "year": "2023",
      "venue": "Llama-adapter v2: Parameter-efficient visual instruction model",
      "arxiv": "arXiv:2304.15010"
    },
    {
      "citation_id": "188",
      "title": "5 d visual sound",
      "authors": [
        "Ruohan Gao",
        "Kristen Grauman"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "189",
      "title": "Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen",
        "Simcse"
      ],
      "year": "2021",
      "venue": "Simple contrastive learning of sentence embeddings",
      "arxiv": "arXiv:2104.08821"
    },
    {
      "citation_id": "190",
      "title": "Mental health monitoring with multimodal sensing and machine learning: A survey",
      "authors": [
        "Enrique Garcia-Ceja",
        "Michael Riegler",
        "Tine Nordgreen",
        "Petter Jakobsen"
      ],
      "year": "2018",
      "venue": "Pervasive and Mobile Computing"
    },
    {
      "citation_id": "191",
      "title": "Uncertainty and structure as psychological concepts",
      "authors": [
        "Wendell Garner"
      ],
      "year": "1962",
      "venue": "Uncertainty and structure as psychological concepts"
    },
    {
      "citation_id": "192",
      "title": "Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies",
      "authors": [
        "Itai Gat",
        "Idan Schwartz",
        "Alexander Schwing",
        "Tamir Hazan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "193",
      "title": "Perceptual score: What data modalities does your model perceive? NeurIPS",
      "authors": [
        "Itai Gat",
        "Idan Schwartz",
        "Alex Schwing"
      ],
      "year": "2021",
      "venue": "Perceptual score: What data modalities does your model perceive? NeurIPS"
    },
    {
      "citation_id": "194",
      "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
      "authors": [
        "Suchin Samuel Gehman",
        "Maarten Gururangan",
        "Yejin Sap",
        "Noah Choi",
        "Smith"
      ],
      "year": "2020",
      "venue": "EMNLP Findings"
    },
    {
      "citation_id": "195",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "Robert Geirhos",
        "Jörn-Henrik Jacobsen",
        "Claudio Michaelis",
        "Richard Zemel",
        "Wieland Brendel",
        "Matthias Bethge",
        "Felix Wichmann"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "196",
      "title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "authors": [
        "Mor Geva",
        "Yoav Goldberg",
        "Jonathan Berant"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "197",
      "title": "Interaction information for causal inference: The case of directed triangle",
      "authors": [
        "Amiremad Ghassami",
        "Negar Kiyavash"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Symposium on Information Theory (ISIT)"
    },
    {
      "citation_id": "198",
      "title": "Interpretation of neural networks is fragile",
      "authors": [
        "Amirata Ghorbani",
        "Abubakar Abid",
        "James Zou"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "199",
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "authors": [
        "David Leilani H Gilpin",
        "Bau",
        "Ayesha Ben Z Yuan",
        "Michael Bajwa",
        "Lalana Specter",
        "Kagal"
      ],
      "year": "2018",
      "venue": "2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)"
    },
    {
      "citation_id": "200",
      "title": "What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis",
      "authors": [
        "Dimitris Gkoumas",
        "Qiuchi Li",
        "Christina Lioma",
        "Yijun Yu",
        "Dawei Song"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "201",
      "title": "Improving the short-term prediction of suicidal behavior",
      "authors": [
        "Matthew Nock"
      ],
      "year": "2014",
      "venue": "S176-S180"
    },
    {
      "citation_id": "202",
      "title": "Approximate inference using conditional entropy decompositions",
      "authors": [
        "Amir Globerson",
        "Tommi Jaakkola"
      ],
      "year": "2007",
      "venue": "Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "203",
      "title": "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "authors": [
        "Xavier Glorot",
        "Antoine Bordes",
        "Yoshua Bengio"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11"
    },
    {
      "citation_id": "204",
      "title": "Vqa-lol: Visual question answering under the lens of logic",
      "authors": [
        "Tejas Gokhale",
        "Pratyay Banerjee",
        "Chitta Baral",
        "Yezhou Yang"
      ],
      "year": "2020",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "205",
      "title": "Rhythms of social interaction: Messaging within a massive online network",
      "authors": [
        "Dennis Scott A Golder",
        "Bernardo Wilkinson",
        "Huberman"
      ],
      "year": "2007",
      "venue": "Communities and technologies"
    },
    {
      "citation_id": "206",
      "title": "Towards transparent ai systems: Interpreting visual question answering models",
      "authors": [
        "Yash Goyal",
        "Akrit Mohapatra",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": "2016",
      "venue": "Towards transparent ai systems: Interpreting visual question answering models",
      "arxiv": "arXiv:1608.08974"
    },
    {
      "citation_id": "207",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "208",
      "title": "Unsupervised alignment of embeddings with wasserstein procrustes",
      "authors": [
        "Edouard Grave",
        "Armand Joulin",
        "Quentin Berthet"
      ],
      "year": "2019",
      "venue": "The 22nd International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "209",
      "title": "Speech recognition with deep recurrent neural networks",
      "authors": [
        "A Graves",
        "A Mohamed",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2013.6638947"
    },
    {
      "citation_id": "210",
      "title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "ICML"
    },
    {
      "citation_id": "211",
      "title": "Quantifying synergistic mutual information",
      "authors": [
        "Virgil Griffith",
        "Christof Koch"
      ],
      "year": "2014",
      "venue": "Guided self-organization: inception"
    },
    {
      "citation_id": "212",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "Jean-Bastien Grill",
        "Florian Strub",
        "Florent Altché",
        "Corentin Tallec",
        "Pierre Richemond",
        "Elena Buchatskaya",
        "Carl Doersch",
        "Bernardo Avila Pires",
        "Zhaohan Guo",
        "Mohammad Gheshlaghi Azar"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "213",
      "title": "Multimodal toolkit",
      "authors": [
        "Ken Gu"
      ],
      "year": "2020",
      "venue": "Multimodal toolkit"
    },
    {
      "citation_id": "214",
      "title": "A knowledge augmented transformer for vision-and-language",
      "authors": [
        "Liangke Gui",
        "Borui Wang",
        "Qiuyuan Huang",
        "Alex Hauptmann",
        "Yonatan Bisk",
        "Jianfeng Gao",
        "Kat"
      ],
      "year": "2021",
      "venue": "A knowledge augmented transformer for vision-and-language",
      "arxiv": "arXiv:2112.08614"
    },
    {
      "citation_id": "215",
      "title": "Multimodal semi-supervised learning for image classification",
      "authors": [
        "Matthieu Guillaumin",
        "Jakob Verbeek",
        "Cordelia Schmid"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer society conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "216",
      "title": "Vizwiz grand challenge: Answering visual questions from blind people",
      "authors": [
        "Danna Gurari",
        "Qing Li",
        "Abigale Stangl",
        "Anhong Guo",
        "Chi Lin",
        "Kristen Grauman",
        "Jiebo Luo",
        "Jeffrey Bigham"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "217",
      "title": "Minerl: a large-scale dataset of minecraft demonstrations",
      "authors": [
        "Brandon William H Guss",
        "Nicholay Houghton",
        "Phillip Topin",
        "Cayden Wang",
        "Manuela Codel",
        "Ruslan Veloso",
        "Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "218",
      "title": "",
      "authors": [
        "David Ha",
        "Andrew Dai",
        "Quoc V Le",
        "Hypernetworks"
      ],
      "year": "2016",
      "venue": "",
      "arxiv": "arXiv:1609.09106"
    },
    {
      "citation_id": "219",
      "title": "Accessible ui design and multimodal interaction through hybrid tv platforms: towards a virtual-user centered design framework",
      "authors": [
        "Pascal Hamisu",
        "Gregor Heinrich",
        "Christoph Jung",
        "Volker Hahn",
        "Carlos Duarte",
        "Pat Langdon",
        "Pradipta Biswas"
      ],
      "year": "2011",
      "venue": "International Conference on Universal Access in Human-Computer Interaction"
    },
    {
      "citation_id": "220",
      "title": "Demystifying local and global fairness trade-offs in federated learning using information theory",
      "authors": [
        "Faisal Hamman",
        "Sanghamitra Dutta"
      ],
      "year": "2023",
      "venue": "Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities"
    },
    {
      "citation_id": "221",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "222",
      "title": "Explaining black box predictions and unveiling data artifacts through influence functions",
      "authors": [
        "Xiaochuang Han",
        "Byron Wallace",
        "Yulia Tsvetkov"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "223",
      "title": "The social impact of deepfakes",
      "authors": [
        "T Jeffrey",
        "Jeremy Hancock",
        "Bailenson"
      ],
      "year": "2021",
      "venue": "The social impact of deepfakes"
    },
    {
      "citation_id": "224",
      "title": "Manymodalqa: Modality disambiguation and qa over diverse inputs",
      "authors": [
        "Darryl Hannan",
        "Akshay Jain",
        "Mohit Bansal"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "225",
      "title": "Learning by aligning videos in time",
      "authors": [
        "Sanjay Haresh",
        "Sateesh Kumar",
        "Huseyin Coskun",
        "N Shahram",
        "Andrey Syed",
        "Zeeshan Konin",
        "Quoc-Huy Zia",
        "Tran"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "226",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amirali Bagher Zadeh",
        "Jianyuan Zhong"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "227",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "228",
      "title": "Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A Data Collection Website That Provides Educators and Researchers with Easy Access to Multimodal Data Streams",
      "authors": [
        "Javaria Hassan",
        "Jovin Leong",
        "Bertrand Schneider"
      ],
      "venue": "Multimodal Data Collection Made Easy: The EZ-MMLA Toolkit: A Data Collection Website That Provides Educators and Researchers with Easy Access to Multimodal Data Streams",
      "doi": "10.1145/3448139.3448201"
    },
    {
      "citation_id": "229",
      "title": "Generalized additive models: some applications",
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani"
      ],
      "year": "1987",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "230",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "231",
      "title": "Pathvqa: 30000+ questions for medical visual question answering",
      "authors": [
        "Xuehai He",
        "Yichen Zhang",
        "Luntian Mou",
        "Eric Xing",
        "Pengtao Xie"
      ],
      "year": "2020",
      "venue": "Pathvqa: 30000+ questions for medical visual question answering",
      "arxiv": "arXiv:2003.10286"
    },
    {
      "citation_id": "232",
      "title": "Women also snowboard: Overcoming bias in captioning models",
      "authors": [
        "Anne Lisa",
        "Kaylee Hendricks",
        "Kate Burns",
        "Trevor Saenko",
        "Anna Darrell",
        "Rohrbach"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "233",
      "title": "Decoupling the role of data, attention, and losses in multimodal transformers",
      "authors": [
        "Anne Lisa",
        "John Hendricks",
        "Rosalia Mellor",
        "Jean-Baptiste Schneider",
        "Aida Alayrac",
        "Nematzadeh"
      ],
      "year": "2021",
      "venue": "Decoupling the role of data, attention, and losses in multimodal transformers",
      "arxiv": "arXiv:2102.00529"
    },
    {
      "citation_id": "234",
      "title": "Aligning AI with shared human values",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Steven Basart",
        "Andrew Critch",
        "Jerry Li",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "235",
      "title": "Trueskill™: A bayesian skill rating system",
      "authors": [
        "Ralf Herbrich",
        "Tom Minka",
        "Thore Graepel"
      ],
      "venue": "Trueskill™: A bayesian skill rating system"
    },
    {
      "citation_id": "236",
      "title": "Advances in Neural Information Processing Systems 19",
      "year": "2007",
      "venue": "Advances in Neural Information Processing Systems 19"
    },
    {
      "citation_id": "237",
      "title": "Does my multimodal model learn cross-modal interactions? it's harder to tell than you might think",
      "authors": [
        "Jack Hessel",
        "Lillian Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "238",
      "title": "Do androids laugh at electric sheep? humor\" understanding\" benchmarks from the new yorker caption contest",
      "authors": [
        "Jack Hessel",
        "Ana Marasović",
        "Jena Hwang",
        "Lillian Lee",
        "Jeff Da",
        "Rowan Zellers",
        "Robert Mankoff",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "Do androids laugh at electric sheep? humor\" understanding\" benchmarks from the new yorker caption contest",
      "arxiv": "arXiv:2209.06293"
    },
    {
      "citation_id": "239",
      "title": "Analyzing multimodal time series as dynamical systems",
      "authors": [
        "Shohei Hidaka",
        "Chen Yu"
      ],
      "year": "2010",
      "venue": "International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, ICMI-MLMI '10",
      "doi": "10.1145/1891903.1891968"
    },
    {
      "citation_id": "240",
      "title": "Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework",
      "authors": [
        "Irina Higgins",
        "Loic Matthey",
        "Arka Pal",
        "Christopher Burgess",
        "Xavier Glorot",
        "Matthew Botvinick"
      ],
      "year": "2016",
      "venue": "Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework"
    },
    {
      "citation_id": "241",
      "title": "Multimodal co-training for selecting good examples from webly labeled video",
      "authors": [
        "Ryota Hinami",
        "Junwei Liang",
        "Shin'ichi Satoh",
        "Alexander Hauptmann"
      ],
      "year": "2018",
      "venue": "Multimodal co-training for selecting good examples from webly labeled video",
      "arxiv": "arXiv:1804.06057"
    },
    {
      "citation_id": "242",
      "title": "Learning deep representations by mutual information estimation and maximization",
      "authors": [
        "Devon Hjelm",
        "Alex Fedorov",
        "Samuel Lavoie-Marchildon",
        "Karan Grewal",
        "Phil Bachman",
        "Adam Trischler",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "243",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "244",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "245",
      "title": "More diverse means better: Multimodal deep learning meets remote-sensing imagery classification",
      "authors": [
        "Danfeng Hong",
        "Lianru Gao",
        "Naoto Yokoya",
        "Jing Yao",
        "Jocelyn Chanussot",
        "Qian Du",
        "Bing Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing"
    },
    {
      "citation_id": "246",
      "title": "Learning to compose and reason with language tree structures for visual grounding",
      "authors": [
        "Richang Hong",
        "Daqing Liu",
        "Xiaoyu Mo",
        "Xiangnan He",
        "Hanwang Zhang"
      ],
      "year": "2019",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "247",
      "title": "Deep multimodal multilinear fusion with high-order polynomial pooling",
      "authors": [
        "Ming Hou",
        "Jiajia Tang",
        "Jianhai Zhang",
        "Wanzeng Kong",
        "Qibin Zhao"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "248",
      "title": "Adaptive structural co-regularization for unsupervised multi-view feature selection",
      "authors": [
        "Tsung-Yu Hsieh",
        "Yiwei Sun",
        "Suhang Wang",
        "Vasant Honavar"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Big Knowledge (ICBK)"
    },
    {
      "citation_id": "249",
      "title": "Unsupervised multimodal representation learning across medical images and reports",
      "authors": [
        "Tzu-Ming Harry Hsu",
        "Wei-Hung Weng",
        "Willie Boag",
        "Matthew Mcdermott",
        "Peter Szolovits"
      ],
      "year": "2018",
      "venue": "Unsupervised multimodal representation learning across medical images and reports",
      "arxiv": "arXiv:1811.08615"
    },
    {
      "citation_id": "250",
      "title": "Measuring the effects of non-identical data distribution for federated visual classification",
      "authors": [
        "Tzu-Ming Harry Hsu",
        "Hang Qi",
        "Matthew Brown"
      ],
      "year": "2019",
      "venue": "Measuring the effects of non-identical data distribution for federated visual classification",
      "arxiv": "arXiv:1909.06335"
    },
    {
      "citation_id": "251",
      "title": "Disentangling by partitioning: A representation learning framework for multimodal sensory data",
      "authors": [
        "Wei-Ning Hsu",
        "James Glass"
      ],
      "year": "2018",
      "venue": "Disentangling by partitioning: A representation learning framework for multimodal sensory data",
      "arxiv": "arXiv:1805.11264"
    },
    {
      "citation_id": "252",
      "title": "Deep multimodal clustering for unsupervised audiovisual learning",
      "authors": [
        "Di Hu",
        "Feiping Nie",
        "Xuelong Li"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "253",
      "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "authors": [
        "Junjie Hu",
        "Sebastian Ruder",
        "Aditya Siddhant",
        "Graham Neubig",
        "Orhan Firat",
        "Melvin Johnson"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "254",
      "title": "Multimodal adversarial network for cross-modal retrieval",
      "authors": [
        "Peng Hu",
        "Dezhong Peng",
        "Xu Wang",
        "Yong Xiang"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "255",
      "title": "Transformer is all you need: Multimodal multitask learning with a unified transformer",
      "authors": [
        "Ronghang Hu",
        "Amanpreet Singh"
      ],
      "year": "2021",
      "venue": "Transformer is all you need: Multimodal multitask learning with a unified transformer",
      "arxiv": "arXiv:2102.10772"
    },
    {
      "citation_id": "256",
      "title": "Natural language object retrieval",
      "authors": [
        "Ronghang Hu",
        "Huazhe Xu",
        "Marcus Rohrbach",
        "Jiashi Feng",
        "Kate Saenko",
        "Trevor Darrell"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "257",
      "title": "Learning to reason: End-to-end module networks for visual question answering",
      "authors": [
        "Ronghang Hu",
        "Jacob Andreas",
        "Marcus Rohrbach",
        "Trevor Darrell",
        "Kate Saenko"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "258",
      "title": "Open graph benchmark: Datasets for machine learning on graphs",
      "authors": [
        "Weihua Hu",
        "Matthias Fey",
        "Marinka Zitnik",
        "Yuxiao Dong",
        "Hongyu Ren",
        "Bowen Liu",
        "Michele Catasta",
        "Jure Leskovec"
      ],
      "year": "2020",
      "venue": "Open graph benchmark: Datasets for machine learning on graphs"
    },
    {
      "citation_id": "259",
      "title": "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
      "authors": [
        "Kexin Huang",
        "Jaan Altosaar",
        "Rajesh Ranganath"
      ],
      "year": "2019",
      "venue": "Clinicalbert: Modeling clinical notes and predicting hospital readmission"
    },
    {
      "citation_id": "260",
      "title": "Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models",
      "authors": [
        "Po-Yao Huang",
        "Mandela Patrick",
        "Junjie Hu",
        "Graham Neubig",
        "Florian Metze",
        "Alexander Hauptmann"
      ],
      "year": "2021",
      "venue": "Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models",
      "arxiv": "arXiv:2103.08849"
    },
    {
      "citation_id": "261",
      "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
      "authors": [
        "Wenlong Huang",
        "Pieter Abbeel",
        "Deepak Pathak",
        "Igor Mordatch"
      ],
      "year": "2022",
      "venue": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
      "arxiv": "arXiv:2201.07207"
    },
    {
      "citation_id": "262",
      "title": "Cross-modal common representation learning by hybrid transfer network",
      "authors": [
        "Xin Huang",
        "Yuxin Peng",
        "Mingkuan Yuan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "263",
      "title": "What makes multi-modal learning better than single (provably)",
      "authors": [
        "Yu Huang",
        "Chenzhuang Du",
        "Zihui Xue",
        "Xuanyao Chen",
        "Hang Zhao",
        "Longbo Huang"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "264",
      "title": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
      "authors": [
        "Yu Huang",
        "Junyang Lin",
        "Chang Zhou",
        "Hongxia Yang",
        "Longbo Huang"
      ],
      "year": "2022",
      "venue": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
      "arxiv": "arXiv:2203.12221"
    },
    {
      "citation_id": "265",
      "title": "Multimodal representation learning for recommendation in internet of things",
      "authors": [
        "Zhenhua Huang",
        "Xin Xu",
        "Juan Ni",
        "Honghao Zhu",
        "Cheng Wang"
      ],
      "year": "2019",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "266",
      "title": "Bidirectional lstm-crf models for sequence tagging",
      "authors": [
        "Zhiheng Huang",
        "Wei Xu",
        "Kai Yu"
      ],
      "year": "2015",
      "venue": "Bidirectional lstm-crf models for sequence tagging"
    },
    {
      "citation_id": "267",
      "title": "Learning by abstraction: The neural state machine",
      "authors": [
        "Drew Hudson",
        "Christopher Manning"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "268",
      "title": "Compositional attention networks for machine reasoning",
      "authors": [
        "A Drew",
        "Christopher Hudson",
        "Manning"
      ],
      "year": "2018",
      "venue": "Compositional attention networks for machine reasoning",
      "arxiv": "arXiv:1803.03067"
    },
    {
      "citation_id": "269",
      "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "authors": [
        "A Drew",
        "Christopher Hudson",
        "Manning"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "270",
      "title": "The measurement of social intelligence",
      "authors": [
        "Thelma Hunt"
      ],
      "year": "1928",
      "venue": "Journal of Applied Psychology"
    },
    {
      "citation_id": "271",
      "title": "Variational interaction information maximization for cross-domain disentanglement",
      "authors": [
        "Hyeongjoo Hwang",
        "Geon-Hyeong Kim",
        "Seunghoon Hong",
        "Kee-Eung Kim"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "272",
      "title": "Facial expression analysis",
      "year": "2017",
      "venue": "Facial expression analysis"
    },
    {
      "citation_id": "273",
      "title": "Evidential sparsification of multimodal latent spaces in conditional variational autoencoders",
      "authors": [
        "Masha Itkina",
        "B Ivanovic",
        "Ransalu Senanayake",
        "J Mykel",
        "Marco Kochenderfer",
        "Pavone"
      ],
      "year": "2020",
      "venue": "Evidential sparsification of multimodal latent spaces in conditional variational autoencoders"
    },
    {
      "citation_id": "274",
      "title": "Feature synergy, redundancy, and independence in global model explanations using shap vector decomposition",
      "authors": [
        "Jan Ittner",
        "Lukasz Bolikowski",
        "Konstantin Hemker",
        "Ricardo Kennedy"
      ],
      "year": "2021",
      "venue": "Feature synergy, redundancy, and independence in global model explanations using shap vector decomposition",
      "arxiv": "arXiv:2107.12436"
    },
    {
      "citation_id": "275",
      "title": "Deep unordered composition rivals syntactic methods for text classification",
      "authors": [
        "Mohit Iyyer",
        "Varun Manjunatha",
        "Jordan Boyd-Graber",
        "Hal Daumé"
      ],
      "year": "2015",
      "venue": "Deep unordered composition rivals syntactic methods for text classification"
    },
    {
      "citation_id": "276",
      "title": "Revisiting visual question answering baselines",
      "authors": [
        "Allan Jabri",
        "Armand Joulin",
        "Laurens Van Der Maaten"
      ],
      "year": "2016",
      "venue": "ECCV"
    },
    {
      "citation_id": "277",
      "title": "Perceiver io: A general architecture for structured inputs & outputs",
      "authors": [
        "Andrew Jaegle",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Carl Doersch",
        "Catalin Ionescu",
        "David Ding",
        "Skanda Koppula",
        "Daniel Zoran",
        "Andrew Brock",
        "Evan Shelhamer"
      ],
      "year": "2021",
      "venue": "Perceiver io: A general architecture for structured inputs & outputs",
      "arxiv": "arXiv:2107.14795"
    },
    {
      "citation_id": "278",
      "title": "General perception with iterative attention",
      "authors": [
        "Andrew Jaegle",
        "Felix Gimeno",
        "Andrew Brock",
        "Andrew Zisserman",
        "Oriol Vinyals",
        "Joao Carreira",
        "Perceiver"
      ],
      "year": "2021",
      "venue": "General perception with iterative attention",
      "arxiv": "arXiv:2103.03206"
    },
    {
      "citation_id": "279",
      "title": "Multimodal human-computer interaction: A survey",
      "authors": [
        "Alejandro Jaimes",
        "Nicu Sebe"
      ],
      "year": "2007",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "280",
      "title": "Mural: multimodal, multitask retrieval across languages",
      "authors": [
        "Aashi Jain",
        "Mandy Guo",
        "Krishna Srinivasan",
        "Ting Chen",
        "Sneha Kudugunta",
        "Chao Jia",
        "Yinfei Yang",
        "Jason Baldridge"
      ],
      "year": "2021",
      "venue": "Mural: multimodal, multitask retrieval across languages",
      "arxiv": "arXiv:2109.05125"
    },
    {
      "citation_id": "281",
      "title": "Recurrent Neural Networks: Design and Applications",
      "authors": [
        "L Jain",
        "L Medsker"
      ],
      "year": "1999",
      "venue": "Recurrent Neural Networks: Design and Applications"
    },
    {
      "citation_id": "282",
      "title": "Quantifying and visualizing attribute interactions: An approach based on entropy",
      "authors": [
        "Aleks Jakulin",
        "Ivan Bratko"
      ],
      "year": "2003",
      "venue": "Quantifying and visualizing attribute interactions: An approach based on entropy"
    },
    {
      "citation_id": "283",
      "title": "You said that?: Synthesising talking faces from audio",
      "authors": [
        "Joon Amir Jamaludin",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "284",
      "title": "Text-image-video summary generation using joint integer linear programming",
      "authors": [
        "Anubhav Jangra",
        "Adam Jatowt",
        "Mohammad Hasanuzzaman",
        "Sriparna Saha"
      ],
      "year": "2020",
      "venue": "European Conference on Information Retrieval"
    },
    {
      "citation_id": "285",
      "title": "Multiplicative interactions and where to find them",
      "authors": [
        "M Siddhant",
        "Wojciech Jayakumar",
        "Jacob Czarnecki",
        "Jonathan Menick",
        "Jack Schwarz",
        "Simon Rae",
        "Yee Osindero",
        "Tim Teh",
        "Razvan Harley",
        "Pascanu"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "286",
      "title": "Multiplicative interactions and where to find them",
      "authors": [
        "M Siddhant",
        "Wojciech Jayakumar",
        "Jacob Czarnecki",
        "Jonathan Menick",
        "Jack Schwarz",
        "Simon Rae",
        "Yee Osindero",
        "Tim Teh",
        "Razvan Harley",
        "Pascanu"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "287",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "288",
      "title": "Mimic-iii, a freely accessible critical care database. Scientific data",
      "authors": [
        "E Alistair",
        "Tom Johnson",
        "Lu Pollard",
        "Li-Wei H Shen",
        "Mengling Lehman",
        "Mohammad Feng",
        "Benjamin Ghassemi",
        "Peter Moody",
        "Leo Szolovits",
        "Roger Anthony Celi",
        "Mark"
      ],
      "year": "2016",
      "venue": "Mimic-iii, a freely accessible critical care database. Scientific data"
    },
    {
      "citation_id": "289",
      "title": "Mimic-iii, a freely accessible critical care database",
      "authors": [
        "E Alistair",
        "Tom Johnson",
        "Lu Pollard",
        "H Shen",
        "Mengling Lehman Li-Wei",
        "Mohammad Feng",
        "Ghassemi"
      ],
      "year": "2016",
      "venue": "Scientific data"
    },
    {
      "citation_id": "290",
      "title": "Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports",
      "authors": [
        "E Alistair",
        "Tom Johnson",
        "Seth Pollard",
        "Nathaniel Berkowitz",
        "Greenbaum",
        "Chih-Ying Matthew P Lungren",
        "Roger Deng",
        "Steven Mark",
        "Horng"
      ],
      "year": "2019",
      "venue": "Scientific data"
    },
    {
      "citation_id": "291",
      "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "authors": [
        "Justin Johnson",
        "Bharath Hariharan",
        "Laurens Van Der Maaten",
        "Li Fei-Fei",
        "C Lawrence Zitnick",
        "Ross Girshick"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "292",
      "title": "A contrastive objective for learning disentangled representations",
      "authors": [
        "Jonathan Kahana",
        "Yedid Hoshen"
      ],
      "year": "2022",
      "venue": "Computer Vision-ECCV 2022: 17th European Conference"
    },
    {
      "citation_id": "293",
      "title": "Llion Jones, and Jakob Uszkoreit. One model to learn them all",
      "authors": [
        "Lukasz Kaiser",
        "Aidan Gomez",
        "Noam Shazeer",
        "Ashish Vaswani",
        "Niki Parmar"
      ],
      "year": "2017",
      "venue": "Llion Jones, and Jakob Uszkoreit. One model to learn them all",
      "arxiv": "arXiv:1706.05137"
    },
    {
      "citation_id": "294",
      "title": "Mdetrmodulated detection for end-to-end multi-modal understanding",
      "authors": [
        "Aishwarya Kamath",
        "Mannat Singh",
        "Yann Lecun",
        "Ishan Misra",
        "Gabriel Synnaeve",
        "Nicolas Carion"
      ],
      "year": "2021",
      "venue": "Mdetrmodulated detection for end-to-end multi-modal understanding",
      "arxiv": "arXiv:2104.12763"
    },
    {
      "citation_id": "295",
      "title": "Multimodal explanations by predicting counterfactuality in videos",
      "authors": [
        "Atsushi Kanehira",
        "Kentaro Takemoto",
        "Sho Inayoshi",
        "Tatsuya Harada"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "296",
      "title": "Deep fragment embeddings for bidirectional image sentence mapping",
      "authors": [
        "Andrej Karpathy",
        "Armand Joulin"
      ],
      "year": "2014",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "297",
      "title": "Analyzing and improving the image quality of stylegan",
      "authors": [
        "Tero Karras",
        "Samuli Laine",
        "Miika Aittala",
        "Janne Hellsten",
        "Jaakko Lehtinen",
        "Timo Aila"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "298",
      "title": "The kinetics human action video dataset",
      "authors": [
        "Will Kay",
        "Joao Carreira",
        "Karen Simonyan",
        "Brian Zhang",
        "Chloe Hillier",
        "Sudheendra Vijayanarasimhan",
        "Fabio Viola",
        "Tim Green",
        "Trevor Back",
        "Paul Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "299",
      "title": "Conjugate mixture models for clustering multimodal data",
      "authors": [
        "Florence Vasil Khalidov",
        "Radu Forbes",
        "Horaud"
      ],
      "year": "2011",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "300",
      "title": "Approximate graph laplacians for multimodal data clustering",
      "authors": [
        "Aparajita Khan",
        "Pradipta Maji"
      ],
      "year": "2019",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "301",
      "title": "Multi-manifold optimization for multi-view subspace clustering",
      "authors": [
        "Aparajita Khan",
        "Pradipta Maji"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "302",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "303",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "Douwe Kiela",
        "Hamed Firooz",
        "Aravind Mohan",
        "Vedanuj Goswami",
        "Amanpreet Singh",
        "Pratik Ringshia",
        "Davide Testuggine"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "304",
      "title": "Social intelligence. Handbook of intelligence",
      "authors": [
        "F John",
        "Nancy Kihlstrom",
        "Cantor"
      ],
      "year": "2000",
      "venue": "Social intelligence. Handbook of intelligence"
    },
    {
      "citation_id": "305",
      "title": "Transferring pre-trained multimodal representations with cross-modal similarity matching",
      "authors": [
        "Byoungjip Kim",
        "Sungik Choi",
        "Dasol Hwang",
        "Moontae Lee",
        "Honglak Lee"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "306",
      "title": "Social robots as embedded reinforcers of social behavior in children with autism",
      "authors": [
        "Elizabeth S Kim",
        "Lauren Berkovits",
        "Emily Bernier",
        "Dan Leyzberg",
        "Frederick Shic",
        "Rhea Paul",
        "Brian Scassellati"
      ],
      "year": "2013",
      "venue": "Journal of autism and developmental disorders"
    },
    {
      "citation_id": "307",
      "title": "Development of person-independent emotion recognition system based on multiple physiological signals",
      "authors": [
        "Kyung Hwan",
        "Seok Bang",
        "Sang Kim"
      ],
      "year": "2002",
      "venue": "Proceedings of the Second Joint 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering Society Engineering in Medicine and Biology"
    },
    {
      "citation_id": "308",
      "title": "Joint patch clustering-based dictionary learning for multimodal image fusion",
      "authors": [
        "Minjae Kim",
        "David Han",
        "Hanseok Ko"
      ],
      "year": "2016",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "309",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "Wonjae Kim",
        "Bokyung Son",
        "Ildoo Kim"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "310",
      "title": "Embedded multimodal interfaces in robotics: applications, future trends, and societal implications",
      "authors": [
        "Elsa Kirchner",
        "Stephen Fairclough",
        "Frank Kirchner"
      ],
      "year": "2019",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions"
    },
    {
      "citation_id": "311",
      "title": "Entropic causal inference",
      "authors": [
        "Murat Kocaoglu",
        "Alexandros Dimakis",
        "Sriram Vishwanath",
        "Babak Hassibi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "312",
      "title": "Text-to-image generation grounded by fine-grained user attention",
      "authors": [
        "Jing Yu Koh",
        "Jason Baldridge",
        "Honglak Lee",
        "Yinfei Yang"
      ],
      "year": "2021",
      "venue": "Text-to-image generation grounded by fine-grained user attention"
    },
    {
      "citation_id": "313",
      "title": "Concept bottleneck models",
      "authors": [
        "Pang Wei Koh",
        "Thao Nguyen",
        "Siang Yew",
        "Stephen Tang",
        "Emma Mussmann",
        "Been Pierson",
        "Percy Kim",
        "Liang"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "314",
      "title": "Wilds: A benchmark of in-the-wild distribution shifts",
      "authors": [
        "Pang Wei Koh",
        "Shiori Sagawa",
        "Henrik Marklund",
        "Sang Michael Xie",
        "Marvin Zhang",
        "Akshay Balsubramani",
        "Weihua Hu",
        "Michihiro Yasunaga",
        "Richard Phillips",
        "Sara Beery"
      ],
      "year": "2020",
      "venue": "Wilds: A benchmark of in-the-wild distribution shifts",
      "arxiv": "arXiv:2012.07421"
    },
    {
      "citation_id": "315",
      "title": "Private traits and attributes are predictable from digital records of human behavior",
      "authors": [
        "Michal Kosinski",
        "David Stillwell",
        "Thore Graepel"
      ],
      "year": "2013",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "316",
      "title": "Multimodal images in the brain. The neurophysiological foundations of mental and motor imagery",
      "authors": [
        "Giorgio Stephen M Kosslyn",
        "William Ganis",
        "Thompson"
      ],
      "year": "2010",
      "venue": "Multimodal images in the brain. The neurophysiological foundations of mental and motor imagery"
    },
    {
      "citation_id": "317",
      "title": "Visual coreference resolution in visual dialog using neural module networks",
      "authors": [
        "Satwik Kottur",
        "M José",
        "Devi Moura",
        "Dhruv Parikh",
        "Marcus Batra",
        "Rohrbach"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "318",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "Klaus Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing krippendorff's alpha-reliability"
    },
    {
      "citation_id": "319",
      "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "authors": [
        "Ranjay Krishna",
        "Yuke Zhu",
        "Oliver Groth",
        "Justin Johnson",
        "Kenji Hata",
        "Joshua Kravitz"
      ],
      "year": "2017",
      "venue": "IJCV"
    },
    {
      "citation_id": "320",
      "title": "The disagreement problem in explainable machine learning: A practitioner's perspective",
      "authors": [
        "Satyapriya Krishna",
        "Tessa Han",
        "Alex Gu",
        "Javin Pombra",
        "Shahin Jabbari",
        "Steven Wu",
        "Himabindu Lakkaraju"
      ],
      "year": "2022",
      "venue": "The disagreement problem in explainable machine learning: A practitioner's perspective",
      "arxiv": "arXiv:2202.01602"
    },
    {
      "citation_id": "321",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "322",
      "title": "Image retrieval from contextual descriptions",
      "authors": [
        "Benno Krojer",
        "Vaibhav Adlakha",
        "Vibhav Vineet",
        "Yash Goyal",
        "Edoardo Ponti",
        "Siva Reddy"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "323",
      "title": "An overview of sequence comparison: Time warps, string edits, and macromolecules",
      "authors": [
        "Joseph"
      ],
      "year": "1983",
      "venue": "SIAM review"
    },
    {
      "citation_id": "324",
      "title": "A new view of language acquisition",
      "authors": [
        "Patricia Kuhl"
      ],
      "year": "2000",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.97.22.11850"
    },
    {
      "citation_id": "325",
      "title": "The nethack learning environment",
      "authors": [
        "Heinrich Küttler",
        "Nantas Nardelli",
        "Alexander Miller",
        "Roberta Raileanu",
        "Marco Selvatici",
        "Edward Grefenstette",
        "Tim Rocktäschel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "326",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Pereira"
      ],
      "year": "2001",
      "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01"
    },
    {
      "citation_id": "327",
      "title": "Kernel and nonlinear canonical correlation analysis",
      "authors": [
        "Pei Ling",
        "Colin Fyfe"
      ],
      "year": "2000",
      "venue": "International Journal of Neural Systems"
    },
    {
      "citation_id": "328",
      "title": "Reusing neural speech representations for auditory emotion recognition",
      "authors": [
        "Egor Lakomkin",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Reusing neural speech representations for auditory emotion recognition"
    },
    {
      "citation_id": "329",
      "title": "Extracting low-dimensional latent structure from time series in the presence of delays",
      "authors": [
        "Karthik Lakshmanan",
        "Patrick Sadtler",
        "Elizabeth Tyler-Kabara",
        "Aaron Batista",
        "Byron Yu"
      ],
      "year": "2015",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "330",
      "title": "In-patient suicide: selection of people at risk, failure of protection and the possibility of causation",
      "authors": [
        "Matthew Michael Large",
        "Daniel Chung",
        "Michael Davidson",
        "Mark Weiser",
        "Christopher James"
      ],
      "year": "2017",
      "venue": "BJPsych Open"
    },
    {
      "citation_id": "331",
      "title": "A dataset of clinically generated visual questions and answers about radiology images",
      "authors": [
        "Jason Lau",
        "Soumya Gayen",
        "Asma Ben Abacha",
        "Dina Demner-Fushman"
      ],
      "year": "2018",
      "venue": "Scientific data"
    },
    {
      "citation_id": "332",
      "title": "A simple way to initialize recurrent networks of rectified linear units",
      "authors": [
        "V Quoc",
        "Navdeep Le",
        "Geoffrey Jaitly",
        "Hinton"
      ],
      "year": "2015",
      "venue": "A simple way to initialize recurrent networks of rectified linear units"
    },
    {
      "citation_id": "333",
      "title": "Phrase-based image captioning",
      "authors": [
        "Rémi Lebret",
        "Pedro Pinheiro",
        "Ronan Collobert"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "334",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Yann Lecun",
        "Léon Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "335",
      "title": "Lecture presentations multimodal dataset: Towards understanding multimodality in educational videos",
      "authors": [
        "Chaitanya Dong Won Lee",
        "Paul Ahuja",
        "Sanika Liang",
        "Louis-Philippe Natu",
        "Morency"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "336",
      "title": "Multimodal sensor fusion with differentiable filters",
      "authors": [
        "Michelle Lee",
        "Brent Yi",
        "Roberto Martín-Martín",
        "Silvio Savarese",
        "Jeannette Bohg"
      ],
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "337",
      "title": "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks",
      "authors": [
        "Michelle Lee",
        "Yuke Zhu",
        "Krishnan Srinivasan",
        "Parth Shah",
        "Silvio Savarese"
      ],
      "year": "2019",
      "venue": "ICRA"
    },
    {
      "citation_id": "338",
      "title": "Detect, reject, correct: Crossmodal compensation of corrupted sensors",
      "authors": [
        "Michelle Lee",
        "Matthew Tan",
        "Yuke Zhu",
        "Jeannette Bohg"
      ],
      "venue": "IEEE International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "339",
      "title": "Parameter efficient multimodal transformers for video representation learning",
      "authors": [
        "Sangho Lee",
        "Youngjae Yu",
        "Gunhee Kim",
        "Thomas Breuel",
        "Jan Kautz",
        "Yale Song"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "340",
      "title": "Multimodal prompting with missing modalities for visual recognition",
      "authors": [
        "Yi-Lun Lee",
        "Yi-Hsuan Tsai",
        "Wei-Chen Chiu",
        "Chen-Yu Lee"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "341",
      "title": "Tvqa: Localized, compositional video question answering",
      "authors": [
        "Jie Lei",
        "Licheng Yu",
        "Mohit Bansal",
        "Tamara Berg"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "342",
      "title": "Enrico: A dataset for topic modeling of mobile ui designs",
      "authors": [
        "Asutosh Luis A Leiva",
        "Antti Hota",
        "Oulasvirta"
      ],
      "year": "2020",
      "venue": "22nd International Conference on Human-Computer Interaction with Mobile Devices and Services"
    },
    {
      "citation_id": "343",
      "title": "Tidigits speech corpus",
      "authors": [
        "Leonard Gary",
        "George Doddington"
      ],
      "year": "1993",
      "venue": "Tidigits speech corpus"
    },
    {
      "citation_id": "344",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "Brian Lester",
        "Rami Al-Rfou",
        "Noah Constant"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "345",
      "title": "Surface form and memory in question answering",
      "authors": [
        "Willem Levelt",
        "Stephanie Kelter"
      ],
      "year": "1982",
      "venue": "Cognitive Psychology",
      "doi": "10.1016/0010-0285(82)90005-6"
    },
    {
      "citation_id": "346",
      "title": "Heterogeneous uncertainty sampling for supervised learning",
      "authors": [
        "D David",
        "Jason Lewis",
        "Catlett"
      ],
      "year": "1994",
      "venue": "Machine learning proceedings"
    },
    {
      "citation_id": "347",
      "title": "Multi-modal summarization for asynchronous collection of text, image, audio and video",
      "authors": [
        "Haoran Li",
        "Junnan Zhu",
        "Cong Ma",
        "Jiajun Zhang",
        "Chengqing Zong"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "348",
      "title": "Deep learning in multimodal remote sensing data fusion: A comprehensive review",
      "authors": [
        "Jiaxin Li",
        "Danfeng Hong",
        "Lianru Gao",
        "Jing Yao",
        "Ke Zheng",
        "Bing Zhang",
        "Jocelyn Chanussot"
      ],
      "year": "2022",
      "venue": "International Journal of Applied Earth Observation and Geoinformation"
    },
    {
      "citation_id": "349",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    },
    {
      "citation_id": "350",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "351",
      "title": "What does bert with vision look at",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "352",
      "title": "Keep meeting summaries on topic: Abstractive multi-modal meeting summarization",
      "authors": [
        "Manling Li",
        "Lingyu Zhang",
        "Ji Heng",
        "Richard Radke"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "353",
      "title": "Clip-event: Connecting text and images with event structures",
      "authors": [
        "Manling Li",
        "Ruochen Xu",
        "Shuohang Wang",
        "Luowei Zhou",
        "Xudong Lin",
        "Chenguang Zhu",
        "Michael Zeng",
        "Ji Heng",
        "Shih-Fu Chang"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "354",
      "title": "Vmsmo: Learning to generate multimodal summary for video-based news articles",
      "authors": [
        "Mingzhe Li",
        "Xiuying Chen",
        "Shen Gao",
        "Zhangming Chan",
        "Dongyan Zhao",
        "Rui Yan"
      ],
      "year": "2020",
      "venue": "Vmsmo: Learning to generate multimodal summary for video-based news articles",
      "arxiv": "arXiv:2010.05406"
    },
    {
      "citation_id": "355",
      "title": "Towards a unified foundation model: Jointly pre-training transformers on unpaired images and text",
      "authors": [
        "Qing Li",
        "Boqing Gong",
        "Yin Cui",
        "Dan Kondratyuk",
        "Xianzhi Du"
      ],
      "year": "2021",
      "venue": "Towards a unified foundation model: Jointly pre-training transformers on unpaired images and text",
      "arxiv": "arXiv:2112.07074"
    },
    {
      "citation_id": "356",
      "title": "Multi-view representation learning with manifold smoothness",
      "authors": [
        "Shu Li",
        "Wei Wang",
        "Wen-Tao Li",
        "Pan Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "357",
      "title": "Pre-trained language models for interactive decision-making",
      "authors": [
        "Shuang Li",
        "Xavier Puig",
        "Yilun Du",
        "Clinton Wang",
        "Ekin Akyurek",
        "Antonio Torralba",
        "Jacob Andreas",
        "Igor Mordatch"
      ],
      "year": "2022",
      "venue": "Pre-trained language models for interactive decision-making",
      "arxiv": "arXiv:2202.01771"
    },
    {
      "citation_id": "358",
      "title": "Federated optimization in heterogeneous networks",
      "authors": [
        "Tian Li",
        "Anit Kumar Sahu",
        "Manzil Zaheer",
        "Maziar Sanjabi",
        "Ameet Talwalkar",
        "Virginia Smith"
      ],
      "year": "2020",
      "venue": "Proceedings of Machine Learning and Systems"
    },
    {
      "citation_id": "359",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "ACL-IJCNLP"
    },
    {
      "citation_id": "360",
      "title": "Brainish: Formalizing a multimodal language for intelligence and consciousness",
      "authors": [
        "Paul Pu"
      ],
      "year": "2022",
      "venue": "Brainish: Formalizing a multimodal language for intelligence and consciousness",
      "arxiv": "arXiv:2205.00001"
    },
    {
      "citation_id": "361",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "362",
      "title": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion",
      "authors": [
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion"
    },
    {
      "citation_id": "363",
      "title": "Multimodal local-global ranking fusion for emotion recognition",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "364",
      "title": "Strong and simple baselines for multimodal utterance embeddings",
      "authors": [
        "Paul Pu Liang",
        "Chong Lim",
        "Yao-Hung Hubert Tsai",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "365",
      "title": "Think locally, act globally: Federated learning with local and global representations. NeurIPS Workshop on Federated Learning",
      "authors": [
        "Paul Pu Liang",
        "Terrance Liu",
        "Liu Ziyin",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Think locally, act globally: Federated learning with local and global representations. NeurIPS Workshop on Federated Learning"
    },
    {
      "citation_id": "366",
      "title": "Learning representations from imperfect time series data via tensor rank regularization",
      "authors": [
        "Paul Pu Liang",
        "Zhun Liu",
        "Yao-Hung Hubert Tsai",
        "Qibin Zhao",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "367",
      "title": "Towards debiasing sentence representations",
      "authors": [
        "Paul Pu Liang",
        "Irene Li",
        "Emily Zheng",
        "Chong Yao",
        "Ruslan Lim",
        "Louis-Philippe Salakhutdinov",
        "Morency"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "368",
      "title": "Learning language and multimodal privacy-preserving markers of mood from mobile data",
      "authors": [
        "Paul Pu Liang",
        "Terrance Liu",
        "Anna Cai",
        "Michal Muszynski",
        "Ryo Ishii",
        "Nicholas Allen",
        "Randy Auerbach"
      ],
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "369",
      "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Yufan Chen"
      ],
      "year": "2021",
      "venue": "NeurIPS Datasets and Benchmarks Track"
    },
    {
      "citation_id": "370",
      "title": "Towards understanding and mitigating social biases in language models",
      "authors": [
        "Paul Pu Liang",
        "Chiyu Wu",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "371",
      "title": "Cross-modal generalization: Learning in low resource modalities via meta-alignment",
      "authors": [
        "Paul Pu Liang",
        "Peter Wu",
        "Liu Ziyin",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "372",
      "title": "High-modality multimodal transformer: Quantifying modality & interaction heterogeneity for high-modality representation learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Jeffrey Tsaw",
        "Yudong Liu",
        "Shentong Mo",
        "Dani Yogatama",
        "Louis-Philippe Morency",
        "Russ Salakhutdinov"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "373",
      "title": "Quantifying & modeling feature interactions: An information decomposition framework",
      "authors": [
        "Paul Pu Liang",
        "Yun Cheng",
        "Xiang Fan",
        "Kai Chun",
        "Suzanne Ling",
        "Richard Nie",
        "Zihao Chen",
        "Faisal Deng",
        "Ruslan Mahmood",
        "Louis-Philippe Salakhutdinov",
        "Morency"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "374",
      "title": "Multimodal fusion interactions: A study of human and automatic quantification",
      "authors": [
        "Paul Pu Liang",
        "Yun Cheng",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2023",
      "venue": "Multimodal fusion interactions: A study of human and automatic quantification"
    },
    {
      "citation_id": "375",
      "title": "Factorized contrastive learning: Going beyond multi-view redundancy",
      "authors": [
        "Paul Pu Liang",
        "Zihao Deng",
        "Martin Ma",
        "James Zou",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "376",
      "title": "Multiviz: Towards visualizing and understanding multimodal models",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Gunjan Chhablani",
        "Nihal Jain",
        "Zihao Deng",
        "Xingbo Wang",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "377",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2023",
      "venue": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions"
    },
    {
      "citation_id": "378",
      "title": "Multimodal learning without multimodal data: Guarantees and applications",
      "authors": [
        "Paul Pu Liang",
        "Kai Ling",
        "Yun Cheng",
        "Alex Obolenskiy",
        "Yudong Liu",
        "Rohan Pandey",
        "Alex Wilf",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2024",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "379",
      "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
      "authors": [
        "Yuhui Victor Weixin Liang",
        "Yongchan Zhang",
        "Serena Kwon",
        "James Yeung",
        "Zou"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "380",
      "title": "Polyvit: Co-training vision transformers on images, videos and audio",
      "authors": [
        "Mostafa Valerii Likhosherstov",
        "Anurag Dehghani",
        "Arnab",
        "Marcin Krzysztof",
        "Mario Choromanski",
        "Yi Lucic",
        "Adrian Tay",
        "Weller"
      ],
      "year": "2022",
      "venue": "Polyvit: Co-training vision transformers on images, videos and audio"
    },
    {
      "citation_id": "381",
      "title": "Temporal fusion transformers for interpretable multi-horizon time series forecasting",
      "authors": [
        "Bryan Lim",
        "Ö Sercan",
        "Nicolas Arık",
        "Tomas Loeff",
        "Pfister"
      ],
      "year": "2021",
      "venue": "International Journal of Forecasting"
    },
    {
      "citation_id": "382",
      "title": "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
      "authors": [
        "Xinyue Bill Yuchen Lin",
        "Jamin Chen",
        "Xiang Chen",
        "Ren"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "383",
      "title": "Microsoft coco: Common objects in context",
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Dollár",
        "C Lawrence"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "384",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Zhouhan Lin",
        "Minwei Feng",
        "Cicero Nogueira Dos Santos",
        "Mo Yu",
        "Bing Xiang",
        "Bowen Zhou",
        "Yoshua Bengio"
      ],
      "year": "2017",
      "venue": "A structured self-attentive sentence embedding",
      "arxiv": "arXiv:1703.03130"
    },
    {
      "citation_id": "385",
      "title": "Artificial intelligence for multimodal data integration in oncology",
      "authors": [
        "Jana Lipkova",
        "Richard Chen",
        "Bowen Chen",
        "Ming Lu",
        "Matteo Barbieri",
        "Daniel Shao"
      ],
      "year": "2022",
      "venue": "Cancer cell"
    },
    {
      "citation_id": "386",
      "title": "Cross-modal discrete representation learning",
      "authors": [
        "Alex Liu",
        "Souyoung Jin",
        "Cheng-I Lai",
        "Andrew Rouditchenko",
        "Aude Oliva",
        "James Glass"
      ],
      "year": "2022",
      "venue": "ACL"
    },
    {
      "citation_id": "387",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Visual instruction tuning",
      "arxiv": "arXiv:2304.08485"
    },
    {
      "citation_id": "388",
      "title": "Mitigating political bias in language models through reinforced calibration",
      "authors": [
        "Ruibo Liu",
        "Chenyan Jia",
        "Jason Wei",
        "Guangxuan Xu",
        "Lili Wang",
        "Soroush Vosoughi"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "389",
      "title": "Mmkg: multi-modal knowledge graphs",
      "authors": [
        "Ye Liu",
        "Hui Li",
        "Alberto Garcia-Duran",
        "Mathias Niepert",
        "Daniel Onoro-Rubio",
        "David Rosenblum"
      ],
      "year": "2019",
      "venue": "European Semantic Web Conference"
    },
    {
      "citation_id": "390",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "391",
      "title": "On the development of a multi-tier, multimodal wireless sensor network for wild life monitoring",
      "authors": [
        "Carlos Eduardo",
        "Rodrigues Lopes",
        "Linnyer Beatrys Ruiz"
      ],
      "year": "2008",
      "venue": "2008 1st IFIP Wireless Days"
    },
    {
      "citation_id": "392",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "393",
      "title": "Pretrained transformers as universal computation engines",
      "authors": [
        "Kevin Lu",
        "Aditya Grover",
        "Pieter Abbeel",
        "Igor Mordatch"
      ],
      "year": "2021",
      "venue": "Pretrained transformers as universal computation engines",
      "arxiv": "arXiv:2103.05247"
    },
    {
      "citation_id": "394",
      "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
      "authors": [
        "Pan Lu",
        "Swaroop Mishra",
        "Tony Xia",
        "Liang Qiu",
        "Kai-Wei Chang",
        "Song-Chun Zhu",
        "Oyvind Tafjord",
        "Peter Clark",
        "Ashwin Kalyan"
      ],
      "venue": "The 36th Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "395",
      "title": "An empirical study of scaling instruct-tuned large multimodal models",
      "authors": [
        "Yadong Lu",
        "Chunyuan Li",
        "Haotian Liu",
        "Jianwei Yang",
        "Jianfeng Gao",
        "Yelong Shen"
      ],
      "year": "2023",
      "venue": "An empirical study of scaling instruct-tuned large multimodal models",
      "arxiv": "arXiv:2309.09958"
    },
    {
      "citation_id": "396",
      "title": "A survey of reinforcement learning informed by natural language",
      "authors": [
        "Jelena Luketina",
        "Nantas Nardelli",
        "Gregory Farquhar",
        "Jakob Foerster",
        "Jacob Andreas",
        "Edward Grefenstette",
        "Shimon Whiteson",
        "Tim Rocktäschel"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "397",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "398",
      "title": "Dime: Finegrained interpretations of multimodal models via disentangled local explanations. AIES '22",
      "authors": [
        "Yiwei Lyu",
        "Paul Liang",
        "Zihao Deng",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "ISBN 9781450392471",
      "doi": "10.1145/3514094.3534148"
    },
    {
      "citation_id": "399",
      "title": "Conditional contrastive learning for improving fairness in self-supervised learning",
      "authors": [
        "Yao-Hung Hubert Martin Q Ma",
        "Paul Tsai",
        "Han Liang",
        "Kun Zhao",
        "Ruslan Zhang",
        "Louis-Philippe Salakhutdinov",
        "Morency"
      ],
      "year": "2021",
      "venue": "Conditional contrastive learning for improving fairness in self-supervised learning",
      "arxiv": "arXiv:2106.02866"
    },
    {
      "citation_id": "400",
      "title": "Smil: Multimodal learning with severely missing modality",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Sergey Tulyakov",
        "Cathy Wu",
        "Xi Peng"
      ],
      "year": "2021",
      "venue": "Smil: Multimodal learning with severely missing modality",
      "arxiv": "arXiv:2103.05677"
    },
    {
      "citation_id": "401",
      "title": "Are multimodal transformers robust to missing modality?",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Davide Testuggine",
        "Xi Peng"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "402",
      "title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "authors": [
        "Xuezhe Ma",
        "Eduard Hovy"
      ],
      "year": "2016",
      "venue": "End-to-end sequence labeling via bi-directional lstm-cnns-crf"
    },
    {
      "citation_id": "403",
      "title": "Multisensory spatial interactions: a window onto functional integration in the human brain",
      "authors": [
        "Emiliano Macaluso",
        "Jon Driver"
      ],
      "year": "2005",
      "venue": "Trends in neurosciences"
    },
    {
      "citation_id": "404",
      "title": "An overview on clustering methods",
      "authors": [
        "Madhulatha Soni"
      ],
      "year": "2012",
      "venue": "An overview on clustering methods",
      "arxiv": "arXiv:1205.1117"
    },
    {
      "citation_id": "405",
      "title": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering",
      "authors": [
        "Tegan Maharaj",
        "Nicolas Ballas",
        "Anna Rohrbach",
        "Aaron Courville",
        "Christopher Pal"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "406",
      "title": "What's cookin'? interpreting cooking videos using text, speech and vision",
      "authors": [
        "Jonathan Malmaud",
        "Jonathan Huang",
        "Vivek Rathod",
        "Nicholas Johnston",
        "Andrew Rabinovich",
        "Kevin Murphy"
      ],
      "year": "2015",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "407",
      "title": "Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings",
      "authors": [
        "Thomas Manzini",
        "Lim Yao Chong",
        "Alan Black",
        "Yulia Tsvetkov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1062"
    },
    {
      "citation_id": "408",
      "title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
      "authors": [
        "Jiayuan Mao",
        "Chuang Gan",
        "Pushmeet Kohli",
        "Joshua Tenenbaum",
        "Jiajun Wu"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "409",
      "title": "Generation and comprehension of unambiguous object descriptions",
      "authors": [
        "Junhua Mao",
        "Jonathan Huang",
        "Alexander Toshev",
        "Oana Camburu",
        "Alan Yuille",
        "Kevin Murphy"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "410",
      "title": "Spoken language interaction with robots: Recommendations for future research",
      "authors": [
        "Matthew Marge",
        "Carol Espy-Wilson",
        "Nigel Ward",
        "Abeer Alwan",
        "Yoav Artzi",
        "Mohit Bansal"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "411",
      "title": "The more you know: Using knowledge graphs for image classification",
      "authors": [
        "Kenneth Marino",
        "Ruslan Salakhutdinov",
        "Abhinav Gupta"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "412",
      "title": "A taxonomy of relationships between images and text",
      "authors": [
        "Emily Marsh",
        "Marilyn White"
      ],
      "year": "2003",
      "venue": "Journal of documentation"
    },
    {
      "citation_id": "413",
      "title": "Semi-supervised aggregation of dependent weak supervision sources with performance guarantees",
      "authors": [
        "Alessio Mazzetto",
        "Dylan Sam",
        "Andrew Park",
        "Eli Upfal",
        "Stephen Bach"
      ],
      "year": "2021",
      "venue": "AISTATS"
    },
    {
      "citation_id": "414",
      "title": "Multivariate information transmission",
      "authors": [
        "William Mcgill"
      ],
      "year": "1954",
      "venue": "Transactions of the IRE Professional Group on Information Theory"
    },
    {
      "citation_id": "415",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "Brendan Mcmahan",
        "Eider Moore",
        "Daniel Ramage",
        "Seth Hampson",
        "Blaise Agüera Y Arcas"
      ],
      "year": "2016",
      "venue": "Communication-efficient learning of deep networks from decentralized data"
    },
    {
      "citation_id": "416",
      "title": "Multimodal document alignment: towards a fully-indexed multimedia archive",
      "authors": [
        "Dalila Mekhaldi"
      ],
      "year": "2007",
      "venue": "Proceedings of the Multimedia Information Retrieval Workshop, SIGIR"
    },
    {
      "citation_id": "417",
      "title": "Training for diversity in image paragraph captioning",
      "authors": [
        "Luke Melas-Kyriazi",
        "Alexander Rush",
        "George Han"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "418",
      "title": "Coco-lm: Correcting and contrasting text sequences for language model pretraining",
      "authors": [
        "Yu Meng",
        "Chenyan Xiong",
        "Payal Bajaj",
        "Paul Bennett",
        "Jiawei Han",
        "Xia Song"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "419",
      "title": "The explanation game: Explaining machine learning models using shapley values",
      "authors": [
        "Luke Merrick",
        "Ankur Taly"
      ],
      "year": "2020",
      "venue": "International Cross-Domain Conference for Machine Learning and Knowledge Extraction"
    },
    {
      "citation_id": "420",
      "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
      "authors": [
        "Ben Mildenhall",
        "P Pratul",
        "Matthew Srinivasan",
        "Jonathan Tancik",
        "Ravi Barron",
        "Ren Ramamoorthi",
        "Ng"
      ],
      "year": "2020",
      "venue": "Nerf: Representing scenes as neural radiance fields for view synthesis"
    },
    {
      "citation_id": "421",
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "authors": [
        "George Miller"
      ],
      "year": "1956",
      "venue": "The magical number seven, plus or minus two: Some limits on our capacity for processing information"
    },
    {
      "citation_id": "422",
      "title": "Wordnet: a lexical database for english",
      "authors": [
        "George Miller"
      ],
      "year": "1995",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "423",
      "title": "Being a supercook: Joint food attributes and multimodal content modeling for recipe retrieval and exploration",
      "authors": [
        "Weiqing Min",
        "Shuqiang Jiang",
        "Jitao Sang",
        "Huayang Wang",
        "Xinda Liu",
        "Luis Herranz"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "424",
      "title": "Character-based bidirectional lstm-crf with words and characters for japanese named entity recognition",
      "authors": [
        "Shotaro Misawa",
        "Motoki Taniguchi",
        "Yasuhide Miura",
        "Tomoko Ohkuma"
      ],
      "year": "2017",
      "venue": "Proceedings of the First Workshop on Subword and Character Level Models in NLP"
    },
    {
      "citation_id": "425",
      "title": "Features for content-based audio retrieval",
      "authors": [
        "Dalibor Mitrović",
        "Matthias Zeppelzauer",
        "Christian Breiteneder"
      ],
      "year": "2010",
      "venue": "Advances in computers"
    },
    {
      "citation_id": "426",
      "title": "Towards large-scale multisensory learning for the internet of things",
      "authors": [
        "Shentong Mo",
        "Paul Liang",
        "Russ Salakhutdinov",
        "Louis-Philippe Morency",
        "Multiiot"
      ],
      "year": "2023",
      "venue": "Towards large-scale multisensory learning for the internet of things",
      "arxiv": "arXiv:2311.06217"
    },
    {
      "citation_id": "427",
      "title": "Predicting cancer outcomes from histology and genomics using convolutional networks",
      "authors": [
        "Pooya Mobadersany",
        "Safoora Yousefi",
        "Mohamed Amgad",
        "Jill David A Gutman",
        "José E Velázquez Barnholtz-Sloan",
        "Daniel Vega",
        "Lee Ad Brat",
        "Cooper"
      ],
      "year": "2018",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "428",
      "title": "Agnostic federated learning",
      "authors": [
        "Mehryar Mohri",
        "Gary Sivek",
        "Ananda Theertha"
      ],
      "year": "2019",
      "venue": "Agnostic federated learning"
    },
    {
      "citation_id": "429",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "430",
      "title": "Co-regularized deep representations for video summarization",
      "authors": [
        "Olivier Morere",
        "Hanlin Goh",
        "Antoine Veillard",
        "Vijay Chandrasekhar",
        "Jie Lin"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "431",
      "title": "A comprehensive survey on multimodal medical signals fusion for smart healthcare systems",
      "authors": [
        "Ghulam Muhammad",
        "Fatima Alshehri",
        "Fakhri Karray",
        "Abdulmotaleb Saddik"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "432",
      "title": "Ccmi: Classifier based conditional mutual information estimation",
      "authors": [
        "Sudipto Mukherjee",
        "Himanshu Asnani",
        "Sreeram Kannan"
      ],
      "year": "2020",
      "venue": "Uncertainty in artificial intelligence"
    },
    {
      "citation_id": "433",
      "title": "Multi-modal domain adaptation for fine-grained action recognition",
      "authors": [
        "Jonathan Munro",
        "Dima Damen"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "434",
      "title": "Measuring stereotypical bias in pretrained language models",
      "authors": [
        "Moin Nadeem",
        "Anna Bethke",
        "Siva Reddy",
        "Stereoset"
      ],
      "year": "2020",
      "venue": "Measuring stereotypical bias in pretrained language models",
      "arxiv": "arXiv:2004.09456"
    },
    {
      "citation_id": "435",
      "title": "Just-in-time adaptive interventions (jitais) in mobile health: key components and design principles for ongoing health behavior support",
      "authors": [
        "Inbal Nahum-Shani",
        "Shawna Smith",
        "Bonnie Spring",
        "Linda Collins",
        "Katie Witkiewitz",
        "Ambuj Tewari",
        "Susan Murphy"
      ],
      "year": "2018",
      "venue": "Annals of Behavioral Medicine"
    },
    {
      "citation_id": "436",
      "title": "Dual attention networks for multimodal reasoning and matching",
      "authors": [
        "Hyeonseob Nam",
        "Jung-Woo Ha",
        "Jeonghee Kim"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "437",
      "title": "Multimodal mental imagery",
      "authors": [
        "Bence Nanay"
      ],
      "year": "2018",
      "venue": "Cortex"
    },
    {
      "citation_id": "438",
      "title": "Large-scale concept ontology for multimedia",
      "authors": [
        "Milind Naphade",
        "John Smith",
        "Jelena Tesic",
        "Shih-Fu Chang",
        "Winston Hsu",
        "Lyndon Kennedy",
        "Alexander Hauptmann",
        "Jon Curtis"
      ],
      "year": "2006",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "439",
      "title": "Grounding language for transfer in deep reinforcement learning",
      "authors": [
        "Karthik Narasimhan",
        "Regina Barzilay",
        "Tommi Jaakkola"
      ],
      "year": "2018",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "440",
      "title": "Text and code embeddings by contrastive pre-training",
      "authors": [
        "Arvind Neelakantan",
        "Tao Xu",
        "Raul Puri",
        "Alec Radford",
        "Jesse Michael Han",
        "Jerry Tworek",
        "Qiming Yuan",
        "Nikolas Tezak",
        "Jong Kim",
        "Chris Hallacy"
      ],
      "year": "2022",
      "venue": "Text and code embeddings by contrastive pre-training",
      "arxiv": "arXiv:2201.10005"
    },
    {
      "citation_id": "441",
      "title": "A hybrid latent space data fusion method for multimodal emotion recognition",
      "authors": [
        "Shahla Nemati",
        "Reza Rohani",
        "Mohammad Basiri",
        "Moloud Abdar",
        "Vladimir Neil Y Yen",
        "Makarenkov"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "442",
      "title": "Cancer Genome Atlas Research Network. Comprehensive, integrative genomic analysis of diffuse lower-grade gliomas",
      "year": "2015",
      "venue": "New England Journal of Medicine"
    },
    {
      "citation_id": "443",
      "title": "Learning to listen: Modeling non-deterministic dyadic facial motion",
      "authors": [
        "Evonne Ng",
        "Hanbyul Joo",
        "Liwen Hu",
        "Hao Li",
        "Trevor Darrell",
        "Angjoo Kanazawa",
        "Shiry Ginosar"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "444",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "445",
      "title": "Multiview learning for understanding functional multiomics",
      "authors": [
        "D Nam",
        "Daifeng Nguyen",
        "Wang"
      ],
      "year": "2020",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "446",
      "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization",
      "authors": [
        "Xuanlong Nguyen",
        "Martin Wainwright",
        "Michael Jordan"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "447",
      "title": "A review of relational machine learning for knowledge graphs",
      "authors": [
        "Maximilian Nickel",
        "Kevin Murphy",
        "Evgeniy Volker Tresp",
        "Gabrilovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "448",
      "title": "A performance evaluation of federated learning algorithms",
      "authors": [
        "Adrian Nilsson",
        "Simon Smith",
        "Gregor Ulm",
        "Emil Gustavsson",
        "Mats Jirstrand"
      ],
      "year": "2018",
      "venue": "DIDL"
    },
    {
      "citation_id": "449",
      "title": "Retinal ganglion cells act largely as independent encoders",
      "authors": [
        "Sheila Nirenberg",
        "Steve Carcieri",
        "Adam Jacobs",
        "Peter Latham"
      ],
      "year": "2001",
      "venue": "Nature"
    },
    {
      "citation_id": "450",
      "title": "Counterfactual vqa: A cause-effect look at language bias",
      "authors": [
        "Yulei Niu",
        "Kaihua Tang",
        "Hanwang Zhang",
        "Zhiwu Lu",
        "Xian-Sheng Hua",
        "Ji-Rong Wen"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "451",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltrušaitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ICMI 2016",
      "doi": "10.1145/2993148.2993176"
    },
    {
      "citation_id": "452",
      "title": "Modeling multimodal human-computer interaction",
      "authors": [
        "Zeljko Obrenovic",
        "Dusan Starcevic"
      ],
      "year": "2004",
      "venue": "Computer"
    },
    {
      "citation_id": "453",
      "title": "Conic optimization via operator splitting and homogeneous self-dual embedding",
      "authors": [
        "O' Brendan",
        "Eric Donoghue",
        "Neal Chu",
        "Stephen Parikh",
        "Boyd"
      ],
      "year": "2016",
      "venue": "Journal of Optimization Theory and Applications"
    },
    {
      "citation_id": "454",
      "title": "Parallel wavenet: Fast high-fidelity speech synthesis",
      "authors": [
        "Aaron Oord",
        "Yazhe Li",
        "Igor Babuschkin",
        "Karen Simonyan",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "ICML"
    },
    {
      "citation_id": "455",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "456",
      "title": "OpenAI. Gpt-4 technical report",
      "year": "2023",
      "venue": "OpenAI. Gpt-4 technical report"
    },
    {
      "citation_id": "457",
      "title": "Investigating user perception of gender bias in image search: the role of sexism",
      "authors": [
        "Jahna Otterbacher",
        "Alessandro Checco",
        "Gianluca Demartini",
        "Paul Clough"
      ],
      "year": "2018",
      "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval"
    },
    {
      "citation_id": "458",
      "title": "Characterization and classification of semantic image-text relations",
      "authors": [
        "Christian Otto",
        "Matthias Springstein",
        "Avishek Anand",
        "Ralph Ewerth"
      ],
      "year": "2020",
      "venue": "International Journal of Multimedia Information Retrieval"
    },
    {
      "citation_id": "459",
      "title": "Ten myths of multimodal interaction",
      "authors": [
        "Sharon Oviatt"
      ],
      "year": "1999",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "460",
      "title": "Wasserstein dependency measure for representation learning",
      "authors": [
        "Sherjil Ozair",
        "Corey Lynch",
        "Yoshua Bengio",
        "Aaron Van Den Oord",
        "Sergey Levine",
        "Pierre Sermanet"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "461",
      "title": "Multisensory interaction: Real and virtual",
      "authors": [
        "K Dinesh",
        "Pai"
      ],
      "year": "2005",
      "venue": "Robotics Research. The Eleventh International Symposium"
    },
    {
      "citation_id": "462",
      "title": "Estimating the unique information of continuous variables",
      "authors": [
        "Ari Pakman",
        "Amin Nejatbakhsh",
        "Dar Gilboa",
        "Abdullah Makkeh",
        "Luca Mazzucato",
        "Michael Wibral",
        "Elad Schneidman"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "463",
      "title": "Multimodal abstractive summarization for how2 videos",
      "authors": [
        "Shruti Palaskar",
        "Jindrich Libovickỳ",
        "Spandana Gella",
        "Florian Metze"
      ],
      "year": "2019",
      "venue": "Multimodal abstractive summarization for how2 videos",
      "arxiv": "arXiv:1906.07901"
    },
    {
      "citation_id": "464",
      "title": "Cross-modal attention congruence regularization for vision-language relation alignment",
      "authors": [
        "Rohan Pandey",
        "Rulin Shao",
        "Paul Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "465",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "Maja Pantic"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "466",
      "title": "Seeing past words: Testing the cross-modal capabilities of pretrained v&l models on counting tasks",
      "authors": [
        "Letitia Parcalabescu",
        "Albert Gatt",
        "Anette Frank",
        "Iacer Calixto"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)"
    },
    {
      "citation_id": "467",
      "title": "A decomposable attention model for natural language inference",
      "authors": [
        "P Ankur",
        "Oscar Parikh",
        "Dipanjan Täckström",
        "Jakob Das",
        "Uszkoreit"
      ],
      "year": "2016",
      "venue": "A decomposable attention model for natural language inference",
      "arxiv": "arXiv:1606.01933"
    },
    {
      "citation_id": "468",
      "title": "Multimodal explanations: Justifying decisions and pointing to the evidence",
      "authors": [
        "Dong Huk",
        "Lisa Hendricks",
        "Zeynep Akata",
        "Anna Rohrbach",
        "Bernt Schiele",
        "Trevor Darrell",
        "Marcus Rohrbach"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "469",
      "title": "Visualcomet: Reasoning about the dynamic context of a still image",
      "authors": [
        "Jae Sung Park",
        "Chandra Bhagavatula",
        "Roozbeh Mottaghi",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "470",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "Sunghyun Park",
        "Suk Han",
        "Moitreya Shim",
        "Kenji Chatterjee",
        "Louis-Philippe Sagae",
        "Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction, ICMI '14",
      "doi": "10.1145/2663204.2663260"
    },
    {
      "citation_id": "471",
      "title": "Communication goes multimodal",
      "authors": [
        "Sarah Partan",
        "Peter Marler"
      ],
      "year": "1999",
      "venue": "Science"
    },
    {
      "citation_id": "472",
      "title": "Issues in the classification of multimodal communication signals",
      "authors": [
        "Peter Sarah R Partan",
        "Marler"
      ],
      "year": "2005",
      "venue": "The American Naturalist"
    },
    {
      "citation_id": "473",
      "title": "Modelling multimodal expression of emotion in a virtual agent",
      "authors": [
        "Catherine Pelachaud"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "474",
      "title": "Multimodal behavior modeling for socially interactive agents",
      "authors": [
        "Catherine Pelachaud",
        "Carlos Busso",
        "Dirk Heylen"
      ],
      "year": "2021",
      "venue": "The Handbook on Socially Interactive Agents"
    },
    {
      "citation_id": "475",
      "title": "Faircvtest demo: Understanding bias in multimodal learning with a testbed in fair automatic recruitment",
      "authors": [
        "Alejandro Peña",
        "Ignacio Serna",
        "Aythami Morales",
        "Julian Fierrez"
      ],
      "year": "2020",
      "venue": "ICMI"
    },
    {
      "citation_id": "476",
      "title": "Kosmos-2: Grounding multimodal large language models to the world",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding multimodal large language models to the world",
      "arxiv": "arXiv:2306.14824"
    },
    {
      "citation_id": "477",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "478",
      "title": "Film: Visual reasoning with a general conditioning layer",
      "authors": [
        "Ethan Perez",
        "Florian Strub",
        "Vincent Harm De Vries",
        "Aaron Dumoulin",
        "Courville"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "479",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "Verónica Pérez-Rosas",
        "Rada Mihalcea",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "ACL (1)"
    },
    {
      "citation_id": "480",
      "title": "Mfas: Multimodal fusion architecture search",
      "authors": [
        "Juan-Manuel Pérez-Rúa",
        "Valentin Vielzeuf",
        "Stéphane Pateux",
        "Moez Baccouche",
        "Frédéric Jurie"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "481",
      "title": "Deep contextualized word representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1202"
    },
    {
      "citation_id": "482",
      "title": "Embedding multimodal relational data for knowledge base completion",
      "authors": [
        "Pouya Pezeshkpour",
        "Liyan Chen",
        "Sameer Singh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "483",
      "title": "Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "authors": [
        "Hai Pham",
        "Thomas Manzini",
        "Paul Liang",
        "Barnabas Poczos"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "484",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "485",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "486",
      "title": "Esc: Dataset for environmental sound classification",
      "authors": [
        "J Karol",
        "Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    },
    {
      "citation_id": "487",
      "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models",
      "authors": [
        "Liwei Bryan A Plummer",
        "Chris Wang",
        "Juan Cervantes",
        "Julia Caicedo",
        "Svetlana Hockenmaier",
        "Lazebnik"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "488",
      "title": "Geometric multimodal contrastive representation learning",
      "authors": [
        "Petra Poklukar",
        "Miguel Vasco",
        "Hang Yin",
        "Ana Francisco S Melo",
        "Danica Paiva",
        "Kragic"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "489",
      "title": "On variational bounds of mutual information",
      "authors": [
        "Ben Poole",
        "Sherjil Ozair",
        "Aaron Van Den",
        "Alex Oord",
        "George Alemi",
        "Tucker"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "490",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "Soujanya Poria",
        "Iti Chaturvedi",
        "Erik Cambria",
        "Amir Hussain"
      ],
      "year": "2016",
      "venue": "Convolutional mkl based multimodal emotion recognition and sentiment analysis"
    },
    {
      "citation_id": "491",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "492",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Mazumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Context-dependent sentiment analysis in user-generated videos"
    },
    {
      "citation_id": "493",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "Shraman Pramanick",
        "Aniket Roy",
        "M Vishal",
        "Patel"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "494",
      "title": "Personalized medicine: Prediction of disease vulnerability in mood disorders",
      "authors": [
        "Stefania Prendes",
        "- Alvarez",
        "Charles Nemeroff"
      ],
      "year": "2018",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "495",
      "title": "Synergistic information supports modality integration and flexible learning in neural networks solving multiple tasks",
      "authors": [
        "Alexandra Proca",
        "Fernando Rosas",
        "Andrea Luppi",
        "Daniel Bor",
        "Matthew Crosby",
        "Pedro Mediano"
      ],
      "year": "2022",
      "venue": "Synergistic information supports modality integration and flexible learning in neural networks solving multiple tasks",
      "arxiv": "arXiv:2210.02996"
    },
    {
      "citation_id": "496",
      "title": "Hidden conditional random fields",
      "authors": [
        "Ariadna Quattoni",
        "Sybor Wang",
        "Louis-Philippe Morency",
        "Michael Collins",
        "Trevor Darrell"
      ],
      "year": "2007",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2007.1124"
    },
    {
      "citation_id": "497",
      "title": "Multimodal distillation for egocentric action recognition",
      "authors": [
        "Gorjan Radevski",
        "Dusan Grujicic",
        "Matthew Blaschko",
        "Marie-Francine Moens",
        "Tinne Tuytelaars"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "498",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeff Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners"
    },
    {
      "citation_id": "499",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "500",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "JMLR"
    },
    {
      "citation_id": "501",
      "title": "Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions",
      "authors": [
        "Anil Rahate",
        "Rahee Walambe",
        "Sheela Ramanna",
        "Ketan Kotecha"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "502",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amirali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "503",
      "title": "Extending long short-term memory for multi-view structured learning",
      "authors": [
        "Shyam Sundar Rajagopalan",
        "Louis-Philippe Morency",
        "Tadas Baltrušaitis",
        "Roland Goecke"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "504",
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "Aditya Ramesh",
        "Mikhail Pavlov",
        "Gabriel Goh",
        "Scott Gray",
        "Chelsea Voss",
        "Alec Radford",
        "Mark Chen",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "505",
      "title": "Integrated genomic characterization of pancreatic ductal adenocarcinoma",
      "authors": [
        "J Benjamin",
        "Ralph Raphael",
        "Andrew Hruban",
        "Richard Aguirre",
        "Jen Moffitt",
        "Jen Yeh",
        "Chip Stewart",
        "A Gordon Robertson",
        "Andrew Cherniack",
        "Manaswi Gupta",
        "Gad Getz"
      ],
      "year": "2017",
      "venue": "Cancer cell"
    },
    {
      "citation_id": "506",
      "title": "A new approach to cross-modal multimedia retrieval",
      "authors": [
        "Nikhil Rasiwasia",
        "Jose Pereira",
        "Emanuele Coviello",
        "Gabriel Doyle",
        "Gert Lanckriet",
        "Roger Levy",
        "Nuno Vasconcelos"
      ],
      "year": "2010",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "507",
      "title": "One model to learn them all",
      "authors": [
        "Scott Reed",
        "Konrad Zolna",
        "Emilio Parisotto",
        "Sergio Colmenarejo",
        "Alexander Novikov",
        "Gabriel Barth-Maron",
        "Mai Giménez",
        "Yury Sulsky"
      ],
      "year": "2022",
      "venue": "One model to learn them all"
    },
    {
      "citation_id": "508",
      "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "authors": [
        "Kaiming Shaoqing Ren",
        "Ross He",
        "Jian Girshick",
        "Sun"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "509",
      "title": "Fastspeech: Fast, robust and controllable text to speech. NeurIPS",
      "authors": [
        "Yi Ren",
        "Yangjun Ruan",
        "Xu Tan",
        "Tao Qin",
        "Sheng Zhao",
        "Zhou Zhao",
        "Tie-Yan Liu"
      ],
      "year": "2019",
      "venue": "Fastspeech: Fast, robust and controllable text to speech. NeurIPS"
    },
    {
      "citation_id": "510",
      "title": "explaining the predictions of any classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "511",
      "title": "why should i trust you?\" explaining the predictions of any classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "KDD"
    },
    {
      "citation_id": "512",
      "title": "Mathematical statistics and data analysis",
      "authors": [
        "John A Rice"
      ],
      "year": "2006",
      "venue": "Mathematical statistics and data analysis"
    },
    {
      "citation_id": "513",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "Robin Rombach",
        "Andreas Blattmann",
        "Dominik Lorenz",
        "Patrick Esser",
        "Björn Ommer"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "514",
      "title": "Measuring social biases in grounded vision and language embeddings",
      "authors": [
        "Candace Ross",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "year": "2020",
      "venue": "Measuring social biases in grounded vision and language embeddings",
      "arxiv": "arXiv:2002.08911"
    },
    {
      "citation_id": "515",
      "title": "Image retrieval: Current techniques, promising directions, and open issues",
      "authors": [
        "Yong Rui",
        "Thomas Huang",
        "Shih-Fu Chang"
      ],
      "year": "1999",
      "venue": "Journal of visual communication and image representation"
    },
    {
      "citation_id": "516",
      "title": "Examining the redundancy of multimodal input",
      "authors": [
        "Natalie Ruiz",
        "Ronnie Taib",
        "Fang Chen"
      ],
      "year": "2006",
      "venue": "Proceedings of the 18th Australia conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments"
    },
    {
      "citation_id": "517",
      "title": "Semantic text summarization of long videos",
      "authors": [
        "Shagan Sah",
        "Sourabh Kulhare",
        "Allison Gray",
        "Subhashini Venugopalan",
        "Emily Prud'hommeaux",
        "Raymond Ptucha"
      ],
      "year": "2017",
      "venue": "WACV"
    },
    {
      "citation_id": "518",
      "title": "Winogrande: An adversarial winograd schema challenge at scale",
      "authors": [
        "Keisuke Sakaguchi",
        "Le Ronan",
        "Chandra Bras",
        "Yejin Bhagavatula",
        "Choi"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "519",
      "title": "Multimodal fusion refiner networks",
      "authors": [
        "Sethuraman Sankaran",
        "David Yang",
        "Ser-Nam Lim"
      ],
      "year": "2021",
      "venue": "Multimodal fusion refiner networks",
      "arxiv": "arXiv:2104.03435"
    },
    {
      "citation_id": "520",
      "title": "Social bias frames: Reasoning about social and power implications of language",
      "authors": [
        "Maarten Sap",
        "Saadia Gabriel",
        "Lianhui Qin",
        "Dan Jurafsky",
        "Noah Smith",
        "Yejin Choi"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "521",
      "title": "Multimodal graph networks for compositional generalization in visual question answering",
      "authors": [
        "Raeid Saqur",
        "Karthik Narasimhan"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "522",
      "title": "Multimodal deep learning for short-term stock volatility prediction",
      "authors": [
        "Marcelo Sardelich",
        "Suresh Manandhar"
      ],
      "year": "2018",
      "venue": "Multimodal deep learning for short-term stock volatility prediction",
      "arxiv": "arXiv:1812.10479"
    },
    {
      "citation_id": "523",
      "title": "Audiovisual synchronization and fusion using canonical correlation analysis",
      "authors": [
        "Yücel Mehmet Emre Sargin",
        "Engin Yemez",
        "A Erzin",
        "Tekalp"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "524",
      "title": "Habitat: A platform for embodied ai research",
      "authors": [
        "Manolis Savva",
        "Abhishek Kadian",
        "Oleksandr Maksymets",
        "Yili Zhao",
        "Erik Wijmans",
        "Bhavana Jain",
        "Julian Straub",
        "Jia Liu",
        "Vladlen Koltun",
        "Jitendra Malik"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "525",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori",
        "Ah Chung Tsoi",
        "Markus Hagenbuchner",
        "Gabriele Monfardini"
      ],
      "year": "2008",
      "venue": "The graph neural network model"
    },
    {
      "citation_id": "526",
      "title": "Predictable robots for autistic children-variance in robot behaviour, idiosyncrasies in autistic children's characteristics, and child-robot engagement",
      "authors": [
        "Dennis Bob R Schadenberg",
        "Vanessa Reidsma",
        "Evers",
        "Jamy Daniel P Davison",
        "Dirk Li",
        "Carlos Heylen",
        "Paulo Neves",
        "Jie Alvito",
        "Maja Shen",
        "Pantić"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Computer-Human Interaction (TOCHI)"
    },
    {
      "citation_id": "527",
      "title": "Multimodal graph-based event detection and summarization in social media streams",
      "authors": [
        "Manos Schinas",
        "Symeon Papadopoulos",
        "Georgios Petkos",
        "Yiannis Kompatsiaris",
        "Pericles Mitkas"
      ],
      "year": "2015",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "528",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "ICMI"
    },
    {
      "citation_id": "529",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "530",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "Trans. Sig. Proc",
      "doi": "10.1109/78.650093"
    },
    {
      "citation_id": "531",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "ICCV"
    },
    {
      "citation_id": "532",
      "title": "Active haptic perception in robots: a review",
      "authors": [
        "Lucia Seminara",
        "Paolo Gastaldo",
        "Simon Watt",
        "Kenneth Valyear",
        "Fernando Zuher",
        "Fulvio Mastrogiovanni"
      ],
      "year": "2019",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "533",
      "title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge",
      "authors": [
        "Luciano Serafini",
        "Artur D'avila Garcez"
      ],
      "year": "2016",
      "venue": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge",
      "arxiv": "arXiv:1606.04422"
    },
    {
      "citation_id": "534",
      "title": "Multimodal analysis of user-generated multimedia content",
      "authors": [
        "Rajiv Shah",
        "Roger Zimmermann"
      ],
      "year": "2017",
      "venue": "Multimodal analysis of user-generated multimedia content"
    },
    {
      "citation_id": "535",
      "title": "Exploring artist gender bias in music recommendation",
      "authors": [
        "Dougal Shakespeare",
        "Lorenzo Porcaro",
        "Emilia Gómez",
        "Carlos Castillo"
      ],
      "year": "2020",
      "venue": "Exploring artist gender bias in music recommendation",
      "arxiv": "arXiv:2009.01715"
    },
    {
      "citation_id": "536",
      "title": "Ernie-vil 2.0: Multi-view contrastive learning for image-text pre-training",
      "authors": [
        "Bin Shan",
        "Weichong Yin",
        "Yu Sun",
        "Hua Hao Tian",
        "Haifeng Wu",
        "Wang"
      ],
      "year": "2022",
      "venue": "Ernie-vil 2.0: Multi-view contrastive learning for image-text pre-training"
    },
    {
      "citation_id": "537",
      "title": "A mathematical theory of communication. The Bell system technical journal",
      "authors": [
        "Claude Elwood"
      ],
      "year": "1948",
      "venue": "A mathematical theory of communication. The Bell system technical journal"
    },
    {
      "citation_id": "538",
      "title": "Vision to language: Methods, metrics and datasets",
      "authors": [
        "Naeha Sharif",
        "Uzair Nadeem",
        "Syed Afaq",
        "Ali Shah",
        "Mohammed Bennamoun",
        "Wei Liu"
      ],
      "year": "2020",
      "venue": "Machine Learning Paradigms"
    },
    {
      "citation_id": "539",
      "title": "Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor! In SemEval",
      "authors": [
        "Chhavi Sharma",
        "William Paka",
        "Deepesh Scott",
        "Amitava Bhageria",
        "Soujanya Das",
        "Poria"
      ],
      "year": "2020",
      "venue": "Task Report: Memotion Analysis 1.0 @SemEval 2020: The Visuo-Lingual Metaphor! In SemEval"
    },
    {
      "citation_id": "540",
      "title": "Toward multimodal human-computer interface",
      "authors": [
        "Rajeev Sharma",
        "Vladimir Pavlović",
        "Thomas Huang"
      ],
      "year": "2002",
      "venue": "Advances in image processing and understanding: A Festschrift for Thomas S Huang"
    },
    {
      "citation_id": "541",
      "title": "Toward practical privacy-preserving analytics for iot and cloud-based healthcare systems",
      "authors": [
        "Sagar Sharma",
        "Keke Chen",
        "Amit Sheth"
      ],
      "year": "2018",
      "venue": "IEEE Internet Computing"
    },
    {
      "citation_id": "542",
      "title": "Self-attention with relative position representations",
      "authors": [
        "Peter Shaw",
        "Jakob Uszkoreit",
        "Ashish Vaswani"
      ],
      "year": "2018",
      "venue": "Self-attention with relative position representations"
    },
    {
      "citation_id": "543",
      "title": "Sentiment analysis using imperfect views from spoken language and acoustic modalities",
      "authors": [
        "Imran Sheikh",
        "Sri Harsha Dumpala",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "544",
      "title": "The woman worked as a babysitter: On biases in language generation",
      "authors": [
        "Emily Sheng",
        "Kai-Wei Chang",
        "Prem Natarajan",
        "Nanyun Peng"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "545",
      "title": "A survey of heterogeneous information network analysis",
      "authors": [
        "Chuan Shi",
        "Yitong Li",
        "Jiawei Zhang",
        "Yizhou Sun",
        "S Yu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "546",
      "title": "Variational mixture-of-experts autoencoders for multimodal deep generative models",
      "authors": [
        "Yuge Shi",
        "Brooks Paige",
        "Philip Torr"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "547",
      "title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
      "authors": [
        "Mohit Shridhar",
        "Jesse Thomason",
        "Daniel Gordon",
        "Yonatan Bisk",
        "Winson Han",
        "Roozbeh Mottaghi",
        "Luke Zettlemoyer",
        "Dieter Fox"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "548",
      "title": "Tabular data: Deep learning is not all you need",
      "authors": [
        "Ravid Shwartz",
        "Amitai Armon"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "549",
      "title": "To compress or not to compress-self-supervised learning and information theory: A review",
      "authors": [
        "Ravid Shwartz",
        "Yann Lecun"
      ],
      "year": "2023",
      "venue": "To compress or not to compress-self-supervised learning and information theory: A review",
      "arxiv": "arXiv:2304.09355"
    },
    {
      "citation_id": "550",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2013",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "arxiv": "arXiv:1312.6034"
    },
    {
      "citation_id": "551",
      "title": "A co-regularization approach to semi-supervised learning with multiple views",
      "authors": [
        "Vikas Sindhwani",
        "Partha Niyogi",
        "Mikhail Belkin"
      ],
      "year": "2005",
      "venue": "Proceedings of ICML workshop on learning with multiple views"
    },
    {
      "citation_id": "552",
      "title": "Make-a-video: Text-to-video generation without text-video data",
      "authors": [
        "Uriel Singer",
        "Adam Polyak",
        "Thomas Hayes",
        "Xi Yin",
        "Jie An",
        "Songyang Zhang",
        "Qiyuan Hu"
      ],
      "year": "2022",
      "venue": "Make-a-video: Text-to-video generation without text-video data",
      "arxiv": "arXiv:2209.14792"
    },
    {
      "citation_id": "553",
      "title": "Flava: A foundational language and vision alignment model",
      "authors": [
        "Amanpreet Singh",
        "Ronghang Hu",
        "Vedanuj Goswami",
        "Guillaume Couairon",
        "Wojciech Galuba",
        "Marcus Rohrbach",
        "Douwe Kiela"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "554",
      "title": "Federated multi-task learning",
      "authors": [
        "Virginia Smith",
        "Chao-Kai Chiang",
        "Maziar Sanjabi",
        "Ameet Talwalkar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17"
    },
    {
      "citation_id": "555",
      "title": "Zero-shot learning through cross-modal transfer",
      "authors": [
        "Richard Socher",
        "Milind Ganjoo",
        "Christopher Manning",
        "Andrew Ng"
      ],
      "year": "2013",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "556",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "Richard Socher",
        "Alex Perelygin",
        "Jean Wu",
        "Jason Chuang",
        "Christopher Manning",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "557",
      "title": "Improved multimodal deep learning with variation of information",
      "authors": [
        "Kihyuk Sohn",
        "Wenling Shang",
        "Honglak Lee"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "558",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "Mohammad Soleymani",
        "David Garcia",
        "Brendan Jou",
        "Björn Schuller",
        "Shih-Fu Chang",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "559",
      "title": "Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition",
      "authors": [
        "Hagen Soltau",
        "Hank Liao",
        "Hasim Sak"
      ],
      "year": "2016",
      "venue": "Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition"
    },
    {
      "citation_id": "560",
      "title": "Kvl-bert: Knowledge enhanced visual-and-linguistic bert for visual commonsense reasoning",
      "authors": [
        "Dandan Song",
        "Siyi Ma",
        "Zhanchen Sun",
        "Sicheng Yang",
        "Lejian Liao"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "561",
      "title": "Multi-view latent variable discriminative models for action recognition",
      "authors": [
        "Yale Song",
        "Louis-Philippe Morency",
        "Randall Davis"
      ],
      "year": "2012",
      "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on"
    },
    {
      "citation_id": "562",
      "title": "Action recognition by hierarchical sequence summarization",
      "authors": [
        "Yale Song",
        "Louis-Philippe Morency",
        "Randall Davis"
      ],
      "year": "2013",
      "venue": "CVPR"
    },
    {
      "citation_id": "563",
      "title": "Decomposed mutual information estimation for contrastive representation learning",
      "authors": [
        "Alessandro Sordoni",
        "Nouha Dziri",
        "Hannes Schulz",
        "Geoff Gordon",
        "Philip Bachman",
        "Remi Tachet",
        "Des Combes"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "564",
      "title": "Detecting statistical interactions with additive groves of trees",
      "authors": [
        "Daria Sorokina",
        "Rich Caruana",
        "Mirek Riedewald",
        "Daniel Fink"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "565",
      "title": "An information theoretic framework for multi-view learning",
      "authors": [
        "Karthik Sridharan",
        "M Sham",
        "Kakade"
      ],
      "year": "2008",
      "venue": "An information theoretic framework for multi-view learning"
    },
    {
      "citation_id": "566",
      "title": "Worst of both worlds: Biases compound in pre-trained vision-and-language models",
      "authors": [
        "Tejas Srinivasan",
        "Yonatan Bisk"
      ],
      "year": "2021",
      "venue": "Worst of both worlds: Biases compound in pre-trained vision-and-language models",
      "arxiv": "arXiv:2104.08666"
    },
    {
      "citation_id": "567",
      "title": "Multimodal learning with deep boltzmann machines",
      "authors": [
        "Nitish Srivastava",
        "Russ Salakhutdinov"
      ],
      "year": "2012",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "568",
      "title": "Which tasks should be learned together in multi-task learning",
      "authors": [
        "Trevor Standley",
        "Amir Zamir",
        "Dawn Chen",
        "Leonidas Guibas",
        "Jitendra Malik",
        "Silvio Savarese"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "569",
      "title": "Absolute identification by relative judgment. Psychological review",
      "authors": [
        "Neil Stewart",
        "D Gordon",
        "Nick Brown",
        "Chater"
      ],
      "year": "2005",
      "venue": "Absolute identification by relative judgment. Psychological review"
    },
    {
      "citation_id": "570",
      "title": "Linguistically-informed self-attention for semantic role labeling",
      "authors": [
        "Emma Strubell",
        "Patrick Verga",
        "Daniel Andor",
        "David Weiss",
        "Andrew Mccallum"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "571",
      "title": "Energy and policy considerations for deep learning in nlp",
      "authors": [
        "Emma Strubell",
        "Ananya Ganesh",
        "Andrew Mccallum"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "572",
      "title": "A molecular multimodal foundation model associating molecule graphs with natural language",
      "authors": [
        "Bing Su",
        "Dazhao Du",
        "Zhao Yang",
        "Yujie Zhou",
        "Jiangmeng Li",
        "Anyi Rao"
      ],
      "year": "2022",
      "venue": "A molecular multimodal foundation model associating molecule graphs with natural language",
      "arxiv": "arXiv:2209.05481"
    },
    {
      "citation_id": "573",
      "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
      "authors": [
        "Weijie Su",
        "Xizhou Zhu",
        "Yue Cao",
        "Bin Li",
        "Lewei Lu",
        "Furu Wei",
        "Jifeng Dai"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "574",
      "title": "Yago: a core of semantic knowledge",
      "authors": [
        "Gjergji Fabian M Suchanek",
        "Gerhard Kasneci",
        "Weikum"
      ],
      "year": "2007",
      "venue": "Yago: a core of semantic knowledge"
    },
    {
      "citation_id": "575",
      "title": "Nlvr2 visual bias analysis",
      "authors": [
        "Alane Suhr",
        "Yoav Artzi"
      ],
      "year": "2019",
      "venue": "Nlvr2 visual bias analysis",
      "arxiv": "arXiv:1909.10411"
    },
    {
      "citation_id": "576",
      "title": "Multimodal engagement analysis from facial videos in the classroom",
      "authors": [
        "Ömer Sümer",
        "Patricia Goldberg",
        "D' Sidney",
        "Peter Mello",
        "Ulrich Gerjets",
        "Enkelejda Trautwein",
        "Kasneci"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "577",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "Baochen Sun",
        "Jiashi Feng",
        "Kate Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "578",
      "title": "Videobert: A joint model for video and language representation learning",
      "authors": [
        "Chen Sun",
        "Austin Myers",
        "Carl Vondrick",
        "Kevin Murphy",
        "Cordelia Schmid"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "579",
      "title": "A survey of multi-view machine learning",
      "authors": [
        "Shiliang Sun"
      ],
      "year": "2013",
      "venue": "Neural computing and applications"
    },
    {
      "citation_id": "580",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "581",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "582",
      "title": "A short review of ethical challenges in clinical natural language processing",
      "authors": [
        "Simon Šuster",
        "Stéphan Tulkens",
        "Walter Daelemans"
      ],
      "year": "2017",
      "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. ACL"
    },
    {
      "citation_id": "583",
      "title": "Sequence to sequence learning with neural networks",
      "authors": [
        "Ilya Sutskever",
        "Oriol Vinyals",
        "V Quoc",
        "Le"
      ],
      "year": "2014",
      "venue": "Sequence to sequence learning with neural networks"
    },
    {
      "citation_id": "584",
      "title": "Reinforcement learning: An introduction",
      "authors": [
        "S Richard",
        "Andrew Sutton",
        "Barto"
      ],
      "year": "2018",
      "venue": "Reinforcement learning: An introduction"
    },
    {
      "citation_id": "585",
      "title": "Synthesizing obama: learning lip sync from audio",
      "authors": [
        "Supasorn Suwajanakorn",
        "Steven Seitz",
        "Ira Kemelmacher-Shlizerman"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Graphics (ToG)"
    },
    {
      "citation_id": "586",
      "title": "A survey of multimodal deep generative models",
      "authors": [
        "Masahiro Suzuki",
        "Yutaka Matsuo"
      ],
      "year": "2022",
      "venue": "Advanced Robotics"
    },
    {
      "citation_id": "587",
      "title": "Joint multimodal learning with deep generative models",
      "authors": [
        "Masahiro Suzuki",
        "Kotaro Nakayama",
        "Yutaka Matsuo"
      ],
      "year": "2016",
      "venue": "Joint multimodal learning with deep generative models",
      "arxiv": "arXiv:1611.01891"
    },
    {
      "citation_id": "588",
      "title": "Multimodal logical inference system for visual-textual entailment",
      "authors": [
        "Riko Suzuki",
        "Hitomi Yanaka",
        "Masashi Yoshikawa",
        "Koji Mineshima",
        "Daisuke Bekki"
      ],
      "year": "2019",
      "venue": "Multimodal logical inference system for visual-textual entailment",
      "arxiv": "arXiv:1906.03952"
    },
    {
      "citation_id": "589",
      "title": "Text2scene: Generating compositional scenes from textual descriptions",
      "authors": [
        "Fuwen Tan",
        "Song Feng",
        "Vicente Ordonez"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "590",
      "title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "591",
      "title": "Vokenization: Improving language understanding with contextualized, visualgrounded supervision",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "592",
      "title": "Why self-attention? a targeted evaluation of neural machine translation architectures",
      "authors": [
        "Gongbo Tang",
        "Mathias Müller",
        "Annette Rios",
        "Rico Sennrich"
      ],
      "year": "2018",
      "venue": "Why self-attention? a targeted evaluation of neural machine translation architectures",
      "arxiv": "arXiv:1808.08946"
    },
    {
      "citation_id": "593",
      "title": "World models and predictive coding for cognitive and developmental robotics: Frontiers and challenges",
      "authors": [
        "Tadahiro Taniguchi",
        "Shingo Murata",
        "Masahiro Suzuki",
        "Dimitri Ognibene",
        "Pablo Lanillos",
        "Emre Ugur",
        "Lorenzo Jamone",
        "Tomoaki Nakamura",
        "Alejandra Ciria",
        "Bruno Lara"
      ],
      "year": "2023",
      "venue": "World models and predictive coding for cognitive and developmental robotics: Frontiers and challenges",
      "arxiv": "arXiv:2301.05832"
    },
    {
      "citation_id": "594",
      "title": "Xiangnan He, Xianglin Huang, and Tat-Seng Chua. Mgat: Multimodal graph attention network for recommendation",
      "authors": [
        "Zhulin Tao",
        "Yinwei Wei",
        "Xiang Wang"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "595",
      "title": "Book2movie: Aligning video scenes with book chapters",
      "authors": [
        "Makarand Tapaswi",
        "Martin Bauml",
        "Rainer Stiefelhagen"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "596",
      "title": "Movieqa: Understanding stories in movies through question-answering",
      "authors": [
        "Makarand Tapaswi",
        "Yukun Zhu",
        "Rainer Stiefelhagen",
        "Antonio Torralba",
        "Raquel Urtasun",
        "Sanja Fidler"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "597",
      "title": "Long range arena : A benchmark for efficient transformers",
      "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Samira Abnar",
        "Yikang Shen",
        "Dara Bahri",
        "Philip Pham",
        "Jinfeng Rao",
        "Liu Yang",
        "Sebastian Ruder",
        "Donald Metzler"
      ],
      "year": "2021",
      "venue": "ICLR"
    },
    {
      "citation_id": "598",
      "title": "Multiple mutual informations and multiple interactions in frequency data",
      "authors": [
        "Han Te"
      ],
      "year": "1980",
      "venue": "Inf. Control"
    },
    {
      "citation_id": "599",
      "title": "The language interpretability tool: Extensible, interactive visualizations and analysis for nlp models",
      "authors": [
        "Ian Tenney",
        "James Wexler",
        "Jasmijn Bastings",
        "Tolga Bolukbasi",
        "Andy Coenen",
        "Sebastian Gehrmann",
        "Ellen Jiang",
        "Mahima Pushkarna",
        "Carey Radebaugh",
        "Emily Reif"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "600",
      "title": "Learning multi-modal grounded linguistic semantics by playing\" i spy",
      "authors": [
        "Jesse Thomason",
        "Jivko Sinapov",
        "Maxwell Svetlik",
        "Peter Stone",
        "Raymond Mooney"
      ],
      "year": "2016",
      "venue": "IJCAI"
    },
    {
      "citation_id": "601",
      "title": "Canonical correlation analysis",
      "authors": [
        "Bruce Thompson"
      ],
      "year": "2000",
      "venue": "Canonical correlation analysis"
    },
    {
      "citation_id": "602",
      "title": "Intelligence and its uses",
      "authors": [
        "Thorndike Edward"
      ],
      "year": "1920",
      "venue": "Harper's magazine"
    },
    {
      "citation_id": "603",
      "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
      "authors": [
        "Tristan Thrush",
        "Ryan Jiang",
        "Max Bartolo",
        "Amanpreet Singh",
        "Adina Williams",
        "Douwe Kiela",
        "Candace Ross"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "604",
      "title": "Audio-visual event localization in unconstrained videos",
      "authors": [
        "Yapeng Tian",
        "Jing Shi",
        "Bochen Li",
        "Zhiyao Duan",
        "Chenliang Xu"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "605",
      "title": "Unified multisensory perception: Weakly-supervised audiovisual video parsing",
      "authors": [
        "Yapeng Tian",
        "Dingzeyu Li",
        "Chenliang Xu"
      ],
      "year": "2020",
      "venue": "ECCV 2020"
    },
    {
      "citation_id": "606",
      "title": "Contrastive multiview coding",
      "authors": [
        "Yonglong Tian",
        "Dilip Krishnan",
        "Phillip Isola"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "607",
      "title": "What makes for good views for contrastive learning?",
      "authors": [
        "Yonglong Tian",
        "Chen Sun",
        "Ben Poole",
        "Dilip Krishnan",
        "Cordelia Schmid",
        "Phillip Isola"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "608",
      "title": "Mujoco: A physics engine for model-based control",
      "authors": [
        "Emanuel Todorov",
        "Tom Erez",
        "Yuval Tassa"
      ],
      "year": "2012",
      "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "609",
      "title": "The cancer genome atlas (tcga): an immeasurable source of knowledge",
      "authors": [
        "Katarzyna Tomczak",
        "Patrycja Czerwińska",
        "Maciej Wiznerowicz"
      ],
      "year": "2015",
      "venue": "Contemporary Oncology/Współczesna Onkologia"
    },
    {
      "citation_id": "610",
      "title": "Contrastive learning, multi-view redundancy, and linear models",
      "authors": [
        "Christopher Tosh",
        "Akshay Krishnamurthy",
        "Daniel Hsu"
      ],
      "year": "2021",
      "venue": "ALT"
    },
    {
      "citation_id": "611",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "Luan Tran",
        "Xiaoming Liu",
        "Jiayu Zhou",
        "Rong Jin"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "612",
      "title": "Openomics: A bioinformatics api to integrate multi-omics datasets and interface with public databases",
      "authors": [
        "C Nhat",
        "Jean Tran",
        "Gao"
      ],
      "year": "2021",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "613",
      "title": "Deep canonical time warping for simultaneous alignment and representation learning of sequences",
      "authors": [
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "614",
      "title": "Learning weakly-supervised contrastive representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Tianqin Li",
        "Weixin Liu",
        "Peiyuan Liao",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "615",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "616",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "617",
      "title": "Multimodal routing: Improving local and global interpretability of multimodal language analysis",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Martin Ma",
        "Muqiao Yang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "618",
      "title": "Self-supervised learning from a multi-view perspective",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Yue Wu",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "619",
      "title": "Neural methods for point-wise dependency estimation",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Han Zhao",
        "Makoto Yamada",
        "Louis-Philippe Morency",
        "Russ Salakhutdinov"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "620",
      "title": "Conditional contrastive learning with kernel",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Tianqin Li",
        "Martin Ma",
        "Han Zhao",
        "Kun Zhang",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2022",
      "venue": "Conditional contrastive learning with kernel",
      "arxiv": "arXiv:2202.05458"
    },
    {
      "citation_id": "621",
      "title": "Detecting statistical interactions from neural network weights",
      "authors": [
        "Michael Tsang",
        "Dehua Cheng",
        "Yan Liu"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "622",
      "title": "Feature interaction interpretability: A case for explaining ad-recommendation systems via neural interaction detection",
      "authors": [
        "Michael Tsang",
        "Dehua Cheng",
        "Hanpeng Liu",
        "Xue Feng",
        "Eric Zhou",
        "Yan Liu"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "623",
      "title": "On mutual information maximization for representation learning",
      "authors": [
        "Josip Michael Tschannen",
        "Djolonga",
        "Sylvain Paul K Rubenstein",
        "Mario Gelly",
        "Lucic"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "624",
      "title": "Multimodal few-shot learning with frozen language models",
      "authors": [
        "Maria Tsimpoukelli",
        "Jacob Menick",
        "Serkan Cabi",
        "Oriol Eslami",
        "Felix Vinyals",
        "Hill"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "625",
      "title": "On negative transfer: Effects of testing one list on the recall of another",
      "authors": [
        "Endel Tulving",
        "Michael Watkins"
      ],
      "year": "1974",
      "venue": "Journal of Verbal Learning and Verbal Behavior"
    },
    {
      "citation_id": "626",
      "title": "Multimodal interaction: A review",
      "authors": [
        "Matthew Turk"
      ],
      "year": "2014",
      "venue": "Pattern recognition letters"
    },
    {
      "citation_id": "627",
      "title": "Corpus-based learning of analogies and semantic relations",
      "authors": [
        "D Peter",
        "Turney",
        "Michael L Littman"
      ],
      "year": "2005",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "628",
      "title": "Multimodality and reading: The construction of meaning through image-text interaction",
      "authors": [
        "Len Unsworth",
        "Chris Cléirigh"
      ],
      "year": "2014",
      "venue": "Multimodality and reading: The construction of meaning through image-text interaction"
    },
    {
      "citation_id": "629",
      "title": "Multimodal research in vision and language: A review of current and emerging trends",
      "authors": [
        "Shagun Uppal",
        "Sarthak Bhagat",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "630",
      "title": "Multimodal summarization of complex sentences",
      "authors": [
        "Naushad Uzzaman",
        "Jeffrey Bigham",
        "James Allen"
      ],
      "year": "2011",
      "venue": "Proceedings of the 16th international conference on Intelligent user interfaces"
    },
    {
      "citation_id": "631",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aaron Van Den",
        "Oriol Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "632",
      "title": "Analyzing differentiable fuzzy logic operators",
      "authors": [
        "Emile Van Krieken",
        "Erman Acar",
        "Frank Van Harmelen"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "633",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "634",
      "title": "Probabilistic neural symbolic models for interpretable visual question answering",
      "authors": [
        "Ramakrishna Vedantam",
        "Karan Desai",
        "Stefan Lee",
        "Marcus Rohrbach",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "635",
      "title": "Graph attention networks",
      "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "ICLR"
    },
    {
      "citation_id": "636",
      "title": "Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language",
      "authors": [
        "Ivan Vendrov",
        "Ryan Kiros"
      ],
      "year": "2015",
      "venue": "Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language",
      "arxiv": "arXiv:1511.06361"
    },
    {
      "citation_id": "637",
      "title": "A review of feature selection methods based on mutual information. Neural computing and applications",
      "authors": [
        "Jorge Vergara",
        "Pablo Estévez"
      ],
      "year": "2014",
      "venue": "A review of feature selection methods based on mutual information. Neural computing and applications"
    },
    {
      "citation_id": "638",
      "title": "Some characteristics of the good judge of personality",
      "authors": [
        "E Philip",
        "Vernon"
      ],
      "year": "1933",
      "venue": "The Journal of Social Psychology"
    },
    {
      "citation_id": "639",
      "title": "Subspace clustering",
      "authors": [
        "René Vidal"
      ],
      "year": "2011",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "640",
      "title": "Centralnet: a multilayer approach for multimodal fusion",
      "authors": [
        "Valentin Vielzeuf",
        "Alexis Lechervy",
        "Stéphane Pateux",
        "Frédéric Jurie"
      ],
      "year": "2018",
      "venue": "Centralnet: a multilayer approach for multimodal fusion"
    },
    {
      "citation_id": "641",
      "title": "Optimal transport: old and new",
      "authors": [
        "Cédric Villani"
      ],
      "year": "2009",
      "venue": "Optimal transport: old and new"
    },
    {
      "citation_id": "642",
      "title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge",
      "authors": [
        "Oriol Vinyals",
        "Alexander Toshev",
        "Samy Bengio",
        "Dumitru Erhan"
      ],
      "year": "2016",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "643",
      "title": "Nbdt: Neural-backed decision tree",
      "authors": [
        "Alvin Wan",
        "Lisa Dunlap",
        "Daniel Ho",
        "Jihan Yin",
        "Scott Lee",
        "Suzanne Petryk",
        "Sarah Bargal",
        "Joseph Gonzalez"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "644",
      "title": "Glue: A multitask benchmark and analysis platform for natural language understanding",
      "authors": [
        "Alex Wang",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2018",
      "venue": "Glue: A multitask benchmark and analysis platform for natural language understanding",
      "arxiv": "arXiv:1804.07461"
    },
    {
      "citation_id": "645",
      "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "authors": [
        "Alex Wang",
        "Yada Pruksachatkun",
        "Nikita Nangia",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R Bowman"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "646",
      "title": "Screen2words: Automatic mobile ui summarization with multimodal learning",
      "authors": [
        "Bryan Wang",
        "Gang Li",
        "Xin Zhou",
        "Zhourong Chen",
        "Tovi Grossman",
        "Yang Li"
      ],
      "year": "2021",
      "venue": "ACM UIST"
    },
    {
      "citation_id": "647",
      "title": "Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis",
      "authors": [
        "Haohan Wang",
        "Aaksha Meghawat",
        "Louis-Philippe Morency",
        "Eric Xing"
      ],
      "year": "2016",
      "venue": "Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis",
      "arxiv": "arXiv:1609.05244"
    },
    {
      "citation_id": "648",
      "title": "Rethinking minimal sufficient representation in contrastive learning",
      "authors": [
        "Haoqing Wang",
        "Xun Guo",
        "Zhi-Hong Deng",
        "Yan Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "649",
      "title": "Multimodal graph-based reranking for web image search",
      "authors": [
        "Meng Wang",
        "Hao Li",
        "Dacheng Tao",
        "Ke Lu",
        "Xindong Wu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "650",
      "title": "Multimodal learning with incomplete modalities by knowledge distillation",
      "authors": [
        "Qi Wang",
        "Liang Zhan",
        "Paul Thompson",
        "Jiayu Zhou"
      ],
      "year": "2020",
      "venue": "KDD"
    },
    {
      "citation_id": "651",
      "title": "Visual commonsense representation learning via causal inference",
      "authors": [
        "Tan Wang",
        "Jianqiang Huang",
        "Hanwang Zhang",
        "Qianru Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "652",
      "title": "On deep multi-view representation learning",
      "authors": [
        "Weiran Wang",
        "Raman Arora",
        "Karen Livescu",
        "Jeff Bilmes"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "653",
      "title": "What makes training multi-modal classification networks hard?",
      "authors": [
        "Weiyao Wang",
        "Du Tran",
        "Matt Feiszli"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "654",
      "title": "Non-local neural networks",
      "authors": [
        "Xiaolong Wang",
        "Ross Girshick",
        "Abhinav Gupta",
        "Kaiming He"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "655",
      "title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
      "authors": [
        "Xin Wang",
        "Qiuyuan Huang",
        "Asli Celikyilmaz",
        "Jianfeng Gao"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "656",
      "title": "M2lens: Visualizing and explaining multimodal models for sentiment analysis",
      "authors": [
        "Xingbo Wang",
        "Jianben He",
        "Zhihua Jin"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Visualization and Computer Graphics"
    },
    {
      "citation_id": "657",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors. AAAI",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors. AAAI"
    },
    {
      "citation_id": "658",
      "title": "Multi-passage bert: A globally normalized bert model for open-domain question answering",
      "authors": [
        "Zhiguo Wang",
        "Patrick Ng",
        "Xiaofei Ma",
        "Ramesh Nallapati",
        "Bing Xiang"
      ],
      "year": "2019",
      "venue": "Multi-passage bert: A globally normalized bert model for open-domain question answering",
      "arxiv": "arXiv:1908.08167"
    },
    {
      "citation_id": "659",
      "title": "Characterizing and avoiding negative transfer",
      "authors": [
        "Zirui Wang",
        "Zihang Dai",
        "Barnabás Póczos",
        "Jaime Carbonell"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "660",
      "title": "Information theoretical analysis of multivariate correlation",
      "authors": [
        "Satosi Watanabe"
      ],
      "year": "1960",
      "venue": "IBM Journal of research and development"
    },
    {
      "citation_id": "661",
      "title": "Unsupervised domain adaptation with regularized optimal transport for multimodal 2d+ 3d facial expression recognition",
      "authors": [
        "Xiaofan Wei",
        "Huibin Li",
        "Jian Sun",
        "Liming Chen"
      ],
      "year": "2018",
      "venue": "FG 2018"
    },
    {
      "citation_id": "662",
      "title": "The cancer genome atlas pan-cancer analysis project",
      "authors": [
        "Eric John N Weinstein",
        "Gordon Collisson",
        "Kenna Mills",
        "Brad Shaw",
        "Kyle Ozenberger",
        "Ilya Ellrott",
        "Chris Shmulevich",
        "Joshua Sander",
        "Stuart"
      ],
      "year": "2013",
      "venue": "Nature genetics"
    },
    {
      "citation_id": "663",
      "title": "Face-to-face contrastive learning for social intelligence question-answering",
      "authors": [
        "Alex Wilf",
        "M Qianli",
        "Paul Ma",
        "Amir Liang",
        "Louis-Philippe Zadeh",
        "Morency"
      ],
      "year": "2022",
      "venue": "Face-to-face contrastive learning for social intelligence question-answering",
      "arxiv": "arXiv:2208.01036"
    },
    {
      "citation_id": "664",
      "title": "Nonnegative decomposition of multivariate information",
      "authors": [
        "L Paul",
        "Randall Williams",
        "Beer"
      ],
      "year": "2010",
      "venue": "Nonnegative decomposition of multivariate information",
      "arxiv": "arXiv:1004.2515"
    },
    {
      "citation_id": "665",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "Martin Wöllmer",
        "Felix Weninger",
        "Tobias Knaup",
        "Björn Schuller",
        "Congkai Sun",
        "Kenji Sagae",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "666",
      "title": "Idtxl: The information dynamics toolkit xl: a python package for the efficient analysis of multivariate information dynamics in networks",
      "authors": [
        "Patricia Wollstadt",
        "Joseph Lizier",
        "Raul Vicente",
        "Conor Finn",
        "Mario Martinez-Zarzuela",
        "Pedro Mediano",
        "Leonardo Novelli",
        "Michael Wibral"
      ],
      "year": "2019",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "667",
      "title": "A rigorous information-theoretic definition of redundancy and relevancy in feature selection based on (partial) information decomposition",
      "authors": [
        "Patricia Wollstadt",
        "Sebastian Schmitt",
        "Michael Wibral"
      ],
      "year": "2021",
      "venue": "A rigorous information-theoretic definition of redundancy and relevancy in feature selection based on (partial) information decomposition",
      "arxiv": "arXiv:2105.04187"
    },
    {
      "citation_id": "668",
      "title": "Leveraging sparse linear layers for debuggable deep networks",
      "authors": [
        "Eric Wong",
        "Shibani Santurkar",
        "Aleksander Madry"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "669",
      "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
      "authors": [
        "Jay Zhangjie Wu",
        "Yixiao Ge",
        "Xintao Wang",
        "Stan Lei",
        "Yuchao Gu",
        "Yufei Shi",
        "Wynne Hsu"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "670",
      "title": "Multimodal generative models for scalable weakly-supervised learning",
      "authors": [
        "Mike Wu",
        "Noah Goodman"
      ],
      "year": "2018",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "671",
      "title": "On mutual information in contrastive learning for visual representations",
      "authors": [
        "Mike Wu",
        "Chengxu Zhuang",
        "Milan Mosse",
        "Daniel Yamins",
        "Noah Goodman"
      ],
      "year": "2020",
      "venue": "On mutual information in contrastive learning for visual representations",
      "arxiv": "arXiv:2005.13149"
    },
    {
      "citation_id": "672",
      "title": "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
      "authors": [
        "Nan Wu",
        "Stanisław Jastrzębski",
        "Kyunghyun Cho",
        "Krzysztof Geras"
      ],
      "year": "2022",
      "venue": "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
      "arxiv": "arXiv:2202.05306"
    },
    {
      "citation_id": "673",
      "title": "Ask me anything: Free-form visual question answering based on knowledge from external sources",
      "authors": [
        "Qi Wu",
        "Peng Wang",
        "Chunhua Shen",
        "Anthony Dick",
        "Anton Van Den",
        "Hengel"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "674",
      "title": "Multimodal dataset distillation for image-text retrieval",
      "authors": [
        "Xindi Wu",
        "Zhiwei Deng",
        "Olga Russakovsky"
      ],
      "year": "2023",
      "venue": "Multimodal dataset distillation for image-text retrieval",
      "arxiv": "arXiv:2308.07545"
    },
    {
      "citation_id": "675",
      "title": "An information-theoretic analysis for transfer learning",
      "authors": [
        "Xuetong Wu",
        "Jonathan Manton",
        "Uwe Aickelin",
        "Jingge Zhu"
      ],
      "year": "2022",
      "venue": "Error bounds and applications",
      "arxiv": "arXiv:2207.05377"
    },
    {
      "citation_id": "676",
      "title": "Multimodal end-to-end autonomous driving",
      "authors": [
        "Yi Xiao",
        "Felipe Codevilla",
        "Akhil Gurram",
        "Onay Urfalioglu",
        "Antonio López"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "677",
      "title": "Adaptive cross-modal few-shot learning",
      "authors": [
        "Chen Xing",
        "Negar Rostamzadeh",
        "Boris Oreshkin",
        "Pedro O O Pinheiro"
      ],
      "year": "2019",
      "venue": "Adaptive cross-modal few-shot learning"
    },
    {
      "citation_id": "678",
      "title": "Dynamic memory networks for visual and textual question answering",
      "authors": [
        "Caiming Xiong",
        "Stephen Merity",
        "Richard Socher"
      ],
      "year": "2016",
      "venue": "ICML"
    },
    {
      "citation_id": "679",
      "title": "A survey on multi-view learning",
      "authors": [
        "Chang Xu",
        "Dacheng Tao",
        "Chao Xu"
      ],
      "year": "2013",
      "venue": "A survey on multi-view learning",
      "arxiv": "arXiv:1304.5634"
    },
    {
      "citation_id": "680",
      "title": "Multi-view intact space learning",
      "authors": [
        "Chang Xu",
        "Dacheng Tao",
        "Chao Xu"
      ],
      "year": "2015",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "681",
      "title": "A large-scale dataset for multimodal teaching and learning analytics",
      "authors": [
        "Fangli Xu",
        "Lingfei Wu",
        "Carol Thai",
        "Wei Hsu",
        "Richard Wang",
        "Tong",
        "Mutla"
      ],
      "year": "2019",
      "venue": "A large-scale dataset for multimodal teaching and learning analytics",
      "arxiv": "arXiv:1910.06078"
    },
    {
      "citation_id": "682",
      "title": "Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "683",
      "title": "Multimodal learning with transformers: A survey",
      "authors": [
        "Peng Xu",
        "Xiatian Zhu",
        "David Clifton"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "684",
      "title": "Multimodal fusion architecture search for electronic health records",
      "authors": [
        "Zhen Xu",
        "David So",
        "Andrew Dai",
        "Mufasa"
      ],
      "year": "2021",
      "venue": "Multimodal fusion architecture search for electronic health records",
      "arxiv": "arXiv:2102.02340"
    },
    {
      "citation_id": "685",
      "title": "Multimodal knowledge expansion",
      "authors": [
        "Zihui Xue",
        "Sucheng Ren",
        "Zhengqi Gao",
        "Hang Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "686",
      "title": "The modality focusing hypothesis: Towards understanding crossmodal knowledge distillation",
      "authors": [
        "Zihui Xue",
        "Zhengqi Gao",
        "Sucheng Ren",
        "Hang Zhao"
      ],
      "year": "2022",
      "venue": "The modality focusing hypothesis: Towards understanding crossmodal knowledge distillation",
      "arxiv": "arXiv:2206.06487"
    },
    {
      "citation_id": "687",
      "title": "Deep multi-view learning methods: A review",
      "authors": [
        "Xiaoqiang Yan",
        "Shizhe Hu",
        "Yiqiao Mao",
        "Yangdong Ye",
        "Hui Yu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "688",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "689",
      "title": "Mtag: Modaltemporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh"
      ],
      "year": "2021",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "690",
      "title": "Unified contrastive learning in image-text-label space",
      "authors": [
        "Jianwei Yang",
        "Chunyuan Li",
        "Pengchuan Zhang",
        "Bin Xiao",
        "Ce Liu",
        "Lu Yuan",
        "Jianfeng Gao"
      ],
      "year": "2022",
      "venue": "Unified contrastive learning in image-text-label space"
    },
    {
      "citation_id": "691",
      "title": "Vision-language pre-training with triple contrastive learning",
      "authors": [
        "Jinyu Yang",
        "Jiali Duan",
        "Son Tran",
        "Yi Xu",
        "Sampath Chanda",
        "Liqun Chen",
        "Belinda Zeng",
        "Trishul Chilimbi",
        "Junzhou Huang"
      ],
      "year": "2022",
      "venue": "Vision-language pre-training with triple contrastive learning"
    },
    {
      "citation_id": "692",
      "title": "Learning to extract semantic structure from documents using multimodal fully convolutional neural networks",
      "authors": [
        "Xiao Yang",
        "Ersin Yumer",
        "Paul Asente",
        "Mike Kraley",
        "Daniel Kifer",
        "C Giles"
      ],
      "year": "2017",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "693",
      "title": "Semi-supervised multi-modal learning with incomplete modalities",
      "authors": [
        "Yang Yang",
        "Xiang-Rong De-Chuan Zhan",
        "Yuan Sheng",
        "Jiang"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "694",
      "title": "Comprehensive semi-supervised multi-modal learning",
      "authors": [
        "Yang Yang",
        "Ke-Tao Wang",
        "De-Chuan Zhan",
        "Hui Xiong",
        "Yuan Jiang"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "695",
      "title": "Bag-of-visual-words and spatial extensions for land-use classification",
      "authors": [
        "Yi Yang",
        "Shawn Newsam"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems"
    },
    {
      "citation_id": "696",
      "title": "Multimodal transformer for multimodal machine translation",
      "authors": [
        "Shaowei Yao",
        "Xiaojun Wan"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "697",
      "title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
      "authors": [
        "Shunyu Yao",
        "Howard Chen",
        "John Yang",
        "Karthik Narasimhan"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "698",
      "title": "Contrastive conditional neural processes",
      "authors": [
        "Zesheng Ye",
        "Lina Yao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "699",
      "title": "Sensor and sensor fusion technology in autonomous vehicles: A review",
      "authors": [
        "Jong De",
        "Gustavo Yeong",
        "John Velasco-Hernandez",
        "Joseph Barry",
        "Walsh"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "700",
      "title": "Clevrer: Collision events for video representation and reasoning",
      "authors": [
        "Kexin Yi",
        "Chuang Gan",
        "Yunzhu Li",
        "Pushmeet Kohli",
        "Jiajun Wu",
        "Antonio Torralba",
        "Joshua Tenenbaum"
      ],
      "year": "2019",
      "venue": "Clevrer: Collision events for video representation and reasoning",
      "arxiv": "arXiv:1910.01442"
    },
    {
      "citation_id": "701",
      "title": "A novel graph-based multi-modal fusion encoder for neural machine translation",
      "authors": [
        "Yongjing Yin",
        "Fandong Meng",
        "Jinsong Su",
        "Chulun Zhou",
        "Zhengyuan Yang",
        "Jie Zhou",
        "Jiebo Luo"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "702",
      "title": "Irfl: Image recognition of figurative language",
      "authors": [
        "Ron Yosef",
        "Yonatan Bitton",
        "Dafna Shahaf"
      ],
      "year": "2023",
      "venue": "Irfl: Image recognition of figurative language",
      "arxiv": "arXiv:2303.15445"
    },
    {
      "citation_id": "703",
      "title": "Understanding neural networks through deep visualization",
      "authors": [
        "Jason Yosinski",
        "Jeff Clune",
        "Thomas Fuchs",
        "Hod Lipson"
      ],
      "year": "2015",
      "venue": "ICML Workshop on Deep Learning"
    },
    {
      "citation_id": "704",
      "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "authors": [
        "M Peter Young",
        "Alice Lai",
        "Julia Hockenmaier"
      ],
      "year": "2014",
      "venue": "TACL"
    },
    {
      "citation_id": "705",
      "title": "Efficient feature selection via analysis of relevance and redundancy",
      "authors": [
        "Lei Yu",
        "Huan Liu"
      ],
      "year": "2004",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "706",
      "title": "Online segment to segment neural transduction",
      "authors": [
        "Lei Yu",
        "Jan Buys",
        "Phil Blunsom"
      ],
      "year": "2016",
      "venue": "Online segment to segment neural transduction",
      "arxiv": "arXiv:1609.08194"
    },
    {
      "citation_id": "707",
      "title": "Pacs: A dataset for physical audiovisual commonsense reasoning",
      "authors": [
        "Samuel Yu",
        "Peter Wu",
        "Paul Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "708",
      "title": "Vision guided generative pre-trained language models for multimodal abstractive summarization",
      "authors": [
        "Tiezheng Yu",
        "Wenliang Dai",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "EMNLP"
    },
    {
      "citation_id": "709",
      "title": "Heterogeneous graph learning for visual commonsense reasoning",
      "authors": [
        "Weijiang Yu",
        "Jingwen Zhou",
        "Weihao Yu",
        "Xiaodan Liang",
        "Nong Xiao"
      ],
      "year": "2019",
      "venue": "Heterogeneous graph learning for visual commonsense reasoning"
    },
    {
      "citation_id": "710",
      "title": "Speaker identification on the scotus corpus",
      "authors": [
        "Jiahong Yuan",
        "Mark Liberman"
      ],
      "year": "2008",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "711",
      "title": "Multimodal contrastive training for visual representation learning",
      "authors": [
        "Xin Yuan",
        "Zhe Lin",
        "Jason Kuen",
        "Jianming Zhang",
        "Yilin Wang",
        "Michael Maire",
        "Ajinkya Kale",
        "Baldo Faieta"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "712",
      "title": "Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency",
        "Mosi"
      ],
      "year": "2016",
      "venue": "Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "713",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "714",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "715",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "716",
      "title": "Multiattention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "717",
      "title": "Social-iq: A question answering benchmark for artificial social intelligence",
      "authors": [
        "Amir Zadeh",
        "Michael Chan",
        "Paul Liang",
        "Edmund Tong",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "718",
      "title": "Foundations of multimodal co-learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "719",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "720",
      "title": "Taskonomy: Disentangling task transfer learning",
      "authors": [
        "Alexander Amir R Zamir",
        "William Sax",
        "Leonidas Shen",
        "Jitendra Guibas",
        "Silvio Malik",
        "Savarese"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "721",
      "title": "From recognition to cognition: Visual commonsense reasoning",
      "authors": [
        "Rowan Zellers",
        "Yonatan Bisk",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "722",
      "title": "Merlot: Multimodal neural script knowledge models",
      "authors": [
        "Rowan Zellers",
        "Ximing Lu",
        "Jack Hessel",
        "Youngjae Yu",
        "Jae Park",
        "Jize Cao",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "723",
      "title": "Merlot reserve: Neural script knowledge through vision and language and sound",
      "authors": [
        "Rowan Zellers",
        "Jiasen Lu",
        "Ximing Lu",
        "Youngjae Yu",
        "Yanpeng Zhao",
        "Mohammadreza Salehi",
        "Aditya Kusupati",
        "Jack Hessel",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "724",
      "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
      "authors": [
        "Andy Zeng",
        "Adrian Wong",
        "Stefan Welker",
        "Krzysztof Choromanski",
        "Federico Tombari"
      ],
      "year": "2022",
      "venue": "Socratic models: Composing zero-shot multimodal reasoning with language",
      "arxiv": "arXiv:2204.00598"
    },
    {
      "citation_id": "725",
      "title": "Multimodal core tensor factorization and its applications to low-rank tensor completion",
      "authors": [
        "Haijin Zeng",
        "Jize Xue",
        "Q Hiêp",
        "Wilfried Luong",
        "Philips"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "726",
      "title": "Leveraging video descriptions to learn video question answering",
      "authors": [
        "Kuo-Hao Zeng",
        "Tseng-Hung Chen",
        "Ching-Yao Chuang",
        "Yuan-Hong Liao",
        "Juan Niebles",
        "Min Sun"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "727",
      "title": "Multimodal deep representation learning for protein interaction identification and protein family classification",
      "authors": [
        "Da Zhang",
        "Mansur Kabuka"
      ],
      "year": "2019",
      "venue": "BMC bioinformatics"
    },
    {
      "citation_id": "728",
      "title": "A survey of controllable text generation using transformer-based pre-trained language models",
      "authors": [
        "Hanqing Zhang",
        "Haolin Song",
        "Shaoyu Li",
        "Ming Zhou",
        "Dawei Song"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "729",
      "title": "Learning concept taxonomies from multi-modal data",
      "authors": [
        "Hao Zhang",
        "Zhiting Hu",
        "Yuntian Deng",
        "Mrinmaya Sachan",
        "Zhicheng Yan",
        "Eric Xing"
      ],
      "year": "2016",
      "venue": "ACL"
    },
    {
      "citation_id": "730",
      "title": "Audio content analysis for online audiovisual data segmentation and classification",
      "authors": [
        "Tong Zhang",
        "C-C Jay Kuo"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on speech and audio processing"
    },
    {
      "citation_id": "731",
      "title": "Multimodal feature fusion by relational reasoning and attention for visual question answering",
      "authors": [
        "Weifeng Zhang",
        "Jing Yu",
        "Hua Hu",
        "Haiyang Hu",
        "Zengchang Qin"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "732",
      "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "authors": [
        "Yizhe Zhang",
        "Siqi Sun",
        "Michel Galley",
        "Yen-Chun Chen",
        "Chris Brockett",
        "Xiang Gao",
        "Jianfeng Gao",
        "Jingjing Liu",
        "William Dolan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "733",
      "title": "Federated learning with non-iid data",
      "authors": [
        "Yue Zhao",
        "Meng Li",
        "Liangzhen Lai",
        "Naveen Suda",
        "Damon Civin",
        "Vikas Chandra"
      ],
      "year": "2018",
      "venue": "Federated learning with non-iid data",
      "arxiv": "arXiv:1806.00582"
    },
    {
      "citation_id": "734",
      "title": "Deep supervised cross-modal retrieval",
      "authors": [
        "Liangli Zhen",
        "Peng Hu",
        "Xu Wang",
        "Dezhong Peng"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "735",
      "title": "Rtfm: Generalising to new environment dynamics via reading",
      "authors": [
        "Victor Zhong",
        "Tim Rocktäschel",
        "Edward Grefenstette"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "736",
      "title": "Silg: The multi-environment symbolic interactive language grounding benchmark",
      "authors": [
        "Victor Zhong",
        "Austin Hanjie",
        "Karthik Narasimhan",
        "Luke Zettlemoyer",
        "Austin Hanjie",
        "Victor Zhong",
        "Karthik Narasimhan",
        "Machel Reid",
        "Victor Zhong",
        "Victor Zhong"
      ],
      "year": "2021",
      "venue": "Silg: The multi-environment symbolic interactive language grounding benchmark"
    },
    {
      "citation_id": "737",
      "title": "A realistic web environment for building autonomous agents",
      "authors": [
        "Shuyan Zhou",
        "Frank Xu",
        "Hao Zhu",
        "Xuhui Zhou",
        "Robert Lo",
        "Abishek Sridhar",
        "Xianyi Cheng"
      ],
      "year": "2023",
      "venue": "A realistic web environment for building autonomous agents",
      "arxiv": "arXiv:2307.13854"
    },
    {
      "citation_id": "738",
      "title": "Minigpt-4: Enhancing visionlanguage understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing visionlanguage understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "739",
      "title": "Arbitrary talking face generation via attentional audio-visual coherence learning",
      "authors": [
        "Huaibo Hao Zhu",
        "Yi Huang",
        "Li"
      ],
      "year": "2021",
      "venue": "Aihua Zheng, and Ran He"
    },
    {
      "citation_id": "740",
      "title": "Multi-modal knowledge graph construction and application: A survey",
      "authors": [
        "Xiangru Zhu",
        "Zhixu Li",
        "Xiaodan Wang",
        "Xueyao Jiang",
        "Penglei Sun",
        "Xuwu Wang"
      ],
      "year": "2022",
      "venue": "Multi-modal knowledge graph construction and application: A survey",
      "arxiv": "arXiv:2202.05786"
    },
    {
      "citation_id": "741",
      "title": "Building a large-scale multimodal knowledge base system for answering visual queries",
      "authors": [
        "Yuke Zhu",
        "Ce Zhang",
        "Christopher Ré",
        "Li Fei-Fei"
      ],
      "year": "2015",
      "venue": "Building a large-scale multimodal knowledge base system for answering visual queries",
      "arxiv": "arXiv:1507.05670"
    },
    {
      "citation_id": "742",
      "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "authors": [
        "Yukun Zhu",
        "Ryan Kiros",
        "Rich Zemel",
        "Ruslan Salakhutdinov",
        "Raquel Urtasun",
        "Antonio Torralba",
        "Sanja Fidler"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "743",
      "title": "Encoder-agnostic adaptation for conditional language generation",
      "authors": [
        "Zachary Ziegler",
        "Luke Melas-Kyriazi",
        "Sebastian Gehrmann",
        "Alexander Rush"
      ],
      "year": "2019",
      "venue": "Encoder-agnostic adaptation for conditional language generation",
      "arxiv": "arXiv:1908.06938"
    },
    {
      "citation_id": "744",
      "title": "",
      "authors": [
        "Julian Georg Zilly",
        "Rupesh Srivastava",
        "Jan Koutník",
        "Jürgen Schmidhuber"
      ],
      "year": "2016",
      "venue": "",
      "arxiv": "arXiv:1607.03474"
    },
    {
      "citation_id": "745",
      "title": "Human behavior and the principle of least effort: An introduction to human ecology",
      "authors": [
        "George Kingsley"
      ],
      "year": "2016",
      "venue": "Human behavior and the principle of least effort: An introduction to human ecology"
    }
  ]
}