{
  "paper_id": "2507.21395v1",
  "title": "Sync-Tva: A Graph-Attention Framework For Multimodal Emotion Recognition With Cross-Modal Fusion",
  "published": "2025-07-29T00:03:28Z",
  "authors": [
    "Zeyu Deng",
    "Yanhui Lu",
    "Jiashu Liao",
    "Shuang Wu",
    "Chongfeng Wei"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Cross-modal Fusion",
    "Graph Neural Network",
    "Attention Mechanism",
    "Affective Computing",
    "Conversational AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) is crucial for enabling emotionally intelligent systems that perceive and respond to human emotions. However, existing methods suffer from limited cross-modal interaction and imbalanced contributions across modalities. To address these issues, we propose Sync-TVA, an end-to-end graph-attention framework featuring modality-specific dynamic enhancement and structured crossmodal fusion. Our design incorporates a dynamic enhancement module for each modality and constructs heterogeneous crossmodal graphs to model semantic relations across text, audio, and visual features. A cross-attention fusion mechanism further aligns multimodal cues for robust emotion inference. Experiments on MELD and IEMOCAP demonstrate consistent improvements over state-of-the-art models in both accuracy and weighted F1 score, especially under class-imbalanced conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "W ITH the rapid development of artificial intelligence (AI) and robotics, robots are taking on increasingly prominent roles in various aspects of daily life, and the demand for emotional intelligence has become more critical than ever  [1] . Modern computer systems can already capable of exhibiting a degree of empathy and perform sophisticated emotional analysis  [2] . For domestic robots, accurately recognising and responding to human emotions is vital for building and sustaining harmonious, long-term relationships-particularly after the initial novelty of interaction fades and the need for continuous re-engagement arises  [3] . However, the integration between emotion recognition and facial recognition systems remains insufficient and has yet to result in a unified framework. This is mainly because these two research fields have historically evolved independently, rather than being systematically studied and integrated into a collaborative recognition system  [4] . Consequently, most existing systems still fail to treat the user as a dynamic interlocutor whose emotional states must be perceived and acted upon in real time. Only a handful of studies address such real-time integration in practical interaction settings  [5] . This fragmentation highlights a broader bottleneck in multimodal human-robot interaction (HRI). Although interest in multimodal HRI is growing, systematic research that fuses core channels-visual, auditory, and linguistic-remains limited  [6] . The underlying collaborative mechanisms among these modalities remain poorly understood, limiting both theoretical progress and deployable applications.\n\nEarly studies on AI-driven emotion recognition focused primarily on unimodal analysis. Text-based mental health analysis has been the most widely used method  [7] , benefiting from abundant data sources and primarily aiming to classify text sentiment  [8] . These methods typically classify sentiment based on linguistic features, ranging from dictionary-based approaches  [9]  to deep learning models such as Convolutional Neural Networks (CNNs)  [10] ,  [11]  and Recurrent Neural Networks (RNNs)  [12] . Long Short-Term Memory (LSTM) networks are a specialized type of RNN that outperform traditional RNNs in many tasks. This mechanism effectively addresses issues related to vanishing or exploding gradients  [13] . Li and Qian applied the LSTM model to address the challenge of text sentiment analysis  [14] . Although CNNs effectively capture local patterns and RNNs/LSTMs model sequential dependencies, these architectures face limitations in handling long texts, integrating multiple modalities, and managing computational complexity  [13] .\n\nOther modalities have also been developing in parallel, such as facial expression analysis  [15] ,  [16]  and speech emotion recognition  [17] ,  [18] . These areas have made substantial progress. Vision-based models extract spatial and temporal cues using hybrid descriptors or deep CNNs  [19] , while audiobased models rely on either handcrafted features or end-to-end learned representations. Despite their effectiveness, unimodal systems remain vulnerable in real-world conditions where input signals may be noisy, occluded, or missing.\n\nTo address these limitations, recent studies have shifted toward multimodal emotion recognition (MER), which integrates complementary signals-such as text, audio, and visual data-to improve recognition performance. MER systems often adopt fusion strategies to align and combine information across modalities. For example, attention-based encoders  [20] , temporal alignment mechanisms  [21] , and canonical correlation analysis  [22]  have been explored to enhance multimodal synergy. However, several challenges remain, including modality imbalance, inconsistent emotional cues across modalities, and the limited adaptability of static fusion architectures  [23] .\n\nBuilding upon these foundations, a growing body of work has leveraged graph neural networks (GNNs) and crossmodel attention mechanisms to explicitly capture inter-modal correlations and semantic dependencies. For instance, graphbased structures have been used to align hierarchical modality relationships across time and semantics  [24]    [25] . Addition-ally, co-attention mechanisms allow for selective focus on distinguishing features in audio, text, and visual channels  [26]    [27] . However, static fusion methods may still face rigidity issues in cross-modal interaction, meaning they cannot handle the differences between sample data distribution and complex dynamic environments  [28] . To solve this problem, some studies have proposed adaptive fusion modules  [29]  or contrastive frameworks  [30] . Despite these advancements, challenges such as modality imbalance and misalignment still exist in real-world multimodal emotion recognition scenarios.\n\nTo overcome these issues, we propose a novel framework-Sync-TVA for multimodal emotion recognition in conversations. Our method introduces two key components: Enforced Graph Construction and Deep Information Interaction Fusion. In the Enforced Graph Construction stage, we design a Modality-Specific Dynamic Enhancement (MSDE) module that performs deep intra-modal feature refinement using dynamic gating, multi-head self-attention, and residual feedforward networks. The enhanced features from the text, audio, and visual modalities are then used to construct three heterogeneous graphs-Visual-Audio (V-A), Text-Visual (T-V), and Audio-Text (A-T)-to explicitly model inter-modal relationships and reduce semantic misalignment.\n\nIn the Deep Information Interaction Fusion stage, these cross-modal graphs serve as the basis for bidirectional feature interactions. We employ attention-based mechanisms to conduct deep fusion of modality-specific features, thereby capturing critical emotional cues and improving semantic alignment. Finally, a Cross-modal Attention Fusion (CAF) module concatenates and refines these representations to support accurate emotion classification.\n\nThe main contributions of this work are as follows:\n\n1) We propose a Modality-Specific Dynamic Enhancement (MSDE) module for modality-specific self-enhancement. This lightweight module combines a dynamic gating mechanism with a local self-attention structure that adaptively adjusts the expressive strength of modality-specific features. It enhances key semantic and structural information within each modality, providing a more robust foundation for cross-modal mapping and fusion. 2) We introduce inter-modal structural graphs (V-A, T-V, A-T) to represent cross-modal interaction relationships. By constructing three types of cross-modal graphs-visual-audio, text-visual, and audio-text-we explicitly model the connections between modalities. This approach mitigates semantic misalignment caused by simple concatenation and provides structural guidance for graph-based interaction. 3) We design a Cross-Modal Attention Fusion (CAF) mechanism to enhance semantic alignment. In the deep information interaction stage, dynamic interactions between modalities are enabled through cross-attention pathways. The CAF module then fuses complementary information, effectively capturing fine-grained emotional cues. 4) We construct an end-to-end emotion recognition framework. The entire architecture is self-contained and sup-ports joint optimization of feature extraction, graph construction, attention-based fusion, and emotion classification in an end-to-end manner, offering strong scalability and transferability. 5) We conduct extensive experimental validation on two widely used multimodal emotion recognition benchmarks. Results on MELD and IEMOCAP demonstrate that the proposed method consistently outperforms or matches state-of-the-art approaches across multiple metrics, validating its effectiveness and robustness. The rest of the paper is organized as follows: Section 2 reviews related work on multimodal emotion recognition, with an emphasis on modality-specific contributions and graphbased modeling strategies. Section 3 presents the proposed Sync-TVA framework, detailing the design of the MSDE module, cross-modal graph construction, and the deep fusion mechanism. Section 4 describes the experimental setup, evaluation metrics, baseline comparisons, and ablation studies, highlighting the effectiveness and robustness of the proposed method. Section 5 concludes the paper and outlines potential directions for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Unimodal Emotion Recognition 1) Visual Modality Approaches: Facial expressions have long been central in unimodal emotion recognition. Early works relied on handcrafted features. For instance, Kalsum et al.  [15]  combined SIFT, SURF, and Bag-of-Features with SVM or KNN classifiers, while Mahersia  [16]  used steerable pyramids with Bayesian neural networks. Deep learning approaches, such as that of Li et al.  [19] , introduced 3D-CNNs that use grayscale and optical flow to capture microexpressions. However, these methods often struggle under real-world dynamics and occlusion. Our framework enhances visual representations via dynamic attention and graph-based interactions, thereby improving robustness.\n\n2) Acoustic Modality Approaches: In scenarios where facial cues are occluded or unavailable, speech serves as a crucial channel for emotional expression. Early studies on speech emotion recognition (SER) emphasized handcrafted prosodic and spectral features, such as energy, pitch, LPCC, MFCC, and LPCMCC. For example, Shen et al.  [17]  applied these features in combination with SVM classifiers, achieving high recognition accuracy on the Berlin emotional speech database. While effective in controlled settings, such handcrafted pipelines often struggle with generalization and robustness under realworld variability. To address these limitations, end-to-end deep learning approaches have emerged. Trigeorgis et al.  [18]  proposed a CNN-LSTM architecture that directly models raw waveform input and learns high-level affective representations without manual feature extraction. This method demonstrated strong performance, particularly in spontaneous emotion prediction, and marked a shift toward data-driven auditory modeling. Our approach embeds acoustic data within cross-modal graphs, capturing context-aware dependencies and enhancing semantic alignment under noisy or prosodically ambiguous conditions.\n\n3) Text Modality Approaches: Text has historically served as a foundational modality in emotion recognition, particularly in dialogue-based contexts. Early efforts in Emotion Recognition in Conversation (ERC) predominantly relied on textual inputs, given their availability and semantic richness. To address challenges such as contextual dependency and emotion imbalance, hierarchical models such as HiGRU and its variants were proposed  [31]  , which leverage gated recurrent units to capture temporal dependencies across utterances in a conversation. As emotion understanding requires both linguistic and affective cues, SENN (Semantic-Emotion Neural Network)  [32]  advanced the field by introducing a dual-branch architecture that combines BiLSTM and CNN modules to jointly model semantic/syntactic structures and emotional intensity. Subsequently, Sentic GCN  [33]  took a more structured approach by embedding affective knowledge from SenticNet into a graph convolutional network for aspect-based sentiment analysis, allowing finer-grained sentiment modeling at the concept and dependency levels. Meanwhile, alternative methods such as BERT-based ABSA  [34]",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multimodal Emotion Recognition (Mer)",
      "text": "MER integrates heterogeneous cues to improve emotional inference in conversational scenarios. Visual-centric models, such as Rupauliha et al.  [35] , use LSTM-based fusion of facial and gesture cues, while Lan et al.  [22]  propose DGCCA-AM to align visual and semantic features with adaptive weighting. In audio-focused systems, Ghaleb et al.  [21]  used temporalmetric learning to capture intermodal timing offsets, while Zhang et al.  [23]  introduced fuzzy-weighted SVR to suppress noisy signals. On the textual side, models like CFN  [36]  and prompt-based MEP  [37]  address long-range context and emotion shifts, though text often remains weakly aligned with other modalities. Our method addresses these gaps by constructing cross-modal graphs and applying attention-based fusion mechanisms to achieve deeper semantic alignment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Graph Neural Networks For Emotion Recognition",
      "text": "While traditional multimodal fusion methods have demonstrated progress in emotion recognition, they often lack the ability to explicitly model complex inter-modal and contextual dependencies in conversation. Recently, Graph Neural Networks (GNNs) have received increasing attention for their strength in handling structured data, making them highly suitable for emotion recognition tasks, particularly in settings involving dynamic interactions among modalities, utterances, or speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gnns In Conversational Multimodal Emotion Recognition",
      "text": "Tasks In ERC, accurately modeling the interplay between utterances and modalities is essential. Traditional methods like DialogueGCN  [38]  and RGAT  [39]  model utterance-level interactions using pairwise edges, but these approaches often fall short in capturing higher-order contextual and emotional dependencies. To overcome these limitations, SDR-GNN  [40]  proposed a spectral-domain reconstruction mechanism that aggregates multi-frequency signals within a sliding-windowbased graph structure. This allows the model to retain highfrequency information that reflects emotion discrepancies and avoids over-smoothing. M 3 Net  [41] further advanced this idea by introducing a multivariate and multi-frequency graph propagation strategy. It models hyperedges that naturally encode high-arity relationships across modalities and utterances, thereby enhancing the graph's expressive capacity in emotionrelated interactions. Moreover, frequency-adaptive filters are used to balance emotional commonality and discrepancy across speakers and contexts. In another recent work, D2GNN  [42]  introduced a decoupled distillation mechanism that separates emotion-aware and emotion-agnostic features at the category level. It propagates these representations through two GNN branches and performs targeted multimodal distillation, enabling more discriminative embeddings, particularly when speaker roles or emotions diverge.\n\nIn conclusion, GNNs have demonstrated strong potential in ERC by modeling fine-grained speaker dynamics, emotional transitions, and modality interactions. With the integration of hypergraph structures, frequency-sensitive propagation, and category-level representation disentanglement, recent advances make GNNs increasingly central to the development of robust and interpretable conversational emotion recognition systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Given the challenges of data integration and semantic understanding in multimodal emotion recognition, this study proposes an innovative multimodal fusion framework designed to enhance both the accuracy and generalization capability of emotion recognition. The system comprises five key modules:\n\n(1) the initial multimodal data input layer, which receives speech, image, and text information; (2) the feature extraction module, which utilizes OpenSmile, ResNet, and BERT to capture deep representations from each modality; (3) the feature graph construction and enhancement module, which strengthens feature representation through MSDE technology;\n\n(4) the deep information interaction and fusion module, which integrates multimodal representations and captures intricate emotional dependencies; and (5) the final classification module, which outputs emotion predictions. Together, these components form a complete pipeline from multimodal input to emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Multimodal Feature Extraction",
      "text": "The input visual data typically consist of video frames of the interlocutors. A pretrained ResNet-50 is adopted as the visual feature encoder. For each frame image, forward propagation is performed, and the global pooling output after the last convolution block of the ResNet-50 backbone network is taken as the visual feature vector:\n\nwhere d v denotes the dimensionality of the visual feature.\n\nThe output vectors are normalized to ensure consistency in the numerical range across different modalities.\n\nThe text modality comprises the linguistic content of the dialogue. A pretrained RoBERTa model is adopted to encode each input text T i . Specifically, input the text into RoBERTa and extract the output at the corresponding position from the last hidden layer as the semantic feature vector:\n\nwhere d t represents the dimensionality of the textual feature. These vectors are also normalized for subsequent integration. The audio modality consists of the acoustic signals in the dialogue. The open-source toolkit OpenSMILE is employed to extract acoustic features. For each audio segment, OpenSMILE extracts a set of high-dimensional acoustic features (such as fundamental frequency, energy, and Mel spectrum):\n\nwhere d a is typically large. The resulting vectors are also L 2normalized.\n\nTo facilitate unified representation, we will concatenate the tri-modal features of all samples to obtain matrix form:\n\nwhere N indicates the total number of frames or segments in the dialogue sequence. These matrices constitute the fundamental multimodal representations and serve as inputs to the subsequent graph construction and fusion modules.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Enforced Graph Construction",
      "text": "To capture the relationships between different modalities, we introduce graph-based modeling after feature extraction. Specifically, we design a Modality-Specific Dependency Encoder (MSDE) module to construct three cross-modal graphs: the Visual-Audio (V-A) graph, the Text-Visual (T-V) graph, and the Audio-Text (A-T) graph. Each graph connects feature nodes from two distinct modalities, and the MSDE module determines the edge weights between nodes to explicitly encode cross-modal dependencies.\n\nTaking the V-A graph as an example, it contains a set of visual nodes V = {v 1 , . . . , v N }, where each v i corresponds to the visual feature of frame I i , and a set of audio nodes A = {a 1 , . . . , a N }, where each a i represents the audio feature of the same frame. In total, there are 2N nodes in the graph.\n\nWe denote the initial node feature matrix as:\n\nwhere d is the unified hidden layer dimension (d v and d a can be mapped to the same dimension through linear transformation).\n\nThe adjacency matrix is defined as:\n\nwhich records the connectivity between visual and audio nodes.\n\nSimilarly, the T-V graph connects textual nodes with visual nodes, while the A-T graph connects audio nodes with textual nodes.\n\n1) Modality-Specific Dynamic Enhancement: The MSDE module is designed to learn cross-modal associative weights based on input features to generate an adjacency matrix. Specifically, we apply similarity measures or deep interactions to each pair of modal features to produce an edge weight matrix.\n\nTaking the V-A graph as an example, for each visual node v i and audio node a j , the association score is computed as:\n\nwhere f MSDE (•, •) represents the scoring function learned within the MSDE module. For example, a small fully connected network or a bilinear mapping can be used to model the correlation between F v i and F a j . After computing all scores e VA ij , we normalize them (e.g., using the sigmoid function) to obtain edge weights:\n\nwhere σ(•) denotes the selected normalization function.\n\nAlso set:\n\nFollowing the same procedure, we compute the adjacency matrices A TV and A AT for the T-V and A-T graphs, respectively. In this way, every edge in the three graphs is governed by cross-modal relevance learned from the MSDE module, which constitutes reinforced graph construction.\n\n2) Graph Convolutional Updates: Given the adjacency matrix A and node feature matrix H, we apply a Graph Convolutional Network (GCN) to update the features. For a general graph G, the GCN update is defined as:\n\nwhere Ã = A + I is the adjacency matrix with self-loops, D is the degree matrix of Ã, W (l) is the trainable weight matrix at layer l, and σ(•) is an activation function such as ReLU.\n\nFor the V-A graph, the initial node features\n\nVA through graph convolution. Similarly, we compute the updated node features H\n\n(1)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Tv And H (1)",
      "text": "AT for the T-V and A-T graphs, respectively.\n\nThrough graph convolution, the model not only retains intramodal features but also enables each node to absorb contextual information from cross-modal neighbors. This process yields richer representations for downstream fusion modules.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Deep Information Interaction Fusion",
      "text": "After obtaining the graph-convolved features from the three cross-modal graphs, we aim to integrate and interact with these representations across layers to capture deeper crossmodal dependencies. To achieve this, we design a graph fusion module consisting of several key components: a 1D convolution layer, layer normalization (LayerNorm), an attention mechanism (Query-Key-Value), and a cross-attention fusion (CAF) block.\n\nFor each graph, we perform a 1D convolution operation on its output features H (1) to extract local sequential patterns and contextual cues. Taking the V-A graph as an example, we apply a 1D convolution with kernel size k to its feature matrix:\n\nwhere HVA has the same dimensionality as H\n\nVA , and captures information from neighboring nodes through convolution. Similarly, we apply the same operation to the T-V and A-T graphs to obtain HTV and HAT , respectively.\n\nTo stabilize training and integrate features from multiple sources, we apply layer normalization to the convolution outputs. For example, given the V-A graph feature HVA , the normalized representation is computed as:\n\nwhere LayerNorm(•) denotes the standard layer normalization operation applied across feature dimensions. This normalization step helps reduce statistical discrepancies across different channels (i.e., feature dimensions), facilitating the convergence of subsequent attention learning.\n\nNext, we apply an attention mechanism to enable crossgraph interactions. Taking the feature matrices H1 and H 2 of two graphs G 1 and G 2 as an example (H 2 can select the corresponding features after convolution. ), we linearly project them into queries (Q), keys (K), and values (V ) as follows:\n\nwhere W Q , W K , and W V are learnable projection matrices. The attention weight matrix S and the attended representation A are computed as:\n\nwhere √ d k is the scaling factor (with d k being the dimensionality of the key vectors), and the softmax operation is applied row-wise for normalization.\n\nThe generated A is the cross-graph feature representation obtained after the key-value pair interaction between query G 2 based on G 1 .\n\n1) Cross-Attention Fusion: After concatenating the attention output A and the reference features H1 , we form a joint representation:\n\nwhich is then passed through a 1D convolution layer to generate the fused embedding:\n\nTo achieve a more flexible and expressive fusion, we adopt a gate-controlled structure inspired by GRU-style gating mech-anisms. Specifically, we compute the final fused output F CAF as:\n\nwhere W f , W g are learnable weight matrices, b f , b g are bias terms, σ(•) is the sigmoid activation function, tanh(•) is the hyperbolic tangent, and ⊙ denotes element-wise multiplication. This gating mechanism adaptively balances the information contribution from the attention-derived features and the original representations, allowing the model to dynamically control the fusion flow at each feature dimension.\n\n2) Iterative Cross-Graph Fusion: In practice, we perform cross-attention fusion between two graphs at a time and may iterate this process multiple times. For example, we can take the V-A and A-T graphs and use H VA and H AT as query-key pairs for one round of fusion. Simultaneously, the T-V and A-T graphs can also be fused with H TV and H AT as query and key inputs, respectively.\n\nThe features from each fusion round are passed to the next fusion iteration as new inputs, enabling the model to progressively integrate multimodal information across all three modalities.\n\nFinally, the outputs from multiple fusion branches are aggregated to form the unified output of the overall fusion layer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Classification Layer",
      "text": "After completing the multimodal fusion, we concatenate the feature vectors from different fusion branches to form a unified final representation z. Suppose the top-level fusion module outputs two features F\n\n(1)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Caf And F",
      "text": "(2) CAF , then the concatenation is performed as:\n\nwhere [• ; •] denotes vector concatenation. This fused representation is then passed through a fully connected layer followed by a softmax function to output the emotion class probabilities:\n\nwhere W c ∈ R C×2dz and b c ∈ R C are trainable parameters, C is the number of emotion categories, and ŷ ∈ R C is the predicted class distribution. The model is trained using the standard cross-entropy loss. Given the ground-truth label y in one-hot format and the predicted distribution ŷ, the loss function is defined as:\n\nMinimizing this loss optimizes the model parameters. As a result, the model can perform fine-grained emotion classification at each time step based on the fused multimodal features, predicting emotion categories such as anger, sadness, and happiness. IV. EXPERIMENTS",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Dataset",
      "text": "This study conducts experimental evaluations on two widely used multimodal emotion recognition datasets: MELD and IEMOCAP. Both datasets are multimodal benchmarks that incorporate visual, auditory, and textual modalities. Table  1  presents the statistical details of these two datasets.\n\n1) MELD: MELD is a conversational multimodal emotion recognition dataset, extended from the textual dataset EmotionLines. It contains conversations extracted from the American TV series Friends and includes three modalities: text, audio, and vision. MELD includes over 1,400 multiparty conversations and approximately 13,000 utterances. Each utterance is annotated with emotion categories such as anger, joy, sadness, and neutral. Moreover, MELD exhibits contextual dependencies within conversations, making it suitable for studying cross-modal and context-aware emotion recognition models.\n\n2) IEMOCAP: IEMOCAP is a conversational multimodal emotion dataset developed by the University of Southern California to support research on emotion analysis across speech, video, and text modalities. It contains approximately 12 hours of audio-visual data, performed in pairs by 10 actors, covering both daily conversations and emotional expressions. Each conversation is segmented into multiple utterances, with emotion categories annotated by multiple raters. The commonly used emotion categories include happy, sad, angry, and neutral. IEMOCAP provides text transcriptions, audio features (e.g., MFCC), video frames, and facial keypoint data, offering a rich set of inputs for multimodal emotion recognition models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "In this study, because of the significant class imbalance issue in the dataset used, where the sample sizes for emotions like happy and neutral far exceed those for rare emotions like anger and sadness, it is likely to lead to insufficient recognition capability for minority emotional classes during the training process. To comprehensively and accurately assess the model's performance, this study selects the weighted average F1 score as the core evaluation metric. This indicator uses the sample size of each emotion category as a weight, comprehensively considering precision and recall, effectively avoiding the recognition performance of minority emotion categories being overshadowed by majority categories. At the same time, to further validate the model's generalization ability across all emotional categories, we supplement the average accuracy scores by calculating the arithmetic mean of the accuracy rates for each category. This objectively reflects the model's correctness in recognizing different emotional categories and provides a more comprehensive perspective for",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Training Detail",
      "text": "To verify the statistical significance of the improvement in model performance, this study employed a paired t -test for hypothesis testing, with a significance level α = 0.05. The experimental environment is configured as follows: all computational tasks are executed on workstations equipped with NVIDIA GeForce RTX 3090 graphics processors, which have 24GB of video memory and a computing power of 8.6. The system uses the CUDA 11.7 parallel computing platform, and the deep learning framework chosen is PyTorch version 2.0.0.\n\nIn terms of model optimization, this study chose the AdamW optimization algorithm for parameter updates, which performs exceptionally well in handling weight decay. The L2 regularization coefficient is uniformly set to 1 × 10 -3 to prevent the model from overfitting. Regarding the characteristics of different datasets, there are differentiated settings for hyperparameter configurations: the learning rate for the IEMOCAP dataset is set to 1 × 10 -4 , the batch size is 16, the dropout rate is 0.2, the number of model layers L is 7, the position encoding dimension P is 17, the number of attention heads B is 19, and the loss function weights λ s and λ o are 1.0 and 0.7, respectively. The MELD dataset uses a minimum learning rate of 7 × 10 -5 , with model structure parameters L, P , B set to 5, 3, and 3 respectively, and loss weights λ s and λ o set to 0.5 and 0.2. This differentiated hyperparameter configuration strategy fully takes into account factors such as the scale, complexity, and annotation quality of each dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Baselines",
      "text": "To evaluate whether our proposed method is outstanding, we compared it with a set of different baseline models. These baselines are classified into graph-based models and nongraph-based models. The graph-based category includes Dia-logueGCN  [38] , MMGCN  [47] , M3Net  [41] , DER-GCN  [52] , and GraphSmile  [51] . The non-graph-based models include DialogueRNN  [45] , DialogueCRN  [46] , Joyful  [49]  and MM-DFN  [50] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Quantitative Comparison",
      "text": "To validate the effectiveness of our proposed Sync-TVA model, we conducted comprehensive quantitative experiments on two benchmark datasets (IEMOCAP and MELD) and compared it with 11 state-of -the-art multimodal dialogue emotion recognition methods based on six (IEMOCAP) or seven (MELD) emotion categories and overall accuracy, weighted F1 (WF1) metrics.   2 ), Sync-TVA achieved the best recognition rate in all six emotion categories: Happy 64.12% (+1.03% compared to GraphSmaile), Sadness 84.34% (+1.18%), Neutral 71.45% (+0.38%), Anger 72.21% (+0.83%), Excited 80.08% (+0.42%), and Frustrated 67.93% (+1.09%), with an average improvement of 0.83% per category. Overall, the model's Accuracy is 73.10% and WF1 is 73.35%, which are 0.33% and 0.54% higher than GraphSmile  [51] , respectively. This performance improvement comes from the efficient mining of emotional details by the synchronized temporal-visual attention mechanism and the cross-modal fusion strategy. It's worth noting that \"Sadness\" and \"Excited\" are usually easy to distinguish, while \"Happy\" is the most difficult to identify because of its fuzzy boundaries and diverse expressions. All methods have the lowest performance in this category, but Sync-TVA still achieves a significant advantages.\n\n2) MELD dataset performance: On the MELD dataset (Table  3 ), Sync-TVA also maintains or ties for the lead in seven emotion categories: Neutral 81.20% (+0.60% over DER-GCN  [52] ), Surprise 60.80% (+1.69%), Fear 20.65% (+2.24%), Sadness 44.10% (+2.24%), Joy 66.70% (+1.71%), Disgust 34.50% (+2.07%), and Anger 55.10% (+1.70%). The overall accuracy is 68.25% and WF1 is 67.40%, which are 0.55% and 0.69% higher than GraphSmile  [51] , respectively. The results show that \"Fear\" and \"Disgust\" are very rare emotions, and all methods are difficult to effectively identify, while \"Neutral\" occupies the majority of samples and has the best recognition effect. The steady improvement of Sync-TVA in minority emotions reflects the robustness of the model under the condition of class imbalance.\n\n3) Statistical significance and model comparison: Compared with RNN-based DialogueCRN  [46]  and SAC-LSTM (IEMOCAP accuracy is less than 66%), Sync-TVA acheved a significant improvement of more than 7% on this dataset, highlighting the limitations of pure recursive structures in capturing complex cross-modal emotional clues. Although graph neural network methods (M3Net  [41] , GraphSmile  [51] ) performed relatively well, they were still surpassed by Sync-TVA in terms of Accuracy by 2.18% -7.54%.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. Ablation Study",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "1) Ablation On The Msde Module:",
      "text": "To quantify the contribution of each sub-mechanism in the MSDE module to the model's performance, we designed three sets of ablation configurations based on the complete model: A 1 is to remove the MSDE module to validate its overall effect on dynamic enhancement of multimodal features and noise suppression (three-modal features are directly mapped without applying dynamic gating, self-attention, or residual enhancement operations); A 2 only retains the dynamic gating mechanism to evaluate its independent contribution to information flow filtering (removing the self-attention and residual enhancement branches from MSDE); A3 only retains the self-attention branch to examine its independent effects on feature reweighting and context modelling (removing dynamic gating and residual enhancement branches). All experiments were conducted on the MELD and IEMOCAP datasets using the same data partitioning and training hyperparameters, with each configuration repeated 3 times to take the average, and the final reported metrics are the weighted F1 score (WF1) and the average accuracy (Acc). Compared with the complete model, the WF1 value of A 1 on the MELD dataset is reduced by 4.28%, and on the IEMOCAP dataset is reduced by 3.85%. This result shows that the overall design of MSDE plays a key role in achieving dynamic nose suppression and information enhancement. The performance of A 2 and A 3 both dropped by 1.5-2.6 percentage points, indicating that the gating mechanism and the self-attention mechanism need to work collaboratively to maximize the efficiency of multimodal information extraction. The three sets of ablation experiment results jointly verified the necessity of the gating branch, self-attention branch and their synergy in MSDE, and provided suppot for the design and optimization of subsequent models. 2) Ablation Study on Graph Structure Types: To verify the impact of different graph connection strategies on multimodal emotion recognition performance, we designed three sets of ablation configurations based on the complete model containing triple graphs (V-A, T-V, A-T): B 1 only constructs the visual-acoustic (V-A) graph. and evaluates the contribution of single-modal pair connections to information flow and emotion discrimination by retaining the V-A edge and removing the T-V and A-T edges. B 2 uses a pairwise combination graph (V-A + T-V), retaining the V-A and T-V edges and removing the A-T edge, to examine the potential of partial binary graphs to replace the complete triple graph in order to preserve the information interaction between speech-vision and vision-acoustics. B 3 removes all cross-model edges, directly concatenates the tri-modal features, and feeds them into the fusion module to verify the overall value of the graph structure. All experiments used the same training/validation/testing partition and hyperparameter settingon the MELD and IEMOCAP datasets. Each configuration was repeated 3 times and the average was taken. The weighted F1 (WF1) and average accuracy (Acc) were finally reported.\n\nIn the B 1 configuration, the connection strategy of only retaining the visual-acoustic (V-A) channel leads to a significant decrease in model performance (the WF1 value on the MELD dataset decreased by 4.50% and on the IEMOCAP dataset decreased by 4.55%). This result shows that the interactive channels of speech-vision (T-V) and emotion-acoustic (A-T) also play a key role in multimodal information fusion. When the B 2 configuration uses a pairwise combination graph (V-A +T-V), the performance of the complete model only shows a small decrease of about 1.3%, indicating that the model can still retain most of the cross-modal information interaction capabilities in the case of partial edge connections. However, the performance is slightly lost due to the lack of the supplement of the motion-acoustic channel. The strategy of removing all cross-modal edges and directly concatenating features in the B 3 configuration performed the worst, with the WF1 value reduced by nearly 6% on the MELD dataset. This result verifies the irreplaceable role of graph structures in capturing complex multimodal dependencies. This ablation study provides important empirical evidence for the trade-off between computational efficiency and performance optimization of subsequent models by quantifying the contribution of each edge connection in the triple graph. 3) Ablation Study on Fusion Strategies: To explore the impact of different information interactions and fusion mechanisms on the performance of multimodal emotion recognition, we designed several ablation configurations based on a complete model that incorporates cross-attention fusion (CAF), GRU-style gating mechanisms, and multiple iterative fusion strategies. Specifically, configuration C 1 removes the CAF module and replaces it with a conventional multi-head selfattention mechanism, such that all modalities perform only intra-modal self-attention without engaging in cross-modal weight learning. This setting is intended to verify the effectiveness of cross-modal cross-attention in information selection and reweighting. Configuration C 2 eliminates the GRU-style gating mechanism and substitutes all fused gating units with a \"direct concatenation + fully connected layer\" structure, thereby removing any gating-based filtering operations. This design allows us to evaluate the contribution of the gating mechanism to information screening and memory retention. Configuration D 1 adopts a single-step fusion strategy, wherein the output is produced directly after one application of the \"cross-graph/cross-modal fusion (CAF + gating)\" process, without further iterative refinement. This configuration is designed to examine the difference between single and multiple iterative fusion steps in facilitating fine-grained information interactions.\n\nIn the C 1 configuration, where the cross-attention fusion (CAF) module was removed, the weighted F1 score (WF1) of the model on the MELD dataset decreased by 1.60%, and on the IEMOCAP dataset by 1.25%. These results indicate that the cross-modal cross-attention mechanism can effectively capture dependencies between key modalities and significantly enhance the model's capacity for information reweighting.\n\nFor the C 2 configuration, in which the GRU-style gating mechanism was replaced by a simple concatenation followed by a fully connected layer, the model performance dropped by approximately 1 percentage point across both datasets. This demonstrates the critical role of the gating mechanism in filtering out redundant information and preserving longrange dependencies during fusion. The D 1 configuration, which employed only a single fusion step rather than multiple iterations, exhibited a slight performance degradation compared to the complete model. This observation further confirms that iterative fusion strategies promote more finegrained interactions among multimodal features, leading to additional performance improvements. Overall, this ablation study quantifies the contributions of various fusion components and provides valuable insights for the lightweight design and performance optimization of future multimodal models.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "G. Confusion Matrix Analysis",
      "text": "To better understand the per-class performance of our model, we visualize the confusion matrices for the MELD and IEMOCAP datasets in Figure X. On the MELD dataset, the model shows strong accuracy in identifying Neutral (81.2%), Joy (66.7%), and Anger (55.1%), suggesting that these emotions are relatively easier to distinguish in multiparty conversations. However, emotions such as Fear, Sadness, and Disgust are frequently misclassified, indicating challenges in recognizing subtle and overlapping negative emotions. In particular, Fear is often confused with Sadness (21.5%) and Disgust (15.1%), which may be due to limited distinct cues in textual or acoustic features.\n\nIn contrast, the confusion matrix for IEMOCAP demonstrates significantly better emotion discrimination. The model achieves high classification performance across most emotions, including Sadness (84.3%), Excited (80.1%), Anger (72.2%), and Neutral (71.5%). These results highlight the model's robustness in dyadic conversations with rich multimodal information. Nonetheless, a certain level of confusion is still observed in Frustrated, which is occasionally misclassified as Neutral (6.9%) or Anger (9.7%), possibly due to the ambiguous nature of frustration in real-world interactions.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "H. Visualization",
      "text": "In order to demonstrate the effectiveness of our model in the feature space, we will visually present the original features as well as the features processed by our Sync-TVA model. These images show the features of test samples from the IEMOCAP dataset and the MELD dataset, projected into a two-dimensional space. From the visualization results, we observed that the Sync-TVA model successfully transformed the raw features. The subplots in the upper left and lower left corners show the original features before processing, while the subplots in the upper right and lower right corners display the features after being processed by Sync-TVA. As shown in Figure  3 , in the original feature space, samples of different emotion categories (neutral, surprise, etc.) are relatively scattered and mixed with each other; whereas in the feature space processed by Sync-TVA, samples of the same emotion category cluster more closely, and the distinguishability between different emotion categories is enhanced. This indicates that our Sync-TVA model captures more discriminative features for emotion recognition, effectively optimizing the feature distribution, which helps to better distinguish between different emotions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study presents a novel multimodal emotion recognition model Sync-TVA, designed to enhance the expressive representation of multi-source emotional information and improve cross-modal collaborative understanding. This model finely models the semantic features within each modality through the 'Modal Specific Dynamic Enhancement Module (MSDE)' and constructs three types of heterogeneous graphs-text-visual, visual-audio, and audio-text-in the 'Enhanced Graph Structure Modeling Mechanism', thus achieving a structured representation of multimodal interaction relationships. On this basis, we introduce the 'Deep Information Interaction Fusion Mechanism', utilizing cross-attention and the Gate Fusion Module (GFM) to achieve deep semantic alignment and supplementation between modalities, effectively enhancing the accuracy and generalization capability of emotion classification.\n\nExperiments on the authoritative multimodal emotion recognition datasets MELD and IEMOCAP show that Sync-TVA outperforms various existing advanced methods in terms of weighted F1 socre and overall recognition accuracy, especially demonstrating higher robustness and distinguishability in recognizing minority emotions such as 'fear' and 'disgust'. Ablation experiments further confimed the significant improvement in overall performance brought by the MSDE module, graph structure design, and fusion strategy, providing structural empirical support for further modal optimization. Future research can further develop in three directions: first, introducing a multi-turn dialogue context modeling mechanism to enhance the model's ability to track emotional evolution trends; second, combining contrastive learning or self-supervised pre-training strategies to mitigate training bias caused by class imbalance; third, designing a more adaptive lightweight cross-modal fusion structure to address the issues of noise interference and data loss encountered in practical applications.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed tri-modal emotion recognition framework. Visual, audio, and textual inputs are processed using",
      "page": 5
    },
    {
      "caption": "Figure 2: Confusion matrices of the proposed model on the MELD and IEMOCAP datasets...",
      "page": 8
    },
    {
      "caption": "Figure 3: Visualization of feature space before and after processing by Sync-TVA on MELD and IEMOCAP datasets.",
      "page": 10
    },
    {
      "caption": "Figure 3: , in the original feature space, samples",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tri-Modal Contexture Enforce Graph Construction Deep Information Interaction Fusion Classification\nQ S\nConv\nRoss I'm sorry. I Prediction\nyelled. K\nSad MSDE F v V\nRachel That's V-A Graph K S Conv\nFine.\nLN C CAF Conv Q\nneutral o n GAM\nmiT a c C V\nRoss No, F t seireS e RRooBBEERRTT MSDE n et s al\nbut you're a s\nmad. oit eif\nn r\nAngry\nV\nT-V Graph\nRachel I'm not mad. Q GAM GAM M G A e u t c t i e h d n a a t n n io i c s e n m -\nI'm just not going. F a LN CAF A C tt r e o n s t s io - n OOppeennSSMMIILLEE MSDE Fusion CAF Conv\nAngry MCSADFE Mod D al y it n y a -s m p i e c c ific K S\nConv Enhancement\nRoss Okey. You LN Norm La a y li e za r tion\nknow that I\nhave to go, Element-Wise Multiplication Q V S\nright?\nSad A-T Graph Conv Softmax S": "Tri-Modal Contexture\nRoss I'm sorry. I\nyelled.\nSad\nRachel That's\nFine.\nneutral\nmiT\nseireS e Ross N o ,\nbut y o u 're\nm a d .\nAngry\nRachel I'm not mad.\nI'm just not\ngoing.\nAngry\nRoss Okey. You\nknow that I\nhave to go,\nright?\nSad"
        },
        {
          "Tri-Modal Contexture Enforce Graph Construction Deep Information Interaction Fusion Classification\nQ S\nConv\nRoss I'm sorry. I Prediction\nyelled. K\nSad MSDE F v V\nRachel That's V-A Graph K S Conv\nFine.\nLN C CAF Conv Q\nneutral o n GAM\nmiT a c C V\nRoss No, F t seireS e RRooBBEERRTT MSDE n et s al\nbut you're a s\nmad. oit eif\nn r\nAngry\nV\nT-V Graph\nRachel I'm not mad. Q GAM GAM M G A e u t c t i e h d n a a t n n io i c s e n m -\nI'm just not going. F a LN CAF A C tt r e o n s t s io - n OOppeennSSMMIILLEE MSDE Fusion CAF Conv\nAngry MCSADFE Mod D al y it n y a -s m p i e c c ific K S\nConv Enhancement\nRoss Okey. You LN Norm La a y li e za r tion\nknow that I\nhave to go, Element-Wise Multiplication Q V S\nright?\nSad A-T Graph Conv Softmax S": "miT\ne\neireS"
        },
        {
          "Tri-Modal Contexture Enforce Graph Construction Deep Information Interaction Fusion Classification\nQ S\nConv\nRoss I'm sorry. I Prediction\nyelled. K\nSad MSDE F v V\nRachel That's V-A Graph K S Conv\nFine.\nLN C CAF Conv Q\nneutral o n GAM\nmiT a c C V\nRoss No, F t seireS e RRooBBEERRTT MSDE n et s al\nbut you're a s\nmad. oit eif\nn r\nAngry\nV\nT-V Graph\nRachel I'm not mad. Q GAM GAM M G A e u t c t i e h d n a a t n n io i c s e n m -\nI'm just not going. F a LN CAF A C tt r e o n s t s io - n OOppeennSSMMIILLEE MSDE Fusion CAF Conv\nAngry MCSADFE Mod D al y it n y a -s m p i e c c ific K S\nConv Enhancement\nRoss Okey. You LN Norm La a y li e za r tion\nknow that I\nhave to go, Element-Wise Multiplication Q V S\nright?\nSad A-T Graph Conv Softmax S": "s"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classification\nPrediction": "C\no\nn\na c C\net al\nn s\na s\noit eif\nn r"
        },
        {
          "Classification\nPrediction": "GAM G A u tt i e d n a t n io ce n -\nMechanism\nCAF A C tt r e o n s t s io - n\nFusion\nMCSADFE Mod D al y it n y a -s m p i e c c ific\nEnhancement\nLN Norm La a y li e za r tion\nElement-Wise\nMultiplication\nS Softmax"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "K S\nConv Q\nGAM": "Conv Q"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Conv",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: iterate this process multiple times. For example, we can take presents the statistical details of these two datasets.",
      "data": [
        {
          "Dataset": "",
          "Conversations": "Train",
          "Column_3": "Valid",
          "Column_4": "Test",
          "Utterances": "Train",
          "Column_6": "Valid",
          "Column_7": "Test"
        },
        {
          "Dataset": "MELD",
          "Conversations": "1039",
          "Column_3": "114",
          "Column_4": "280",
          "Utterances": "9989",
          "Column_6": "1109",
          "Column_7": "2610"
        },
        {
          "Dataset": "IEMOCAP",
          "Conversations": "120",
          "Column_3": "",
          "Column_4": "31",
          "Utterances": "5810",
          "Column_6": "",
          "Column_7": "1623"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Exploring emotional connections: A systematic literature review of attachment in human-robot interaction",
      "authors": [
        "J Mitchell",
        "M Jeon"
      ],
      "year": "2025",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "2",
      "title": "Ai, robotics, medicine and health sciences",
      "authors": [
        "N Gasteiger",
        "E Broadbent"
      ],
      "venue": "The Routledge Social Science Handbook of AI. Routledge, 2021"
    },
    {
      "citation_id": "3",
      "title": "A scoping review of the literature on prosodic elements related to emotional speech in human-robot interaction",
      "authors": [
        "N Gasteiger",
        "J Lim",
        "M Hellou",
        "B Macdonald",
        "H Ahn"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "4",
      "title": "A novel emotion recognition system for humanrobot interaction (hri) using deep ensemble classification",
      "authors": [
        "K Zaman",
        "G Zengkang",
        "S Zhaoyun",
        "S Shah",
        "W Riaz",
        "J Ji",
        "T Hussain",
        "R Attar"
      ],
      "year": "2025",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "5",
      "title": "Towards the development of affective facial expression recognition for human-robot interaction",
      "authors": [
        "D Faria",
        "M Vieira",
        "F Faria"
      ],
      "year": "2017",
      "venue": "Proceedings of the 10th international conference on pervasive technologies related to assistive environments"
    },
    {
      "citation_id": "6",
      "title": "Multimodal human-robot interaction for human-centric smart manufacturing: a survey",
      "authors": [
        "T Wang",
        "P Zheng",
        "S Li",
        "L Wang"
      ],
      "year": "2024",
      "venue": "Advanced Intelligent Systems"
    },
    {
      "citation_id": "7",
      "title": "Beyond sentiment analysis: A review of recent trends in text based sentiment analysis and emotion detection",
      "authors": [
        "L Hung",
        "S Alias"
      ],
      "year": "2023",
      "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics"
    },
    {
      "citation_id": "8",
      "title": "Deep learning-based text classification: a comprehensive review",
      "authors": [
        "S Minaee",
        "N Kalchbrenner",
        "E Cambria",
        "N Nikzad",
        "M Chenaghlu",
        "J Gao"
      ],
      "year": "2021",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "9",
      "title": "Building emotional dictionary for sentiment analysis of online news",
      "authors": [
        "Y Rao",
        "J Lei",
        "L Wenyin",
        "Q Li",
        "M Chen"
      ],
      "year": "2014",
      "venue": "World Wide Web"
    },
    {
      "citation_id": "10",
      "title": "Deep convolutional neural networks for sentiment analysis of short texts",
      "authors": [
        "Dos Santos",
        "M Gatti"
      ],
      "year": "2014",
      "venue": "Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers"
    },
    {
      "citation_id": "11",
      "title": "A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification",
      "authors": [
        "Y Zhang",
        "B Wallace"
      ],
      "year": "2015",
      "venue": "A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification",
      "arxiv": "arXiv:1510.03820"
    },
    {
      "citation_id": "12",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "13",
      "title": "Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network",
      "authors": [
        "A Sherstinsky"
      ],
      "year": "2020",
      "venue": "Physica D: Nonlinear Phenomena"
    },
    {
      "citation_id": "14",
      "title": "Text sentiment analysis based on long short-term memory",
      "authors": [
        "D Li",
        "J Qian"
      ],
      "year": "2016",
      "venue": "2016 First IEEE International Conference on Computer Communication and the Internet (ICCCI"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from facial expressions using hybrid feature descriptors",
      "authors": [
        "T Kalsum",
        "S Anwar",
        "M Majid",
        "B Khan",
        "S Ali"
      ],
      "year": "2018",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "16",
      "title": "Using multiple steerable filters and bayesian regularization for facial expression recognition",
      "authors": [
        "H Mahersia",
        "K Hamrouni"
      ],
      "year": "2015",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of 2011 international conference on electronic & mechanical engineering and information technology"
    },
    {
      "citation_id": "18",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "19",
      "title": "Micro-expression recognition based on 3d flow convolutional neural network",
      "authors": [
        "J Li",
        "Y Wang",
        "J See",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "20",
      "title": "Applying segment-level attention on bimodal transformer encoder for audio-visual emotion recognition",
      "authors": [
        "J.-H Hsu",
        "C.-H Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition using deep generalized canonical correlation analysis with an attention mechanism",
      "authors": [
        "Y.-T Lan",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "23",
      "title": "Outlier processing in multimodal emotion recognition",
      "authors": [
        "G Zhang",
        "T Luo",
        "W Pedrycz",
        "M El-Meligy",
        "M Sharaf",
        "Z Li"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Graph-based hierarchical semantic consistency network for remote sensing image-text retrieval",
      "authors": [
        "M Wang",
        "J Guo",
        "B Song",
        "K Su"
      ],
      "year": "2025",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing"
    },
    {
      "citation_id": "25",
      "title": "Fine-grained alignment and interaction for video grounding with crossmodal semantic hierarchical graph",
      "authors": [
        "R Ran",
        "J Wei",
        "S He",
        "Y Zhou",
        "P Wang",
        "Y Yang",
        "H Shen"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "26",
      "title": "Hybrid multi-attention network for audio-visual emotion recognition through multimodal feature fusion",
      "authors": [
        "S Moorthy",
        "Y.-K Moon"
      ],
      "year": "2025",
      "venue": "Mathematics"
    },
    {
      "citation_id": "27",
      "title": "Aia-net: Adaptive interactive attention network for text-audio emotion recognition",
      "authors": [
        "T Zhang",
        "S Li",
        "B Chen",
        "H Yuan",
        "C Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "28",
      "title": "Dynamic and static fusion mechanisms of infrared and visible images",
      "authors": [
        "A Fang",
        "Y Li"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Adaptive fusion techniques for multimodal data",
      "authors": [
        "G Sahu",
        "O Vechtomova"
      ],
      "year": "2019",
      "venue": "Adaptive fusion techniques for multimodal data",
      "arxiv": "arXiv:1911.03821"
    },
    {
      "citation_id": "30",
      "title": "Cortx: Contrastive framework for real-time explanation",
      "authors": [
        "Y.-N Chuang",
        "G Wang",
        "F Yang",
        "Q Zhou",
        "P Tripathi",
        "X Cai",
        "X Hu"
      ],
      "year": "2023",
      "venue": "Cortx: Contrastive framework for real-time explanation",
      "arxiv": "arXiv:2303.02794"
    },
    {
      "citation_id": "31",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "32",
      "title": "Semantic-emotion neural network for emotion recognition from text",
      "authors": [
        "E Batbaatar",
        "M Li",
        "K Ryu"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "33",
      "title": "Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks",
      "authors": [
        "B Liang",
        "H Su",
        "L Gui",
        "E Cambria",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "34",
      "title": "Graph convolutional network with multiple weight mechanisms for aspect-based sentiment analysis",
      "authors": [
        "Z Zhao",
        "M Tang",
        "W Tang",
        "C Wang",
        "X Chen"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "Multimodal emotion recognition in polish (student consortium)",
      "authors": [
        "K Rupauliha",
        "A Goyal",
        "A Saini",
        "A Shukla",
        "S Swaminathan"
      ],
      "year": "2020",
      "venue": "2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)"
    },
    {
      "citation_id": "36",
      "title": "Cfn-esa: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Multi-modal emotion recognition in conversation based on prompt learning with text-audio fusion features",
      "authors": [
        "Y Wu",
        "S Zhang",
        "P Li"
      ],
      "year": "2025",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "38",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "39",
      "title": "Rgat: A deeper look into syntactic dependency information for coreference resolution",
      "authors": [
        "Y Meng",
        "X Pan",
        "J Chang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "40",
      "title": "Sdr-gnn: Spectral domain reconstruction graph neural network for incomplete multimodal learning in conversational emotion recognition",
      "authors": [
        "F Fu",
        "W Ai",
        "F Yang",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2025",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "41",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "S Zhu",
        "H Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "Multimodal decoupled distillation graph neural network for emotion recognition in conversation",
      "authors": [
        "Y Dai",
        "Y Li",
        "D Chen",
        "J Li",
        "G Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "43",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "44",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "45",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "46",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "47",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "48",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "49",
      "title": "Joyful: Joint modality fusion and graph contrastive learning for multimoda emotion recognition",
      "authors": [
        "D Li",
        "Y Wang",
        "K Funakoshi",
        "M Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "50",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "arxiv": "arXiv:2407.21536"
    },
    {
      "citation_id": "52",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "53",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "Y Bao",
        "L Wei",
        "W Zhou",
        "S Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "54",
      "title": "Jiaoshu Liao received his Ph.D. in Computer Science from the University of Warwick, U.K. He is currently a postdoctoral researcher at the University of Glasgow. His research interests include image recognition, image segmentation, image generation, and video analysis. Shuang Wu received an MSc in Computer Systems Engineering from the University of Glasgow (UK) in 2022 and is currently a PhD candidate in the Department of Civil, Environmental & Geomatic Engineering at University College London (UCL), focusing on multimodal emotion recognition. Chongfeng Wei received his Ph.D. degree in mechanical engineering from the University of Birmingham in 2015. He is now an Associate Professor U niversitySeniorLecturer at University of Glasgow, UK. His current research interests include decision-making and control of intelligent vehicles, human-centric autonomous driving, cooperative automation, and dynamics and control of mechanical systems",
      "authors": [
        "V Section Zeyu",
        "Deng"
      ],
      "venue": "Jiaoshu Liao received his Ph.D. in Computer Science from the University of Warwick, U.K. He is currently a postdoctoral researcher at the University of Glasgow. His research interests include image recognition, image segmentation, image generation, and video analysis. Shuang Wu received an MSc in Computer Systems Engineering from the University of Glasgow (UK) in 2022 and is currently a PhD candidate in the Department of Civil, Environmental & Geomatic Engineering at University College London (UCL), focusing on multimodal emotion recognition. Chongfeng Wei received his Ph.D. degree in mechanical engineering from the University of Birmingham in 2015. He is now an Associate Professor U niversitySeniorLecturer at University of Glasgow, UK. His current research interests include decision-making and control of intelligent vehicles, human-centric autonomous driving, cooperative automation, and dynamics and control of mechanical systems"
    }
  ]
}