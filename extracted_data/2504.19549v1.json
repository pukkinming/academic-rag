{
  "paper_id": "2504.19549v1",
  "title": "Deemo: De-Identity Multimodal Emotion Recognition And Reasoning",
  "published": "2025-04-28T07:55:11Z",
  "authors": [
    "Deng Li",
    "Bohao Xing",
    "Xin Liu",
    "Baiqiang Xia",
    "Bihan Wen",
    "Heikki KÃ¤lviÃ¤inen"
  ],
  "keywords": [
    "Affective Computing",
    "Emotion Understanding",
    "Identity-free",
    "Multimodal Large Language Model Data De-identification De-identity Data Intense Clip Selection Human Review LLM Pre-label Human Review NFBL Annotation Qwen2-Audio Qwen2.5-VL Whisper Non Facial Body Language Cues Audio Transcription Cues De-identified Acoustic Cues De-identified Video Cues LLM Pre-label Emotion Recognition Annotation Human Review Emotion Resoning Annotation LLM Generation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion understanding is a critical yet challenging task. Most existing approaches rely heavily on identity-sensitive information, such as facial expressions and speech, which raises concerns about personal privacy. To address this, we introduce the De-identity Multimodal Emotion Recognition and Reasoning (DEEMO), a novel task designed to enable emotion understanding using de-identified video and audio inputs. The DEEMO dataset consists of two subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion Recognition and Reasoning using identityfree cues. This design supports emotion understanding without compromising identity privacy. In addition, we propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates de-identified audio, video, and textual information to enhance both emotion recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves state-of-the-art performance on both tasks, outperforming existing MLLMs by a significant margin, achieving 74.49% accuracy and 74.45% F1-score in de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap in de-identity emotion reasoning. Our work contributes to ethical AI by advancing privacy-preserving emotion understanding and promoting responsible affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion understanding is one of the most fundamental yet challenging tasks  [22, 26] . In recent decades, emotion understanding has attracted a lot of attention from the research community  [14, 15, 31, 46, 51, 60] . However, most existing approaches rely on free? HUMAINE  [13]  V+A+T âœ“ Ã— âœ“ Ã— VAM  [20]  V+A âœ“ Ã— Ã— Ã— IEMOCAP  [5]  V+A+T âœ“ Ã— Ã— Ã— Youtube  [45]  V+A+T âœ“ Ã— Ã— Ã— AFEW  [11]  V+A âœ“ Ã— Ã— Ã— AM-FED  [42]  V âœ“ Ã— Ã— Ã— AFEW-VA  [12]  V+A âœ“ Ã— Ã— Ã— LIRIS-ACCEDE  [4]  V+T âœ“ Ã— âœ“ Ã— EMILYA  [17]  V+T âœ“ Ã— âœ“ Ã— SEWA  [27]  V+A âœ“ Ã— Ã— Ã— CMU-MOSEI  [62]  V+A+T âœ“ Ã— Ã— Ã— iMiGUE  [39]  V+T âœ“ Ã— âœ“ âœ“ MERR  [8]  V+A+T âœ“ âœ“ Ã— Ã— EMER  [33]  V+A+T\n\nRecognition and Reasoning and introduce the corresponding dataset, DEEMO. Our goal is to advance emotion understanding under privacy constraints. To this end, the DEEMO dataset explicitly promotes privacy preservation by excluding identity-sensitive modalities such as facial image/video and raw speech. Instead, it leverages de-identified video and audio to enable emotion recognition and reasoning in a privacy-preserving manner. Specifically, we collect post-match press interview videos, where professional athletes from various sports (e.g., tennis, football, and basketball) respond to multiple rounds of questions following intense competitions.\n\nTo enrich the dataset with high-quality recognition and reasoning annotations, we adopt a semi-automatic pipeline that combines advanced Large Language Models (LLMs) with human review. This approach allows DEEMO to provide both standard emotion recognition labels and detailed reasoning based on de-identified visual cues, de-identified acoustic cues, audio transcription cues, and Non-Facial Body Language (NFBL) cues, as shown in Figure  1 . The DEEMO consists of two subsets: DEEMO-NFBL, which contains 24,722 NFBL annotations of 37 classes, and DEEMO-MER, which includes 2,060 annotated videos with both emotion recognition labels and reasoning instructions via LLM-human collaborative annotation.\n\nTo facilitate this task, we further propose DEEMO-LLaMA, a framework that integrates multimodal de-identified cues for both emotion recognition and reasoning. Through extensive experiments, we demonstrate that our approach outperforms existing baselines under the de-identified setting. We hope that DEEMO can serve as a foundation for future research in privacy-preserving affective computing and responsible AI. Our main contributions are summarized as follows:\n\nâ€¢ We propose new tasks: de-identity multimodal emotion recognition and reasoning. To support this task, we construct the corresponding DEEMO dataset. It consists of two subsets: 1) DEEMO-NFBL, which includes 37 classes of NFBL with a total of 24,722 annotations; 2) DEEMO-MER, which contains 2,060 videos annotated with both emotion recognition and reasoning instructions. Unlike previous datasets that rely on identity-sensitive modalities, DEEMO is built entirely from de-identified video, audio, and transcriptions, making it the first dataset specifically designed for privacy-preserving emotion understanding. â€¢ We propose DEEMO-LLaMA, a novel multimodal framework for de-identified emotion recognition and reasoning. It integrates three modalities: 1) de-identified video, 2) de-identified audio, and 3) audio transcription.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Emotion Understanding Datasets",
      "text": "Various signals have been explored for emotion understanding, including facial expressions  [6, 28, 59]  and speech  [23, 52, 55] . These biological signals provide valuable cues: facial expressions reflect subtle affective states, while speech encodes emotions through prosody and vocal characteristics. However, these signals are inherently identity-sensitive and often raise privacy concerns when deployed in real-world applications. In addition, physiological signals such as electrocardiogram (ECG)  [24] , electroencephalogram (EEG)  [32] , and Galvanic Skin Response (GSR)  [37]  have also been investigated to capture implicit emotional states. However, the acquisition of such signals is often intrusive and impractical in daily interactive scenarios, limiting their applicability. NFBL also plays a crucial role in conveying emotions, particularly in nonverbal communication. Recent efforts, including datasets such as iMiGUE  [39] , SMG  [7] , and MA-52  [21] , have explored NFBL as an alternative modality for emotion understanding  [18, 29] . These works highlight the potential of body language in expressing affective states. However, most existing studies still focus primarily on the recognition of NFBL, while progress on leveraging these cues for higherlevel emotion understanding remains relatively limited. Our work builds upon these insights by focusing on identity-free emotional cues, particularly NFBL. DEEMO promotes privacy-preserving affective computing by leveraging de-identified video and audio inputs alongside NFBL annotations. This allows us to explore emotion understanding in a way that balances expressiveness with privacy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Understanding",
      "text": "Previous approaches to emotion understanding have often focused on unimodal signals  [23, 59]  or relied on relatively simple multimodal fusion techniques  [38, 44] , which struggle to capture the complexity and subtle interplay of emotional cues across modalities. These limitations have hindered the performance and generalization of emotion recognition systems. With the recent advancements in LLMs  [48] , a new direction of research has emerged that leverages the powerful reasoning and representation capabilities of LLMs for emotion understanding. These models enable the integration of rich contextual, semantic, and multimodal information, making it  feasible to perform complex affective reasoning. Representative works in this direction include AffectGPT  [33] , EMER  [34] , and Emotion-LLAMA  [8] , which explore how LLMs can be adapted or extended to recognize and interpret emotions across video, audio, and text inputs. These approaches mark a significant shift toward more flexible and scalable emotion understanding frameworks, where LLMs act not only as language processors but also as central reasoning engines for affective cognition. However, these methods also face privacy concerns, particularly in the visual modality, where they primarily rely on facial information. In contrast, our approach de-identifies multimodal inputs to MLLMs and places greater emphasis on NFBL in the visual modality, providing a more privacy-preserving and ethically aligned solution for emotion understanding.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Deemo Dataset",
      "text": "In this section, we present the construction and overall structure of the DEEMO dataset, which consists of two primary subsets: 1) DEEMO-NFBL, containing a large number of annotated NFBL instances; 2) DEEMO-MER, which provides fine-grained annotations for de-identity multimodal emotion recognition and reasoning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection",
      "text": "In the DEEMO dataset, we use videos of athletes participating in post-game English interviews as the primary data source. This scenario is particularly well-suited for studying emotion understanding for several reasons: 1) The outcome of the game, whether it's a victory or a loss, serves as a natural trigger for emotions, eliciting either positive or negative states in the interviewed player;\n\n2) The players had no (or very little) time to prepare because the press conference would be held immediately after the game, and athletes needed to respond to the questions rapidly. Unlike acting in movies or series, athletes' NFBL is natural; 3) Athletes come from diverse cultural and geographic backgrounds, contributing to the dataset's demographic diversity. Based on these factors, we collect 500 post-game interview videos from 6 sports domains such as tennis, football, basketball, boxing, American football, and UFC via YouTube.\n\nData De-identification. The primary objective of the proposed DEEMO dataset is to facilitate the study of emotion understanding  while preserving privacy. As noted in prior work  [54] , most identity information is embedded in facial and speech signals. To address this, we apply both video and audio de-identification techniques to remove identity information.\n\nFor video de-identification, we employ the CenterFace model  [61] , denoted as ğ‘š face , to locate facial regions in each video frame. Formally, given a video sequence ğ‘‰ = {ğ‘£ 1 , ğ‘£ 2 , . . . , ğ‘£ ğ‘ sample }, where ğ‘£ ğ‘– denotes the ğ‘–-th frame, the facial bounding box coordinates ğº are obtained as ğº = ğ‘š face (ğ‘£ ğ‘– ). We then apply a Gaussian blur over the detected regions based on ğº to produce the de-identified video, denoted as ğ‘‰ de . Examples of de-identified video frames from the subset DEEMO-NFBL are shown in Fig.  2 .\n\nRegarding audio de-identification, our goal is to remove speaker identity information while preserving the emotional content of the audio. Recent work on anonymization-based Speech Emotion Recognition (SER) aligns well with this objective. These approaches focus on modifying the speaker's voice to conceal identity as effectively as possible while preserving both the linguistic content and paralinguistic cues essential for emotion recognition. To this end, we adopt the method proposed in  [58]  for audio de-identification. More specifically, the de-identification process adjusts the spectral envelope of the speech signal using the McAdams coefficient  [41]  to alter speaker-specific characteristics without significantly distorting emotion-related cues. The resulting de-identified audio is denoted as ğ´ de .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Deemo-Nfbl",
      "text": "DEEMO-NFBL Annotation. After data collection and data deidentification, each video sequence is annotated with many NFBLs. NFBL has been shown to be an important cue for understanding  subtle human emotions in affective computing  [49] . According to research on human behavior  [47] , NFBL includes self-manipulations (e.g., scratching the nose, touching the ear), manipulation of objects (e.g., playing with rings, pens, or paper), and self-protective behaviors (e.g., rubbing the eyes and folding arms). In total, 37 NFBL categories are defined, as illustrated in Fig.  3 . We annotate NFBL from the collected athlete interview videos and organize the resulting annotations into a subset of the dataset, which we refer to as DEEMO-NFBL. The annotation process consists of two stages: First, the first group of trained annotators manually label NFBL instances in the videos, recording the start and end timestamps along with the NFBL type at the clip level. Then, a second group of trained annotators reviews and, if necessary, revises the annotations to ensure labeling accuracy and consistency.\n\nDEEMO-NFBL Statistics. Table  2  summarizes the statistics of the DEEMO-NFBL subset. It contains 500 long videos with a total of 24,722 annotated NFBL instances. The average video resolution is 1658Ã—932 with a frame rate of 26.18 FPS. Each video lasts approximately 7 minutes on average, totaling 19.99 hours of de-identified videos. In addition, the DEEMO-NFBL subset demonstrates cultural and demographic diversity. It includes 207 subjects from 75 different nationalities. Moreover, the gender distribution is relatively balanced, comprising 81 female and 126 male participants, which helps mitigate bias in downstream model training.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Deemo-Mer",
      "text": "De-identity Multimodal Emotion Recognition Annotation.\n\nConsidering the nature of scenario post-game interviews with athletes, their emotional expressions may change throughout the video. Therefore, instead of assigning a single global emotion label to the entire video, we adopt a semi-automated annotation strategy that combines LLMs with human review to label emotionally intense clips more precisely. As shown in Fig.  4 , the annotation workflow of emotion recognition is as follows: 1) Given the strong textunderstanding capabilities of LLMs, we first transcribe each video using a speech-to-text tool (e.g., Whisper  [50] ), resulting in an audio transcription denoted as ğ‘‡ ğ‘¡ğ‘Ÿ ; 2) We then feed ğ‘‡ ğ‘¡ğ‘Ÿ into an LLM (e.g., GPT-4o), prompting it to identify one or more clips where the athlete exhibits strong emotional responses based on both the interview questions and the athlete's replies. The prompt of LLM for intense emotion clip selection is present in supplementary material. Each selected clip is cross-reviewed by two human annotators. Specifically, two annotators cross-review each clip to ensure it reflects an intense emotional segment, as initially identified by GPT-4o. Intense clip is\n\n, where ğ‘¡ ğ‘  and ğ‘¡ ğ‘’ denote the start and end timestamps, and\n\nis the corresponding transcription, de-identified audio, de-identified video; 3) For each selected segment, the LLM assigns an initial emotional label (positive or negative). Two human annotators cross-review each labeled clip to verify the accuracy of the emotion classification. The resulting emotion-labeled clips form a subset of the DEEMO dataset, which we name DEEMO-MER.\n\nDe-identity Multimodal Emotion Reasoning Annotation. Our goal is to enable the model to predict not only what emotions are being expressed but also to infer why these emotions occur. To this end, we further explore the use of the DEEMO-MER dataset for a new task: De-identity Multimodal Emotion Reasoning. The pipeline of DEEMO-MER annotation is shown in Fig.  4  and Alg. 1. ğ‘Œ multi [ğ‘¡ğ‘  ,ğ‘¡ğ‘’ ] â† HumanReview(ğ· multi [ğ‘¡ğ‘  ,ğ‘¡ğ‘’ ] ) 17: end for Specifically, we adopt the following approach. First, Qwen2.5-VL  [2]  is used to extract visual contextual description ğ· visual [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] from the de-identified video clip ğ‘‰ de [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , including information such as characters, scenes, and environment which are elements that contribute to interpreting emotional background. Next, Qwen2-Audio  [10]  is employed to process the corresponding de-identified audio clip ğ´ de [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] and extract emotion-relevant acoustic description ğ· acoustic [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , such as pitch and speech rate. Finally, we utilize gpt-4o-2024-08-06  [1]  to aggregate the visual cues ğ· visual [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , acoustic cues ğ· acoustic [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , transcription text ğ‘‡ ğ‘¡ğ‘Ÿ [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , and NFBL annotations ğ‘ ğ¹ ğµğ¿ [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] into a de-identity multimodal emotion reasoning description ğ· ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–\n\n[ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] . To ensure the quality and reliability of the annotations, we introduce a human review step at multiple stages of the pipeline. Specifically, all emotional clips selected by the LLM and the emotion labels assigned to each clip are reviewed by two trained annotators. The annotators are instructed to verify whether the emotional clips are accurately selected and whether the assigned labels (positive or negative) are consistent with the multimodal emotion reasoning description ğ‘Œ ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘– [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] . The full annotation process for the DEEMO-MER dataset, including both de-identified multimodal emotion recognition and reasoning preparation, is summarized in Algorithm 1. The prompt of MLLM (e.g, Qwen2.5-VL, Qwen2-audio, and GPT-4o) for visual, acoustic cues generation and multi-cues aggregation is present in the supplementary material.\n\nDEEMO-MER Statistics. Table  3  summarizes the statistics of the DEEMO-MER. It contains 2,060 de-identified clips, including 1,236 labeled as positive and 824 as negative. In addition, 2060 emotion reasoning instructions are included. An example of emotion reasoning instruction is present in Figure  5 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Deemo-Llama",
      "text": "To support the tasks introduced in DEEMO, namely de-identity multimodal emotion recognition and de-identity multimodal emotion reasoning, we propose an MLLM model called DEEMO-LLaMA, which accepts de-identified video, de-identified audio, and text as inputs. The architecture of DEEMO-LLaMA is shown in Fig.  5 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Architecture",
      "text": "Given a de-identified video ğ‘‰ de\n\n[ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] and de-identified audio ğ´ de [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , we begin by extracting visual and audio features using pre-trained encoders. We uniformly sample ğ‘‡ frames {ğ‘£ de  1 , ğ‘£ de 2 , . . . , ğ‘£ de ğ‘ }, ğ‘£ ğ‘– âˆˆ R ğ» Ã—ğ‘Š Ã—3 from the video. Each frame ğ‘£ de ğ‘– is processed using a frozen ViT-G/14 from EVA-CLIP  [16] . Then, we get ğ‘ tokens of dimension ğ‘‘ ğ‘£ for each frame, resulting in ğ¹ video âˆˆ R ğ‘‡ ğ‘ Ã—ğ‘‘ ğ‘£ . Visual features ğ¹ video are then passed into a pre-trained video Q-Former from BLIP-2  [30] . The Q-Former compresses these into ğ¾ learnable query embeddings of dimension ğ‘‘ ğ‘ , denoted as V ğ‘ âˆˆ R ğ¾ ğ‘£ Ã—ğ‘‘ ğ‘ . The Q-Former is used to compress relevant task-aware information from the dense visual features into learnable queries. For the audio modality, we utilize the pre-trained ImageBind  [19]  as the audio encoder. Given a de-identified audio segment ğ´ de [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , we uniformly sample ğ‘€ short audio clips of 2 seconds ğ´ de [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] = {ğ‘ 1 , ğ‘ 2 , . . . , ğ‘ ğ‘€ }, ğ‘ ğ‘– âˆˆ R ğ¹ Ã—ğ‘‡ ğ‘ , where ğ‘ ğ‘– denotes the spectrogram of the ğ‘–-th audio clip, computed using 128 mel-spectrogram bins (ğ¹ = 128) over ğ‘‡ ğ‘ time frames. We choose ImageBind  [19]  as our audio encoder due to its strong generalization ability across modalities (such as audio-text). Similar to video processing, we use an audio Q-Former to obtain audio embeddings A ğ‘ âˆˆ R ğ¾ ğ‘ Ã—ğ‘‘ ğ‘ , where ğ¾ ğ‘ is the number of output queries and ğ‘‘ ğ‘ is the query embedding dimension. To integrate audio and visual embeddings with textual prompt input, we utilize a set of trainable linear projection layers that map video and audio embeddings into the LLM's input embedding space. V = ğœ visual (V ğ‘ ) and A = ğœ audio (A ğ‘ ) where ğœ visual and ğœ acoustic are modality-specific linear mappings. To format the input for emotion recognition and reasoning in a unified prompt style, we follow an instruction-based template that guides the LLM to attend to different modalities through structured tokens. Given visual tokens V âˆˆ R ğ¾ ğ‘£ Ã—ğ‘‘ llm , audio tokens A âˆˆ R ğ¾ ğ‘ Ã—ğ‘‘ llm and transcription ğ‘‡ ğ‘¡ğ‘Ÿ [ğ‘¡ ğ‘  ,ğ‘¡ ğ‘’ ] , we construct the input sequence ğ‘‹ as:\n\nThis multimodal input sequence ğ‘‹ is then processed by the LLM using its internal cross-attention layers, allowing it to capture emotional correlations across modalities and generate outputs for tasks, such as emotion recognition and reasoning explanations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training",
      "text": "Pre-Training. We first conduct large-scale pre-training using multimodal datasets. Specifically, we leverage WebVid-2M  [3] , a diverse",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Non Facial Body Language Cues",
      "text": "Touching or scratching head, Scratching or touching facial parts, ...",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "De-Identified Visual Cues De-Identified Visual Cues",
      "text": "The video shows a person seated at a table with microphones, suggesting they are participating in an interview or press conference. ...",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "De-Identified Visual Cues",
      "text": "The video shows a person seated at a table with microphones, suggesting they are participating in an interview or press conference. ...",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "De-Identified Acoustic Cues De-Identified Acoustic Cues",
      "text": "The speech in the audio exhibits a slow pace with a consistent, low-pitched tone that fluctuates slightly in intensity. ...",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "De-Identified Acoustic Cues",
      "text": "The speech in the audio exhibits a slow pace with a consistent, low-pitched tone that fluctuates slightly in intensity. ... The model takes de-identified video and audio as input. Each modality is processed by a dedicated encoder followed by a Q-former and a linear projection layer, and the resulting embeddings are integrated into a unified prompt for input to an LLM. In the emotion reasoning answer, different colors are used to highlight how DEEMO-LLaMA leverages various modality-specific cues (visual cues, acoustic cues, audio transcription cues and non facial body language cues) to infer emotional states. collection of video-text pairs, and CC3M-595K  [36] , a image-caption data derived from CC3M  [64] . These datasets provide rich visual semantics aligned with natural language descriptions. During this stage, the visual encoder and visual Q-Former are trained to encode visual content such that the frozen LLM can generate the corresponding captions. The task objective is formulated as a videoto-text generation task, where the model is encouraged to translate visual information into coherent natural language. Although the datasets may not fully capture the emotional or contextual depth of the video, this pre-training phase equips the model with a broad understanding of generic visual concepts.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Audio Transcription Cues",
      "text": "Multimodal Instruction Tuning. Following AffectGPT  [33] , we further refined it through multimodal instruction tuning to enhance its capabilities in both emotion recognition and reasoning. This stage focuses on aligning the model's generative responses with di-identity emotion understanding. The instruction tuning is conducted on our proposed DEEMO-MER-training. Each training sample is formatted as a multimodal prompt comprising audio tokens, visual tokens, and transcription, as illustrated in Eq. 1. The model is trained not only to predict the emotion label but also to generate a detailed explanation of the underlying emotional cause based on multimodal cues. The ID of the DEEMO-MER-training videos is present in the supplemental material.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 5.1 Metrics",
      "text": "For the task of de-identity emotion recognition, the MLLM's predictions are evaluated against the ground-truth emotion labels using standard classification metrics, including Top-1 accuracy and F1score, defined as follows:\n\nFor the task of de-identity multimodal emotion reasoning, we follow the GPT-based evaluation methodology proposed in Emotion-LLaMA  [8] . Specifically, we utilize gpt-4o-mini-2024-07-18 to assess the model outputs by focusing on two key aspects: 1) clue overlap (1-10), the degree of overlap in emotion-related cues and the completeness of the cross-modal reasoning process, and 2) label overlap (1-10), the consistency of the summarized emotional states. The process of GPT-based evaluation of identity multimodal emotion reasoning is shown in supplementary material Algorithm 1.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Following Video-LLaMA  [63] , we use the Vicuna-7B  [9]  model as the language backbone. We uniformly sampled frames 8 per video, each resized to 224 Ã— 224 and processed through a visual encoder, namely, ViT-G/14 from EVA-CLIP  [16] . The audio branch employs ImageBind  [19]  for extracting de-identified acoustic embeddings. Then, Q-Formers  [30]  are used for both modalities, each producing a fixed number of learnable queries (32 for video, 8 for audio), which are further projected to the LLM token space via trainable linear layers. During multimodal instruction tuning, we freeze all encoder backbones and only fine-tune the video/audio Q-Formers and the linear projection heads. We use NVIDIA A100 GPU for training and inference. The batch size is 2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "In the ablation study, we aim to answer two key questions: 1) Does non-facial body language help for de-identity emotion understanding? 2) Is multimodal information necessary for de-identity emotion understandings? To evaluate the impact of different cues, we progressively introduced various types of input cues during the inference, including 1) audio transcription, 2) de-identified video, 3) de-identified audio, and 4) NFBL. The NFBL information will be fed to MLLM in text form. Then, we compared their performance on the emotion recognition task by using DEEMO-MER-testing. The ID of the DEEMO-MER-testing clips is present in the supplemental material. Dose non-facial body language helpful? As shown in Table  4 , we first evaluate four de-identified cues independently: audio transcription, de-identified video, de-identified audio, and NFBL. Among all individual modalities, audio transcription performed the best, achieving 66.67% accuracy. This suggests that the semantic content of the spoken responses retains strong emotional signals. NFBL slightly outperforms audio in terms of F1-score (52.14% vs. 50.12%), highlighting the effectiveness of NFBL in conveying affective states. This supports findings from psychology  [49]  that emphasize the emotional expressiveness of NFBL. By contrast, only de-identified video has the lowest performance, indicating that visual context is less informative when facial expressions are masked.\n\nIs multimodal information necessary for de-identity emotion understandings? As shown in Table  4 , we take the unimodal setting using only audio transcription as the baseline. When additional modalities are introduced, performance consistently improves. For example, combining audio transcription, de-identified video, and de-identified audio achieves an accuracy of 74.49% and an F1-score of 74.45%. These results indicate that each modality contributes complementary information, and integrating them enhances the model's ability to recognize emotions. During multimodal instruction tuning, DEEMO-LLaMA implicitly learns to leverage NFBL cues. To further assess their impact, we additionally use explicit NFBL annotations in textual form as input. The observed performance gain confirms that that NFBL is an effective privacypreserving cue for emotion understanding. However, due to the current lack of reliable NFBL detection models, we use audio transcription, de-identified video, and de-identified audio as the final input to our model, as shown in Figure  5 .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Comparative Methods",
      "text": "To evaluate the capability of recent MLLMs in the task of de-identity multimodal emotion understanding, we conduct a comprehensive comparison on the DEEMO-MER-testing set. The evaluated MLLMs are categorized into three groups: 1) General-purpose Video-based MLLMs: including VideoChatGPT  [40] , Video-LLaVA  [35] , Chat-UniVi  [25] , and Movie-Chat  [56] ; 2) General-purpose Video-Audiobased MLLMs: including Video-LLaMA  [63]  and PandaGPT  [57] ; Emotional Video-Audio-based MLLMs: including Emotion-LLaMA  [8]  and AffectGPT  [33] .To ensure fair comparisons, we use the 7B version for all models, follow their official implementations, and utilize their pre-trained weights.\n\nDe-identity Mutlimoda Emotion Recognition. In the Table  5 , DEEMO-LLaMA achieves the best performance on the emotion recognition task, with an accuracy of 74.49% and an F1-score of 74.45%, significantly outperforming all other models across the three MLLM categories. Compared with general-purpose videoonly MLLMs (e.g., VideoChatGPT, Video-LLaVA, Chat-UniVi, and Movie-Chat), models that incorporate audio inputs (e.g., Video-LLaMA, PandaGPT) perform notably better, which highlights the Table  6 : An example of de-identity multimodal emotion reasoning comparing the proposed DEEMO-LLaMA with other MLLMs. Incorrect reasoning is marked in red, and correct reasoning in green.\n\nAudio transcription \"She was striking the ball really good because I gave her this chance. \" Ground Truth \"Based on the cues provided, the athlete's emotion appears to lean towards the negative. The transcription reveals a sense of blame or regret in the statement about her being given a chance, indicating that the athlete might not feel entirely satisfied or happy with the outcome. The body language shows illustrative hand gestures and the act of scratching or touching facial parts, which can be signs of nervousness or discomfort, potentially hinting at underlying frustration or disappointment. In addition, the audio cues suggest a calm and composed delivery, but the monotone quality of the speech, lacking variations in pitch or intensity, might reflect a sense of resignation rather than enthusiasm or positivity. Overall, the combination of these elements suggests an emotional state that is not particularly upbeat and leans towards a more negative sentiment regarding the situation. \" Video-LLaMA  [63]  \"The athlete's emotion is positive. During the post-match interview, she is smiling and seems content with her performance. She even thanks her coach for giving her the chance to strike the ball really well. Therefore, her emotions are positive. \" Emotion-LLaMA  [57]  \"the athlete's emotion is positive, as they express gratitude and acknowledgement towards their coach for providing them with the opportunity to strike the ball well\" AffectGPT  [63]  \"In the video, an athlete wearing a green shirt can be seen sitting in a conference room with a microphone and a table full of various drinks and bottles. The athlete is interviewed by a journalist after a match. The audio cues indicate that the athlete is speaking loudly and confidently, with a positive tone. The video cues show the athlete sitting in a conference room, speaking into a microphone, and looking directly at the camera. The non-facial body language cues include the athlete's posture, which is upright and confident, and the fact that they are speaking loudly. Overall, the video and audio suggest a positive and confident mood from the athlete.\" DEEMO-LLaMA (Ours) \"Based on the provided cues, the athlete's emotion is more likely 'negative. ' The transcription indicates a focus on what could have been done better, specifically emphasizing \"gave her this chance, \" which suggests disappointment or frustration with a performance. The non-facial body language, specifically touching or scratching the face, often indicates discomfort or anxiety, which aligns with a negative emotional state. In the audio, the slow-paced, monotone delivery lacking in fluctuations could imply a deflated or depressed mood, contrasting with the uplifting emotions associated with a positive experience. Collectively, these elements lead to the inference that the athlete is grappling with negative feelings about the situation.\"\n\nimportance of acoustic information. However, even among emotional video-audio MLLMs, such as Emotion-LLaMA and Affect-GPT, performance remains limited. This suggests that existing MLLMs struggle to generalize emotion understanding in a privacypreserving, de-identified emotion understanding. In contrast, DEEMO-LLaMA effectively leverages all available identity-agnostic cues (transcription, acoustic, visual, and NFBL). The prompt for MLLMs of emotion recognition is present in the supplementary material.\n\nDe-identity Mutlimoda Emotion Reasoning. DEEMO-LLaMA achieves the best performance with a clue overlap of 6.20, a label overlap of 7.66, and an overall average of 6.93, as shown in Table  5 . These results suggest that DEEMO-LLaMA is not only capable of correctly identifying emotional state, but also excels at generating coherent and well-grounded reasoning based on multimodal cues (e.g., visual, acoustic, and NFBL). The prompt for MLLMs of emotion reasoning is present in the supplementary material.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "To better understand the reasoning ability of different MLLMs under the de-identity setting, we provide a qualitative comparison in Table  6 . Given the same de-identified video, audio, and transcription inputs, only DEEMO-LLaMA correctly infers a negative emotional state and supports its inference using diverse modality-specific evidence. Specifically, it captures the cues: 1) The athlete's phrasing (\"gave her this chance\") may imply regret or frustration; 2) NFBL (e.g., touching or scratching the face), which conveys anxiety; 3) Acoustic patterns like a low-pitched, monotonous delivery, which reflect a subdued emotional tone. In contrast, other MLLMs, such as VideoLLaMA, Emotion-LLaMA, and AffectGPT, incorrectly predict a positive emotion, relying primarily on superficial features such as word sentiment or posture while overlooking more subtle, affectrich signals. This highlights a key advantage of DEEMO-LLaMA: its ability to reason over implicit, privacy-preserving cues and deliver well-grounded reasoning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce novel tasks called De-identity Multimodal Emotion Recognition and Reasoning. To support this task, we construct the DEEMO dataset, which includes two subsets: DEEMO-NFBL, which contains 24,722 Non Facial Body Language (NFBL) annotations, and DEEMO-MER, which includes 2,060 annotated videos with both emotion labels and reasoning instructions through LLM-human collaborative annotation. To the best of our knowledge, DEEMO is the first dataset specifically designed for privacy-preserving emotion understanding for Multimodal Large Language Models (MLLMs). Building on this foundation, we propose DEEMO-LLaMA, a unified MLLM that integrates de-identified cues across video, audio, and text. Through extensive experiments, we demonstrate that DEEMO-LLaMA achieves state-of-the-art performance on both de-identified emotion recognition and emotion reasoning, outperforming existing MLLMs by a significant margin. Our results highlight the importance of leveraging implicit and privacy-preserving emotional signals for interpretable affective modeling. We hope that our dataset and framework can inspire future research in privacy-preserving emotion understanding and ethical human-centered AI.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Previous multimodal emotion understanding tends",
      "page": 1
    },
    {
      "caption": "Figure 1: To explore emotion understanding beyond identity-related fea-",
      "page": 1
    },
    {
      "caption": "Figure 1: The DEEMO",
      "page": 2
    },
    {
      "caption": "Figure 2: Exapmles of DEEMO-NFBL.",
      "page": 3
    },
    {
      "caption": "Figure 3: Statistical visualization of the DEEMO dataset. The",
      "page": 3
    },
    {
      "caption": "Figure 2: Regarding audio de-identification, our goal is to remove speaker",
      "page": 3
    },
    {
      "caption": "Figure 4: Overview of the annotation workflow for the DEEMO-MER. The process consists of three stages: 1) data pre-labeling via",
      "page": 4
    },
    {
      "caption": "Figure 3: We annotate NFBL",
      "page": 4
    },
    {
      "caption": "Figure 4: , the annotation work-",
      "page": 4
    },
    {
      "caption": "Figure 4: and Alg. 1.",
      "page": 4
    },
    {
      "caption": "Figure 5: Overview of the proposed DEEMO-LLaMA framework. The model takes de-identified video and audio as input. Each",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Audio transcription": "Ground Truth",
          "\"She was striking the ball really good because I gave her this chance.\"": "\"Based on the cues provided, the athleteâ€™s emotion appears to lean towards the negative. The transcription reveals a sense\nof blame or regret in the statement about her being given a chance, indicating that the athlete might not feel entirely\nsatisfied or happy with the outcome. The body language shows illustrative hand gestures and the act of scratching or\ntouching facial parts, which can be signs of nervousness or discomfort, potentially hinting at underlying frustration or\ndisappointment. In addition, the audio cues suggest a calm and composed delivery, but the monotone quality of the speech,\nlacking variations in pitch or intensity, might reflect a sense of resignation rather than enthusiasm or positivity. Overall,\nthe combination of these elements suggests an emotional state that is not particularly upbeat and leans towards a more\nnegative sentiment regarding the situation.\""
        },
        {
          "Audio transcription": "Video-LLaMA [63]",
          "\"She was striking the ball really good because I gave her this chance.\"": "\"The athleteâ€™s emotion is positive. During the post-match interview, she is smiling and seems content with her performance.\nShe even thanks her coach for giving her the chance to strike the ball really well. Therefore, her emotions are positive.\""
        },
        {
          "Audio transcription": "Emotion-LLaMA [57]",
          "\"She was striking the ball really good because I gave her this chance.\"": "\"the athleteâ€™s emotion is positive, as they express gratitude and acknowledgement towards their coach for providing them\nwith the opportunity to strike the ball well\""
        },
        {
          "Audio transcription": "AffectGPT [63]",
          "\"She was striking the ball really good because I gave her this chance.\"": "\"In the video, an athlete wearing a green shirt can be seen sitting in a conference room with a microphone and a table full\nof various drinks and bottles. The athlete is interviewed by a journalist after a match. The audio cues indicate that the\nathlete is speaking loudly and confidently, with a positive tone. The video cues show the athlete sitting in a conference\nroom, speaking into a microphone, and looking directly at the camera. The non-facial body language cues include the\nathleteâ€™s posture, which is upright and confident, and the fact that they are speaking loudly. Overall, the video and audio\nsuggest a positive and confident mood from the athlete.\""
        },
        {
          "Audio transcription": "DEEMO-LLaMA (Ours)",
          "\"She was striking the ball really good because I gave her this chance.\"": "\"Based on the provided cues, the athleteâ€™s emotion is more likely â€˜negative.â€™ The transcription indicates a focus on what\ncould have been done better, specifically emphasizing \"gave her this chance,\" which suggests disappointment or frustration\nwith a performance. The non-facial body language, specifically touching or scratching the face, often indicates discomfort\nor anxiety, which aligns with a negative emotional state. In the audio, the slow-paced, monotone delivery lacking in\nfluctuations could imply a deflated or depressed mood, contrasting with the uplifting emotions associated with a positive\nexperience. Collectively, these elements lead to the inference that the athlete is grappling with negative feelings about the\nsituation.\""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Qwen2. 5-vl technical report",
      "authors": [
        "Shuai Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Sibo Song",
        "Kai Dang",
        "Peng Wang",
        "Shijie Wang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-vl technical report",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "3",
      "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
      "authors": [
        "Max Bain",
        "Arsha Nagrani",
        "GÃ¼l Varol",
        "Andrew Zisserman"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "LIRIS-ACCEDE: A video database for affective content analysis",
      "authors": [
        "Yoann Baveye",
        "Emmanuel Dellandrea",
        "Christel Chamaret",
        "Liming Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "Felipe Zago Canal",
        "Tobias Rossi MÃ¼ller",
        "Cristine Jhennifer",
        "Gustavo Matias",
        "Antonio Gino Scotton",
        "Junior Reis De Sa",
        "Eliane Pozzebon",
        "Antonio Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "7",
      "title": "SMG: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "Haoyu Chen",
        "Henglin Shi",
        "Xin Liu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2023",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2025",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez",
        "Ion Stoica",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "citation_id": "10",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "11",
      "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia",
      "doi": "10.1109/MMUL.2012.26"
    },
    {
      "citation_id": "12",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "Abhinav Dhall",
        "Roland Ov Ramana Murthy",
        "Jyoti Goecke",
        "Tom Joshi",
        "Gedeon"
      ],
      "year": "2015",
      "venue": "ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "13",
      "title": "The HUMAINE database: Addressing the collection and annotation of naturalistic and induced emotional data",
      "authors": [
        "Ellen Douglas-Cowie",
        "Roddy Cowie",
        "Ian Sneddon",
        "Cate Cox",
        "Orla Lowry",
        "Margaret Mcrorie",
        "Jean-Claude Martin",
        "Laurence Devillers",
        "Sarkis Abrilian",
        "Anton Batliner"
      ],
      "year": "2007",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "14",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "16",
      "title": "Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Yuxin Fang",
        "Wen Wang",
        "Binhui Xie",
        "Quan Sun",
        "Ledell Wu",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "Emilya: Emotional body expression in daily actions database",
      "authors": [
        "Nesrine Fourati",
        "Catherine Pelachaud"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "18",
      "title": "Identity-free artificial emotional intelligence via micro-gesture understanding",
      "authors": [
        "Rong Gao",
        "Xin Liu",
        "Bohao Xing",
        "Zitong Yu",
        "Bjorn Schuller",
        "Heikki KÃ¤lviÃ¤inen"
      ],
      "year": "2024",
      "venue": "Identity-free artificial emotional intelligence via micro-gesture understanding",
      "arxiv": "arXiv:2405.13206"
    },
    {
      "citation_id": "19",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "21",
      "title": "Benchmarking micro-action recognition: Dataset, methods, and applications",
      "authors": [
        "Dan Guo",
        "Kun Li",
        "Bin Hu",
        "Yan Zhang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "22",
      "title": "Emotion analysis: A survey",
      "authors": [
        "Nida Manzoor Hakak",
        "Mohsin Mohd",
        "Mahira Kirmani",
        "Mudasir Mohd"
      ],
      "year": "2017",
      "venue": "2017 international conference on computer, communications and electronics (COMPTELIX)"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition approaches: A systematic review",
      "authors": [
        "Ahlam Hashem",
        "Muhammad Arif",
        "Manal Alghamdi"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "24",
      "title": "Automatic ECG-based emotion recognition in music listening",
      "authors": [
        "Yu-Liang Hsu",
        "Jeen-Shing Wang",
        "Wei-Chun Chiang",
        "Chien-Han Hung"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Wancai Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "28",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "Lina Eva G Krumhuber",
        "Harold Ch Skora",
        "Karen Hill",
        "Lander"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "29",
      "title": "Enhancing micro gesture recognition for emotion understanding via context-aware visual-text contrastive learning",
      "authors": [
        "Deng Li",
        "Bohao Xing",
        "Xin Liu"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "30",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "EEG based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "Comput. Surveys"
    },
    {
      "citation_id": "33",
      "title": "AffectGPT: Dataset and framework for explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "AffectGPT: Dataset and framework for explainable multimodal emotion recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "35",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Yang Ye",
        "Bin Zhu",
        "Jiaxi Cui",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "36",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing systems"
    },
    {
      "citation_id": "37",
      "title": "Retracted: Human emotion recognition based on galvanic skin response signal feature selection and svm",
      "authors": [
        "Mingyang Liu",
        "Di Fan",
        "Xiaohan Zhang",
        "Xiaopeng Gong"
      ],
      "year": "2016",
      "venue": "International Conference on Smart City and Systems Engineering"
    },
    {
      "citation_id": "38",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "39",
      "title": "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "Xin Liu",
        "Henglin Shi",
        "Haoyu Chen",
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Khan"
      ],
      "year": "2024",
      "venue": "62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Spectral fusion, spectral parsing and the formation of auditory images",
      "authors": [
        "Stephen Edward"
      ],
      "year": "1984",
      "venue": "Spectral fusion, spectral parsing and the formation of auditory images"
    },
    {
      "citation_id": "42",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "43",
      "title": "Privacy-enhancing face biometrics: A comprehensive survey",
      "authors": [
        "Peter BlaÅ¾ Meden",
        "Philipp Rot",
        "Naser TerhÃ¶rst",
        "Arjan Damer",
        "Kuijper",
        "J Walter",
        "Arun Scheirer",
        "Peter Ross",
        "Vitomir Peer",
        "Å truc"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "44",
      "title": "Deep learning based multimodal emotion recognition using model-level fusion of audio-visual modalities",
      "authors": [
        "Asif Iqbal Middya",
        "Baibhav Nag",
        "Sarbani Roy"
      ],
      "year": "2022",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "45",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "46",
      "title": "A review on sentiment analysis and emotion detection from text",
      "authors": [
        "Pansy Nandwani",
        "Rupali Verma"
      ],
      "year": "2021",
      "venue": "Social network analysis and mining"
    },
    {
      "citation_id": "47",
      "title": "What every body is saying",
      "authors": [
        "Julia Navarro",
        "Marvin Karlins"
      ],
      "year": "2008",
      "venue": "What every body is saying"
    },
    {
      "citation_id": "48",
      "title": "A comprehensive overview of large language models",
      "year": "2023",
      "venue": "A comprehensive overview of large language models",
      "arxiv": "arXiv:2307.06435"
    },
    {
      "citation_id": "49",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Adrian Ciprian",
        "Dorota Corneanu",
        "Tomasz KamiÅ„ska",
        "Sergio SapiÅ„ski",
        "Gholamreza Escalera",
        "Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "50",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "51",
      "title": "A multimodal emotion recognition system using facial landmark analysis",
      "authors": [
        "Farhad Rahdari",
        "Esmat Rashedi",
        "Mahdi Eftekhari"
      ],
      "year": "2019",
      "venue": "Iranian Journal of Science and Technology"
    },
    {
      "citation_id": "52",
      "title": "Analyzing the influence of different speech data corpora and speech features on speech emotion recognition: A review",
      "authors": [
        "Tarun Rathi",
        "Manoj Tripathy"
      ],
      "year": "2024",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "53",
      "title": "General data protection regulation",
      "year": "2018",
      "venue": "Intouch"
    },
    {
      "citation_id": "54",
      "title": "On the use of speech and face information for identity verification",
      "authors": [
        "Conrad Sanderson",
        "Kuldip K Paliwal"
      ],
      "year": "2004",
      "venue": "On the use of speech and face information for identity verification"
    },
    {
      "citation_id": "55",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Beer Youddha",
        "Shivani Singh",
        "Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "56",
      "title": "Moviechat: From dense token to sparse memory for long video understanding",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Guanhong Wang",
        "Yucheng Zhang",
        "Haoyang Zhou",
        "Feiyang Wu",
        "Haozhe Chi",
        "Xun Guo",
        "Tian Ye",
        "Yanting Zhang"
      ],
      "year": "2024",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Pandagpt: One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "58",
      "title": "The VoicePrivacy 2024 Challenge Evaluation Plan",
      "authors": [
        "Natalia Tomashenko",
        "Xiaoxiao Miao",
        "Pierre Champion",
        "Sarina Meyer",
        "Xin Wang",
        "Emmanuel Vincent",
        "Michele Panariello",
        "Nicholas Evans",
        "Junichi Yamagishi",
        "Massimiliano Todisco"
      ],
      "year": "2024",
      "venue": "The VoicePrivacy 2024 Challenge Evaluation Plan",
      "arxiv": "arXiv:2404.02677[eess.AS]"
    },
    {
      "citation_id": "59",
      "title": "Learning facial expression and body gesture visual information for video emotion recognition",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Xinyu Yang",
        "Anh Luu",
        "Yizhuo Dong"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "60",
      "title": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "authors": [
        "Bohao Xing",
        "Zitong Yu",
        "Xin Liu",
        "Kaishen Yuan",
        "Qilang Ye",
        "Weicheng Xie",
        "Huanjing Yue",
        "Jingyu Yang",
        "Heikki KÃ¤lviÃ¤inen"
      ],
      "year": "2024",
      "venue": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "arxiv": "arXiv:2408.11424"
    },
    {
      "citation_id": "61",
      "title": "CenterFace: joint face detection and alignment using face as point",
      "authors": [
        "Yuanyuan Xu",
        "Wan Yan",
        "Genke Yang",
        "Jiliang Luo",
        "Tao Li",
        "Jianan He"
      ],
      "year": "2020",
      "venue": "Scientific Programming"
    },
    {
      "citation_id": "62",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "63",
      "title": "Video-LLaMA: An Instructiontuned Audio-Visual Language Model for Video Understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "64",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}