{
  "paper_id": "2510.03986v1",
  "title": "A Multilingual Framework For Dysarthria: Detection, Severity Classification, Speech-To-Text, And Clean Speech Generation",
  "published": "2025-10-05T00:52:04Z",
  "authors": [
    "Ananya Raghu",
    "Anisha Raghu",
    "Nithika Vivek",
    "Sofie Budman",
    "Omar Mansour"
  ],
  "keywords": [
    "dysarthria",
    "signal processing",
    "machine learning",
    "automatic speech recognition",
    "speech synthesis",
    "voice cloning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dysarthria is a motor speech disorder that results in slow and often incomprehensible speech. Speech intelligibility significantly impacts communication, leading to barriers in social interactions. Dysarthria is often a characteristic of neurological diseases including Parkinson's and ALS, yet current tools lack generalizability across languages and levels of severity. In this study, we present a unified AI-based multilingual framework that addresses six key components: (1) binary dysarthria detection, (2) severity classification, (3) clean speech generation, (4) speechto-text conversion, (5) emotion detection, and (  6 ) voice cloning. We analyze datasets in English, Russian, and German, using spectrogram-based visualizations and acoustic feature extraction to inform model training. Our binary detection model achieved 97% accuracy across all three languages, demonstrating strong generalization across languages. The severity classification model also reached 97% test accuracy, with interpretable results showing model attention focused on lower harmonics. Our translation pipeline, trained on paired Russian dysarthric and clean speech, reconstructed intelligible outputs with low training (0.03) and test (0.06) L1 losses. Given the limited availability of English dysarthric-clean pairs, we finetuned the Russian model on English data and achieved improved losses of 0.02 (train) and 0.03 (test), highlighting the promise of cross-lingual transfer learning for low-resource settings. Our speech-to-text pipeline achieved a Word Error Rate of 0.1367 after three epochs, indicating accurate transcription on dysarthric speech and enabling downstream emotion recognition and voice cloning from transcribed speech. Overall, the results and products of this study can be used to diagnose dysarthria and improve communication and understanding for patients across different languages.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Problem Statement",
      "text": "Dysarthria is a motor speech disorder and a common symptom of neurological conditions such as ALS, Parkinson's disease, stroke, and cerebral palsy  [1] . It arises when the nervous system damage impairs the muscles involved in speaking, leading to slurred, slow speech that is difficult to understand  [2] . While not a disease itself, dysarthria significantly impairs communication and quality of life, frequently leading to social isolation, misdiagnosis, or reduced access to care. Studies report that dysarthria occurs in up to 60% of stroke patients and affects as many as 90% of individuals with Parkinson's * Equal contribution.\n\ndisease  [3] . Despite its prevalence, Dysarthria is often underrecognized, particularly in its milder forms or in multilingual populations  [4] . A recent study demonstrated that a listener's native language significantly influences their perceptual ratings of dysarthria, particularly for articulatory and rhythmic characteristics  [5] . This highlights a fundamental limitation in human-based assessment, as a clinician's ability to accurately perceive and rate a speaker's dysarthria can be compromised when they are not a native speaker of the language. This suggests that current diagnostic methods that rely on subjective evaluations by speech-language pathologists are constrained by language familiarity and clinical access. Furthermore, they are prone to human error and bias, which can delay proper treatment, especially for early stage dysarthria  [6] ,  [7] .\n\nIn contrast, machine learning models trained on diverse, labeled datasets offer an objective alternative to human assessments. By extracting language-agnostic acoustic features and learning features across speech samples, ML can reduce diagnostic bias and enable more consistent screening. This makes machine learning based tools especially promising for accessible dysarthria detection across diverse healthcare settings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Prior Machine Learning Approaches",
      "text": "1) Prior work: Dysarthria Detection: Recent advances in machine learning have led to the development of models capable of detecting dysarthria using acoustic features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectrograms, or prosodic cues. Prior work has focused largely on binary classification, distinguishing dysarthric from healthy speech, using convolutional or recurrent neural networks trained on datasets like TORGO and UA-Speech  [8] ,  [9] . However, these models are typically trained and evaluated on a single language, limiting their clinical applicability across multilingual populations.\n\n2) Prior Work: Severity Classification: While recent approaches  [10]  to dysarthria severity classification have received high performance using neural networks, they are often black boxes, not explaining the reasoning behind model classification. Furthermore, features extracted for the model, including embeddings from wav2vec2  [10] , do not offer insight on which acoustic characteristics of the slurred speech distinguish it from clean or less severe dysarthric speech. An interpretable model can thereby give insight as to which features of the speech are altered in dysarthria, helping with speech therapy and increasing patient understanding.\n\n3) Prior work: Speech Synthesis: Prior research has explored several generative voice conversion and augmentation approaches for translating dysarthric speech to regular speech. The CycleGAN-VC model architecture was applied to Korean dysarthric speech (18700) utterances and healthy controls reducing Word-Error-Rate by 33.4%  [11] . However, CycleGANs often produce artifacts especially in highly impaired speech and pixel level consistency can potentially be problematic and cause unrealistic images  [12] . The DVC 3.1 system, which combines data augmentation with a StarGAN-VC backbone  [13]  improved both ASR word recognition and listener ratings. Still, the quality of generated speech heavily depends on the synthetic data distribution and can degrade for highly variable input. More recently, diffusion-based voice conversion with Fuzzy Expectation Maximization (FEM)  [14]  improved intelligibility and accuracy using soft clustering, but diffusion models are typically slow to sample.\n\n4) Prior work: Speech to Text, Emotion, and Voice Cloning: Prior work has explored speech to text pipelines on patients with dysarthria, marking the first step towards increased patient understanding  [15] . However, these methods have stopped at the transcription stage and have not progressed to sentiment classification or synthetic speech generation. In order to improve communication and understanding of dysarthric patients, it is crucial to provide this population with a way to communicate using acoustic features of their voice prior to their dysarthric diagnosis, enabling better understanding of sentiment and needs.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "1) Datasets:",
      "text": "We utilized four primary datasets in this study. The TORGO dataset  [16]  provides paired audio samples and textual prompts from individuals with and without dysarthria, supporting analysis of articulatory impairments in English Speech. From this corpus, we accessed a subset of approximately 2,000 audio files (500 each for female non-dysarthric, female dysarthric, male dysarthric, and male non-dysarthric speakers) made available on Kaggle  [17] , and paired them with textual prompts sourced from a separate Kaggle dataset  [18] . The UA Dysarthria dataset  [19]  was used for severity classification and includes 11,436 speech spectrograms labeled across 4 severity levels: very low, low, medium, high. For Russianlanguage data, the Hyperkinetic Dysarthria Speech dataset  [20]  was utilized, providing 2000 samples from both Dysarthric and non-dysarthric patients reciting the same phrase, along with corresponding prompts. Additionally, the Dysarthric German dataset  [21]  contributed 1,272 samples of Dysarthric German speech for crosslingual prediction.\n\n2) Initial Feature Extraction: To gain initial insight into speech patterns associated with dysarthria, we visualized spectrograms across gender and condition (dysarthric vs nondysarthric). In the highlighted regions shown in Figure  1 , we observe that dysarthric speech tends to exhibit prolonged",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dysarthria Detection Task",
      "text": "Building on these qualitative differences, we extracted quantitative features for classification using Mel-Frequency Cepstral Coefficients (MFCCs), a widely used representation in speech processing. The pipeline begins with a Fast Fourier Transform to convert the raw audio into its frequency spectrum, followed by a logarithmic amplitude scaling that mimics human loudness perception. Mel scaling is then applied to emphasize perceptually relevant frequency bands. Finally, a Discrete Cosine Transform reduces dimensionality while preserving key spectral features. The resulting MFCCs capture articulatory and phonatory characteristics that are especially relevant for identifying dysarthric patterns.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Severity Classification Task",
      "text": "The second stage of our project involved classifying each dysarthric patient's severity level as an indicator of how far along they are in their progression towards more serious diseases like ALS and Parkinson's. Accurate severity diagnosis can help a patient tailor their healthcare plan and treatment accordingly to better suit their needs  [22] .\n\n11436 spectrogram images of dysarthric audio were downloaded from Kaggle, resized to 128x128, converted to an array, and normalized. Class labels belonging to high severity (3036/11426), medium severity (2295/11426), low severity (2280/11426), and very low severity (3825/11426) were onehot-encoded. The dataset split into train, test, and validation (70-20-10).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Multi-Lingual Clean Speech Synthesis",
      "text": "The third component of the proposed framework is a two-stage pipeline to translate dysarthric speech into normal speech. Translating dysarthric speech into more understandable forms is crucial because reduced intelligibility severely limits a dysarthric patient's ability to engage in daily conversations  [23] . In the first stage of our pipeline, we trained a U-Net model to map dysarthric speech to clean speech using Russian speech. As outlined in Figure  4 , raw .wav files were converted into mel spectrograms using Librosa, then rescaled in decibels and resized to a uniform 128x128 image for consistent model input. These paired spectrograms were saved as .npy files for training. The U-Net model was trained for 300 epochs, with a learning rate of 1e-4, to output a clean spectrogram from a dysarthric spectrogram, and the learned weights were exported. This step allows the model to learn structural transformations between distorted and healthy speech, that generalize across languages due to shared characteristics. 2) Stage 2: Adapting Russian Model to English Dysarthric Speech: Although the Torgo database contains substantial English dysarthric speech, we encountered a key challenge: very few clean-dysarthric pairs were spoken with the same text, which is necessary for paired spectrogram training. To construct a usable dataset, we manually filtered for matched male and female speakers saying the same sentences. After preprocessing, we identified only 190 matched female and 37 matched male samples. These were processed into mel spectrograms using the same pipeline as in Stage 1. To address the limitations of training from scratch, we leveraged our U-Net model trained on the larger Russian dataset which had already learned to correct dysarthric distortions and followed the steps shown in Figure  5 . We processed English dysarthric audio using the same preprocessing steps outlined in Phase 1, then initialized the model with Russian weights. We then fine tuned the model using the small available English dataset for 300 epochs and observed an improved performance over models trained from scratch. This cross-lingual transfer approach demonstrates how transfer learning can be used to compensate for scarcity of data and can be potentially applied to lowresource languages.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Automatic Speech Recognition, Emotion Classification, And Voice Cloning",
      "text": "The overall pipeline for automatic speech recognition, emotion classification, and voice cloning is shown in Figure  6 . We start by converting the audio to text, then use the text to classify emotion and perform voice cloning.\n\n1) Speech to Text: The fourth component of our pipeline is a speech-to-text converter for dysarthric audio as shown in Figure  7 . This feature is crucial for increased communication for dysarthric patients, reducing the impact of dysarthria on their daily lives.\n\nThe dataset was obtained from Kaggle, containing audio files of patients with and without dysarthria as well as their corresponding text. Our approach is to fine-tune an existing speech-to-text model on audio files of patients with dysarthria, thereby increasing the ability of these existing frameworks to comprehend slurred and slowed speech.\n\nAfter matching each audio file to their corresponding text, all instances of non-dysarthric patients were dropped to ensure that the model was only fine-tuned on patients with dysarthria, as the chosen models already performed well on clean speech.\n\nThree speech-to-text models were chosen for this application: Wave2Vec, Whisper, and Whisper Tiny. Transcriptions were cleaned to remove brackets and unnecessary spaces, and audio files were converted to numpy arrays and split into batches for quicker processing. The dataset was split with test size as 0.1, and transcriptions were converted using a tokenizer. Fine-tuning on Wave2Vec and Whisper proved to be difficult due to computational constraints and a large inference time, so Whisper Tiny was used for final mode fine-tuning.\n\n2) Emotion Classification: After obtaining speech to text results, an important component was adding an emotion classifier as shown in Figure  6 , as sentiment is often lost in their reduced speech intelligibility and limitation in expressing nonverbal information  [24] . We used the pretrained Emotion English DistilRoBERTa-base transformer  [25]  to classify english text into 7 sentiments: anger, disgust, fear, joy, sadness, surprise, and neutral. Audio recordings were converted to text using the speech-to-text converter and then subsequently passed into the DistilRoBERTa-base model to infer the speaker's emotional state.\n\n3) Voice Cloning: For patients with dysarthria, preserving their voice identity prior to their diseases is often crucial for better communication and understanding. Voice cloning is a process that reproduces a given dysarthric speech using input speech tokens from audio samples prior to the patient's dysarthria as showcased in Figure  8 . Our voice-cloning text-to-speech (TTS) pipeline loads a user-provided sample audio (saying an arbitrary sentence) and resembles it to 16 hertz mono for input into an XCodec2 model. All reference code was taken from a pre-existing SOTA Text-to-speech and Zero Shot Voice cloning model  [26] . This speech codec model encodes a speaker's vocal characteristics in a sequence of discrete tokens. The pipeline packages the speech tokens with the output from the speech-to-text pipeline, and feeds it into a LLASA-3B model which is fine-tuned to generate speech token sequences based on the text and speaker voice tokens. The generated speech tokens are then passed back onto the XCodec2 decoded to synthesize audio.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dysarthria Detection",
      "text": "To detect the presence of dysarthria, we trained a binary classifier on MFCC features extracted from the TORGO English dataset. The model achieved a high accuracy of 97.5%, and the training curve in Figure  9a  shows stable convergence with minimal overfitting, supported by consistent validation loss.\n\nTo evaluate cross-lingual generalization, we fine-tuned the English-trained model on German and Russian datasets. The model maintained high accuracy on both these languages as shown in Table  I . The confusion matrix in Figure  9b  should that 98 out of 100 dysarthric and 98 out of 100 non-dysarthric samples were correctly identified in the Russian dataset, with only minimal misclassifications.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Severity Classification",
      "text": "Our severity classifier was trained on spectrogram images of patients ranging across different severities of dysarthria. The model received a testing accuracy of 97.64% as shown in Table  II . The loss curves in Figure  10  show convergence after   Another key feature of our model is interpretability. Grad-CAM heatmaps shown in Figure  12     the model successfully learns to denoise and restructure distorted speech patterns. While the outputs preserve the broad frequency distribution of the clean speech, they exhibit slight smoothing and blurring, likely due to the limited phase reconstruction during waveform conversion. 2) Dysarthria Clean Speech Generation (Extended to English): Figure  14  shows the results of the pretrained Russian U-Net model to generate normal speech from dysarthric speech fine tuned on the small English dataset.\n\nIn Phase 2, the Russian-trained U-Net model was fine-tuned on a much smaller, carefully filtered English dataset. The figure showcases the input English dysarthric spectrograms, model predictions, and their matched ground truth normal counterparts.\n\n3) Comparison of Speech to Speech Results: The model achieved a relatively low L1 training loss (0.03) and validation loss (0.06) shown in Table  III . The fine-tuned model for the English dataset achieved a low train and test loss of 0.02 loss of 0.03 respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Speech To Text",
      "text": "Our Speech-to-Text model received its best accuracy after 3 epochs. Table  IV  shows a training loss (word error rate) of 0.1367 towards the 3rd epoch, indicating an accuracy of 87.33%. Figure  15  indicates decreasing loss in only three epochs, a result of Whisper Tiny's light framework designed for slurred speech. The output transcription can then be use for patient voice cloning, recreating the speaker's original voice identity in what they say after being diagnosed with dysarthria.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Emotion",
      "text": "Patients with dysarthria often have altered sentiment in their voice due to speech impairment. Model confidences shown in table V indicate moderate levels of confidence across",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Discussion",
      "text": "Overall, we were able to achieve high classification accuracy across English, Russian, and German demonstrating that our model is able to capture the acoustic patterns of Dysarthric speech while also generalizing to cross linguistic patterns. This cross-linguistic robustness is promising for improved dysarthria classification in other languages without as much available data. Using a CNN for spectrogram-based severity detection also yielded promising and interpretable results, which enables early detection of dysarthria that a human examiner may not be able to pick up. Saliency heatmaps show that across all classes, the model is focusing on the lower harmonics, centered around the timepoints and frequencies that the signal lies in. This confirms the reason behind model prediction, as our model is able to look at a signal with interpretable results.\n\nThe results from our speech to speech model suggests that U-Net based spectrogram translation is promising for translating dysarthric speech to normal speech. By directly learning mappings from disordered to normalized speech spectrograms, the model is able to recover key time-frequency structures associated with clarity. While most existing systems rely on large matched datasets, our approach shows that using a pretrained architecture trained on Russian speech can be applied to lowresource languages. This highlights the potential of transfer learning for low resource languages, where collecting large paired datasets may not be feasible.\n\nASR using transfer learning with Whisper Tiny effectively used the pretrained model and adjusted well to the limited dysarthria data using freezing and data augmentation. Whisper Tiny supports 99 languages and further research is needed for fine-tuning the transfer learning model to generalize to multiple languages, improving accessibility  [27] . The main benefits of Whisper Tiny is that it is very robust, trained on 680,000 hours of multilingual data while also providing faster inferences than the two other transfer learning models tested (Wav2Vec and Whisper Tiny)  [28] .\n\nResults demonstrate the effectiveness of our voice-cloning pipeline in reconstructing a patient's original voice identity. While there is a significant degradation in voice identity between original voice and dysarthric voice, further improvements can can be made by incorporating phonetic patterns such as how a speaker stresses syllables or transitions between vowels to improve voice intelligibility.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Limitations",
      "text": "While our framework demonstrates promising results across multiple tasks and languages, it has some limitations. First, our English speech-to-speech model was fine-tuned on a small paired dataset, which may restrict generalization across broader accents or sentence structures. Second, the emotion classifier was trained solely on clean transcriptions and may not fully capture emotion from more spontaneous or emotionally complex utterances. Finally, while our cross-lingual transfer approach worked well from Russian to English, its effectiveness across more structurally distant language pairs remains untested. Future work includes expanding our dataset to include more diverse speakers and dialects, improving robustness to spontaneous speech, and testing the framework on additional low-resource languages. We also aim to refine the voice cloning pipeline to better preserve speaker identity over longer and more variable inputs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Mobile Launch",
      "text": "To maximize accessibility we developed a web app using Flask and Javascript for backend and HTML and Tailwind CSS frontend. Images of the app interface are shown in Figure  16 . The Web app allows people to get timely, at-home dysarthria diagnosis results and easily use communication-aiding tools.\n\nIn the future we hope to also develop a mobile app to improve accessibility.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusions And Future Work",
      "text": "In this work, we present a multilingual framework for addressing the many dimensions of dysarthria, including detection, severity classification, speech-to-text transcription, clean speech generation, emotion classification, and voice cloning. Our models show high performance across English, Russian, and German datasets, demonstrating the potential for use in real-world multilingual settings. To expand the reach of our framework, our next goal is to incorporate more low-resource languages where dysarthria diagnosis tools are especially scarce. We also aim to further reduce the Word Error Rate (WER) in our speech-to-text module by increasing dataset size, fine-tuning on more speech, and exploring multimodal data (e.g. combining acoustic features with visual inputs such as lip movements) to improve transcription accuracy. Our work is the foundation for a globally inclusive system for speech-based assistive technologies to bridge linguistic gaps and support communication and care for all patients with dysarthria.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Spectrogram visualizations of dysarthric and non-dysarthric speech",
      "page": 2
    },
    {
      "caption": "Figure 2: Dysarthria Classification Model Architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: , is effective at capturing time-frequency patterns",
      "page": 2
    },
    {
      "caption": "Figure 3: Severity Classification Model Architecture",
      "page": 3
    },
    {
      "caption": "Figure 3: Three 2D convolutional layers were used",
      "page": 3
    },
    {
      "caption": "Figure 4: Stage 1: Dysarthric Speech to Normal Speech (Russian)",
      "page": 3
    },
    {
      "caption": "Figure 4: , raw .wav files were converted into",
      "page": 3
    },
    {
      "caption": "Figure 5: Phase 2: Dysarthric Speech to Normal Speech (English)",
      "page": 3
    },
    {
      "caption": "Figure 5: We processed English dysarthric",
      "page": 3
    },
    {
      "caption": "Figure 6: We start by converting the audio to text, then use the text to",
      "page": 3
    },
    {
      "caption": "Figure 7: This feature is crucial for increased communication",
      "page": 3
    },
    {
      "caption": "Figure 6: Overall Speech to text, voice cloning, and emotion pipeline",
      "page": 4
    },
    {
      "caption": "Figure 7: Preprocessing pipeline for fine-tuning the Whisper model. The audio",
      "page": 4
    },
    {
      "caption": "Figure 6: , as sentiment is often lost",
      "page": 4
    },
    {
      "caption": "Figure 8: Pipeline for voice cloning from dysarthric speech. The patient’s",
      "page": 4
    },
    {
      "caption": "Figure 9: a shows stable convergence",
      "page": 4
    },
    {
      "caption": "Figure 10: show convergence after",
      "page": 4
    },
    {
      "caption": "Figure 9: (a) Training and validation loss curves for the model trained on the",
      "page": 5
    },
    {
      "caption": "Figure 11: shows very few",
      "page": 5
    },
    {
      "caption": "Figure 10: Training and validation loss over epochs.",
      "page": 5
    },
    {
      "caption": "Figure 12: were produced by taking",
      "page": 5
    },
    {
      "caption": "Figure 13: above displays the input dysarthric spectrograms, the U-",
      "page": 5
    },
    {
      "caption": "Figure 11: Confusion matrix for severity classification.",
      "page": 5
    },
    {
      "caption": "Figure 12: Saliency Heatmap Showcasing Regions of Importance in Severity",
      "page": 5
    },
    {
      "caption": "Figure 13: Speech-to-speech transformation using Russian data. The left",
      "page": 5
    },
    {
      "caption": "Figure 14: Speech-to-speech transformation using English data. The left",
      "page": 6
    },
    {
      "caption": "Figure 14: shows the results of the pretrained Rus-",
      "page": 6
    },
    {
      "caption": "Figure 15: indicates decreasing loss in only three",
      "page": 6
    },
    {
      "caption": "Figure 15: Speech to Text Word Error Rate over Epochs",
      "page": 6
    },
    {
      "caption": "Figure 16: Dysarthria Detection App built using HTML",
      "page": 7
    },
    {
      "caption": "Figure 16: The Web app allows people to get timely, at-home dysarthria",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Anger\nDisgust\nFear\nJoy\nNeutral\nSadness\nSurprise",
          "Model Confidence": "0.619131\n0.675264\n0.625634\n0.789678\n0.724674\n0.645108\n0.575870"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sentence": "The snow blew into large drifts.\nDon’t ask me to carry an oily rag like that.\nBefore Thursday’s exam,\nreview every formula.\nBright sunshine shimmers on the ocean.\nThe store serves meals every day.\nThe family requests that flowers be omitted.\nYet he still\nthinks as swiftly as ever.",
          "Predicted Emotion": "Anger\nDisgust\nFear\nJoy\nNeutral\nSadness\nSurprise"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Dysarthria: Symptoms and causes",
      "authors": [
        "Clinic Mayo",
        "Staff"
      ],
      "year": "2024",
      "venue": "Dysarthria: Symptoms and causes"
    },
    {
      "citation_id": "2",
      "title": "Dysarthria",
      "authors": [
        "Dilip Kumar",
        "Joe Das"
      ],
      "year": "2025",
      "venue": "StatPearls. StatPearls Publishing, Treasure Island (FL)"
    },
    {
      "citation_id": "3",
      "title": "Dysarthria in adults",
      "year": "2025",
      "venue": "Dysarthria in adults"
    },
    {
      "citation_id": "4",
      "title": "Introduction: Crosslanguage perspectives on motor speech disorders",
      "authors": [
        "Nick Miller",
        "Anja Lowit",
        "Anja Kuschmann"
      ],
      "year": "2014",
      "venue": "Motor Speech Disorders: A Cross-Language Perspective"
    },
    {
      "citation_id": "5",
      "title": "Does native language matter in perceptual ratings of dysarthria?",
      "authors": [
        "Yunjung Kim",
        "Austin Thompson",
        "Seung Jin Lee"
      ],
      "year": "2024",
      "venue": "J. Speech Lang. Hear. Res"
    },
    {
      "citation_id": "6",
      "title": "Enhanced dysarthria detection in cerebral palsy and als patients using wavenet and cnn-bilstm models: A comparative study with model interpretability",
      "authors": [
        "Esraa Hassan",
        "Abeer Saber",
        "Abd Tarek",
        "T El-Hafeez",
        "Mahmoud Medhat",
        "Shams"
      ],
      "year": "2025",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "7",
      "title": "Clinical assessment of communication-related speech parameters in dysarthria: The impact of perceptual adaptation",
      "authors": [
        "Vera Wolfrum",
        "Katharina Lehner",
        "Stefan Heim",
        "Wolfram Ziegler"
      ],
      "year": "2023",
      "venue": "J. Speech Lang. Hear. Res"
    },
    {
      "citation_id": "8",
      "title": "Dysarthria detection using convolution neural network",
      "authors": [
        "M Mahendran",
        "R Visalakshi",
        "S Balaji"
      ],
      "year": "2023",
      "venue": "Measurement: Sensors"
    },
    {
      "citation_id": "9",
      "title": "Dysarthria speech detection using convolutional neural networks with gated recurrent unit",
      "authors": [
        "Dong-Her Shih",
        "Ching-Hsien Liao",
        "Ting-Wei Wu",
        "Xiao-Yin Xu",
        "Ming-Hung Shih"
      ],
      "year": "2022",
      "venue": "Healthcare (Basel)"
    },
    {
      "citation_id": "10",
      "title": "Automatic dysarthria detection and severity level assessment using deep learning",
      "authors": [
        "S Sajiha"
      ],
      "year": "2024",
      "venue": "EURASIP Journal on Audio"
    },
    {
      "citation_id": "11",
      "title": "Improving dysarthric speech intelligibility using cycle-consistent adversarial training",
      "authors": [
        "Hee Seung",
        "Minhwa Yang",
        "Chung"
      ],
      "year": "2020",
      "venue": "Improving dysarthric speech intelligibility using cycle-consistent adversarial training"
    },
    {
      "citation_id": "12",
      "title": "Cyclegan with better cycles",
      "authors": [
        "Tongzhou Wang",
        "Yihan Lin"
      ],
      "year": "2024",
      "venue": "Cyclegan with better cycles"
    },
    {
      "citation_id": "13",
      "title": "Improving the efficiency of dysarthria voice conversion system based on data augmentation",
      "authors": [
        "Wei-Zhong Zheng",
        "Ji-Yan Han",
        "Chen-Yu Chen",
        "Yuh-Jer Chang",
        "Ying-Hui Lai"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "14",
      "title": "Enhancing dysarthric voice conversion with fuzzy expectation maximization in diffusion models for phoneme prediction",
      "authors": [
        "Wen-Shin Hsu",
        "Guang-Tao Lin",
        "Wei-Hsun Wang"
      ],
      "year": "2024",
      "venue": "Diagnostics (Basel)"
    },
    {
      "citation_id": "15",
      "title": "Dysarthric speech transformer: A sequence-to-sequence dysarthric speech recognition system",
      "authors": [
        "Reza Seyed",
        "Vanshika Shahamiri",
        "Dhvani Lal",
        "Shah"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "16",
      "title": "Torgo: Dysarthric and control speech corpus",
      "year": "2009",
      "venue": "Torgo: Dysarthric and control speech corpus"
    },
    {
      "citation_id": "17",
      "title": "Dysarthria and non-dysarthria speech dataset",
      "year": "2022",
      "venue": "Dysarthria and non-dysarthria speech dataset"
    },
    {
      "citation_id": "18",
      "title": "Dysarthria detection dataset",
      "authors": [
        "Iamhungundji"
      ],
      "year": "2023",
      "venue": "Dysarthria detection dataset"
    },
    {
      "citation_id": "19",
      "title": "Audio and text dataset of UA-Speech with severity labels",
      "authors": [
        "Vinotha"
      ],
      "year": "2024",
      "venue": "Audio and text dataset of UA-Speech with severity labels"
    },
    {
      "citation_id": "20",
      "title": "Russian voice dataset (hyperkinetic dysarthria speech)",
      "authors": [
        "Mhantor"
      ],
      "year": "2023",
      "venue": "Russian voice dataset (hyperkinetic dysarthria speech)"
    },
    {
      "citation_id": "21",
      "title": "Dysarthric german speech dataset",
      "authors": [
        "B Czarnetzki"
      ],
      "year": "2023",
      "venue": "Dysarthric german speech dataset"
    },
    {
      "citation_id": "22",
      "title": "Improving Diagnosis in Health Care",
      "authors": [
        "E Balogh",
        "B Miller",
        "J Ball"
      ],
      "year": "2015",
      "venue": "Board on Health Care Services; Committee on Diagnostic Error in Health Care"
    },
    {
      "citation_id": "23",
      "title": "Communicative participation in dysarthria: Perspectives for management",
      "authors": [
        "Allyson D Page",
        "Kathryn Yorkston"
      ],
      "year": "2022",
      "venue": "Brain Sci"
    },
    {
      "citation_id": "24",
      "title": "Recognising emotions in dysarthric speech using typical speech data",
      "authors": [
        "Lubna Alhinti",
        "Stuart Cunningham",
        "Heidi Christensen"
      ],
      "venue": "Recognising emotions in dysarthric speech using typical speech data"
    },
    {
      "citation_id": "25",
      "title": "Emotion english distilroberta-base",
      "authors": [
        "Jochen Hartmann"
      ],
      "year": "2022",
      "venue": "Emotion english distilroberta-base"
    },
    {
      "citation_id": "26",
      "title": "The SOTA text-to-speech and zero shot voice cloning model that no one knows about",
      "authors": [
        "Srinivas Billa"
      ],
      "year": "2025",
      "venue": "LLaSA TTS model blog post"
    },
    {
      "citation_id": "27",
      "title": "What is OpenAI whisper?",
      "venue": "What is OpenAI whisper?"
    },
    {
      "citation_id": "28",
      "title": "Edit a Transcription",
      "venue": "Edit a Transcription"
    }
  ]
}