{
  "paper_id": "2509.24322v1",
  "title": "Multimodal Large Language Models Meet Multimodal Emotion Recognition And Reasoning: A Survey",
  "published": "2025-09-29T06:13:14Z",
  "authors": [
    "Yuntao Shou",
    "Tao Meng",
    "Wei Ai",
    "Keqin Li"
  ],
  "keywords": [
    "Multi-modal emotion recognition",
    "Deep Learning",
    "Multimodal large language models",
    "Artificial general intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, large language models (LLMs) have driven major advances in language understanding, marking a significant step toward artificial general intelligence (AGI). With increasing demands for higherlevel semantics and cross-modal fusion, multimodal large language models (MLLMs) have emerged, integrating diverse information sources (e.g., text, vision, and audio) to enhance modeling and reasoning in complex scenarios. In AI for Science, multimodal emotion recognition and reasoning has become a rapidly growing frontier. While LLMs and MLLMs have achieved notable progress in this area, the field still lacks a systematic review that consolidates recent developments. To address this gap, this paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering model architectures, datasets, and performance benchmarks. We further highlight key challenges and outline future research directions, aiming to offer researchers both an authoritative reference and practical insights for advancing this domain. To the best of our knowledge, this paper is the first attempt to comprehensively survey the intersection of MLLMs with multimodal emotion recognition and reasoning. The summary of existing methods mentioned is in our Github: https://github.com/yuntaoshou/Awesome-Emotion-Reasoning. CCS Concepts: â€¢ General and reference â†’ Surveys and overviews; â€¢ Human-centered computing â†’ Natural language interfaces.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, large language models (LLMs)  [15, 142]  have made significant progress in natural language processing (NLP). By expanding the scale of training data and the number of model parameters, LLMs have demonstrated unprecedented emergent capabilities, enabling them to perform well in many tasks, especially instruction following (IF)  [136] , in-context learning (ICL)  [130] , and chain-of-thought (CoT)  [138] . IF enables the model to understand and perform complex tasks, ICL enables the model to flexibly handle different problems based on context without explicit training, and chain-of-thought enhances the model's decision-making process through step-by-step reasoning.\n\nAlthough LLMs have performed well in many NLP tasks  [60]  and have also demonstrated amazing zero-shot and few-shot reasoning capabilities in some complex real-world applications  [38, 140, 166] , LLMs are essentially still \"blind\" to visual information. LLM working principle is mainly based on text data and cannot directly process multimodal data (e.g., images or videos). In contrast, large visual models (LVMs) can efficiently process and understand image content  [54, 105] . Through convolutional neural networks (CNNs)  [53, 107, 113]  and Transformers  [91, 95]  architectures, LVMs have demonstrated excellent performance in visual recognition and image generation  [86, 172] . However, despite their powerful ability to reason about visual information, LVMs have certain limitations in natural language understanding and generation, resulting in lagging or lack of flexibility in reasoning. Given the outstanding performance and complementarity of LLMs and LVMs in their respective fields, combining the advantages of both has become a hot topic of research, giving rise to the emerging field of multimodal large language models (MLLMs)  [63, 150] . Specifically, MLLMs are designed to receive, reason about, and output information from multiple modalities, such as text, images, audio, etc. Through cross-modal fusion, MLLMs enable models to process and understand more complex and diverse data, reason under multimodal inputs, and provide more accurate and rich outputs  [82] . The development of MLLM provides new perspectives and approaches for realizing true artificial intelligence, especially in tasks that require simultaneous understanding of language and visual information (e.g., visual question answering, image description generation, etc.), showing great potential and application value  [37, 99] .\n\nMultimodal emotion recognition and reasoning, as a challenging task, requires not only the model to extract emotion information from a single modality, but also to perform deep reasoning in multimodal interactions to understand and capture complex emotion expressions and contexts  [68, 81, 147, 164] . With the rapid advancement of MLLMs, the solutions to multimodal emotion recognition and reasoning tasks have also changed significantly  [21, 23, 149] . Through a unified generation paradigm, MLLMs are not only able to process multimodal data, but also to integrate information between multiple modalities, significantly improving the effect of emotion recognition and reasoning. It is worth noting that some MLLMs have been able to perform multimodal emotion recognition and reasoning without any additional training data, which means that they have strong zero-shot and few-shot reasoning capabilities  [10, 145] . Compared with traditional multimodal emotion recognition models, the latter usually rely on supervised learning and require a large amount of labeled data to fine-tune the model to adapt to different emotion recognition tasks  [2, 96, 106, 108, 110, 114] . MLLMs are able to surpass traditional multimodal emotion recognition models without the need for large-scale labeled data, which gives them a significant advantage in multimodal emotion recognition and reasoning tasks. More importantly, MLLMs can share knowledge across multiple modalities and process multiple data sources through joint training and reasoning, which provides stronger reasoning ability and higher accuracy.\n\nDue to the remarkable progress of LLMs and MLLMs in emotion recognition and reasoning, the interest and investment in these models in academia and industry have also shown a sharp growth trend. Therefore, this paper aims to explore the following key questions: (1) What is the current application status of LLMs and MLLMs in emotion recognition and reasoning tasks? We will review the relevant literature and analyze the specific usage and advantages of LLMs and MLLMs in emotion recognition and reasoning.  (2)  Have traditional multimodal emotion recognition methods been replaced by MLLMs, or can they effectively make up for the shortcomings of traditional methods? (3) What is the future development direction of MLLMs for multimodal emotion recognition and reasoning?\n\nTo answer the above questions, we made the first attempt to conduct a comprehensive and detailed survey of LLMs and MLLMs for emotion recognition and reasoning as shown in Fig.  1 . The goal of this study is to explore and summarize the current development and applications of LLMs and MLLMs in emotion recognition and reasoning tasks. In view of the rapid development of this field, our study not only aims to clarify the specific applications of these models in emotion recognition and reasoning, but also hopes to reveal the potential, limitations and future research directions of these models through systematic analysis. To this end, this paper first introduces the basic concepts and background of multimodal emotion recognition and reasoning, and reviews the preliminary related research results. Then, we focus on introducing the application paradigms of LLMs and MLLMs in emotion recognition and reasoning, and propose a unified framework to help understand how MLLMs process and reason about cross-modal data. Specifically, we classify the existing application paradigms of LLMs and MLLMs into two categories. (1) Parameter freezing applications: LLMs and MLLMs pre-trained models are able to perform zero-shot learning (ZSL)  [8, 50, 131]  and few-shot learning (FSL)  [13, 41, 70]  without much additional data, thanks to their strong language understanding capabilities. By freezing most of the model parameters and only adjusting a few key parameters, this approach greatly improves the efficiency of the model and reduces the need for labeled data  [16, 123, 135] . (2) Parameter tuning applications: LLMs and MLLMs require parameter tuning on specific tasks to further improve the accuracy and adaptability of the model  [133, 171] . The Full Parameter Tuning method involves fine-tuning the parameters of the entire model to ensure that it can better handle modal data in emotion recognition tasks  [87, 161] . Efficient Parameter Tuning focuses on using optimization algorithms (e.g., learning rate adjustment, progressive training, etc.) to effectively adjust a few parameters of the model with less training data and computing resources, thereby improving performance under resource-constrained conditions  [67] . Finally, we explore the current research bottlenecks and challenges, and conclude by identifying potential frontiers for future research, as well as relevant challenges that inspire further exploration.\n\nIn summary, our contributions are as follows:\n\nâ€¢ New Taxonomy: We provide a new taxonomy including (1) parameter-frozen and (2) parameter-tuning methods, which provides a unified view to understand LLMs and MLLMs for emotion recognition and reasonging. To the best of our knowledge, we present the first comprehensive review of LLMs and MLLMs for emotion recognition and reasoning.   [3, 109, 112, 114] . Unlike unimodal recognition, multimodal emotion recognition integrates complementary and redundant information across modalities, thereby enabling more robust and reliable affective inference  [2] . Formally, given an input set of ğ‘€ âˆˆ {1, 2, . . . ğ‘š} modalities as follows: X = {ğ‘‹ (1) , ğ‘‹ (2) , . . . , ğ‘‹ (ğ‘š) }\n\nwhere each modality ğ‘‹ (ğ‘š) âˆˆ R ğ‘‘ ğ‘› Ã—ğ‘‡ represents a temporal sequence of dimension ğ‘‘ ğ‘› over ğ‘‡ time steps, the task of multimodal emotion recognition is to learn a mapping function ğ‘“ : X â†’ Y, where Y denotes the label space of emotions (categorical, dimensional, or mixed).\n\nTo capture inter-modal and intra-modal dependencies, multimodal emotion recognition models often introduce a latent joint representation z that integrates modality-specific embeddings: z = F ğœ™ 1 (ğ‘‹ (1) ), ğœ™ 2 (ğ‘‹ (2) ), . . . , ğœ™ ğ‘š (ğ‘‹ (ğ‘š) )\n\n(\n\nwhere ğœ™ ğ‘š (â€¢) denotes the feature extractor for modality ğ‘š, and F (â€¢) is the fusion function capturing cross-modal interactions. The final prediction is obtained via:\n\nwith ğœƒ denoting the trainable parameters of the model. Therefore, multimodal emotion recognition can be rigorously defined as the problem of learning the optimal ğœƒ * such that:\n\nwhere D is the multimodal dataset and L (â€¢) is an emotion recognition loss (e.g., cross-entropy for classification or mean squared error for dimensional regression). Multimodal emotion reasoning. While multimodal emotion recognition focuses on mapping multimodal inputs to emotion labels, multimodal emotion reasoning extends the task by generating not only predictions but also interpretable explanations or causal reasoning chains  [117, 158] . Formally, the reasoning-enhanced task can be defined as follows:\n\nwhere R denotes the reasoning space, consisting of causal relations, evidential supports, or natural language explanations. A typical probabilistic formulation is\n\nwhere ğ‘¦ âˆˆ Y is the predicted emotion label and ğ‘Ÿ âˆˆ R represents its associated reasoning path. This expansion highlights the importance of transparency, explainability, and causal understanding of reasoning. This shift is crucial for building trustworthy multimodal emotion systems.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Evolution Of Traditional Multimodal Emotion Recognition Methods",
      "text": "The evolution of Multimodal Emotion Recognition has been driven by the need to effectively integrate heterogeneous signals and capture both their complementary and conflicting cues. Early approaches primarily relied on feature-level fusion  [27, 170]  and decision-level fusion  [120, 167] , where unimodal features were either concatenated or late-averaged to yield the final prediction: ) ); ğœ™ 2 (ğ‘‹ (2) ); . . . ; ğœ™ ğ‘š (ğ‘‹ (ğ‘š) )]\n\nwhere\n\nTo enable cross-modal interaction, these modality-specific features are projected into a shared embedding space via learnable transformation matrices W (ğ‘š) , yielding\n\nwhere vec(â€¢) denotes vectorization. The multimodal fusion is then performed as:\n\nwhere denotes a modality fusion operator (e.g., concatenation, bilinear pooling, or tensor fusion), and ğœ™ (â€¢) denotes a non-linear transformation. This fusion mechanism not only preserves intramodal representations but also enables cross-modal interactions, thereby capturing emotion-relevant dependencies across modalities.\n\nRecurrent Neural Networks (RNNs), and especially Long Short-Term Memory (LSTM) models  [100] , are widely adopted to capture sequential dependencies in speech and conversational data. Formally, given ğ‘š modalities, we first compute modality-specific gates:\n\nand candidate memory content as:\n\nThen, multimodal fusion is introduced at the cell state level via:\n\nwhere ğ›¼ (ğ‘š) are learnable or attention-based modality weights ensuring adaptive contribution from each modality. Finally, the hidden state is updated as:\n\nThese advances enabled models to learn temporal emotion dynamics rather than relying solely on static fusion. The introduction of attention mechanisms further enhanced multimodal emotion recognition by allowing selective focus on salient cross-modal cues  [96, 110] . Cross-modal attention is formalized as follows:\n\nwhere q (ğ‘– ) ğ‘¡ and k ( ğ‘— ) ğ‘¡ are query and key representations from modality ğ‘– and ğ‘—. Building upon this principle, Transformer architectures enabled scalable modeling of long-range dependencies through multi-head self-attention  [72, 91] :\n\nwhere Q, K, V denoting query, key, and value matrices. Transformers have since become a dominant backbone in multimodal emotion recognition due to their ability to align asynchronous multimodal signals at scale. To extend attention into multimodal contexts, modality-specific queries, keys, and values {Q (ğ‘š) , K (ğ‘š) , V (ğ‘š) } ğ‘€ ğ‘š=1 are first derived. Then, cross-modal attention aggregates complementary signals by computing:\n\nThis allows each modality ğ‘– to selectively attend to relevant information from other modalities ğ‘—.\n\nFinally, the fused multimodal representation at time ğ‘¡ is obtained as\n\nThis formulation enables both intra-modal temporal modeling and cross-modal interaction, thereby aligning asynchronous multimodal signals and enhancing emotion-related representation learning. Beyond sequence-based modeling, graph neural networks (GNNs) advanced multimodal emotion recognition by representing multimodal interactions as structured graphs  [94, 111, 115, 116] . Given a multimodal graph G = (V, E), with nodes denoting modality-specific features and edges encoding inter-modal relations, message passing iteratively refines node embeddings. To explicitly capture cross-modal interactions, we extend the graph to a multimodal setting. Each node is associated with a modality index ğ‘š âˆˆ {audio, text, video} and initialized with modality-specific feature h (0,ğ‘š) ğ‘£ . During message passing, cross-modal attention weights are introduced to adaptively fuse signals from different modalities:\n\nwhere N (ğ‘£) is the neighborhood of node ğ‘£, ğ›¼ (ğ‘šâ†ğ‘›)   ğ‘¢ğ‘£ denotes the attention coefficient that controls the contribution of modality ğ‘› at node ğ‘¢ to modality ğ‘š at node ğ‘£. The final multimodal embedding is obtained by fusing across modalities:\n\nThis formulation enables GNN-based multimodal emotion recognition to (i) preserve modalityspecific structural information, (ii) adaptively integrate complementary cues through cross-modal message passing, and (iii) mitigate modality imbalance and redundancy by dynamically weighting heterogeneous edges.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Llm-Based Methods",
      "text": "The first stage of multimodal emotion recognition and reasoning primarily relied on large language models (LLMs) as the central reasoning engine  [56] . In this paradigm, modality-specific encoders are employed to extract unimodal features. For example, text inputs are processed by a pre-trained text encoder to obtain embeddings. To leverage the powerful reasoning capability of LLMs, these embeddings are further projected into a unified textual space before being fed into the LLM, as illustrated in Fig.  2 (a). Formally, let ğœ™ ğ‘š (â€¢) denote the encoder for modality ğ‘š âˆˆ {text}, the embedding is defined as:\n\nThe final prediction is then generated by the LLM as:\n\nDespite their simplicity, such pipelines suffer from one main limitations: the reliance on textual features alone and the inability to capture fine-grained cross-modal interactions, which are particularly crucial for affective understanding. As a result, although LLM-based methods established the feasibility of leveraging language models for emotion recognition, their scalability and reasoning fidelity remain limited.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Mllm-Based Methods",
      "text": "The emergence of multimodal large language models (MLLMs) marks a fundamental paradigm shift in multimodal emotion recognition and reasoning  [122, 147] . Unlike LLM-based pipelines that retrofit unimodal embeddings into the textual space, MLLMs are inherently designed to integrate heterogeneous signals in an end-to-end manner. As shown in Fig.  2 (b), embeddings from text, audio, and video encoders are aligned with the token space of the language model through connector modules, which serve as the critical bridge between modality encoders and the generative reasoning capability of LLMs. The design of these connectors has gradually evolved to support more expressive multimodal alignment  [131] . The simplest approach employs a multilayer perceptron (MLP) to linearly project modality embeddings into the token dimension of the LLM, providing a lightweight yet effective alignment mechanism. To further enhance cross-modal interaction, Q-Former  [62]  structures have been introduced, which rely on a set of learnable queries to selectively extract salient semantic features from modality embeddings before injecting them into the LLM, thereby",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "(B) Mllm-Based Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Audio Embeddings",
      "text": "Video Embeddings reducing redundancy while preserving task-relevant cues. More recent works adopt cross-attention mechanisms, where the LLM directly attends to non-textual embeddings through multi-head attention layers, enabling dynamic, context-dependent fusion that better captures the complementary or conflicting nature of multimodal signals. Formally, given encoder outputs ğ‘§ ğ‘š = ğœ™ ğ‘š (ğ‘‹ (ğ‘š) ), a connector ğ‘”(â€¢) transforms them into aligned representations:\n\nThese aligned embeddings are then jointly consumed by the MLLM for reasoning:\n\nThis connector-centric design allows MLLMs to preserve modality-specific information while performing end-to-end multimodal reasoning, thereby offering a more faithful and semantically grounded approach for emotion recognition and affective reasoning tasks, where subtle interdependencies across language, prosody, and visual cues are essential.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Taxonomy",
      "text": "As summarized in Fig.  3 , we categorize multimodal emotion recognition and reasoning methods along two overarching paradigms: parameter-frozen and parameter-tuning. The former relies on zero-shot and few-shot prompting over frozen backbones, prioritizing rapid generalization with minimal compute, while the latter adapts model parameters either via full fine-tuning for maximal specialization or via parameter-efficient tuning (e.g., adapters, prefixes, LoRA) to balance accuracy and cost.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter-Frozen Paradigm",
      "text": "In the parameter-frozen paradigm, the backbone LLM/MLLM remains unchanged and task adaptation is realized purely through prompting and in-context specification  [58, 133] . This setting is particularly attractive for multimodal emotion recognition and reasoning because (i) it avoids expensive fine-tuning on large models, and (ii) it enables rapid transfer across datasets with heterogeneous label spaces. Let ğ‘‹ = {ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘ , ğ‘¥ ğ‘£ } denote text, audio, and video inputs after modality-specific encoding or serialization into model-acceptable inputs, and let I denote a task instruction template that defines the goal (e.g., \"Infer the speaker's emotion and briefly explain the evidence. \"). The model produces a distribution over label strings given the constructed prompt Î (ğ‘‹, I):\n\nwhere Î˜ are frozen parameters and ğœˆ (ğ‘¦) is a verbalizer mapping each class ğ‘¦ to one or more label strings (e.g., \"happy\", \"joyful\"). The final decision is Å· = arg max ğ‘¦ ğ‘ƒ Î˜ (ğ‘¦ | ğ‘‹, I), optionally with length normalization when ğœˆ (ğ‘¦) contains multi-token strings.\n\nZero-shot learning. Zero-shot multimodal emotion recognition and reasoning relies solely on natural-language instructions without task-specific examples  [77, 150] . A well-engineered prompt serializes heterogeneous evidence into a structured context that highlights affective cues while minimizing spurious correlations. In LLM-based pipelines, non-text modalities are first converted into textual evidence snippets, e.g., prosodic summaries, facial action units, and salient visual events-through auxiliary encoders, and concatenated with raw transcripts using role tags (e.g., <Text>, <Audio-Prosody>, <Visual-AU>). In MLLM-based pipelines, raw embeddings are passed alongside textual tokens, but the model is still queried through instructions; hence Eq. (  25 ) applies with Î (â€¢) injecting modality tokens or connector-produced embeddings. To reduce prompt sensitivity and improve robustness, zero-shot systems commonly employ (i) instruction variants and prompt ensembling with majority voting or probability averaging, (ii) self-consistency with reasoning where latent rationales ğ‘Ÿ are sampled and marginalized as follows:\n\nand (iii) contextual calibration, which subtracts a prior estimated from a content-free prompt to alleviate label-word frequency bias. Zero-shot decoding can output both a label and an explanation, providing weak but valuable interpretability for auditing emotion decisions.\n\nRepresentative systems have instantiated these principles in diverse ways. CLIP  [102]  pioneered large-scale contrastive training on image-text pairs, enabling zero-shot transfer simply by casting downstream tasks into natural-language prompts. Building on this idea, Qwen-Audio  [26]  and Video-LLaVA  [79]  extend frozen LLMs with lightweight modality encoders for audio and video streams, respectively, illustrating that non-text signals can be seamlessly injected into zero-shot pipelines. MiniGPT-v2  [17]  and LLaVA-Next  [83]  further refine visual-language alignment by introducing enhanced connector modules, thereby improving the sensitivity of emotion-related cues under zeroshot prompting. Scaling up this paradigm, InternVL2.5  [20]  and Qwen-VL  [5]  leverage extensive multimodal pretraining to unify images and videos, demonstrating strong generalization across recognition and reasoning benchmarks. Beyond perception, reasoning-oriented variants such as GPT-4 (CoT) [1] highlight the benefit of integrating chain-of-thought prompting, which enhances interpretability by generating intermediate rationales without any parameter updates.\n\nFew-shot learning. Few-shot multimodal emotion recognition and reasoning augments the instruction with ğ‘˜ demonstration pairs {(ğ‘‹ ğ‘– , ğ‘¦ ğ‘– )} ğ‘˜ ğ‘–=1 injected into the prompt as in-context exemplars  [13, 70] . The predictive distribution is\n\nwhere the selection of D ğ‘˜ is crucial. Effective strategies balance relevance to the query and diversity across emotions and modalities, often via embedding-based retrieval with a determinantal or max-min objective to avoid redundancy  [123] . Demonstrations should expose the model to prototypical prosodic contours, facial micro-expressions, and discourse contexts (e.g., irony, negation) that are underrepresented in pre-training. Ordering also matters: placing task definition first, then structured exemplars (\"context â†’ rationale â†’ label\"), and finally the query tends to reduce hallucination. For classification with free-form generation, a rationale-then-verbalizer template first eliciting a short explanation ğ‘Ÿ and then constraining the final answer to ğœˆ (Y) often improves calibration and stability. When label spaces differ across datasets, dynamic verbalizers provide a principled bridge by defining synonyms per class and aggregating token probabilities as in Eq. (  25 ). Representative few-shot systems have further substantiated these principles. Otter  [58]  and BingChat  [163]  illustrate how general-purpose large language models can be adapted to multimodal settings with only a handful of in-context exemplars, showing consistent improvements over zeroshot prompting. LVLMs  [57]  extend this approach by explicitly integrating visual evidence with frozen LLMs, thereby enhancing the model's ability to capture subtle affective cues such as facial micro-expressions in few-shot contexts. Domain-specific adaptations like EMOTIC  [34]  demonstrate that curated multimodal demonstrations can guide models toward recognizing nuanced emotional states beyond basic categories. At a larger scale, Flamingo  [4]  exemplifies the state of the art in multimodal few-shot learning by introducing a gated cross-attention mechanism that allows visual tokens to interact directly with frozen LLM layers, enabling strong performance across a wide variety of benchmarks without full fine-tuning. Subsequent works such as ARC-LLM  [35]  and Exp-CLIP  [168]  push this direction further by leveraging explicit rationales or structured demonstrations, improving interpretability while mitigating label imbalance. Collectively, these systems show that few-shot learning provides a pragmatic balance between efficiency and adaptability, making it an attractive paradigm for emotion recognition and reasoning where annotated data is scarce yet diverse affective cues must be reliably captured.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Parameter-Tuning Paradigm",
      "text": "Unlike the parameter-frozen setting, the parameter-tuning paradigm adapts the internal weights of LLMs/MLLMs to the target task, thereby improving domain specialization and calibration for multimodal emotion recognition and reasoning  [87, 171] . Let ğœ™ ğ‘š be modality encoders for ğ‘š âˆˆ {text, audio, video}, ğ‘” ğœ“ the connector(s), and ğ‘“ Î˜ the LLMs/MLLMs with parameters Î˜. Given D = {(ğ‘‹, ğ‘¦, ğ‘Ÿ )} where ğ‘‹ = {ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘ , ğ‘¥ ğ‘£ }, ğ‘¦ is a discrete label or continuous affect (valence-arousal), and ğ‘Ÿ is an optional rationale, a generic supervised objective is  (28)  where L task is cross-entropy or MSE for continuous affect; L gen is a token-level negative loglikelihood for explanation sequences; L align often adopts a contrastive InfoNCE  [44]  between modality summaries ğ‘¢ ğ‘– and textual anchors ğ‘£ ğ‘– as follows:\n\nand L temp encourages smooth predictions across time for videos, e.g.,\n\nwhere ğ‘ ğ‘¡ is the distribution at time ğ‘¡. Class imbalance is commonly handled via class-balanced weighting ğ›¼ ğ‘¦ âˆ 1-ğ›½ 1-ğ›½ ğ‘›ğ‘¦ or focal loss. Full-parameter tuning. Full fine-tuning  [148, 154]  updates all parameters of the backbone LLM/MLLM as shown in Fig.  4 , the connectors, and optionally the modality encoders:\n\nThis yields the greatest flexibility, facilitating domain-specific lexical cues, prosodic-linguistic coupling, and long-context dialogue reasoning but demands substantial memory and compute. Practical recipes include stage-wise training (first warm up ğ‘” ğœ“ and cross-attention, then unfreeze upper Transformer blocks), mixed-precision with gradient checkpointing, and curriculum schedules that progress from single-utterance supervision to multi-turn dialogue windows. Instruction tuning with multimodal emotion recognition and reasoning-style prompts further improves alignment between training signals and inference-time usage; when high-quality human preferences on explanations are available, one may add preference optimization (e.g., DPO  [139] ) with a pairwise  Video frames are encoded by a visual encoder and mapped to the LLM token space by multiple visual projectors. A projector fusion gate with learnable coefficients (ğœ” 1 , ğœ” 2 , ğœ” 3 ) aggregates the projected features and emits visual tokens. Audio is processed by an audio encoder and projector to produce audio tokens, while the text instruction is embedded by a text encoder. The resulting visual, audio, and text tokens are interleaved into the pretrained LLM's token stream for joint reasoning  [165] .\n\nobjective for preferred ğ‘¦ + over ğ‘¦ -as follows:\n\nRepresentative full-tuning approaches have demonstrated the strengths and limitations of this paradigm. Early multimodal systems such as VisualBERT  [66]  and UNITER  [19]  established the foundation by fully fine-tuning pretrained transformers on paired image-text datasets, achieving strong performance in tasks like visual question answering and image-text retrieval. Building on these successes, video-oriented models such as VideoMAE  [128]  and Video-LLaMA  [22]  extended full-parameter tuning to spatiotemporal settings, showing that large-scale video pretraining coupled with complete fine-tuning yields substantial gains in downstream multimodal reasoning tasks. OneLLM  [42]  and LLaMA-VID  [69]  exemplify more recent efforts that scale this strategy to cover multiple modalities simultaneously, relying on end-to-end optimization to align speech, vision, and language under a unified backbone. Similarly, Chat-UniVi  [52]  integrates full-tuning with dialogue-style supervision, demonstrating that updating all parameters can improve long-context reasoning and multimodal dialogue alignment. While these models underscore the versatility and performance benefits of full-parameter tuning, they also reveal the accompanying computational burdens, motivating the exploration of more efficient alternatives such as parameter-efficient tuning.\n\nParameter-efficient tuning. To lower cost while retaining most gains of full fine-tuning, parameter-efficient tuning (PET)  [171]  updates a small, carefully placed subset of parameters, keeping the base weights frozen as shown in Fig.  5 . Popular instantiations include Adapter-tuning  [49] , Prefix/Prompt-tuning  [133, 138] , and LoRA  [45] /IA 3  [85] /BitFit  [11]  families. Adapter-tuning inserts a bottleneck MLP in each block and learns only the adapter weights as follows:\n\nPrefix/Prompt-tuning prepends learnable key/value vectors to each attention layer; if ğ¾, ğ‘‰ are original keys/values, the attention uses K = [ğ¾ ğ‘ ; ğ¾], á¹¼ = [ğ‘‰ ğ‘ ; ğ‘‰ ] with trainable prefixes (ğ¾ ğ‘ , ğ‘‰ ğ‘ ) while freezing backbone weights, thereby steering attention without changing ğ‘Š ğ‘ ,ğ‘Š ğ‘˜ ,ğ‘Š ğ‘£ .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Global Encoder",
      "text": "Local Encoder Temporal Encoder ... LoRA often with dropout on ğ´ and target-specific selection of layers and learns low-rank updates for selected weight as follows:\n\nIA 3 applies multiplicative gating to attention/MLP projections, and BitFit updates only bias terms. For resource-constrained training, QLoRA quantizes the frozen base to 4-bit (NF4) and learns LoRA adapters in BF16/FP16, giving near full-FT performance at a fraction of memory; at inference, one can either keep the decomposition or merge Î”ğ‘Š into ğ‘Š for deployment. In MLLMs, PET is frequently combined with connector tuning (train ğ‘” ğœ“ and cross-attention blocks, keep the language core frozen), or partial unfreezing of the last ğ¿ Transformer blocks to better adapt discourse-level reasoning while keeping compute bounded.\n\nRepresentative PET-based systems have validated the practicality of this paradigm in multimodal emotion recognition and reasoning. For example, SECap  [144]  leverages selective adaptation to capture prosodic and semantic cues in speech-driven emotion captioning, significantly reducing training overhead while maintaining interpretability. PandaGPT  [118]  and VideoChatGPT  [92]  extend PET strategies to video-language reasoning, integrating lightweight LoRA modules into frozen LLMs to handle long-context temporal dependencies efficiently. VideoChat2  [65]  further improves alignment by combining low-rank adapters with cross-attention, highlighting the scalability of PET for complex video-language benchmarks. Emotion-specific adaptations such as Emotion-LLaMA  [21]  demonstrate that parameter-efficient designs can preserve sensitivity to subtle affective states when domain-specific supervision is scarce. At the same time, SALMON  [124]  and mPLUG-Owl  [152]  show how adapter-style connectors can be tuned alongside frozen backbones to support flexible multimodal extensions. More recently, works like BLIVA-FlanT5  [152] , Monkey  [71] , and GLM4V-9B  [134]  illustrate the integration of PET with instruction tuning, combining efficiency",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluated Mllms",
      "text": "Video-LLaVA AffectGPT ......",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "User:",
      "text": "Please analyze which features in the audio reflect that the speaker is feeling {Emotion Label}. Pay special attention to emotional features such as crying sounds, laughter, changes in tone, speech rate, pauses, emphasis and stress, voice trembling, and other emotional characteristics.\n\n{Emotion Label}: Anger",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Audio Agent:",
      "text": "The speaker exhibits intense anger through their aggressive tone, loud voicing, and the use of forceful language indicating they are upset or agitated about the situation involving Rachel.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Extract Audio Clues",
      "text": "Judge Agent",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Human Verification",
      "text": "Sampling Evaluating Fig.  6 . Pipeline for building multimodal emotion recognition and reasoning.\n\nwith enhanced reasoning ability. Together, these systems exemplify how PET techniques strike a balance between the high performance of full-parameter tuning and the scalability required for real-world deployment, underscoring their growing role in advancing MLLMs.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Pipeline For Constructing Emotion Recognition Datasets With Mllms",
      "text": "With the rapid advances of Multimodal Large Language Models (MLLMs), a promising paradigm has emerged to automate the construction of high-quality emotion recognition datasets. Instead of relying exclusively on labor-intensive manual annotation, MLLMs can be integrated into a multiagent pipeline to generate emotion labels, extract rationales, and validate multimodal evidence. Fig.  6  illustrates that audio, visual, and textual cues are jointly processed to produce explainable and verifiable annotations. We summarize the overall pipeline into six key stages.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Data Sampling And Task Formulation",
      "text": "Raw sources include movies, dialogues, interviews, or spontaneous recordings. Segments are obtained by shot detection or voice activity detection (VAD)  [76] , followed by key-frame extraction using frame-difference, optical flow, or ORB similarity to ensure semantic diversity. Each segment is paired with an instruction template that explicitly requires step-by-step reasoning: \"Identify the most prominent emotion, provide intermediate steps, and highlight modality-specific evidence. \"",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Multimodal Preprocessing",
      "text": "The audio stream undergoes denoising, speaker separation, and automatic speech recognition (ASR), while extracting prosodic features such as pitch, intensity, and pauses. Visual processing involves face detection, action unit (AU) extraction, and geometric descriptors of facial regions  [23, 74] . Temporal alignment ensures that audio, video, and transcript are synchronized into a unified multimodal sample representation.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Generative Annotation Via Mllms",
      "text": "Given the preprocessed input, multiple MLLMs (e.g., GPT, Gemini, Video-LLaMA) are prompted to output emotion labels, step-by-step rationales, and modality-specific evidence [1,  125, 160] . This stage converts implicit multimodal cues into explicit annotation triplets {label, rationales, evidence}.\n\nA rationale extractor normalizes the responses into concise Step 1...N sequences, improving consistency and interpretability.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Judge Agent And Automated Quality Control",
      "text": "To ensure annotation reliability, a Judge Agent evaluates MLLM outputs across three dimensions: (i) recognition score ğ‘† rec for correctness of the predicted label, (ii) reasoning score ğ‘† reason for the logical soundness of the explanation, and (iii) chain-of-thought score ğ‘† cot for completeness of multimodal reasoning  [159] . A unified score is computed as:\n\nwith ğ›¼, ğ›½, ğ›¾ tunable across tasks. Conflicting annotations from multiple MLLMs are resolved via voting or uncertainty-aware aggregation.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Human Verification And Consistency Checks",
      "text": "Low-confidence or semantically ambiguous samples are flagged for human verification  [43, 159] . A subset of annotations is cross-evaluated by expert raters, and agreement is quantified by statistical measures such as Spearman correlation, Cohen's ğœ…, and the intra-class correlation coefficient (ICC). High consistency (e.g., ğœ… > 0.8, ICC > 0.9) indicates near-expert annotation quality, thus minimizing manual overhead.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Structured Storage And Benchmark Release",
      "text": "Finalized entries are stored in structured formats (e.g., JSON/Parquet), containing emotion labels, rationales, multimodal evidence, and quality-control metadata. The dataset supports multi-label emotions, intensity levels, and uncertainty estimates. For benchmarking, data is split into training, validation, and testing subsets, with the test set optionally hiding labels to promote explainable evaluation. Beyond accuracy and F1, evaluation protocols include rationale alignment (ROUGE/BLEU), cross-modal evidence coverage, and causal sufficiency tests  [75, 75, 76] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Popular Benchmark Datasets",
      "text": "In multimodal large language model research for emotion recognition, the use of benchmark datasets is essential for algorithm validation and performance evaluation. As shown in Table  1 , we summarize several commonly adopted multimodal emotion recognition datasets. Below, we provide a brief description of each dataset. HUMAINE  [32] : The HUMAINE database is one of the earliest large-scale resources dedicated to affective computing and emotion recognition. The dataset contains both naturalistic and experimentally induced emotional expressions, spanning multiple modalities such as audio, video, and physiological signals. Importantly, it provides detailed manual annotations of emotional states based on categorical labels (e.g., joy, anger) as well as dimensional descriptors (e.g., arousal, valence).\n\nVAM  [40] : The Vera am Mittag (VAM) corpus consists of spontaneous emotional expressions extracted from the German television talk show Vera am Mittag, providing a rich collection of\n\nnatural audio-visual interactions. Each segment is annotated using dimensional emotion descriptors-valence, arousal, and dominance by multiple human raters, ensuring both reliability and fine-grained analysis of affective states.\n\nYoutube  [97] : The YouTube dataset comprises video clips collected from YouTube, capturing spontaneous expressions of opinion across diverse topics and speakers. Each clip contains audio, visual, and textual modalities, with sentiment annotations assigned at the utterance level.\n\nAFEW  [30] : The Acted Facial Expressions in the Wild (AFEW) dataset is constructed from a diverse collection of movie excerpts, containing audio-visual samples that encompass a wide range of speakers, contexts, and environmental conditions such as varying lighting, and occlusions. Each sample is annotated with one of the basic emotion categories (e.g., anger, happiness, sadness).\n\nAM-FED  [93] : Affectiva-MIT Facial Expression Dataset (AM-FED) contains more than 1,500 video recordings comprising over 242 hours of spontaneous facial behavior. Each video is annotated with affective labels, covering both basic emotions and continuous affective dimensions.\n\nAFEW-VA  [31] : The Acted Facial Expressions in the Wild-Valence and Arousal (AFEW-VA) dataset extends the original AFEW corpus by introducing continuous annotations of emotional dimensions. Specifically, it provides frame-level labels for valence and arousal in video clips extracted from movies, thereby enabling fine-grained analysis of affective dynamics.\n\nLIRIS-ACCEDE  [9] : The LIRIS-ACCEDE corpus consists of more than 9,800 short movie excerpts carefully selected from Creative Commons films, covering a broad range of genres, visual styles, and emotional contexts. Each clip is annotated with continuous valence and arousal ratings.\n\nEMILYA  [36] : The EMILYA corpus contains a wide range of motion capture recordings in which actors perform daily actions under different induced emotional states such as happiness, anger, sadness, and fear. Each sequence is annotated with categorical emotion labels as well as dimensional descriptors of valence and arousal.\n\nSEWA  [55] : The SEWA database contains audio-visual recordings of more than 400 participants engaged in naturalistic dyadic interactions across six different cultural backgrounds. Each recording is richly annotated at multiple levels, including continuous valence, arousal, and liking dimensions.\n\nCMU-MOSEI  [155] : The CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset contains more than 23,000 annotated video segments collected from over 1,000 speakers across a wide range of topics on YouTube. Annotations cover both sentiment (from highly negative to highly positive) and six basic emotion categories, with continuous intensity scores.\n\niMiGUE  [89] : The iMiGUE comprises more than 3,000 video samples collected from over 60 participants, with deliberate anonymization to remove identity-related cues such as facial appearance. Each sequence is annotated with both categorical emotion labels and dimensional descriptors.\n\nDFEW  [51] : The DFEW dataset comprises 16,372 video clips extracted from movies, capturing a wide spectrum of real-world complexities as shown in Fig.  7 . Each video clip is annotated through a rigorous crowdsourcing pipeline involving 12 expert annotators, with every clip independently labeled ten times to ensure annotation reliability. The dataset provides both single-label annotations covering the seven basic emotion categories and 7-dimensional expression distribution vectors.\n\nMER2023  [74] : The MER 2023 dataset was specifically designed to evaluate system robustness through three complementary tracks as shown in Fig.  7 . The MER-MULTI track requires participants to recognize both discrete categorical emotions and continuous dimensional attributes. The MER-NOISE track introduces artificial noise into test videos to systematically assess modality robustness,. The MER-SEMI track provides large volumes of unlabeled samples.\n\nEMER  [75] : The Explainable Multimodal Emotion Recognition (EMER) seeks to address the inherent subjectivity and ambiguity of affective labeling as shown in Fig.  7 . Each instance in the dataset not only contains multimodal signals (text, audio, and video), but is also paired with explanatory rationales that justify the emotion annotations.\n\nMERR  [23] : The Multimodal Emotion Recognition in Real-world (MERR) dataset integrates audio, visual, and textual modalities with rich affective annotations as shown in Fig.  7 . Each instance is labeled with both categorical emotions and continuous sentiment dimensions.\n\nDEEMO  [59] : The DEEMO corpus contains audio-visual-textual recordings paired with annotations covering categorical emotions, dimensional descriptors, and reasoning rationales, enabling not only recognition but also interpretability in emotion analysis.\n\nEmotionBench  [104] : The EmoBench dataset is a recently introduced benchmark specifically designed to assess the emotional intelligence of large language models (LLMs) as shown in Fig.  8 . EmoBench provides a comprehensive suite of emotion-related evaluations, including emotion recognition, cause identification, empathy assessment, and reasoning about affective situations. The dataset is linguistically diverse, containing carefully curated text-based scenarios that span a wide spectrum of emotional states and social contexts.\n\nMOSABench  [117] : The MOSABench corpus is designed to evaluate the fine-grained sentiment understanding capabilities of multimodal large language models (MLLMs). MOSABench emphasizes complex real-world scenes containing multiple objects and entities, each associated with distinct sentiment cues. The dataset pairs images with structured annotations covering object-level sentiment labels, relational context, and overall scene-level affect. MM-InstructEval  [150] : The MM-InstructEval benchmark is designed to assess the multimodal reasoning capabilities of large language models under zero-shot settings. It encompasses a broad spectrum of tasks that require integrating and reasoning over textual and visual modalities, including visual question answering, commonsense reasoning with images, multimodal entailment, and instruction following grounded in visual context.\n\nEIBench  [80] : The EIBench benchmark is specifically designed to evaluate the emotional intelligence of multimodal large language models (MLLMs). EIBench emphasizes higher-level emotional reasoning, including understanding the causes of emotions, interpreting multimodal affective cues, and generating contextually appropriate empathetic responses.",
      "page_start": 15,
      "page_end": 18
    },
    {
      "section_name": "Unified Label Mapping",
      "text": "The Feeling Wheel EmoryNLP  [156] : The EmoryNLP dataset is specifically constructed to capture fine-grained affective dynamics across multi-turn dialogues as shown in Fig.  7 . Extracted from episodes of the television series Friends, it contains over 12,000 utterances annotated with seven emotion categories, including neutral, joyful, peaceful, powerful, scared, mad, and sad.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Emotional Label Mapping",
      "text": "",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "One-Hot Speaker Label Mapping",
      "text": "IEMOCAP  [12] : The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset contains approximately 12 hours of audiovisual recordings from ten actors engaged in both scripted and improvised dyadic interactions as shown in Fig.  7 . Each utterance is annotated with both categorical emotion labels and dimensional attributes such as valence, arousal, and dominance.\n\nMELD  [101] : Derived from the EmotionLines corpus, the Multimodal EmotionLines Dataset (MELD) contains more than 1,400 dialogues and approximately 13,000 utterances sourced from the television series Friends, offering naturalistic, multi-party conversational settings as shown in Fig.  7 . Each utterance is annotated with categorical emotion labels and sentiment polarity.\n\nMC-EIU  [88] : The MC-EIU dataset is specifically designed to support joint modeling of emotions and intents in conversational settings. The dataset features over seven emotion categories and nine intent categories, spanning three modalities and is annotated in both English and Mandarin.\n\nOV-MER  [73] : The OV-MER dataset introduces the open-vocabulary paradigm into multimodal emotion recognition (MER). Unlike traditional datasets constrained by fixed emotion taxonomies, OV-MER is designed to support flexible prediction of emotions beyond predefined categories, thereby better reflecting the complexity, subtlety, and multi-appraisal nature of human affective experiences.\n\nCA-MER  [43] : The CA-MER dataset is designed to address the overlooked challenge of emotion conflicts in multimodal emotion recognition. Unlike conventional datasets that assume consistency across modalities, CA-MER explicitly models scenarios where emotional cues from audio and visual streams are misaligned.\n\nEmoBench-M  [46] : The EmoBench-M benchmark is designed to assess the emotional intelligence (EI) of multimodal large language models (MLLMs) in realistic interaction scenarios. Grounded in established psychological theories of EI, the dataset covers 13 evaluation scenarios spanning three critical dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis.\n\nEMER-Coarse  [76] : The EMER-Coarse dataset was developed as a complement to the smaller, manually verified EMER-Fine dataset, which is limited by high annotation costs. To expand coverage, EMER-Coarse adopts a simplified annotation pipeline, reduces reliance on manual verification, and leverages open-source models for large-scale automatic labeling.\n\nMME-Emotion  [158] : The MME-Emotion benchmark comprises over 6,000 curated video clips paired with task-specific question-answer sets, systematically covering eight emotion-related tasks across diverse scenarios as shown in Fig.  10 . MME-Emotion introduces a holistic evaluation framework that jointly assesses both emotional understanding and causal reasoning.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Experimental Performance",
      "text": "This study systematically evaluated the performance of large multimodal models in affective computing across multiple public benchmarks, covering basic emotion recognition (e.g., DFEW, MER, MELD, IEMOCAP, CMU-MOSI/MOSEI, etc.), cross-distance robustness testing (MOSABench), conversation and fine-grained sentiment analysis (EmoBench-M), humor, sarcasm, and reasoning comprehension (EmoBench-H), and comprehensive performance evaluation (MME-Emotion). MME-Emotion specifically measures the model's overall capabilities across three dimensions: recognition score (Rec-S), reasoning score (Rea-S), and Chain-of-Thought score (CoT-S). The experiments compared open-source and closed-source models, and zero-shot and fine-tuning settings. A systematic analysis was conducted across classification accuracy, precision, recall, and reasoning capabilities.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Results On Dfew Dataset",
      "text": "Table  2  contrasts zero-shot and fine-tuned performance on DFEW. The generic MLLMs operating in the parameter-frozen mode trail specialized video-affect models, whereas instruction-tuned multimodal models tailored for affect, such as Emotion-LLaMA, narrow the gap within the zero-shot regime. Once fine-tuning is allowed, video-centric foundations (e.g., masked autoencoding families and spatiotemporal learners) overtake zero-shot LLM baselines by a substantial margin, with pronounced gains on difficult categories like disgust, fear, and surprise. The class-wise distribution further indicates that neutral or high-arousal emotions are easier, while subtle negative states remain challenging, suggesting that model capacity alone is insufficient without targeted temporal supervision and calibrated class priors.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Results On Mosabench Dataset",
      "text": "Table  3  studies interlap/close/far object distances. Open-source MLLMs suffer a monotonic degradation as distance increases, reflecting their reliance on coarse visual tokenization that attenuates micro-expressions and local motion cues. Closed-source systems alleviate, but do not remove this decay. They typically trade higher precision for recall under far-distance settings, indicating stronger internal denoising or attention budgeting yet persistent difficulty in recovering fine-grained affect ), and OV-MERD+. Two consistent trends stand out. First, models that truly exploit audio and video rather than treating them as auxiliary captions achieve the highest mean across primary metrics, especially on speech-centric corpora (CH-SIMS, OV-MERD+), confirming the complementary nature of prosody and facial dynamics. Second, affect-oriented instruction tuning yields robust gains across heterogeneous label spaces, improving both word-level sentiment understanding (MOSI/MOSEI) and multi-party dialogue emotion tracking (MELD). The ranking also shows that text-only LLMs plateau on multi-modal datasets even with strong prompting, highlighting the necessity of connector learning and temporal reasoning in MLLMs.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Results On Emobench-M Dataset",
      "text": "Table  5  splits evaluation into foundational recognition tasks and conversational understanding. Open-source models excel when the target is categorical recognition or simple intensity regression, but they lag on dialogic intent and multi-facet affect reasoning, where closed-source systems Open-Source Model lead by a stable margin. Audio-augmented LLMs are particularly competitive on speech-centric subtasks, validating the importance of pitch/energy contours and pause structures, yet they still underperform on intent-level pragmatic cues that require longer-range discourse modeling and world knowledge. Table  6  probes humor, sarcasm, and laughter reasoning. These tasks are less about surface affect and more about incongruity, implicature, and commonsense. Closed-source models consistently dominate, and the gap widens as the evaluation emphasizes logical consistency and multi-hop reasoning. Longer generated chains and higher average token counts correlate with better reasoning scores but exhibit diminishing returns, reminding us that eliciting verbose chains is not a substitute for calibrated multi-modal grounding. The results motivate hybrid training with explicit counterfactuals and audiovisual incongruity exemplars.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Results On Mme-Emotion Dataset",
      "text": "",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Temporal Reasoning In Multimodal Emotion Understanding",
      "text": "A critical yet underexplored challenge in multimodal emotion recognition lies in modeling longrange and cross-modal temporal dependencies. Emotional states evolve dynamically over time, often manifesting through subtle, asynchronous cues across modalities. For instance, a delayed facial reaction following a spoken utterance, or a gradual shift in vocal pitch preceding a visible expression of distress. However, most existing multimodal large language models (MLLMs) either process inputs in fixed-length, non-overlapping windows or rely on static fusion mechanisms that ignore temporal ordering altogether. Even when recurrent or transformer-based encoders are employed, they often treat each modality independently before late fusion, thereby failing to capture inter-modal temporal alignment, such as whether a smile coincides with a positive lexical choice or contradicts it. Moreover, standard self-attention mechanisms in vision-language transformers are not inherently designed for fine-grained temporal reasoning. They may attend to salient frames or tokens but struggle to model causal progression, emotional inertia (e.g., lingering sadness after a triggering event), or anticipatory cues (e.g., rising tension before an outburst). This limitation is exacerbated in real-world scenarios where modalities are sampled at heterogeneous rates (e.g., 30 Hz video vs. 16 kHz audio) and exhibit variable latency or missing segments.\n\nTo address these issues, future MLLMs for emotion recognition should incorporate temporally grounded cross-modal architectures. Promising directions include: (1) synchronous temporal alignment modules that learn modality-specific time warping functions to align affective events across streams; (2) state-space models or neural ODEs that model emotion as a continuous latent trajectory; and (3) causal transformer variants with explicit memory buffers or recurrence to maintain emotional context over extended interactions. Additionally, benchmarks must evolve beyond frame-level or utterance-level labels to provide dense temporal annotations of emotional dynamics, enabling models to learn not just what emotion is present, but how it unfolds.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Scalable And Efficient Multimodal Architectures",
      "text": "Scaling MLLMs to handle multimodal emotion tasks raises both computational and generalization challenges, and future work must focus on architectures that balance performance with efficiency. Parameter-efficient tuning methods such as LoRA and QLoRA  [29]  should be extended to multimodal contexts, enabling adaptation to emotion recognition tasks without prohibitive memory and energy costs. Beyond fine-tuning, modular adapters  [39]  specialized for affective features could provide a lightweight pathway for injecting emotional sensitivity into general-purpose MLLMs. Mixture-of-Experts (MoE) frameworks  [78]  also hold promise, as they allow selective activation of modalityspecific experts during inference, reducing unnecessary computation while preserving task accuracy. Multi-scale temporal modeling represents another promising avenue: by first capturing coarse emotional trends at lower granularity and then refining local details such as micro-expressions or vocal tremors, models can achieve a more efficient trade-off between latency and fidelity. Selfsupervised pretraining on large, in-the-wild multimodal corpora will be indispensable to improve generalization across domains and demographics, as affective signals are inherently diverse and context-dependent. However, efficiency must also extend to deployment: quantization, pruning, and hardware-aware compilation should be systematically studied to enable real-time inference on mobile and wearable devices, thereby democratizing access to affective intelligence technologies.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Conclusion",
      "text": "As one of the fastest-growing areas in artificial intelligence, multimodal large language models (MLLMs) for emotion recognition and reasoning have made significant progress in recent years. Therefore, we provide a comprehensive review of this research area. First, we introduce its background and motivation and highlight the unique challenges posed by multimodal emotion understanding. Second, we present some preliminary research results, including a formal definition of multimodal emotion recognition and reasoning, the evolution of traditional multimodal methods, and the transition from LLM-based to MLLM-based approaches. Third, we propose a taxonomy of current methods, categorizing them into parameter freezing and parameter tuning paradigms, and further analyze zero-shot, few-shot, full-parameter, and parameter-efficient strategies. We then review representative models within each paradigm, tracing their development history and methodological innovations. Fourth, we summarize several commonly used multimodal emotion recognition and reasoning datasets. Fifth, we compare the performance of different methods on various multimodal emotion recognition datasets. Finally, we outline open challenges and future research directions, including unified multimodal backbone models, efficient adaptation strategies, and the integration of causal reasoning and common-sense knowledge in emotion.",
      "page_start": 27,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The goal of this study is to explore and summarize the current development and applications of",
      "page": 2
    },
    {
      "caption": "Figure 1: A chronological overview of representative MLLMs is presented, highlighting the rapid growth of this",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). Formally, let ğœ™ğ‘š(Â·) denote the encoder for modality ğ‘šâˆˆ{text}, the",
      "page": 7
    },
    {
      "caption": "Figure 2: (b), embeddings from text, audio,",
      "page": 7
    },
    {
      "caption": "Figure 2: Comparison between LLM-based and MLLM-based methods for multimodal emotion recognition",
      "page": 8
    },
    {
      "caption": "Figure 3: , we categorize multimodal emotion recognition and reasoning methods",
      "page": 8
    },
    {
      "caption": "Figure 3: Taxonomy of Multimodal Emotion Recognition and Reasoning. We systematically categorize",
      "page": 9
    },
    {
      "caption": "Figure 4: , the connectors, and optionally the modality encoders:",
      "page": 11
    },
    {
      "caption": "Figure 4: End-to-end full-parameter tuning architecture for multimodal emotion recognition and reasoning.",
      "page": 12
    },
    {
      "caption": "Figure 5: Popular instantiations include Adapter-tuning",
      "page": 12
    },
    {
      "caption": "Figure 5: Parameter-efficient tuning (PET) architecture for multimodal emotion recognition and reasoning.",
      "page": 13
    },
    {
      "caption": "Figure 6: Pipeline for building multimodal emotion recognition and reasoning.",
      "page": 14
    },
    {
      "caption": "Figure 6: illustrates that audio, visual, and textual cues are jointly processed to produce explainable",
      "page": 14
    },
    {
      "caption": "Figure 7: Each video clip is annotated through",
      "page": 17
    },
    {
      "caption": "Figure 7: The MER-MULTI track requires participants",
      "page": 17
    },
    {
      "caption": "Figure 7: Each instance in",
      "page": 17
    },
    {
      "caption": "Figure 7: Each instance",
      "page": 17
    },
    {
      "caption": "Figure 8: EmoBench provides a comprehensive suite of emotion-related evaluations, including emotion",
      "page": 17
    },
    {
      "caption": "Figure 7: Comparative analysis of label distributions across the DFEW, MER2023, EMER, and MERRdatasets.",
      "page": 18
    },
    {
      "caption": "Figure 8: Category Distribution in EmotionBench. The main categories are depicted within the chart, and the",
      "page": 18
    },
    {
      "caption": "Figure 9: Unified Label Mapping Across three Open-source Benchmarks. [56]",
      "page": 19
    },
    {
      "caption": "Figure 10: Overview of MME-Emotion Statistics. Left: Task Types. MME-Emotion encompasses eight emotional",
      "page": 19
    },
    {
      "caption": "Figure 7: Extracted from episodes of",
      "page": 19
    },
    {
      "caption": "Figure 7: Each utterance is annotated with both categorical",
      "page": 19
    },
    {
      "caption": "Figure 7: Each utterance is annotated with categorical emotion labels and sentiment polarity.",
      "page": 20
    },
    {
      "caption": "Figure 10: MME-Emotion introduces a holistic evaluation",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Main results. â€œAâ€, â€œVâ€, and â€œTâ€ represent audio, video, and text. The gray-highlighted columns",
      "data": [
        {
          "A\nV\nT\nâˆš\nâˆš": "Otter [58]\nÃ—\nâˆš\nâˆš\nOneLLM [42]\nÃ—\nâˆš\nâˆš\nVideo-LLaVA [79]\nÃ—\nâˆš\nâˆš\nSECap [144]\nÃ—\nâˆš\nâˆš\nPandaGPT [118]\nÃ—\nâˆš\nâˆš\nQwen-Audio [26]\nÃ—\nâˆš\nâˆš\nPandaGPT [118]\nÃ—\nâˆš\nâˆš\nVideo-ChatGPT [92]\nÃ—\nâˆš\nâˆš\nVideoChat2 [65]\nÃ—\nâˆš\nâˆš\nâˆš\nPandaGPT [118]\nâˆš\nâˆš\nLLaMA-VID [69]\nÃ—\nâˆš\nâˆš\nVideoChat [64]\nÃ—\nâˆš\nâˆš\nSALMONN [124]\nÃ—\nâˆš\nâˆš\nChat-UniVi [52]\nÃ—\nâˆš\nâˆš\nmPLUG-Owl [152]\nÃ—\nâˆš\nâˆš\nâˆš\nEmotion-LLaMA [21]\nâˆš\nâˆš\nâˆš\nAffectGPT [76]",
          "MER2023\nHIT(â†‘)": "16.41\n25.52\n36.93\n40.95\n33.57\n41.85\n39.13\n44.86\n33.67\n40.21\n50.72\n48.73\n55.53\n57.62\n56.86\n59.38\n78.54",
          "MER2024\nHIT(â†‘)": "14.65\n17.21\n30.25\n52.46\n39.04\n31.61\n47.16\n46.80\n54.50\n51.89\n57.60\n57.30\n45.38\n65.67\n59.89\n73.62\n78.80",
          "MELD\nHIT(â†‘)": "22.57\n28.32\n30.73\n25.56\n31.91\n49.09\n38.33\n37.33\n36.64\n37.88\n42.75\n41.11\n45.62\n45.61\n49.11\n46.76\n55.65",
          "IEMOCAP\nHIT(â†‘)": "29.08\n33.44\n38.95\n36.92\n36.55\n35.47\n47.21\n56.83\n48.70\n44.04\n46.02\n48.38\n46.84\n52.37\n55.54\n55.47\n60.54",
          "CMU-MOSI\nWAF(â†‘)\nACC(â†‘)": "52.89\n54.27\n64.01\n64.48\n56.37\n57.62\n55.76\n56.71\n66.06\n65.85\n70.09\n71.49\n58.50\n60.21\n54.42\n57.77\n66.84\n67.23\n61.92\n62.80\n61.78\n62.65\n65.13\n65.09\n81.00\n81.25\n54.53\n57.62\n72.40\n72.26\n66.13\n66.31\n81.30\n81.25",
          "CMU-MOSEI\nWAF(â†‘)\nACC(â†‘)": "50.44\n50.77\n54.09\n54.18\n61.64\n64.20\n54.18\n53.85\n61.33\n60.73\n46.90\n51.16\n64.25\n65.55\n63.12\n65.66\n54.32\n54.82\n67.61\n68.82\n63.89\n66.21\n63.61\n63.02\n67.03\n66.90\n63.18\n67.47\n72.91\n73.17\n67.25\n67.66\n80.90\n80.68"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 4: Main results. â€œAâ€, â€œVâ€, and â€œTâ€ represent audio, video, and text. The gray-highlighted columns",
      "data": [
        {
          "A\nV\nT\nâˆš\nâˆš": "Otter [58]\nÃ—\nâˆš\nâˆš\nOneLLM [42]\nÃ—\nâˆš\nâˆš\nVideo-LLaVA [79]\nÃ—\nâˆš\nâˆš\nSECap [144]\nÃ—\nâˆš\nâˆš\nPandaGPT [118]\nÃ—\nâˆš\nâˆš\nQwen-Audio [26]\nÃ—\nâˆš\nâˆš\nPandaGPT [118]\nÃ—\nâˆš\nâˆš\nVideo-ChatGPT [92]\nÃ—\nâˆš\nâˆš\nVideoChat2 [65]\nÃ—\nâˆš\nâˆš\nâˆš\nPandaGPT [118]\nâˆš\nâˆš\nLLaMA-VID [69]\nÃ—\nâˆš\nâˆš\nVideoChat [64]\nÃ—\nâˆš\nâˆš\nSALMONN [124]\nÃ—\nâˆš\nâˆš\nChat-UniVi [52]\nÃ—\nâˆš\nâˆš\nmPLUG-Owl [152]\nÃ—\nâˆš\nâˆš\nâˆš\nEmotion-LLaMA [21]\nâˆš\nâˆš\nâˆš\nAffectGPT [76]",
          "CH-SIMS\nWAF(â†‘)\nACC(â†‘)": "57.56\n60.57\n63.39\n63.92\n53.28\n54.64\n59.51\n62.89\n62.93\n62.37\n70.73\n73.45\n62.07\n61.60\n64.82\n64.43\n69.49\n69.59\n68.38\n67.78\n69.35\n68.81\n69.52\n69.33\n68.69\n69.85\n68.15\n67.78\n71.65\n72.13\n78.32\n78.61\n88.49\n88.40",
          "CH-SIMS v2\nWAF(â†‘)\nACC(â†‘)": "53.12\n56.20\n61.98\n62.46\n57.45\n59.28\n57.41\n60.92\n58.88\n58.84\n65.26\n68.17\n65.25\n65.31\n65.80\n66.85\n70.66\n71.13\n67.23\n67.40\n67.48\n67.73\n72.14\n72.12\n65.93\n67.07\n66.36\n67.18\n75.00\n74.97\n77.23\n77.39\n86.18\n86.17",
          "OV-MERD+\nFs(â†‘)\nPrecisions(â†‘)\nRecalls(â†‘)": "16.63\n17.67\n15.74\n22.25\n24.49\n20.41\n34.00\n36.48\n31.86\n36.97\n43.51\n32.17\n31.33\n33.08\n29.77\n32.36\n38.52\n27.91\n35.07\n37.86\n32.67\n39.80\n43.12\n36.97\n39.21\n42.85\n36.16\n37.12\n39.64\n34.91\n45.01\n46.83\n43.32\n44.52\n44.55\n44.49\n45.00\n43.57\n46.61\n48.00\n47.81\n48.20\n47.91\n48.18\n48.47\n52.97\n54.85\n51.22\n62.52\n62.21\n63.00",
          "Mean": "34.82\n41.14\n44.40\n46.64\n46.84\n49.26\n50.77\n52.64\n52.67\n52.92\n56.07\n56.71\n57.89\n57.94\n62.45\n64.17\n74.77"
        }
      ],
      "page": 22
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Recognition and Reasoning Parameter-Frozen Paradigm ( Â§3",
      "venue": "Emotion Recognition and Reasoning Parameter-Frozen Paradigm ( Â§3"
    },
    {
      "citation_id": "2",
      "title": "MiniCPM-V-2.6-8B [151] Video-LLaMA2-7B [22], Video-LLaMA2.1-7B-16F [22], Video-LLaMA2.1-7B-AV [22], InternVideo2-Chat-8B",
      "venue": "MiniCPM-V-2.6-8B [151] Video-LLaMA2-7B [22], Video-LLaMA2.1-7B-16F [22], Video-LLaMA2.1-7B-AV [22], InternVideo2-Chat-8B"
    },
    {
      "citation_id": "3",
      "title": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. 2023. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "4",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "Wei Ai",
        "Yuntao Shou",
        "Tao Meng",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "5",
      "title": "Revisiting Multimodal Emotion Recognition in Conversation from the Perspective of Graph Spectrum",
      "authors": [
        "Wei Ai",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Tao Meng",
        "Haowen Chen",
        "Keqin Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "Shuai Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Sibo Song",
        "Kai Dang",
        "Peng Wang"
      ],
      "year": "2025",
      "venue": "",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "9",
      "title": "Qwen2. 5-vl technical report",
      "authors": [
        "Shuai Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Sibo Song",
        "Kai Dang",
        "Peng Wang",
        "Shijie Wang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-vl technical report",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "10",
      "title": "Prompting language-informed distribution for compositional zero-shot learning",
      "authors": [
        "Wentao Bao",
        "Lichang Chen",
        "Heng Huang",
        "Yu Kong"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "LIRIS-ACCEDE: A video database for affective content analysis",
      "authors": [
        "Yoann Baveye",
        "Emmanuel Dellandrea",
        "Christel Chamaret",
        "Liming Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Deeppavlov at semeval-2024 task 3: Multimodal large language models in emotion reasoning",
      "authors": [
        "Julia Belikova",
        "Dmitrii Kosenko"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "13",
      "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
      "authors": [
        "Elad Ben-Zaken",
        "Shauli Ravfogel",
        "Yoav Goldberg"
      ],
      "year": "2022",
      "venue": "60th Annual Meeting of the Association for Computational Linguistics, ACL 2022"
    },
    {
      "citation_id": "14",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "LLMs Are Few-Shot In-Context Low-Resource Language Learners",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Pascale Fung"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter"
    },
    {
      "citation_id": "16",
      "title": "Internlm2 technical report",
      "authors": [
        "Zheng Cai",
        "Maosong Cao",
        "Haojiong Chen",
        "Kai Chen",
        "Keyu Chen",
        "Xin Chen",
        "Xun Chen",
        "Zehui Chen",
        "Zhi Chen",
        "Pei Chu"
      ],
      "year": "2024",
      "venue": "Internlm2 technical report",
      "arxiv": "arXiv:2403.17297"
    },
    {
      "citation_id": "17",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Hao Chen",
        "Xiaoyuan Yi",
        "Cunxiang Wang",
        "Yidong Wang"
      ],
      "year": "2024",
      "venue": "ACM transactions on intelligent systems and technology"
    },
    {
      "citation_id": "18",
      "title": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative Approach to Few-shot Multimodal Dialogue Intention Recognition",
      "authors": [
        "Bin Chen",
        "Yu Zhang",
        "Hongfei Ye",
        "Ziyi Huang",
        "Hongyang Chen"
      ],
      "year": "2025",
      "venue": "Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative Approach to Few-shot Multimodal Dialogue Intention Recognition",
      "arxiv": "arXiv:2503.04201"
    },
    {
      "citation_id": "19",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "20",
      "title": "From static to dynamic: Adapting landmarkaware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "Uniter: Universal image-text representation learning"
    },
    {
      "citation_id": "22",
      "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Zhaoyang Hao Tian",
        "Liu"
      ],
      "year": "2024",
      "venue": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "arxiv": "arXiv:2412.05271"
    },
    {
      "citation_id": "23",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "authors": [
        "Zesen Cheng",
        "Sicong Leng",
        "Hang Zhang",
        "Yifei Xin",
        "Xin Li",
        "Guanzheng Chen",
        "Yongxin Zhu",
        "Wenqi Zhang",
        "Ziyang Luo",
        "Deli Zhao",
        "Lidong Bing"
      ],
      "year": "2024",
      "venue": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "25",
      "title": "Sztu-cmu at mer2024: Improving emotion-llama with conv-attention for multimodal emotion recognition",
      "authors": [
        "Zebang Cheng",
        "Shuyuan Tu",
        "Dawei Huang",
        "Minghan Li",
        "Xiaojiang Peng",
        "Zhi-Qi Cheng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo"
      ],
      "year": "2024",
      "venue": "",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "27",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "28",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "29",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities",
      "authors": [
        "Gheorghe Comanici",
        "Eric Bieber",
        "Mike Schaekermann",
        "Ice Pasupat",
        "Noveen Sachdeva",
        "Inderjit Dhillon",
        "Marcel Blistein",
        "Ori Ram",
        "Dan Zhang",
        "Evan Rosen"
      ],
      "year": "2025",
      "venue": "Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities",
      "arxiv": "arXiv:2507.06261"
    },
    {
      "citation_id": "31",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "Tim Dettmers",
        "Artidoro Pagnoni",
        "Ari Holtzman",
        "Luke Zettlemoyer"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "32",
      "title": "Collecting Large, Richly Annotated Facial-Expression Databases from Movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "33",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "Abhinav Dhall",
        "Roland Ov Ramana Murthy",
        "Jyoti Goecke",
        "Tom Joshi",
        "Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "34",
      "title": "The HUMAINE database: Addressing the collection and annotation of naturalistic and induced emotional data",
      "authors": [
        "Ellen Douglas-Cowie",
        "Roddy Cowie",
        "Ian Sneddon",
        "Cate Cox",
        "Orla Lowry",
        "Margaret Mcrorie",
        "Jean-Claude Martin",
        "Laurence Devillers",
        "Sarkis Abrilian",
        "Anton Batliner"
      ],
      "year": "2007",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "35",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Contextual emotion recognition using large vision language models",
      "authors": [
        "Yasaman Etesam",
        "Ã–zge Nilay YalÃ§Ä±n",
        "Chuxuan Zhang",
        "Angelica Lim"
      ],
      "year": "2024",
      "venue": "2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "37",
      "title": "Affect Recognition in Conversations Using Large Language Models",
      "authors": [
        "Shutong Feng",
        "Guangzhi Sun",
        "Nurul Lubis",
        "Wen Wu",
        "Chao Zhang",
        "Milica Gasic"
      ],
      "year": "2024",
      "venue": "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "38",
      "title": "Emilya: Emotional body expression in daily actions database",
      "authors": [
        "Nesrine Fourati",
        "Catherine Pelachaud"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14"
    },
    {
      "citation_id": "39",
      "title": "AIM: Let Any Multimodal Large Language Models Embrace Efficient In-Context Learning",
      "authors": [
        "Jun Gao",
        "Qian Qiao",
        "Tianxiang Wu",
        "Zili Wang",
        "Ziqiang Cao",
        "Wenjie Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "2023. iEarth: an interdisciplinary framework in the era of big data and AI for sustainable development",
      "authors": [
        "Peng Gong",
        "Huadong Guo",
        "Bin Chen",
        "Fang Chen",
        "Guojun He",
        "Dong Liang",
        "Zhonghui Liu",
        "Zhongchang Sun",
        "Jin Wu",
        "Zhenci Xu"
      ],
      "year": "2023",
      "venue": "National Science Review"
    },
    {
      "citation_id": "41",
      "title": "Fe-adapter: Adapting image-based emotion classifiers to videos",
      "authors": [
        "Boyan Shreyank N Gowda",
        "David Gao",
        "Clifton"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "42",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "2008 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "43",
      "title": "Few-shot object detection with foundation models",
      "authors": [
        "Guangxing Han",
        "Ser-Nam Lim"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Onellm: One framework to align all modalities with language",
      "authors": [
        "Jiaming Han",
        "Kaixiong Gong",
        "Yiyuan Zhang",
        "Jiaqi Wang",
        "Kaipeng Zhang",
        "Dahua Lin",
        "Yu Qiao",
        "Peng Gao",
        "Xiangyu Yue"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning",
      "authors": [
        "Zhiyuan Han",
        "Beier Zhu",
        "Yanlong Xu",
        "Peipei Song",
        "Xun Yang"
      ],
      "year": "2025",
      "venue": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning",
      "arxiv": "arXiv:2508.01181"
    },
    {
      "citation_id": "46",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "48",
      "title": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models",
      "authors": [
        "He Hu",
        "Yucheng Zhou",
        "Lianzhong You",
        "Hongbo Xu",
        "Qianning Wang",
        "Zheng Lian",
        "Richard Fei",
        "Fei Yu",
        "Laizhong Ma",
        "Cui"
      ],
      "year": "2025",
      "venue": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models",
      "arxiv": "arXiv:2502.04424"
    },
    {
      "citation_id": "49",
      "title": "Bliva: A simple multimodal llm for better handling of text-rich visual questions",
      "authors": [
        "Wenbo Hu",
        "Yifan Xu",
        "Yi Li",
        "Weiyue Li",
        "Zeyuan Chen",
        "Zhuowen Tu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Gpt-4o system card",
      "authors": [
        "Aaron Hurst",
        "Adam Lerer",
        "Adam Goucher",
        "Adam Perelman",
        "Aditya Ramesh",
        "Aidan Clark",
        "Akila Ostrow",
        "Alan Welihinda",
        "Alec Hayes",
        "Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "51",
      "title": "Elp-adapters: Parameter efficient adapter tuning for various speech processing tasks",
      "authors": [
        "Nakamasa Inoue",
        "Shinta Otake",
        "Takumi Hirose",
        "Masanari Ohi",
        "Rei Kawakami"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Zero-shot Prompting for LLM-based Machine Translation Using In-domain Target Sentences",
      "authors": [
        "Baijun Ji",
        "Xiangyu Duan",
        "Yue Zhang",
        "Kaixin Wu",
        "Min Zhang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "53",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "54",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Wancai Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "56",
      "title": "Segment anything",
      "authors": [
        "Alexander Kirillov",
        "Eric Mintun",
        "Nikhila Ravi",
        "Hanzi Mao",
        "Chloe Rolland",
        "Laura Gustafson",
        "Tete Xiao",
        "Spencer Whitehead",
        "Alexander Berg",
        "Wan-Yen Lo"
      ],
      "year": "2023",
      "venue": "Proceedings"
    },
    {
      "citation_id": "57",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "58",
      "title": "InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Runqi Qiao",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "InstructERC: Reforming Emotion Recognition in Conversation with Multi-task Retrieval-Augmented Large Language Models",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "59",
      "title": "Large Vision-Language Models as Emotion Recognizers in Context Awareness",
      "authors": [
        "Yuxuan Lei",
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Jiawei Chen",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2025",
      "venue": "Asian Conference on Machine Learning"
    },
    {
      "citation_id": "60",
      "title": "Otter: A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Fanyi Pu",
        "Adrian Cahyono",
        "Jingkang Yang",
        "Chunyuan Li",
        "Ziwei Liu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Deemo: De-identity multimodal emotion recognition and reasoning",
      "authors": [
        "Deng Li",
        "Bohao Xing",
        "Xin Liu",
        "Baiqiang Xia",
        "Bihan Wen",
        "Heikki KÃ¤lviÃ¤inen"
      ],
      "year": "2025",
      "venue": "Deemo: De-identity multimodal emotion recognition and reasoning",
      "arxiv": "arXiv:2504.19549"
    },
    {
      "citation_id": "62",
      "title": "Deep learning for natural language processing: advantages and challenges",
      "authors": [
        "Hang Li"
      ],
      "year": "2018",
      "venue": "National Science Review"
    },
    {
      "citation_id": "63",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "64",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "65",
      "title": "Chemvlm: Exploring the power of multimodal large language models in chemistry area",
      "authors": [
        "Junxian Li",
        "Di Zhang",
        "Xunzhi Wang",
        "Zeying Hao",
        "Jingdi Lei",
        "Qian Tan",
        "Cai Zhou",
        "Wei Liu",
        "Yaotian Yang",
        "Xinrui Xiong"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "67",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "68",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "Liunian Harold",
        "Mark Yatskar",
        "Cho-Jui Da Yin",
        "Kai-Wei Hsieh",
        "Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "69",
      "title": "Facial affective behavior analysis with instruction tuning",
      "authors": [
        "Yifan Li",
        "Anh Dao",
        "Wentao Bao",
        "Zhen Tan",
        "Tianlong Chen",
        "Huan Liu",
        "Yu Kong"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "70",
      "title": "Multimodal pear chain-of-thought reasoning for multimodal sentiment analysis",
      "authors": [
        "Yan Li",
        "Xiangyuan Lan",
        "Haifeng Chen",
        "Ke Lu",
        "Dongmei Jiang"
      ],
      "year": "2025",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "71",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "72",
      "title": "Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering",
      "authors": [
        "Zhenyu Li",
        "Sunqi Fan",
        "Yu Gu",
        "Xiuxing Li",
        "Zhichao Duan",
        "Bowen Dong",
        "Ning Liu",
        "Jianyong Wang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "73",
      "title": "Monkey: Image resolution and text label are important things for large multi-modal models",
      "authors": [
        "Zhang Li",
        "Biao Yang",
        "Qiang Liu",
        "Zhiyin Ma",
        "Shuo Zhang",
        "Jingxu Yang",
        "Yabo Sun",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "year": "2024",
      "venue": "Monkey: Image resolution and text label are important things for large multi-modal models"
    },
    {
      "citation_id": "74",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "75",
      "title": "OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Haoyu Chen",
        "Lan Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Zhang Siyuan",
        "Hailiang Yao"
      ],
      "year": "2024",
      "venue": "Forty-second International Conference on Machine Learning"
    },
    {
      "citation_id": "76",
      "title": "Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM international conference on multimedia"
    },
    {
      "citation_id": "77",
      "title": "Explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Hao Gu",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Mingyu Xu",
        "Ke Xu",
        "Kang Chen"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion recognition",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "78",
      "title": "AffectGPT: Dataset and framework for explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "AffectGPT: Dataset and framework for explainable multimodal emotion recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "79",
      "title": "GPT-4V with emotion: A zero-shot benchmark for Generalized Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "80",
      "title": "Moe-llava: Mixture of experts for large vision-language models",
      "authors": [
        "Bin Lin",
        "Zhenyu Tang",
        "Yang Ye",
        "Jiaxi Cui",
        "Bin Zhu",
        "Peng Jin",
        "Jinfa Huang",
        "Junwu Zhang",
        "Yatian Pang",
        "Munan Ning"
      ],
      "year": "2024",
      "venue": "Moe-llava: Mixture of experts for large vision-language models",
      "arxiv": "arXiv:2401.15947"
    },
    {
      "citation_id": "81",
      "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
      "authors": [
        "Bin Lin",
        "Yang Ye",
        "Bin Zhu",
        "Jiaxi Cui",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "82",
      "title": "Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models",
      "authors": [
        "Yuxiang Lin",
        "Jingdong Sun",
        "Zhi-Qi Cheng",
        "Jue Wang",
        "Haomin Liang",
        "Zebang Cheng",
        "Yifei Dong",
        "Jun-Yan He",
        "Xiaojiang Peng",
        "Xian-Sheng Hua"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "83",
      "title": "Mind with eyes: from language reasoning to multimodal reasoning",
      "authors": [
        "Zhiyu Lin",
        "Yifei Gao",
        "Xian Zhao",
        "Yunfan Yang",
        "Jitao Sang"
      ],
      "year": "2025",
      "venue": "Mind with eyes: from language reasoning to multimodal reasoning",
      "arxiv": "arXiv:2503.18071"
    },
    {
      "citation_id": "84",
      "title": "Boosting multimodal large language models with visual tokens withdrawal for rapid inference",
      "authors": [
        "Zhihang Lin",
        "Mingbao Lin",
        "Luxi Lin",
        "Rongrong Ji"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "85",
      "title": "Llava-next: Improved reasoning, ocr, and world knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Llava-next: Improved reasoning, ocr, and world knowledge"
    },
    {
      "citation_id": "86",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "87",
      "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
      "authors": [
        "Haokun Liu",
        "Derek Tam",
        "Mohammed Muqeeth",
        "Jay Mohta",
        "Tenghao Huang",
        "Mohit Bansal",
        "Colin Raffel"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "88",
      "title": "Llm4gen: Leveraging semantic representation of llms for text-to-image generation",
      "authors": [
        "Mushui Liu",
        "Yuhang Ma",
        "Zhen Yang",
        "Jun Dan",
        "Yunlong Yu",
        "Zeng Zhao",
        "Zhipeng Hu",
        "Bai Liu",
        "Changjie Fan"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "89",
      "title": "When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications",
      "authors": [
        "Qidong Liu",
        "Xian Wu",
        "Xiangyu Zhao",
        "Yuanshao Zhu",
        "Derong Xu",
        "Feng Tian",
        "Yefeng Zheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "90",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Xiaofen Xing",
        "BjÃ¶rn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "91",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "Xin Liu",
        "Henglin Shi",
        "Haoyu Chen",
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "92",
      "title": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Zhiwei Liu",
        "Kailai Yang",
        "Qianqian Xie",
        "Tianlin Zhang",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "93",
      "title": "A transformer-based model with selfdistillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "94",
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "authors": [
        "Muhammad Maazi",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Khan"
      ],
      "year": "2024",
      "venue": "62nd Annual Meeting of the Association-for-Computational-Linguistics (ACL)/Student Research Workshop (SRW)"
    },
    {
      "citation_id": "95",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "96",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Haiyan Liu",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "97",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "Tao Meng",
        "Yuntao Shou",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "98",
      "title": "Masked graph learning with recurrent alignment for multimodal emotion recognition in conversation",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Hongen Shao",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "99",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "100",
      "title": "Gpt-4.1 system card",
      "authors": [
        "Openai"
      ],
      "year": "2025",
      "venue": "Gpt-4.1 system card"
    },
    {
      "citation_id": "101",
      "title": "Convis: Contrastive decoding with hallucination visualization for mitigating hallucinations in multimodal large language models",
      "authors": [
        "Yeji Park",
        "Deokyeong Lee",
        "Junsuk Choe",
        "Buru Chang"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "102",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "103",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "104",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "105",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "Machel Reid",
        "Nikolay Savinov",
        "Denis Teplyashin",
        "Dmitry Lepikhin",
        "Timothy Lillicrap"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "106",
      "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
      "authors": [
        "Sahand Sabour",
        "Siyang Liu",
        "Zheyuan Zhang",
        "June Liu",
        "Jinfeng Zhou",
        "Alvionna Sunaryo",
        "Tatia Lee",
        "Rada Mihalcea",
        "Minlie Huang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "107",
      "title": "Aligning and prompting everything all at once for universal visual perception",
      "authors": [
        "Yunhang Shen",
        "Chaoyou Fu",
        "Peixian Chen",
        "Mengdan Zhang",
        "Ke Li",
        "Xing Sun",
        "Yunsheng Wu",
        "Shaohui Lin",
        "Rongrong Ji"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "108",
      "title": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Tao Meng",
        "Haiyan Liu",
        "Nan Yin"
      ],
      "year": "2024",
      "venue": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "109",
      "title": "Masked contrastive graph representation learning for age estimation",
      "authors": [
        "Yuntao Shou",
        "Xiangyong Cao",
        "Huan Liu",
        "Deyu Meng"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "110",
      "title": "Spegcl: Self-supervised graph spectrum contrastive learning without positive samples",
      "authors": [
        "Yuntao Shou",
        "Xiangyong Cao",
        "Deyu Meng"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "111",
      "title": "Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "authors": [
        "Yuntao Shou",
        "Haozhi Lan",
        "Xiangyong Cao"
      ],
      "year": "2025",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "112",
      "title": "A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Yuntao Shou",
        "Huan Liu",
        "Xiangyong Cao",
        "Deyu Meng",
        "Bo Dong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "113",
      "title": "Dynamic Graph Neural ODE Network for Multi-modal Emotion Recognition in Conversation",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Keqin Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the 31st International Conference on Computational Linguistics"
    },
    {
      "citation_id": "114",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Sihan Yang",
        "Keqin Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "115",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "116",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "117",
      "title": "Graph domain adaptation with dual-branch encoder and two-level alignment for whole slide image-based survival prediction",
      "authors": [
        "Yuntao Shou",
        "Peiqiang Yan",
        "Xingjian Yuan",
        "Xiangyong Cao",
        "Qian Zhao",
        "Deyu Meng"
      ],
      "year": "2024",
      "venue": "Graph domain adaptation with dual-branch encoder and two-level alignment for whole slide image-based survival prediction",
      "arxiv": "arXiv:2411.14001"
    },
    {
      "citation_id": "118",
      "title": "GSDNet: Revisiting Incomplete Multimodality-Diffusion Emotion Recognition from the Perspective of Graph Spectrum",
      "authors": [
        "Yuntao Shou",
        "Jun Yao",
        "Tao Meng",
        "Wei Ai",
        "Cen Chen",
        "Keqin Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "119",
      "title": "MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image",
      "authors": [
        "Shezheng Song",
        "Chengxiang He",
        "Shasha Li",
        "Shan Zhao",
        "Chengyu Wang",
        "Tianwei Yan",
        "Xiaopeng Li",
        "Qian Wan",
        "Jun Ma",
        "Jie Yu"
      ],
      "year": "2024",
      "venue": "MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image",
      "arxiv": "arXiv:2412.00060"
    },
    {
      "citation_id": "120",
      "title": "PandaGPT: One Model To Instruction-Follow Them All",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "121",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "122",
      "title": "Muti-modal emotion recognition via hierarchical knowledge distillation",
      "authors": [
        "Teng Sun",
        "Yinwei Wei",
        "Juntong Ni",
        "Zixin Liu",
        "Xuemeng Song",
        "Yaowei Wang",
        "Liqiang Nie"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "123",
      "title": "Ernie 2.0: A continual pre-training framework for language understanding",
      "authors": [
        "Yu Sun",
        "Shuohuan Wang",
        "Yukun Li",
        "Shikun Feng",
        "Hua Hao Tian",
        "Haifeng Wu",
        "Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "124",
      "title": "DialogueMLLM: Transforming Multimodal Emotion Recognition in Conversation through Instruction-Tuned MLLM",
      "authors": [
        "Yuanyuan Sun",
        "Ting Zhou"
      ],
      "year": "2025",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "125",
      "title": "Link-context learning for multimodal llms",
      "authors": [
        "Yan Tai",
        "Weichen Fan",
        "Zhao Zhang",
        "Ziwei Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "126",
      "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "M Zejun",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "127",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth",
        "Katie Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "128",
      "title": "LearnLM: Improving Gemini for Learning",
      "authors": [
        "Learnlm Team",
        "Abhinit Modi",
        "Aditya Veerubhotla",
        "Aliya Rysbek",
        "Andrea Huber",
        "Brett Wiltshire",
        "Brian Veprek",
        "Daniel Gillick",
        "Daniel Kasenberg",
        "Derek Ahmed"
      ],
      "year": "2024",
      "venue": "LearnLM: Improving Gemini for Learning",
      "arxiv": "arXiv:2412.16429"
    },
    {
      "citation_id": "129",
      "title": "QVQ: To See the World with Wisdom",
      "authors": [
        "Qwen Team"
      ],
      "year": "2024",
      "venue": "QVQ: To See the World with Wisdom"
    },
    {
      "citation_id": "130",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural Information Processing Systems"
    },
    {
      "citation_id": "131",
      "title": "Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "132",
      "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
      "authors": [
        "Zhen Wan",
        "Fei Cheng",
        "Zhuoyuan Mao",
        "Qianying Liu",
        "Haiyue Song",
        "Jiwei Li",
        "Sadao Kurohashi"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "133",
      "title": "Llms as zero-shot graph learners: Alignment of gnn representations with llm token embeddings",
      "authors": [
        "Duo Wang",
        "Yuan Zuo",
        "Fengzhi Li",
        "Junjie Wu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "134",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "135",
      "title": "M2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
      "authors": [
        "Taowen Wang",
        "Yiyang Liu",
        "James Liang",
        "Junhan Zhao",
        "Yiming Cui",
        "Yuning Mao",
        "Shaoliang Nie",
        "Jiahao Liu",
        "Fuli Feng",
        "Zenglin Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "136",
      "title": "Cogvlm: Visual expert for pretrained language models",
      "authors": [
        "Weihan Wang",
        "Qingsong Lv",
        "Wenmeng Yu",
        "Wenyi Hong",
        "Ji Qi",
        "Yan Wang",
        "Junhui Ji",
        "Zhuoyi Yang",
        "Lei Zhao",
        "Song Xixuan"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "137",
      "title": "Few-Shot In-Context Learning for Implicit Semantic Multimodal Content Detection and Interpretation",
      "authors": [
        "Xiuxian Wang",
        "Lanjun Wang",
        "Yuting Su",
        "Hongshuo Tian",
        "Guoqing Jin",
        "An-An Liu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "138",
      "title": "How far can camels go? exploring the state of instruction tuning on open resources",
      "authors": [
        "Yizhong Wang",
        "Hamish Ivison",
        "Pradeep Dasigi",
        "Jack Hessel",
        "Tushar Khot",
        "Khyathi Chandu",
        "David Wadden",
        "Kelsey Macmillan",
        "Noah Smith",
        "Iz Beltagy"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "139",
      "title": "Internvideo2: Scaling foundation models for multimodal video understanding",
      "authors": [
        "Yi Wang",
        "Kunchang Li",
        "Xinhao Li",
        "Jiashuo Yu",
        "Yinan He",
        "Guo Chen",
        "Baoqi Pei",
        "Rongkun Zheng",
        "Zun Wang",
        "Yansong Shi"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "140",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "141",
      "title": "ğ‘ğ‘’ğ‘¡ğ‘-DPO: Direct Preference Optimization with Dynamicğ‘ğ‘’ğ‘¡ğ‘",
      "authors": [
        "Junkang Wu",
        "Yuexiang Xie",
        "Zhengyi Yang",
        "Jiancan Wu",
        "Jinyang Gao",
        "Bolin Ding",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "142",
      "title": "Harnessing generative AI to decode enzyme catalysis and evolution for enhanced engineering",
      "authors": [
        "Wen Jun",
        "Arieh Warshel"
      ],
      "year": "2023",
      "venue": "National Science Review"
    },
    {
      "citation_id": "143",
      "title": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "year": "2025",
      "venue": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "arxiv": "arXiv:2503.02318"
    },
    {
      "citation_id": "144",
      "title": "Large language models and brain-inspired general intelligence",
      "authors": [
        "Bo Xu",
        "Mu-Ming Poo"
      ],
      "year": "2023",
      "venue": "National Science Review"
    },
    {
      "citation_id": "145",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "146",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Qiaochu Huang",
        "Zhiyong Wu",
        "Shi-Xiong Zhang",
        "Guangzhi Li",
        "Yi Luo",
        "Rongzhi Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "147",
      "title": "Multimodal Emotion Captioning Using Large Language Model with Prompt Engineering",
      "authors": [
        "Yaoxun Xu",
        "Yixuan Zhou",
        "Yunrui Cai",
        "Jingran Xie",
        "Runchuan Ye",
        "Zhiyong Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "148",
      "title": "Open large-scale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open large-scale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "149",
      "title": "Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis",
      "authors": [
        "Qize Yang",
        "Detao Bai",
        "Yi-Xing Peng",
        "Xihan Wei"
      ],
      "year": "2025",
      "venue": "Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis",
      "arxiv": "arXiv:2501.09502"
    },
    {
      "citation_id": "150",
      "title": "From Understanding to Omni-Modal Reasoning with Context",
      "authors": [
        "Qize Yang",
        "Shimin Yao",
        "Weixuan Chen",
        "Shenghao Fu",
        "Detao Bai",
        "Jiaxing Zhao",
        "Boyuan Sun",
        "Xihan Bowen Yin",
        "Jingren Wei",
        "Zhou"
      ],
      "year": "2025",
      "venue": "From Understanding to Omni-Modal Reasoning with Context",
      "arxiv": "arXiv:2506.21277"
    },
    {
      "citation_id": "151",
      "title": "Emollm: Multimodal emotional understanding meets large language models",
      "authors": [
        "Qu Yang",
        "Mang Ye",
        "Bo Du"
      ],
      "year": "2024",
      "venue": "Emollm: Multimodal emotional understanding meets large language models",
      "arxiv": "arXiv:2406.16442"
    },
    {
      "citation_id": "152",
      "title": "MM-InstructEval: Zero-shot evaluation of (Multimodal) Large Language Models on multimodal reasoning tasks",
      "authors": [
        "Xiaocui Yang",
        "Wenfang Wu",
        "Shi Feng",
        "Ming Wang",
        "Daling Wang",
        "Yang Li",
        "Qi Sun",
        "Yifei Zhang"
      ],
      "year": "2025",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "153",
      "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone",
      "authors": [
        "Yuan Yao",
        "Tianyu Yu",
        "Ao Zhang",
        "Chongyi Wang",
        "Junbo Cui",
        "Hongji Zhu",
        "Tianchi Cai"
      ],
      "year": "2024",
      "venue": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone",
      "arxiv": "arXiv:2408.01800"
    },
    {
      "citation_id": "154",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "155",
      "title": "Yi: Open foundation models by 01",
      "authors": [
        "Alex Young",
        "Bei Chen",
        "Chao Li",
        "Chengen Huang",
        "Ge Zhang",
        "Guanwei Zhang",
        "Guoyin Wang",
        "Heng Li",
        "Jiangcheng Zhu",
        "Jianqun Chen"
      ],
      "year": "2024",
      "venue": "Yi: Open foundation models by 01",
      "arxiv": "arXiv:2403.04652"
    },
    {
      "citation_id": "156",
      "title": "Rrhf: Rank responses to align language models with human feedback",
      "authors": [
        "Hongyi Yuan",
        "Zheng Yuan",
        "Chuanqi Tan",
        "Wei Wang",
        "Songfang Huang",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "157",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "158",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-Based Convolutional Neural Networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "159",
      "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
      "authors": [
        "Aohan Zeng",
        "Bin Xu",
        "Bowen Wang",
        "Chenhui Zhang",
        "Da Yin"
      ],
      "year": "2024",
      "venue": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
      "arxiv": "arXiv:2406.12793"
    },
    {
      "citation_id": "160",
      "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
      "authors": [
        "Fan Zhang",
        "Zebang Cheng",
        "Chong Deng",
        "Haoxuan Li",
        "Zheng Lian",
        "Qian Chen",
        "Huadai Liu",
        "Wen Wang",
        "Yi-Fan Zhang",
        "Renrui Zhang"
      ],
      "year": "2025",
      "venue": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
      "arxiv": "arXiv:2508.09210"
    },
    {
      "citation_id": "161",
      "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
      "authors": [
        "Fan Zhang",
        "Zebang Cheng",
        "Chong Deng",
        "Haoxuan Li",
        "Zheng Lian",
        "Qian Chen",
        "Huadai Liu",
        "Wen Wang",
        "Yi-Fan Zhang",
        "Renrui Zhang"
      ],
      "year": "2025",
      "venue": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models",
      "arxiv": "arXiv:2508.09210"
    },
    {
      "citation_id": "162",
      "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "163",
      "title": "Automated multi-level preference for mllms",
      "authors": [
        "Mengxi Zhang",
        "Wenhao Wu",
        "Yu Lu",
        "Yuxin Song",
        "Kang Rong",
        "Huanjin Yao",
        "Jianbo Zhao",
        "Fanglong Liu",
        "Haocheng Feng",
        "Jingdong Wang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "164",
      "title": "Long Context Transfer from Language to Vision",
      "authors": [
        "Peiyuan Zhang",
        "Kaichen Zhang",
        "Bo Li",
        "Guangtao Zeng",
        "Jingkang Yang"
      ],
      "year": "2024",
      "venue": "Long Context Transfer from Language to Vision",
      "arxiv": "arXiv:2406.16852"
    },
    {
      "citation_id": "165",
      "title": "Refashioning emotion recognition modeling: the advent of generalized large models",
      "authors": [
        "Zixing Zhang",
        "Liyizhe Peng",
        "Tao Pang",
        "Jing Han",
        "Huan Zhao",
        "BjÃ¶rn Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "166",
      "title": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
      "authors": [
        "Jiaxing Zhao",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning",
      "arxiv": "arXiv:2503.05379"
    },
    {
      "citation_id": "167",
      "title": "HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding",
      "authors": [
        "Jiaxing Zhao",
        "Qize Yang",
        "Yixing Peng",
        "Detao Bai",
        "Shimin Yao",
        "Boyuan Sun",
        "Xiang Chen",
        "Shenghao Fu",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding",
      "arxiv": "arXiv:2501.15111"
    },
    {
      "citation_id": "168",
      "title": "A panel discussion on AI for science: the opportunities, challenges and reflections",
      "authors": [
        "Weijie Zhao"
      ],
      "year": "2024",
      "venue": "National Science Review"
    },
    {
      "citation_id": "169",
      "title": "Multimodal affective states recognition based on multiscale cnns and biologically inspired decision fusion model",
      "authors": [
        "Yuxuan Zhao",
        "Xinyan Cao",
        "Jinlong Lin",
        "Dunshan Yu",
        "Xixin Cao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "170",
      "title": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "authors": [
        "Zengqun Zhao",
        "Yu Cao"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "171",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "172",
      "title": "Information fusion in attention networks using adaptive and multi-level factorized bilinear pooling for audio-visual emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Jun Du",
        "Yuanyuan Zhang",
        "Qing Wang",
        "Qing-Feng Liu",
        "Chin-Hui Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "173",
      "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models",
      "authors": [
        "Xiongtao Zhou",
        "Jie He",
        "Yuhua Ke",
        "Guangyao Zhu",
        "Victor Gutierrez Basulto",
        "Jeff Pan"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "174",
      "title": "Multibooth: Towards generating all your concepts in an image from text",
      "authors": [
        "Chenyang Zhu",
        "Kai Li",
        "Yue Ma",
        "Chunming He",
        "Xiu Li"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}