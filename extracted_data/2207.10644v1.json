{
  "paper_id": "2207.10644v1",
  "title": "Ctl-Mtnet: A Novel Capsnet And Transfer Learning-Based Mixed Task Net For The Single-Corpus And Cross-Corpus Speech Emotion Recognition",
  "published": "2022-07-18T09:09:23Z",
  "authors": [
    "Xin-Cheng Wen",
    "Jia-Xin Ye",
    "Yan Luo",
    "Yong Xu",
    "Xuan-Ze Wang",
    "Chang-Li Wu",
    "Kun-Hong Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) has become a growing focus of research in human-computer interaction. An essential challenge in SER is to extract common attributes from different speakers or languages, especially when a specific source corpus has to be trained to recognize the unknown data coming from another speech corpus. To address this challenge, a Capsule Network (CapsNet) and Transfer Learning based Mixed Task Net (CTL-MTNet) are proposed to deal with both the singlecorpus and cross-corpus SER tasks simultaneously in this paper. For the single-corpus task, the combination of Convolution-Pooling and Attention Cap-sNet module (CPAC) is designed by embedding the self-attention mechanism to the CapsNet, guiding the module to focus on the important features that can be fed into different capsules. The extracted high-level features by CPAC provide sufficient discriminative ability. Furthermore, to handle the cross-corpus task, CTL-MTNet employs a Corpus Adaptation Adversarial Module (CAAM) by combining CPAC with Margin Disparity Discrepancy (MDD), which can learn the domain-invariant emotion representations through extracting the strong emotion commonness. Experiments including ablation studies and visualizations on both singleand cross-corpus tasks using four well-known SER datasets in different languages are conducted for performance evaluation and comparison. The results indicate that in both tasks the CTL-MTNet showed better performance in all cases compared to a number of state-of-the-art methods. The source code and the supplementary materials are available at: https://github.com/MLDMXM2017/CTL-MTNet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As an important communication way among people, speech conveys emotion, pitch and rhythm to express their thoughts and intentions to others. The understanding of emotional information in a speech is important in predicting the speaker's tendencies and reactions. The prediction of human emotions from speech signals is known as the Speech Emotion Recognition (SER) task, which has been attracting more and more researchers' attention in the human-computer interaction research field.\n\nIn the SER task, different speech signals collected from various speakers tend to exhibit diverse characteristics, but there are implicit emotional attributes in the same emotion category. Therefore, the essential challenge in SER is to discover such common representations so as to build robust models to predict human speech emotions. Most of such models were developed based on Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks [Akc ¸ay and Og ˘uz, 2020] with manually designed features to extract local spatial and temporal information from speech.  Verbitskiy et al.[2021]  designed a CNN architecture by carefully adjusting its hyperparameters to reduce the size of the SER model significantly. In addition, the Capsule Neural Network (CapsNet)  [Sabour et al., 2017]  was also first used by  Wu et al.[2019]  in the SER domain to process the spatial relationships of speech features in spectrograms due to its effectiveness in extracting spatial information.  Liu et al.[2020]  further embedded CapsNet into a local-global aware deep representation learning system to perform emotion classification. All these methods could lead to high performance on the SER task.\n\nHowever, these SER methods were proposed without considering the scenarios where the training data (source corpus) and test data (target corpus) come from different speech corpora. In this case, the huge gap in the model's performance between the source and target corpora was ignored. This gap is mainly caused by the speech signals collected from different language environments, such as various vocal environments or different ways of expressing emotions. Consequently, different from the single-corpus task, which only needs to split the training and test data from the same corpus, the cross-corpus SER task tends to distinguish the training × and test speech signals coming from different corpora. The weak potential correlation of data distributions between the source and target corpora makes the cross-corpus task more challenging than the single-corpus task.\n\nIn the cross-corpus task, researchers tried to unlabel the agglomerate data and align features from diversified spaces  [Zhang et al., 2021] . For example, Domain Adversarial Neural Network (DANN) was proposed to efficiently align mismatched feature distributions  [Abdelwahab and Busso, 2018] . A non-negative matrix factorization based transfer subspace learning method  [Luo and Han, 2020]  was designed to search for an optimally shared feature subspace for the source and target corpora, aiming to better eliminate the discrepancy between two distributions. Furthermore, the Variational Auto-encoding Wasserstein Generative Adversarial Network (VAW-GAN) implemented an emotional style transfer framework to bridge the signals across different data sets  [Zhou et al., 2021] .\n\nNevertheless, there are still some limitations to the proposed methods for the SER task, including:\n\n1. There are no known algorithms that suit both the singlecorpus and cross-corpus SER tasks simultaneously.\n\n2. Most methods utilized complex hand-crafted Low-Level Descriptors as inputs  [Schuller et al., 2010] , in which the extraction of salient features was highly dependent on the training data.\n\n3. The mainstream transfer learning methods in SER can only learn 0-1 loss, which is not suitable to handle the multiclass classification problems related to the cross-corpus task directly.\n\nTo address these challenges, this paper proposes a Cap-sNet incorporated with the Transfer Learning-based Mixed Task Net (CTL-MTNet) to tackle the single-and cross-corpus SER tasks simultaneously. It is noted that in the traditional CapsNet, each capsule individually represents some properties of the entity, which require proper strategies to dynamically evaluate their importance. Therefore, for the singlecorpus task, the module is designed through the combination of Convolution-Pooling and Attention-based CapsNet (CPAC for short) to enhance the effectiveness of important capsules. CPAC calculates the emotion information scores to adjust the attention weights for each capsule entity accordingly. In this way, larger weights are assigned to the more informative capsule entities to promote the discriminative ability.\n\nBesides, the Corpus Adaptation Adversarial Module (CAAM) is adopted in the cross-corpus task with the most commonly used Mel-Frequency Cepstral Coefficients (MFCCs) features. CAAM employs the Margin Disparity Discrepancy (MDD) loss  [Zhang et al., 2019]  and the crossentropy loss to balance the prediction results on sentiment feature alignment and emotion discrimination. The combination of CPAC and CAAM guarantees the high performance of our framework in both single-and cross-corpus SER tasks.\n\nThe main contributions are summarized as follows:\n\nThe first framework for handling two SER tasks simultaneously. To the best of our knowledge, CTL-MTNet is the first model that can handle the single-corpus and cross-corpus tasks simultaneously. It can not only extract the common fea-tures of the source corpus and the target corpus for the crosscorpus task, but also extract the individual emotional characteristics in the speech for the single-corpus task.\n\nA novel high-level feature extraction component. The proposed CPAC mechanism inherits CapsNet's ability to capture local relevance and global contextual information, and generates high-level features. In addition, it can also identify important features contained in various capsules through the attention mechanism.\n\nThe design of CAAM module for the cross-corpus task.\n\nThe CAAM module is embedded in our framework with the aid of MDD to learn the domain-invariant emotion representations with high robustness to bridge the gap between different corpora.\n\n2 Methodology",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Framework Of Ctl-Mtnet",
      "text": "As is shown in Fig.  1 , our model employs CPAC for the singlecorpus task, and CAAM for the cross-corpus task. MFCC features are extracted from audio data to serve as input features. The CNN-Pooling module generates relevant emotion features by capturing local correlations from MFCC features. The Attention CapsNet module consists of a PrimaryCaps layer, a self-attention layer and a DigitCaps layer. The Pri-maryCaps layer adjusts each sample from a scalar neuron to a vector neuron layer. The vector length is used to represent the probability of the capsule entity. Furthermore, the self-attention layer calculates the correlation scores among capsules, aiming to guide the module to focus on the connections across different capsules at the Dig-itCaps layer. The attentional representations learned by the DigitCaps are iterated through the capsule routing algorithm to produce robust and discriminative high-level speech features, which are sequentially fed to the global average pooling layer. The final prediction is generated based on the softmax function in the single-corpus task.\n\nBesides, the proposed algorithm employs the CAAM module to effectively deal with the cross-corpus task by extracting common speech features. For the MFCC features of the source corpus and the target corpus, CPAC extracts highlevel features with discriminative emotion-preserving representations, and MDD is used to further learn the domaininvariant emotion representation via feature alignment. After successive iterations, CAAM gradually generates representations with more discriminative emotion-preserving and a minor discrepancy between corpora.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cnn-Pooling Feature Learning",
      "text": "As is shown in Fig.  1 , three one-dimensional convolution blocks are connected to extract high-level features from MFCC further. Each CNN-pooling block consists of a 2D convolution operation followed by batch normalization, elu activation function, and an average pooling layer with a fixed dropout at a rate of 0.25. The kernel size of all convolutional layers is [3 3], and the number of filters is 64. The CNNpooling blocks act as a mapping of the MFCC features to a high-dimensional space for training capsules.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attention Capsnet",
      "text": "(3) Then, the output is fed to the DigitCaps and Avepooling layers to further extract high-level speech features. The selfattention score is gradually improved through iterative refinement to establish the connections among different capsules. This approach provides more robust and discriminative representations for the subsequent operations in both single-corpus and cross-corpus tasks, thus improving the overall performance of our algorithm.\n\nWhere j represents the specified capsule and sj represents the output of the convolution layer within the PrimaryCaps. The squash function ensures that the length of the short vector is reduced to almost zero, while the length of the long vector is scaled to close to but no more than one. The output of the PrimaryCaps layer is also adjusted to fit the vector neuron layer and is mapped to three identical linear mappings A Q , A K , A V . These mappings represent the queries, keys, and values of the self-attention mechanism, which all have the same dimension d. The output attention is then calculated by:",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Margin Loss",
      "text": "In the single-corpus task, the output layer uses a vector other than scalar representation. The length of each vector represents the probability that the capsule entity exists. Since there are multiple emotions classes, the overall margin loss is the sum of all samples' loss, calculated by:\n\nwhere k represents the k-th class, Vk is the output of the Digit-Caps and Avepooling layers, and Tk represents the predicted results by the softmax function. That is, Tk equals 1 if the (2) With the aid of the attention mechanism, our CapsNet can further extract spatial information from the inputs, and establish sample is assigned to the k-th emotion class, and 0 otherwise. m + , m and ρ are the hyperparameters, and they are set to 0.9, 0.1 and 0.5 respectively in this study.\n\n(4)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Corpus Adaptation Adversarial Module",
      "text": "Different corpora contain unique characteristics, such as distinct environments or different accents. By discovering the commonness in human emotion, the difference in these unique characteristics can be bridged. The MDD-based CAAM is proposed to identify the cross-corpus emotion by mining the domain-invariant emotion features, so as to discover the feature subspace where the source corpus P and the target corpus Q can be aligned. In CAAM, the model is trained on a labeled sample set containing n samples drawn from the source corpus distribution, P ˆ = {(xi, yi)} n . An tropy loss function L(P ˆ; f ) and the MDD loss function Dγ P ˆ, Q ˆ; f ′ . And η = 1, γ = 1.5 in this study.\n\nAs is depicted in Fig.  1 , CAAM contains our proposed CPAC and two output modules. The CPAC acts as the feature extractor ψ to encode the MFCC features x s , x t from the source and target corpus, and then the high-level features ψ(x s ), ψ(x t ) are fed to the two output modules. The upper output module f contains one fully-connected (FC) layer of five dimensional neurons to classify speech emotions with the cross-entropy loss function L(P ˆ; f ), which calculates the loss only in the source corpus by:\n\ntional Speech and Song (RAVDESS). The four datasets include 1200, 535, 480, and 1440 data. Their details are given in Tables  S1  and S2  in the Supplementary Materials.\n\nFeature Extraction. In this experiment, 39-dimensional MFCCs are extracted from the Librosa toolbox  [McFee et al., 2015]  to serve as the inputs with a frame shift of 0.0125 s and a frame length of 0.05 s.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Implementation And Training.",
      "text": "The proposed algorithm is implemented with TensorFlow and optimized by using Adam algorithm  [Kingma and Ba, 2014] . Especially, the gradient reversal layer (GRL)  [Ganin and Lempitsky, 2015]  is employed to train ψ to minimize the MDD loss function. Be-\n\n(5) sides, all experimental results in the single-corpus task are based on the 10-fold cross-validation. Details about the train-\n\nHere n, f (ψ(x s )) and y s are the number of training examples, outputs of f with the softmax operation and target labels of training samples respectively. The lower output module f ′ also contains the same FC layer to reduce the discrepancy in different corpora, and the MDD loss function Dγ P ˆ, Q ˆ; f ′ is employed to calculate the loss between the labeled dataset from source corpus and the unlabeled dataset from target corpus, as defined by:\n\nEvaluation Metrics. The weighted average recall (WAR) and the unweighted average recall (UAR) are adopted for performance comparisons. The former refers the mean of recall for different emotional classes and the latter is the weighted mean of recall with weights equal to class probability. Their definitions are given in Equations S1-S2 in the Supplementary Materials.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison To State-Of-The-Art",
      "text": "The Single-Corpus Task. As there are various state-of-the-\n\nart methods proposed for different datasets, we select three representative SER methods reporting the top three results on\n\neach dataset for the performance comparisons. Consequently, there are different algorithms employed as baselines for different datasets.\n\nγ\n\n)\n\nAs is shown in Table  1 , compared to other approaches, CTL-MTNet improves WAR by +4.85%, +3.65%, +0.83%, n i i=1 i (6) and +3.40% on CASIA, EMODB, SAVEE, and RAVDESS datasets, respectively. The results confirm that CPAC can eswhere γ is a positive hyperparameter larger than 1, named margin factor. disp x t Q ˆ (f, f ′ ) and disp x s P ˆ (f, f ′ ) are two margin disparities of the source and target corpus respectively. f ′ is trained to maximize the distribution discrepancy between two corpora, and f, ψ is trained to minimize the maximum MDD. The objective of adversarial learning for extracting corpus-confused features is formulated as follows:\n\ntablish the sentiment link among different capsules and provide a more robust representation for the SER task.\n\nThe Cross-Corpus Task. Our method is compared with some representative methods: CDAN  [Long et al., 2017] , DANN  [Abdelwahab and Busso, 2018] , and NMFTSL  [Luo and Han, 2020] . The results in Table  2  show that our method outperforms all these methods with obvious advantages in all cases. The average UAR and WAR obtained from our method reach 39.90% and 41.57%, respectively. As is shown in   S4  in the Supplementary Materials and +12.28% compared to DANN, and +9.78% and +11.63% compared to NMFTSL. The excellent performance on the cross-corpus datasets verifies that our method can better fit the environments of various accents and cross-languages.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "The Ablation Study",
      "text": "To verify the importance of each component of the proposed method, we conduct experiments in the following four aspects:\n\n(1) Algorithm 1 uses the convolutional network provided by the original CapsNet for feature learning;\n\n(2) Algorithm 2 removes the Self-Attention mechanism from the CPAC architecture;\n\n(3) Algorithm 3 replaces the capsule layer with LSTM to extract high-level features;\n\n(4) Algorithm 4 removes the domain adaptation method for the target corpus and trains only with the source corpus.\n\nThe first three algorithms are adapted for the single-corpus task, and the last one is used for the cross-corpus task. As is shown in Table  3 , on average, our proposed CPAC obtains a relative improvement of +12.58% on WAR compared with the results of Algorithm 1 using only the Conv-Pool component and +7.03% compared with Algorithm 2. These results validate the effectiveness of the CPAC module. Compared to Algorithm 3, applying the CapsNet-based module can beat the LTSM-based module by achieving +12.78% higher score. Similar results can be observed on all datasets, indicating the contribution of different components.\n\nTo verify the generalization ability of the CPAC module, we visualize the higher-order features in the test set using Al- As is shown in Table  4 , compared with Algorithm 4, our algorithm achieves higher performance in all cases with the aid of MDD, gaining +11.96% and +14.99% improvements for the average UAR and WAR indices, and promotes the results to +14.85% and +25.97% higher in UAR and WAR scores in the best cases.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Visualization Of Speech Representations",
      "text": "The t-SNE technique is employed to visualize the representations learned by CPAC from different aspects, including distinctive emotional features and domain-invariant emotion features.\n\nThe Single-Corpus Task. To verify the discriminative ability of our algorithm, Fig.  3",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "Speech Emotion Recognition with source and target data from different corpora containing different speakers and languages still remains a big challenge. In this paper, a novel SER algorithm is proposed to tackle the single-corpus and cross-corpus tasks simultaneously. Through the construction of CNN-Pooling and Self-Attention CapsNet, the CPAC model can establish the sentiment connection among different capsules, generating more constant high-level features. Furthermore, the CAAM module by combining CPAC and two MDD algorithms is designed to create more discriminant features and common representation, thus reducing the gap between source and target corpus space. Extensive experiments on four SER datasets validate the stability and superiority of the proposed CTL-MTNet model, with significant improvements over the state-of-the-art algorithms in both tasks.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of CTL-MTNet. The upper and lower parts represent the cross-corpus task and the single-corpus task, respectively.",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE visualization of the high-level features obtained by",
      "page": 6
    },
    {
      "caption": "Figure 3: t-SNE visualization of the high-level features on the CA-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The performance comparisons on CASIA, EMODB, SAVEE and RAVDESS datasets.",
      "data": [
        {
          "CASIA": "",
          "Reference": "[Yildirim et al., 2021]",
          "Methods  WAR": "87.66",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "[Tuncer et al., 2021]",
          "Methods  WAR": "90.09",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "[Ilyas, 2021]",
          "Methods  WAR": "91.32",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "Our approach",
          "Methods  WAR": "94.97",
          "EMODB": ""
        },
        {
          "CASIA": "SAVEE",
          "Reference": "Reference",
          "Methods  WAR": "Methods  WAR",
          "EMODB": "RAVDESS"
        },
        {
          "CASIA": "",
          "Reference": "[Kwon, 2021]",
          "Methods  WAR": "85.00",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "[Tuncer et al., 2021]",
          "Methods  WAR": "87.43",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "[Ibrahim et al., 2021]",
          "Methods  WAR": "74.54",
          "EMODB": ""
        },
        {
          "CASIA": "",
          "Reference": "Our approach",
          "Methods  WAR": "90.83",
          "EMODB": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: The performance comparisons on CASIA, EMODB, SAVEE and RAVDESS datasets.",
      "data": [
        {
          "Source Corpus": "Target Corpus",
          "CASIA": "SAVEE",
          "EMODB": "SAVEE",
          "SAVEE": "EMODB",
          "Average": ""
        },
        {
          "Source Corpus": "[Long et al., 2017]",
          "CASIA": "31.67",
          "EMODB": "28.17",
          "SAVEE": "30.95",
          "Average": "31.66"
        },
        {
          "Source Corpus": "",
          "CASIA": "33.61",
          "EMODB": "26.67",
          "SAVEE": "30.15",
          "Average": "30.61"
        },
        {
          "Source Corpus": "[Abdelwahab and Busso, 2018]",
          "CASIA": "26.67",
          "EMODB": "25.67",
          "SAVEE": "28.58",
          "Average": "30.41"
        },
        {
          "Source Corpus": "",
          "CASIA": "25.00",
          "EMODB": "29.17",
          "SAVEE": "26.23",
          "Average": "29.29"
        },
        {
          "Source Corpus": "[Luo and Han, 2020]",
          "CASIA": "34.79",
          "EMODB": "24.17",
          "SAVEE": "29.14",
          "Average": "30.12"
        },
        {
          "Source Corpus": "",
          "CASIA": "34.67",
          "EMODB": "24.52",
          "SAVEE": "28.89",
          "Average": "29.95"
        },
        {
          "Source Corpus": "Our approach",
          "CASIA": "40.17",
          "EMODB": "34.33",
          "SAVEE": "36.14",
          "Average": "39.90"
        },
        {
          "Source Corpus": "",
          "CASIA": "49.44",
          "EMODB": "29.72",
          "SAVEE": "36.52",
          "Average": "41.57"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: The performance comparisons on CASIA, EMODB, SAVEE and RAVDESS datasets.",
      "data": [
        {
          "Method": "",
          "MDD": "",
          "Source Corpus": "Target Corpus",
          "CASIA": "SAVEE",
          "EMODB": "SAVEE",
          "SAVEE": "EMODB",
          "Average": ""
        },
        {
          "Method": "Algorithm 4",
          "MDD": "No",
          "Source Corpus": "UAR",
          "CASIA": "25.83",
          "EMODB": "25.50",
          "SAVEE": "25.26",
          "Average": "27.94"
        },
        {
          "Method": "",
          "MDD": "",
          "Source Corpus": "WAR",
          "CASIA": "23.61",
          "EMODB": "25.83",
          "SAVEE": "25.25",
          "Average": "26.58"
        },
        {
          "Method": "CAAM",
          "MDD": "Yes",
          "Source Corpus": "UAR",
          "CASIA": "40.17",
          "EMODB": "34.33",
          "SAVEE": "36.14",
          "Average": "39.90"
        },
        {
          "Method": "",
          "MDD": "",
          "Source Corpus": "WAR",
          "CASIA": "49.44",
          "EMODB": "29.72",
          "SAVEE": "36.52",
          "Average": "41.57"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Akc ¸ay and Og ˘uz, 2020] Mehmet Berkehan Akc ¸ay and Kaya Og ˘uz. Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Busso Abdelwahab",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Hajarolasvadi and Demirel, 2019] Noushin Hajarolasvadi and Hasan Demirel. 3d cnn-based speech emotion recognition using k-means clustering and spectrograms",
      "authors": [
        "Lempitsky Ganin",
        "Yaroslav Ganin",
        "Victor Lempitsky",
        "; Gao"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Research on psychological counseling and personality analysis algorithm based on speech emotion",
      "authors": [
        "Hong"
      ],
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Security"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition by late fusion for bidirectional reservoir computing with random projection",
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Pseudo-colored rate map representation for speech emotion recognition",
      "authors": [
        "; Ilyas",
        "; Ozer Ilyas",
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "6",
      "title": "Optimal feature selection based speech emotion recognition using two-stream deep convolutional neural network",
      "authors": [
        "Soonil Kwon",
        "Kwon"
      ],
      "year": "2021",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "Liu"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Nonnegative matrix factorization based transfer subspace learning for cross-corpus speech emotion recognition",
      "authors": [
        "Long"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "arxiv": "arXiv:1705.10667"
    },
    {
      "citation_id": "9",
      "title": "Mustaqeem and Kwon, 2021] Mustaqeem Mustaqeem and Soonil Kwon. Speech emotion recognition based on deep networks: A review",
      "authors": [
        "Mcfee"
      ],
      "year": "2015",
      "venue": "Proceedings of the Korea Information Processing Society Conference",
      "arxiv": "arXiv:1710.09829"
    },
    {
      "citation_id": "10",
      "title": "Tuncer et al., 2021] Turker Tuncer, Sengul Dogan, and U Rajendra Acharya. Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques. Knowledge-Based Systems",
      "authors": [
        "Schuller"
      ],
      "year": "2008",
      "venue": "Efficient residual audio neural networks for audio pattern recognition",
      "arxiv": "arXiv:2106.01621"
    },
    {
      "citation_id": "11",
      "title": "Yildirim et al., 2021] Serdar Yildirim, Yasin Kaya, and Fatih Kılıc ¸. A modified feature selection method based on metaheuristic algorithms for speech emotion recognition",
      "authors": [
        "Wu"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Zhang"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}