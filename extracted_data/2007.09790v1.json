{
  "paper_id": "2007.09790v1",
  "title": "Generative Adversarial Stacked Autoencoders For Facial Pose Normalization And Emotion Recognition",
  "published": "2020-07-19T21:47:16Z",
  "authors": [
    "Ariel Ruiz-Garcia",
    "Vasile Palade",
    "Mark Elshaw",
    "Mariette Awad"
  ],
  "keywords": [
    "Emotion Recognition",
    "Facial Pose Normalization",
    "Generative Adversarial Networks",
    "Illumination Invariance",
    "Generative Adversarial Stacked Autoencoders"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we propose a novel Generative Adversarial Stacked Autoencoder that learns to map facial expressions with up to ±60 degrees to an illumination invariant facial representation of 0 degrees. We accomplish this by using a novel convolutional layer that exploits both local and global spatial information, and a convolutional layer with a reduced number of parameters that exploits facial symmetry. Furthermore, we introduce a generative adversarial gradual greedy layer-wise learning algorithm designed to train Adversarial Autoencoders in an efficient and incremental manner. We demonstrate the efficiency of our method and report state-of-the-art performance on several facial emotion recognition corpora, including one collected in the wild.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expression recognition continues to be of great interest in the machine learning community due to the many challenges it presents. Many works in the literature have proposed a variety of models that produce state-of-the-art accuracy on various facial expression corpora collected in controlled environments. However, most of these models are unable to deal with non-frontal facial expression images or with drastic changes in the environment, such as different lightning conditions. This is evident on models trained on data with nonuniform conditions collected in the wild  [1] , for which state-of-the-art is significantly lower than on datasets taken in controlled environments. This is partly due to the fact that in non-frontal faces -i.e. faces with pose greater than 0 degrees -much of the information essential for emotion recognition is nonexistent. Moreover, the more variations in facial pose the larger the data distribution, and the more difficult for a neural network to provide good generalization due to the high dimensional search space. In addition, for real-time emotion recognition in unconstrained environments, it is difficult to obtain images without facial pose. In this work we introduce a model that explicitly addresses facial pose and illumination invariance. The proposed model can deal with faces with a facial pose of up to ±60 degrees and several degrees of illumination. Our main contributions can be summarized as:\n\n• a novel deep Generative Adversarial Stacked Convolutional Autoencoder model that learns to map faces with facial pose of up ±60 degrees to 0 degrees representations. • a hybrid deep learning layer employing convolutional filters to retain spatial information and learn salient features, and fully connected units shared across the depth dimension to facilitate the reduction of facial pose. • a convolutional layer with reduced number of parameter that exploits facial symmetry and learns from only one half of the face. • a gradual greedy layer-wise algorithm for Generative Adversarial Autoencoders. • an illumination and pose invariant emotion recognition classifier that produces state-of-the-art classification performance in images taken in both controlled and unconstrained environments.\n\nWe also show the difference of training for pose invariance only and for pose and illumination invariance. Our model is tested on several empirical facial expression datasets as well as on one collected in the wild.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Deep neural networks are known to be difficult to train. This was particularly true before the introduction of several regularization techniques  [2]  in recent years, and before Batch Normalization  [3] . Before such techniques existed, pre-training models was often a preferred choice for deep models over random initialization. Autoencoders  [4]  are suitable for such tasks given that they are trained in an unsupervised fashion, overcoming constrains imposed by lack of labelled data.\n\nAutoencoders learn an encoder function f that maps an input image x to a hidden representation h = f (x), and learn a function g that maps h to a reconstruction y = g(f (x)) where y is an approximation of x. However, in recent works,  [5]  it has been established that the target reconstruction does not need to be the same as the input x to the autoencoder. This is supported by the theory that to be useful, an autoencoder should only learn an approximation of the target reconstruction and not an identity function that replicates it  [6] .\n\nIn principle this establishes that the input and target vectors in an autoencoder do not need to be the same, and therefore we can learn a function that maps an input from a given distribution to a target that lies in a different distribution. Autoencoder can also be trained in a greedy layer-wise (GLW)  [7]  fashion which has proven to be more effective than joint training  [8] .\n\nGenerative Adversarial Autoencoders  [9]  are some of the latest autoencoder models that follow the trending popularity of Generative Adversarial Networks (GANs)  [10] . GANs are composed of two networks: a generative model G and a discriminator model D. Both models are trained by playing a min-max adversarial game where the discriminator model tries to determine if a given sample is from the generator or the dataset. In contrast, the generator maps samples z from a prior distribution p(z) and maps it to the data space. Generative Adversarial Autoencoders follow a similar approach where the generator is an autoencoder that maps an input x to a latent representation z that lies in an aggregate posterior distribution q(z) and back to a reconstruction y which is an approximation of x. The discriminator network in this framework attempts to determine if a sample has been drawn from a prior distribution p(z) or from the latent distribution q(z).\n\nAlthough GANs are mainly used for data synthesis, some works have explored their use in classification  [11] . For emotion recognition, works employing GANs mainly focus on using GANs for data augmentations, whether in emotion recognition from speech  [12]  or from facial expressions  [13] ,  [14] . Most other works focus on generating data, but do not explore emotion recognition. Such works include multipose face recognition  [15] ,  [16] , or facial expression image completion  [17] .\n\nSome works attempt to deal with some of the common challenges in emotion recognition, such as, illumination invariance. Contemporary attempts to address illumination invariance in the domain of facial expression recognition include the use of noise injection  [4] , blurring images with Gaussian filters  [18] , a combination of histograms, principal component analysis (PCA) and discrete cosine transforms  [19] , or complex models  [18] ,  [20]  and very deep CNN architectures  [21] .\n\nTo the best of the knowledge of the authors, although many works target pose invariant face recognition, no existing work focuses on pose invariant facial expression recognition.\n\nIn this work we normalize facial pose ϕ, where |ϕ| > 0 to frontal images 0 degrees pose. We also normalize images with relative luminance Y to a target luminance µ. Considering that autoencoders allow us to learn a mapping from an input image to a target image that does not necessarily lie in the same distribution, they are suitable for this task. This in effect means that we are interested in imposing a distribution on the input data to produce reconstructions that resemble the desired target. Adversarial autoencoders can facilitate this task as they uniformly impose a data distribution on the code vector, i.e. the hidden representation produced by the encoder element, to generate realistic reconstructions. Moreover, adversarial autoencoders are designed to produce very realistic reconstructions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Generative Adversarial Stacked Convolutional Autoencoders",
      "text": "Fig.  1 : Visualization of the first shallow autoencoder in the GASCA model.\n\nWe introduce the Generative Adversarial Stacked Convolutional Autoencoders (GASCA) framework. Just like in conventional adversarial autoencoders, in a GASCA model, the discriminator attempts to tell whether a sample comes from the training dataset or if it is a reconstruction produced by the autoencoder.\n\nLet x ϕ be a sample from the data distribution p d (x ϕ ) and x µ the sample from the data distribution p d (x µ ) used as the desired target reconstruction. The autoencoder G model learns to map x ϕ to a latent space z, note that this is not an aggregate posterior as in conventional adversarial autoencoders, and back to a reconstruction y that resembles x µ and lies in the distribution q(y). The discriminator D attempts to differentiate between y and x µ .\n\nIn the conventional adversarial autoencoder framework  [9]  a distribution p(z) -often a Gaussian distribution-is imposed on q(z) by estimating the divergence between q and p. This imposition can be used to produce reconstructions with specific features. However, in this work, the objective is to produce reconstructions that are as close as possible to the desired target image x µ . Consequently, instead of imposing random noise on the hidden representation vector, the GASCA model imposes p d (x µ ) on q(y) in the following way:\n\nWith this formulation, the discriminator model D is optimized to rate samples from p d (x µ ) with a higher probability, and samples from q(y) with a low probability. Formally this is defined as:\n\nwhere x µ is an input image and x ϕ is the target reconstruction image. Note that since (x ϕ = x µ ) is not necessarily true, the discriminator D is not guaranteed to see the input to G.\n\nThe objective of the autoencoder model G, which in term plays the role as the generator, is to convince the discriminator model D that a sample reconstruction y was drawn from the data distribution p d (x µ ) and not from q(y). This optimization is done according to:\n\nFurthermore, since GANs are known to be difficult to train due to their sensitivity to hyper-parameters and parameter initialization which often leads to mode collapse, the GASCA model is trained in a GLW fashion. However, since the greedy nature of GLW leads to error accumulation as individual layers are trained and stacked  [5] , we build on the gradual greedy layer-wise training algorithm from  [5]  and adapt it for adversarial autoencoders. Accordingly, we introduce the GAN gradual greedy layer-wise (GANGGLW) training framework and formally define it in Algorithm 1.\n\nAlgorithm 1 Given a training set X and validation set X each containing input images x ϕ and target images x µ , m shallow autoencoders, an unsupervised feature learning algorithm L -see Algorithm 2 -which returns a trained shallow autoencoder and a discriminator model, and a finetuning algorithm T -see Algorithm 3: train D 1 and G 1 jointly with raw data and add them to their corresponding stacks G and D. For the remaining autoencoders and generator models: encode X and X using the encoder layers ξ from the stack G. Create a new discriminator D k and train together with the new autoencoder G k and add them to their corresponding stacks. Fine-tune G on raw pixel data. Forward propagate x ϕ ⊂ X through G and use the resulting features, along with x µ , to fine-tune D for binary classification.\n\nBy fine-tuning G and D in Algorithm 1 we avoid error accumulation from one layer to the next and reduce the required number of fine-tuning steps for deeper layers. However, in Algorithm 2 Given a training dataset X with m mini-batches of size b, an autoencoder model G and discriminator model D both with weight matrices W g and W d , an absolute value cost function loss: train G and D jointly such that:\n\nfor n = 1, . . . , m do 5:\n\np y ← predict(y g , D)\n\n13:\n\nL q(y) ← loss(0, p y ) 14:\n\nAdam(L, G) for n = 1, . . . , m do 3:\n\nend for 8: end for 9: return f the special case where the input and target images are significantly different, GANGGLW is not very compatible with Convolutional Neural Networks (CNNs). CNNs are designed to retain spatial information through filter kernels, whereas in GANGGLW the input and target reconstruction images are not always the same and as such spatial information often needs to be shifted or transformed, or partially ignored. For this reason, and since in our experimental set up we are trying to normalize facial pose, we introduce ConvMLP layers in the next section. IV. CONVMLP AND HALFCONV LAYERS One of the main advantages offered by CNNs over multilayer perceptron networks (MLPs) is their ability to self-learn a translation invariant downsampled feature vector that highlights salient features and retains spatial information through filter kernels. However, CNNs are constrained to preserve the spatial structure of images and therefore are not suitable to reduce or increase facial pose: since every output value produced by convolutional layers is the results of the dot product between a filter kernel and a small view of the input image, the pixel values can only be shifted within the space covered by the filter kernel. Normally, filter kernels tend to be small in order to capture small salient features.\n\nTo overcome the limitations imposed by convolutional kernels and fully connected layers, and at the same time exploit the advantages offered by both, we introduce a hybrid layer that combines both approaches. The most straightforward to accomplish this is by simply placing an MLP after the convolutional layer. And, by having a smaller number of hidden units in the MLP than the number of features produced by the convolutional kernels, there would be no need for down-sampling layers such as average or max pooling or convolutional layers with a stride greater than one, which often result in the loss of important information. However, because convolutional layers normally employ a high number of convolutional kernels, this approach would require a significantly large weight matrix W . Accordingly, W would need to have a connection weight for each feature in the feature maps produced by convolutional kernels, resulting in a large number of learnable parameters, increased computational cost, and increased training difficulty.\n\nIn contrast, the novel layer presented here, referred to as ConvMLP hereafter, shapes the resulting feature map produced by a convolution operation with a fully connected layer that is shared between all the resulting feature maps. Refer to Figure  2  for a pictorial description. Given an input image I and a filter kernel K with m × n dimensions, and a second weight matrix W , the output of ConvMLP layers is defined as:\n\nwhere:\n\nJust as in empirical convolutional layers, the non-linearity is provided by a ReLU activation function, extending the above equation to:\n\nIn this formulation of ConvMLP layers, during the forward pass, the weight matrix W is used to shape every feature map produced by the convolution operation and is updated only once using backpropagation. Sharing this layer across the third dimension-not taking into account the batch dimension for simplicity-its weight matrix is many orders of magnitude smaller than without weight sharing. This also ensures that the shif ting layer learns to shift all the features highlighted in every feature plane in the same manner. Notice in Figure  2  how the pixels on the second feature map are at a different location.\n\nIn addition to ConvMLP layers, and in order to support the pose invariant training approach and models presented in this work, a second convolutional layer is introduced here. This novel layer, referred to as HalfConv hereafter, exploits facial symmetry present in face images with an estimated pose of zero degrees. HalfConv layers slice the input vector vertically in half. The half containing all the facial features belonging to the left side of a face is then used as input for a convolutional layer that has half the number of parameters than an empirical convolutional layer. The resulting feature map is then simply mirrored across the y axis.\n\nWhen applied to face or facial expression images, HalfConv layers give up some important information on the right edge of the input image, which in effect corresponds to the features in the middle of a face. This is due to the nature of the convolution operation, which convolves a kernel across an input image, resulting in a feature plane with smaller dimensions than the input image. For this reason, HalfConv layers enforce zero padding p on right side edge of the input image to allow the filter kernel to capture the features closer to the edge. Their output is then defined by:\n\nwhere p = j 2 +1. Then every resulting feature plane is reflected over the y axis, resulting in a full image. Note that padding p is enforced to avoid losing features at the edges of the image.\n\nThe main advantage offered by HalfConv layers is the reduced number of learnable parameters, which in effect results in easier and faster training. Because the only extra operation required by this layer is simply mirroring a feature vector vertically, HalfConv layers are significantly less computationally expensive than empirical convolutional layers. Furthermore, because this layer only deals with frontal faces, there is no need to employ any shifting neurons. Note that these layers are only suitable for cases where symmetry is existent in the input image or is desired in the resulting feature plane. Therefore, in the GASCA model, these layers are only used when α = 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Unsupervised Feature Learning",
      "text": "We train two Generative Adversarial Stacked Convolutional Autoencoders, one to normalize facial pose (GASCA 1 ) and another to normalize facial pose and illumination (GASCA 2 ). We train both models using the GANGGLW training method introduced earlier. We train each shallow autoencoder to gradually reduce facial pose, or keep it the same if it is already smaller than the desired target. This process is repeated until reaching a facial pose of 0 degrees. Effectively, the search space for the upper layers is greater than that of the deepest layer, which only has to learn one facial pose of 0 degrees.\n\nFor the GASCA 2 model we incorporate illumination invariance normalization training by taking images with disproportionate degrees of illumination and reconstructing them as images with good illumination: good illumination is determined by their relative luminance Y as done by  [5] .\n\nWe employ the MultiPie dataset  [22]  to train these models given that it contains facial images with multi-pose and multiillumination.\n\nThe training dataset is built according to:\n\nwhere -60 <= ϕ <= 60, α denotes the desired target pose, α ∈ {0, ±15, ±30, ±45}, and d denotes the change in pose by degrees: 15 degrees in this work. Each subset is further split into 70% training and 30% validation subsets.\n\nFor the GASCA 2 model, since every image in the MultiPie dataset has 19 copies with different levels of relative luminance, we measure their cumulative relative luminance and pick the image closest to the mean as the target reconstruction for all others.\n\nSince α ∈ {0, ±15, ±30, ±45}, we design the GASCA models with three ConvMLP layers and one HalfConv Layer for the encoder element. ConvMLP layers use 5×5, 3×3, 3×3 filter kernels and 100 hidden units for the shif ting units. The decoder element only uses deconvolutional layers to force the encoder to learn a downsampled pose invariant hidden representation, since we fine-tune it later on to do classification. Accordingly, every shallow autoencoder is trained on a single target pose. For instance, subset A 1 contains all the images with {0, ±15} degrees. A 2 contains all images at angles {0, ±15, ±30}, thus A 1 ∩ A 2 , Every shallow autoencoder in the GASCA models is trained for 100 and fine-tuned for 20 epochs. G is optimized using ADAM  [23] , whereas D employs SGD with Nesterov momentum. The initial learning rates for each individual shallow autoencoder in G were set to λ ∈ {0.1, 0.3, 0.5, 0.7, 0.75}. Since D learns faster than G, the shallow autoencoders employ smaller learning rates: λ ∈ {0.01, 0.03, 0.5, 0.07}. During fine-tuning, the stacks G and D use a learning rate of 0.001. This combination of hyper-parameters provided the best results for both models.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Pose Invariant Reconstruction Results",
      "text": "The novel pose invariant Generative Adversarial Stacked Convolutional Autoencoder models proposed in this work are trained to gradually reduce facial pose using GANGGLW training. As it can be observed in Figure  3 , the pose and illumination invariant GASCA 2 model manages to reduce facial pose in facial images with an estimated pose of up to ±60 degrees. It also produces reconstructions with similar illumination.\n\nIt can also be observed that on the images with pose of ±60 degrees half of the face is not visible, yet the pose invariant model manages to fill in the missing information, and more importantly keeps the shape of facial shapes which are important for emotion recognition: eyes, eyebrows, mouth, nose, cheeks, among others. Nonetheless, the greater the pose in x ϕ the poorer the quality of the reconstruction y. This is justified by (i) the fact that the model has to compensate for missing information, (ii) the fact that only one layer is trained specifically to deal with that particular facial pose, (iii) the smaller the pose the more the images get seen by every layer in G during training, and (iv) increased network depth.\n\nIf the shallow autoencoder at step k = 1 fails to learn a pose invariant feature vector, the shallow autoencoder at step k = 2 will struggle even more to learn a pose invariant feature vector, and so forth. GANGGLW greatly helps to address this issue by allowing inter-layer fine-tuning, which helps strengthen the weight connections between D k and D k+1 .\n\nOne of the main remarks observed in the reconstructions is that although these retain all the important salient features, they are visually different than the input images. These reconstructions could be improved by unsupervised fine-tuning of G for a significantly longer number of epochs. Likewise, secondary methods such as super resolution CNNs  [24]  could be used to improve the visual quality of the reconstructed images. However, because the objective of this research is to only learn a pose invariant feature vector z that can be used for emotion recognition, the quality or resolution of the reconstructions is trivial.\n\nOne of the main advantages offered by ConvMLP layers is that the number of shif ting neurons can be adjusted as needed. In the GASCA models, every ConvMLP layer only employs 100, which are enough to reposition facial features and eventually reduce facial pose. Another advantage offered by ConvMLP layers is that they can be used for dimensionality reduction by mapping a feature plane to a smaller feature plane. Although, this is not evaluated in this research.\n\nAs illustrated in Figure  3 , the reconstructed images also do not have a horizontal line diving the face in two, as it would be expected due to the use of HalfConv layers. When visualizing the feature planes produced by these layers, the line is somewhat visible. However, because in the final stack G this layer is followed by all the layers in the decoder stack of G, and since the line is not visible in the target reconstruction images, it vanishes during fine-tuning.\n\nWe did not notice significant differences between the reconstructions of both GASCA 1 and GASCA 2 models. However, the reconstruction loss for the latter was marginally smaller and as seen in the next section it generalizes better.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Pose And Illumination Invariant Emotion Recognition",
      "text": "Once a GASCA model is trained and fine-tuned for reconstruction, it can be used as a generic feature extractor for facial expression images. However, it can only provide feature vectors that are pose and illumination invariant but that do not necessarily discriminate between different emotions. Therefore, we fine-tune the encoder element of G for classification. We discard both the discriminator model D along with the decoder element g D of the generator D and attach a classification layer to the encoder.\n\nThe pose invariant GASCA 1 model is used to initialize a classifier model, CNN 1 , which is fine-tuned and tested on the KDEF corpus  [25] . This dataset contains frontal and images at ±45 degrees. No other publicly available datasets with multiple poses have facial expression labels.\n\nThe pose and illumination invariant GASCA 2 model is used to initialize a second classifier, CNN 2a . This model is also fine-tuned and tested on the KDEF corpus.\n\nIn addition, in an attempt to test the robustness of our proposed methodology, we use GASCA 2 to initialize a third model CNN 2b . However, due to the lack of publicly available data taken in realistic environments with multi-pose and varying illumination, as well as labels for the emotions being expressed, we build a large dataset composed of the CK+  [26] , JAFFE  [27] , KDEF  [25] , and FEEDTUM  [28]  corpora. We refer to this corpus as combined facial expressions (CFE). Note that because, as later discussed, we obtain over 99.6% on this corpus, this model is evaluated on completely novel data: the entire NAOFaces corpus  [29] . This set has total of 196 images collected in unconstrained environments. Participants were 28 21 males and 7 females between ages 18 and 55 from at least five different ethnic backgrounds.\n\nAs opposed to empirical CNN classifier models which employ a fully connected layer after the last convolutional layer, the classifiers in this work map the resulting feature planes produced by the last convolutional layer, which is a HalfConv layer, directly to an output SoftMax layer for classification, as done in  [30] .\n\nThe CNN 1 and CNN 2a models are fine-tuned for 10 epochs and, because the CFE corpus has more images, CNN 2b is only fine-tuned for two epochs. Since the stacked autoencoders are optimized using ADAM, all classifiers are fine-tuned also using ADAM and a learning rate of 0.01. Using a different optimizer like SGD for fine-tuning would lead to the gradients changing drastically and require a longer fine-tuning process.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Viii. Pose Invariant Emotion Recognition Results",
      "text": "As it can be observed in Table  I , the pose invariant model, CNN 1 , obtains a classification performance of 96.81%. In contrast, the pose invariant model that also incorporated illumination invariance obtains a state-of-the-art classification rate of 98.07%. The main differences in performance are observed for classes: surprise, Fear, and Angry, whereas both CNN 1 and CNN 2a obtained the same classification accuracy for the remaining classes. Because both models are trained using a relatively similar approach, it is hypothesized that these discrepancies in classification performance are due to these three classes containing more images with varying image luminance, thus the pose and illumination invariant model is able to generalize better.\n\nThe CNN 2b model is evaluated on the NAOFaces corpus and achieves 81.36% accuracy. This is significantly lower than the performance of the other models on the KDEF corpus. We attribute this lower performance to the fact that the NAOFaces contains images that are substantially more difficult, i.e. people with glasses, at different poses, and different ethnicity. Moreover, this model was not fine-tuned on any images from this corpus. This theory is further supported by the fact that when we split the CFE corpus 80% training and 20% testing, we obtain 99% on the test set.\n\nOne important observation in Table  II  is that, when looking at the missclassified images for a given class, on average 40% of them are frontal images, i.e. images with zero degrees pose, and the remaining 60% are those with a pose. However, because the ratio of images with a facial pose is 2:1 compared to those without one. This means that on average, more images without facial pose are missclassified. These results and observations are of great importance given that they support the pose invariant pretraining approach presented in this work. Another observation is that not a single image from the other classes was confused with Neutral. This particular score is significant taking into account that all emotions derive from a neutral state, often resulting in low precision scores.\n\nDespite the good performance offered by the CNN 2a on the NAOFaces corpus, the classification performance offered by this model is not ideal. This is attributed to one major factor: cultural differences. Because the model was trained solely on images from Caucasian people, the model has never learned to adjust to cultural difference. The NAOFaces corpus contains images of people from at least five different backgrounds including: Asian, Arab, Black, Irish and Hispanic, among others unrevealed ones. In effect, because people from different ethnic backgrounds express emotions differently  [31] , the classifier should be trained with images of participants from a wide range of ethnic backgrounds and cultures.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ix. Comparison Against State-Of-The-Art",
      "text": "We now compare our methodology to contemporary stateof-the-art methods on the KDEF corpus. Due to the lack of contemporary work designed explicitly for pose invariant emotion recognition, the methods proposed in this work are compared against one of the most common and state-of-the-art classifiers: a ResNet  [30] . Accordingly, a ResNet-34, i.e. with 34 parametrised layers, is trained using SGD, a momentum of 0.9 and learning rate of 0.1. This model is trained for 100 on the training subset of the KDEF corpus and achieves an accuracy rate of 87.472% on the test subset, as illustrated in Table  III . Note that even though the authors of  [8]  report 92.52% on the KDEF corpus, those results are only reported on frontal faces without facial pose. On the contrary, all the models in this section are evaluated on images with multiple poses, hence the marginally lower performance of the ResNet model.\n\nAs seen in Table  III , the pose and illumination invariance model, CNN 2a outperforms the state-of-the-art classifier ResNet-34 model by over 10%. Similarly, it outperforms CNN 1 marginally, supporting the pose and illumination invariant training approach. The pose invariant GASCA models also have an exponentially smaller number of parameters compared to the ResNet-34 model.\n\nThe novelty of this work also arises from combining greedy layer-wise training with adversarial learning. Generative Adversarial Autoencoders are trained jointly as opposed to layer-wise. They impose a random distribution p(z) on the distribution q(z) produced by the encoder element of G, and use the resulting aggregate posterior distribution is mapped to reconstruction y. The discriminator D tries to guess if the sample was drawn from q(z) or p(z). The GASCA models do not use a random distribution and instead use the reconstruction y produced by forward propagating x ϕ through G, along with the target image x µ as input for the discriminator. The generator G is optimized to reduce the distance between y and x µ . By fine-tuning the stacks G and D at every step k, both models become better at their respective job. By improving the ability of D to differentiate between y and x µ , G is forced to produce remarkable reconstructions and learn an encoder function that produces downsampled pose invariant feature vectors.\n\nIn terms of work on pose reductions, a similar model was proposed by  [32] . However, the authors focused on face detection and their model does not make use of Convolutional Autoencoders and instead uses MLPs, which are prone to overfitting when applied to this problem. Furthermore, because their model does not take into account spatial information, it is unable to retain salient features that are essential for emotion recognition. Whereas the GASCA models are able to retain facial features, or compensate for missing information when this is not present in the image. Additionally, the GASCA 2a model also takes into account illumination and produces an illumination and pose invariant feature vector.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "X. Conclusions And Future Directions",
      "text": "This work has introduced a novel pose and illumination invariant facial expression recognition model. A CNN classifier is pretrained as a Generative Adversarial Stacked Convolutional Autoencoder in a gradual greedy layer-wise semisupervised fashion. The GASCA model learns to map an input image containing a face, with an estimate pose ϕ, to a hidden representation z with an estimated pose of 0 degrees. Once the GASCA model is trained, the encoder element is used to initialize a CNN model which is fine-tuned for classification.\n\nThe outstanding performance of the GASCA models is derived from four concepts: (i) our GANGGLW training method (ii) the ConvMLP layers with shif ting neurons, (iii) the HalfConv layers which take exploit of facial symmetry, and (iv) multi-pose facial expressions data. Our pose and illumination invariant method produces state-of-the-art classification performance on multi-pose facial expression corpora. Moreover, the GASCA model produces reconstruction with very small errors and is able to generalize on unseen data.\n\nThe success of the pose invariant models is in part due to ConvMLP layers, which learn salient features and shift them as needed to reduce facial pose. HalfConv layers also play an important role as they reduce the number of learning parameters. HalfConv layers were inspired by the model presented by  [33] , which splits the input images in half to simplify feature learning.\n\nTo the best of the authors' knowledge, this is the first approach that combines a greedy layer-wise training method with adversarial learning. This is also the first approach to solely focus on pose and illumination invariant emotion recognition. Future work will look at exploiting the ability of our model to generate new data in order to deal with scenarios where lack of multi-pose labeled exists.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "List Of References",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualization of the ﬁrst shallow autoencoder in the",
      "page": 2
    },
    {
      "caption": "Figure 2: ConvMLP layers illustration. Connection weights for",
      "page": 4
    },
    {
      "caption": "Figure 2: for a pictorial description. Given an input image I and a",
      "page": 4
    },
    {
      "caption": "Figure 2: how the pixels on the second feature map are at a different",
      "page": 4
    },
    {
      "caption": "Figure 3: Top row: input images xϕ to the GASCA2 model",
      "page": 5
    },
    {
      "caption": "Figure 3: , the pose and",
      "page": 5
    },
    {
      "caption": "Figure 3: , the reconstructed images also",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "invariance. The\nproposed model\ncan\ndeal with\nfaces with"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "up\nto ±60\ndegrees\nto\nan\nillumination\ninvariant\nfacial",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "a\nfacial pose of up to ±60 degrees\nand several degrees of"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "illumination. Our main contributions can be summarized as:"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "layer\nthat\nexploits both local\nand global\nspatial",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "layer with a reduced number",
          "a model\nthat explicitly addresses facial pose and illumination": "•\na novel deep Generative Adversarial Stacked Convolu-"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "that\nexploits\nfacial\nsymmetry. Furthermore, we",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "tional Autoencoder model\nthat\nlearns to map faces with"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "a\ngenerative\nadversarial\ngradual\ngreedy\nlayer-wise",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "facial pose of up ±60 degrees\nto 0 degrees\nrepresenta-"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "tions."
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "an\nefﬁcient\nand\nincremental manner. We\ndemonstrate\nthe",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "•\na\nhybrid\ndeep\nlearning\nlayer\nemploying\nconvolutional"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "several\nfacial\nemotion\nrecognition\ncorpora,\nincluding\none",
          "a model\nthat explicitly addresses facial pose and illumination": "ﬁlters\nto\nretain\nspatial\ninformation\nand\nlearn\nsalient"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "features, and fully connected units shared across the depth"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "Index Terms—Emotion Recognition, Facial Pose Normaliza-",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "dimension to facilitate the reduction of\nfacial pose."
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "•\na convolutional\nlayer with reduced number of parameter"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "that exploits\nfacial\nsymmetry and learns\nfrom only one"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "half of\nthe face."
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "I.\nINTRODUCTION",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "•\na\ngradual\ngreedy\nlayer-wise\nalgorithm for Generative"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "Facial expression recognition continues\nto be of great\nin-",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "Adversarial Autoencoders."
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "in the machine\nlearning community due\nto the many",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "•\nan illumination and pose\ninvariant\nemotion recognition"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "it\npresents. Many works\nin\nthe\nliterature\nhave",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "classiﬁer\nthat produces state-of-the-art classiﬁcation per-"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "a\nvariety\nof models\nthat\nproduce\nstate-of-the-art",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "formance in images taken in both controlled and uncon-"
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "on\nvarious\nfacial\nexpression\ncorpora\ncollected\nin",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "strained environments."
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "these models are",
          "a model\nthat explicitly addresses facial pose and illumination": ""
        },
        {
          "Abstract—In this work, we propose a novel Generative Adver-": "",
          "a model\nthat explicitly addresses facial pose and illumination": "We also show the difference of training for pose invariance"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "input\nimage x to a hidden representation h = f (x), and learn",
          "image to a target\nimage that does not necessarily lie in the": "same distribution,\nthey are suitable for this task. This in effect"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "a\nfunction g\nthat maps h to a\nreconstruction y = g(f (x))",
          "image to a target\nimage that does not necessarily lie in the": "means\nthat we\nare\ninterested in imposing a distribution on"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "where y is an approximation of x. However,\nin recent works,",
          "image to a target\nimage that does not necessarily lie in the": "the\ninput data\nto produce\nreconstructions\nthat\nresemble\nthe"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "[5]\nit has been established that\nthe target\nreconstruction does",
          "image to a target\nimage that does not necessarily lie in the": "desired target. Adversarial autoencoders can facilitate this task"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "not need to be the same as the input x to the autoencoder. This",
          "image to a target\nimage that does not necessarily lie in the": "as\nthey\nuniformly\nimpose\na\ndata\ndistribution\non\nthe\ncode"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "is\nsupported by the theory that\nto be useful, an autoencoder",
          "image to a target\nimage that does not necessarily lie in the": "vector,\ni.e.\nthe hidden representation produced by the encoder"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "should only learn an approximation of the target reconstruction",
          "image to a target\nimage that does not necessarily lie in the": "element,\nto generate\nrealistic\nreconstructions. Moreover,\nad-"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "and not an identity function that\nreplicates it\n[6].",
          "image to a target\nimage that does not necessarily lie in the": "versarial autoencoders are designed to produce very realistic"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "In principle this establishes that\nthe input and target vectors",
          "image to a target\nimage that does not necessarily lie in the": "reconstructions."
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "in an autoencoder do not need to be the same, and therefore",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "III. GENERATIVE ADVERSARIAL STACKED"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "we\ncan\nlearn\na\nfunction\nthat maps\nan\ninput\nfrom a\ngiven",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "CONVOLUTIONAL AUTOENCODERS"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "distribution\nto\na\ntarget\nthat\nlies\nin\na\ndifferent\ndistribution.",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Autoencoder can also be trained in a greedy layer-wise (GLW)",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "[7]\nfashion which has proven to be more effective than joint",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "training [8].",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Generative Adversarial Autoencoders\n[9] are some of\nthe",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "latest autoencoder models that\nfollow the trending popularity",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "of Generative Adversarial Networks (GANs)\n[10]. GANs are",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "composed\nof\ntwo\nnetworks:\na\ngenerative model G and\na",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "discriminator model D. Both models are trained by playing",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "a min-max adversarial game where\nthe discriminator model",
          "image to a target\nimage that does not necessarily lie in the": "Fig. 1: Visualization of\nthe ﬁrst\nshallow autoencoder\nin the"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "tries to determine if a given sample is from the generator or the",
          "image to a target\nimage that does not necessarily lie in the": "GASCA model."
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "dataset. In contrast, the generator maps samples z from a prior",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "distribution p(z)\nand maps\nit\nto the data\nspace. Generative",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "We\nintroduce\nthe Generative Adversarial\nStacked Con-"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Adversarial Autoencoders follow a similar approach where the",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "volutional Autoencoders\n(GASCA)\nframework.\nJust\nlike\nin"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "generator\nis an autoencoder\nthat maps an input x to a latent",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "conventional\nadversarial\nautoencoders,\nin a GASCA model,"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "representation z that\nlies in an aggregate posterior distribution",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "the discriminator attempts to tell whether a sample comes from"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "q(z) and back to a reconstruction y which is an approximation",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "the training dataset or if it\nis a reconstruction produced by the"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "of x. The discriminator network in this framework attempts to",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "autoencoder."
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "determine if a sample has been drawn from a prior distribution",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "Let xϕ be a sample from the data distribution pd(xϕ) and"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "p(z) or\nfrom the latent distribution q(z).",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "xµ the sample from the data distribution pd(xµ) used as the"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Although GANs are mainly used for data synthesis,\nsome",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "desired target reconstruction. The autoencoder G model\nlearns"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "works\nhave\nexplored\ntheir\nuse\nin\nclassiﬁcation\n[11].\nFor",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "to map xϕ to a latent space z, note that this is not an aggregate"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "emotion recognition, works\nemploying GANs mainly focus",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "posterior\nas\nin\nconventional\nadversarial\nautoencoders,\nand"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "on using GANs\nfor data augmentations, whether\nin emotion",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "back to a reconstruction y that\nin the\nresembles xµ and lies"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "recognition from speech [12] or\nfrom facial expressions [13],",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "distribution q(y). The discriminator D attempts to differentiate"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "[14]. Most\nother works\nfocus\non\ngenerating\ndata,\nbut\ndo",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "between y and xµ."
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "not explore emotion recognition. Such works\ninclude multi-",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "In the conventional adversarial autoencoder framework [9] a"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "pose\nface\nrecognition [15],\n[16], or\nfacial\nexpression image",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "distribution p(z) —often a Gaussian distribution—is imposed"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "completion [17].",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "q(z)\nq\non\nby\nestimating\nthe\ndivergence\nbetween\nand\np."
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Some works attempt to deal with some of the common chal-",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "This imposition can be used to produce reconstructions with"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "lenges in emotion recognition, such as, illumination invariance.",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "speciﬁc\nfeatures. However,\nin this work,\nthe objective\nis\nto"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "Contemporary attempts\nto address\nillumination invariance in",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "produce\nreconstructions\nthat\nare\nas\nclose\nas possible\nto the"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "the domain of facial expression recognition include the use of",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "desired target\ninstead of\nimposing\nimage xµ. Consequently,"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "noise injection [4], blurring images with Gaussian ﬁlters [18],",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "random noise on the hidden representation vector, the GASCA"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "a\ncombination\nof\nhistograms,\nprincipal\ncomponent\nanalysis",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "model\nimposes pd(xµ) on q(y) in the following way:"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "(PCA) and discrete cosine transforms [19], or complex models",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "(1)\nq(y) = Rxµ q(y|xµ)pd(xϕ)dxµ"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "[18],\n[20] and very deep CNN architectures [21].",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "To the best of the knowledge of the authors, although many",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "With this\nformulation,\nthe discriminator model D is opti-"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "works target pose invariant face recognition, no existing work",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "mized to rate samples from pd(xµ) with a higher probability,"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "focuses on pose invariant\nfacial expression recognition.",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "and samples from q(y) with a low probability. Formally this"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "In this work we normalize facial pose ϕ, where |ϕ| > 0",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "is deﬁned as:"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "to frontal\nimages 0 degrees pose. We also normalize images",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "(cid:104)"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "with relative luminance Y to a target luminance µ. Considering",
          "image to a target\nimage that does not necessarily lie in the": "1 m\nm(cid:88) i\nlog D(x(i)\n(2)\n∇θd\nµ ) + log (cid:0)1 − D(G(y(i)))(cid:1)(cid:105)"
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "that autoencoders allow us to learn a mapping from an input",
          "image to a target\nimage that does not necessarily lie in the": ""
        },
        {
          "f\nAutoencoders\nlearn\nan\nencoder\nfunction\nthat maps\nan": "",
          "image to a target\nimage that does not necessarily lie in the": "=1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "of\nsize b, an autoencoder model G and discriminator model"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "D both with weight matrices Wg\nand Wd, an absolute value"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "cost\nfunction loss:\ntrain G and D jointly such that:"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "2"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "1: V (Wd) ←\nN in+N out"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "2"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "2: V (Wg) ←\nN in+N out"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "3:\nfor\nk = 1,\n. . . , M do"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "4:\nfor\nn = 1,\n. . . , m do"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "5:\n[xϕ, xµ]n ⊂ 1, . . . , m ← random(X, b)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "6:\nyg ← predict(xϕ, G)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "7:\nLg ← loss(xµ, yg)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "8:\nG ← update(G, Lg)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "9:\npµ ← predict(xµ, D)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "10:\nLpd(xµ) ← loss(1, pµ)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "11:\nD ← update(D, Lpd(xµ))"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "12:\npy ← predict(yg, D)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "13:\nLq(y) ← loss(0, py)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "14:\nLadversary = Lpd(xµ) + Lq(y)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "15:\nLminimax ← loss(1, py)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "16:\nL = Lminimax + Lg"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "17:\nM MLg ← lossGrad(1, py)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "18:\nM Mg ← Grad(yg, M ML−g, D)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "19:\nG ← update(G, M Mg)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "20:\nAdam(L, G)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "21:\nSGD(Ladversary, D)"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "22:\nend for"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "23:\nend for"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": "24:\nreturn G, D"
        },
        {
          "Algorithm 2 Given a training dataset X with m mini-batches": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "Fine-tune G on raw pixel data. Forward propagate xϕ ⊂ X",
          "epochs": "1:\nfor\nk = 1,\n. . . , M do"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "through G and use the resulting features, along with xµ,",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "2:\nfor\nn = 1,\n. . . , m do"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "ﬁne-tune D for binary classiﬁcation.",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "3:\n[x, xµ]n ⊂ 1, . . . , m ← random(X, b)"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "y ← predict(x, f )"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "1:\n[G1, D1] ← L(G1, D1, X, ˜X)",
          "epochs": "4:"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "5:\nL ← loss(xµ, y)"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "2: G ← G ◦ G1",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "6:\nf ← update(f, L)"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "3: D ← D ◦ D1",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "7:\nend for"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "4:\nfor\nk = 2,\n. . . , m do",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "8:\nend for"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "5:\n[ξ, δ] ← D",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "return f"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "6:\n[Xg, ˜Xg] ← ξ(X, ˜X)",
          "epochs": "9:"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "7:\n[Gk, Dk] ← L(Gk, Dk, Xd, ˜Xd)",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "8:\nG ← G(k) ◦ G",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "9:\nD ← D(k) ◦ D",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "10:\nG ← T (G, X, ˜X)",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "the\nspecial\ncase where\nthe\ninput\nand target\nimages\nare\nsig-"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "11:\nXϕ ← G(xϕ)",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "niﬁcantly different, GANGGLW is not very compatible with"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "12:\nD ← T (D, {Xϕ, xµ ⊂ X})",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "Convolutional Neural Networks (CNNs). CNNs are designed"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "13:\nend for",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "to retain spatial\ninformation through ﬁlter kernels, whereas in"
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "14:\nreturn G, D",
          "epochs": ""
        },
        {
          "autoencoder Gk\nand add them to their corresponding stacks.": "",
          "epochs": "GANGGLW the input and target reconstruction images are not"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In this formulation of ConvMLP layers, during the forward": "pass,\nthe weight matrix W is used to shape every feature map"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "produced by the\nconvolution operation and is updated only"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "once using backpropagation. Sharing this layer across the third"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "dimension—not\ntaking into account\nthe batch dimension for"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "simplicity—its weight matrix is many orders of magnitude"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "smaller\nthan without weight\nsharing. This\nalso ensures\nthat"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "the shif ting layer\nlearns to shift all\nthe features highlighted"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "in every feature plane in the same manner. Notice in Figure"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "2 how the pixels on the second feature map are at a different"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "location."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "In addition to ConvMLP layers, and in order to support\nthe"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "pose invariant\ntraining approach and models presented in this"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "work,\na\nsecond convolutional\nlayer\nis\nintroduced here. This"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "novel\nlayer,\nreferred to as HalfConv hereafter, exploits facial"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "symmetry present\nin face images with an estimated pose of"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "zero degrees. HalfConv layers slice the input vector vertically"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "in half. The half containing all\nthe facial features belonging to"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "the left side of a face is then used as input for a convolutional"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "layer that has half the number of parameters than an empirical"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "convolutional\nlayer. The resulting feature map is then simply"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "mirrored across the y axis."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "When applied to face or facial expression images, HalfConv"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "layers give up some important\ninformation on the right edge"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "of the input\nimage, which in effect corresponds to the features"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "in the middle of a face. This is due to the nature of\nthe con-"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "volution operation, which convolves a kernel across an input"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "image,\nresulting in a\nfeature plane with smaller dimensions"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "than the input\nimage. For this reason, HalfConv layers enforce"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "zero padding p on right side edge of the input\nimage to allow"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "the ﬁlter kernel to capture the features closer to the edge. Their"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "output\nis then deﬁned by:"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "C(i, j) = (I ∗K)(i, j) =\nI(m, n)K(i−m, (j +p)−n)"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "(cid:88) m\n(cid:88) n"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "(7)"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "where p = j"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "2 +1. Then every resulting feature plane is reﬂected"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "over the y axis, resulting in a full\nimage. Note that padding p"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "is enforced to avoid losing features at\nthe edges of the image."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "The main\nadvantage\noffered\nby HalfConv\nlayers\nis\nthe"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "reduced\nnumber\nof\nlearnable\nparameters, which\nin\neffect"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "results\nin easier\nand faster\ntraining. Because\nthe only extra"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "operation required by this\nlayer\nis\nsimply mirroring a\nfea-"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "ture vector vertically, HalfConv layers\nare\nsigniﬁcantly less"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "computationally expensive than empirical convolutional layers."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "Furthermore, because this layer only deals with frontal\nfaces,"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "there\nis no need to employ any shifting neurons. Note\nthat"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "these\nlayers\nare only suitable\nfor\ncases where\nsymmetry is"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "existent in the input image or is desired in the resulting feature"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "plane. Therefore,\nin the GASCA model,\nthese layers are only"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "used when α = 0."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": ""
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "V. UNSUPERVISED FEATURE LEARNING"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "We train two Generative Adversarial Stacked Convolutional"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "Autoencoders, one\nto normalize\nfacial pose\nand\n(GASCA1)"
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "another to normalize facial pose and illumination (GASCA2)."
        },
        {
          "In this formulation of ConvMLP layers, during the forward": "We train both models using the GANGGLW training method"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Every shallow autoencoder in the GASCA models is trained": "for 100 and ﬁne-tuned for 20 epochs. G is optimized using"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "ADAM [23], whereas D employs SGD with Nesterov mo-"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "mentum. The initial\nlearning rates for each individual shallow"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "autoencoder\nin G were\nset\nto λ ∈ {0.1, 0.3, 0.5, 0.7, 0.75}."
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "Since D learns faster than G, the shallow autoencoders employ"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "smaller\nlearning\nrates: λ ∈ {0.01, 0.03, 0.5, 0.07}. During"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "ﬁne-tuning,\nthe stacks G and D use a learning rate of 0.001."
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "This combination of hyper-parameters provided the best results"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "for both models."
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "VI. POSE INVARIANT RECONSTRUCTION RESULTS"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "The novel pose\ninvariant Generative Adversarial Stacked"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "Convolutional Autoencoder models proposed in this work are"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "trained\nto\ngradually\nreduce\nfacial\npose\nusing GANGGLW"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "training. As\nit\ncan\nbe\nobserved\nin Figure\n3,\nthe\npose\nand"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "illumination\nto\nreduce\ninvariant GASCA2 model manages"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "facial\npose\nin\nfacial\nimages with\nan\nestimated\npose\nof\nup"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "to ±60 degrees.\nIt also produces reconstructions with similar"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "illumination."
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "It\ncan also be observed that on the\nimages with pose of"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "±60\ndegrees\nhalf\nof\nthe\nface\nis\nnot\nvisible,\nyet\nthe\npose"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "invariant model manages\nto ﬁll\nin the missing information,"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "and more importantly keeps the shape of\nfacial shapes which"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "are important for emotion recognition: eyes, eyebrows, mouth,"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "nose, cheeks, among others. Nonetheless,\nthe greater the pose"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": ""
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "the quality of\nis\nin xϕ the poorer"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "justiﬁed by (i)\nthe fact\nthat\nthe model has to compensate for"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "missing information, (ii) the fact\nthat only one layer is trained"
        },
        {
          "Every shallow autoencoder in the GASCA models is trained": "speciﬁcally to deal with that particular\nfacial pose,\n(iii)\nthe"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "plane. Although,\nthis is not evaluated in this research.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "and,\nbecause\nthe CFE corpus\nhas more\nis\nimages, CNN2b"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "As\nillustrated in Figure 3,\nthe\nreconstructed images\nalso",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "only ﬁne-tuned for two epochs. Since the stacked autoencoders"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "do not have\na horizontal\nline diving the\nface\nin two,\nas\nit",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "are optimized using ADAM, all classiﬁers are ﬁne-tuned also"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "would be expected due to the use of HalfConv layers. When",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "using ADAM and a learning rate of 0.01. Using a different"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "visualizing the\nfeature planes produced by these\nlayers,\nthe",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "optimizer like SGD for ﬁne-tuning would lead to the gradients"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "line is somewhat visible. However, because in the ﬁnal stack",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "changing drastically and require a longer ﬁne-tuning process."
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "G this layer is followed by all the layers in the decoder stack of",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "G, and since the line is not visible in the target reconstruction",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "VIII. POSE INVARIANT EMOTION RECOGNITION RESULTS"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "images,\nit vanishes during ﬁne-tuning.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "As it can be observed in Table I,\nthe pose invariant model,"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "We did not notice signiﬁcant differences between the recon-",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "obtains\na\nclassiﬁcation\nperformance\nof\n96.81%.\nIn\nCNN1,"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "structions of both GASCA1 and GASCA2 models. However,",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "contrast,\nthe pose\ninvariant model\nthat\nalso incorporated il-"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "the reconstruction loss\nfor\nthe latter was marginally smaller",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "lumination invariance obtains\na\nstate-of-the-art\nclassiﬁcation"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "and as seen in the next section it generalizes better.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "rate\nof\n98.07%. The main\ndifferences\nin\nperformance\nare"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "observed for classes: surprise, Fear, and Angry, whereas both"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "VII. POSE AND ILLUMINATION INVARIANT EMOTION",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "CNN1 and CNN2a obtained the same classiﬁcation accuracy"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "RECOGNITION",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": ""
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "for\nthe\nremaining classes. Because both models\nare\ntrained"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "using\na\nrelatively\nsimilar\napproach,\nit\nis\nhypothesized\nthat"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "Once\na GASCA model\nis\ntrained\nand ﬁne-tuned\nfor\nre-",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "these discrepancies\nin classiﬁcation performance\nare due\nto"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "construction,\nit\ncan\nbe\nused\nas\na\ngeneric\nfeature\nextractor",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "these three classes containing more images with varying image"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "for\nfacial\nexpression images. However,\nit\ncan only provide",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "luminance,\nthus the pose and illumination invariant model\nis"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "feature vectors that are pose and illumination invariant but that",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "able to generalize better."
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "do not necessarily discriminate between different\nemotions.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "is\nevaluated on the NAOFaces\ncorpus\nThe CNN2b model"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "Therefore, we ﬁne-tune\nthe\nencoder\nelement of G for\nclas-",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "and achieves 81.36% accuracy. This is signiﬁcantly lower than"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "siﬁcation. We discard both the discriminator model D along",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "the performance of the other models on the KDEF corpus. We"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "the generator D and attach a\nwith the decoder element gD of",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "attribute this lower performance to the fact\nthat\nthe NAOFaces"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "classiﬁcation layer\nto the encoder.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "contains images that are substantially more difﬁcult, i.e. people"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "is used to initialize a\nThe pose invariant GASCA1 model",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "with glasses, at different poses, and different ethnicity. More-"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "classiﬁer model, CNN1, which is ﬁne-tuned and tested on the",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "over,\nthis model was not ﬁne-tuned on any images from this"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "KDEF corpus\n[25]. This dataset contains\nfrontal and images",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "corpus. This theory is further supported by the fact\nthat when"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "at ±45\ndegrees. No\nother\npublicly\navailable\ndatasets with",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "we split\nthe CFE corpus 80% training and 20% testing, we"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "multiple poses have facial expression labels.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "obtain 99% on the test set."
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "The pose and illumination invariant GASCA2 model is used",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "One important observation in Table II is that, when looking"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "to initialize\na\nis\nalso\nsecond classiﬁer, CNN2a. This model",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "at\nthe missclassiﬁed images for a given class, on average 40%"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "ﬁne-tuned and tested on the KDEF corpus.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "of\nthem are\nfrontal\nimages,\ni.e.\nimages with\nzero\ndegrees"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "In\naddition,\nin\nan\nattempt\nto\ntest\nthe\nrobustness\nof\nour",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "pose, and the remaining 60% are those with a pose. However,"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "to initialize a third\nproposed methodology, we use GASCA2",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "because the ratio of images with a facial pose is 2:1 compared"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "to the\nlack of publicly avail-\nmodel CNN2b. However, due",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "to\nthose without\none. This means\nthat\non\naverage, more"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "able data taken in realistic environments with multi-pose and",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "images without\nfacial pose\nare missclassiﬁed. These\nresults"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "varying illumination, as well as labels for\nthe emotions being",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "and\nobservations\nare\nof\ngreat\nimportance\ngiven\nthat\nthey"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "expressed, we build a large dataset composed of the CK+ [26],",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "support\nthe pose invariant pretraining approach presented in"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "JAFFE [27], KDEF [25], and FEEDTUM [28] corpora. We",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "this work. Another observation is that not a single image from"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "refer to this corpus as combined facial expressions (CFE). Note",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "the other classes was confused with Neutral. This particular"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "that because, as later discussed, we obtain over 99.6% on this",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "score is signiﬁcant taking into account that all emotions derive"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "corpus,\nthis model\nis evaluated on completely novel data:\nthe",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "from a neutral state, often resulting in low precision scores."
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "entire NAOFaces corpus [29]. This set has total of 196 images",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "Despite\non\nthe good performance offered by the CNN2a"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "collected in unconstrained environments. Participants were 28",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "the NAOFaces corpus,\nthe classiﬁcation performance offered"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "21 males and 7 females between ages 18 and 55 from at\nleast",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "by this model\nis not\nideal. This\nis\nattributed to one major"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "ﬁve different ethnic backgrounds.",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "factor:\ncultural\ndifferences. Because\nthe model was\ntrained"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "As\nopposed\nto\nempirical CNN classiﬁer models which",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "solely on images from Caucasian people,\nthe model has never"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "employ a\nfully connected layer\nafter\nthe\nlast\nconvolutional",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "learned to adjust\nto cultural difference. The NAOFaces corpus"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "layer,\nthe\nclassiﬁers\nin this work map the\nresulting feature",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "contains\nimages of people from at\nleast ﬁve different back-"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "planes\nproduced\nby\nthe\nlast\nconvolutional\nlayer, which\nis",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "grounds\nincluding: Asian, Arab, Black,\nIrish and Hispanic,"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "a HalfConv\nlayer,\ndirectly\nto\nan\noutput SoftMax\nlayer\nfor",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "among others unrevealed ones. In effect, because people from"
        },
        {
          "reduction\nby mapping\na\nfeature\nplane\nto\na\nsmaller\nfeature": "classiﬁcation, as done in [30].",
          "The CNN1 and CNN2a models are ﬁne-tuned for 10 epochs": "different ethnic backgrounds express emotions differently [31],"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": ""
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "A"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "94.44"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "0.00"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "000"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "0.00"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "0.00"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "0.79"
        },
        {
          "TABLE I: (left) Classiﬁcation performance (96.810%) of the CNN1 model on the KDEF corpus pretrained for pose invariance.": "0.00"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "model on the NAOFaces corpus.",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": "KDEF corpus: ResNet-34 —state-of-the-art classiﬁer; CNN1"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": "—pose invariant classiﬁer proposed; CNN2a pose and illumi-"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "A\nD\nF\nH\nN\nSa\nSu",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "nation invariant classiﬁer proposed.",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "A\n92.86\n7.14\n0.00\n0.00\n0.00\n0.00\n0.00",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "D\n8.33\n75.00\n8.33\n0.00\n0.00\n8.33\n0.00",
          "TABLE III: Classiﬁcation": "Resnet34",
          "performance\ncomparison": "CN N1\nCN N2a"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "F\n9.09\n0.00\n81.81\n0.00\n0.00\n0.00\n9.09",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "A\n84.127%",
          "performance\ncomparison": "94.444%\n96.825%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "H\n0.00\n0.00\n0.00\n100.00\n0.00\n0.00\n0.00",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "D\n85.600%",
          "performance\ncomparison": "97.600%\n97.600%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "N\n3.85\n0.00\n3.85\n15.38\n57.69\n11.54\n7.69",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "F\n73.810%",
          "performance\ncomparison": "89.683%\n93.651%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "Sa\n9.09\n0.00\n18.18\n0.00\n0.00\n72.72\n0.00",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "H\n98.413%",
          "performance\ncomparison": "100.000%\n100.000%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "Su\n0.00\n0.00\n10.53\n0.00\n0.00\n0.00\n89.47",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "N\n90.400%",
          "performance\ncomparison": "100.000%\n100.000%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "Sa\n84.921%",
          "performance\ncomparison": "98.413%\n98.413%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "Su\n95.161%",
          "performance\ncomparison": "97.581%\n100.000%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "",
          "TABLE III: Classiﬁcation": "T otal\n87.472%",
          "performance\ncomparison": "96.810%\n98.070%"
        },
        {
          "TABLE II: Classiﬁcation performance (81.36%) of the CNN2b": "classiﬁer\nshould be\ntrained with images of participants",
          "TABLE III: Classiﬁcation": "",
          "performance\ncomparison": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "los, M. Nikandrou, T. Giannakopoulos, A. Katsamanis, A. Potamianos,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "representation z with an estimated pose of 0 degrees. Once",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "and S. Narayanan, “Data Augmentation using GANs for Speech Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the GASCA model\nis trained,\nthe encoder element\nis used to",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition,” 2019."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "initialize a CNN model which is ﬁne-tuned for classiﬁcation.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[13] W. Yi, Y. Sun, and S. He, “Data Augmentation Using Conditional GANs"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in Electromagnetics Re-\nfor Facial Emotion Recognition,” in Progress"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "The\noutstanding\nperformance\nof\nthe GASCA models\nis",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "search Symposium, vol. 2018-Augus, pp. 710–714, Institute of Electrical"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "derived\nfrom four\nconcepts:\n(i)\nour GANGGLW training",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "and Electronics Engineers Inc., dec 2018."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "method (ii)\nthe ConvMLP layers with shif ting neurons,\n(iii)",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[14] X. Zhu, Y. Liu, Z. Qin,\nand J. Li,\n“Data Augmentation in Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Classiﬁcation Using Generative Adversarial Networks,” nov 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the HalfConv layers which take\nexploit of\nfacial\nsymmetry,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[15]\nL. Tran, X. Yin,\nand X. Liu,\n“Disentangled\nrepresentation\nlearning"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "and\n(iv) multi-pose\nfacial\nexpressions\ndata. Our\npose\nand",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "GAN for pose-invariant\nface recognition,” in Proceedings - 30th IEEE"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "illumination invariant method produces state-of-the-art classi-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Conference on Computer Vision and Pattern Recognition, CVPR 2017,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "vol. 2017-Janua, pp. 1283–1292, 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ﬁcation performance on multi-pose facial expression corpora.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[16]\nJ. Zhao, L. Xiong, K.\nJayashree,\nJ. Li, F. Zhao, Z. Wang, S. Pranata,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Moreover,\nthe GASCA model produces\nreconstruction with",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "S. Shen, S. Yan, and J. Feng, “Dual-Agent GANs for Photorealistic and"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "very small errors and is able to generalize on unseen data.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Identity Preserving Proﬁle Face Synthesis,” Nips 2017, no. 15, pp. 1–11,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "The\nsuccess of\nthe pose\ninvariant models\nis\nin part due",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[17]\nJ. Chen, J. Konrad, and P. Ishwar, “VGAN-Based Image Representation"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "to ConvMLP layers, which\nlearn\nsalient\nfeatures\nand\nshift",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Learning for Privacy-Preserving Facial Expression Recognition,” tech."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "them as needed to reduce facial pose. HalfConv layers also",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "rep."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[18] D. H. Liu, K. M. Lam,\nand L. S. Shen,\n“Illumination invariant\nface"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "play an important\nrole as they reduce the number of\nlearning",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "recognition,” Pattern Recognition, vol. 38, pp. 1705–1716, oct 2005."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "parameters. HalfConv\nlayers were\ninspired\nby\nthe model",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[19] C. Tosik, A. Eleyan, and M. Salman, “Illumination invariant face recog-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "presented by [33], which splits\nthe\ninput\nimages\nin half\nto",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "nition system,”\nin 2013 21st Signal Processing and Communications"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Applications Conference, SIU 2013, pp. 1–4,\nIEEE, apr 2013."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "simplify feature learning.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[20] O. Gupta, D. Raviv,\nand R. Raskar,\n“Deep video gesture\nrecognition"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "To\nthe\nbest\nof\nthe\nauthors’\nknowledge,\nthis\nis\nthe ﬁrst",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "using illumination invariants,” Arxiv, pp. 1–9, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "approach that combines a greedy layer-wise training method",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[21] X.\nChen, X.\nLan, G.\nLiang,\nJ.\nLiu,\nand N.\nZheng,\n“Pose-and-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "illumination-invariant\nface representation via a triplet-loss trained deep"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "with\nadversarial\nlearning.\nThis\nis\nalso\nthe\nﬁrst\napproach",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Tools\nreconstruction model,” Multimedia\nand Applications,\nvol.\n76,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "to solely focus on pose\nand illumination invariant\nemotion",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "pp. 22043–22058, nov 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "recognition. Future work will\nlook at exploiting the ability of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[22] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-PIE,” in"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2008 8th IEEE International Conference on Automatic Face & Gesture"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "our model\nto generate new data in order to deal with scenarios",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition, pp. 1–8,\nIEEE, sep 2008."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "where lack of multi-pose labeled exists.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[23] D. P. Kingma and J. L. Ba, “Adam,” pp. 1–15, 2015."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[24] W.\nShi,\nJ. Caballero,\nF. Huszar,\nJ. Totz, A.\nP. Aitken, R. Bishop,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "LIST OF REFERENCES",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "D. Rueckert, and Z. Wang, “Real-Time Single Image and Video Super-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Resolution Using an Efﬁcient Sub-Pixel Convolutional Neural Network,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[1] G. Levi and T. Hassner, “Emotion Recognition in the Wild via Convolu-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in 2016 IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "tional Neural Networks and Mapped Binary Patterns,” in Proceedings of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "(CVPR), pp. 1874–1883, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the 2015 ACM on International Conference on Multimodal\nInteraction",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "¨\n[25] D. Lundqvist, A.\nFlykt,\nand A.\nOhman,\n“The Karolinska Directed"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "-\nICMI\n’15,\n(New York, New York, USA), pp. 503–510, ACM Press,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Emotional\nFaces\n- KDEF CD ROM from Department\nof Clinical"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Neuroscience, Psycology section,” Karolinska Institutet, pp. 3–5, 1998."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[2] A. Krizhevsky, L. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[26]\nP. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "with Deep Convolutional Neural Networks,” NIPS,\npp.\n1106–1114,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "“The Extended Cohn-Kanade Dataset\n(CK+): A complete dataset\nfor"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2012.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "action unit and emotion-speciﬁed expression,” tech.\nrep."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[3]\nS.\nIoffe\nand C.\nSzegedy,\n“Batch Normalization: Accelerating Deep",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[27] M. Lyons, S. Akamatsu, M. Kamachi,\nand J. Gyoba,\n“Coding facial"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Network Training by Reducing Internal Covariate Shift,” feb 2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "- 3rd IEEE Inter-\nexpressions with Gabor wavelets,”\nin Proceedings"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[4]\nP.\nVincent\nPASCALVINCENT\nand\nH.\nLarochelle\nLAROCHEH,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "national Conference on Automatic Face and Gesture Recognition, FG"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "“Stacked Denoising Autoencoders: Learning Useful Representations",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "1998, pp. 200–205, 1998."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "in a Deep Network with a Local Denoising Criterion Pierre-Antoine",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[28]\nF. Wallhoff, B. Schuller, M. Hawellek, and G. Rigoll, “Efﬁcient\nrecog-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Manzagol,” Journal of Machine Learning Research, vol. 11, pp. 3371–",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "nition of authentic dynamic facial expressions on the feedtum database,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "3408, 2010.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in 2006 IEEE International Conference on Multimedia and Expo, ICME"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[5] A. Ruiz-Garcia, V. Palade, M. Elshaw, and I. Almakky, “Deep Learning",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2006 - Proceedings, vol. 2006, pp. 493–496,\nIEEE,\njul 2006."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "for\nIllumination Invariant Facial Expression Recognition,” in Proceed-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[29] A. Ruiz-Garcia, N. Webb, V. Palade, M. Eastwood,\nand M. Elshaw,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ings of\nthe International Joint Conference on Neural Networks, (Rio de",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "“Deep Learning for Real Time Facial Expression Recognition in Social"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Janeiro),\nIEEE, 2018.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "of\nthe\nInternational Conference\non Neural\nRobots,”\nin Proceedings"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[6]\nI. Goodfellow, Bengio, Yoshua, and A. Courville, Deep Learning. MIT",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Information Processing, 2018."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Press, 2016.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[30] K. He, X. Zhang, S. Ren,\nand J. Sun,\n“Deep Residual Learning for"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[7] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy Layer-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Image Recognition,” Arxiv.Org, vol. 7, pp. 171–180, dec 2015."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Wise Training of Deep Networks,” tech.\nrep.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[31] N. M. Hewahi and A. R. M. Baraka, “Impact of Ethnic Group on Human"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[8] A. Ruiz-Garcia, M. Elshaw, A. Altahhan,\nand V.\nPalade,\n“Stacked",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Emotion Recognition Using Backpropagation Neural Network,” BRAIN."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "deep convolutional\nauto-encoders\nfor\nemotion recognition from facial",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Broad Research in Artiﬁcial, pp. 20–27, 2012."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the International Joint Conference on\nexpressions,” in Proceedings of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[32] M. Kan, S. Shan, H. Chang, and X. Chen, “Stacked progressive auto-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Neural Networks, vol. 2017-May, pp. 1586–1593,\nIEEE, may 2017.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "encoders (SPAE)\nfor\nface recognition across poses,” in Proceedings of"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[9] A. Makhzani, J. Shlens, N. Jaitly,\nI. Goodfellow, and B. Frey, “Adver-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "the IEEE Computer Society Conference on Computer Vision and Pattern"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "sarial Autoencoders,” 2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition, pp. 1883–1890,\nIEEE,\njun 2014."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[10]\nI. Goodfellow, J. Pouget-Abadie, M. M. A.\nin neural\n. . . , and U. 2014,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[33] A. Ruiz-Garcia, M. Elshaw, A. Altahhan,\nand V. Palade,\n“Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "“Generative adversarial nets,” Advances in Neural Information Process-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition Using Facial Expression Images for a Robotic Companion,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ing Systems 27, pp. 2672–2680, 2014.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "of Neural Networks:\n17th\nInternational\nin Engineering Applications"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[11]\nS. A. Israel, J. Goldstein, J. S. Klein, J. Talamonti, F. Tanner, S. Zabel,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Conference, EANN 2016, Aberdeen, UK, September 2-5, 2016, Proceed-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "P. A. Sallee,\nand L. McCoy,\n“Generative Adversarial Networks\nfor",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "ings, pp. 79–93, Springer, Cham, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2017\nIEEE Applied\nImagery Pattern Recognition\nClassiﬁcation,”\nin",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Workshop (AIPR), pp. 1–4,\nIEEE, oct 2017.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "los, M. Nikandrou, T. Giannakopoulos, A. Katsamanis, A. Potamianos,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "representation z with an estimated pose of 0 degrees. Once",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "and S. Narayanan, “Data Augmentation using GANs for Speech Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the GASCA model\nis trained,\nthe encoder element\nis used to",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition,” 2019."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "initialize a CNN model which is ﬁne-tuned for classiﬁcation.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[13] W. Yi, Y. Sun, and S. He, “Data Augmentation Using Conditional GANs"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in Electromagnetics Re-\nfor Facial Emotion Recognition,” in Progress"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "The\noutstanding\nperformance\nof\nthe GASCA models\nis",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "search Symposium, vol. 2018-Augus, pp. 710–714, Institute of Electrical"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "derived\nfrom four\nconcepts:\n(i)\nour GANGGLW training",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "and Electronics Engineers Inc., dec 2018."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "method (ii)\nthe ConvMLP layers with shif ting neurons,\n(iii)",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[14] X. Zhu, Y. Liu, Z. Qin,\nand J. Li,\n“Data Augmentation in Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Classiﬁcation Using Generative Adversarial Networks,” nov 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the HalfConv layers which take\nexploit of\nfacial\nsymmetry,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[15]\nL. Tran, X. Yin,\nand X. Liu,\n“Disentangled\nrepresentation\nlearning"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "and\n(iv) multi-pose\nfacial\nexpressions\ndata. Our\npose\nand",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "GAN for pose-invariant\nface recognition,” in Proceedings - 30th IEEE"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "illumination invariant method produces state-of-the-art classi-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Conference on Computer Vision and Pattern Recognition, CVPR 2017,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "vol. 2017-Janua, pp. 1283–1292, 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ﬁcation performance on multi-pose facial expression corpora.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[16]\nJ. Zhao, L. Xiong, K.\nJayashree,\nJ. Li, F. Zhao, Z. Wang, S. Pranata,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Moreover,\nthe GASCA model produces\nreconstruction with",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "S. Shen, S. Yan, and J. Feng, “Dual-Agent GANs for Photorealistic and"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "very small errors and is able to generalize on unseen data.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Identity Preserving Proﬁle Face Synthesis,” Nips 2017, no. 15, pp. 1–11,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "The\nsuccess of\nthe pose\ninvariant models\nis\nin part due",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[17]\nJ. Chen, J. Konrad, and P. Ishwar, “VGAN-Based Image Representation"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "to ConvMLP layers, which\nlearn\nsalient\nfeatures\nand\nshift",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Learning for Privacy-Preserving Facial Expression Recognition,” tech."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "them as needed to reduce facial pose. HalfConv layers also",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "rep."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[18] D. H. Liu, K. M. Lam,\nand L. S. Shen,\n“Illumination invariant\nface"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "play an important\nrole as they reduce the number of\nlearning",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "recognition,” Pattern Recognition, vol. 38, pp. 1705–1716, oct 2005."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "parameters. HalfConv\nlayers were\ninspired\nby\nthe model",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[19] C. Tosik, A. Eleyan, and M. Salman, “Illumination invariant face recog-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "presented by [33], which splits\nthe\ninput\nimages\nin half\nto",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "nition system,”\nin 2013 21st Signal Processing and Communications"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Applications Conference, SIU 2013, pp. 1–4,\nIEEE, apr 2013."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "simplify feature learning.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[20] O. Gupta, D. Raviv,\nand R. Raskar,\n“Deep video gesture\nrecognition"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "To\nthe\nbest\nof\nthe\nauthors’\nknowledge,\nthis\nis\nthe ﬁrst",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "using illumination invariants,” Arxiv, pp. 1–9, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "approach that combines a greedy layer-wise training method",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[21] X.\nChen, X.\nLan, G.\nLiang,\nJ.\nLiu,\nand N.\nZheng,\n“Pose-and-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "illumination-invariant\nface representation via a triplet-loss trained deep"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "with\nadversarial\nlearning.\nThis\nis\nalso\nthe\nﬁrst\napproach",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Tools\nreconstruction model,” Multimedia\nand Applications,\nvol.\n76,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "to solely focus on pose\nand illumination invariant\nemotion",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "pp. 22043–22058, nov 2017."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "recognition. Future work will\nlook at exploiting the ability of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[22] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-PIE,” in"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2008 8th IEEE International Conference on Automatic Face & Gesture"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "our model\nto generate new data in order to deal with scenarios",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition, pp. 1–8,\nIEEE, sep 2008."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "where lack of multi-pose labeled exists.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[23] D. P. Kingma and J. L. Ba, “Adam,” pp. 1–15, 2015."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[24] W.\nShi,\nJ. Caballero,\nF. Huszar,\nJ. Totz, A.\nP. Aitken, R. Bishop,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "LIST OF REFERENCES",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "D. Rueckert, and Z. Wang, “Real-Time Single Image and Video Super-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Resolution Using an Efﬁcient Sub-Pixel Convolutional Neural Network,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[1] G. Levi and T. Hassner, “Emotion Recognition in the Wild via Convolu-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in 2016 IEEE Conference on Computer Vision and Pattern Recognition"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "tional Neural Networks and Mapped Binary Patterns,” in Proceedings of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "(CVPR), pp. 1874–1883, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the 2015 ACM on International Conference on Multimodal\nInteraction",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "¨\n[25] D. Lundqvist, A.\nFlykt,\nand A.\nOhman,\n“The Karolinska Directed"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "-\nICMI\n’15,\n(New York, New York, USA), pp. 503–510, ACM Press,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Emotional\nFaces\n- KDEF CD ROM from Department\nof Clinical"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Neuroscience, Psycology section,” Karolinska Institutet, pp. 3–5, 1998."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[2] A. Krizhevsky, L. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[26]\nP. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "with Deep Convolutional Neural Networks,” NIPS,\npp.\n1106–1114,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "“The Extended Cohn-Kanade Dataset\n(CK+): A complete dataset\nfor"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2012.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "action unit and emotion-speciﬁed expression,” tech.\nrep."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[3]\nS.\nIoffe\nand C.\nSzegedy,\n“Batch Normalization: Accelerating Deep",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[27] M. Lyons, S. Akamatsu, M. Kamachi,\nand J. Gyoba,\n“Coding facial"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Network Training by Reducing Internal Covariate Shift,” feb 2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "- 3rd IEEE Inter-\nexpressions with Gabor wavelets,”\nin Proceedings"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[4]\nP.\nVincent\nPASCALVINCENT\nand\nH.\nLarochelle\nLAROCHEH,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "national Conference on Automatic Face and Gesture Recognition, FG"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "“Stacked Denoising Autoencoders: Learning Useful Representations",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "1998, pp. 200–205, 1998."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "in a Deep Network with a Local Denoising Criterion Pierre-Antoine",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[28]\nF. Wallhoff, B. Schuller, M. Hawellek, and G. Rigoll, “Efﬁcient\nrecog-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Manzagol,” Journal of Machine Learning Research, vol. 11, pp. 3371–",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "nition of authentic dynamic facial expressions on the feedtum database,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "3408, 2010.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "in 2006 IEEE International Conference on Multimedia and Expo, ICME"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[5] A. Ruiz-Garcia, V. Palade, M. Elshaw, and I. Almakky, “Deep Learning",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "2006 - Proceedings, vol. 2006, pp. 493–496,\nIEEE,\njul 2006."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "for\nIllumination Invariant Facial Expression Recognition,” in Proceed-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[29] A. Ruiz-Garcia, N. Webb, V. Palade, M. Eastwood,\nand M. Elshaw,"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ings of\nthe International Joint Conference on Neural Networks, (Rio de",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "“Deep Learning for Real Time Facial Expression Recognition in Social"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Janeiro),\nIEEE, 2018.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "of\nthe\nInternational Conference\non Neural\nRobots,”\nin Proceedings"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[6]\nI. Goodfellow, Bengio, Yoshua, and A. Courville, Deep Learning. MIT",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Information Processing, 2018."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Press, 2016.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[30] K. He, X. Zhang, S. Ren,\nand J. Sun,\n“Deep Residual Learning for"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[7] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy Layer-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Image Recognition,” Arxiv.Org, vol. 7, pp. 171–180, dec 2015."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Wise Training of Deep Networks,” tech.\nrep.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[31] N. M. Hewahi and A. R. M. Baraka, “Impact of Ethnic Group on Human"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[8] A. Ruiz-Garcia, M. Elshaw, A. Altahhan,\nand V.\nPalade,\n“Stacked",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Emotion Recognition Using Backpropagation Neural Network,” BRAIN."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "deep convolutional\nauto-encoders\nfor\nemotion recognition from facial",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Broad Research in Artiﬁcial, pp. 20–27, 2012."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "the International Joint Conference on\nexpressions,” in Proceedings of",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[32] M. Kan, S. Shan, H. Chang, and X. Chen, “Stacked progressive auto-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Neural Networks, vol. 2017-May, pp. 1586–1593,\nIEEE, may 2017.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "encoders (SPAE)\nfor\nface recognition across poses,” in Proceedings of"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[9] A. Makhzani, J. Shlens, N. Jaitly,\nI. Goodfellow, and B. Frey, “Adver-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "the IEEE Computer Society Conference on Computer Vision and Pattern"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "sarial Autoencoders,” 2015.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition, pp. 1883–1890,\nIEEE,\njun 2014."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[10]\nI. Goodfellow, J. Pouget-Abadie, M. M. A.\nin neural\n. . . , and U. 2014,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "[33] A. Ruiz-Garcia, M. Elshaw, A. Altahhan,\nand V. Palade,\n“Emotion"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "“Generative adversarial nets,” Advances in Neural Information Process-",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Recognition Using Facial Expression Images for a Robotic Companion,”"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "ing Systems 27, pp. 2672–2680, 2014.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "of Neural Networks:\n17th\nInternational\nin Engineering Applications"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "[11]\nS. A. Israel, J. Goldstein, J. S. Klein, J. Talamonti, F. Tanner, S. Zabel,",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "Conference, EANN 2016, Aberdeen, UK, September 2-5, 2016, Proceed-"
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "P. A. Sallee,\nand L. McCoy,\n“Generative Adversarial Networks\nfor",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": "ings, pp. 79–93, Springer, Cham, 2016."
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "2017\nIEEE Applied\nImagery Pattern Recognition\nClassiﬁcation,”\nin",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        },
        {
          "image containing a face, with an estimate pose ϕ,\nto a hidden": "Workshop (AIPR), pp. 1–4,\nIEEE, oct 2017.",
          "[12] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Pantazopou-": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Recognition in the Wild via Convolutional Neural Networks and Mapped Binary Patterns",
      "authors": [
        "G Levi",
        "T Hassner"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction -ICMI '15"
    },
    {
      "citation_id": "2",
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "authors": [
        "A Krizhevsky",
        "L Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "NIPS"
    },
    {
      "citation_id": "3",
      "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
    },
    {
      "citation_id": "4",
      "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion Pierre-Antoine Manzagol",
      "authors": [
        "P Vincent",
        "H Larochelle"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "5",
      "title": "Deep Learning for Illumination Invariant Facial Expression Recognition",
      "authors": [
        "A Ruiz-Garcia",
        "V Palade",
        "M Elshaw",
        "I Almakky"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "I Goodfellow",
        "Bengio",
        "A Yoshua",
        "Deep Courville",
        "Learning"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Greedy Layer-Wise Training of Deep Networks",
      "authors": [
        "Y Bengio",
        "P Lamblin",
        "D Popovici",
        "H Larochelle"
      ],
      "venue": "Greedy Layer-Wise Training of Deep Networks"
    },
    {
      "citation_id": "8",
      "title": "Stacked deep convolutional auto-encoders for emotion recognition from facial expressions",
      "authors": [
        "A Ruiz-Garcia",
        "M Elshaw",
        "A Altahhan",
        "V Palade"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Adversarial Autoencoders",
      "authors": [
        "A Makhzani",
        "J Shlens",
        "N Jaitly",
        "I Goodfellow",
        "B Frey"
      ],
      "year": "2015",
      "venue": "Adversarial Autoencoders"
    },
    {
      "citation_id": "10",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Generative Adversarial Networks for Classification",
      "authors": [
        "S Israel",
        "J Goldstein",
        "J Klein",
        "J Talamonti",
        "F Tanner",
        "S Zabel",
        "P Sallee",
        "L Mccoy"
      ],
      "year": "2017",
      "venue": "2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)"
    },
    {
      "citation_id": "12",
      "title": "Data Augmentation using GANs for Speech Emotion Recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data Augmentation using GANs for Speech Emotion Recognition"
    },
    {
      "citation_id": "13",
      "title": "Data Augmentation Using Conditional GANs for Facial Emotion Recognition",
      "authors": [
        "W Yi",
        "Y Sun",
        "S He"
      ],
      "year": "2018",
      "venue": "Progress in Electromagnetics Research Symposium"
    },
    {
      "citation_id": "14",
      "title": "Data Augmentation in Emotion Classification Using Generative Adversarial Networks",
      "authors": [
        "X Zhu",
        "Y Liu",
        "Z Qin",
        "J Li"
      ],
      "year": "2017",
      "venue": "Data Augmentation in Emotion Classification Using Generative Adversarial Networks"
    },
    {
      "citation_id": "15",
      "title": "Disentangled representation learning GAN for pose-invariant face recognition",
      "authors": [
        "L Tran",
        "X Yin",
        "X Liu"
      ],
      "year": "2017",
      "venue": "Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017"
    },
    {
      "citation_id": "16",
      "title": "Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis",
      "authors": [
        "J Zhao",
        "L Xiong",
        "K Jayashree",
        "J Li",
        "F Zhao",
        "Z Wang",
        "S Pranata",
        "S Shen",
        "S Yan",
        "J Feng"
      ],
      "year": "2017",
      "venue": "Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis"
    },
    {
      "citation_id": "17",
      "title": "VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition",
      "authors": [
        "J Chen",
        "J Konrad",
        "P Ishwar"
      ],
      "venue": "VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition"
    },
    {
      "citation_id": "18",
      "title": "Illumination invariant face recognition",
      "authors": [
        "D Liu",
        "K Lam",
        "L Shen"
      ],
      "year": "2005",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Illumination invariant face recognition system",
      "authors": [
        "C Tosik",
        "A Eleyan",
        "M Salman"
      ],
      "year": "2013",
      "venue": "2013 21st Signal Processing and Communications Applications Conference"
    },
    {
      "citation_id": "20",
      "title": "Deep video gesture recognition using illumination invariants",
      "authors": [
        "O Gupta",
        "D Raviv",
        "R Raskar"
      ],
      "year": "2016",
      "venue": "Arxiv"
    },
    {
      "citation_id": "21",
      "title": "Pose-andillumination-invariant face representation via a triplet-loss trained deep reconstruction model",
      "authors": [
        "X Chen",
        "X Lan",
        "G Liang",
        "J Liu",
        "N Zheng"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "22",
      "title": "Multi-PIE",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2008",
      "venue": "2008 8th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
      "authors": [
        "W Shi",
        "J Caballero",
        "F Huszar",
        "J Totz",
        "A Aitken",
        "R Bishop",
        "D Rueckert",
        "Z Wang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "The Karolinska Directed Emotional Faces -KDEF CD ROM from Department of Clinical Neuroscience, Psycology section",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "Karolinska Institutet"
    },
    {
      "citation_id": "26",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "venue": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression"
    },
    {
      "citation_id": "27",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings -3rd IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "28",
      "title": "Efficient recognition of authentic dynamic facial expressions on the feedtum database",
      "authors": [
        "F Wallhoff",
        "B Schuller",
        "M Hawellek",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Multimedia and Expo, ICME 2006 -Proceedings"
    },
    {
      "citation_id": "29",
      "title": "Deep Learning for Real Time Facial Expression Recognition in Social Robots",
      "authors": [
        "A Ruiz-Garcia",
        "N Webb",
        "V Palade",
        "M Eastwood",
        "M Elshaw"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Neural Information Processing"
    },
    {
      "citation_id": "30",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Arxiv.Org"
    },
    {
      "citation_id": "31",
      "title": "Impact of Ethnic Group on Human Emotion Recognition Using Backpropagation Neural Network",
      "authors": [
        "N Hewahi",
        "A Baraka"
      ],
      "year": "2012",
      "venue": "BRAIN. Broad Research in Artificial"
    },
    {
      "citation_id": "32",
      "title": "Stacked progressive autoencoders (SPAE) for face recognition across poses",
      "authors": [
        "M Kan",
        "S Shan",
        "H Chang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Emotion Recognition Using Facial Expression Images for a Robotic Companion",
      "authors": [
        "A Ruiz-Garcia",
        "M Elshaw",
        "A Altahhan",
        "V Palade"
      ],
      "year": "2016",
      "venue": "Engineering Applications of Neural Networks: 17th International Conference"
    }
  ]
}