{
  "paper_id": "2405.13924v3",
  "title": "Narrative Review Of Emotional Expression Support In Xr: Psychophysiology Of Speech-To-Text Interfaces",
  "published": "2024-05-22T18:53:27Z",
  "authors": [
    "Sunday David Ubur",
    "Denis Gracanin"
  ],
  "keywords": [
    "Emotional expression",
    "Psychophysiology",
    "Speech-to-text",
    "Empathic machine",
    "Emotion-aware interfaces",
    "Accessible communication",
    "AR/VR captioning",
    "Multimodal interaction",
    "Human-computer empathy",
    "Neurodiversity",
    "DHH technology"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This narrative review examines recent advancements, limitations, and research gaps in integrating emotional expression into speech-to-text (STT) interfaces within extended reality (XR) environments. Drawing from 37 peer-reviewed studies published between 2020 and 2024, we synthesized literature across multiple domains-including affective computing, psychophysiology, captioning innovation, and immersive human-computer interaction. Thematic categories include communication enhancement technologies for Deaf and Hard of Hearing (DHH) users, emotive captioning strategies, visual and affective augmentation in AR/VR, speech emotion recognition, and the development of empathic systems. Despite the growing accessibility of real-time STT tools, such systems largely fail to convey affective nuance, limiting the richness of communication for DHH users and other caption consumers. This review highlights emerging approaches such as animated captions, emojilization, color-coded overlays, and avatar-based emotion visualization, but finds a persistent gap in real-time emotion-aware captioning within immersive XR contexts. We identify key research opportunities at the intersection of accessibility, XR, and emotional expression, and propose future directions for the development of affect-responsive, user-centered captioning interfaces.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the ever-evolving landscape of communication technologies, Speech-to-Text (STT) interfaces play a pivotal role in enhancing accessibility and inclusivity, particularly for individuals with hearing impairments. The ability to convert spoken language into text facilitates communication in various contexts, ranging from online meetings to educational settings. However, amidst the strides made in improving accessibility, a critical aspect often overlooked is the incorporation of emotional expression into transcribed text generated by STT systems. Additionally, the review introduces a new thematic synthesis on visual and affective augmentation in captioning, highlighting emerging methods that combine real-time emotional recognition with user-centered visual design in immersive environments.\n\nThis study embarks on a comprehensive exploration of the existing literature surrounding emotional expression in STT interfaces, with a specific emphasis on the psychophysiology aspect within Extended Reality (XR). The objective is to identify advancements, limitations, and research gaps in the incorporation of emotional expression in transcribed text generated by STT systems. As communication technologies continue to advance, the ability to convey not only the semantic content but also the emotional nuances of speech becomes paramount for fostering richer and more meaningful interactions.\n\nThe ubiquity of STT applications, exemplified by tools like Live Transcribe, has significantly contributed to breaking communication barriers for the Deaf and Hard of Hearing (DHH) community. These applications offer real-time transcriptions that are invaluable in various scenarios, from professional communication to educational contexts. However, the inherent challenge lies in the loss of emotional nuance during the transcription process, posing a communication hurdle that this study seeks to address.\n\nTo unravel the complexities surrounding emotional expression in STT interfaces, our investigation spans various dimensions. From examining innovations in live transcription and closed captioning to delving into advancements in augmented reality (AR), emotive captioning, emotion recognition, and empathic machines, This section provides the narrative review of the literature, with study limitations and comparisions among related studies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Communication Enhancement Technologies For Dhh Individuals",
      "text": "The exploration of communication enhancement technologies for DHH individuals has seen significant strides, particularly with the non-traditional applications of Live Transcribe, an automatic speech recognition (ASR) application  [3] . This tool proves invaluable in various scenarios, from technical support and communication with colleagues to note-taking during meetings, especially during the prevalence of online meetings amid COVID-19 lockdowns. The adaptability of Live Transcribe for immobile DHH individuals by mounting the tablet on a boom mic stand is a notable advancement  [4] , emphasizing its pivotal role in promoting inclusivity and accessibility for this demographic.\n\nClosed captioning is a technology that has seen minimal innovation since its inception in the 1970s, however recent studies are shedding light on potential enhancements. One such is the animated text in captions  [5]  which offers a promising avenue, providing improved access to emotive information often overlooked in traditional captions, such as music, sound effects, and intonation. This innovation, preferred by both hard of hearing and hearing participants, emphasizes the need to bridge the gap in conveying nonverbal nuances. This study and other related works was inspired by  [6]  which delved into the impact of captions on the affective reactions of hearing-impaired children to television programming, revealing the potential of captions in enhancing emotional perception.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Innovations In Captioning For Enhanced Emotional Communication",
      "text": "Shifting focus to innovations in captioning for enhanced emotional communication, Rashid's framework  [7]  introduces the concept of using animation and standard properties to express basic emotions. This framework, associating emotions with animation properties, establishes a consistent method for applying animation to text captions. Similarly, to enhance emotions in captions  [8]  incorporating graphics, color, and animation to illustrate sound and emotive information in television and film, a user study compared viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive responses, particularly among hard of hearing viewers.\n\nIn the realm of Voice User Interfaces (VUI), Hu's paper  [9]  highlights the lack of emotional information in STT by proposing an emojilization tool. This tool automatically attaches emojis to generated text, compensating for emotional loss in the conversion process. The pilot study indicates that emojilized text enhances perceived emotions compared to plain text.\n\nWe can also understand how to represent emotions in captions by analyzing how emotions are expressed in text. A study along this direction is the representation of emotions in text-based messaging applications  [10]  through visualizations in facial expression recognition in WhatsApp Web, and findings underscore users' preference for maintaining control over their emotions in private settings, emphasizing the need for exclusive presentation and sharing of emotions in certain contexts. An important application of emotional expressions in text and imagery on social media during the COVID-19 pandemic  [11]  adds a temporal dimension to the narrative, shedding light on how emotions are expressed in different modalities during challenging times.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition In Ar And Vr",
      "text": "VR has gained attention in various fields due to its unique advantages in manipulating perceived scenarios and providing controlled experiences  [12] . It has been found that VR can generate emotional reactions and experiences in users, as demonstrated by studies on 360 degree videos and VR educational tools  [13] . In fact, emotional responses in VR games have been observed to be more intense compared to desktop games, both psychologically and physiologically  [14] . This suggests that VR has the potential to enhance positive emotions, and it is worth to study if similar outcome can be experienced from STT in an AR or VR environment.\n\nMore studies have discussed advancements in AR technologies for DHH individuals, particularly in the classroom setting  [15] . The AR visual-captions system, implemented using Unity, ARKit, and AR Foundation, aims to provide real-time STT and visual elements around the teacher, creating an immersive learning experience. The study not only introduced the prototype but also outlined future research directions, especially on refining the design, conducting user studies, and extending the system's capabilities for group conversations. Similarly, Real-time AR visual-captions  [16]  for DHH children in classrooms builds on the AR-based system, providing a real-time solution for STT and keyword extraction, addressing the challenges faced by DHH children in mainstream education.\n\nIn higher education application, Pirker's exploration into the potential of VR for computer science education  [17]  sheds light on the positive impact of VR environments on learning outcomes. The user study reveals higher engagement, immersion, and positive emotions in the VR application compared to a web-based alternative, emphasizing VR's potential to enhance computer science education. Given the importance of AR and VR in education training, making the virtual environment convey emotional expression is essential. Chen's paper  [18]  focuses on facial expression recognition within immersive environments. The proposed solution involves collecting real facial expression data using an infrared camera and light source, showcasing promising results for understanding human emotions in VR contexts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual And Affective Augmentation In Xr",
      "text": "As immersive AR and VR environments continue to gain traction in accessibility research, recent studies have emphasized the importance of incorporating visual and affective augmentations into captioning systems. These augmentations not only support emotional expression but also reduce cognitive strain and improve comprehension, particularly for DHH users and others who rely on captions in complex, real-time scenarios. A conceptual workflow of this integration is illustrated in Figure  2 , showing how STT outputs can be enhanced with emotional cues before delivery in XR displays.\n\nRecent research highlights the cognitive demands of prolonged exposure to captions within VR HMDs, including increased eye strain and stress when captions are not personalized or well-positioned  [19] . Participants expressed preferences for adjustable caption features such as text size, avatar use, and caption placement, underscoring the need for personalization in immersive captioning interfaces.\n\nTo support the design of emotionally expressive captions, a framework and accompanying interactive tool were developed to help designers visualize emotions in AR using abstract or representational visual elements  [20] . The framework guides decisions related to shape, animation, and spatial configuration of affective visuals, enabling real-time augmentation of emotional content in communication.\n\nA related framework for AR-based affect visualization  [21]  builds on the dimensional model of emotion and focuses on ambiguous visual representations such as gradients, forms, and motion patterns. The study emphasizes that ambiguous, user-interpretable visuals can enhance emotional communication and user engagement without overwhelming or over-simplifying complex emotional states.\n\nIn shared VR environments, bi-directional emotion-sharing mechanisms have been shown to improve interpersonal trust, decision-making, and collaborative performance  [22] . Visualizing emotions through simplified indicators such as valence icons contributed to more frequent emotional consensus and helped build rapport between unfamiliar collaborators, pointing to the potential of similar strategies in educational and captioning systems.\n\nColor also plays a central role in emotional communication. One study explored mapping vocal emotional states to colored message bubbles  [23] , showing that users perceived emotional intensity more clearly through contextual color cues. However, negative emotional states sometimes reduced user comfort, highlighting the importance of careful calibration when applying visual augmentation to speech-based captions.\n\nThese findings are directly relevant to the development of accessible XR for education. A user-centric investigation into educational captioning for DHH individuals recommends embedding emotional context into captions through visual highlights, emojis, and keyword emphasis  [24] . Such enhancements are not only beneficial for comprehension and engagement but also critical for reducing the sense of isolation and cognitive load associated with conventional captioning.\n\nCollectively, these studies demonstrate that visual and affective augmentation is a necessary evolution in XR-based captioning. When combined with emotion recognition technologies and user-centered design, these strategies can transform how captions communicate meaning-enhancing not only what is said, but how it is felt.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition: Techniques And Challenges",
      "text": "An important technique found in the literature is the use of color coding and visualization of emotion in speech, which offer a unique perspective on conveying emotional information.  [25]  utilizes haptic feedback vests and immersive VR to quantify human emotion, providing insights for the development of emotionally relevant input devices. The use of bubble coloring in chat platforms  [26, 27]  explores the visual representation of speech emotion, considering color alterations and participant feedback for conveying specific emotions effectively. Other techniques in machine learning are being employed to include emotional recognition in text. One is Human-robot interaction and perception of emotions  [28] , demonstrating the effectiveness of animated text associated with emotional information in enhancing user interactions. The system's potential applications in healthcare and education underscore its versatile impact.\n\nFurther, Schiano's paper  [29]  delves into the perception of facial affect, bridging the gap between human facial expressions and prototype robot faces. The study informs the design of affective robot faces, contributing valuable insights into how emotions can be effectively communicated through facial expressions.In a related study,the analysis of humanoid avatar representations and emotions  [30]  delves into the uncanniness factor, highlighting the importance of selecting appropriate avatar types for accurate expression communication. The study's findings support the need for avatar representations that effectively convey emotional cues in interactive systems. Finally, gestures can also be generated directly from speech using GANs  [31]  thereby opening up new possibilities in human-computer interaction. The user study using Turing test-inspired evaluation demonstrates the potential of the proposed technique in creating realistic gestures from speech. In conclusion, the narrative literature review traverses a spectrum of communication enhancement technologies, showcasing the evolution of tools and techniques to address the unique needs of DHH individuals. From innovations in live transcription and closed captioning to advancements in AR, emotive captioning, and speech emotion recognition, the literature reflects a dynamic landscape of research and development. The exploration of humanoid avatars, voice user interfaces, VR, human-robot interaction, and the color coding of speech emotions further emphasizes the interdisciplinary nature of this field. The review also underscores the significance of understanding emotional expressions in text, images, and gestures, offering a holistic perspective on communication technologies that cater to diverse sensory and cognitive needs. As we navigate the intricacies of enhancing communication for individuals with hearing impairments, these technological advancements pave the way for a more inclusive and accessible future.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Empathic Machine",
      "text": "Emotions serve as an implicit communication channel among humans, conveyed through spoken words, facial expressions, behavior, and physiological responses. This empathic form of communication enables individuals to recognize cues and respond empathetically. Despite computing devices excelling in understanding user context, there are ongoing challenges in the scientific community to create emotion-sensing and empathic applications for human-computer interaction  [32] . In this section we look at the literature contributions in tackling these challenges and support empathic ability in machines.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Enhancing Empathic Abilities With Chatbots:",
      "text": "The evolution of chatbots began with the creation of ELIZA in 1966  [33] , and today they are commonly used in customer service but have limitations in showing empathy. Empathy, defined as understanding and sharing another's feelings, is deemed challenging for conversational agents. Recent research, however, indicates the feasibility of generating empathic responses in chatbots, particularly in customer service contexts, including in the healthcare, especially in providing physical health diagnosis through short text conversations.\n\nSeveral studies available in the literature to compensate for the shortcomings of lack of empathic expressions in computer and AI employed the use of chatbots to convey emptional expressions. One study used chatbots to mediate social presence and trust in consumer emotions  [34] , creating a sense of social presence using emoticons, appropriate language styles, and other cues that contribute to a more authentic and human-like interaction. Another study used empathic chatbot to understand users' emotional states and generate responses that convey understanding and addressing challenges in handling multi-layered, contextsensitive, and implicitly expressed emotions in text  [35] . It utilized advanced models and tools involving a benchmark bot and an empathic bot. To enhance empathic capabilities, they fine-tuned the language model on empathic conversations, incorporating the user's emotional state into the input. The findings from the empathic bot include the effectiveness of transformer-based language models, the positive impact of training on empathic conversations, and the influence of emotional valence on perceived empathy.\n\nSimilarly, chatbot application is finding its way into gender voice discrimination.  [36]  explores the tension between designing empathic agents and the gender assignment of chatbots and how they can relate to the design of the metaphor of the chatbots in conversational agents (Cas). Computers lack genuine emotions, however CAs can simulate and trigger empathy by adhering to human-social rules during interactions. This simulation involves techniques like sentiment analysis, emotion detection, and mimicking emotions to enhance user engagement. Notably, the paper explores whether CAs, specifically those perceived as having feminine qualities, evoke more empathy in human-chatbot interactions compared to other gender perceptions.\n\nDigitizing Human Emotions: As computing machines and AI need to be programmed to work, inculcating empathy into machines is through digitizing human emotions. One study used non-invasive techniques like electroencephalography (EEG), specifically the Emotive Epoc headset to digitize human emotions  [37]  and the primary goal was to conduct a proof-of-concept experiment enabling a humanoid robot's control through digitized emotions, emphasizing potential applications in healthcare . Their contribution lies in adapting mature image recognition tools from Artificial Neural Networks (ANNs) for emotion recognition, and the resulting Brain-Computer Interface (BCI) system is designed to enable the robot to emulate empathy and interact with subjects based on predefined behavioral models. Similarly,  [38]  explores the significance of recognizing emotions in face-to-face conversations and discussed the limitations of traditional cues like facial expressions and gestures, emphasizing the potential of analyzing brain activity and physiological signals for more reliable emotion recognition, especially in situations where emotions may be concealed. The study presents an experimental setup for inducing spontaneous emotions in conversations and creating a dataset incorporating EEG, Photoplethysmography (PPG), and Galvanic Skin Response (GSR) signals. The researchers developed an intelligent user interface for video conferencing and systems with conversational digital humans capable of recognizing and responding to emotions, using PEGCONV) dataset to train these systems to enhance their ability to understand and appropriately respond to human behavior.\n\nFurther, Robots can also respond to user emotional states.  [39]  enhanced human-robot interactions (HRI) by proposing an Automatic Cognitive Empathy Model (ACEM) for humanoid robots. The goal was to achieve more extended and engaging interactions by having robots respond appropriately to users' emotional states using ACEM model to continuously detects users' affective states based on facial expressions, utilizing a stacked autoencoder network trained on the RAVDESS dataset. The model generates empathic behaviors adapted to users' personalities, either in parallel or as reactive responses.\n\nEmpathic Systems in AR/VR: Empathic machine is also finding applications into wearable. In exploring the use of smart glasses equipped with an emotion recognition system to enhance human-to-human communication, with a focus on doctors and autistic adults  [40] , this study identifies the potential benefits for doctors in improving patient-doctor communication and for autistic adults facing challenges in adhering to neurotypical communication norms. The proposed system combines emotion recognition, AI, and smart glasses to provide real-time feedback on the emotional state of conversation partners. User evaluations indicate positive responses from both doctors and autistic adults, highlighting the potential for improving empathy and communication. Design considerations include customizable output preferences and addressing privacy and social impact concerns. The authors emphasize the system's potential for educational purposes and ongoing development in the context of social impact and user experience. In a related study,  [41]  explores the use of VR with wearable physiological sensors to examine how recalling autobiographical memories (AM) with emotional content affects an individual's physiological state. By replicating the Autobiographical Memory Test (AMT) in VR, participants were presented with positive, negative, and neutral words to trigger memory recall. The study observed a positive influence of AM recall on electrodermal activity (EDA) peak amplitude, EDA peak number, and pupil diameter compared to situations without recall. However, emotional AM recall did not produce a significant impact. The article concludes by discussing the potential and limitations of utilizing autobiographical memory to enhance personalized mobile VR experiences in conjunction with physiological sensors. The findings reveal a significant effect of AM recall on EDA mean peak amplitude, EDA peak number, and pupil diameter compared to a no-recall condition. However, no significant differences were identified in emotional AM recall (positive, negative, neutral).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "This review identifies key advancements, limitations, and research gaps in the integration of emotional expression into speech-to-text (STT) interfaces, particularly within immersive environments such as virtual and augmented reality (VR/AR). While STT technologies have significantly improved accessibility-especially for DHH users-they typically fail to convey affective cues, resulting in reduced expressiveness and diminished communication quality  [6] . Motivated by this persistent limitation in traditional captioning systems, our review synthesizes current literature to assess the state of research and inform future directions for more emotionally resonant captioning systems.\n\nThrough a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we examined developments across communication enhancement technologies, captioning innovation, emotion recognition in XR, and empathic human-computer systems. Our thematic synthesis included a focused review of visual and affective augmentation strategies, which show promise for conveying emotion through ambient cues and interactive design elements  [20, 19, 23] . However, these strategies are largely decoupled from real-time STT outputs, underscoring a significant gap in current systems: the absence of emotionally expressive, real-time captions in immersive environments.\n\nDespite growing interest in affective computing and XR accessibility, few studies directly address the integration of emotion into STT interfaces within AR/VR contexts. Current approaches focus primarily on visual enhancements or emotion-aware avatars, without embedding affective cues directly into the transcribed text  [9, 5] . This limitation presents a critical opportunity for innovation-particularly in education and training scenarios where emotional tone can support comprehension, reduce cognitive load, and increase learner engagement  [17, 18] .\n\nResearch into captioning innovations such as Rashid's animated text framework  [7, 5]  and Hu's emojization tool  [9]  exemplifies early efforts to embed emotion into transcription. These methods-whether through animated captions, color cues, or emoji augmentations-have shown positive responses from DHH users  [8] , and highlight the potential for more expressive, inclusive STT systems. Yet, scalability and vocabulary limitations remain challenges, especially for deployment in real-time XR applications.\n\nExtending these innovations into immersive platforms offers new possibilities. Recent AR and VR captioning prototypes demonstrate technical feasibility for real-time STT with spatial awareness and keyword extraction  [16, 15] . However, few efforts have integrated emotional nuance into these systems. In educational settings, where XR-based captioning is increasingly applied, the inclusion of affective information could enhance both accessibility and engagement  [17, 18] . Research on presence and affective response in VR environments further suggests that immersive platforms are well-suited for emotionally resonant experiences  [12] .\n\nBeyond captioning, affective computing in adjacent domains-such as healthcare, robotics, and humanagent interaction-provides valuable insights. Studies show that emotionally responsive systems, powered by machine learning, facial recognition, and physiological sensing, can enhance empathy, personalization, and user satisfaction  [28, 27, 30] . Applications in education and clinical settings demonstrate that recognizing and responding to user emotion improves outcomes and fosters trust  [29] . The digitization of human emotion, including efforts using brain-computer interfaces (BCIs) and affective robotics, represents a broader trend toward emotionally intelligent technology. BCI-driven systems that interpret EEG or physiological signals offer promising methods for capturing emotional states  [37, 38] , though their integration with STT remains underexplored. Similarly, empathy models like ACEM for humanoid robots illustrate how emotional awareness can be incorporated into interaction design to enable adaptive and human-like behavior  [39] .\n\nWearable technologies, such as smart glasses with built-in emotion recognition, have also shown promise in improving communication among autistic individuals and between doctors and patients  [40] . These tools deliver real-time emotional feedback to enhance interpersonal understanding, and their integration into educational contexts could support more personalized and empathetic learning experiences. Multisensory XR systems that combine physiological data, autobiographical memory, and real-time feedback offer new frontiers for emotionally responsive interaction. Studies exploring such experiences emphasize the potential for tailoring virtual environments to individual affective states  [41] , providing valuable frameworks for the design of emotionally expressive captioning systems.\n\nOverall, the findings of this review suggest a compelling case for interdisciplinary research that bridges affective computing, psychophysiology, XR design, and accessibility. The future of inclusive STT systems lies in integrating emotional expression not as a secondary visual layer, but as an intrinsic component of the transcription process-transforming captions from neutral information displays into emotionally meaningful communication tools.\n\nIn sum, advancing emotional expression in STT systems holds particular promise for enhancing communication accessibility for DHH users. By embedding emotion-aware cues into real-time captions, especially within XR environments, designers and researchers can move toward more human-centered, empathetic technologies. This work underscores the importance of inclusivity not only in functionality but in emotional fidelity-ensuring that all users, regardless of hearing ability, can access the full depth of spoken communication.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Limitations",
      "text": "The limitations of this study were: (i) it was a rapid review whose purpose is to survey existing work in emotional expressions, find existing gaps and inspire further research. (ii) the narrative review did not follow a strict rule expected of systematic literature review, hence could have missed some relevant papers. However, the papers that were found eligible and used in this review are the most relevant. (iii) the review is limited to peer-reviewed papers that were in English.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This narrative review explored the integration of emotional expression into STT interfaces within immersive XR environments, emphasizing the intersection of psychophysiology, accessibility, and user-centered design. While STT technologies have significantly advanced accessibility for DHH users, the omission of emotional sues in transcribed speech remains a critical barrier to natural and expressive communication.\n\nOur findings highlight a growing recognition in HCI of the need for emotion-aware captioning, with emerging strategies such as emotion-driven avatars, color-coded overlays, and ambient cues offering promising-but largely disconnected-approaches. A key research gap lies in the lack of direct integration between real-time STT outputs and affective visualization techniques.\n\nAddressing this gap requires interdisciplinary collaboration across affective computing, neuroscience, and XR systems design. Future research should investigate the effectiveness of real-time emotional augmentation-such as emojis, avatars, and dynamic text formatting-in reducing cognitive load and enhancing comprehension in AR/VR settings. This review highlights the urgent need to move beyond neutral captioning toward systems that can capture and convey the emotional context of speech.\n\nBy bridging accessibility with emotional expressiveness, the next generation of STT systems can support more inclusive, empathetic, and engaging communication-paving the way for richer user experiences in immersive environments and beyond.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Thematic map summarizing the key areas reviewed in this study, including communication en-",
      "page": 3
    },
    {
      "caption": "Figure 2: , showing how STT outputs can be",
      "page": 4
    },
    {
      "caption": "Figure 2: Workflow of an emotion-aware STT system in XR environments. Speech is converted to text,",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Narrative Review of Emotional Expression Support in XR:": "Psychophysiology of Speech-to-Text Interfaces"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "Sunday D. Ubur, Denis Gracanin"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "Abstract"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "This narrative review examines\nrecent advancements,\nlimitations, and research gaps\nin integrating"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "emotional expression into speech-to-text\n(STT)\ninterfaces within extended reality (XR) environments."
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "Drawing from 37 peer-reviewed studies published between 2020 and 2024, we synthesized literature across"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "multiple domains—including affective computing, psychophysiology, captioning innovation, and immer-"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "sive human-computer interaction. Thematic categories include communication enhancement technologies"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "for Deaf and Hard of Hearing (DHH) users, emotive captioning strategies, visual and affective augmen-"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "tation in AR/VR, speech emotion recognition, and the development of empathic systems. Despite the"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "growing accessibility of real-time STT tools, such systems largely fail to convey affective nuance,\nlimit-"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "ing the richness of communication for DHH users and other caption consumers. This review highlights"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "emerging approaches\nsuch as animated captions, emojilization, color-coded overlays, and avatar-based"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "emotion visualization, but finds a persistent gap in real-time emotion-aware captioning within immersive"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "XR contexts. We identify key research opportunities at the intersection of accessibility, XR, and emo-"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "tional expression, and propose future directions for the development of affect-responsive, user-centered"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "captioning interfaces."
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "Keywords: Emotional expression, Psychophysiology, Speech-to-text, Empathic machine, Emotion-aware"
        },
        {
          "Narrative Review of Emotional Expression Support in XR:": "interfaces, Accessible communication, AR/VR captioning, Multimodal\ninteraction, Human-computer empa-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "exploration extends\ninto the realms of VR and AR, where unique opportunities arise to create immersive"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "and emotionally resonant experiences, especially in the context of education and training.\nIn the subsequent"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "sections, we detail the methodology employed in conducting the literature review, present the background"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "encompassing communication enhancement technologies, and delve into innovations in captioning, emotion"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "recognition, and empathic systems. The synthesis of these findings not only identifies current advancements"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "but also points towards avenues for future research, emphasizing the ongoing need for inclusive and accessible"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "communication technologies."
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "This paper contributes:\n(1) A narrative synthesis of 37 studies from 2020–2024; (2) A categorization of"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "emotion integration methods in STT; (3) A synthesized review of visual and affective augmentation strategies"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "in immersive captioning; (4) Identification of key research gaps in VR/AR-based emotional captioning; (5)"
        },
        {
          "this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the": "Proposed research questions for future empirical\ninvestigation."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "hancement technologies,\ninnovations in captioning, emotion recognition in XR, empathic machine interfaces,"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "and emerging strategies in visual and affective augmentation."
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "This section provides the narrative review of the literature, with study limitations and comparisions among"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "related studies."
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "3.1\nCommunication Enhancement Technologies for DHH Individuals"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "The exploration of communication enhancement technologies for DHH individuals has seen significant strides,"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "particularly with the non-traditional applications of Live Transcribe, an automatic speech recognition (ASR)"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "application [3]. This tool proves invaluable in various scenarios,\nfrom technical support and communication"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "with colleagues\nto note-taking during meetings,\nespecially during the prevalence of online meetings amid"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "COVID-19 lockdowns. The adaptability of Live Transcribe for immobile DHH individuals by mounting the"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "tablet on a boom mic stand is a notable advancement [4], emphasizing its pivotal role in promoting inclusivity"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "and accessibility for this demographic."
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "Closed captioning is a technology that has seen minimal\ninnovation since its inception in the 1970s, how-"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "ever recent studies are shedding light on potential enhancements. One such is the animated text in captions"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "[5] which offers a promising avenue, providing improved access to emotive information often overlooked in"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "traditional captions, such as music, sound effects, and intonation. This innovation, preferred by both hard"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "of hearing and hearing participants, emphasizes the need to bridge the gap in conveying nonverbal nuances."
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "This study and other related works was inspired by [6] which delved into the impact of captions on the af-"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "fective reactions of hearing-impaired children to television programming, revealing the potential of captions"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "in enhancing emotional perception."
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "3.2\nInnovations in Captioning for Enhanced Emotional Communication"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "Shifting focus\nto innovations\nin captioning for\nenhanced emotional\ncommunication, Rashid’s\nframework"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "[7]\nintroduces\nthe\nconcept of using animation and standard properties\nto express basic\nemotions.\nThis"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "framework, associating emotions with animation properties,\nestablishes a consistent method for applying"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "animation to text captions.\nSimilarly,\nto enhance emotions\nin captions\n[8]\nincorporating graphics,\ncolor,"
        },
        {
          "Figure 1: Thematic map summarizing the key areas\nreviewed in this\nstudy,\nincluding communication en-": "and animation to illustrate sound and emotive information in television and film, a user\nstudy compared"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "responses, particularly among hard of hearing viewers."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "In the realm of Voice User Interfaces (VUI), Hu’s paper [9] highlights the lack of emotional\ninformation"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "in STT by proposing an emojilization tool.\nThis\ntool automatically attaches\nemojis\nto generated text,"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "compensating for emotional\nloss\nin the conversion process. The pilot\nstudy indicates\nthat emojilized text"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "enhances perceived emotions compared to plain text."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "We can also understand how to represent emotions in captions by analyzing how emotions are expressed"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "in text. A study along this direction is the representation of emotions in text-based messaging applications"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "[10] through visualizations in facial expression recognition in WhatsApp Web, and findings underscore users’"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "preference for maintaining control over their emotions in private settings, emphasizing the need for exclusive"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "presentation and sharing of emotions in certain contexts. An important application of emotional expressions"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "in text and imagery on social media during the COVID-19 pandemic [11] adds a temporal dimension to the"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "narrative, shedding light on how emotions are expressed in different modalities during challenging times."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "3.3\nEmotion Recognition in AR and VR"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "VR has gained attention in various fields due to its unique advantages in manipulating perceived scenarios"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "and providing controlled experiences\n[12].\nIt has been found that VR can generate\nemotional\nreactions"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "and experiences in users, as demonstrated by studies on 360 degree videos and VR educational tools [13]."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "In fact,\nemotional\nresponses\nin VR games have been observed to be more\nintense\ncompared to desktop"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "games, both psychologically and physiologically [14]. This suggests that VR has the potential\nto enhance"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "positive emotions, and it is worth to study if similar outcome can be experienced from STT in an AR or VR"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "environment."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "More studies have discussed advancements in AR technologies for DHH individuals, particularly in the"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "classroom setting [15]. The AR visual-captions system,\nimplemented using Unity, ARKit, and AR Founda-"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "tion, aims to provide real-time STT and visual elements around the teacher, creating an immersive learning"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "experience. The study not only introduced the prototype but also outlined future research directions, es-"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "pecially on refining the design, conducting user\nstudies, and extending the system’s capabilities\nfor group"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "conversations.\nSimilarly, Real-time AR visual-captions\n[16]\nfor DHH children in classrooms builds on the"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "AR-based system, providing a real-time solution for STT and keyword extraction, addressing the challenges"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "faced by DHH children in mainstream education."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "In higher education application, Pirker’s exploration into the potential of VR for computer science ed-"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "ucation [17] sheds light on the positive impact of VR environments on learning outcomes. The user study"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "reveals higher engagement, immersion, and positive emotions in the VR application compared to a web-based"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "alternative, emphasizing VR’s potential\nto enhance computer science education. Given the importance of"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "AR and VR in education training, making the virtual environment convey emotional expression is essential."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "Chen’s paper\n[18]\nfocuses on facial expression recognition within immersive environments. The proposed"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "solution involves collecting real\nfacial expression data using an infrared camera and light source, showcasing"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "promising results for understanding human emotions in VR contexts."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "3.3.1\nVisual and Affective Augmentation in XR"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "As\nimmersive AR and VR environments continue to gain traction in accessibility research,\nrecent\nstudies"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "have emphasized the importance of incorporating visual and affective augmentations into captioning systems."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "These augmentations not only support emotional expression but also reduce cognitive strain and improve"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "comprehension, particularly for DHH users and others who rely on captions in complex, real-time scenarios."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "A conceptual workflow of\nthis\nintegration is\nillustrated in Figure 2,\nshowing how STT outputs\ncan be"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "enhanced with emotional cues before delivery in XR displays."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "Recent research highlights the cognitive demands of prolonged exposure to captions within VR HMDs,"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "including increased eye strain and stress when captions are not personalized or well-positioned [19]. Par-"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "ticipants\nexpressed preferences\nfor adjustable\ncaption features\nsuch as\ntext\nsize, avatar use, and caption"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "placement, underscoring the need for personalization in immersive captioning interfaces."
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "To support\nthe design of emotionally expressive captions, a framework and accompanying interactive"
        },
        {
          "viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive": "tool were developed to help designers visualize emotions\nin AR using abstract or\nrepresentational visual"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "affective visuals, enabling real-time augmentation of emotional content in communication."
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "A related framework for AR-based affect visualization [21] builds on the dimensional model of\nemo-"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "tion and focuses on ambiguous visual representations such as gradients,\nforms, and motion patterns. The"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "study emphasizes that ambiguous, user-interpretable visuals can enhance emotional communication and user"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "engagement without overwhelming or over-simplifying complex emotional states."
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "In shared VR environments, bi-directional\nemotion-sharing mechanisms have been shown to improve"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "interpersonal trust, decision-making, and collaborative performance [22]. Visualizing emotions through sim-"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "plified indicators such as valence icons contributed to more frequent emotional consensus and helped build"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "rapport between unfamiliar collaborators, pointing to the potential of similar strategies in educational and"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "captioning systems."
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "Color also plays a central role in emotional communication. One study explored mapping vocal emotional"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "states to colored message bubbles [23], showing that users perceived emotional intensity more clearly through"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "contextual color cues. However, negative emotional states sometimes reduced user comfort, highlighting the"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "importance of careful calibration when applying visual augmentation to speech-based captions."
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "These findings are directly relevant\nto the development of accessible XR for education. A user-centric"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "investigation into educational captioning for DHH individuals\nrecommends embedding emotional context"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "into captions\nthrough visual highlights,\nemojis, and keyword emphasis\n[24].\nSuch enhancements are not"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "only beneficial\nfor comprehension and engagement but also critical\nfor reducing the sense of\nisolation and"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "cognitive load associated with conventional captioning."
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "Collectively,\nthese studies demonstrate that visual and affective augmentation is a necessary evolution"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "in XR-based captioning. When combined with emotion recognition technologies and user-centered design,"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "these strategies can transform how captions communicate meaning—enhancing not only what is said, but"
        },
        {
          "elements\n[20].\nThe framework guides decisions\nrelated to shape, animation, and spatial configuration of": "how it is felt."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "facial expressions and prototype robot\nfaces.\nThe study informs\nthe design of affective robot\nfaces,\ncon-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "tributing valuable insights into how emotions can be effectively communicated through facial expressions.In"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "a related study,the analysis of humanoid avatar representations and emotions [30] delves into the uncanniness"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "factor, highlighting the importance of selecting appropriate avatar types for accurate expression communica-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "tion. The study’s findings support the need for avatar representations that effectively convey emotional cues"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "in interactive systems. Finally, gestures can also be generated directly from speech using GANs [31] thereby"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "opening up new possibilities in human-computer interaction. The user study using Turing test-inspired eval-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "uation demonstrates the potential of the proposed technique in creating realistic gestures from speech.\nIn"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "conclusion, the narrative literature review traverses a spectrum of communication enhancement technologies,"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "showcasing the evolution of\ntools and techniques\nto address\nthe unique needs of DHH individuals.\nFrom"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "innovations\nin live\ntranscription and closed captioning to advancements\nin AR,\nemotive\ncaptioning, and"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "speech emotion recognition,\nthe literature reflects a dynamic landscape of research and development. The"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "exploration of humanoid avatars, voice user interfaces, VR, human-robot interaction, and the color coding"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "of speech emotions further emphasizes the interdisciplinary nature of this field. The review also underscores"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "the significance of understanding emotional expressions in text,\nimages, and gestures, offering a holistic per-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "spective on communication technologies that cater to diverse sensory and cognitive needs. As we navigate"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "the intricacies of enhancing communication for\nindividuals with hearing impairments,\nthese technological"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "advancements pave the way for a more inclusive and accessible future."
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "3.5\nEmpathic Machine"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "Emotions serve as an implicit communication channel among humans, conveyed through spoken words, facial"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "expressions, behavior, and physiological responses. This empathic form of communication enables individuals"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "to recognize cues and respond empathetically. Despite computing devices excelling in understanding user"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "context,\nthere are ongoing challenges\nin the scientific community to create emotion-sensing and empathic"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "applications for human-computer interaction [32].\nIn this section we look at the literature contributions in"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "tackling these challenges and support empathic ability in machines."
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "Enhancing Empathic Abilities with Chatbots:\nThe evolution of chatbots began with the creation of"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "ELIZA in 1966 [33], and today they are commonly used in customer service but have limitations in showing"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "empathy.\nEmpathy, defined as understanding and sharing another’s\nfeelings,\nis deemed challenging for"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "conversational agents. Recent research, however,\nindicates the feasibility of generating empathic responses"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "in chatbots, particularly in customer\nservice contexts,\nincluding in the healthcare, especially in providing"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "physical health diagnosis through short text conversations."
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "Several\nstudies available in the literature to compensate for\nthe shortcomings of\nlack of empathic ex-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "pressions\nin computer and AI employed the use of chatbots\nto convey emptional expressions. One study"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "used chatbots\nto mediate social presence and trust\nin consumer emotions\n[34],\ncreating a sense of\nsocial"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "presence using emoticons, appropriate language styles, and other cues that contribute to a more authentic"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "and human-like interaction. Another study used empathic chatbot to understand users’ emotional states and"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "generate responses that convey understanding and addressing challenges in handling multi-layered, context-"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "sensitive, and implicitly expressed emotions in text [35].\nIt utilized advanced models and tools involving a"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "benchmark bot and an empathic bot. To enhance empathic capabilities, they fine-tuned the language model"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "on empathic conversations,\nincorporating the user’s emotional state into the input. The findings from the"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "empathic bot include the effectiveness of transformer-based language models, the positive impact of training"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "on empathic conversations, and the influence of emotional valence on perceived empathy."
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "Similarly, chatbot application is finding its way into gender voice discrimination.\n[36] explores the tension"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "between designing empathic agents and the gender assignment of chatbots and how they can relate to the"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "design of the metaphor of the chatbots in conversational agents (Cas). Computers lack genuine emotions,"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "however CAs can simulate and trigger empathy by adhering to human-social rules during interactions. This"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "simulation involves techniques like sentiment analysis, emotion detection, and mimicking emotions to enhance"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "user engagement. Notably, the paper explores whether CAs, specifically those perceived as having feminine"
        },
        {
          "Further, Schiano’s paper [29] delves into the perception of\nfacial affect, bridging the gap between human": "qualities, evoke more empathy in human-chatbot interactions compared to other gender perceptions."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "cating empathy into machines is through digitizing human emotions. One study used non-invasive techniques"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "like electroencephalography (EEG), specifically the Emotive Epoc headset to digitize human emotions [37]"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "and the primary goal was\nto conduct a proof-of-concept experiment enabling a humanoid robot’s control"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "through digitized emotions,\nemphasizing potential applications\nin healthcare .\nTheir contribution lies\nin"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "adapting mature image recognition tools from Artificial Neural Networks (ANNs) for emotion recognition,"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "and the resulting Brain-Computer Interface (BCI) system is designed to enable the robot to emulate empathy"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "and interact with subjects based on predefined behavioral models. Similarly,\n[38] explores the significance of"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "recognizing emotions in face-to-face conversations and discussed the limitations of traditional cues like facial"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "expressions and gestures, emphasizing the potential of analyzing brain activity and physiological signals for"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "more reliable emotion recognition,\nespecially in situations where emotions may be concealed.\nThe study"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "presents an experimental setup for inducing spontaneous emotions in conversations and creating a dataset"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "incorporating EEG, Photoplethysmography (PPG), and Galvanic Skin Response (GSR)\nsignals.\nThe re-"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "searchers developed an intelligent user\ninterface\nfor video conferencing and systems with conversational"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "digital humans capable of recognizing and responding to emotions, using PEGCONV) dataset to train these"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "systems to enhance their ability to understand and appropriately respond to human behavior."
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "Further, Robots can also respond to user emotional states.\n[39] enhanced human-robot interactions (HRI)"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "by proposing an Automatic Cognitive Empathy Model (ACEM) for humanoid robots. The goal was to achieve"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "more extended and engaging interactions by having robots respond appropriately to users’ emotional states"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "using ACEM model\nto continuously detects users’ affective states based on facial expressions, utilizing a"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "stacked autoencoder network trained on the RAVDESS dataset. The model generates empathic behaviors"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "adapted to users’ personalities, either in parallel or as reactive responses."
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "Empathic Systems\nin AR/VR:\nEmpathic machine is also finding applications\ninto wearable.\nIn ex-"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "ploring the use of smart glasses equipped with an emotion recognition system to enhance human-to-human"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "communication, with a focus on doctors and autistic adults [40], this study identifies the potential benefits"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "for doctors in improving patient-doctor communication and for autistic adults facing challenges in adhering"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "to neurotypical communication norms. The proposed system combines emotion recognition, AI, and smart"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "glasses\nto provide\nreal-time\nfeedback on the\nemotional\nstate of\nconversation partners.\nUser\nevaluations"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "indicate positive responses from both doctors and autistic adults, highlighting the potential\nfor improving"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "empathy and communication. Design considerations include customizable output preferences and addressing"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "privacy and social\nimpact concerns. The authors emphasize the system’s potential\nfor educational purposes"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "and ongoing development in the context of social\nimpact and user experience.\nIn a related study,\n[41] ex-"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "plores the use of VR with wearable physiological sensors to examine how recalling autobiographical memories"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "(AM) with emotional content affects an individual’s physiological state. By replicating the Autobiographical"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "Memory Test (AMT) in VR, participants were presented with positive, negative, and neutral words to trig-"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "ger memory recall. The study observed a positive influence of AM recall on electrodermal activity (EDA)"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "peak amplitude, EDA peak number, and pupil diameter compared to situations without\nrecall. However,"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "emotional AM recall did not produce a significant impact. The article concludes by discussing the poten-"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "tial and limitations of utilizing autobiographical memory to enhance personalized mobile VR experiences in"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "conjunction with physiological sensors. The findings reveal a significant effect of AM recall on EDA mean"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "peak amplitude, EDA peak number, and pupil diameter compared to a no-recall condition. However, no"
        },
        {
          "Digitizing Human Emotions:\nAs computing machines and AI need to be programmed to work,\nincul-": "significant differences were identified in emotional AM recall (positive, negative, neutral)."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "examined developments across communication enhancement\ntechnologies,\ncaptioning innovation,\nemotion"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "recognition in XR, and empathic human-computer\nsystems.\nOur\nthematic\nsynthesis\nincluded a focused"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "review of visual and affective augmentation strategies, which show promise for conveying emotion through"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "ambient cues and interactive design elements [20, 19, 23]. However,\nthese strategies are largely decoupled"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "from real-time STT outputs, underscoring a significant gap in current systems:\nthe absence of emotionally"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "expressive, real-time captions in immersive environments."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Despite growing interest\nin affective computing and XR accessibility,\nfew studies directly address\nthe"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "integration of emotion into STT interfaces within AR/VR contexts. Current approaches focus primarily on"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "visual enhancements or emotion-aware avatars, without embedding affective cues directly into the transcribed"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "text\n[9, 5].\nThis\nlimitation presents a critical opportunity for\ninnovation—particularly in education and"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "training scenarios where\nemotional\ntone\ncan support\ncomprehension,\nreduce\ncognitive\nload, and increase"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "learner engagement [17, 18]."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Research into captioning innovations such as Rashid’s animated text framework [7, 5] and Hu’s emojiza-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "tion tool\n[9] exemplifies early efforts to embed emotion into transcription. These methods—whether through"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "animated captions, color cues, or emoji augmentations—have shown positive responses from DHH users [8],"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "and highlight\nthe potential\nfor more\nexpressive,\ninclusive STT systems.\nYet,\nscalability and vocabulary"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "limitations remain challenges, especially for deployment in real-time XR applications."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Extending these innovations into immersive platforms offers new possibilities. Recent AR and VR cap-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "tioning prototypes demonstrate technical\nfeasibility for real-time STT with spatial awareness and keyword"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "extraction [16, 15]. However,\nfew efforts have integrated emotional nuance into these systems.\nIn educa-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "tional\nsettings, where XR-based captioning is\nincreasingly applied,\nthe\ninclusion of affective\ninformation"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "could enhance both accessibility and engagement\n[17, 18]. Research on presence and affective response in"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "VR environments further suggests that immersive platforms are well-suited for emotionally resonant experi-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "ences [12]."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Beyond captioning, affective computing in adjacent domains—such as healthcare, robotics, and human-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "agent interaction—provides valuable insights. Studies show that emotionally responsive systems, powered by"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "machine learning,\nfacial recognition, and physiological sensing, can enhance empathy, personalization, and"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "user satisfaction [28, 27, 30]. Applications in education and clinical settings demonstrate that recognizing"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "and responding to user emotion improves outcomes and fosters trust [29]. The digitization of human emo-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "tion,\nincluding efforts using brain-computer\ninterfaces\n(BCIs) and affective robotics,\nrepresents a broader"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "trend toward emotionally intelligent\ntechnology. BCI-driven systems\nthat\ninterpret EEG or physiological"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "signals offer promising methods for capturing emotional states [37, 38], though their integration with STT"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "remains underexplored. Similarly, empathy models like ACEM for humanoid robots illustrate how emotional"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "awareness can be incorporated into interaction design to enable adaptive and human-like behavior [39]."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Wearable technologies, such as smart glasses with built-in emotion recognition, have also shown promise"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "in improving communication among autistic individuals and between doctors and patients [40]. These tools"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "deliver\nreal-time\nemotional\nfeedback to enhance\ninterpersonal understanding,\nand their\nintegration into"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "educational contexts could support more personalized and empathetic learning experiences. Multisensory"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "XR systems\nthat combine physiological data, autobiographical memory, and real-time feedback offer new"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "frontiers for emotionally responsive interaction. Studies exploring such experiences emphasize the potential"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "for tailoring virtual environments to individual affective states [41], providing valuable frameworks for the"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "design of emotionally expressive captioning systems."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "Overall, the findings of this review suggest a compelling case for interdisciplinary research that bridges"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "affective computing, psychophysiology, XR design, and accessibility. The future of\ninclusive STT systems"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "lies in integrating emotional expression not as a secondary visual\nlayer, but as an intrinsic component of the"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "transcription process—transforming captions from neutral\ninformation displays into emotionally meaningful"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "communication tools."
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "In sum, advancing emotional expression in STT systems holds particular promise for enhancing commu-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "nication accessibility for DHH users. By embedding emotion-aware cues into real-time captions, especially"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "within XR environments, designers and researchers\ncan move\ntoward more human-centered,\nempathetic"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "technologies.\nThis work underscores\nthe\nimportance of\ninclusivity not only in functionality but\nin emo-"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "tional fidelity—ensuring that all users,\nregardless of hearing ability,\ncan access\nthe\nfull depth of\nspoken"
        },
        {
          "Through a structured narrative review of 37 peer-reviewed studies published between 2020 and 2024, we": "communication."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1\nLimitations": "The limitations of\nthis\nstudy were:\n(i)\nit was a rapid review whose purpose is"
        },
        {
          "4.1\nLimitations": "emotional\nexpressions, find existing gaps and inspire\nfurther\nresearch.\n(ii)\nthe narrative"
        },
        {
          "4.1\nLimitations": "follow a strict rule expected of systematic literature review, hence could have missed some relevant papers."
        },
        {
          "4.1\nLimitations": "However, the papers that were found eligible and used in this review are the most relevant."
        },
        {
          "4.1\nLimitations": "is limited to peer-reviewed papers that were in English."
        },
        {
          "4.1\nLimitations": "4.2\nGaps, Challenges and Opportunities"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is limited to peer-reviewed papers that were in English.": "4.2\nGaps, Challenges and Opportunities"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "1.\nInnovations\nin emotive captioning and speech emotion recognition warrant\nfurther exploration, par-"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "ticularly in educational\nsettings where DHH students and other\nlearners\nrely on STT as\ntheir main"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "method of communication during lectures."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "2. Further\nintegration of\ncognitive\nempathy models\nis needed to improve\nemotion detection in short"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "conversations. Enhancements in medical chatbots show potential\nfor empathy-driven systems [33]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "3. There is a need for automated metrics to assess empathy in text-based systems. Future studies should"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "also refine definitions of empathy and explore emotion intensity and categories, while addressing ethical"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "concerns in human-like chatbot design [35]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "4. Research should explore alternative empathic response markers,\nincluding diverse character styles and"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "physiological\nindicators, and test these in immersive environments like VR [42]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "5. Emotion recognition systems should incorporate multimodal\ninputs,\nincluding EEG and physiological"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "signals,\nfor a holistic approach to understanding affective states in conversation [38]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "6. Future\ninvestigations\nshould assess applications of\nemotion-aware\nsystems\nin educational\ncontexts,"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "including medical education, and evaluate their integration into training curricula [40]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "7. Larger-scale studies are needed with more emotionally intense stimuli and diverse biosignals, applicable"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "to VR learning, exposure therapy, and adaptive gaming [41]."
        },
        {
          "is limited to peer-reviewed papers that were in English.": "8. The field should prioritize real-time integration of emotional visualization techniques with ASR captions"
        },
        {
          "is limited to peer-reviewed papers that were in English.": "in XR, moving beyond ambient indicators to emotion-aware captions that directly reflect STT output."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nConclusion": "This narrative review explored the integration of emotional expression into STT interfaces within immersive"
        },
        {
          "5\nConclusion": "XR environments, emphasizing the intersection of psychophysiology, accessibility, and user-centered design."
        },
        {
          "5\nConclusion": "While STT technologies have significantly advanced accessibility for DHH users, the omission of emotional"
        },
        {
          "5\nConclusion": "sues in transcribed speech remains a critical barrier to natural and expressive communication."
        },
        {
          "5\nConclusion": "Our findings highlight a growing recognition in HCI of"
        },
        {
          "5\nConclusion": "emerging strategies such as emotion-driven avatars, color-coded overlays, and ambient cues offering promis-"
        },
        {
          "5\nConclusion": "ing—but largely disconnected—approaches. A key research gap lies in the lack of direct integration between"
        },
        {
          "5\nConclusion": "real-time STT outputs and affective visualization techniques."
        },
        {
          "5\nConclusion": ""
        },
        {
          "5\nConclusion": "XR systems design.\nFuture\nresearch should investigate"
        },
        {
          "5\nConclusion": "tation—such as emojis, avatars, and dynamic text"
        },
        {
          "5\nConclusion": "comprehension in AR/VR settings. This review highlights the urgent need to move beyond neutral caption-"
        },
        {
          "5\nConclusion": "ing toward systems that can capture and convey the emotional context of speech."
        },
        {
          "5\nConclusion": ""
        },
        {
          "5\nConclusion": "more inclusive,\nempathetic, and engaging communication—paving the way for"
        },
        {
          "5\nConclusion": "immersive environments and beyond."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] A.-W. Harzing, “Publish or perish.” https://harzing.com/resources/publish-or-perish, 2007."
        },
        {
          "References": "[2] M. Ouzzani, H. Hammady, Z. Fedorowicz, and A. Elmagarmid, “Rayyan — a web and mobile app for"
        },
        {
          "References": "systematic reviews,” Systematic Reviews, vol. 5, p. 210, 2016."
        },
        {
          "References": "[3] F. Loizides, S. Basson, D. Kanevsky, O. Prilepova, S. Savla, and S. Zaraysky, “Breaking boundaries"
        },
        {
          "References": "with live transcribe: Expanding use cases beyond standard captioning scenarios,” in Proceedings of\nthe"
        },
        {
          "References": "22nd International ACM SIGACCESS Conference on Computers and Accessibility, pp. 1–6, 2020."
        },
        {
          "References": "[4] D. Arnold and A. Tremblay, “Interaction of deaf and hearing preschool children,” Journal of Commu-"
        },
        {
          "References": "nication Disorders, vol. 12, no. 3, pp. 245–251, 1979."
        },
        {
          "References": "[5] R. Rashid, V. Quoc, R. Hunt, and D. I. Fels, “Dancing with words: Using animated text for captioning,”"
        },
        {
          "References": "Intl. Journal of Human–Computer Interaction, vol. 24, no. 5, pp. 505–519, 2008."
        },
        {
          "References": "[6] V. Murphy-Berman and L. Whobrey, “The impact of captions on hearing-impaired children’s affective"
        },
        {
          "References": "reactions to television,” The Journal of Special Education, vol. 17, no. 1, pp. 47–62, 1983."
        },
        {
          "References": "[7] R. Rashid, J. Aitken, and D. I. Fels, “Expressing emotions using animated text captions,” in Computers"
        },
        {
          "References": "Helping People with Special Needs:\n10th International Conference,\nICCHP 2006, Linz, Austria, July"
        },
        {
          "References": "11-13, 2006. Proceedings 10, pp. 24–31, Springer, 2006."
        },
        {
          "References": "[8] D. G. Lee, D. I. Fels, and J. P. Udo, “Emotive captioning,” Computers in Entertainment (CIE), vol. 5,"
        },
        {
          "References": "no. 2, p. 11, 2007."
        },
        {
          "References": "[9] J. Hu, Q. Xu, L. P. Fu, and Y. Xu, “Emojilization: An automated method for speech to emoji-labeled"
        },
        {
          "References": "text,” in Extended Abstracts of\nthe 2019 CHI Conference on Human Factors\nin Computing Systems,"
        },
        {
          "References": "pp. 1–6, 2019."
        },
        {
          "References": "[10] R. Poguntke, T. Mantz, M. Hassib, A. Schmidt, and S. Schneegass, “Smile to me:\ninvestigating emotions"
        },
        {
          "References": "and their representation in text-based messaging in the wild,” in Proceedings of Mensch und Computer"
        },
        {
          "References": "2019, pp. 373–385, ACM, 2019."
        },
        {
          "References": "[11] Q. Li, “Text vs.\nimages: Understanding emotional expressions on social media during covid-19 pan-"
        },
        {
          "References": "demic,” Human Factors in Communication of Design, vol. 49, p. 18, 2022."
        },
        {
          "References": "[12] K. Kilteni, R. Groten, and M. Slater, “The sense of embodiment in virtual reality,” Presence: Virtual"
        },
        {
          "References": "and Augmented Reality, vol. 21, no. 4, pp. 373–387, 2012."
        },
        {
          "References": "[13] S. Lie, K. Røykenes, A. Sæheim,\nand K. Groven,\n“Developing a virtual\nreality educational\ntool\nto"
        },
        {
          "References": "stimulate emotions for learning:\nfocus group study,” JMIR Formative Research, vol. 7, p. e41829, 2023."
        },
        {
          "References": "[14] F. Pallavicini and A. Pepe,\n“Virtual\nreality games and the\nrole of body involvement\nin enhancing"
        },
        {
          "References": "positive emotions and decreasing anxiety: within-subjects pilot study,” JMIR Serious Games, vol. 8,"
        },
        {
          "References": "no. 2, p. e15635, 2020."
        },
        {
          "References": "[15] A. Alnafjan, A. Aljumaah, H. Alaskar, and R. Alshraihi, “Designing “najeeb”: Technology-enhanced"
        },
        {
          "References": "learning for children with impaired hearing using arabic sign-language arsl applications,” in 2017 Inter-"
        },
        {
          "References": "national Conference on Computer and Applications (ICCA), pp. 238–273, IEEE, 2017."
        },
        {
          "References": "[16] J. Li, “Real-time augmented reality visual-captions for deaf and hard-of-hearing children in classrooms,”"
        },
        {
          "References": "in 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),"
        },
        {
          "References": "pp. 641–642, IEEE, 2023."
        },
        {
          "References": "[17] J. Pirker, J. Kopf, A. Kainz, A. Dengel, and B. Buchbauer, “The potential of virtual reality for computer"
        },
        {
          "References": "science education-engaging students\nthrough immersive visualizations,” in 2021 IEEE Conference on"
        },
        {
          "References": "Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pp. 297–302, IEEE, 2021."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "application,” Virtual Reality, vol. 27, no. 3, pp. 1717–1732, 2023."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[19] S. D. Ubur, N. A. Etori, S. Ghasemi, K. King, D. Graˇcanin, and M. Gini, “Easycaption:\nInvestigating"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "the impact of prolonged exposure to captioning on vr hmd on general population,” in Universal Access"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "in Human-Computer Interaction (M. Antona and C. Stephanidis, eds.), (Cham), pp. 382–403, Springer"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Nature Switzerland, 2024."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[20] S. S¸emsio˘glu, P. Karaturhan, and A. E. Yanta¸c, “Emote: An interactive online tool\nfor designing real-"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "time emotional ar visualizations,” in 13th Augmented Human International Conference, AH2022, (New"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "York, NY, USA), Association for Computing Machinery, 2022."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[21] S. S¸emsio˘glu and A. E. Yanta¸c, “Co-exploring the design space of emotional ar visualizations,” in HCI"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "International 2021 - Late Breaking Posters (C. Stephanidis, M. Antona, and S. Ntoa, eds.),\n(Cham),"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "pp. 377–384, Springer International Publishing, 2021."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[22] A. Jing, M. Frederick, M. Sewell, A. Karlson, B. Simpson, and M. Smith, “How visualising emotions"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "affects interpersonal trust and task collaboration in a shared virtual space,” in 2023 IEEE International"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Symposium on Mixed and Augmented Reality (ISMAR), pp. 849–858, 2023."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[23] Q. Chen, Y. Yan, and H.-J. Suk, “Bubble coloring to visualize the speech emotion,” in Extended Abstracts"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "of\nthe 2021 CHI Conference on Human Factors in Computing Systems, CHI EA ’21, (New York, NY,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "USA), Association for Computing Machinery, 2021."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[24] S. D. Ubur, “[DC] Enhancing Accessibility and Emotional Expression in Educational Extended Reality"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "for Deaf and Hard of Hearing: A User-Centric Investigation”,\nin 2024 IEEE Conference on Virtual"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Reality and 3D User Interfaces Abstracts and Workshops (VRW), (New York), pp. 1136–1137,\nIEEE,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "2024."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[25] A. Elor, A. Song, and S. Kurniawan, “Understanding emotional expression with haptic feedback vest"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "patterns and immersive virtual\nreality,”\nin 2021 IEEE Conference on Virtual Reality and 3D User"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Interfaces Abstracts and Workshops (VRW), pp. 183–188, IEEE, 2021."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[26] L. Bartram, A. Patra, and M. Stone, “Affective color in visualization,” in Proceedings of\nthe 2017 CHI"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "conference on human factors in computing systems, pp. 1364–1374, 2017."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[27] D. Cernea, C. Weber, A. Ebert, and A. Kerren, “Emotion-prints:\nInteraction-driven emotion visualiza-"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "tion on multi-touch interfaces,” in Visualization and Data Analysis 2015, vol. 9397, pp. 82–96, SPIE,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "2015."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[28] H. Wang, H. Prendinger, and T. Igarashi, “Communicating emotions in online chat using physiological"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "sensors and animated text,” in CHI’04 extended abstracts on Human factors\nin computing\nsystems,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "pp. 1171–1174, 2004."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[29] D. J. Schiano, S. M. Ehrlich, K. Rahardja, and K. Sheridan, “Face to interface:\nfacial affect\nin (hu)"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "man and machine,” in Proceedings of\nthe SIGCHI conference on Human factors in computing systems,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "pp. 193–200, 2000."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[30] D. Kumarapeli, S. Jung, and R. W. Lindeman, “Emotional avatars: Effect of uncanniness in identifying"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "emotions using avatar expressions,” in 2022 IEEE Conference on Virtual Reality and 3D User Interfaces"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Abstracts and Workshops (VRW), pp. 650–651, IEEE, 2022."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[31] M. Rebol, C. G¨uti, and K. Pietroszek, “Passing a non-verbal turing test: Evaluating gesture animations"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "generated from speech,” in 2021 IEEE Virtual Reality and 3D User Interfaces (VR), pp. 573–581, IEEE,"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "2021."
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "[32] E. Bosch, D. Bethge, M. Klosterkamp, and T. Kosch, “Empathic technologies shaping innovative inter-"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "action: Future directions of affective computing,” in Adjunct Proceedings of\nthe 2022 Nordic Human-"
        },
        {
          "[18] X. Chen and H. Chen, “Emotion recognition using facial expressions\nin an immersive virtual\nreality": "Computer Interaction Conference, pp. 1–3, 2022."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "in Proceedings of\nthe 20th ACM International Conference on Intelligent Virtual Agents, pp. 1–3, 2020."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[34] X. Sun and H. Guan, “Research on empathic remediation mechanism of chatbots mediated by social"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "presence and trust,” in Proceedings of the 2022 6th International Conference on Electronic Information"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "Technology and Computer Engineering, pp. 772–776, 2022."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[35] J. Casas, T. Spring, K. Daher, E. Mugellini, O. A. Khaled, and P. Cudr´e-Mauroux, “Enhancing con-"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "versational agents with empathic abilities,” in Proceedings of\nthe 21st ACM International Conference"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "on Intelligent Virtual Agents, pp. 41–47, 2021."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[36] J.-Y. Jung and A. Bozzon, “Are female chatbots more empathic?-discussing gendered conversational"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "agent through empathic design,” in Proceedings of the 2nd Empathy-Centric Design Workshop, pp. 1–5,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "2023."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[37] A. Roshdy, S. Al Kork, A. Karar, A. Al Sabi, Z. Al Barakeh, F. ElSayed, T. Beyrouthy, and A. Nait-"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "Ali, “Machine empathy: Digitizing human emotions,” in 2021 International Symposium on Electrical,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "Electronics and Information Engineering, pp. 307–311, 2021."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[38] N. Saffaryazdi, Y. Goonesekera, N. Saffaryazdi, N. D. Hailemariam, E. G. Temesgen, S. Nanayakkara,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "E. Broadbent, and M. Billinghurst, “Emotion recognition in conversations using brain and physiological"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "signals,” in 27th International Conference on Intelligent User Interfaces, pp. 229–242, 2022."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[39] E. Bagheri, P. G. Esteban, H.-L. Cao, A. D. Beir, D. Lefeber, and B. Vanderborght, “An autonomous"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "cognitive empathy model responsive to users’\nfacial emotion expressions,” ACM Transactions on Inter-"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "active Intelligent Systems (TIIS), vol. 10, no. 3, pp. 1–23, 2020."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[40] T. Lin, L. Huang, B. Hannaford, C. Tran, J. Raiti, R. Zaragoza, T. Feng, L. Wagner, and J. James,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "“Empathics system: application of emotion analysis ai through smart glasses,” in Proceedings of the 13th"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "ACM International Conference on PErvasive Technologies Related to Assistive Environments, pp. 1–4,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "2020."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[41] K. Gupta, S. W. Chan, Y. S. Pai, A. Sumich, S. Nanayakkara, and M. Billinghurst, “Towards under-"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "standing physiological responses to emotional autobiographical memory recall\nin mobile vr scenarios,”"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "in Adjunct Publication of\nthe 23rd International Conference on Mobile Human-Computer Interaction,"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "pp. 1–5, 2021."
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "[42] D. Higgins, Y. Zhan, B. R. Cowan, and R. McDonnell, “Investigating the effect of visual realism on em-"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "pathic responses to emotionally expressive virtual humans,” in ACM Symposium on Applied Perception"
        },
        {
          "[33] K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,”": "2023, pp. 1–7, 2023."
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Challenges and Opportunities",
      "venue": "Challenges and Opportunities"
    },
    {
      "citation_id": "2",
      "title": "Innovations in emotive captioning and speech emotion recognition warrant further exploration, particularly in educational settings where DHH students and other learners rely on STT as their main method of communication during lectures",
      "venue": "Innovations in emotive captioning and speech emotion recognition warrant further exploration, particularly in educational settings where DHH students and other learners rely on STT as their main method of communication during lectures"
    },
    {
      "citation_id": "3",
      "title": "Further integration of cognitive empathy models is needed to improve emotion detection in short conversations. Enhancements in medical chatbots show potential for empathy-driven systems",
      "venue": "Further integration of cognitive empathy models is needed to improve emotion detection in short conversations. Enhancements in medical chatbots show potential for empathy-driven systems"
    },
    {
      "citation_id": "4",
      "title": "There is a need for automated metrics to assess empathy in text-based systems. Future studies should also refine definitions of empathy and explore emotion intensity and categories, while addressing ethical concerns in human-like chatbot design",
      "venue": "There is a need for automated metrics to assess empathy in text-based systems. Future studies should also refine definitions of empathy and explore emotion intensity and categories, while addressing ethical concerns in human-like chatbot design"
    },
    {
      "citation_id": "5",
      "title": "Research should explore alternative empathic response markers, including diverse character styles and physiological indicators, and test these in immersive environments like VR",
      "venue": "Research should explore alternative empathic response markers, including diverse character styles and physiological indicators, and test these in immersive environments like VR"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition systems should incorporate multimodal inputs, including EEG and physiological signals, for a holistic approach to understanding affective states in conversation",
      "venue": "Emotion recognition systems should incorporate multimodal inputs, including EEG and physiological signals, for a holistic approach to understanding affective states in conversation"
    },
    {
      "citation_id": "7",
      "title": "Future investigations should assess applications of emotion-aware systems in educational contexts, including medical education, and evaluate their integration into training curricula",
      "venue": "Future investigations should assess applications of emotion-aware systems in educational contexts, including medical education, and evaluate their integration into training curricula"
    },
    {
      "citation_id": "8",
      "title": "Larger-scale studies are needed with more emotionally intense stimuli and diverse biosignals, applicable to VR learning, exposure therapy, and adaptive gaming",
      "venue": "Larger-scale studies are needed with more emotionally intense stimuli and diverse biosignals, applicable to VR learning, exposure therapy, and adaptive gaming"
    },
    {
      "citation_id": "9",
      "title": "The field should prioritize real-time integration of emotional visualization techniques with ASR captions in XR, moving beyond ambient indicators to emotion-aware captions that directly reflect STT output",
      "venue": "The field should prioritize real-time integration of emotional visualization techniques with ASR captions in XR, moving beyond ambient indicators to emotion-aware captions that directly reflect STT output"
    },
    {
      "citation_id": "10",
      "title": "Future Work Directions To address the outlined challenges and research gaps, we propose the following directions for future exploration: 1. Integrating Visual Augmentation in AR/VR STT Systems: Investigate how the use of emojis, animated avatars, and symbolic visuals can enhance emotional expression in real-time captions",
      "venue": "Future Work Directions To address the outlined challenges and research gaps, we propose the following directions for future exploration: 1. Integrating Visual Augmentation in AR/VR STT Systems: Investigate how the use of emojis, animated avatars, and symbolic visuals can enhance emotional expression in real-time captions"
    },
    {
      "citation_id": "11",
      "title": "Designing Emotionally Resonant Caption Formats: Identify optimal caption formats and placement strategies that preserve speaker affect and reduce viewer distraction in immersive settings",
      "venue": "Designing Emotionally Resonant Caption Formats: Identify optimal caption formats and placement strategies that preserve speaker affect and reduce viewer distraction in immersive settings"
    },
    {
      "citation_id": "12",
      "title": "Reducing Cognitive Load through Personalization: Develop adaptive captioning interfaces that adjust visual emphasis, pacing, and layout based on user preferences and cognitive workload",
      "venue": "Reducing Cognitive Load through Personalization: Develop adaptive captioning interfaces that adjust visual emphasis, pacing, and layout based on user preferences and cognitive workload"
    },
    {
      "citation_id": "13",
      "title": "Dynamic Substitution of Emotive Language: Explore mechanisms for substituting emotionally expressive language with visual or symbolic equivalents",
      "venue": "Dynamic Substitution of Emotive Language: Explore mechanisms for substituting emotionally expressive language with visual or symbolic equivalents"
    },
    {
      "citation_id": "14",
      "title": "Evaluation Frameworks: Build comprehensive evaluation frameworks that assess the emotional clarity, usability, and accessibility of captioning strategies across XR platforms. References",
      "authors": [
        "Cross-Platform"
      ],
      "venue": "Evaluation Frameworks: Build comprehensive evaluation frameworks that assess the emotional clarity, usability, and accessibility of captioning strategies across XR platforms. References"
    },
    {
      "citation_id": "15",
      "title": "Publish or perish",
      "authors": [
        "A.-W Harzing"
      ],
      "year": "2007",
      "venue": "Publish or perish"
    },
    {
      "citation_id": "16",
      "title": "Rayyan -a web and mobile app for systematic reviews",
      "authors": [
        "M Ouzzani",
        "H Hammady",
        "Z Fedorowicz",
        "A Elmagarmid"
      ],
      "year": "2016",
      "venue": "Systematic Reviews"
    },
    {
      "citation_id": "17",
      "title": "Breaking boundaries with live transcribe: Expanding use cases beyond standard captioning scenarios",
      "authors": [
        "F Loizides",
        "S Basson",
        "D Kanevsky",
        "O Prilepova",
        "S Savla",
        "S Zaraysky"
      ],
      "year": "2020",
      "venue": "Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility"
    },
    {
      "citation_id": "18",
      "title": "Interaction of deaf and hearing preschool children",
      "authors": [
        "D Arnold",
        "A Tremblay"
      ],
      "year": "1979",
      "venue": "Journal of Communication Disorders"
    },
    {
      "citation_id": "19",
      "title": "Dancing with words: Using animated text for captioning",
      "authors": [
        "R Rashid",
        "V Quoc",
        "R Hunt",
        "D Fels"
      ],
      "year": "2008",
      "venue": "Intl. Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "20",
      "title": "The impact of captions on hearing-impaired children's affective reactions to television",
      "authors": [
        "V Murphy-Berman",
        "L Whobrey"
      ],
      "year": "1983",
      "venue": "The Journal of Special Education"
    },
    {
      "citation_id": "21",
      "title": "Expressing emotions using animated text captions",
      "authors": [
        "R Rashid",
        "J Aitken",
        "D Fels"
      ],
      "year": "2006",
      "venue": "Computers Helping People with Special Needs: 10th International Conference"
    },
    {
      "citation_id": "22",
      "title": "Emotive captioning",
      "authors": [
        "D Lee",
        "D Fels",
        "J Udo"
      ],
      "year": "2007",
      "venue": "Computers in Entertainment (CIE)"
    },
    {
      "citation_id": "23",
      "title": "Emojilization: An automated method for speech to emoji-labeled text",
      "authors": [
        "J Hu",
        "Q Xu",
        "L Fu",
        "Y Xu"
      ],
      "year": "2019",
      "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "24",
      "title": "Smile to me: investigating emotions and their representation in text-based messaging in the wild",
      "authors": [
        "R Poguntke",
        "T Mantz",
        "M Hassib",
        "A Schmidt",
        "S Schneegass"
      ],
      "year": "2019",
      "venue": "Proceedings of Mensch und Computer"
    },
    {
      "citation_id": "25",
      "title": "Text vs. images: Understanding emotional expressions on social media during covid-19 pandemic",
      "authors": [
        "Q Li"
      ],
      "year": "2022",
      "venue": "Human Factors in Communication of Design"
    },
    {
      "citation_id": "26",
      "title": "The sense of embodiment in virtual reality",
      "authors": [
        "K Kilteni",
        "R Groten",
        "M Slater"
      ],
      "year": "2012",
      "venue": "Presence: Virtual and Augmented Reality"
    },
    {
      "citation_id": "27",
      "title": "Developing a virtual reality educational tool to stimulate emotions for learning: focus group study",
      "authors": [
        "S Lie",
        "K Røykenes",
        "A Saeheim",
        "K Groven"
      ],
      "year": "2023",
      "venue": "JMIR Formative Research"
    },
    {
      "citation_id": "28",
      "title": "Virtual reality games and the role of body involvement in enhancing positive emotions and decreasing anxiety: within-subjects pilot study",
      "authors": [
        "F Pallavicini",
        "A Pepe"
      ],
      "year": "2020",
      "venue": "JMIR Serious Games"
    },
    {
      "citation_id": "29",
      "title": "Designing \"najeeb\": Technology-enhanced learning for children with impaired hearing using arabic sign-language arsl applications",
      "authors": [
        "A Alnafjan",
        "A Aljumaah",
        "H Alaskar",
        "R Alshraihi"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Computer and Applications (ICCA)"
    },
    {
      "citation_id": "30",
      "title": "Real-time augmented reality visual-captions for deaf and hard-of-hearing children in classrooms",
      "authors": [
        "J Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)"
    },
    {
      "citation_id": "31",
      "title": "The potential of virtual reality for computer science education-engaging students through immersive visualizations",
      "authors": [
        "J Pirker",
        "J Kopf",
        "A Kainz",
        "A Dengel",
        "B Buchbauer"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition using facial expressions in an immersive virtual reality application",
      "authors": [
        "X Chen",
        "H Chen"
      ],
      "year": "2023",
      "venue": "Virtual Reality"
    },
    {
      "citation_id": "33",
      "title": "Easycaption: Investigating the impact of prolonged exposure to captioning on vr hmd on general population",
      "authors": [
        "S Ubur",
        "N Etori",
        "S Ghasemi",
        "K King",
        "D Gračanin",
        "M Gini"
      ],
      "year": "2024",
      "venue": "Universal Access in Human-Computer Interaction"
    },
    {
      "citation_id": "34",
      "title": "Emote: An interactive online tool for designing realtime emotional ar visualizations",
      "authors": [
        "P Karaturhan",
        "A Yantaç"
      ],
      "year": "2022",
      "venue": "13th Augmented Human International Conference"
    },
    {
      "citation_id": "35",
      "title": "Co-exploring the design space of emotional ar visualizations",
      "authors": [
        "A Yantaç"
      ],
      "year": "2021",
      "venue": "HCI International 2021 -Late Breaking Posters"
    },
    {
      "citation_id": "36",
      "title": "How visualising emotions affects interpersonal trust and task collaboration in a shared virtual space",
      "authors": [
        "A Jing",
        "M Frederick",
        "M Sewell",
        "A Karlson",
        "B Simpson",
        "M Smith"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)"
    },
    {
      "citation_id": "37",
      "title": "Bubble coloring to visualize the speech emotion",
      "authors": [
        "Q Chen",
        "Y Yan",
        "H.-J Suk"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21"
    },
    {
      "citation_id": "38",
      "title": "DC] Enhancing Accessibility and Emotional Expression in Educational Extended Reality for Deaf and Hard of Hearing: A User-Centric Investigation",
      "authors": [
        "S Ubur"
      ],
      "year": "2024",
      "venue": "2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)"
    },
    {
      "citation_id": "39",
      "title": "Understanding emotional expression with haptic feedback vest patterns and immersive virtual reality",
      "authors": [
        "A Elor",
        "A Song",
        "S Kurniawan"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)"
    },
    {
      "citation_id": "40",
      "title": "Affective color in visualization",
      "authors": [
        "L Bartram",
        "A Patra",
        "M Stone"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "41",
      "title": "Emotion-prints: Interaction-driven emotion visualization on multi-touch interfaces",
      "authors": [
        "D Cernea",
        "C Weber",
        "A Ebert",
        "A Kerren"
      ],
      "year": "2015",
      "venue": "Visualization and Data Analysis"
    },
    {
      "citation_id": "42",
      "title": "Communicating emotions in online chat using physiological sensors and animated text",
      "authors": [
        "H Wang",
        "H Prendinger",
        "T Igarashi"
      ],
      "year": "2004",
      "venue": "CHI'04 extended abstracts on Human factors in computing systems"
    },
    {
      "citation_id": "43",
      "title": "Face to interface: facial affect in (hu) man and machine",
      "authors": [
        "D Schiano",
        "S Ehrlich",
        "K Rahardja",
        "K Sheridan"
      ],
      "year": "2000",
      "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems"
    },
    {
      "citation_id": "44",
      "title": "Emotional avatars: Effect of uncanniness in identifying emotions using avatar expressions",
      "authors": [
        "D Kumarapeli",
        "S Jung",
        "R Lindeman"
      ],
      "year": "2022",
      "venue": "2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)"
    },
    {
      "citation_id": "45",
      "title": "Passing a non-verbal turing test: Evaluating gesture animations generated from speech",
      "authors": [
        "M Rebol",
        "C Güti",
        "K Pietroszek"
      ],
      "year": "2021",
      "venue": "2021 IEEE Virtual Reality and 3D User Interfaces (VR)"
    },
    {
      "citation_id": "46",
      "title": "Empathic technologies shaping innovative interaction: Future directions of affective computing",
      "authors": [
        "E Bosch",
        "D Bethge",
        "M Klosterkamp",
        "T Kosch"
      ],
      "year": "2022",
      "venue": "Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference"
    },
    {
      "citation_id": "47",
      "title": "Empathic chatbot response for medical assistance",
      "authors": [
        "K Daher",
        "J Casas",
        "O Khaled",
        "E Mugellini"
      ],
      "year": "2020",
      "venue": "Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "48",
      "title": "Research on empathic remediation mechanism of chatbots mediated by social presence and trust",
      "authors": [
        "X Sun",
        "H Guan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering"
    },
    {
      "citation_id": "49",
      "title": "Enhancing conversational agents with empathic abilities",
      "authors": [
        "J Casas",
        "T Spring",
        "K Daher",
        "E Mugellini",
        "O Khaled",
        "P Cudré-Mauroux"
      ],
      "year": "2021",
      "venue": "Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "50",
      "title": "Are female chatbots more empathic?-discussing gendered conversational agent through empathic design",
      "authors": [
        "J.-Y Jung",
        "A Bozzon"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2nd Empathy-Centric Design Workshop"
    },
    {
      "citation_id": "51",
      "title": "Machine empathy: Digitizing human emotions",
      "authors": [
        "A Roshdy",
        "S Al Kork",
        "A Karar",
        "A Al",
        "Z Sabi",
        "F Al Barakeh",
        "T Elsayed",
        "A Beyrouthy",
        "Nait-Ali"
      ],
      "year": "2021",
      "venue": "2021 International Symposium on Electrical, Electronics and Information Engineering"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition in conversations using brain and physiological signals",
      "authors": [
        "N Saffaryazdi",
        "Y Goonesekera",
        "N Saffaryazdi",
        "N Hailemariam",
        "E Temesgen",
        "S Nanayakkara",
        "E Broadbent",
        "M Billinghurst"
      ],
      "year": "2022",
      "venue": "27th International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "53",
      "title": "An autonomous cognitive empathy model responsive to users' facial emotion expressions",
      "authors": [
        "E Bagheri",
        "P Esteban",
        "H.-L Cao",
        "A Beir",
        "D Lefeber",
        "B Vanderborght"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TIIS)"
    },
    {
      "citation_id": "54",
      "title": "Empathics system: application of emotion analysis ai through smart glasses",
      "authors": [
        "T Lin",
        "L Huang",
        "B Hannaford",
        "C Tran",
        "J Raiti",
        "R Zaragoza",
        "T Feng",
        "L Wagner",
        "J James"
      ],
      "year": "2020",
      "venue": "Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments"
    },
    {
      "citation_id": "55",
      "title": "Towards understanding physiological responses to emotional autobiographical memory recall in mobile vr scenarios",
      "authors": [
        "K Gupta",
        "S Chan",
        "Y Pai",
        "A Sumich",
        "S Nanayakkara",
        "M Billinghurst"
      ],
      "year": "2021",
      "venue": "Adjunct Publication of the 23rd International Conference on Mobile Human-Computer Interaction"
    },
    {
      "citation_id": "56",
      "title": "Investigating the effect of visual realism on empathic responses to emotionally expressive virtual humans",
      "authors": [
        "D Higgins",
        "Y Zhan",
        "B Cowan",
        "R Mcdonnell"
      ],
      "year": "2023",
      "venue": "ACM Symposium on Applied Perception 2023"
    }
  ]
}