{
  "paper_id": "2405.11222v1",
  "title": "Isds-Nlp At Semeval-2024 Task 10: Transformer Based Neural Networks For Emotion Recognition In Conversations",
  "published": "2024-05-18T08:05:05Z",
  "authors": [
    "Claudiu Creanga",
    "Liviu P. Dinu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper outlines the approach of the ISDS-NLP team in the SemEval 2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF). For Subtask 1 we obtained a weighted F1 score of 0.43 and placed 12 in the leaderboard. We investigate two distinct approaches: Masked Language Modeling (MLM) and Causal Language Modeling (CLM). For MLM, we employ pre-trained BERT-like models in a multilingual setting, fine-tuning them with a classifier to predict emotions. Experiments with varying input lengths, classifier architectures, and fine-tuning strategies demonstrate the effectiveness of this approach. Additionally, we utilize Mistral 7B Instruct V0.2, a state-of-the-art model, applying zero-shot and few-shot prompting techniques. Our findings indicate that while Mistral shows promise, MLMs currently outperform them in sentencelevel emotion classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Task 10 from SemEval 2024 competition  (Kumar et al., 2024)  addresses the complex challenge of identifying the emotions within dialogues (English and Hindi). This task comprises two primary objectives: firstly, assigning an emotion label to each utterance within a dialogue, and secondly, discerning the trigger utterance or utterances responsible for an emotion-flip within the dialogue  (Kumar et al., 2022) . Emotions play a crucial role in human interaction and one can understand more from a text if one knows the underlying sentiment of the writer. In contexts where disagreements may arise, such as customer service platforms, virtual assistant chats or forums, identifying trigger utterances for emotion flips can help mediate conflicts and prevent escalation. A chatbot dealing with an angry customer would benefit from knowing how to speak in order to generate empathetic responses. If it knows that the chatbot's current sentence can trigger an emotion flip from neutral to anger, the chatbot should refine it, or if the emotion flip is from anger to joy, the chatbot should be more confident in such a response in the future.\n\nBoth types of models we tried for Subtask 1 were based on transformers. The first one used BERTlike models and we achieved the best accuracy with them, while the second one is a state of the art causal model (Mistral,  (Jiang et al., 2023) ) that was tested in zero-shot and few-shot settings with poorer results.\n\nAlthough in the first task our system worked well, placing 12th in the leaderboard, the other 2 tasks were much harder and we placed 14th on the second subtask. We believed that with a better strategy to prevent overfitting (like under or oversampling), our system would have improved. Our code is open source and available to use on GitHub.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "The competition had 3 subtasks explained in Figure  1  and we participated in all of them with the best results on subtask 1 where we placed 12th with an F1 score of 0.43.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "The dataset contains English and Hindi code-mixed conversations for Subtask 1 and 2 and English only conversations for Subtask 3 (Table  1 ). The dataset is quite small, except for the training dataset for There were 8 distinct emotions to predict: neutral, anger, surprise, fear, joy, sadness, disgust, and contempt. By far the most predominant emotion is neutral, followed by joy and anger (Figure  2 ). If we look at Subtask 2, most often the emotion flips are from neutral to joy or anger (Figure  3 ).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Previous Work",
      "text": "Since the release of the first small datasets for emotion recognition in 1992  (Ekman, 1992) , the field has evolved substantially, marked by significant contributions from big companies in the form of extensive datasets  (Demszky et al., 2020) . In the beginning, lexicon based methods were used in which there was a manually curated dictionary which associates words with specific emotions. The algorithm was simply picking the most expressed emotion according to the dictionary. This method had severe limitations because it was ignoring context, sentence structure and negations which can flip a sentiment. Today, state of the art models are based on transformer architecture and use either Masked Language Modelling (BERT based models  (Devlin et al., 2018)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "System Overview",
      "text": "We tried two approaches, both of them based on transformer architecture: Masked Language Modelling and Casual Modelling. We chose these two architectures because of their recent successes in NLP.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Masked Language Modelling",
      "text": "We used pre-trained BERT-like models in a multilingual setting so that it can tokenise Hindi sentences. These pre-trained models will give us the features from sentences and then we pass them through a classifier which will do the prediction for each task Figure  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Input",
      "text": "Analysis of dialogue sentences reveals a predominantly short length, with a sharp decline in frequency after 30 tokens (see Figure  5 ). To optimize performance, various maximum sequence lengths were tested, with 55 tokens yielding the best results (Figure  6 ). Data preprocessing, (such as lemmatization, removing punctuation or stopwords) didn't help the model learn better so we kept the input as is. Probably this is because punctuation and stopwords contain useful information that the models is able to learn.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Output",
      "text": "Selecting the optimal hidden state layer is crucial for leveraging the pre-trained model's results. Our experiments demonstrated that using the final layer's output yielded the strongest performance, with accuracy declining in earlier layers. For MLMtype models, the [CLS] token encodes the features, which is what we pass to our classification layer.\n\nAmong various classifiers tested (Table  2 ), fully connected layers excelled, likely due to their ability to model complex, non-linear relationships. The top-performing model employed a fully connected",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning",
      "text": "The large pre-trained language models we employed offer a robust foundation for understanding language in general. Through fine-tuning, we adapt them to the nuances of our emotion recognition task. Inspired by the strategy presented in  (Sun et al., 2020) , we initially train only the classifier with a larger learning rate (5e-5) and a warm-up period of 10,000 steps over 'k' epochs (we tried a range of 'k' from 1 to 10). Subsequently, we fine-tune both the classifier and the transformer's final layer using a smaller learning rate (2e-5). Our goal in freezing the transformer weights at first, and then training them with a reduced learning rate, is to minimize the risk of overfitting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Causal Modelling",
      "text": "Given the success of generative models we also tried Mistral 7B Instruct V0.2 which is believed to be state of the art in its category of models  (Jiang et al., 2023) . These type of LLMs have had success in a large number of NLP tasks, but seem to still lag Masked Language Models in sentence classification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Prompting",
      "text": "In Causal Modelling, how you prompt the model significantly influences its performance. We tested different prompting strategies in both zero-shot and few-shot settings:\n\n• Zero-Shot Learning: Here, we provide the model with a single example and ask it to predict the emotion without any additional references. We employed a classic data split approach:\n\n• Initial Development: We combined the training and development sets and shuffled the data. Subsequently, we used 70% for training, 10% for validation, and the remaining 20% as a held-out test set.\n\n• Competition Test Set Release: Upon the competition's test set release, we directly evaluated our models using the platform. To maximize training data, we trained on the combined training set with a 20% validation split.\n\n• Final Model: Once we selected our best model, we re-trained it on the entire dataset without validation. This re-training didn't yield significant improvements",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Subtask 1",
      "text": "We'll focus on Subtask 1, where we achieved strong results. The key hyperparameters used:\n\n• Batch Size: A batch size of 64 provided the best balance. Smaller sizes hurt performance, while larger sizes exceeded our memory constraints.\n\n• Fine-Tuning: We trained for 4 epochs with frozen model weights, followed by 3 epochs with only the last layer unfrozen (as detailed in section 3.1.3).\n\n• Classifier: Our classifier used 128 neurons, 0.5 dropout, and a softmax activation.\n\n• Optimization: We used cross-entropy loss, the AdamW optimizer, and experimented with different learning rates (see section 3.1.3).\n\n• Evaluation: We measured performance using the MulticlassF1Score with 8 classes and 'macro' averaging.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Our top-performing model (Table  3 ) was a finetuned FacebookAI/xlm-roberta-large  (Conneau et al., 2019) . This highlights the superiority of fine-tuned Masked Language Models (MLMs) over Mistral for sentence classification tasks. The results suggest that smaller Causal models remain less effective than fine-tuned MLMs in this domain. We also see that few-shot Mistral is worse than zeroshot, probably because too much data in the prompt confuses the model. In terms of number of epochs, our best model was overfitting when finetuned for too many epochs (Table  4 ) and we finally trained for 4 + 3 epochs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Error Analysis",
      "text": "Our confusion matrix (Figure  7 ) reveals that the model overpredicts the 'neutral' emotion, likely due to its prevalence in the training data. This created a bias, leading the model to misclassify instances of other emotions as 'neutral'. While we attempted to mitigate this with class weights in the loss function, it proves insufficient. In the future, we should explore more robust techniques like oversampling or undersampling to address the class imbalance. As seen in the emotion accuracy chart (Figure  8 ), the model performs best on the dominant 'neutral' class, along with well-represented emotions like 'joy' and 'sadness'. Conversely, the model struggles to predict the 'disgust' emotion, which aligns with its under-representation in the training data. This suggests a direct correlation between dataset frequency and model proficiency for each emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "Overall, our system achieved encouraging results in Subtask 1, despite exhibiting some overfitting for dominant labels. While performance on the emotion-flip detection tasks (Subtasks 2 and 3) highlights areas for improvement, we still placed in the first half of the leaderboard. Looking ahead, we plan to investigate hybrid transformer-LSTM architectures for a more nuanced understanding of emotion-flip triggers. Additionally, enriching the data by incorporating a broader conversational context through multi-turn analysis could enhance our model's capabilities. Not least, even though we tried Mistral, there are newer causal models like Mixtral  (Jiang et al., 2024)  and Solar  (Kim et al., 2023)  which could perform better at this type of task.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Three sub-tasks explained",
      "page": 1
    },
    {
      "caption": "Figure 2: Emotion Distribution Comparison between",
      "page": 2
    },
    {
      "caption": "Figure 3: Task 2: Emotion-flip counts",
      "page": 2
    },
    {
      "caption": "Figure 4: Figure 4: Model architecture",
      "page": 2
    },
    {
      "caption": "Figure 5: ). To optimize",
      "page": 3
    },
    {
      "caption": "Figure 6: ). Data preprocessing, (such as lemmati-",
      "page": 3
    },
    {
      "caption": "Figure 5: Distribution of utterances lengths.",
      "page": 3
    },
    {
      "caption": "Figure 6: Best model score with different maximum",
      "page": 3
    },
    {
      "caption": "Figure 7: ) reveals that the",
      "page": 5
    },
    {
      "caption": "Figure 7: Confusion matrix. On y-axis true labels, on",
      "page": 5
    },
    {
      "caption": "Figure 8: Accuracy by emotion. Accuracy directly",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: ). The dataset",
      "page": 1
    },
    {
      "caption": "Table 1: Datasets sizes used in this competition by tasks.",
      "page": 2
    },
    {
      "caption": "Table 2: Test scores of different classifiers.",
      "page": 3
    },
    {
      "caption": "Table 3: ) was a fine-",
      "page": 4
    },
    {
      "caption": "Table 3: Results for Subtask 1 - Masked Language",
      "page": 4
    },
    {
      "caption": "Table 4: ) and we finally trained for 4 + 3 epochs.",
      "page": 4
    },
    {
      "caption": "Table 4: Finding the optimal number of epochs to avoid",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "B Tom",
        "Benjamin Brown",
        "Nick Mann",
        "Melanie Ryder",
        "Jared Subbiah",
        "Prafulla Kaplan",
        "Arvind Dhariwal",
        "Pranav Neelakantan",
        "Girish Shyam",
        "Amanda Sastry",
        "Sandhini Askell",
        "Ariel Agarwal",
        "Gretchen Herbert-Voss",
        "Tom Krueger",
        "Rewon Henighan",
        "Aditya Child",
        "Daniel Ramesh",
        "Jeffrey Ziegler",
        "Clemens Wu",
        "Christopher Winter",
        "Mark Hesse",
        "Eric Chen",
        "Mateusz Sigler",
        "Scott Litwin",
        "Gray"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Unsupervised cross-lingual representation learning at scale"
    },
    {
      "citation_id": "3",
      "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "58th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "4",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "5",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "6",
      "title": "Mistral 7B",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7B",
      "arxiv": "arXiv:2310.06825"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Albert Jiang",
        "Alexandre Sablayrolles",
        "Antoine Roux",
        "Arthur Mensch",
        "Blanche Savary",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego De Las Casas",
        "Emma Hanna",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Bour",
        "Guillaume Lample",
        "Renard Lélio",
        "Lucile Lavaud",
        "Marie-Anne Saulnier",
        "Pierre Lachaux",
        "Sandeep Stock",
        "Sophia Subramanian",
        "Szymon Yang",
        "Teven Antoniak",
        "Théophile Le Scao",
        "Thibaut Gervet",
        "Thomas Lavril",
        "Timothée Wang",
        "Lacroix"
      ],
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "SOLAR 10.7B: Scaling large language models with simple yet effective depth upscaling",
      "authors": [
        "Dahyun Kim",
        "Chanjun Park",
        "Sanghoon Kim",
        "Wonsung Lee",
        "Wonho Song",
        "Yunsu Kim",
        "Hyeonwoo Kim",
        "Yungi Kim",
        "Hyeonju Lee",
        "Jihoo Kim",
        "Changbae Ahn",
        "Seonghoon Yang",
        "Sukyung Lee",
        "Hyunbyung Park",
        "Gyoungjin Gim",
        "Mikyoung Cha",
        "Hwalsuk Lee",
        "Sunghun Kim"
      ],
      "year": "2023",
      "venue": "SOLAR 10.7B: Scaling large language models with simple yet effective depth upscaling"
    },
    {
      "citation_id": "9",
      "title": "Semeval 2024 -task 10: Emotion discovery and reasoning its flip in conversation (ediref)",
      "authors": [
        "Shivani Kumar",
        "Shad Md",
        "Erik Akhtar",
        "Tanmoy Cambria",
        "Chakraborty"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer",
      "authors": [
        "Shivani Kumar",
        "Anubhav Shrimal",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2021",
      "venue": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer"
    },
    {
      "citation_id": "11",
      "title": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer",
      "authors": [
        "Shivani Kumar",
        "Anubhav Shrimal",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2022",
      "venue": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer",
      "doi": "10.1016/j.knosys.2021.108112"
    },
    {
      "citation_id": "12",
      "title": "Yige Xu, and Xuanjing Huang. 2020. How to fine-tune bert for text classification?",
      "authors": [
        "Chi Sun",
        "Xipeng Qiu"
      ],
      "venue": "Yige Xu, and Xuanjing Huang. 2020. How to fine-tune bert for text classification?"
    }
  ]
}