{
  "paper_id": "2112.06603v1",
  "title": "Detecting Emotion Carriers By Combining Acoustic And Lexical Representations",
  "published": "2021-12-13T12:39:53Z",
  "authors": [
    "Sebastian P. Bayerl",
    "Aniruddha Tammewar",
    "Korbinian Riedhammer",
    "Giuseppe Riccardi"
  ],
  "keywords": [
    "emotion carrier",
    "speech emotion recognition",
    "natural language understanding"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Personal narratives (PN) -spoken or written -are recollections of facts, people, events, and thoughts from one's own experience. Emotion recognition and sentiment analysis tasks are usually defined at the utterance or document level. However, in this work, we focus on Emotion Carriers (EC) defined as the segments (speech or text) that best explain the emotional state of the narrator (\"loss of father\", \"made me choose\"). Once extracted, such EC can provide a richer representation of the user state to improve natural language understanding and dialogue modeling. In previous work, it has been shown that EC can be identified using lexical features. However, spoken narratives should provide a richer description of the context and the users' emotional state. In this paper, we leverage word-based acoustic and textual embeddings as well as early and late fusion techniques for the detection of ECs in spoken narratives. For the acoustic word-level representations, we use Residual Neural Networks (ResNet) pretrained on separate speech emotion corpora and fine-tuned to detect EC. Experiments with different fusion and system combination strategies show that late fusion leads to significant improvements for this task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "People express and communicate emotions consciously as well as subconsciously. This is done by modifying the manner of speaking, the content of a conversation or written text, facial expressions, gestures, or even the way of walking. The combination of these signals, especially speech and text, has successfully been used to determine the emotional state of a person making a statement or telling a narrative  [1, 2] . Most of the emotion research in affective computing aims at categorical or continuous recognition of emotions. However, these tasks are not necessarily able to provide an explanation for the emotional state. Tammewar et al. defined the linguistic carrier of emotions and evaluated an annotation protocol in the context of PNs  [3] . Detecting the fragments from the narratives that best explain the emotional state of the narrator would provide deeper emotion analysis that has potential benefits in the context of mental well-being applications, to analyze a user's Personal Narratives (PN). This application of EC is described in  [4] . A conversational agent, as part of a mental well-being application, uses previously extracted ECs, from the dialogue with the user, and generates a tailored response. A deeper analysis of the the perceived emotion may help to ask better questions and get more information from the user. This additional information in turn may help to better understand a user's emotional state through a conversation and makes showing empathy towards the user easier.\n\nA previous analysis on how different fragments of PNs help explain the current mental state of the narrator in terms of valence can be found in  [5] . Not only fragments containing emotion words but also events (''high school exam\"), people (''grandpa\"), and actions (''made me choose\") were proven to be useful to predict the valence. Following up on this analysis, Emotion Carriers (EC) for PNs can be defined as the concepts that best explain and carry the emotional state of the narrator  [3] . ECs thus include not only explicitly emotionally charged words, such as ''happy\", but also mentions of people, places, objects, predicates, and events that carry an emotional weight within the context of a narrative. They performed the annotation of German PNs from the Ulm State-of-Mind in Speech (USoMs) corpus  [6]  with the emotion carriers  [3] . In Figure  1 , we explain and differentiate the three different emotion concepts: Emotion State, Emotion Lexicon, and Emotion carriers.\n\nFurther work on the automatic detection of emotion carriers from transcriptions of spoken PNs, can be found in  [4] . However, relying only on lexical features leaves out the possibility of the same lexical content conveying different things based on acoustic context.\n\nIvanov et al. showed that there is a relationship between meaning-bearing parts of utterances and their acoustic properties  [7] . Following up on that research, we have found evidence supporting distinct prosodic profiles for emotion vs non-emotion carriers: Figure  2  compares the spectrograms of two occurrences of the phrase ''vor die Wahl gestellt\"; \"made me choose\"; while (a) was annotated as an emotion carrier, (b) was not. The strong rise in fundamental frequency (f0), as well as the strong fluctuations at the beginning of Figure  2a , indicates emotional speech  [8] . In contrast, the same phrase Fig.  1 . This figure explains and differentiates the concepts of Emotion State, Emotion Lexicon, and Emotion Carriers. In the first example the emotional state 'sad' is directly described by the emotion lexicon using the word 'sad', whereas the second example does not use emotion lexicon to describe the emotional state 'excited', it is rather implicit and can be explained using emotion carriers. that was not marked as an EC has a very flat f0 contour (cf. Figure  2b ). While the figure provides only anecdotal and motivational evidence, in this paper we provide ample evidence of the complementarity of acoustic and lexical information.\n\nOur contributions are:\n\n• evidence for the acoustic discriminability of EC\n\n• word-level acoustic embeddings using a modified ResNet architecture and transfer learning from EmoDB\n\n• analysis of early and late fusion of textual and acoustic embeddings\n\n• rule-based late fusion based on posterior probabilities leveraging the strength of the lexical system",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "A related concept to EC are affective events (AE)  [9] . Affective events are a predefined set of events that have a stereotipycally (w/o specific context) positive or negative impact on the people who experience the event. In the set of AEs, an event is a tuple of subject, verb and object and the polarity is provided to the entire tuple, while ECs could be any entity or event, with no strict restriction on the syntactic categories, that carry the emotional state of the narrator. AEs are closely linked to the satisfaction or violation of human needs  [10] . While this can be true for EC, it is not a necessity, as EC can be identified in personal narratives, where only the recollection of events leads to the current emotional state.\n\nIdentifying what moves or creates an emotional reaction in a personal narrative can for example help a personalized mental health application ask questions about what is important w.r.t. the emotional state of the narrator. To our knowledge, there is no prior work on merging acoustic and lexical features for detecting AE.\n\nRelevant techniques for utilizing multiple modalities can be found in the related field of Speech Emotion Recognition (SER), where the use of multiple modalities is common  [11, 12, 13, 14] . Most research on Emotion Recognition (ER) is focused on classifying the emotion of a writer or narrator into discrete values such as fear, anger, or joy. This is usually done on an utterance, dialogue, or narrative level  [15] .\n\nThe SER task is concerned with the direct classification of a persons' emotional state whereas EC detection is looking for events, entities, and people that explain the subject's current emotional state. ECs are the linguistic vehicles of emotions and provide insights into the entities, people, and events that explain the narrators' emotional state. Related work focusing on acoustic cues in meaning structures can be found in  [7]  where the authors found a relationship between acoustics and meaning. Nastase et al. studied the relation between words that express emotions and the way they sound. The study found statistical evidence that phonetic features are useful in determining if words express the same emotion  [16] . Batliner et al. used word-level emotional labels and tried classifying emotional state of children on word and turn level, using acoustic features as well as a combination of lexical and acoustic features  [17] . Huang et al. combine word-level acoustic embeddings with Word2Vec for utterance-level emotion recognition  [18]  Current fusion approaches for multi-modal emotion recognition essentially follow a similar approach. The first step is finding appropriate intermediate representations, usually produced by using some kind of encoder neural network, and then, depending on the problem, train a classifier applying different fusion strategies  [11, 12, 13, 14] . For example, Pepino et al. use CNNs to extract sentence-level embeddings from pre-trained word embeddings and utterance-level embeddings using a CNN on handcrafted acoustic features comparing different fusion approaches  [11] .\n\n(a) emotion carrier (b) non emotion carrier Fig.  2 . Spectrograms with f0-contour of the phrase: \"vor die Wahl gestellt,\"(Translation: \"made me choose\"). (a) was marked as an EC, showing signs of emotional speech and the voice cracking in the center part whereas (b) was taken from the same recording session, but was not marked as EC. While this is an anecdotal example, statistical analysis revealed significant differences in f0, energy, and shimmer on nouns marked as EC when comparing them with all other nouns in the dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "All experiments presented to detect EC and analyses in this paper were conducted using the dataset and annotations described in  [3] . The dataset is based on the USoMs corpus described in the Interspeech 2018 ComParE paralingiustics challenge  [19] . The USoMs dataset consists of spoken PNs collected from 100 participants; audio data was converted to 16 kHz mono, and a noise profile was removed where necessary. Manual transcriptions were obtained from a professional transcription service (verbatim approach  [20] ), with a vocabulary of 6438 words. The data of 66 participants (239 PNs) was annotated with the ECs by four annotators, selecting emotion-carrying text spans using the transcript only  [3] . ECs are annotated with the inside-outside (IO) scheme which is common in natural language processing. Words that belong to a text span marked as EC are annotated as I others as O. The resulting dataset is heavily imbalanced with only 6.6 % of tokens marked as EC.\n\nAccurate time alignment of text to audio was produced by forced alignment (FA) using a speaker-adaptive HMM-GMM (Hidden Markov Model, Gaussian Mixture Model) automatic speech recognition system (ASR) based on  [21] . Missing entries in the pronunciation lexicon were generated using a grapheme-to-phoneme tool  [22]  where necessary.\n\nBefore running any classification and fusion experiments for the detection of EC, we performed an in-depth feature analysis. We focused on prosodic word-level features of nouns marked as EC, as only they were labeled in a significant number of cases being an EC (N = 15600, of which 2019 were EC nouns). While Fig.  2  provides anecdotal evidence only, we could identify significant differences between the nouns marked as EC and those that were not. F0, energy, and HNR (Harmonic to Noise Ratio) were extracted using Praat with the Parselmouth library and Jitter and Shimmer were calculated based on the extracted f0 using the method described in  [23] . Our analysis revealed significant differences on a word-level for mean f0 (and its derivatives), mean energy (and its derivatives) as well as shimmer using an independent two-sample t-test (p = 0.05).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "We follow the approach of finding representations for EC from different feature spaces. As EC are a word-, and phraselevel concept, we try to find appropriate representations from the linguistic and acoustic input feature space. The representations are then used in uni-modal experiments as well as multi-modal fusion experiments for EC recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Word-Based Textual Embeddings (Wte)",
      "text": "For word-based textual embeddings, we use 100-dimensional pre-trained GloVe word embeddings trained on the German Twitter corpus  [24] . A total of 656 (10.2%) words were not present in the pre-trained embeddings. The word-embeddings where fine-tuned on the actual task inside the cross-validation loops.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Word-Based Acoustic Embeddings (Wae)",
      "text": "The previously performed feature analysis revealed differences between EC and non EC with respect to handcrafted acoustic features. This motivated us to use embeddings based on convolutional neural networks (CNN) and handcrafted acoustic features as described in  [11] . Our early stage experiments failed to produce good results on an utterance level with the German EmoDB dataset  [25]  on the speech emotion recognition (SER) task.\n\nThe failure to produce good results with CNNs and handcrafted acoustic features led us to explore other network architectures and input features. We decided to use a ResNet architecture which was successfully applied to a number of speech applications such as speaker recognition, and SER and has been shown to produce good embeddings  [26, 27] . ResNets are fully convolutional neural networks (FCN) and can handle inputs of different sizes (or lengths, respectively) due to a global pooling layer at the end of the convolutional part of the neural network. The network we use is very similar to the one described in  [28]  and consists of 18 convolutional blocks (ResNet18). Its architecture was adapted by removing the initial max-pooling layer to keep more features prior to the residual blocks as the expected inputs are already relatively small. The dimensionality of the embedding layer was reduced from 1000 to 512 and the final classification layer was altered to match the number of classes (2).\n\nAs acoustic input features for the ResNet, we extract 40dimensional Mel-frequency cepstrum coefficients (MFCC) with a window length of 0.025 s, a frameshift of 0.01 s, along with 1st and 2nd order moments, stacking them to a tensor with three dimensions (frequency x time x moments) and apply z-score normalization. Those acoustic input features are then used to train the acoustic only classifier and to extract neural acoustic word embeddings from the trained acoustic encoder. The word-based acoustic contexts are extracted using the aforementioned FAs.\n\nThe network was pre-trained on short utterances from the German EmoDB corpus to differentiate between neutral and emotional speech  [25] . This is done to learn filters that are already primed to extract features from speech that are important to classify emotional speech. Both pre-training and training were done using stochastic gradient descent with a cross entropy loss function. To overcome the class imbalance problem, an oversampling strategy was applied as it had proven to be the best performing technique in our experiments.\n\nTo obtain word-based acoustic embeddings (WAE), we froze to resulting model to act as an acoustic word encoder. We feed the word-based acoustic context to the model and extract WAE from the embedding layer of the model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Sequence Tagging",
      "text": "We model the task of detecting emotion carriers as a binary sequence labeling problem using both modalities, with targets encoded as I if the token is part of an EC and O otherwise. For this task, we adopt a bidirectional Long Short-Term Memory (LSTM) neural network with an attention-based sequence tagging (ST) architecture previously used for labeling candidates for emphasis in written text  [29] . 1 An overview of the architecture is located in the central part of Fig.  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fusion",
      "text": "There are two main challenges in combining multiple modalities: How to combine features of different dimensionality and valuation, and at which stage to combine the streams. In general, three different kinds of approaches can be differentiated: early, late, and decision-level fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Early And Late Fusion",
      "text": "In early fusion (EF), features for each modality are extracted separately, i.e. each modality represents a view of the same concept. The resulting feature vectors are then combined, e.g. by concatenation or stacking, and then treated as a single input channel. In late fusion (LF), each modality has its own model and is often trained independently. The outputs of those classifiers are used as input to another classifier that combines them for an overall best prediction. EF as used in this paper can be found at the center of Figure  3  and the LF approach can be found at the top right.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Decision Level Fusion",
      "text": "In our experiments the sequence tagger using WTE is trained as a regression problem with the Kullback-Leibler (KL) divergence, predicting the probability of a token being an emotion carrier. For this, the best decision threshold was experimentally found to be p db = 0.15 for lexical features only. This motivated us to explore a rather heuristic late fusion approach: a rule-based cascaded classifier based on posterior probabilities. Applying a similar technique to the normalized probabilities in the output of the ResNet classifier, we can find a decision threshold and then merge the decisions, defining decision states around these thresholds. This way it is possible to leverage long-range lexical information as well as local acoustic information.\n\nWe define the lexical-based ST model to be the primary model and the ResNet classifier to be the disambiguator, leveraging local acoustic information. In our merging approach, we define an parameter that indicates how certain a classifier is with the decision if a token is an EC. The decision boundary (DB) is defined by setting a probability value p db .\n\nWe only consider the probability for the EC to determine certainty. If the normalized probability of a token being an EC p ec is within the epsilon interval (p db ± ) the classifier is considered to be uncertain regarding a positive decision of a token being an EC. p ec > p db + is considered to be certain. Those certainty indicators are computed for both models separately. Merging is then done by checking certainty indicators: If the lexical model is certain, the token is considered 1 Implementation: https://tinyurl.com/seqtagging to be an EC. If the lexical model is uncertain and the acoustic model is certain, the token is also considered as an EC. In all other cases, the token is not considered as an EC. We call this heuristic decision level fusion (DLF).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "Results for single modality, fusion experiments, and baselines, are reported in Tab. 1. As this is an information retrieval task, we report metrics for class I in this unbalanced task. The equal priors baseline constitutes random guessing with no knowledge about the actual class distribution with p I = p O = 0.5, resembling a fair coin toss whereas the class priors baselines resembles a heavily biased coin with p I = 0.066 and p O = 0.934.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Details",
      "text": "All experiments were performed using five-fold cross-validation with consistent folds across all experiments. The folds were split by speaker to ensure no speaker in the test set was present in the training set and hyper-parameters were tuned on separate development folds, as is common when working with acoustic data and small datasets. Tab. 1 contains results for single modality classification using a ResNet classifier as well as ST using either WTE or WAE as inputs. We report one result for an EF experiment concatenating WTE and WAE to a single word vector as well as a logit-based LF experiment combining the ResNet classifier and the ST using WTE as inputs only. Lastly, we show our overall best results, obtained with DLF and oracle results. Oracle results are obtained by a fictitious fusion of classifiers, which is considered to be right, if at least one of the contributing classifiers (ResNet18 and ST WTE), predicted the correct label. Details of the proposed neural network architectures can be found in Fig.  3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Direct word-level EC detection using only MFCC features (ResNet18) improved results compared to both random baseline classifiers using class priors for both actual class priors in the dataset as well as equal class priors. It can therefore be assumed that useful representations can be extracted from the embedding layer of the ResNet classifier. The analysis of the word-based acoustic embeddings produced by the ResNet system also looked promising. Fig.  4  contains a t-distributed Stochastic Neighbor Embedding (t-SNE) plot of embeddings marked as EC vs. embeddings not marked as EC. The plot shows that there is potential to differentiate EC from non-EC tokens in this low-dimensional projection.\n\nWhile we achieved good results with the ST using WTE only, results for the WAE failed to perform better than the ResNet classifier that solely relied on local acoustic information. It barely improved results compared to the random clas-sifiers' expected baseline precision. We, therefore, decided to not use the ST with WAE in late fusion experiments and rather use the ResNet classifier in LF.\n\nThe EF experiment combining WAE and WTE performed worse than WTE alone as described in this paper and only slightly improved previous WTE only results  [4] . The LF experiment using logit outputs from the ResNet classifier and the ST using WTE improved the ST using WTE only in terms of recall, but lowered the precision which lead to overall worse results w.r.t. F1-I. Experiments with Logistic Regression to model the probability of a word being an EC using the logit outputs of the ST using WTE and the ResNet classifier did not improve over the LF experiment with the FCNN.\n\nUnfortunately, the experiments using the standard EF and LF approaches couldn't improve over the already strong textual system (ST WTE). However as shown in Fig.  2  and the feature analysis, there definitely is evidence that acoustic information can help with the detection of EC. Our experiments with the word-level ResNet classifier could not completely convince but still beat all statistical baselines as a stand-alone system. Lastly oracle results presented in Tab. 1 show that the combination of the ResNet classifier and the ST using WTE has a lot of room for improvements still. This led us to explore the rather heuristic decision level fusion (DLF) approach described in 4.4.2 and yielded the best overall results. The decision boundary was tuned for the ResNet classifier only since the ST using WTE was already trained using KL divergence with a tuned decision boundary at p db = 0.15. The DB for the ResNet classifier was determined using 5-fold cross-validation. Results are reported in Tab. 1 (DLF). The decision boundary for certainty of the ResNet classifier was found to be p DB = 0.75 with = 0.05.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "With a strong lexical baseline and the promising results from previous work, we were convinced that ordinary fusion strategies would help to improve our results. The high recall on the acoustic ResNet18 system was encouraging. However, the results for EF and LF experiments suggest that simply adding the acoustic representations, extracted from the ResNet, adds a lot of entropy that the system in its current architecture can't handle, yielding worse accuracy than the textual system.\n\nThe analysis of the extracted representations and our knowledge about the existence of acoustic cues led us to explore heuristic ways to combine the modalities. The final DLF experiments show that the accuracy of the lexical model with its knowledge about context and content of a narrative could be improved by relying on local acoustic information in case of uncertainty.\n\nThe task of detecting EC is an important step towards a deeper understanding and better modeling of a person's emotional state; ECs can also benefit natural language understanding tasks such as dialog modeling. Combining acoustic and lexical modalities yields higher accuracy than the uni-modal approaches to this difficult task if done the right way. We could show that local acoustic information alone is not reliable to detect EC but helps to improve results when combined with a text-based system that captures long-range semantic relations.\n\nThe research on acoustic cues of emotion carriers is still in the initial stage. In future work, we will look into more effective word-level acoustic representations that can be used in typical fusion approaches.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we explain and differentiate the three different emo-",
      "page": 1
    },
    {
      "caption": "Figure 2: compares the spectrograms of",
      "page": 1
    },
    {
      "caption": "Figure 1: This ﬁgure explains and differentiates the concepts of Emotion State, Emotion Lexicon, and Emotion Carriers. In the",
      "page": 2
    },
    {
      "caption": "Figure 2: b). While the ﬁgure provides only anecdotal and mo-",
      "page": 2
    },
    {
      "caption": "Figure 2: Spectrograms with f0-contour of the phrase: “vor die Wahl gestellt,”(Translation: “made me choose”). (a) was",
      "page": 3
    },
    {
      "caption": "Figure 2: provides anecdotal evi-",
      "page": 3
    },
    {
      "caption": "Figure 3: Overview of the neural network architecture used in the experiments. The left part shows the ResNet classiﬁer",
      "page": 4
    },
    {
      "caption": "Figure 3: 4.4. Fusion",
      "page": 5
    },
    {
      "caption": "Figure 3: and the LF",
      "page": 5
    },
    {
      "caption": "Figure 3: 5.2. Results",
      "page": 5
    },
    {
      "caption": "Figure 4: contains a t-distributed",
      "page": 5
    },
    {
      "caption": "Figure 4: t-SNE plot for the word-based acoustic embeddings",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Precision, Recall and F1 scores for class I of EC",
      "data": [
        {
          "Model": "Baseline",
          "Features": "equal priors",
          "Prec-I": "6.6",
          "Recall-I": "50.0",
          "F1-I": "0.12"
        },
        {
          "Model": "Baseline",
          "Features": "class priors",
          "Prec-I": "6.6",
          "Recall-I": "6.6",
          "F1-I": "0.07"
        },
        {
          "Model": "ResNet18",
          "Features": "MFCCs",
          "Prec-I": "19.4 (5.3)",
          "Recall-I": "64.6 (14.6)",
          "F1-I": "0.29 (0.05)"
        },
        {
          "Model": "ST",
          "Features": "WAE",
          "Prec-I": "7.6 (2.3)",
          "Recall-I": "40.3 (9.0)",
          "F1-I": "0.13 (0.03)"
        },
        {
          "Model": "ST",
          "Features": "WTE",
          "Prec-I": "37.9 (6.9)",
          "Recall-I": "46.7 (6.4)",
          "F1-I": "0.41 (0.03)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Vocalic, lexical and prosodic cues for the interspeech 2018 selfassessed affect challenge",
      "authors": [
        "Claude Montacié",
        "Marie-José Caraty"
      ],
      "year": "2018",
      "venue": "Proc. Annual Conference of the Int'l Speech Communication Association (INTER-SPEECH)"
    },
    {
      "citation_id": "3",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Annotation of emotion carriers in personal narratives",
      "authors": [
        "Aniruddha Tammewar",
        "Alessandra Cervone",
        "Eva-Maria Messner",
        "Giuseppe Riccardi"
      ],
      "year": "2020",
      "venue": "Proceedings of The 12th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "5",
      "title": "Emotion carrier recognition from personal narratives",
      "authors": [
        "Aniruddha Tammewar",
        "Alessandra Cervone",
        "Giuseppe Riccardi"
      ],
      "year": "2021",
      "venue": "Emotion carrier recognition from personal narratives"
    },
    {
      "citation_id": "6",
      "title": "Modeling user context for valence prediction from narratives",
      "authors": [
        "Aniruddha Tammewar",
        "Alessandra Cervone",
        "Eva-Maria Messner",
        "Giuseppe Riccardi"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "State of mind: Classification through self-reported affect and word use in speech",
      "authors": [
        "Eva-Maria Rathner",
        "Yannik Terhorst",
        "Nicholas Cummins",
        "Björn Schuller",
        "Harald Baumeister"
      ],
      "year": "2018",
      "venue": "Proc. Annual Conference of the Int'l Speech Communication Association (INTER-SPEECH)"
    },
    {
      "citation_id": "8",
      "title": "Acoustic Correlates of Meaning Structure in Conversational Speech",
      "authors": [
        "Alexei Ivanov",
        "Giuseppe Riccardi",
        "S Ghosh",
        "S Tonelli",
        "E A Stepanov"
      ],
      "year": "2010",
      "venue": "Proc. Annual Conference of the Int'l Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "9",
      "title": "F0contours in emotional speech",
      "authors": [
        "A Paeschke",
        "Miriam Kienast",
        "W Sendlmeier"
      ],
      "year": "1999",
      "venue": "Psychology"
    },
    {
      "citation_id": "10",
      "title": "Acquiring knowledge of affective events from blogs using label propagation",
      "authors": [
        "Haibo Ding",
        "Ellen Riloff"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Why is an event affective? classifying affective events based on human needs",
      "authors": [
        "Haibo Ding",
        "Tianyu Jiang",
        "Ellen Riloff"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "12",
      "title": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer",
        "Agustin Gravano"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Deep Hierarchical Fusion with Application in Sentiment Analysis",
      "authors": [
        "Efthymios Georgiou",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
      ],
      "year": "2019",
      "venue": "Proc. Annual Conference of the Int'l Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "14",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Long Papers"
    },
    {
      "citation_id": "15",
      "title": "Speech Emotion Recognition From 3D Log-Mel Spectrograms With Deep Learning Network",
      "authors": [
        "Tianhao Hao Meng",
        "Fei Yan",
        "Hongwei Yuan",
        "Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "17",
      "title": "Do Happy Words Sound Happy? A study of the relation between form and meaning for English words expressing emotions",
      "authors": [
        "Vivi Nastase",
        "Marina Sokolova",
        "Jelber Sayyad"
      ],
      "year": "2007",
      "venue": "Proc. Recent Advances in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Combining Efforts for Improving Automatic Classification of Emotional User States",
      "authors": [
        "Anton Batliner",
        "Stefan Steidl",
        "Bjorn Schuller",
        "Dino Seppi",
        "Kornel Laskowski",
        "Thurid Vogt",
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Noam Amir"
      ],
      "year": "2006",
      "venue": "Proc. IS-LTC"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition using convolutional neural network with audio word-based embedding",
      "authors": [
        "K Huang",
        "C Wu",
        "Q Hong",
        "M Su",
        "Y Zeng"
      ],
      "year": "2018",
      "venue": "2018 11th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "20",
      "title": "The INTERSPEECH 2018 Computational Paralinguistics Challenge: Atypical and Self-Assessed Affect, Crying and Heart Beats",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Peter Marschik",
        "Harald Baumeister",
        "Fengquan Dong",
        "Simone Hantke",
        "Florian Pokorny",
        "Eva-Maria Rathner",
        "D Katrin",
        "Christa Bartl-Pokorny",
        "Dajie Einspieler",
        "Alice Zhang",
        "Shahin Baird",
        "Kun Amiriparian",
        "Qian",
        "Maximilian Zhao Ren",
        "Panagiotis Schmitt",
        "Stefanos Tzirakis",
        "Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proc. Annual Conference of the Int'l Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "21",
      "title": "Praxisbuch Transkription : Regelsysteme, Software und praktische Anleitungen für qualitative ForscherInnen",
      "authors": [
        "Thorsten Dresing",
        "Thorsten Pehl"
      ],
      "year": "2011",
      "venue": "Praxisbuch Transkription : Regelsysteme, Software und praktische Anleitungen für qualitative ForscherInnen"
    },
    {
      "citation_id": "22",
      "title": "Open source automatic speech recognition for german",
      "authors": [
        "Benjamin Milde",
        "Arne Köhn"
      ],
      "year": "2018",
      "venue": "Proceedings of ITG 2018"
    },
    {
      "citation_id": "23",
      "title": "Joint-sequence models for grapheme-to-phoneme conversion",
      "authors": [
        "Maximilian Bisani",
        "Hermann Ney"
      ],
      "year": "2008",
      "venue": "Speech communication"
    },
    {
      "citation_id": "24",
      "title": "Using Jitter and Shimmer in speaker verification",
      "authors": [
        "M Farrús",
        "J Hernando"
      ],
      "year": "2009",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "26",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "27",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "An end-toend deep learning framework for speech emotion recognition of atypical individuals",
      "authors": [
        "Dengke Tang",
        "Junlin Zeng",
        "Ming Li"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Learning emphasis selection for written text in visual media from crowd-sourced label distributions",
      "authors": [
        "Amirreza Shirani",
        "Franck Dernoncourt",
        "Paul Asente",
        "Nedim Lipka",
        "Seokhwan Kim",
        "Jose Echevarria",
        "Thamar Solorio"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}