{
  "paper_id": "2209.07629v2",
  "title": "Self-Relation Attention And Temporal Awareness For Emotion Recognition Via Vocal Burst",
  "published": "2022-09-15T22:06:42Z",
  "authors": [
    "Dang-Linh Trinh",
    "Minh-Cong Vo",
    "Guee-Sang Lee"
  ],
  "keywords": [
    "vocal burst",
    "self-supervised learning",
    "selfrelation attention",
    "temporal awareness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The technical report presents our emotion recognition pipeline for high-dimensional emotion task (A-VB High) in The ACII Affective Vocal Bursts (A-VB) 2022 Workshop & Competition. Our proposed method contains three stages. Firstly, we extract the latent features from the raw audio signal and its Mel-spectrogram by self-supervised learning methods. Then, the features from the raw signal are fed to the self-relation attention and temporal awareness (SA-TA) module for learning the valuable information between these latent features. Finally, we concatenate all the features and utilize a fully-connected layer to predict each emotion's score. By empirical experiments, our proposed method achieves a mean concordance correlation coefficient (CCC) of 0.7295 on the test set, compared to 0.5686 on the baseline model. The code of our method is available at https://github.com/linhtd812/A-VB2022.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human speech is one of the most valuable resources to help identify people's emotions or feelings  [7] . There are several kinds of human speech, including verbal and nonverbal. Recently, tremendous research has been conducted in speech emotion recognition with verbal speech, which is applied in human-computer interfaces. Non-verbal speech, known as vocal burst (VB), is the voice signal without meaning by a human being but could be translated to words such as laughter, groans, and grunts. However, the research on VB field is sparse because of the lack of data related to nonverbal human speech. Therefore, to discover the new trend of speech emotion recognition (SER), A-VB 2022 competition  [2]  provides us with the HUME-VB corpus  [1]  to find the meaning of VB related to people's emotions. For example, laughter could have some related emotion like amusement or triumph.\n\nIn supervised learning, data augmentation could enlarge the scale of data for over-fitting prevention and model generalization improvements  [8] . Besides, self-supervised learning  [14]  is a trending method, which could learn the generic representation from large-scale data without manual annotations. From this point, recent research proved that the SSL model could achieve competitive results compared to the supervised learning method  [16] . Also, the pre-trained SSL model is Correspondence to: Guee-Sang Lee (gslee@jnu.ac.kr) utilized for feature extracting in many downstream applications  [9] . Attention mechanisms significantly impact deep learning models in many fields, which enrich the information the model could learn from inputs  [15] . Attention mechanisms can select, modulate, and focus on the information most important to the target of our problem, like human attention  [10] . Therefore, this paper will investigate the effectiveness of data augmentation, SSL models, and attention modules on emotion recognition via vocal burst.\n\nThis technical report focuses on the first task of the A-VB 2022 competition: the High-Dimensional Emotion Task (A-VB High). This task's target is to predict the scores of 10 emotions. In our proposed method, several techniques and our contribution are listed below.\n\n• We investigate the efficiency of self-supervised learning (SSL) for extracting the latent features from both raw audio signal and its Mel-spectrogram by applying HuBERT  [6]  and DINO  [7]  models. • The Self-Relation Attention and Temporal Awareness (SA-TA) module helps captures the meaningful information from not only essential parts in audio signal but also the temporal information of latent features extracted from HuBERT  [6]  model. • The result improves slightly by utilizing a Melspectrogram containing the information related to the frequency and loudness of VB. The paper contains the list of content as follows. Section 2 describes the data analysis and pre-processing of this data. After that, the architecture of the proposed method is described in detail in section 3. Experimental results are shown in section 4, and section 5 concludes the overall paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Data Analysis And Pre-Processing",
      "text": "The Hume Vocal Burst Database (H-VB)  [1]  is utilized for the ACII A-VB 2022 challenge, which consists of 59,201 nonvocal audio from 1,702 speakings from 4 different cultures, including the U.S, South Africa, China, and Venezuela. Also, the dataset is split into the train, validation, and test subsets. The labels for the A-VB High task are the scores for each emotion, and we evaluate the results based on the mean CCC metric over ten emotion scores. There are ten basic emotions for the A-VB High task: Awe, Excitement, Amusement, There are two audio forms, including .wav and .webm files (a compressed format). Our team utilizes the .wav format with a sample rate of 16kHZ. For pre-processing, we trim the silence in the audio file and set the duration of the audio input is 3.5 seconds. Then, we apply some augmentation techniques to a raw audio signal, including random pitch shift and random time warping, to enlarge the scale of the data. If the duration after trimming is smaller than 3.5 secs, we add zero-padding at the beginning of the audio file. Otherwise, we randomly cut the sample into an audio file with a duration of 3.5 secs. Also, after applying the pre-processing, we transform the processed audio signal to Mel-spectrogram as the other input of our proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we describe the architecture of our proposed method. Firstly, we introduce our approach generally. After that, each component of this pipeline will be described in detail.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Architecture Overview",
      "text": "Our proposed method is shown in Fig.  1 . The input for this architecture is the pre-processed audio waveform and the Mel-spectrogram (mentioned in the previous section). Then, the self-supervised learning method includes Hidden-Unit Bert  [6] , and DINO  [7]  for extracting the latent features from original inputs. While HuBERT is applied for the audio signal, Mel-spectrogram is treated as images, and DINO is utilized to extract features from these input types. After that, latent features extracted from HuBERT are fed into an SA-TA module to accentuate the vital time point in each audio signal. Finally, we concatenated all the features and fed them in FC layers to predict the score of each emotion individually.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Features Extraction By Self-Supervised Learning Method",
      "text": "Self-supervised learning (SSL) is the method that learns from unlabeled sample data. Recently, SSL has been utilized as a pre-task to learn nontrivial data representations. Inspired by  [1] , we explore two pre-trained SSL models, HuBERT  [6]  for audio signal and DINO  [7]  for its Mel-spectrogram. For audio signal, we hypothesize that HuBERT can capture the general information, not only acoustic information, but also the phonemes of VB. Besides, by utilizing the Mel-spectrogram, we capture helpful information on the frequency and loudness of the sound. Furthermore, by using pre-trained models on large-scale dataset like Hubert and DINO, we can fine-tune, which lead to better latent features for the following stages in our proposed method. All the pre-trained models are based on the Transformer achitecture. While HuBERT model is pretrained on the Libri-light dataset  [17]  for speech recognition without supervision, DINO pre-trained weights on the Google Landmark v2 dataset  [18]  are utilized for extracting features from the Mel-Spectrogram of audio signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Self-Relation Attention And Temporal Awareness Module",
      "text": "The Self-Relation Attention and Temporal Awareness (ST-TA) module consists of two parts, including Self-Relation Attention, which teaches self-attention for each feature and the relationship between all the time-point features. Self-Relation Attention is inspired by  [3] . We hypothesize that this could automatically capture the vital part of each latent feature because the vocal burst is concise, and the meaningful information only appears for a short time, not all the duration of an audio sample. Furthermore, we can consider latent features extracted from HuBERT  [6]  as time-series data so that the Gated Recurrent Unit (GRU)  [4]  is considered a Temporal Awareness Part for capturing temporal information of these features.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Multi-Labels Regression Module",
      "text": "The latent features from DINO  [7]  go through the Global Pooling module to be a one-dimensional vector. After that, we concatenated all the features from the previous module and fed them into fully-connected (FC) layers. Because of the multiregression problem, we use ten separate FC layers to predict the score of each emotion. The score of each emotion is a range of (0,1).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Loss Function",
      "text": "Because all the test results are evaluated by Concordance Correlation Coefficient (CCC) metric, our loss function is  designed based on the CCC metric below. CCC is the concordance between prediction (1) and the ground truth (2), which identifies the agreement between two variables.\n\nwhere σ, µ and σ 12 are denoted by mean, standard deviation and their covariance between predicted score and ground truth, respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "As input features, we use both raw audio signal and its Mel-spectrogram. Some function in the Torchaudio package augments each raw audio signal. Through the experiments, the Adam optimizer is applied with a learning rate of 1e-5, and early stopping is utilized with an patience of 10 epochs to prevent over-fitting. Also, the learning rate is halved if the loss on the validation dataset does not decrease. The maximum epochs of the training process are set to 50, and the batch size is 4. All the model is trained with Nvidia RTX 2080Ti GPU and Pytorch 1.7.1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Results",
      "text": "Firstly, we investigate the efficiency of recent SSL models for audio signal including Wav2vec2-large  [11]  and HuBERTlarge  [6] . These two models were trained on public large dataset such as Libri-Light and Librispeech.\n\nFrom Table  I , the HuBERT model is better for non-verbal emotion recognition tasks than Wav2Vec2. Also, the large version of SSL models is chosen because the effectiveness is illustrated in previous research  [13] .\n\nNon-verbal speech is always expressed in a short duration. Therefore, the valuable information is only in a short time or several time-point during the audio signal. Therefore, we use the SA-TA module to help the model focus on valuable timepoint from latent features extracted from HuBERT. Using this module, the average CCC on the validation dataset increases by 0.2 compared to using only the HuBERT model for extracting features. The result of the SA-TA module is shown in Table  I .\n\nFinally, by using DINO for extracting features from Melspectrogram and global pooling module, we get slightly better results than the baseline model, which achieves 0.5920 of the mean CCC metric. However, using the feature from the DINO model is not good compared to those from HUBERT because the dataset for DINO is in another domain, not trained in the speech signal domain like the HUBERT pre-trained model. By combining both features from DINO and HUBERT model, we find that the improvement of mean CCC by up to around 0.005 compared to using only raw audio signal. The result shows that the information about the frequency and the loudness of the sound is valuable for emotion prediction related to non-verbal human speech.\n\nWe evaluate the CCC metric on each emotion by applying the proposed method, shown in Table  II . From the empirical experiment, our method works better on Awe and Surprise emotions than others. Furthermore, the result on the test dataset is 0.7295, almost the same as the validation dataset, which means the model has a good generalization ability.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "The technical report shows the emotion recognition method for vocal burst submitted to the High-Dimensional Emotion Task of A-VB 2022 challenge. The proposed architecture use SSL models to extract latent feature from raw signal and its Mel-spectrogram. ST-TA module is applied to focus on the critical time-point from latent features and use multiregression FC layers to individually predict scores of ten emotions. Experiment results show that our proposed method achieves 0.7295 mean CCC compared to 0.5686 of the baseline model from the competition organizer.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall Architecture of our proposed method",
      "page": 2
    },
    {
      "caption": "Figure 1: The input for",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Baseline [2]\nWav2Vec2-large\nHuBERT-large\nDINO\nHuBERT-large + SA-TA\nHuBERT-large + DINO + SA-TA",
          "Mean CCC": "0.5638\n0.6902\n0.7012\n0.5920\n0.7265\n0.7303"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Hume Vocal Burst Competition Dataset (H-VB) -Raw Data",
      "authors": [
        "Alan Cowen",
        "Alice Bard",
        "Panagiotis Tzirakis",
        "Michael Opara",
        "Lauren Kim",
        "Jeff Brooks",
        "Jacob Metrick"
      ],
      "year": "2022",
      "venue": "The Hume Vocal Burst Competition Dataset (H-VB) -Raw Data"
    },
    {
      "citation_id": "2",
      "title": "The ACII 2022 Affective Vocal Bursts Workshop and Competition: Understanding a critically understudied modality of emotional expression",
      "authors": [
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Anton Batliner",
        "Björn Schuller",
        "Dacher Keltner",
        "Alan Cowen"
      ],
      "year": "2022",
      "venue": "The ACII 2022 Affective Vocal Bursts Workshop and Competition: Understanding a critically understudied modality of emotional expression"
    },
    {
      "citation_id": "3",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "Debin Meng",
        "Xiaojiang Peng",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "2019 IEEE international conference on image processing (ICIP)"
    },
    {
      "citation_id": "4",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "5",
      "title": "Exploring the Effectiveness of Self-supervised Learning and Classifier Chains in Emotion Recognition of Nonverbal Vocalizations",
      "authors": [
        "Detai Xin",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "year": "2022",
      "venue": "Exploring the Effectiveness of Self-supervised Learning and Classifier Chains in Emotion Recognition of Nonverbal Vocalizations",
      "arxiv": "arXiv:2206.10695"
    },
    {
      "citation_id": "6",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning And Hsu",
        "Benjamin Bolte",
        "Yao-Hung Tsai",
        "Hubert",
        "Lakhotia",
        "Ruslan Salakhutdinov",
        "Abdelrahman Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "Mathilde Caron",
        "Hugo Touvron",
        "Ishan Misra",
        "Hervé Jégou",
        "Julien Mairal",
        "Piotr Bojanowski",
        "Armand Joulin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "Charles Darwin",
        "Phillip Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "9",
      "title": "Deep speaker conditioning for speech emotion recognition",
      "authors": [
        "Andreas Triantafyllopoulos",
        "Shuo Liu",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "2021 IEEE international conference on multimedia and expo (ICME)"
    },
    {
      "citation_id": "10",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "Shuo Liu",
        "Adria Mallol-Ragolta",
        "Emilia Parada-Cabeleiro",
        "Qian",
        "Jing Kun",
        "Xin Kathan",
        "Alexander Hu",
        "Bin Schuller"
      ],
      "year": "2022",
      "venue": "Audio self-supervised learning: A survey",
      "arxiv": "arXiv:2203.01205"
    },
    {
      "citation_id": "11",
      "title": "Attention, please! A survey of neural attention models in deep learning",
      "authors": [
        "Alana De Santana Correia",
        "Esther Colombini",
        "Luna"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "12",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Wierstorf",
        "Hagen Schmitt",
        "Maximilian Eyben",
        "Florian Schuller"
      ],
      "year": "2022",
      "venue": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "arxiv": "arXiv:2203.07378"
    },
    {
      "citation_id": "14",
      "title": "Self-supervised learning of pretext-invariant representations",
      "authors": [
        "Ishan Misra",
        "Laurens Maaten",
        "Van Der"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "An attention enhanced graph convolutional lstm network for skeleton-based action recognition",
      "authors": [
        "Chenyang Si",
        "Wentao Chen",
        "Wei Wang",
        "Liang Tan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "A survey on contrastive self-supervised learning",
      "authors": [
        "Ashish Jaiswal",
        "Ashwin Babu",
        "Ramesh",
        "Mohammad Zadeh",
        "Zaki",
        "Debapriya Banerjee",
        "Fillia Makedon"
      ],
      "year": "2020",
      "venue": "A survey on contrastive self-supervised learning"
    },
    {
      "citation_id": "17",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "Jacob Kahn",
        "Morgane Rivière",
        "Weiyi Zheng",
        "Evgeny Kharitonov",
        "Qiantong Xu",
        "Pierre-Emmanuel And Mazaré",
        "Julien Karadayi",
        "Liptchinsky",
        "Vitaliy",
        "Collobert",
        "Christian Ronan And Fuegen",
        "Others"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval",
      "authors": [
        "Tobias Weyand",
        "Andre Araujo",
        "Bingyi Cao",
        "Jack Sim"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    }
  ]
}