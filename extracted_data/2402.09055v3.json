{
  "paper_id": "2402.09055v3",
  "title": "Comment-Aided Video-Language Alignment Via Contrastive Pre-Training For Short-Form Video Humor Detection",
  "published": "2024-02-14T10:05:19Z",
  "authors": [
    "Yang Liu",
    "Tongfei Shen",
    "Dong Zhang",
    "Qingying Sun",
    "Shoushan Li",
    "Guodong Zhou"
  ],
  "keywords": [
    "Humor Detection",
    "Short-form Video",
    "Dataset",
    "Interactive Comments",
    "Video-Language Alignment",
    "Contrastive Pre-Training"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of shortform video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multimodal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset and code release at https://github.com/yliu-cs/CVLA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Short-form video platforms such as DouYin (globally known as Tik-Tok), Kwai, Snapchat, and Snack have risen as novel entertainment avenues, offering users the ability to view, create, comment on, and share captivating content typically lasting from a few seconds to several minutes. The meteoric rise in these platforms' popularity has normalized the consumption of short-form videos as a quick respite during leisure time  [43] . Accurately detecting the humorous aspects within these videos can greatly augment a platform's capacity to deliver targeted and tailored content recommendations that align with user preferences. To this end, employing multi-modal approaches for the automatic detection of humor in short-form videos is imperative  [20, 24] .\n\nHowever, we argue that short-form video humor detection (SVHD) currently faces at least three primary challenges: 1) The scale of unlabeled data currently utilized for pre-training is inadequate for ensuring the robustness of pre-trained models; 2) Conventional video humor detection approaches typically rely on pre-processed data features  [19, 27] , where the additional step of feature extraction can introduce instability in data interpretation and increase time and resource expenditure; 3) The discrepancies may arise between the semantics of vision and language modalities. For instance, as depicted in Figure  1 , the video and title alone provide insufficient cues for humor prediction. Moreover, the distinctive phrase pants all gone significantly contributes to humor recognition in this short-form video. Additionally, the presence of elements typically associated with non-humor, such as security guards and high-rise buildings, contrasts with the clear humorous cue provided by the phrase pants all gone in the textual content. This indicates that when the semantics of different modalities conflict, comments can often provide supplemental background knowledge that aids in the accurate interpretation of the video. Consequently, we posit that aligning the semantics of multiple modalities as consistently as possible is crucial to integrate them from diverse modal perspectives into an appropriate multi-modal representation, thereby enhancing the accuracy of humor detection.\n\nTo address these issues, we first enlarge the unlabeled data scale for pre-training in DY24h  [27]  to alleviate the dependency on annotation in SVHD task. Then, we propose a novel hierarchical Comment-aided Video-Language Alignment (CVLA) approach by data-augmented contrastive learning for SVHD. Note that our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. Specifically, we first recast vision and audio as video branch information, and title and comments as language branch information. Then, we tailor three Transformer encoders: two are used to encode the video branch and the language branch, separately. The other is used to fuse the information of the two branches and yield the multi-modal representation for SVHD. To align the semantics of the three representation, we propose a data-augmented contrastive pre-training strategy with large-scale unlabeled short-form videos. This strategy not only utilizes the representations from both single branches but also leverages multi-modal fusion representation. By constructing the positive samples with mutually complementary variables and the negative samples with mutually irrelevant variables, we employ noise contrastive estimation to obtain a proper multi-modal fusion representation via aligning it with video and language branches for better humor detection.\n\nBesides, we also evaluate on the UR-FUNNY  [20]  dataset (from long-form videos with only aligned text, audio and vision features) to further verify the effectiveness of our CVLA.\n\nOverall, our principal contributions are summarized as follows: â€¢ We expand the unlabeled data (and keep the labeled data) in DY24h dataset to enhance the robustness of pre-training, yielding a new short-form video dataset, namely DY11k; â€¢ We propose a novel Comment-aided Video-Language Alignment (CVLA) approach via the assistance of comments and data-augmented contrastive pre-training for detecting humor; â€¢ Extensive experimental results and analysis on two humor detection datasets (DY11k and UR-FUNNY) demonstrate the effectiveness of CVLA in video humor detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Humor Detection. Previous studies predominantly concentrate on utilizing deep-learning models for textual humor detection  [3, 4, 6, 10, 12, 44, 47] . However, with the growing popularity of multimodal expressions, multi-modal humor detection becomes an increasingly important area of focus. Hasan et al.  [20]  and Hasan et al.  [19]  conduct humor detection in the punchline of TED talks, using a multi-modal approach by incorporating the pre-processed features of text (transcriptions), audio, and video. Additionally, a number of studies  [2, 8, 9, 45]  attempt to identify humor labels for each individual utterance in TV show dialogues from diverse cultures mianly towards the long-form videos. Although  Liu et al. [27]  also construct a new short-form video dataset (DY24h) with interactive comments for humor detection, their dataset is limited and they rely on too many meta data to build the heterogeneous pre-training for video representation learning.\n\nUnlike the above studies, CVLA operates on raw signals across various modal channels eschewing the reliance on pre-extracted features. We employ a semi-supervised approach CVLA, devising a data-augmented contrastive pre-training strategy that utilizes a substantial corpus of unlabeled short-form videos (even more) for video understanding. The novel contrastive strategy is applicable to a broad range of tasks in video-language multi-modal analysis.\n\nVideo Understanding. Previous studies in video understanding normally ignore linguistic information (probably not available), thus taking object tracking, action recognition, and object segmentation as the challenges for video understanding  [34, 38] . Although several studies  [37, 42]  start to explore both video and language modalities for better video understanding, they don't have access to the interactive comments on the social aspect. Besides, CNNs have long been the standard for backbone architectures in this area  [7, 16, 39, 40, 46] . Only Recently, thanks to the great success of Transformer in computer vision, Transformerbased architectures  [14]  have attracted much attention in video understanding  [5, 28] . For example, for video emotion detection, Han et al.  [18]  propose a Transformer-based bimodal fusion approach by modality regularization and gated control mechanism. Moreover, due to the efficient ability of multi-layer perception (MLP), Sun et al.  [36]  introduce MLP-based structure from different axes to fuse multi-modality for sentiment analysis.\n\nDifferent from the above studies, we additionally leverage the potential linguistic knowledge that may be ignored, e.g., comments below a short-form video. Besides, we propose a contrastive pretraining approach to mine the unlabeled data. In this way, we can more accurately determine the humor expressed by short-form videos with low resources.\n\nVideo-Language Pre-training. Video-language pre-training has been verified to be effective for various downstream tasks, e.g., text-to-video retrieval and video question-answering  [37, 38] . In the literature, general pre-training tasks are normally employed for the downstream tasks, such as masked frame modeling  [35, 51] , masked modal modeling  [48] , video-language matching  [26] , frame ordering modeling  [25, 26] . Although Akbari et al.  [1]  also propose to align the video and text by multi-modal pre-training, they only aim to enhance the representation of each modality rather than multimodal representation, then conduct single-modal classification and cross-modal retrieval.\n\nDissimilar to the above studies, we propose a novel commentaided video-language alignment approach via contrastive pre-training with a few labeled and numerous unlabeled data. This approach can align video and language to a consistent semantic space, and simultaneously produce a specific multi-modal representation for short-form video humor detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we first give the definition of short-form video detection (SVHD) task. Then, we present the base framework of our model, including uni-modal and multi-modal encoding, and twobranch hierarchical architecture. Afterward, we detail our proposed video-language alignment strategy with contrastive pre-training, including cross-modal alignment from a video perspective and language perspective with multi-modality fused representation. Finally, we adopt the classification network to achieve humor detection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Formulation",
      "text": "Given sample ğ‘  ğ‘– = (ğ‘£ ğ‘– , ğ‘ ğ‘– , ğ‘¡ ğ‘– , ğ‘ ğ‘– ), where ğ‘£ ğ‘– , ğ‘ ğ‘– denotes the vision and audio respectively, and ğ‘¡ ğ‘– , ğ‘ ğ‘– denotes its accompany textual information including the video title and the interactive comments, we aim to detect whether the video of the sample ğ‘  ğ‘– is humorous, by a model F (ğ‘  ğ‘– ; Î˜) â†¦ â†’ ğ‘¦ ğ‘– trained with a few labeled samples and a large number of unlabeled samples. The annotated sample ğ‘  ğ‘– corresponds to a binary label ğ‘¦ ğ‘– âˆˆ {0, 1}, where ğ‘¦ ğ‘– = 1 indicates humorous, and ğ‘¦ ğ‘– = 0 means ğ‘  ğ‘– not humorous.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Architecture",
      "text": "In this section, we mainly introduce the base modules of our model, excluding the contrastive learning strategy. Figure  2  illustrates the overall framework of our proposed approach for SVHD. We divide four modalities into two categories-video, which includes vision and audio, and language, encompassing title and comments. These are then directed into separate branches. Following this, we implement a two-branch hierarchical network Comment-aided Video-Language Alignment (CVLA).\n\nVideo Encoding. For video branch, we process the vision and audio features by patchifying:\n\n1) Vision: Raw video ğ‘£ ğ‘– can be expressed as ğ‘£ ğ‘– âˆˆ R 48Ã—3Ã—224Ã—224 (frames Ã— channel Ã— height Ã— width). Following previous studies  [15, 38]  on video-based understanding, we adopt ViT  [14] -style visual modality embedding. The patches are flattened and embedded by linear projection, then trainable space-time positional embedding are added to the embedded patches. The final embedding of vision can be expressed as\n\n32 Ã—ğ‘‘ when patch size is (6, 32, 32) in temporal, height and width dimension respectively, and ğ‘‘ denotes the dimension of embedding.\n\n2) Audio: We extract the log mel-spectrogram\n\nfor audio at 8ğ‘˜-Hz with a total duration of ğ‘œ seconds and 512 samples between successive audio frames (hop length). Then, we treat the 2-dimension audio mel-spectrogram as an image, divide it into patches. Similar to vision, the patches are flattened and embedded by a linear projection, then add the trainable positional embedding. The final embedding of audio can be expressed as\n\nwhen patch size is  (16, 16)  in time and frequency axis respectively. On the above basis, we treat both visual and auditory data as components of the video branch. Therefore, we input them as a sequence into an multi-layer Transformer  [41]  encoder, where they completely interact with each other via self-attention mechanism:\n\nwhere VE indicates the video encoder. ğ‘‹ ; ğ‘Œ denotes concatenate operation in sequence dimension between ğ‘‹ and ğ‘Œ , and we denote the output ğ‘‰ ğ‘’ ğ‘– (from [CLS] token in ğ‘‰ ğ‘– ) to represent the video features of the ğ‘–-th sample.\n\nLanguage Encoding. For language branch, we tokenize title and comments then encode:\n\n1) Title: Each token (e.g., the ğ‘¡-th token)in the ğ‘–-th title is represented as the sum of word embedding ğ‘¡ ğ‘¤ ğ‘–,ğ‘˜ , position embedding ğ‘¡ ğ‘ ğ‘–,ğ‘˜ . Formally: ğ‘¡ ğ‘–,ğ‘˜ = ğ‘¡ ğ‘¤ ğ‘–,ğ‘˜ + ğ‘¡ ğ‘ ğ‘–,ğ‘˜ , where ğ‘¡ ğ‘–,ğ‘˜ denotes the ğ‘˜-th token representation input into our model. Thus, ğ‘¡ ğ‘– indicates the whole title sequence of ğ‘–-th sample.\n\n2) Comment: Similarly, each token (e.g., the ğ‘˜-th token) in the ğ‘—-th comment of the ğ‘–-th sample is represented as the sum of word embedding ğ‘ ğ‘¤ ğ‘–,ğ‘—,ğ‘˜ , position embedding ğ‘ ğ‘ ğ‘–,ğ‘—,ğ‘˜ . Formally: ğ‘ ğ‘–,ğ‘—,ğ‘˜ = ğ‘ ğ‘¤ ğ‘–,ğ‘—,ğ‘˜ +ğ‘ ğ‘ ğ‘–,ğ‘—,ğ‘˜ , where ğ‘ ğ‘–,ğ‘—,ğ‘˜ denotes the ğ‘˜-th token in the ğ‘—-th comment of the ğ‘–-th sample. Thus, the whole comment sequence can be represented as {ğ‘ ğ‘–,1 , â€¢ â€¢ â€¢ , ğ‘ ğ‘– ğ‘— }. Note that every instance contains up to 10 comments.\n\nOn the above basis, we treat both title and comments data as components of the language branch. Therefore, we cast the title and comments as one document but distinguish them with the [SEP] token, and add the [CLS] token at the beginning. Then, we employ the pre-trained language model BERT  [13]  to encode them:\n\nwhere LE indicates the language encoder BERT that can be easily replaced with Large Language Model (LLM). We take the output ğ¿ ğ‘’ ğ‘– (from [CLS] token in ğ¿ ğ‘– ) to represent the language features of the ğ‘–-th sample.\n\nMulti-modal Fusion. To make multi-modal data completely interact with each other and produce a unified multi-modal fusion representation, we employ self-attention mechanism  [33]  by the Transformer-based multi-modal encoder (MME) that can be easily replaced with Large Vision-Language Model (LVLM). Formally:\n\nwhere we denote the output ğ‘€ ğ‘’ ğ‘– (from the first token [CLS] in ğ‘€ ğ‘– ) to represent the multi-modal features of ğ‘–-th sample.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data-Augmented Contrastive Pre-Training",
      "text": "To effectively align the video and language with multi-modal representation, we attempt contrastive learning, which can also utilze the large-scale unlabeled data at the pre-training stage. In the literature, data augmentation is a common strategy for contrastive learning to construct positive pairs, such as SimCLR  [11]  in computer vision (CV) and SimCSE  [17]  in natural language processing (NLP).\n\nData Augmentation. For the video branch, to adapt our task, we randomly apply three of the Erase (10 âˆ¼ 20% area), Color jitter, 2D affine, Color drop, Gaussian blur, and Gaussian noise following Chen et al.  [11]  to the visual modality with a probability of 33. 3%. Besides, we randomly remove 10 âˆ¼ 20% of the mel-spectrogram for audio. To this end, we feed original patches of vision and audio into the video encoder (VE) to get ğ‘‰ ğ‘– . Meanwhile, we also feed augmented vision and audio patches data from the same sample into VE module to obtain á¹¼ğ‘– , then pair both [CLS] tokens of them as (ğ‘‰ ğ‘’ ğ‘– , á¹¼ ğ‘’ ğ‘– ). For the language branch, we feed the title and comments of a sample into the language encoder (LE) twice to obtain the different language sequence representation (due to dropout probability), i.e., ğ¿ ğ‘– and Lğ‘– , respectively. Then, we pair both [CLS] token of them as (ğ¿ ğ‘’ ğ‘– , Lğ‘’ ğ‘– ). Finally, to obtain the proper representation for better SVHD, we need to involve fused multi-modal representation in contrastive pre-training. Therefore, we adopt the multi-modal encoder (MME) to yield two kinds of fused multi-modal representations Mğ‘’ ğ‘– and ğ‘€ ğ‘’ ğ‘– from ( á¹¼ ğ‘’ ğ‘– , ğ¿ ğ‘– with additional knowledge. Note that although the original video and language modalities may exist gap, after modality encoding, we expect them to achieve semantic alignment. Therefore, we can obtain the positive tuple (ğ‘‰ ğ‘’ ğ‘– , ğ¿ ğ‘’ ğ‘– , Lğ‘’ ğ‘– , Mğ‘’ ğ‘– ), then reduce the distance of latent space among them. Through subsequent contrastive learning, this also can be considered as to augment the video and multi-modal representation for our humor detection. Regarding the negative tuples, in a batch of the training samples, we take the contrary guidance from other samples, i.e., (ğ‘‰ ğ‘’ ğ‘– , ğ¿ ğ‘’ ğ‘— , Lğ‘’ ğ‘— , Mğ‘’ ğ‘— ), where ğ‘— â‰  ğ‘–. Similarly, from the language perspective, we can obtain the positive tuple (ğ¿ ğ‘’ ğ‘– , ğ‘‰ ğ‘’ ğ‘– , á¹¼ ğ‘’ ğ‘– , ğ‘€ ğ‘’ ğ‘– ) to augment the language and multi-modal representations. Besides, the negative tuple is defined as (ğ¿ ğ‘’ ğ‘– , ğ‘‰ ğ‘’ ğ‘— , á¹¼ ğ‘’ ğ‘— , ğ‘€ ğ‘’ ğ‘— ) where ğ‘— â‰  ğ‘– to keep distance among them.\n\nContrastive Pre-Training. For the contrastive learning among the multiple variables, we leverage the noise contrastive estimation (NCE) loss  [23]  within a batch B.\n\nFrom the video perspective, we compute the NCE loss of the positive and negative tuples in a batch as follows:\n\nğ‘”(ğ‘, ğ‘) = log\n\nğ‘“ (ğ‘, ğ‘) = ğ‘’ sim(ğ‘,ğ‘)/ğœ  (7)  where ğ‘ and ğ‘ are the formal parameters, which can accept the variables in positive and negative tuples. ğœ denotes a trainable temperature scalar parameter, and sim(â€¢, â€¢) is the dot-product similarity function to measure the degree of alignment between different representations. To avoid the sub-optimal situation (e. g., ğ¿ ğ‘’ ğ‘– may be closer to ğ‘‰ ğ‘’ ğ‘– while Lğ‘’ ğ‘– and Mğ‘’ ğ‘– become away from ğ‘‰ ğ‘’ ğ‘– .), we further narrow the semantic distance between the video and the complementary representations of language and multi-modal fusion, in which the semantic distance are highlighted by normalization:\n\nğœ‘ (ğ‘, ğ‘, ğ‘–) = log  The two losses in 4 and 8 are summed up to give the learning objective of alignment: L ğ‘‰ = L ğ‘£ + L ğ‘£ â€² from the video perspective.\n\nSimilarily, from the language perspective, we define the alignment objective L ğ¿ = L ğ‘™ +L ğ‘™ â€² . Then, we obtain the final learning objective in pre-training:\n\nwhere Î˜ denotes all trainable parameters of the model F , ğœ† represents the coefficient of ğ¿ 2 -regularization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Humor Detection",
      "text": "In the fine-tuning stage, we do not need to use the procedure of data augmentation, and we adopt multi-modal hybrid representation ğ‘€ ğ‘’ ğ‘– to infer the humor status: Å·ğ‘– = Softmax(MLP(ğ‘€ ğ‘’ ğ‘– )). Here, crossentropy is adopted for the learning objective:\n\nwhere ğ‘ denotes training set size, Î˜ denotes all trainable parameters of the model F , ğœ† represents the coefficient of ğ¿ 2 -regularization.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimentation 4.1 Data Settings",
      "text": "In our experiments, we utilized the DY24h  [27]  dataset, supplemented with additional unlabeled data for pre-training, named DY11k. Further, we conducted experiments on the UR-FUNNY  [20]  dataset to detect humor labels in TED talk punchlines.\n\nData Expansion. Adhering to the data collection protocols established for DY24h  [27] , we have amassed an additional 3060 unlabeled short-form video samples, each encompassing four modalities: vision, audio, title, and comments. These samples are incorporated into the unlabeled data part of DY24h for the pre-training stage. Following this inclusion, the expanded dataset, now known as DY11k, consists of a total of 11150 samples, amounting to 35.38 hours. Of these, 9915 samples (29.37 hours) without annotation are used to conduct self-supervised pre-training, while the remaining 1235 samples (6.01 hours) are manually annotated to perform supervised learning and testing for humor detection same as DY24h. The statistic summary can refer to Figure  3 . Data Split. For DY11k, we randomly allocate the 1235 labeled samples into training, development, and test sets, ensuring a balanced distribution across labels, facilitated by five distinct random seeds (the training of the model employs the corresponding random seed used for data segmentation). For UR-FUNNY, we segment the 6334 available samples with raw videos into unlabeled and labeled groups. The 4634 unlabeled samples is utilized for pre-training stage without regard to its labels. Similar to DY11k, we randomly allocate the other 1700 labeled samples into training, development, and test sets. Table  1  delineates the specifics of the data division for both humor detection datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We uniform sampling 48 frames for video and resize frames to a spatial size of 224 Ã— 224, and patch size is (6, 32, 32) for it. We use mel-spectrogram of librosa  [30]  with 8-kHz sampling rate, 512 hop length and 128 mel bands, and patch size is  (16, 16)  for it. The maximum length of language (title and comments) token sequence is 16 for each sentence, and we discard all the #Funny and #Funny Video hashtags. AdamW  [29]  is utilized as the optimizer and the learning rate is 0.00001 for both pre-training and fine-tuning. The coefficient of ğ¿ 2 -regularization ğœ† is set to 0.01 for all optimizer. The dimension of all embedding and encoding in our model is 512. The number of heads in the Transformer  [41]  encoder is set to 12, and the encoder is set to 8 layers. The dropout rate is set to 0.1.\n\nMean and standard deviation are calculated based on five runs (remove the top and bottom) with different random seeds to avoid unstable extreme values. Multiple dataset partitions and model training give a more robust measure of performance and a better estimate of the standard. We pre-train our model with batch size Table  2 : Performance comparison on DY11k dataset. V, A, T and C indicate different modalities of visual, acoustic, title, and comments accordingly. HT100M  [31] , YTT180M  [49] , LF-VILA-8M  [37] . â†‘ denotes the datasets Books and Wiki in the lastrow.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "â€¢ TVLT  [38] : a state-of-the-art (SOTA) multi-modal approach for video understanding (e.g., image retrieval, video retrieval, and multimodal sentiment analysis) with video-audio pre-training on datasets HT100M  [31]  and YTT180M  [49] . However, since the model is particularly designed for vision and audio modality, we can not directly input language modality;\n\nâ€¢ BERT  [13]  and DeBERTa  [22] : the SOTA approaches for natural language understanding with large-scale pre-training on multiple corpus. We conduct humor detection as text classification task;\n\nâ€¢ LF-VILA  [37] : the SOTA approach for long-form (similar to shortform setting in DY11k) video understanding (e.g., cross-modal retrieval and long-form video classification) with video-language pre-training on their dataset LF-VILA-8M  [37] . We adapt the title and comments in DY11k to the language modality of this model LF-VILA, then conduct humor detection. However, since this model is particularly designed for vision and language modality, we can not directly input audio;\n\nâ€¢ C-MFN  [20] : the baseline approach for UR-FUNNY  [20]  dataset, that designed for the detection of punchline humor in TED talks, accompanied by synchronous transcriptions in long-form videos;\n\nâ€¢ BBFN  [18]  and CubeMLP  [36] : the SOTA approaches for emotion detection of videos without multi-modal pre-training;\n\nâ€¢ VATT  [1] : the SOTA approach in multi-modal video understanding (e.g., audio event classification, image classification, and textto-video retrieval) with video-audio-language pre-training;\n\nâ€¢ M3GAT  [50] : the SOTA approach for multi-modal dialogue sentiment analysis and emotion recognition;\n\nâ€¢ CMHP  [27] : the SOTA approach for short-form video detection with multi-modal heterogeneous pre-training and the assistance of interactive comments. Note that for a fair comparison, we adapt the title and comments in DY11k to the language modality (dialogue for M3GAT) of these model. We also implement these pre-training approach on DY11k and UR-FUNNY, then conduct humor detection.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Results",
      "text": "Tables  2  and 3  present the comparative performance of the CVLA against SOTA baselines on two humor detection datasets.\n\nExperimental results of Tables  2  and 3  reveals that: 1) CVLA surpasses all baselines obviously, demonstrating the advantage of our architecture even without pre-training on DY11k. This is attributed to effective modality encoding and the multi-modal of video and language. Crucially, the video-language alignment facilitated by contrastive pre-training is instrumental, generating a nuanced multi-modal representation for humor detection. Moreover, existing approaches such as C-MFN, BBFN rely on the feature provided by UR-FUNNY dataset, while CMHP relies on the feature extracted by ResNet  [21] , which may result in the omission of important information. In contrast, our CVLA differs from the above since it operates on raw signals across various modal channels, the trait enhances performance and also streamlines the training and inference process. 2) Baselines lacking pre-training exhibit markedly lower performance compared to their pre-trained counterparts, with the exception of VATT. This discrepancy suggests that BBFN, CubeMLP, M3GAT, and C-MFN do not fully leverage the available unlabeled data, potentially overlooking critical information for These findings substantiate the efficacy of our approach CVLA in addressing the distinctive challenges posed by short-form video humor detection and suggest its applicability to long-form video contexts as well.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Analysis And Discussion",
      "text": "The effect of different modality combinations. Table  4  shows the performance comparison of our CVLA using different modality combinations without and with our contrastive pre-training. From this table, we can observe that: 1) As long as using contrastive pre-training, CVLA can perform much better than not using pretraining, regardless of the scenario on different modality combinations. This demonstrates the effectiveness of our proposed videolanguage contrastive pre-training strategy; 2) Without pre-training, CVLA on a bimodal scenario of T+C achieves the best performance due to the BERT's excellent language understanding, even better than using three or four modalities, as it is impractical for the video branch to understand the video content with only 100 labeled training samples and without any pre-trained visual backbone due to the computing resource constraint. This suggests that although our model without pre-training has extracted features of each modality well and performed nicely in many scenarios, it does not fully exploit the advantages of each modality and the complementarity among different modalities more than two; Therefore, we attempt to introduce a video-language alignment approach via contrastive pre-training, simultaneously producing a hybrid multi-modal representation for better SVHD. The effectiveness of this attempt is verified by the results of CVLA with pre-training. For example, with our pre-training, CVLA based on V+T+C and A+T+C outperform T+C, in which V and A are well utilized based on T+C.\n\nThe indicator trend with and without pre-training. Figure  4  shows the loss-decreasing trend during pre-training (PT) and the accuracyincreasing trend of development set during fine-tuning after PT and validation without PT by our CVLA. As the course of 10 epochs in pre-training, loss quickly decreases from the initial 16 to approximately 3. This suggests that this phase allows the model to have a solid grasp of the video-language alignment and multi-modal fusion supplied by large-scale short-form videos. Besides, regarding the accuracy-increasing trend of development set, we can guess that the pre-trained model has captured a large amount of unlabeled generic knowledge, e.g., video-language semantic aligned representation. Then, by fine-tuning, the model can produce better specific multi-modal representations for humor detection, compared to not using pre-training.   The effect of pre-training scale. Table  5  displays the performance variation resulting from our CVLA using pre-trained data of different sizes. From this table, we can see that using too less pre-training data (e.g., 3K) may lead to worse performance. This is mainly because our contrastive pre-training requires an appropriate data scale to take advantage of its benefits for SVHD. Besides, from the overall trend, our CVLA can consistently improve the performance by feeding more unlabeled data. However, due to the limited computational resources, we did not explore more data, which becomes our future work.\n\nThe ablation study. Table  6  shows the performance comparison of different ablated approaches from our CVLA. The approaches from top to bottom are our full model (CVLA), completely removing the MME block and concatenating ğ‘‰ ğ‘’ ğ‘– and ğ¿ ğ‘’ ğ‘– for humor detection (w/o MME), removing of multi-modal representation learning during pretraining (w/o ğ‘€ ğ‘’ ), removing the loss L ğ‘‰ (w/o L ğ‘‰ ), and removing the loss L ğ¿ (w/o L ğ¿ ), removing L ğ‘£ â€² and L ğ‘™ â€² (w/o L ğ‘£ â€² + L ğ‘™ â€² ). From this table, we can find that the additional losses (L ğ‘£ â€² + L ğ‘™ â€² ) avoiding the sub-optimal problem bring in the minimal impact, but it should not be ignored. In general, removing any of the blocks leads to obvious performance degradation. This shows that each block of our CVLA has its own necessity.\n\nThe visualization in multi-modal encoder. First, we provide the visualization of self-attention in the multi-modal encoder (MME) using average attention score of all attention heads, layers, and test samples without and with our proposed contrastive pre-training strategy, as illustrated in Figure  5 . From this figure, we can observe that without pre-training, the video and language branches do not    have any mutual interactions, i.e. it is impossible to semantically align them. However, after using pre-training, the video clearly starts to pay attention to the language features. In other words, the video representation tries to move closer to the language.\n\nThen, we illustrate the t-SNE visualization of our multi-modal fusion representation ğ‘€ ğ‘’ without and with our pre-training strategy, as shown in Figure  6 . From this figure, we can observe that the distribution after pre-training becomes more uniform and the distinction between humor labels is clearer, compared to the distribution without pre-training. This suggests our CVLA can learn a proper representation for better SVHD by contrastive pre-training.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we concentrate on the short-form video humor detection (SVHD) utilizing commentary data to incorporate the social dimension. Initially, we develop an expanded multi-modal dataset DY11k from DY24h. Subsequently, we propose a Comment-aided Video-Language Alignment (CVLA) approach, employing dataaugmented contrastive pre-training to tackle the challenges of SVHD. We then conduct comparative analyses of CVLA against the state-of-the-art and other competitive baselines on two humor detection datasets, DY11k and UR-FUNNY, to demonstrate the advantages of CVLA. Furthermore, we provide various interesting analysis to verify the effectiveness of CVLA in details.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example for SVHD.",
      "page": 1
    },
    {
      "caption": "Figure 1: , the video and title alone provide insuffi-",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall framework of our proposed CVLA for short-form video humor detection.",
      "page": 4
    },
    {
      "caption": "Figure 3: The statistics of our expanded dataset DY11K.",
      "page": 5
    },
    {
      "caption": "Figure 3: Table 1: The data split of two humor detection datasets.",
      "page": 5
    },
    {
      "caption": "Figure 4: The loss of our pre-training (PT), the accuracy (Acc)",
      "page": 7
    },
    {
      "caption": "Figure 5: Self-Attention visualization of our multi-modal",
      "page": 8
    },
    {
      "caption": "Figure 5: From this figure, we can observe",
      "page": 8
    },
    {
      "caption": "Figure 6: t-SNE visualization of our multi-modal fusion rep-",
      "page": 8
    },
    {
      "caption": "Figure 6: From this figure, we can observe that",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "CVLA(Ours)": "",
          "âœ—": "DY11k",
          "77.84Â±1.57": "80.68Â±0.49",
          "78.97Â±0.22": "81.11Â±0.31",
          "78.01Â±1.34": "80.72Â±0.45",
          "77.53Â±1.69": "80.62Â±0.53",
          "79.10Â±0.28": "81.16Â±0.38",
          "77.50Â±1.73": "80.62Â±0.54"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Loss of PT\nDev Acc of HD w/ PT\nDev Acc of HD w/o PT": "",
          "Column_2": "Loss of PT\nDev Acc of HD w/ PT\nDev Acc of HD w/o PT"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "authors": [
        "Hassan Akbari",
        "Liangzhe Yuan",
        "Rui Qian",
        "Wei-Hong Chuang",
        "Shih-Fu Chang",
        "Yin Cui",
        "Boqing Gong"
      ],
      "year": "2021",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "2",
      "title": "When to Laugh and How Hard? A Multimodal Approach to Detecting Humor and Its Intensity",
      "authors": [
        "Khalid Alnajjar",
        "Mika HÃ¤mÃ¤lÃ¤inen",
        "JÃ¶rg Tiedemann",
        "Jorma Laaksonen",
        "Mikko Kurimo"
      ],
      "year": "2022",
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "3",
      "title": "ColBERT: Using BERT Sentence Embedding for Humor Detection",
      "authors": [
        "Annamoradnejad Issa"
      ],
      "year": "2020",
      "venue": "ColBERT: Using BERT Sentence Embedding for Humor Detection"
    },
    {
      "citation_id": "4",
      "title": "Combining Humor and Sarcasm for Improving Political Parody Detection",
      "authors": [
        "Xiao Ao",
        "Danae Villegas",
        "Daniel Preotiuc-Pietro",
        "Nikolaos Aletras"
      ],
      "year": "2022",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "5",
      "title": "Is Space-Time Attention All You Need for Video Understanding?",
      "authors": [
        "Gedas Bertasius",
        "Heng Wang",
        "Lorenzo Torresani"
      ],
      "year": "2021",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "6",
      "title": "Large Dataset and Language Model Fun-Tuning for Humor Recognition",
      "authors": [
        "Vladislav Blinov",
        "Valeria Bolotova-Baranova",
        "Pavel Braslavski"
      ],
      "year": "2019",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "7",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "JoÃ£o Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "8",
      "title": "Asif Ekbal, and Pushpak Bhattacharyya. 2022. A Sentiment and Emotion Aware Multimodal Multiparty Humor Recognition in Multilingual Conversational Setting",
      "authors": [
        "Dushyant Singh Chauhan",
        "Gopendra Vikram Singh",
        "Aseem Arora"
      ],
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "9",
      "title": "Asif Ekbal, Pushpak Bhattacharyya, Louis-Philippe Morency, and Soujanya Poria. 2021. M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations",
      "authors": [
        "Dushyant Singh Chauhan",
        "Gopendra Vikram Singh",
        "Navonil Majumder",
        "Amir Zadeh"
      ],
      "venue": "Proc. of ICMI"
    },
    {
      "citation_id": "10",
      "title": "Humor Recognition Using Deep Learning",
      "authors": [
        "Peng-Yu Chen",
        "Von-Wun Soo"
      ],
      "year": "2018",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "11",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "12",
      "title": "Can Pre-trained Language Models Understand Chinese Humor?",
      "authors": [
        "Yuyan Chen",
        "Zhixu Li",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Bang Liu",
        "Yunwen Chen"
      ],
      "year": "2023",
      "venue": "Proc. of WSDM"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "14",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "15",
      "title": "Masked Autoencoders As Spatiotemporal Learners",
      "authors": [
        "Christoph Feichtenhofer",
        "Haoqi Fan",
        "Yanghao Li",
        "Kaiming He"
      ],
      "year": "2022",
      "venue": "Masked Autoencoders As Spatiotemporal Learners"
    },
    {
      "citation_id": "16",
      "title": "Slow-Fast Networks for Video Recognition",
      "authors": [
        "Christoph Feichtenhofer",
        "Haoqi Fan",
        "Jitendra Malik",
        "Kaiming He"
      ],
      "year": "2019",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "17",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "18",
      "title": "Louis-Philippe Morency, and Soujanya Poria. 2021. Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Alexander Gelbukh",
        "Amir Zadeh"
      ],
      "venue": "Proc. of ICMI"
    },
    {
      "citation_id": "19",
      "title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor",
      "authors": [
        "Kamrul Md",
        "Sangwu Hasan",
        "Wasifur Lee",
        "Amir Rahman",
        "Rada Zadeh",
        "Louis-Philippe Mihalcea",
        "Ehsan Morency",
        "Hoque"
      ],
      "year": "2021",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "20",
      "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
      "authors": [
        "Kamrul Md",
        "Wasifur Hasan",
        "Amirali Rahman",
        "Jianyuan Bagher Zadeh",
        "Md Zhong",
        "Louis-Philippe Tanveer",
        "Mohammed (ehsan) Morency",
        "Hoque"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "21",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "22",
      "title": "Deberta: decoding-Enhanced Bert with Disentangled Attention",
      "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "23",
      "title": "Clover: Towards a Unified Video-Language Alignment and Fusion Model",
      "authors": [
        "Jingjia Huang",
        "Yinan Li",
        "Jiashi Feng",
        "Xinglong Wu",
        "Xiaoshuai Sun",
        "Rongrong Ji"
      ],
      "year": "2023",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "24",
      "title": "Yuta Nakashima, and Haruo Takemura. 2021. The Laughing Machine: Predicting Humor in Video",
      "authors": [
        "Yuta Kayatani",
        "Zekun Yang",
        "Mayu Otani",
        "Noa Garcia",
        "Chenhui Chu"
      ],
      "venue": "Proc. of WACV"
    },
    {
      "citation_id": "25",
      "title": "Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training",
      "authors": [
        "Chenyi Lei",
        "Shixian Luo",
        "Yong Liu",
        "Wanggui He",
        "Jiamang Wang",
        "Guoxin Wang",
        "Haihong Tang",
        "Chunyan Miao",
        "Houqiang Li"
      ],
      "year": "2021",
      "venue": "Proc. of ACM MM"
    },
    {
      "citation_id": "26",
      "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training",
      "authors": [
        "Linjie Li",
        "Yen-Chun Chen",
        "Yu Cheng",
        "Zhe Gan",
        "Licheng Yu",
        "Jingjing Liu"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "27",
      "title": "Comment-Aware Multi-Modal Heterogeneous Pre-Training for Humor Detection in Short-Form Videos",
      "authors": [
        "Yang Liu",
        "Huanqin Ping",
        "Dong Zhang",
        "Qingying Sun",
        "Shoushan Li",
        "Guodong Zhou"
      ],
      "year": "2023",
      "venue": "Proc. of ECAI"
    },
    {
      "citation_id": "28",
      "title": "Video Swin Transformer",
      "authors": [
        "Ze Liu",
        "Jia Ning",
        "Yue Cao",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Han Hu"
      ],
      "year": "2022",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "29",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "30",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in Python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proc. of SciPy"
    },
    {
      "citation_id": "31",
      "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
      "authors": [
        "Antoine Miech",
        "Dimitri Zhukov",
        "Jean-Baptiste Alayrac",
        "Makarand Tapaswi",
        "Ivan Laptev",
        "Josef Sivic"
      ],
      "year": "2019",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "32",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas KÃ¶pf",
        "Edward Yang",
        "Zachary Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "33",
      "title": "Is Cross-Attention Preferable to Self-Attention for Multi-Modal Emotion Recognition?",
      "authors": [
        "Alessio Vandana Rajan",
        "Andrea Brutti",
        "Cavallaro"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "34",
      "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
      "authors": [
        "Andrew Rouditchenko",
        "Angie Boggust",
        "David Harwath",
        "Brian Chen",
        "Dhiraj Joshi",
        "Samuel Thomas",
        "Kartik Audhkhasi",
        "Hilde Kuehne",
        "Rameswar Panda",
        "RogÃ©rio Schmidt Feris",
        "Brian Kingsbury",
        "Michael Picheny",
        "Antonio Torralba",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "35",
      "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
      "authors": [
        "Chen Sun",
        "Austin Myers",
        "Carl Vondrick",
        "Kevin Murphy",
        "Cordelia Schmid"
      ],
      "year": "2019",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "36",
      "title": "Cube-MLP: An MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation",
      "authors": [
        "Hongyi Hao Sun",
        "Jiaqing Wang",
        "Yen-Wei Liu",
        "Lanfen Chen",
        "Lin"
      ],
      "year": "2022",
      "venue": "Proc. of ACM MM"
    },
    {
      "citation_id": "37",
      "title": "Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning",
      "authors": [
        "Yuchong Sun",
        "Hongwei Xue",
        "Ruihua Song",
        "Bei Liu",
        "Huan Yang",
        "Jianlong Fu"
      ],
      "year": "2022",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "TVLT: Textless Vision-Language Transformer",
      "authors": [
        "Zineng Tang",
        "Jaemin Cho",
        "Yixin Nie",
        "Mohit Bansal"
      ],
      "year": "2022",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "39",
      "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "40",
      "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "41",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "42",
      "title": "All in One: Exploring Unified Video-Language Pre-Training",
      "authors": [
        "Jinpeng Wang",
        "Yixiao Ge",
        "Rui Yan",
        "Yuying Ge",
        "Kevin Qinghong Lin",
        "Satoshi Tsutsui",
        "Xudong Lin",
        "Guanyu Cai",
        "Jianping Wu",
        "Ying Shan",
        "Xiaohu Qie",
        "Mike Shou"
      ],
      "year": "2023",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "43",
      "title": "Humor and camera view on mobile short-form video apps influence user experience and technology-adoption intent, an example of TikTok (DouYin)",
      "authors": [
        "Yunwen Wang"
      ],
      "year": "2020",
      "venue": "Comput. Hum. Behav"
    },
    {
      "citation_id": "44",
      "title": "Humor Detection: A Transformer Gets the Last Laugh",
      "authors": [
        "Orion Weller",
        "Kevin Seppi"
      ],
      "year": "2019",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "45",
      "title": "MUMOR: A Multimodal Dataset for Humor Detection in Conversations",
      "authors": [
        "Jiaming Wu",
        "Hongfei Lin",
        "Liang Yang",
        "Bo Xu"
      ],
      "year": "2021",
      "venue": "Proc. of NLPCC"
    },
    {
      "citation_id": "46",
      "title": "Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification",
      "authors": [
        "Saining Xie",
        "Chen Sun",
        "Jonathan Huang",
        "Zhuowen Tu",
        "Kevin Murphy"
      ],
      "year": "2018",
      "venue": "Proc. of ECCV"
    },
    {
      "citation_id": "47",
      "title": "Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition",
      "authors": [
        "Yubo Xie",
        "Junze Li",
        "Pearl Pu"
      ],
      "year": "2021",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "48",
      "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
      "authors": [
        "Hu Xu",
        "Gargi Ghosh",
        "Po-Yao Huang",
        "Prahal Arora",
        "Masoumeh Aminzadeh",
        "Christoph Feichtenhofer",
        "Florian Metze",
        "Luke Zettlemoyer"
      ],
      "year": "2021",
      "venue": "Proc. of ACL Findings"
    },
    {
      "citation_id": "49",
      "title": "MERLOT: Multimodal Neural Script Knowledge Models",
      "authors": [
        "Rowan Zellers",
        "Ximing Lu",
        "Jack Hessel",
        "Youngjae Yu",
        "Jae Park",
        "Jize Cao",
        "Ali Farhadi",
        "Yejin Choi"
      ],
      "year": "2021",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "50",
      "title": "M3GAT: A Multi-modal, Multitask Interactive Graph Attention Network for Conversational Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Yazhou Zhang",
        "Ao Jia",
        "Bo Wang",
        "Peng Zhang",
        "Dongming Zhao",
        "Pu Li",
        "Yuexian Hou",
        "Xiaojia Jin",
        "Dawei Song",
        "Jing Qin"
      ],
      "year": "2024",
      "venue": "ACM Trans. Inf. Syst"
    },
    {
      "citation_id": "51",
      "title": "ActBERT: Learning Global-Local Video-Text Representations",
      "authors": [
        "Linchao Zhu",
        "Yi Yang"
      ],
      "year": "2020",
      "venue": "Proc. of CVPR"
    }
  ]
}