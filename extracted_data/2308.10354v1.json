{
  "paper_id": "2308.10354v1",
  "title": "Imaginations Of Wall-E : Reconstructing Experiences With An Imagination-Inspired Module For Advanced Ai Systems",
  "published": "2023-08-20T20:10:55Z",
  "authors": [
    "Zeinab Sadat Taghavi",
    "Soroush Gooran",
    "Seyed Arshan Dalili",
    "Hamidreza Amirzadeh",
    "Mohammad Jalal Nematbakhsh",
    "Hossein Sameti"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we introduce a novel Artificial Intelligence (AI) system inspired by the philosophical and psychoanalytical concept of imagination as a \"Re-construction of Experiences\". Our AI system is equipped with an imaginationinspired module that bridges the gap between textual inputs and other modalities, enriching the derived information based on previously learned experiences. A unique feature of our system is its ability to formulate independent perceptions of inputs. This leads to unique interpretations of a concept that may differ from human interpretations but are equally valid, a phenomenon we term as \"Interpretable Misunderstanding\". We employ large-scale models, specifically a Multimodal Large Language Model (MLLM), enabling our proposed system to extract meaningful information across modalities while primarily remaining unimodal. We evaluated our system against other large language models across multiple tasks, including emotion recognition and questionanswering, using a zero-shot methodology to ensure an unbiased scenario that may happen by fine-tuning. Significantly, our system outperformed the best Large Language Models (LLM) on the MELD, IEMOCAP, and CoQA datasets, achieving Weighted F1 (WF1) scores of 46.74%, 25.23%, and Overall F1 (OF1) score of 17%, respectively, compared to 22.89%, 12.28%, and 7% from the well-performing LLM. The goal is to go beyond the statistical view of language processing and tie it to human concepts such as philosophy and psychoanalysis. This work represents a significant advancement in the development of imagination-inspired AI systems, opening new possibilities for AI to generate deep and interpretable information across modalities, thereby enhancing human-AI interaction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Imagination, as defined in the Cambridge dictionary, is \"the ability to form pictures in the mind\"  (Harley 2000) . In some philosophical papers, it is considered as the process of \"Reconstruction of Experiences\"  (Currie and Ravenscroft 2002; Van Leeuwen 2013) . It allows us to form mental images from text, drawing on our past experiences like what happens in Figure  2 . These mental images substantially influence our understanding and emotional reactions to various concepts  (Van Leeuwen 2014; Paivio 1990 ). This theory, Figure  1 : Proposed system in comparison to Unimodal LLMs in textual tasks. As you can see, the humaninterpretable input for processing unit in common LLMs is just input text, but in our proposed system, the system generates human-interpretable input by itself in another modality for MLLM. This process happens in the Imagination-Inspired (I-I) module.\n\nemphasized by psychoanalysts, highlights the crucial interplay between past experiences and current perception  (Freud 2004; Lacan 2007) .\n\nFor the first time, drawing inspiration from this philosophical and psychoanalytical foundation, we introduce an AI system embedded with an imagination-inspired module, as you can see in Figure  1 . This system bridges the gap between textual inputs and other modalities, thereby enriching the information derived from the text, using previously learned experiences which act as \"Re-Construction of Experiences\". A unique attribute of this AI system is its capability to formulate its own independent perception of an input. Thus, Our system can arrive at unique, yet equally valid, interpretations of a concept-a phenomenon we term as AI's \"Interpretable Misunderstanding\". For example, when we give the system input \"apple\", we may have in mind a green apple. In contrast, the image generated by the AI system may depict a yellow apple. Nevertheless, both representations are still considered as apples. Furthermore, the generated image may contain additional information, such as being curved or having leaves, which were not mentioned directly. This is not a computational mistake, this is a \"Different Point of View\" and it should be considered important, as you can see in Figure  2 . It might be a bit different, but same in major points which were mentioned in the input. This can cause \"Interpretable Misunderstanding\".\n\nFor adding this imagination-inspired module to our system for a unimodal textual task (text-input text-output), we made a pipeline which is described in Figure  1 . In our implementation, our system takes a text as input, then generates an image using a text-to-image model  (Rombach et al. 2022) . Due to the inherent randomness of text-to-image models, the generated image varies each time, even for the same text input  (Rombach et al. 2022) . Finally, by giving both text and image to an MLLM  (Dai et al. 2023) , the system provides us with the text output. Generating meaningful information from textual input to other modalities, which in this case is image, roles as the imagination-inspired module. While fine-tuning often enables models to excel in specific tasks  (Devlin et al. 2018; Liu et al. 2019; Chowdhery et al. 2022; Touvron et al. 2023) , we shifted our focus towards Large-scale Language Models for their depth and interactivity. Our system prominently features an MLLM at its core. This design choice not only enables our system to extract valuable insights from different modalities but also ensures its primary operation remains unimodal. Additionally, the model's substantial size allows the integration of an imagination-inspired module as a secondary smaller component behind the primary bigger language processing module. Using Large-scale Language Models ensures the model's versatility across multiple tasks without the need for finetuning. We assessed our system by benchmarking its performance against other LLMs across multiple tasks. We initially concentrated on emotion recognition, using the MELD and IEMOCAP datasets  (Poria et al. 2019; Busso et al. 2008) . Subsequently, we focused on the question-answering task using the CoQA dataset  (Reddy, Chen, and Manning 2018) . The CoQA dataset aligns with our aim to evaluate the model's ability to comprehend the text based solely on the provided input without relying on any pre-existing knowledge. More reasons are in Appendix 1. We compared our system to other LLMs exhibiting robust performance on the TriviaQA dataset  (Joshi et al. 2017) . Our approach employs a zero-shot methodology, which allows us to assess the models' raw abilities without task-specific fine-tuning, ensuring an unbiased evaluation scenario. Our built system outperformed the best LLM (Koala-7B) on the MELD, IEMOCAP, and CoQA datasets. This accomplishment signals a remarkable advancement in developing AI systems by adding the imagination-inspired module. Our work paves the way for a new era in artificial intelligence, where AI systems can generate deep and meaningful information from input in one modality, starting with images and potentially expanding to audio or even action in the future. Imagine a day when we tell an AI system that we're walking, and it can reconstruct the experience within itself, thereby enhancing human-AI interaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Here we will delve into the current state of LLMs and MLLMs in AI, the philosophical and psychoanalytical underpinnings of imagination in order to discuss our system's implementation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Current State Of Llms And Mllms In Ai",
      "text": "In recent years, Large Language Models (LLMs) have emerged as a transformative technology in natural language processing, exhibiting remarkable capabilities in various tasks such as text generation and question-answering.\n\nSubsequently, Unimodal-trained LLMs, such as GPT-3  (Brown et al. 2020)  and BERT  (Devlin et al. 2018) , leveraged transformer models, delivering significant improvements in language understanding tasks but remaining confined to the linguistic modality  (Hoffmann et al. 2022) .\n\nThe AI field then progressed towards multimodal models, capable of processing diverse data types  (Yin et al. 2023; Kiros, Salakhutdinov, and Zemel 2014) . However, these models typically relied on user-provided multimodal data  (Ngiam et al. 2011; Gong et al. 2023; Zhu et al. 2023; Driess et al. 2023; Zhang et al. 2023) .\n\nUnlike previous works, our study adopts a modalityconversion approach  (Taghavi, Satvaty, and Sameti 2023) , wherein the AI system generates additional modality data, eliminating the exclusive dependence on user-provided data. This enables the AI system to process, generate, and integrate multiple data types, creating a more comprehensive representation of the problem. The effectiveness of this innovative approach is demonstrated through our experimental results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Essence Of Imagination In Philosophy",
      "text": "Constructive imagination is a complex cognitive process that goes beyond mere whimsical thought. It forms a fundamental building block of our cognitive and perceptual experiences and plays a crucial role in our understanding of the world  (Van Leeuwen 2013 , 2014) . In the context of our AI system, understanding the philosophy of constructive imagination is essential. This section explores key elements of constructive imagination that have informed the design of our system.\n\n• Connection to Sensory Perception: Constructive imagination engages our ability to amalgamate elements from memory, sensory experiences, and emotions  (Van Leeuwen 2013) . Brain studies indicate that the parts of the brain responsible for sensory perception also play a crucial role in the imagination process. These areas activate not only during sensory experiences but also when visualizing content related to a specific sensory modality, showing the importance of sensory parts of the brain in imagination  (Slotnick, Thompson, and Kosslyn 2005) . • Influence of Beliefs: Our beliefs, largely shaped by our past experiences, exert considerable influence over our imaginative processes. Studies show how people's beliefs influence their understanding of stories  (Weisberg and Goodstein 2009; Harris 2000) . This suggests that initiating imagination depends heavily on what we believe, meaning past beliefs affect current imagination  (Nichols 2000; Nichols and Stich 2003)    (Funkhouser and Spaulding 2009; Nanay 2013; Nichols 2006) . This connection between imagination and action underscores the potential applications of our AI system in various fields.\n\nIn summary, the philosophy of constructive imagination offers valuable insights into the cognitive processes that underlie imagination. These insights have been instrumental in shaping our AI system, enabling it to bridge the gap between textual inputs and other modalities, and generating deep and meaningful information.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ai Through The Psychoanalytic Lens",
      "text": "AI has been making strides in mimicking human cognitive processes, and one area of human cognition that offers intriguing possibilities is psychoanalysis. Psychoanalysis, a field of psychology that delves into the unconscious mind, could provide valuable insights into the development of AI systems. This section explores how the principles of psychoanalysis can be applied to AI, enhancing its ability to interpret and respond to inputs in a way that more closely resembles human cognition.\n\nPsychoanalysis, founded by Sigmund Freud, posits that the unconscious mind houses memories, desires, and feelings that may influence behavior and thought processes without conscious awareness  (Freud 2004; Lacan 2007 ). Freud's view provides a useful framework for understanding how past experiences can influence current behavior. For instance, a suppressed memory from childhood, such as a fear of darkness, can manifest in an adult's preference for well-lit environments, influencing their behavior without them being consciously aware of the cause  (Freud 2004 ).\n\nLacan, another influential figure in psychoanalysis, offered another perspective on the unconscious mind. He likened it to \"spoken language\", composed of symbols, images, and patterns of speech that unknowingly shape indi- viduals. This suggests that the unconscious mind is not just shaped by our social interactions and language use but also plays an active role in forming our individuals  (Lacan 2007) .\n\nIt is suggested that future AI systems could possess aspects of human psychological constructs, such as consciousness and unconsciousness  (Wallace 2005; Kurzweil 2005 ). These psychoanalytical theories offer a roadmap for developing systems that can generate a nuanced understanding of information, much like how humans use past experiences to inform present understanding. By incorporating elements of psychoanalysis into the design of our AI system, we aim to create a system that can interpret and respond to inputs in a way that more closely resembles human cognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "The Proposed System: Ai Systems With Imagination-Inspired Module",
      "text": "In the field of artificial intelligence, we often utilize datasets that pair texts with images under the assumption that these pairs are accurately matched  (Lin et al. 2014; Schuhmann et al. 2022) . We can train a model to generate an image from a piece of text. The objective is to discover a pattern or distribution that, when sampled, yields text and image pairs akin to those in our original dataset. This objective is encapsulated in the model's \"loss function\", which quantifies the discrepancy between the model's predictions and the actual data.\n\nOur system operates by taking a piece of text and generating an image based on the pattern it has learned from the correctly matched examples in the dataset  (Yasunaga et al. 2022; Rombach et al. 2022) . This learned pattern serves as a memory bank for the model, storing the information it has gleaned from past data. This stored information, which we can think of as the model's 'experiences', enables the model to generate an image from text.\n\nIn the aspect of philosophy, this process mirrors the human cognitive process of \"Re-construction of Experiences\" based on a given text, as discussed in the articles  (Van Leeuwen 2013 , 2014) . In the aspect of psychoanalysis, these systems, trained on historical data, generate an image   similar to the one they were trained on when presented with a familiar text. This can influence the model's current perception in ways that may not be immediately apparent.\n\nFor instance, if our model is trained with images associated with the concept of fear, and these images are predominantly dark, the system is likely to generate a dark image when given a text with the emotion of fear. This behavior mirrors the way our unconscious mind influences our perceptions and actions based on past experiences.\n\nWe employ text-to-image models to augment the input data, enabling us to \"Re-construction of Experiences\", a process we refer to as drawing \"Inspiration from Imagination\". We incorporate this as an imagination-inspired module in our system. The output of this module, along with the user's input, is then given the MLLM by the specific instruction, which is determined based on the task and experiment.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "System Architecture",
      "text": "The proposed system architecture designed for tasks involving image generation from textual inputs and leveraging MLLMs for zero-shot emotion recognition and question answering. The foundation of our system is common for both tasks, and two distinct architectures were required to accommodate the unique data features of each task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Common Foundation",
      "text": "The common foundation of our system involves generating an image from the input text using text-to-image models. After that, the input text and generated image are passed to an MLLM, as you can see in Figure  1 . We employed Stable Diffusion V2 for the text-toimage system and Instructblip as our MLLM due to its capability to interpret instructions  (Rombach et al. 2022; Dai et al. 2023) .\n\nEmotion Recognition Architecture For emotion recognition, the system processes the input text and generates an im-  age, as you can see in Figure  3 . Both the text and image are then forwarded to the MLLM. The model is guided in selecting an emotion through a series of instructions. However, the outputs from the MLLM and LLM may exceed the intended output label. To address this, we employ a helper model. This model embeds the output of the MLLM (or LMM) and each emotion, then identifies the label with the highest cosine similarity score, as you can see in Figure  4   put given to the model, followed by the answer produced by the model. In these cases, we separate the answer field and examine the new results named \"output-processing\". Examples are provided in Appendix 2.\n\nQuestion-Answering Architecture For the questionanswering task, special adjustments were necessary due to the longer textual inputs of the CoQA dataset. As shown in the token histogram in Figure  5 , most stories had between 308 and 385 tokens, which is five times the capacity of the Stable Diffusion 2's tokenize. To address this, stories were divided into five sections using GPT-3.5. We asked GPT-3, as the helper model, to give us four token numbers to split the story. The story was divided at the nearest full stop to  the token numbers specified. These sections were then converted into images and concatenated horizontally for input to the MLLM, as shown in Figure  7 . For each question, the model was provided with the story, previous questions, and prior responses to generate the final answer, as you can see in Figure  6 . In summary, the proposed system architecture is tailored to the specific requirements of emotion recognition and question answering, leveraging text-to-image models and MLLMs to achieve nuanced understanding and response generation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "We had some experiments in the direction of some questions. To conduct experiments, we utilized various instructions to guide the model's attention to specific parts of the input. Implementation tips are provided in Appendix 3. As shown in the Table  1 , our approach involved these:\n\n• Instructed to pay attention to both the textual and visual inputs (Gen Image Inp Text Both).\n\n• Instructed to focus only on the text input, despite receiving both textual and visual inputs (Gen Image Inp Text Txt).\n\n• Instructed to consider only the image while still receiving both textual and visual inputs (Gen Image Inp Text Img).\n\n• Instructed to consider only the image while receiving no textual input and just processed visual inputs (Gen Image No Text Img).\n\n• Special instruction 1: instructed to look at the task as a classification task (not emotion recognition), but the dataset's labels were given to the model by receiving both textual and visual inputs (Gen Image Inp Text P1).\n\n• Special instruction 2: instructed to choose an emotion (no label was given, and the model was free to choose any emotion) by receiving both textual and visual inputs (Gen Image Inp Text P2).\n\n• Special instruction 3: No instruction was given to the model, and it was free to generate any text by receiving both textual and visual inputs (Gen Image Inp Text P3). • Instructed to pay attention to both the textual and visual inputs. But the image was a Demo image (Dem Image Inp Text B).\n\nWe choose LLMs with a good score on the TriviaQA dataset, including MPT (Team 2023), GPT-J (Wang and Komatsuzaki 2021), Pythia  (Biderman et al. 2023) , Cerebras-GPT  (Dey et al. 2023) , StableLM  (Andonian et al. 2021) , Koala  (Geng et al. 2023)  and OPT  (Zhang et al. 2022) .\n\nThe results are in Tables 2 3 and the questions are as follows: Due to being a classification task in emotion recognition, the metric is the Weighted F1 score (WF1), and the metric for the question-answering task is the Overall F1 Score (OF1).\n\nIs Adding of the Imagination-inspired Module Effective? In addition to the good performance of LMMs in several tasks, in both emotion recognition and questionanswering tasks, in our experiments, the results were that our proposed system was able to significantly (WF1: 46.74%, 25.23%, and OF1:17% for MELD, IEMOCAP and CoQA datasets) outperforms the best model among LLMs (Koala-7B model, with WF1: 22.89%, 12.28% for MELD and IEMOCAP, and Cerebras-GPT-2.7B with WF1:7% for the CoQA dataset), which shows performance improvement by using imagination-inspired module in our system.\n\nIs Multimodality Sufficient, or Does Independent Perception Through Generated Images Truly Make a Difference? We conducted an experiment using a static image provided as a demo on the official MLLM GitHub page instead of the generated image. The results (WF1: 43.61%, 24.3%, and OF1:16.7% for MELD, IEMOCAP, and CoQA datasets) outperformed the best model among the unimodal-trained LLMs (Koala-7B model, with WF1: 22.89%, 12.28%, and Cerebras-GPT-2.7B model, with OF1: 7% for MELD, IEMOCAP and CoQA datasets). That clearly shows the ability of multimodal approaches and their superiority of them over unimodal approaches, even for inherently unimodal tasks. However, the best results achieved using generated images surpassed those achieved with the demo image and LLM (WF1: 46.74%, 25.23%, and OF1:17% for MELD, IEMO-CAP, and CoQA datasets). Which again shows the potential of using imagination-inspired modules.\n\nCan only Independent Perception Through Generated Images be in line with emotions? Our results unveiled significant potential for independent perception through generated images. Our MLLM exhibited superior performance over the leading unimodal model, Koala-7B, registering a WF1 score of 29.60% compared to Koala-7B's 22.89% on the MELD dataset. Although this outperformance was not consistent across all datasets, it underscored the impressive capabilities of our system in processing and understanding emotions, even in the absence of textual data. That also illuminated the potential of processing only generated im-ages, interpreted as \"the system's imagination\", in conveying information.\n\nStep-by-Step Instruction: Do the Model's Outputs Align with a Specific Emotion without Explicit Emotion Detection Instruction? We evaluated the use of \"Gradually Introduced\" instructions. Initially, we examined the model's outputs without any specific instructions (P3). Subsequently, we tested the instruction like \"What emotion do you perceive in this sentence\" (P2). After That, the instruction \"This is a classification task, choose one of the emotions\" followed by the dataset's emotions (P1). At the End of our primary test, the complete instruction \"What emotions do you think this pair of IMAGE and TEXT has?\" along with the dataset emotions (Gen Image Inp Text Both). We gave text and images to the model in all these four experiments. Exact instructions are provided in Appendix 2. We observed improvement of quality in each step from P3 to P1 (WF1: 14.90%, 40.96%, and 45.19% for MELD dataset, WF1: 10.54%, 12.56% and 22.55% for IEMOCAP dataset). The results highlight the crucial role of instruction in enhancing the model's performance. By mapping MLLM's outputs to labels in the P3 test using helper model, our system outperformed (WF1:14.9% and 10.54% for MELD and IEMOCAP datasets) some LLMs such as MPT-7B (WF1:10.09% for MELD dataset), Pythia-6.9b (WF1: 5.48% and 3.83% for MELD and IEMOCAP datasets), and OPT-6.7B (WF1:5.59% and 3.8% for MELD and IEMOCAP datasets). So we can say the system's output tended to reflect the emotional signs conveyed from the input. This finding suggests that our system can effectively perceive and express emotions in its output, even without explicit guidance. For instance, when the input text is filled with anger or happiness, the output text, whatever it be, is probable to have the same emotion. However, we remind you that our goal is not to reach a Stateof-The-Art result but to examine this issue in Large-scale Language models. These findings highlight the enhanced performance of our proposed AI system compared to other LLMs, even in inherently unimodal tasks. They also demonstrate the positive impact of using independent perception as generated images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion And Future Outlook",
      "text": "In this paper, we introduced an imagination-inspired module to AI systems for the first time, drawing inspiration from philosophy and psychoanalysis. We incorporated a text-toimage model, enabling a process similar to \"Re-construction of Experiences\". This innovation allows AI systems to develop unique interpretations that can significantly deviate from human interpretations, marking the advent of AI's \"Interpretable Misunderstanding\". In the zero-shot approach, our system outperformed the best Large Language Models (LLM) in emotion recognition tasks on the MELD, IEMO-CAP, and question-answering tasks on CoQA datasets. This is just the beginning. Looking forward, we envision more intuitive AI systems. For instance, when told about \"walking on the beach\", it could process the visualization of the grandeur sky, the tumultuous waves, and countless other details that weren't explicitly mentioned. These visualizations will not merely be numerical but interpretable for humans. We anticipate the next versions of our AI system that can generate other modalities from input in forms such as audio, and even action, to complete the scene with the sound of the ocean and the sense of walking. This approach could broaden our interaction spectrum with AI systems.  video. This feature enables us to assess the effectiveness of our imagination-inspired module in generating outputs that resemble real data across various modalities in future stages of our research. That is, if we ask ourselves at the next stage, \"Now that we have created this module inspired by imagination, how close are the data produced by this measure to the expected real data?\" now we have Valid data to measure the performance in comparison with them. It should be considered that this measure is beyond the ability of this module to produce natural samples. For example, if it is the modality of the image if we use the text \"How happy I am.\" the module generates an image, making it consistent with human logic (what text-to-image systems are trained to minimize the value of the loss function between the generated image and the actual resolution) is an issue, but We need to be able to measure how similar this generated image is to that actual situation. Or if we consider that in the future we will go to other modalities, we can use this example that the voice produced from the sentence \"I'm afraid\" is natural, but it is also important how similar the voice of the same person in the same situation is.\n\n• Compared to other emotion recognition datasets available in the field, these two datasets offer a diverse range of text samples that are derived from everyday conversations. They simulate ordinary dialogues among individuals, reflecting the natural interactions that occur in real-life scenarios.\n\nYou can see some pairs of input text, generated image, and corresponding true label in Figure  9  Question Answering\n\nWe have selected the coQA dataset to evaluate the performance of our system in the task of question and answer for the following reasons:\n\n• One noteworthy characteristic of this dataset is the presence of interconnected questions, where some input questions serve as continuations of previous ones. This structure allows us to explore the realm of multi-turn question and answer systems. For instance, in two consecutive questions, they may be formulated as follows: \"What color was Cotton? Where did she live?\" Here, the second question refers to the person mentioned in the preceding question. While our current research focuses on single-question scenarios, this dataset provides an opportunity to extend our investigations into the field of multi-turn question and answer systems. • The coQA dataset adopts a narrative mode, which is evident in the accompanying images. These images are derived from the input text and contribute to the overall storytelling by providing meaningful visual representations. For instance, in the Robinson reference image, the pictures progressively depict the story, showcasing elements such as adventure, sailing, solitude, cultivation, and the presence of the author, as you can see in Figure  10 . In contrast, other datasets like TriviaQA are primarily factbased, as depicted in Figure  11 . The rich diversity of data in the coQA dataset enables us to explore our system's capabilities across various data types, facilitating a comprehensive evaluation of its strengths and weaknesses. • The input text in the coQA dataset contains all the necessary information for processing. Since our objective is to assess the AI system's ability to comprehend the input text, it is crucial that the input data allows the system to answer questions based on its acquired prior knowledge (background knowledge) obtained during training. Thus, the absence of additional external knowledge in the input data ensures a focus on the system's internal understanding and reasoning abilities. • Another benefit of these stories is that it allows us to add a nice detail to the images. For example, in the story \"CHAPTER II. ON A MOUNTAIN PATH\" in Figure  12 , from where it describes a war, we see that our images have a darker color and details such as armor, helmets, and other things are added. While they are not explicitly mentioned in the story. Or where he talks about Rome in the story \"CHAPTER III. THE ROMAN BARONS\" in Figure  13 , we see that our images add details of Roman architecture, or the way people dressed at that time.",
      "page_start": 7,
      "page_end": 11
    },
    {
      "section_name": "Access To Datasets",
      "text": "In this section, to enhance clarity and facilitate a comprehensive understanding of the article's process, we provide sample inputs and outputs from a series of tests. The results for the emotion recognition task are presented for the IEMO-CAP dataset:",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Our Ai System Experiments",
      "text": "Here we present all out experiments. Based on our system's architecture, at first we should converts texts to images. These images are the same for all experiment. you can see inputs and corresponding generated image in Figure  14  There are experiments there:\n\n• Gen Image Inp Text Both :",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: These mental images substantially influ-",
      "page": 1
    },
    {
      "caption": "Figure 1: Proposed system in comparison to Unimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: This system bridges the gap between",
      "page": 1
    },
    {
      "caption": "Figure 2: arXiv:2308.10354v1  [cs.AI]  20 Aug 2023",
      "page": 1
    },
    {
      "caption": "Figure 2: “Re-construction of Experiences”. When we re-",
      "page": 2
    },
    {
      "caption": "Figure 1: In our imple-",
      "page": 2
    },
    {
      "caption": "Figure 3: MELD and IEMOCAP Text-to-Image sample. The",
      "page": 3
    },
    {
      "caption": "Figure 4: Emotion recognition system. The Text-to-Image model translates input text into an image and forwards it to the",
      "page": 4
    },
    {
      "caption": "Figure 1: We employed Stable Diffusion V2 for the text-to-",
      "page": 4
    },
    {
      "caption": "Figure 5: Histogram of the Text-to-Image model input",
      "page": 5
    },
    {
      "caption": "Figure 3: Both the text and image are",
      "page": 5
    },
    {
      "caption": "Figure 4: (Thakur et al.",
      "page": 5
    },
    {
      "caption": "Figure 5: , most stories had between",
      "page": 5
    },
    {
      "caption": "Figure 6: Question answering system. A helper LLM converts the story to five segments, then the Text-to-Image model converts",
      "page": 6
    },
    {
      "caption": "Figure 7: CoQA Text-to-Image sample. GPT3.5 as a helper LLM was employed to partition each narrative into five distinct",
      "page": 6
    },
    {
      "caption": "Figure 7: For each question, the",
      "page": 6
    },
    {
      "caption": "Figure 6: In summary, the proposed system architecture is tailored",
      "page": 6
    },
    {
      "caption": "Figure 8: Example pairs of input text, generated image, and true label in MELD dataset.",
      "page": 10
    },
    {
      "caption": "Figure 9: Example pairs of input text, generated image, and true label in the IEMOCAP dataset.",
      "page": 10
    },
    {
      "caption": "Figure 9: Question Answering",
      "page": 10
    },
    {
      "caption": "Figure 10: An example of images produced by the text-to-image conversion model from the CoQA dataset. As you can see, there",
      "page": 11
    },
    {
      "caption": "Figure 11: The rich diversity of data",
      "page": 11
    },
    {
      "caption": "Figure 13: , we see that our images add details of Roman",
      "page": 11
    },
    {
      "caption": "Figure 14: There are experiments there:",
      "page": 11
    },
    {
      "caption": "Figure 11: An example of images produced by the text-to-image conversion model from the TriviaQA dataset. This is a sample",
      "page": 12
    },
    {
      "caption": "Figure 12: Example of CoQA dataset. As you can see in this example, while maintaining the general state of the story, which is",
      "page": 13
    },
    {
      "caption": "Figure 13: Another example of CoQA dataset. In this example of the data, the addition of the unmentioned details can be well",
      "page": 13
    },
    {
      "caption": "Figure 14: The input data and the corresponding generated image. As you can see in the image, we created the corresponding",
      "page": 14
    },
    {
      "caption": "Figure 15: Demo image provided in instructblip GitHub. We used it as constant image to examine effect of using generated",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Experiments guide. This Table presents the experimental specifications for each investigation, including the type of",
      "data": [
        {
          "Model": "Gen Image Inp Text Both\nGen Image Inp Text Txt\nGen Image Inp Text\nImg\nGen Image No Text\nImg\nGen Image Inp Text P1\nv Image Inp Text P2\nGen Image Inp Text P3\nDem Image Inp Text Both",
          "Modality": "Multimodal\nMultimodal\nMultimodal\nMultimodal\nMultimodal\nMultimodal\nMultimodal\nMultimodal",
          "Image": "Generated\nGenerated\nGenerated\nGenerated\nGenerated\nGenerated\nGenerated\nDemo",
          "Text": "Input\nInput\nInput\nNone\nInput\nInput\nInput\nInput",
          "Instruction": "Consider Inputs\nConsider Text\nConsider Image\nConsider Image\nChoose one of emotions\nWhat emotion does perceive\nNo Instruction\nConsider Inputs",
          "Output Processing": "-\n-\n-\n-\n-\n-\n-\n-"
        },
        {
          "Model": "MPT-7B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "GPT-J-6B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "Pythia-6.9b",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "Cerebras-GPT-2.7B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "StableLM-7B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "Koala-7B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        },
        {
          "Model": "OPT-6.7B",
          "Modality": "Unimodal",
          "Image": "-",
          "Text": "Input",
          "Instruction": "-",
          "Output Processing": "Yes"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Weighted F1 and Accuracy scores of multimodal and unimodal LLMs for emotion recognition task on MELD and",
      "data": [
        {
          "Experiments": "",
          "Modality": "",
          "Output Processing": "",
          "MELD": "WF1(%)\nAcc(%)",
          "IEMOCAP": "WF1(%)\nAcc(%)"
        },
        {
          "Experiments": "Gen Image Inp Text Both\nGen Image Inp Text Tex\nGen Image Inp Text\nImg\nGen Image No Text\nImg\nGen Image Inp Text P1\nGen Image Inp Text P2\nGen Image Inp Text P3\nDem Image Inp Text Both",
          "Modality": "Multimodal",
          "Output Processing": "-\n-\n-\n-\n-\n-\n-\n-",
          "MELD": "40.13\n40.76\n46.74\n46.51\n42.16\n42.29\n29.60\n30.72\n45.19\n44.98\n40.96\n39.27\n14.90\n12.03\n43.61\n43.56",
          "IEMOCAP": "25.23\n25.79\n23.25\n25.09\n22.35\n24.25\n6.92\n15.63\n22.55\n24.50\n12.56\n13.59\n10.54\n8.46\n24.3\n25.59"
        },
        {
          "Experiments": "MPT-7B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "10.09\n16.24",
          "IEMOCAP": "11.16\n10.70"
        },
        {
          "Experiments": "MPT-7B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "5.73\n15.36",
          "IEMOCAP": "10.51\n12.35"
        },
        {
          "Experiments": "GPT-J-6B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "18.71\n17.31",
          "IEMOCAP": "8.44\n8.21"
        },
        {
          "Experiments": "GPT-J-6B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "7.78\n16.09",
          "IEMOCAP": "4.63\n9.56"
        },
        {
          "Experiments": "Pythia-6.9B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "5.23\n15.13",
          "IEMOCAP": "3.83\n11.15"
        },
        {
          "Experiments": "Pythia-6.9B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "5.48\n15.40",
          "IEMOCAP": "3.57\n11.55"
        },
        {
          "Experiments": "Cerebras-GPT-2.7B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "15.09\n13.67",
          "IEMOCAP": "8.51\n8.01"
        },
        {
          "Experiments": "Cerebras-GPT-2.7B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "7.80\n15.59",
          "IEMOCAP": "5.09\n9.66"
        },
        {
          "Experiments": "StableLM-7B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "22.89\n20.91",
          "IEMOCAP": "10.20\n8.96"
        },
        {
          "Experiments": "StableLM-7B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "7.75\n16.13",
          "IEMOCAP": "5.12\n9.06"
        },
        {
          "Experiments": "Koala-7B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "22.89\n19.42",
          "IEMOCAP": "12.28\n13.34"
        },
        {
          "Experiments": "Koala-7B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "35.24\n10.76",
          "IEMOCAP": "7.42\n11.40"
        },
        {
          "Experiments": "OPT-6.7B",
          "Modality": "Unimodal",
          "Output Processing": "Yes",
          "MELD": "5.59\n15.28",
          "IEMOCAP": "3.79\n11.25"
        },
        {
          "Experiments": "OPT-6.7B",
          "Modality": "Unimodal",
          "Output Processing": "-",
          "MELD": "5.5\n15.47",
          "IEMOCAP": "3.8\n11.40"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Weighted F1 and Accuracy scores of multimodal and unimodal LLMs for emotion recognition task on MELD and",
      "data": [
        {
          "Experiments": "Gen Image Inp Text Both\nGen Image Inp Text Txt\nGen Image Inp Text\nImg\nGen Image No Text\nImg\nDem Image Inp Text Both",
          "Modality": "Multimodal",
          "OF1 (%)": "17.0\n16.7\n15.3\n1.7\n16.7"
        },
        {
          "Experiments": "Koala-7B\nStableLM-7B\nMPT-7B\nCerebras-GPT-2.7B",
          "Modality": "Unimodal\nUnimodal\nUnimodal\nUnimodal",
          "OF1 (%)": "2.1\n1.1\n1.9\n7.0"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiment": "Gen Image Inp Text Both\nGen Image Inp Text P1\nGen Image Inp Text P2\nGen Image Inp Text P3",
          "Instruction": "“What emotions do you think this pair of\nIMAGE and\nTEXT has?” + Datasets Emotions\n“This is a classification task, choose one of the emotions.”\n+ Datasets Emotions\n“What emotion do you perceive in this sentence”\n-"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 5: Experiment 1: Gen Image Inp Text Both. on IEMOCAP This is our basic experiment. As you can see we gave",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this pair of IMAGE and TEXT has? you answer should be one of\nfollowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT : I’m so sorry. Answer:",
          "Output": "[’Sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this pair of IMAGE and TEXT has? you answer should be one of\nfollowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT : [BREATHING] So what\ndo you think? Answer:",
          "Output": "[’Unknown’]",
          "Label": "Unknown"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this pair of IMAGE and TEXT has? you answer should be one of\nfollowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT : You’ve got a lot- oh, awe-\nsome Answer:",
          "Output": "[’Excitement’]",
          "Label": "Excitement"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 5: Experiment 1: Gen Image Inp Text Both. on IEMOCAP This is our basic experiment. As you can see we gave",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this TEXT has? you answer should be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT : I’m so sorry. Answer:",
          "Output": "[’Sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this TEXT has? you answer should be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT :\n[BREATHING] So what do you think? An-\nswer:",
          "Output": "[’Unknown’]",
          "Label": "Unknown"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this TEXT has? you answer should be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT : You’ve got a lot- oh, awesome Answer:",
          "Output": "[’Excitement’]",
          "Label": "Excitement"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 8: Experiment 4: Gen Image No Text Img. on IEMOCAP As you can see we gave these Instruction without user",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT : I’m so sorry. Answer:",
          "Output": "[’Sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT :\n[BREATHING] So what do you think? An-\nswer:",
          "Output": "[’Unknown’]",
          "Label": "Unknown"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown TEXT : You’ve got a lot- oh, awesome Answer:",
          "Output": "[’Excitement’]",
          "Label": "Excitement"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 8: Experiment 4: Gen Image No Text Img. on IEMOCAP As you can see we gave these Instruction without user",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown Answer:",
          "Output": "[’Anger,\nAnger,\nAnger,\nAnger,\nAnger,\nAnger,\nAnger,\nAnger,\nAnger, Anger,’]",
          "Label": "Anger"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown Answer:",
          "Output": "[’Neutral,\nHap-\npiness,\nSadness,\nAnger,\nFrus-\ntration,\nFear,\nExcitement,\nDisgust\nSur-\nprise,Unknown’]",
          "Label": "Happiness"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this IMAGE has? you answer should be one of following emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown Answer:",
          "Output": "[’Neutral,\nHap-\npiness,\nSadness,\nAnger,\nFrus-\ntration,\nFear,\nExcitement,\nDisgust,\nSur-\nprise,Unknown’]",
          "Label": "Happiness"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: This\nis\na\nclassification\nTask, choose one of emotions: Neutral, Happiness, Sadness, Anger, Frus-\ntration, Fear, Excitement, Disgust Surprise ,Unknown TEXT: I’m so sorry.\nAnswer:",
          "Output": "[’Sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: This\nis\na\nclassification\nTask, choose one of emotions: Neutral, Happiness, Sadness, Anger, Frus-\ntration, Fear, Excitement, Disgust Surprise ,Unknown TEXT: [BREATH-\nING] So what do you think? Answer:",
          "Output": "[’Neutral’]",
          "Label": "Neutral"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: This\nis\na\nclassification\nTask, choose one of emotions: Neutral, Happiness, Sadness, Anger, Frus-\ntration, Fear, Excitement, Disgust Surprise ,Unknown TEXT: You’ve got a\nlot- oh, awesome. Answer:",
          "Output": "[’Excitement’]",
          "Label": "Excitement"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what emotions do you per-\nceive in one sentence ? TEXT: I’m so sorry. Answer:",
          "Output": "[’sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what emotions do you per-\nceive in one sentence ? TEXT: [BREATHING] So what do you think? An-\nswer:",
          "Output": "[”I\nthink\nit’s\na\npositive\nemo-\ntion”]",
          "Label": "Happiness"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what emotions do you per-\nceive in one sentence ? TEXT: You’ve got a lot- oh, awesome. Answer:",
          "Output": "[’positive’]",
          "Label": "Happiness"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "I’m so sorry.",
          "Output": "[”I’m\nso\nsorry\ni’m so sorry i’m\nso\nsorry\ni’m so\nsorry i’m so sorry\n”]",
          "Label": "Fear"
        },
        {
          "No.": "2",
          "Input Instruction": "[BREATHING] So what do you think?",
          "Output": "[’What\ndo\nyou\nthink? ’]",
          "Label": "Fear"
        },
        {
          "No.": "3",
          "Input Instruction": "You’ve got a lot- oh, awesome.",
          "Output": "[”You’ve\ngot\na\nlot- oh, awesome\nt-shirt”]",
          "Label": "Excitement"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 12: Experiment 8: Dem Image Inp Text Both. on IEMOCAP As you can see we gave these Instruction with user",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink pair of\nIMAGE and TEXT has? you answer should be one of\nfol-\nlowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT: [BREATHING] So what\ndo you think? Answer:",
          "Output": "[’Sadness’]",
          "Label": "Sadness"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink pair of\nIMAGE and TEXT has? you answer should be one of\nfol-\nlowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT: [BREATHING] So what\ndo you think? Answer:",
          "Output": "[’Unknown’]",
          "Label": "Unknown"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink pair of\nIMAGE and TEXT has? you answer should be one of\nfol-\nlowing emotions: Neutral, Happiness, Sadness, Anger, Frustration, Fear,\nExcitement, Disgust Surprise ,Unknown TEXT: [BREATHING] So what\ndo you think? Answer:",
          "Output": "[’Excitement’]",
          "Label": "Excitement"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 14: Experiment 10: Koala-7B with “output-processing” on IEMOCAP. We gave these instructions to MLLM with",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING\nOF\nCONVERSATION:\nUSER: what emotions do you think this Text\nhas?\nyou\nanswer\nshould\nbe\none\nof\nfollow-\ning\nemotions: Neutral, Happiness, Sadness,\nAnger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown. Text: I’m so sorry.\nAnswer:",
          "LLM’s Answer": "”BEGINNING\nOF\nCONVERSATION:\nUSER : what\nemotion\ndo\nyou\nthink\nthis\nTEXT has?\nyou\nanswer\nshould\nbe\none\nof\nfollowing\nemotions:\nNeutral,\nHappiness,\nSadness, Anger,\nFrustration,\nFear,\nExcite-\nment,Disgust,Surprise,Unknown TEXT :\nI’m\nso sorry. Answer: ”",
          "Label": "Anger"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING\nOF\nCONVERSATION:\nUSER: what\nemotions\ndo\nyou\nthink\nthis\nTEXT has?\nyou\nanswer\nshould\nbe\none\nof\nfollowing\nemotions:\nNeutral,\nHappiness,\nSadness, Anger, Frustration, Fear, Excitement,\nDisgust Surprise ,Unknown. Text: [BREATH-\nING] So what do you think? Answer:",
          "LLM’s Answer": "’BEGINNING\nOF\nCONVERSATION:\nUSER : what\nemotion\ndo\nyou\nthink\nthis\nTEXT has?\nyou\nanswer\nshould\nbe\none\nof\nfollowing\nemotions:\nNeutral,\nHappiness,\nSadness, Anger,\nFrustration,\nFear,\nExcite-\nment,Disgust,Surprise,Unknown\nTEXT\n:\n[BREATHING] So what do you think? An-\nswer: Happiness’",
          "Label": "Neutral"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING\nOF\nCONVERSATION:\nUSER: what\nemotions\ndo\nyou\nthink\nthis\nTEXT has?\nyou\nanswer\nshould\nbe\none\nof\nfollowing\nemotions:\nNeutral,\nHappiness,\nSadness, Anger, Frustration, Fear, Excitement,\nDisgust Surprise ,Unknown. Text: You’ve got\na lot- oh, awesome. Answer:",
          "LLM’s Answer": "’BEGINNING\nOF\nCONVERSATION:\nUSER : what\nemotion\ndo\nyou\nthink\nthis\nTEXT has?\nyou\nanswer\nshould\nbe\none\nof\nfollowing\nemotions:\nNeutral,\nHappiness,\nSadness, Anger,\nFrustration,\nFear,\nExcite-\nment,Disgust,Surprise,Unknown\nTEXT\n:\nYou’ve\ngot\na\nlot-\noh,\nawesome. Answer:\nSurprised",
          "Label": "Excitement"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 14: Experiment 10: Koala-7B with “output-processing” on IEMOCAP. We gave these instructions to MLLM with",
      "data": [
        {
          "No.": "1",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this Text has? you answer\nshould be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown. Text: I’m so sorry.\nAnswer:",
          "Extracted\nAn-\nswer": "[” ”]",
          "Label": "Disgust"
        },
        {
          "No.": "2",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this TEXT has? you answer should be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown. Text: [BREATHING] So what do you think? Answer:",
          "Extracted\nAn-\nswer": "[’Happiness’]",
          "Label": "Happiness"
        },
        {
          "No.": "3",
          "Input Instruction": "BEGINNING OF CONVERSATION: USER: what\nemotions do you\nthink this TEXT has? you answer should be one of\nfollowing emotions:\nNeutral, Happiness, Sadness, Anger, Frustration, Fear, Excitement, Disgust\nSurprise ,Unknown. Text: You’ve got a lot- oh, awesome. Answer:",
          "Extracted\nAn-\nswer": "[”Surprised”]",
          "Label": "Surprise"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch",
      "authors": [
        "A Andonian",
        "Q Anthony",
        "S Biderman",
        "S Black",
        "P Gali",
        "L Gao",
        "E Hallahan",
        "J Levy-Kramer",
        "C Leahy",
        "L Nestler",
        "K Parker",
        "M Pieler",
        "S Purohit",
        "T Songz",
        "W Phil",
        "S Weinbach"
      ],
      "year": "2021",
      "venue": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch"
    },
    {
      "citation_id": "2",
      "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "authors": [
        "S Biderman",
        "H Schoelkopf",
        "Q Anthony",
        "H Bradley",
        "K O'brien",
        "E Hallahan",
        "M Khan",
        "S Purohit",
        "U Prashanth",
        "E Raff",
        "A Skowron",
        "L Sutawika"
      ],
      "year": "2023",
      "venue": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "arxiv": "arXiv:2304.01373"
    },
    {
      "citation_id": "3",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Language Models are Few-Shot Learners"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H Chung",
        "C Sutton",
        "S Gehrmann",
        "P Schuh",
        "K Shi",
        "S Tsvyashchenko",
        "J Maynez",
        "A Rao",
        "P Barnes",
        "Y Tay",
        "N Shazeer",
        "V Prabhakaran",
        "E Reif",
        "N Du",
        "B Hutchinson",
        "R Pope",
        "J Bradbury",
        "J Austin",
        "M Isard",
        "G Gur-Ari",
        "P Yin",
        "T Duke",
        "A Levskaya",
        "S Ghemawat",
        "S Dev",
        "H Michalewski",
        "X Garcia",
        "V Misra",
        "K Robinson",
        "L Fedus",
        "D Zhou",
        "D Ippolito",
        "D Luan",
        "H Lim",
        "B Zoph",
        "A Spiridonov",
        "R Sepassi",
        "D Dohan",
        "S Agrawal",
        "M Omernick",
        "A Dai",
        "T Pillai",
        "M Pellat",
        "A Lewkowycz",
        "E Moreira",
        "R Child",
        "O Polozov",
        "K Lee",
        "Z Zhou",
        "X Wang",
        "B Saeta",
        "M Diaz",
        "O Firat",
        "M Catasta",
        "J Wei",
        "K Meier-Hellstern",
        "D Eck",
        "J Dean",
        "S Petrov",
        "N Fiedel"
      ],
      "year": "2022",
      "venue": "PaLM: Scaling Language Modeling with Pathways",
      "arxiv": "arXiv:2204.02311"
    },
    {
      "citation_id": "6",
      "title": "Recreative Minds: Imagination in Philosophy and Psychology",
      "authors": [
        "G Currie",
        "I Ravenscroft"
      ],
      "year": "2002",
      "venue": "Recreative Minds: Imagination in Philosophy and Psychology"
    },
    {
      "citation_id": "7",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "arxiv": "arXiv:2305.06500"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "9",
      "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
      "authors": [
        "N Dey",
        "G Gosal",
        "H Zhiming; Chen; Khachane",
        "W Marshall",
        "R Pathria",
        "M Tom",
        "J Hestness",
        "D Driess",
        "F Xia",
        "M Sajjadi",
        "C Lynch",
        "A Chowdhery",
        "B Ichter",
        "A Wahid",
        "J Tompson",
        "Q Vuong",
        "T Yu",
        "W Huang",
        "Y Chebotar",
        "P Sermanet",
        "D Duckworth",
        "S Levine",
        "V Vanhoucke",
        "K Hausman",
        "M Toussaint",
        "K Greff",
        "A Zeng",
        "I Mordatch",
        "P Florence"
      ],
      "year": "2023",
      "venue": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
      "arxiv": "arXiv:2304.03208"
    },
    {
      "citation_id": "10",
      "title": "The Unconscious. Penguin Modern Classics",
      "authors": [
        "S Freud"
      ],
      "year": "2004",
      "venue": "The Unconscious. Penguin Modern Classics"
    },
    {
      "citation_id": "11",
      "title": "Imagination and Other Scripts",
      "authors": [
        "E Funkhouser",
        "S Spaulding"
      ],
      "year": "2009",
      "venue": "Philosophical Studies"
    },
    {
      "citation_id": "12",
      "title": "Koala: A Dialogue Model for Academic Research",
      "authors": [
        "X Geng",
        "A Gudibande",
        "H Liu",
        "E Wallace",
        "P Abbeel",
        "S Levine",
        "D Song"
      ],
      "year": "2023",
      "venue": "Koala: A Dialogue Model for Academic Research"
    },
    {
      "citation_id": "13",
      "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
      "authors": [
        "T Gong",
        "C Lyu",
        "S Zhang",
        "Y Wang",
        "M Zheng",
        "Q Zhao",
        "K Liu",
        "W Zhang",
        "P Luo",
        "K Chen"
      ],
      "year": "2023",
      "venue": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
      "arxiv": "arXiv:2305.04790"
    },
    {
      "citation_id": "14",
      "title": "Cambridge Dictionaries Online",
      "authors": [
        "A Harley"
      ],
      "year": "2000",
      "venue": "Proceedings of the 9th EURALEX International Congress"
    },
    {
      "citation_id": "15",
      "title": "Institut für Maschinelle Sprachverarbeitung",
      "venue": "Institut für Maschinelle Sprachverarbeitung"
    },
    {
      "citation_id": "16",
      "title": "The work of the imagination. Understanding children's worlds",
      "authors": [
        "P Harris"
      ],
      "year": "2000",
      "venue": "The work of the imagination. Understanding children's worlds"
    },
    {
      "citation_id": "17",
      "title": "Training Compute-Optimal Large Language Models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya",
        "T Cai",
        "E Rutherford",
        "D De Las Casas",
        "L Hendricks",
        "J Welbl",
        "A Clark",
        "T Hennigan",
        "E Noland",
        "K Millican",
        "G Van Den Driessche",
        "B Damoc",
        "A Guy",
        "S Osindero",
        "K Simonyan",
        "E Elsen",
        "J Rae",
        "O Vinyals",
        "L Sifre"
      ],
      "year": "2022",
      "venue": "Training Compute-Optimal Large Language Models",
      "arxiv": "arXiv:2203.15556"
    },
    {
      "citation_id": "18",
      "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
      "authors": [
        "M Joshi",
        "E Choi",
        "D Weld",
        "L Zettlemoyer",
        "R Salakhutdinov",
        "R Zemel"
      ],
      "year": "2014",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "The singularity is near: When humans transcend biology",
      "authors": [
        "R Kurzweil"
      ],
      "year": "2005",
      "venue": "The singularity is near: When humans transcend biology"
    },
    {
      "citation_id": "20",
      "title": "Ecrits",
      "authors": [
        "J Lacan"
      ],
      "year": "2007",
      "venue": "Ecrits"
    },
    {
      "citation_id": "21",
      "title": "Mothers' Behavior Modifications During Pretense and Their Possible Signal Value for Toddlers",
      "authors": [
        "A Lillard",
        "D Witherington"
      ],
      "year": "2004",
      "venue": "Developmental Psychology"
    },
    {
      "citation_id": "22",
      "title": "",
      "authors": [
        "T.-Y Lin",
        "M Maire",
        "S Belongie",
        "L Bourdev",
        "R Girshick",
        "J Hays",
        "P Perona",
        "D Ramanan",
        "C Zitnick",
        "P Dollár"
      ],
      "year": "2014",
      "venue": ""
    },
    {
      "citation_id": "23",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "24",
      "title": "Between Perception and Action",
      "authors": [
        "B Nanay"
      ],
      "year": "2013",
      "venue": "Between Perception and Action"
    },
    {
      "citation_id": "25",
      "title": "Multimodal Deep Learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML'11"
    },
    {
      "citation_id": "26",
      "title": "A cognitive theory of pretense",
      "authors": [
        "S Nichols"
      ],
      "year": "2000",
      "venue": "Cognition"
    },
    {
      "citation_id": "27",
      "title": "The Architecture of the Imagination: New Essays on Pretence, Possibility, and Fiction",
      "authors": [
        "S Nichols"
      ],
      "year": "2006",
      "venue": "The Architecture of the Imagination: New Essays on Pretence, Possibility, and Fiction"
    },
    {
      "citation_id": "28",
      "title": "Mindreading: An Integrated Account of Pretence, Self-Awareness, and Understanding Other Minds",
      "authors": [
        "S Nichols",
        "S Stich"
      ],
      "year": "2003",
      "venue": "Mindreading: An Integrated Account of Pretence, Self-Awareness, and Understanding Other Minds"
    },
    {
      "citation_id": "29",
      "title": "Mental Representations. Oxford Psychology Series",
      "authors": [
        "A Paivio"
      ],
      "year": "1990",
      "venue": "Mental Representations. Oxford Psychology Series"
    },
    {
      "citation_id": "30",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "CoQA: A Conversational Question Answering Challenge",
      "authors": [
        "S Reddy",
        "D Chen",
        "C Manning"
      ],
      "year": "2018",
      "venue": "CoQA: A Conversational Question Answering Challenge"
    },
    {
      "citation_id": "32",
      "title": "High-Resolution Image Synthesis With Latent Diffusion Models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "LAION-5B: An open large-scale dataset for training next generation image-text models",
      "authors": [
        "C Schuhmann",
        "R Beaumont",
        "R Vencu",
        "C Gordon",
        "R Wightman",
        "M Cherti",
        "T Coombes",
        "A Katta",
        "C Mullis",
        "M Wortsman",
        "P Schramowski",
        "S Kundurthy",
        "K Crowson",
        "L Schmidt",
        "R Kaczmarczyk",
        "J Jitsev"
      ],
      "year": "2022",
      "venue": "LAION-5B: An open large-scale dataset for training next generation image-text models"
    },
    {
      "citation_id": "34",
      "title": "Visual Mental Imagery Induces Retinotopically Organized Activation of Early Visual Areas",
      "authors": [
        "S Slotnick",
        "W Thompson",
        "S Kosslyn"
      ],
      "year": "2005",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "35",
      "title": "A Change of Heart",
      "authors": [
        "Z Taghavi",
        "A Satvaty",
        "H Sameti"
      ],
      "year": "2023",
      "venue": "2023. Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",
      "arxiv": "arXiv:2307.11584"
    },
    {
      "citation_id": "36",
      "title": "Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
      "authors": [
        "N Thakur",
        "N Reimers",
        "J Daxenberger",
        "I Gurevych"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "37",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and Efficient Foundation Language Models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "38",
      "title": "The Meanings of \"Imagine\" Part I: Constructive Imagination: The Meanings of \"Imagine",
      "authors": [
        "N Van Leeuwen"
      ],
      "year": "2013",
      "venue": "Philosophy Compass"
    },
    {
      "citation_id": "39",
      "title": "The Meanings of \"Imagine\" Part II: Attitude and Action: The Meanings of \"Imagine",
      "authors": [
        "N Van Leeuwen"
      ],
      "year": "2014",
      "venue": "The Meanings of \"Imagine\" Part II: Attitude and Action: The Meanings of \"Imagine"
    },
    {
      "citation_id": "40",
      "title": "Consciousness: A mathematical treatment of the global neuronal workspace model",
      "authors": [
        "R Wallace"
      ],
      "year": "2005",
      "venue": "Consciousness: A Mathematical Treatment of the Global Neuronal Workspace Model"
    },
    {
      "citation_id": "41",
      "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
      "authors": [
        "B Wang",
        "A Komatsuzaki"
      ],
      "year": "2021",
      "venue": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"
    },
    {
      "citation_id": "42",
      "title": "What Belongs in a Fictional World",
      "authors": [
        "D Weisberg",
        "J Goodstein"
      ],
      "year": "2009",
      "venue": "Journal of Cognition and Culture"
    },
    {
      "citation_id": "43",
      "title": "",
      "authors": [
        "M Yasunaga",
        "A Aghajanyan",
        "W Shi",
        "R James",
        "J Leskovec",
        "P Liang",
        "M Lewis",
        "L Zettlemoyer",
        "W Yih"
      ],
      "venue": ""
    },
    {
      "citation_id": "44",
      "title": "A Survey on Multimodal Large Language Models",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2023",
      "venue": "A Survey on Multimodal Large Language Models",
      "arxiv": "arXiv:2306.13549"
    },
    {
      "citation_id": "45",
      "title": "Transfer Visual Prompt Generator across LLMs",
      "authors": [
        "A Zhang",
        "H Fei",
        "Y Yao",
        "W Ji",
        "L Li",
        "Z Liu",
        "T.-S Chua"
      ],
      "year": "2023",
      "venue": "Transfer Visual Prompt Generator across LLMs",
      "arxiv": "arXiv:2305.01278"
    },
    {
      "citation_id": "46",
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "authors": [
        "S Zhang",
        "S Roller",
        "N Goyal",
        "M Artetxe",
        "M Chen",
        "S Chen",
        "C Dewan",
        "M Diab",
        "X Li",
        "X Lin",
        "T Mihaylov",
        "M Ott",
        "S Shleifer",
        "K Shuster",
        "D Simig",
        "P Koura",
        "A Sridhar",
        "T Wang",
        "L Zettlemoyer"
      ],
      "year": "2022",
      "venue": "OPT: Open Pre-trained Transformer Language Models",
      "arxiv": "arXiv:2205.01068"
    },
    {
      "citation_id": "47",
      "title": "Datasets Emotion Recognition We have selected the MELD and IEMOCAP databases to evaluate the performance of our system in the task of emotion recognition due to the following reasons: • Apart from textual data, these datasets also provide corresponding data in other modalities",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Datasets Emotion Recognition We have selected the MELD and IEMOCAP databases to evaluate the performance of our system in the task of emotion recognition due to the following reasons: • Apart from textual data, these datasets also provide corresponding data in other modalities",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}