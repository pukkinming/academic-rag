{
  "paper_id": "2308.03022v1",
  "title": "Sapien: Affective Virtual Agents Powered By Large Language Models*",
  "published": "2023-08-06T05:13:16Z",
  "authors": [
    "Masum Hasan",
    "Cengiz Ozel",
    "Sammy Potter",
    "Ehsan Hoque"
  ],
  "keywords": [
    "Virtual Avatars",
    "Virtual Agents",
    "Affective AI",
    "Large Language Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this demo paper, we introduce SAPIEN, a platform for high-fidelity virtual agents driven by large language models that can hold open domain conversations with users in 13 different languages, and display emotions through facial expressions and voice. The platform allows users to customize their virtual agent's personality, background, and conversation premise, thus providing a rich, immersive interaction experience. Furthermore, after the virtual meeting, the user can choose to get the conversation analyzed and receive actionable feedback on their communication skills. This paper illustrates an overview of the platform and discusses the various application domains of this technology, ranging from entertainment to mental health, communication training, language learning, education, healthcare, and beyond. Additionally, we consider the ethical implications of such realistic virtual agent representations and the potential challenges in ensuring responsible use.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Allowing a user to define the traits and characteristics of a virtual agent, carrying a dynamic conversation, and receiving automated feedback has been an open-ended research problem for many years  [1] . The rapid advancement of Large Language Models (LLMs) in recent years has enabled possibilities in designing user experiences that didn't exist before  [2] -  [4] . In this demo, we present Synthetic Anthropomorphic Personal Interaction ENgine (SAPIEN), a platform for LLM-powered high-fidelity virtual agents that can engage in real-time opendomain conversations, while also expressing emotions through voice and facial expressions.\n\nOne of the notable features of SAPIEN is its extensive range of customization options, allowing users to engage in immersive and meaningful interactions. Users can choose from a wide range of virtual agent avatars that reflect a diverse array of ages, gender, and ethnicities. Going further, users can select the desired personality, background, and conversational context of a virtual agent, creating an experience tailored to their specific needs or preferences.\n\nOnce a virtual agent is selected and its traits are defined, users can begin a real-time video call interaction with it. With the help of the large language model, the virtual agents dynamically adjust their emotional state, vocal, and facial expressions, showcasing a spectrum of seven basic emotions.\n\nNSF and NSF REU IIS-1750380, Seedling from Goergen Institute for Data Science (GIDS), and Gordon and Moore Foundation. SAPIEN leverages state-of-the-art models in Speech-to-Text  [5] ,  [6] , Text-to-Speech  [7] -  [9] , and large language modeling  [2] ,  [4] ,  [10] -  [14] . The virtual agents fluently speak thirteen different languages and counting, making it accessible across a global user base.\n\nUpon finishing a video call with the virtual agents, a user can choose to get their conversation analyzed for personalized feedback. The system provides AI-generated feedback to the user based on the user's goal. The user can decide the topic of the feedback to suit their learning goal and repeat the conversation until the learning goal is met. The inherent flexibility of the virtual agent persona and the feedback could make it potentially applicable to a myriad of applications, including communication training, language learning, and professional applications like healthcare, sales, and leadership training.\n\nWith the rising technical capabilities of LLMs, there is expected to be a drastic shift in the labor market in the coming years  [15] . According to recent studies  [15] , the importance of the job market is going to shift from hard technical skills to soft \"human\" skills. In this changing landscape, SAPIEN can help people adapt and cope, by helping them cultivate human skills with the help of AI.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. System Description",
      "text": "The overall working of SAPIEN Virtual Agents, referred to as 'Bot' for simplicity, is represented in Figure  2 . The SAPIEN system is initialized when a user's speech utterance is captured and transmitted to our back-end server for processing. This utterance is transcribed into text by a high-precision Speech  to Text (STT) model  [5] ,  [6] ,  [16] ,  [17]  and subsequently processed by an autoregressive Large Language Model (LLM) fine-tuned for instruction following  [3] ,  [4] ,  [10] -  [14] ,  [18] . The LLM is conditioned on user-defined parameters like personality traits, conversation premise, user information, and previous conversation history. To prevent inappropriate or offensive behavior, the LLM also adheres to system guardrails. A notable aspect of the LLM is also predicting the virtual agent's emotional state. Conditioning on the user-defined parameters, system guardrails, and previous conversation history, the LLM is instructed to generate the bot's response, alongside the appropriate emotional state of the bot from the following list: Neutral, Happy, Sad, Angry, Surprised, Afraid, and Disgusted.\n\nThis emotional state, along with the text response, is used to generate an audio file of the bot's response using a Text to Speech (TTS) model. Concurrently, the emotional state triggers the selection of a corresponding facial expression from our pre-recorded motion capture database. This facial expression data, in the form of blendshapes, is passed to a 3D game engine to animate the virtual agent.\n\nThe resultant animation and generated audio are synchronized, forming a coherent, visually expressive response from the virtual agent. This combined output is streamed to the user's web browser in near real-time, allowing for an immersive experience close to an actual video call.\n\nOnce the conversation is over, the user can opt-in to receive feedback on their conversation. An LLM is instructed to analyze the conversation transcript based on the user's goal, identify strengths and weaknesses on the user's communication skill, and generate actionable feedback for the user.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Applications",
      "text": "The customizability of the conversation scenario, dynamic dialogues, and the feedback system combined make SAPIEN uniquely suitable for a variety of communication training purposes. For example, the system can be used as a com-munication practice tool for people with social anxiety or neurodiversity  [19] ,  [20] , public speaking  [21] , job interviews  [22] , helping elderly with social skills  [23] , and even speed dating  [24] . It also has an excellent potential for professional applications. Such as training doctors in bedside manners or delivering difficult news to their patients  [25] , personalized training for leadership, business negotiation, sales, marketing, etc. The multilingual ability makes the platform a powerful tool for language learners. Furthermore, the non-judgemental, low stake, repeatable conversations with virtual agents make the platform a helpful tool for anyone to roleplay any difficult interpersonal scenario in a personal or professional setup.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. The Demo",
      "text": "Our platform is hosted in the cloud and accessible from any part of the world. During the conference demo, we wish to have the visitors live interact with SAPIEN virtual agents in a variety of interesting scenarios and receive immediate feedback on their communication skills. We will also prepare some pre-recorded user interaction videos to demonstrate any rare or difficult cases or as a backup for technical failures.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "SAPIEN is designed to augment and enrich our capacity for communication, empathy, and understanding, but not substitute human connections. To safeguard against potential emotional dependencies on the system, SAPIEN does not retain the memory of previous interactions, and the conversations are limited to a 10 minutes window with a warning at the 8minute mark. To prevent the practice of bullying or abusive behaviors using our system, we enabled our virtual agents to end the video call if the user repeatedly displays aggressive or offensive behavior. We are continuously investigating more safety and ethical issues regarding the use of the system.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Face-to-face video call interaction with SAPIENTM Virtual Agent",
      "page": 1
    },
    {
      "caption": "Figure 2: The SAPIEN",
      "page": 1
    },
    {
      "caption": "Figure 2: A single turn conversation flow in SAPIEN. User utterance is transcribed and sent to LLM. The LLM response is spoken out by the virtual agent.",
      "page": 2
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Rich nonverbal sensing technology for automated social skills training",
      "authors": [
        "M Hoque",
        "R Picard"
      ],
      "year": "2014",
      "venue": "Computer"
    },
    {
      "citation_id": "2",
      "title": "Introducing chatgpt",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Introducing chatgpt"
    },
    {
      "citation_id": "3",
      "title": "Anthropic -introducing claude",
      "year": "2023",
      "venue": "Anthropic -introducing claude"
    },
    {
      "citation_id": "4",
      "title": "An important next step on our ai journey",
      "authors": [
        "G Ai"
      ],
      "year": "2023",
      "venue": "An important next step on our ai journey"
    },
    {
      "citation_id": "5",
      "title": "Recent advances in end-to-end automatic speech recognition",
      "authors": [
        "J Li"
      ],
      "year": "2022",
      "venue": "Recent advances in end-to-end automatic speech recognition"
    },
    {
      "citation_id": "6",
      "title": "The microsoft 2017 conversational speech recognition system",
      "authors": [
        "W Xiong",
        "L Wu",
        "F Alleva",
        "J Droppo",
        "X Huang",
        "A Stolcke"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Tacotron: Towards endto-end speech synthesis",
      "arxiv": "arXiv:1703.10135"
    },
    {
      "citation_id": "8",
      "title": "Lightspeech: Lightweight and fast text to speech with neural architecture search",
      "authors": [
        "R Luo",
        "X Tan",
        "R Wang",
        "T Qin",
        "J Li",
        "S Zhao",
        "E Chen",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Priorgrad: Improving conditional denoising diffusion models with data-driven adaptive prior",
      "authors": [
        "H S.-G. Lee",
        "C Kim",
        "X Shin",
        "C Tan",
        "Q Liu",
        "T Meng",
        "W Qin",
        "S Chen",
        "T.-Y Yoon",
        "Liu"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "10",
      "title": "Stanford alpaca: An instruction-following llama model",
      "authors": [
        "R Taori",
        "I Gulrajani",
        "T Zhang",
        "Y Dubois",
        "X Li",
        "C Guestrin",
        "P Liang",
        "T Hashimoto"
      ],
      "year": "2023",
      "venue": "Stanford alpaca: An instruction-following llama model"
    },
    {
      "citation_id": "11",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "13",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida",
        "C Wainwright",
        "P Mishkin",
        "C Zhang",
        "S Agarwal",
        "K Slama",
        "A Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Openassistant conversations-democratizing large language model alignment",
      "authors": [
        "A Köpf",
        "Y Kilcher",
        "D Von Rütte",
        "S Anagnostidis",
        "Z.-R Tam",
        "K Stevens",
        "A Barhoum",
        "N Duc",
        "O Stanley",
        "R Nagyfi"
      ],
      "year": "2023",
      "venue": "Openassistant conversations-democratizing large language model alignment",
      "arxiv": "arXiv:2304.07327"
    },
    {
      "citation_id": "15",
      "title": "Gpts are gpts: An early look at the labor market impact potential of large language models",
      "authors": [
        "T Eloundou",
        "S Manning",
        "P Mishkin",
        "D Rock"
      ],
      "year": "2023",
      "venue": "Gpts are gpts: An early look at the labor market impact potential of large language models",
      "arxiv": "arXiv:2303.10130"
    },
    {
      "citation_id": "16",
      "title": "Fastcorrect: Fast error correction with edit alignment for automatic speech recognition",
      "authors": [
        "Y Leng",
        "X Tan",
        "L Zhu",
        "J Xu",
        "R Luo",
        "L Liu",
        "T Qin",
        "X Li",
        "E Lin",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Cross-domain speech recognition with unsupervised character-level distribution matching",
      "authors": [
        "W Hou",
        "J Wang",
        "X Tan",
        "T Qin",
        "T Shinozaki"
      ],
      "year": "2021",
      "venue": "Cross-domain speech recognition with unsupervised character-level distribution matching"
    },
    {
      "citation_id": "18",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "W.-L Chiang",
        "Z Li",
        "Z Lin",
        "Y Sheng",
        "Z Wu",
        "H Zhang",
        "L Zheng",
        "S Zhuang",
        "Y Zhuang",
        "J Gonzalez",
        "I Stoica",
        "E Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "19",
      "title": "A virtual conversational agent for teens with autism spectrum disorder: Experimental results and design lessons",
      "authors": [
        "M Ali",
        "S Razavi",
        "R Langevin",
        "A Al Mamun",
        "B Kane",
        "R Rawassizadeh",
        "L Schubert",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents, ser. IVA '20",
      "doi": "10.1145/3383652.3423900"
    },
    {
      "citation_id": "20",
      "title": "The lissa virtual human and asd teens: An overview of initial experiments",
      "authors": [
        "S Razavi",
        "M Ali",
        "T Smith",
        "L Schubert",
        "M Hoque"
      ],
      "year": "2016",
      "venue": "Intelligent Virtual Agents"
    },
    {
      "citation_id": "21",
      "title": "Roc speak: Semiautomated personalized feedback on nonverbal behavior from recorded videos",
      "authors": [
        "M Fung",
        "Y Jin",
        "R Zhao",
        "M Hoque"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, ser. UbiComp '15",
      "doi": "10.1145/2750858.2804265"
    },
    {
      "citation_id": "22",
      "title": "Mach: My automated conversation coach",
      "authors": [
        "M Hoque",
        "M Courgeon",
        "J.-C Martin",
        "B Mutlu",
        "R Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, ser. UbiComp '13",
      "doi": "10.1145/2493432.2493502"
    },
    {
      "citation_id": "23",
      "title": "Discourse behavior of older adults interacting with a dialogue agent competent in multiple topics",
      "authors": [
        "S Razavi",
        "L Schubert",
        "K Van Orden",
        "M Ali",
        "B Kane",
        "E Hoque"
      ],
      "year": "2022",
      "venue": "ACM Trans. Interact. Intell. Syst",
      "doi": "10.1145/3484510"
    },
    {
      "citation_id": "24",
      "title": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "authors": [
        "M Ali",
        "D Crasta",
        "L Jin",
        "A Baretto",
        "J Pachter",
        "R Rogge",
        "M Hoque"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "25",
      "title": "Novel computational linguistic measures, dialogue system and the development of sophie: Standardized online patient for healthcare interaction education",
      "authors": [
        "M Ali",
        "T Sen",
        "B Kane",
        "S Bose",
        "T Carroll",
        "R Epstein",
        "L Schubert",
        "E Hoque"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3054717"
    }
  ]
}