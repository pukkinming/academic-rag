{
  "paper_id": "2201.09165v1",
  "title": "A Pre-Trained Audio-Visual Transformer For Emotion Recognition",
  "published": "2022-01-23T03:09:16Z",
  "authors": [
    "Minh Tran",
    "Mohammad Soleymani"
  ],
  "keywords": [
    "Emotion recognition",
    "Transformer",
    "multiomdal fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we introduce a pretrained audio-visual Transformer trained on more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2 dataset for human behavior understanding. The model aims to capture and extract useful information from the interactions between human facial and auditory behaviors, with application in emotion recognition. We evaluate the model performance on two datasets, namely CREMAD-D (emotion classification) and MSP-IMPROV (continuous emotion regression). Experimental results show that fine-tuning the pre-trained model helps improving emotion classification accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous emotion recognition by 0.03-0.09 compared to the same model trained from scratch. We also demonstrate the robustness of finetuning the pre-trained model in a low-resource setting. With only 10% of the original training set provided, finetuning the pre-trained model can lead to at least 10% better emotion recognition accuracy and a CCC score improvement by at least 0.1 for continuous emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent advances in machine learning and signal processing enable an unprecedented opportunity to computationally analyze and predict social behaviors. A better understanding of how people behave and express themselves could have wide applicability. Much human interaction research (e.g. emotion recognition) is task-oriented, which often requires time-consuming and expensive data collection processes; and hence, suffers from small population that prevents ML models to generalize well. Despite the scarcity of labeled data, there is an abundance of data on human communication that is unlabeled, multi-modal, and easily accessible  [1] . This opens an opportunity to address the challenge, by creating self-supervised pretrained models that are trained on unlabeled data and can be finetuned for downstream tasks. Similar approaches have been very successful in NLP  [2]  and speech processing  [3]  tasks.\n\nMost research extending the standard Transformer  [4]  in a multimodal context focuses on the visual-and-language domain. Existing work generally utilize the language-pretrained BERT  [2]  and train only the visual components through either the single-stream framework (image and text are jointly processed by a single encoder)  [5, 6]  or dual-stream framework (with separate visual and text encoders)  [7, 8] . To the best of our knowledge, Lee et al. present the only pretrained Transformer-based model for the audio-and-visual domain  [9] . Their end-to-end model contains two Transformers to encode audio and visual inputs independently, followed by another Transformer that processes the encoded audio and video signals sequentially. Lee et al. pretrain their model on Kinetics-700 (containing 700 human action classes)  [10]  and AudioSet (containing 632 audio event classes)  [11] . Since both datasets contain little information on human interactions, the pretrained models would not be appropriate for downstream tasks such as emotion recognition.\n\nIn this study, we present the first pretrained audio-visual Transformer-based model that learns from human communicative behaviors. We then validate the the pretrained model for the downstream task of emotion recognition on the CREMA-D dataset  [12]  and the MSP-IMPROV dataset  [13] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Transformer Architecture",
      "text": "We adapt the Multimodal Transformer (MulT) architecture  [14]  for the pretraining task. At the high level, the architecture consists of 4 main components: the temporal convolutions that projects features of different modalities to the same dimension, a sinusoidal positional encoding to capture temporal information, the Cross-modal Transformers that allow one modality to pass information to another, and the standard Self-Attention Transformers that process the fused information produced by the Cross-modal Transformers. An overview of the MulT architecture is available in Figure  1  (right).\n\nAt the core of MulT is the Cross-modal Attention Block (Fig 1  left ), which differs from the standard Transformer b is passed to the b → a Cross-modal Attention Block regardless of the layer position) while the standard Transformer takes intermediate-level features as input. Tsai et al.  [14]  empirically show that adapting low-level features for the Cross-modal Attention Block is beneficial for MulT. With only 2 modalities in consideration, we have 2 types of Cross-modal Attention Blocks (V → A and A → V ). Eventually, we concatenate the outputs of the Cross-modal Transformers and pass them through a standard Self-attention Transformer  [4] . The outputs of the Self-attention Transformer are finally converted to the original audio and visual feature dimensions for predictions using two independent fully-connected layers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pretraining Procedure",
      "text": "Following prior work on pretrained Transformers  [2, 3, 9] , we use the masked frame prediction task to train our model. Specifically, we randomly select 15% of the frames, mask them for both the audio and visual inputs, and train the model to reconstruct the masked frames. Following  [2, 3]  on the selected frames for masking, we mask the frames all to zero with a probability of 0.8, replace them with randomly selected frames with a probability of 0.1 and keep them untouched with a probability of 0.1. Similar to  [3] , we use the L1-Loss to measure reconstruction error. The loss for each prediction is the sum of the L1-loss for the audio modality and the L1loss for the visual modality. We adapt the strategy of masking consecutive frames from  [3]  to prevent the model from ex-ploiting local smoothness. We use dynamic masking for the training set (the masked frames of each input sequence are selected independently every time the sequence is called) and static masking for the validation set (the masked frames for each input sequence are pre-computed) to make the comparison between models' performances fair.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Selection",
      "text": "The Multimodal Transformer is not end-to-end, so we need to extract acoustic and visual features as inputs to the model. Since features can be more or less powerful depending on the context of their usage and the target of our study is emotion recognition, we compare different baseline features on the CREMA-D and MSP-IMPROV datasets  [12, 13] . The motivation for pretraining data with MulT is to capture and model temporal dependencies so we also want the base features to be temporally independent. Thus, even though features extracted from pretrained Speech Transformers such as  [3, 15, 16]  are powerful, they are not suitable to be base features for MulT.\n\nWith these considerations, we select and compare the features extracted from pretrained Facenet  [17] , pretrained ResNet  [18]  and OpenFace Action Units' intensities  [19]  for the visual modality. For the acoustic modality, we compare Mel-scale spectrogram, Linear-scale spectrogram and features extracted by TRILL  [20] . To extract features from FaceNet and ResNet for a given video, we extract frames from the video at a constant rate and crop the face regions before feeding them into the pretrained models. Because TRILL only provides one vector representation for each input acoustic sequence, we split the input sequence into segments that matches a specified frame rate and extract the representations of the segments. To make the comparison fair, we apply all extracted features to the same model (a single-layer Gated Recurrent Unit with a hidden size of 512 and dropout ra-tio of 0.2 with fixed initialization) for emotion classification (CREMA-D dataset) and continuous emotion estimations (MSP-IMRPOV dataset). We find that extracted OpenFace and TRILL features outperform other baseline features by a considerable margin on both datasets. On the CREMA-D dataset, OpenFace shows a gain of 17% in Accuracy in comparison with the ResNet representations, and TRILL shows a gain of more than 11% in Accuracy from the Linear-scale spectrogram. On the MSP-IMPROV dataset, TRILL outperforms the Linear-scale spectrogram with a CCC margin of at least 0.03 while OpenFace outperforms the second-best baseline with a CCC margin of at least 0.14.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "Voxceleb2 We use the Voxceleb2 dataset for pretraining  [1] . It contains more than 1M utterances from more than 6,000 celebrities collected from around 150K videos on Youtube. The dataset is fairly gender balanced (61% are men). For the acoustic modality, we first segment the audio, into 200ms segments, before feeding them into TRILL  [20]  for feature extraction. Because TRILL originally provides a single embedding for an audio input as a whole, we do not want to extract the features with smaller segment duration. For the visual modality, we use OpenFace2.0  [19]  to track 17 Facial Action Unit (AU) intensities from the videos at 30 FPS. Since there is a high variation in the video quality of the Vox-celeb2 dataset, we remove frames with detection confidence below 80%. We then downsample OpenFace outputs to 5 FPS to match the frame rate of the acoustic modality. We remove utterance samples with the audio and video features misaligned for more than 1 second (more than 5 frames difference). Although MulT can handle unaligned multimodal sequences, the model achieves better performance with aligned sequences. In the end, we end up with a training dataset of 524K utterances from about 4K speakers (the average duration of each utterance is 5s with a standard deviation of 0.7s). CREMA-D The CREMA-D dataset is an acted audiovisual database consisting of 6 basic emotional states (happy, sad, anger, fear, disgust and neutral). It includes 7,442 video clips from 91 actors speaking 12 sentences with different emotions. The emotion labels are collected through crowd-sourcing from 2,443 raters, and the human recognition accuracy of intended emotion is 63.6%. The emotion classes in the dataset are balanced. In this study, we perform speaker-independent split of the CREMA-D dataset into the train-validation-test set with a ratio of 60%-20%-20% respectively. MSP-IMRPOV The MSP-IMPROV dataset is an acted audiovisual database that includes emotional interactions between people in a dyadic conversational setting. The conversation scenarios are designed to invoke realistic emotions. The dataset consists of 8,450 video recordings that are",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pretraining Implementation Details",
      "text": "Following prior work on pretraining Transformers  [2, 3] , we implement the pretraining task with two model settings: BASE and LARGE. For both configurations, we set the number of attention heads to 12, the number of consecutive frames for masking to 3 (∼ 0.6 sec) and the length of each processed sequence is 50 ( ∼ 10 sec). The hidden sizes for each of the audio and visual modality are 288 (BASE) and 576 (LARGE). The sizes of the feedforward layers in each cross-modal attention block are 1152 (BASE) and 1536 (LARGE). The sizes of the feed-forward layers in each Self-Attention Block are 2304 (BASE) and 3072 (LARGE). The BASE configuration has 6 A → V cross-modal attention blocks, 6 V → A cross-modal attention blocks and 6 self-attention blocks, which sums up to 38.3M parameters. The LARGE configuration has 8 A → V cross-modal attention blocks, 8 V → A cross-modal attention blocks and 8 self-attention blocks that totals 89.2M parameters. We train both models with the Adam optimizer  [22] . The learning rate is set to 5e -4 , with a linear learning rate scheduler and a warmup portion of 0.1. Both models are trained with a batch size of 64 for 30 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Application On Downstream Task",
      "text": "For fine-tuning, the last elements from the outputs of the pretrained MulT are passed to a residual block followed by a fully-connected layer to make final predictions. We compare the performance of the fine-tuned MulT to 4 baseline models: Early Fusion GRU (EF-GRU), Late Fusion GRU (LF-GRU), the Tensor Fusion Network (TFN)  [21]  and the Multi- modal Transformer without the pretrained weights initialization. It is important to note that TFN only processes static inputs, i.e., each modality of a sample is represented by a vector. However, we decide to include it as a baseline model because TRILL is originally developed to represent an audio as a whole with a vector  [20] . Hence, for each video, we use the vector representation extracted from TRILL for the acoustic modality along with the average of the 17 OpenFace AU intensities for the visual modality as inputs to TFN.\n\nTo make the comparisons fair for EF-GRU and LF-GRU, we make them Bidirectional and control the hidden size as well as number of layers such that the number of parameters of these models are approximately the same with the BASE configuration of MulT. For MulT without pretrained weights, we perform experiments with both the BASE and LARGE configurations and report the better performing configuration based on the validation set. We train all of the models until early stopping occurs on the validation set.\n\nTable  1  shows the performance of different models on the CREMA-D and MSP-IMPROV datasets. Since CREMA-D's classes are balanced, we use accuracy as our evaluation metric. Following  [23, 24, 25] , we report the Mean Absolute Error (MAE) and the Concordance Correlation Coefficients (CCC) to assess the quality of the regression models on the MSP-IMPROV dataset.\n\nThe fine-tuned models outperform the baseline models by a considerable margin. For emotion recognition accuracy, we see a 5% improvement for the BASE model and 7% improvement for LARGE model in comparison with the baselines. On the MSP-IMPROV dataset, the fine-tuned models also shows improvements over the baselines on both Arousal and Valence regressions. Specifically, fine-tuning the BASE model achieves 3.2% and 5.1% gain in CCC for Arousal and Valence regression respectively. Although there might be discrepancies between train-validation-test set split, we find our best results (accuracy of 70.22% on CREMA-D, CCC of 0.697 and 0.692 on MSP-IMPROV Arousal and Valence regression) competitive with existing benchmarks on CREMA-D  [26, 27, 28]  and MSP-IMPROV  [29, 24] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Limited Resource Setting",
      "text": "Since the ultimate motivation of transfer learning is to reduce the requirements on labeled data, we are interested in exploring the capability of the pretrained MulT in a limited resource setting. Figure  2  shows the performance of the models when only N % of the original training set are used for training. We can see that the performance drop curves for the pretrained models are less steep in comparison with training MulT from scratch and TFN. With only 10% of the original training set (less than 500 training samples on both datasets), finetuning the pretrained models outperforms training from scratch by at least 10% for emotion recognition, and more than 20% and 10% CCC improvements for Arousal and Valence regression respectively. This further suggests the robustness of the pretrained weights in preventing overfitting with limited data. We can also note that TFN tends to perform better than training MulT from scratch with small training sets, which is expected because light models tend to be less susceptible to overfitting than complex ones with limited data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we present the potential of pretraining the Multimodal Transformer architecture  [14]  to model human communicative behaviors. We validate the usefulness of the pretrained model for the task of emotion recognition on two datasets, and demonstrate the robustness of the model in a low-resource setting. In the future, we will explore the performance of the model on other domains relating to communication such as mental health assessment.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: left), which differs from the standard Transformer",
      "page": 1
    },
    {
      "caption": "Figure 1: The architecture of the ith layer in a b →a Cross-modal Transformer (Cross-modal Attention Block) is shown on the",
      "page": 2
    },
    {
      "caption": "Figure 2: Performance of the models with restricted data.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the performance of the models when",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CREMA-D": "",
          "MSP-IMPROV": "Arousal"
        },
        {
          "CREMA-D": "Accu. ↑",
          "MSP-IMPROV": "MAE ↓"
        },
        {
          "CREMA-D": "63.09",
          "MSP-IMPROV": "0.466"
        },
        {
          "CREMA-D": "57.06",
          "MSP-IMPROV": "0.676"
        },
        {
          "CREMA-D": "58.53",
          "MSP-IMPROV": "0.496"
        },
        {
          "CREMA-D": "63.93",
          "MSP-IMPROV": "0.466"
        },
        {
          "CREMA-D": "68.87",
          "MSP-IMPROV": "0.456"
        },
        {
          "CREMA-D": "70.22",
          "MSP-IMPROV": "0.431"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Joon Son"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition",
      "arxiv": "arXiv:1806.05622"
    },
    {
      "citation_id": "3",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "4",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "Andy Liu"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "6",
      "title": "Unified vision-language pretraining for image captioning and vqa",
      "authors": [
        "Luowei Zhou"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "7",
      "title": "Actbert: Learning globallocal video-text representations",
      "authors": [
        "Linchao Zhu",
        "Yi Yang"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Lxmert: Learning crossmodality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "Lxmert: Learning crossmodality encoder representations from transformers",
      "arxiv": "arXiv:1908.07490"
    },
    {
      "citation_id": "9",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu"
      ],
      "year": "2019",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "10",
      "title": "Parameter efficient multimodal transformers for video representation learning",
      "authors": [
        "Sangho Lee"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "11",
      "title": "A short note on the kinetics-700 human action dataset",
      "authors": [
        "Joao Carreira"
      ],
      "year": "2019",
      "venue": "A short note on the kinetics-700 human action dataset",
      "arxiv": "arXiv:1907.06987"
    },
    {
      "citation_id": "12",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Hung Yao",
        "Tsai"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "16",
      "title": "Tera: Self-supervised learning of transformer encoder representation for speech",
      "authors": [
        "Andy Liu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech, Language Process"
    },
    {
      "citation_id": "17",
      "title": "Audio albert: A lite bert for selfsupervised learning of audio representation",
      "authors": [
        "Po-Han Chi"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "18",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "19",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "20",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis"
      ],
      "year": "2018",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "21",
      "title": "Towards learning a universal nonsemantic representation of speech",
      "authors": [
        "Joel Shor"
      ],
      "year": "2020",
      "venue": "Towards learning a universal nonsemantic representation of speech",
      "arxiv": "arXiv:2002.12764"
    },
    {
      "citation_id": "22",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "23",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "24",
      "title": "Enforcing semantic consistency for cross corpus valence regression from speech using adversarial discrepancy learning",
      "authors": [
        "Gao-Yi Chao"
      ],
      "year": "2019",
      "venue": "Enforcing semantic consistency for cross corpus valence regression from speech using adversarial discrepancy learning"
    },
    {
      "citation_id": "25",
      "title": "Deep multilayer perceptrons for dimensional speech emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "APSIPA ASC. IEEE"
    },
    {
      "citation_id": "26",
      "title": "Jointly predicting arousal, valence and dominance with multi-task learning",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "Jointly predicting arousal, valence and dominance with multi-task learning"
    },
    {
      "citation_id": "27",
      "title": "Metric learning-based multimodal audio-visual emotion recognition",
      "authors": [
        "Esam Ghaleb"
      ],
      "year": "2019",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Temporal aggregation of audio-visual modalities for emotion recognition",
      "authors": [
        "Andreea Birhala"
      ],
      "year": "2020",
      "venue": "Int'l Conf. Telecommunications and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Multimodal attention-mechanism for temporal emotion recognition",
      "authors": [
        "Esam Ghaleb"
      ],
      "year": "2020",
      "venue": "ICIP"
    },
    {
      "citation_id": "30",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "ICASSP"
    }
  ]
}