{
  "paper_id": "2308.14317v1",
  "title": "Symbolic & Acoustic: Multi-Domain Music Emotion Modeling For Instrumental Music",
  "published": "2023-08-28T05:47:57Z",
  "authors": [
    "Kexin Zhu",
    "Xulong Zhang",
    "Jianzong Wang",
    "Ning Cheng",
    "Jing Xiao"
  ],
  "keywords": [
    "piano emotion recognition",
    "music information retrieval",
    "multi-domain analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music Emotion Recognition involves the automatic identification of emotional elements within music tracks, and it has garnered significant attention due to its broad applicability in the field of Music Information Retrieval. It can also be used as the upstream task of many other human-related tasks such as emotional music generation and music recommendation. Due to existing psychology research, music emotion is determined by multiple factors such as the Timbre, Velocity, and Structure of the music. Incorporating multiple factors in MER helps achieve more interpretable and finer-grained methods. However, most prior works were uni-domain and showed weak consistency between arousal modeling performance and valence modeling performance. Based on this background, we designed a multi-domain emotion modeling method for instrumental music that combines symbolic analysis and acoustic analysis. At the same time, because of the rarity of music data and the difficulty of labeling, our multi-domain approach can make full use of limited data. Our approach was implemented and assessed using the publicly available piano dataset EMOPIA, resulting in a notable improvement over our baseline model with a 2.4% increase in overall accuracy, establishing its state-of-the-art performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The emotional aspect of music, commonly known as its affective content, holds significant importance and is often regarded as the essence of musical expression. The recognition of emotions in music, known as Music Emotion Recognition (MER), has emerged as a prominent topic and crucial objective within the field of Music Information Retrieval (MIR). This recognition process assumes paramount significance due to its widespread application in various scenarios involving emotion-driven music retrieval and recommendation. Restricted by the complexity of emotion, research on MER has encountered great difficulties  [9, 33] . Emotion is a very complex psychological state, and different people have different emotional thresholds  [34] . This makes emotional annotation more difficult and emotional data more scarce.\n\nThe recognition and understanding of the intricate interplay between various factors within music and their impact on music emotion constitute a central concern in ongoing research on MER. Investigating this matter not only facilitates the advancement of more efficient and nuanced MER techniques but also contributes to the development of comprehensive insights into the complex nature of music emotion. Existing research usually applies disentanglement or multi-domain analysis to modeling music emotion from multiple aspects. Berardinis et al.  [1]  applies Music Source Separation during pre-processing and analyze the emotional content in vocal, bass, drums, and other parts separately, their proposed method shows promising performance. Zhao et al.  [39]  provide a new perspective by modeling music emotion with both music content and music context, their proposed method applies multi-modal analysis on audio content, lyrics, track name, and artist name of the music.\n\nTo further explore the essence of music emotion, research was also carried out on instrumental music. In the field of psychology and affective computing, Laukka et al.  [18]  proposed a convincing music emotion perception model for instrumental music and concluded six factors that affect music emotion: Dynamics, Rhythm, Timbre, Register, Tonality, and Structure. Those factors reflect both the acoustic characteristics and the structural characteristics of the music. Laukka's model indicates the importance of incorporating both acoustic analysis and symbolic analysis for MER. Acoustic factors such as Dynamics and Timbre are highly related to the Arousal expression of the music but are not included in the symbolic representations of music. Therefore symbolic-only methods show relatively weaker performance on Arousal detection. Structural factors such as Tonality and Structure are highly related to the Valence expression. Although those factors are included in the acoustic domain, existing acoustic analysis methods can hardly learn the structural information without extra supervision. To incorporate all the important factors, both acoustic analysis and symbolic analysis are needed.\n\nHowever, most existing MER methods for instrumental music are uni-domain and fail to model music emotion from multiple aspects. Existing researches mainly apply deep-learning-based methods on the acoustic domain or uses sequencemodeling methods on the symbolic domain representations of the music. In their recent publication on emotion recognition in symbolic music, Qiu et al.  [31]  introduced a pioneering approach utilizing the MIDIBERT model  [4] , a large-scale pre-trained music understanding model. At present, no existing research on Music Emotion Recognition (MER) for instrumental music integrates both acoustic and symbolic analyses. As a result, we present an innovative method in this study that encompasses music emotion modeling from both acoustic and symbolic perspectives. Given the representative nature of piano music within the instrumental domain, we implemented and conducted an evaluation of our proposed approach using the publicly available piano emotion dataset EMOPIA  [16] .\n\nOur contribution can be summarized as follows:\n\n-Inspired by existing psychology and affective computing research, we proposed a multi-domain emotion modeling method for instrumental music, which only needs audio input. Our method used a pre-trained transcription model to obtain symbolic representation, therefore can be used on each instrument that can be automatically transcripted. -We designed a refined acoustic model with mixed acoustic features input and a transformer-based symbolic model. Both models showed promising performance.\n\n-We implemented and evaluated our proposed method on the public piano emotion dataset EMOPIA  [16] . Our method achieved state-of-the-art performance on EMOPIA with better consistency between Valence detection and Arousal detection performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "There have been many studies in the research field of MER. According to the different domains of focus, these studies include MER with acoustic-only and MER with symbolic-only studies. These works have promoted progress in MER, and there are also some points that can be improved.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer With Acoustic-Only",
      "text": "In order to explore which part of the vocal or accompaniment music carries more emotional information, Xu et al.  [38]  used the sound source separation technology, combined with the 84-dimensional manual low-level features (such as Mel frequency cepstrum coefficient (MFCC), spectral center, spectral attenuation point, spectral flux, and other similar measures.), and then used a classifier to recognize music emotion. Coutinho et al.  [6]  extracted 65 Low-level Descriptors (LLDs) in a time window of 1 second and calculated their first-order difference to obtain a total of 130 low-level features, then calculated the mean and standard deviation of each LLD in one second, and finally formed a 260-dimensional feature vector, and then used Long Short-term Memory (LSTM) network to carry out regression prediction of dynamic V/A (Valence / Arousal) value. Fukayama et al.  [8]  proposed a method to adapt to aggregation by considering new acoustic signal input based on multi-stage regression. At the same time, a method of adjusting the aggregation weight is introduced to deal with the emotion caused by the new input that cannot be known in advance, and the deviation observed in the training data is utilized by using Gaussian process regression. Li et al.  [19]  introduced a novel approach to tackle dynamic emotion regression by leveraging Deep Bi-directional Long Short-term Memory (DBiLSTM) in a multi-scale regression framework . Moreover, the author also examined the influence of dissimilar sequence lengths between the training and prediction stages on the overall performance of DBiLSTM. By investigating this aspect, the study aimed to gain insights into the effects of such variations on the efficacy of the model.  [23]  uses the CNN network that can process local information with fewer parameters and the RNN network that can process context information, that is, the CRNN structure, which uses the least parameters than Media Eval 2015.\n\nOther methods have achieved the best results in the dynamic regression prediction of emotion at that time. Huang et al.  [14]  introduced the attention mechanism into the music emotion classification task, and introduced the attention layer with short-term and short-term memory units into the deep convolution neural network for music emotion classification. Different weights are allocated on different time blocks (chunks), and the song-level emotion classification prediction is obtained through fusion. Liu et al.  [22]  regards music emotion recognition as a multi-label classification task, and uses convolutional neural networks and spectrum diagram to complete end-to-end classification. Chen et al.  [2]  considered the complementarity between CNN with different structures and between CNN and LSTM, and combined multi-channel CNN with different structures and LSTM into a unified structure (Multi-channel Convolutional LSTM, MCCLSTM) to extract advanced music descriptors. Choi et al.  [3]  employed a pre-trained convolutional neural network (CNN) feature, which was initially trained for music auto-tagging purposes. They then successfully transferred this CNN to various music-related classification and regression tasks, showcasing its adaptability and versatility. Similarly, Panda et al.  [28]  introduced a collection of innovative affective audio features to enhance emotional classification in audio music. The authors observed that conventional feature extractors primarily focus on low-level timbre-related aspects, neglecting essential elements like musical form, texture, and expressive skills. To address this limitation, the authors devised a novel set of algorithms specifically designed to capture information related to music texture and expression, effectively compensating for the significant gaps in music emotion recognition research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer With Symbolic-Only",
      "text": "Previous research employed manual extraction of statistical musical characteristics, which were subsequently inputted into machine learning classifiers to forecast the emotional aspects of notated music. Grekow et al.  [10]  conducted an analysis on classical music in MIDI format and extracted 63 distinct features. In a similar vein, Lin et al.  [20]  conducted a comparative investigation involving multiple features (audio, lyrics, and MIDI) extracted from the same music. Remarkably, they discovered that MIDI features exhibited superior performance in emotion recognition. Building upon this finding, the researchers utilized the JSymbolic library  [25]  to extract 112 advanced music features from MIDI files. Subsequently, Support Vector Machine (SVM) was employed to classify the data. Similarly, Panda et al.  [29]  employed various tools to extract features from MIDI files and utilized SVM for classification purposes.\n\nMore recent studies demonstrate a growing adoption of a symbolic music encoding technique similar to MIDI  [26] , which is gaining popularity among researchers. Additionally, deep learning models have emerged as the prominent approach in this field. Ferreira  [7]  devised a method to encode MIDI files into MIDIlike sequences, leveraging LSTM and GPT2 for sentiment classification purposes. This approach offers simplicity and efficiency. Drawing inspiration from the remarkable achievements of BERT, Chou et al.  [5]  introduced MidiBERTPiano, a large-scale pre-trained model utilizing CP representation. The proposed model showcases promising outcomes in various domains, including symbolic music emotion recognition. Highlighting the paramount importance of emotional expression in music's intrinsic structure, Liu et al.  [32]  proposed a straightforward multi-task framework for the symbolic MER task. Notably, this approach benefits from readily available labels for auxiliary tasks, eliminating the need for manual annotation of labels beyond emotion classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "The complete diagram illustrating the overall architecture of our proposed approach can be observed in Figure  1 . The structure contains two branches: the acoustic domain branch (marked in yellow) applies acoustic analysis on mixed acoustic features with a Conv-based acoustic encoder, and the symbolic domain branch (marked in blue) applies symbolic analysis on music score sequence by using a Transformer-based symbolic encoder. It is worth noting that the outputs of the two branches come from the same modality, that is, from the acoustic input, so they belong to different domains of the same modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Acoustic Domain Analysis For Arousal Modeling",
      "text": "For the acoustic domain analysis, we want to explicitly extract the information that relates to music emotion expressions, such as Timbre and Dynamics  [18] . We use a mixed feature as input, which consists of the Mel-frequency Cepstral Coefficient (MFCC), Mel-spectrogram, Spectral Centroid (SC), and Root Mean Square Energy (RMSE) of the audio input. SC and RMSE reflect the energy distribution and changes of the audio, which is strongly correlated to music emotion expression. We use mel-spectrogram instead of STFT spectrogram because it better fits the human auditory perception process. We also calculate a 20-dimensional MFCC with librosa  [24] . After these features are obtained, we resize and align them in the time dimension. The mixed feature can be obtained by splicing these features.\n\nThe processing flow of the acoustic domain branch is shown at the top of Figure  1 . We use a 2D-ConvNet module as the acoustic encoder for its great ability to encode temporal and frequency domain information simultaneously. After the feature extraction process, the extracted features are flattened and combined in the channel dimension to form the acoustic domain output. A comprehensive summary of the settings used in the experiment can be found in Table  1 .\n\nAcoustic domain analysis shows better performance on Arousal detection than symbolic domain analysis. Arousal is mainly decided by acoustic attributes such as Dynamics, Energy, and Timbre, which are not included in symbolic domain representation. Therefore we calculate an extra arousal classification loss function using Binary Cross Entropy (BCE) on the acoustic domain analysis branch during the training process.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Symbolic Domain Analysis For Valence Modeling",
      "text": "As mentioned above, our proposed method is designed to perform both acoustic and symbolic domain analysis with only audio input. That is to say, our symbolic part uses the automatic piano transcription module to form the symbolic domain representation instead of directly using the MIDI files in the EMOPIA dataset. This provides a common paradigm for other transcribable musical instruments. Therefore for the symbolic domain analysis branch, we use a pre-trained automatic transcription model to perform piano transcription. Specifically, we use the refined version of Onsets and Frames  [11, 12]  proposed by Zhao et al.  [40] , which shows better generalizability and costs fewer computation resources. The transcripted piano score is converted into MIDI format, which includes the onset, offset, duration, and velocity of each note.\n\nThe music score is the \"language\" of the music and is a semantic sequence similar to natural language. Therefore the symbolic representation of the music score is similar to that of the natural language.\n\nIn this work, we use a refined MIDI-like representation for note embedding, which is shown in Figure  2 . Unlike the original MIDI-like  [27]  representation, we add an attribute named \"harmonic\" which explicitly denotes the number of sounding notes at the onset of a note. Since harmonic is an important part of musical performance, we decide to add extra information about it. Therefore, the symbolic domain representation for a single note consists of the onset time, harmonic, velocity, time shift, and offset time of the note.\n\nThe structure of the symbolic domain analysis branch is shown at the bottom of former encoder module  [36]  to extract the emotional representation of the piano score. The Transformer encoder module consists of four original Transformer encoder layers adopted in  [36] . We pre-trained the encoder with the MIDI data from the MAESTRO dataset, for there are not enough samples in EMOPIA to train our Transformer encoder module. Symbolic domain analysis mainly focuses on the high-level semantics of the note sequences, which leads to better Valence detection accuracy than acoustic analysis. As we want to make use of its advantage, we calculate an extra valence classification loss on the symbolic domain analysis branch during the training process.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Combining Symbolic And Acoustic Analysis",
      "text": "The final purpose of our method is to perform 4-Quadrant (4Q) classification concerning both Arousal and Valence, therefore the cross-domain feature fusion method is important. When combining extracted acoustic domain features and symbolic domain features, the Cross-domain Attention (CDA) module is used for cross-domain feature fusion. CDA has a similar mechanism to multi-head crossmodal attention  [35] . In CDA module, Query and Key-Value pairs come from two different domains instead of different modalities in cross-modal attention. Each attention head can be calculated separately:\n\nLet F Q , F K , and F V denote the vectors for Query, Key, and V alue, respectively. Within the attention mechanism, these input vectors are obtained by multiplying the extracted features of the α and β domains, represented as F α and F β , with their respective learnable weight matrices W Q , W K , and W V . Here, d represents the dimension size of the Key vector. The multi-head attention can be defined as the concatenation of each individual head:\n\nThe learnable weight matrix W O and the number of attention heads H play crucial roles in this multi-head attention mechanism. By leveraging multiple attention heads, this mechanism effectively highlights the significant aspects of each domain, which cannot be achieved through simple concatenation alone.\n\nAs shown in Figure  1 , in each processing procedure, our model calculates the CDA mechanism twice. We calculate an acoustic cross-domain attention mechanism and a symbolic cross-domain attention mechanism separately. This bidirectional CDA fusion strategy brings higher fusing efficiency. The output of acoustic CDA and symbolic CDA are concatenated and input into a classifier for 4Q emotion classification. During the training process, we calculate a 4Q Label loss on this classifier using Cross Entropy (CE) loss function.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "To assess the effectiveness of our proposed model, we conducted two primary types of experiments in this study: comparative studies and ablation studies. These experiments were designed to thoroughly evaluate and analyze the performance of our model from different perspectives.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Expriments Setup",
      "text": "We use the EMOPIA  [16]  dataset, which is an open-source dataset for pianobased emotion recognition. EMOPIA contains 1087 piano clips from 387 songs, all piano clips are annotated with their MIDI files and emotion labels. As only music metadata is available, we collect all music files by their corresponding YouTube ID with the 'youtube-dl' package. Following the configuration employed in  [16] , the dataset was divided into train-validation-test splits with a ratio of 7:2:1, ensuring appropriate proportions for training, validation, and testing stages. However, due to the unavailability of several music pieces on YouTube, we're only able to use approximately 90% data of the whole dataset. Similarly, we not only perform the classification of 4 quadrants but also carry out the binary classification tasks of high/low Valence and high/low Arousal. For the pre-training phase of the Automatic Piano Transcription model, we utilized the MAESTRO dataset (\"MIDI and Audio Edited for Synchronous TRacks and Organization\")  [12] , encompassing a comprehensive collection of more than 200 hours of meticulously paired audio and MIDI recordings. During the training process, the training data is divided into mini-batches with a batch size of 64. The Adam optimizer  [17]  is employed, utilizing a learning rate of 0.0001. To implement all experiments, the PyTorch framework  [30]  is utilized.\n\nIt is important to note that MIDI files from the EMOPIA dataset were not utilized in our experiments. As our proposed model exclusively takes audio files as input, our aim is to evaluate the overall performance of the complete model, including the refined AMT module.   [15]  .615 .890\n\n.746 symbolic-LR  [16]  .581 .849 .651 MIDIBERT  [4]  .634 / / MT-MIDIBERT  [4, 31]  .676 / / proposed model .708 .874 .869",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparative Studies",
      "text": "We compared our proposed model with other existing methods on the same EMOPIA dataset. To the best of our knowledge, there is no existing multidomain piano emotion recognition research. So we compared our model with several uni-domain symbolic-domain models proposed in  [16]  and  [31] , including two models based on BLSTM and self-attention mechanism (LSTM-Attn for short) using MIDI-like and REMI symbolic representation, a linear regression model based on hand-crafted features, and two pre-trained Bert-like models. For a fair comparison, we directly used the original results announced in their works.\n\nIn  [4, 31] , valence metrics and arousal metrics are not provided, therefore are not shown in the table.\n\nTable  2  shows the comparison between our method and the other five symbolicdomain methods. All the methods show high and similar performance on Arousal detection, which indicates that Arousal detection is a relatively simple task. Due to the strong sequence-modeling ability of our transformer-based symbolic domain model, our method shows the highest Valence detection performance and outperforms the LSTM-Attn+MIDI-like model by 3.6%. On 4Q classification metrics, our model also achieves state-of-the-art performance and outperforms the LSTM-Attn+MIDI-like model by 2.4%.\n\nWe also compared our model with two existing acoustic-domain models, one uses linear regression on hand-crafted features and the other uses a ResNetlike network. Table  3  shows the comparison between our method and the other two acoustic-domain methods. All acoustic-domain methods show strong performance on Arousal detection as well. This is in line with common sense, because Arousal is greatly affected by energy, velocity, and dynamics, and this information is evident in acoustic information. Though our method is slightly weaker on Arousal detection, it still outperforms the Short-chunk ResNet model by 3.1% on the 4Q metrics.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation Studies",
      "text": "We designed and carried out a series of ablation studies to test the effect of our improvements. In the symbolic-only model and acoustic-only model, we use our symbolic branch and acoustic branch individually in order to test the effect of combining them. In the STFT-input model, we use an STFT spectrogram as input instead of the mixed acoustic feature. In the Single-loss model, we do not calculate the extra loss on the two branches and only calculate the Label loss. The experimental results of the ablation studies are shown in Table  4 . Compared to the two uni-domain models, our cross-domain fusion strategy costs performance loss on Arousal and Valence detection. However, our proposed model outperforms these two models by over 5% on the overall 4Q accuracy metrics. This indicates that our model is able to make better decisions by considering both symbolic and acoustic information.\n\nThe STFT-input model shows huge performance loss on Arousal metrics, which proves that using mixed acoustic features can improve Arousal detection performance. When using the STFT spectrogram as input, a deeper network is needed to extract the acoustic features. By using hand-crafted features, our method shows strong acoustic modeling ability with only three Conv layers. The single-loss model also shows over 2.5% performance loss on both 4Q and Arousal metrics, which indicates that our strategy of calculating the extra loss function works.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we introduce a novel multi-domain approach for piano emotion recognition, which can also be extended to other instruments with automatic transcription capabilities. Our proposed model leverages a pre-trained transcription model, enabling multi-domain analysis solely based on audio input. To the best of our knowledge, there is a lack of research specifically addressing piano emotion recognition. Our proposed model capitalizes on the complementary and redundant aspects between the acoustic and symbolic domains, leading to improved consistency in valence detection and arousal detection. Experimental results demonstrate that our proposed model surpasses the baseline approaches in terms of Valence classification and 4Q classification metrics. Moving forward, our future work will focus on designing enhanced symbolic representations for music, investigating superior cross-domain fusion strategies to enhance overall performance, and developing a universal framework for addressing the emotional aspects of transcribed musical instruments.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The structure contains two branches: the",
      "page": 5
    },
    {
      "caption": "Figure 1: The overall structure. The feature representations of the two domains are gen-",
      "page": 5
    },
    {
      "caption": "Figure 2: Unlike the original MIDI-like [27] representation,",
      "page": 6
    },
    {
      "caption": "Figure 1: After the note embeddings are obtained, we input them into a Trans-",
      "page": 6
    },
    {
      "caption": "Figure 2: The refined MIDI-like symbolic representation we used.",
      "page": 7
    },
    {
      "caption": "Figure 1: , in each processing procedure, our model calculates",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "Ping An Technology (Shenzhen) Co., Ltd."
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "Abstract. Music Emotion Recognition involves the automatic identifi-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "cation of emotional elements within music tracks, and it has garnered"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "significant attention due to its broad applicability in the field of Music"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "Information Retrieval. It can also be used as the upstream task of many"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "other human-related tasks such as emotional music generation and music"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "recommendation. Due to existing psychology research, music emotion is"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "determined by multiple factors such as the Timbre, Velocity, and Struc-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "ture of the music.\nIncorporating multiple factors in MER helps achieve"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "more interpretable and finer-grained methods. However, most prior works"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "were uni-domain and showed weak consistency between arousal model-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "ing performance and valence modeling performance. Based on this back-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "ground, we designed a multi-domain emotion modeling method for\nin-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "strumental music that combines symbolic analysis and acoustic analysis."
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "At the same time, because of the rarity of music data and the difficulty of"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "labeling, our multi-domain approach can make full use of\nlimited data."
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "Our approach was\nimplemented and assessed using the publicly avail-"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "able piano dataset EMOPIA,\nresulting in a notable improvement over"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "our baseline model with a 2.4% increase in overall accuracy, establishing"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "its state-of-the-art performance."
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "information retrieval\n·\nKeywords: piano emotion recognition · music"
        },
        {
          "Kexin Zhu‡, Xulong Zhang‡, Jianzong Wang⋆, Ning Cheng, Jing Xiao": "multi-domain analysis."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multi-domain analysis.": "1\nIntroduction"
        },
        {
          "multi-domain analysis.": "The emotional aspect of music, commonly known as its affective content, holds"
        },
        {
          "multi-domain analysis.": "significant\nimportance and is often regarded as\nthe essence of musical expres-"
        },
        {
          "multi-domain analysis.": "sion. The recognition of emotions in music, known as Music Emotion Recogni-"
        },
        {
          "multi-domain analysis.": "tion (MER), has emerged as a prominent topic and crucial objective within the"
        },
        {
          "multi-domain analysis.": "field of Music Information Retrieval\n(MIR). This\nrecognition process assumes"
        },
        {
          "multi-domain analysis.": "paramount significance due to its widespread application in various scenarios in-"
        },
        {
          "multi-domain analysis.": "volving emotion-driven music retrieval and recommendation. Restricted by the"
        },
        {
          "multi-domain analysis.": "complexity of emotion, research on MER has encountered great difficulties [9,33]."
        },
        {
          "multi-domain analysis.": "Emotion is a very complex psychological state, and different people have differ-"
        },
        {
          "multi-domain analysis.": "ent emotional\nthresholds\n[34]. This makes emotional annotation more difficult"
        },
        {
          "multi-domain analysis.": "and emotional data more scarce."
        },
        {
          "multi-domain analysis.": "⋆ Corresponding author: Jianzong Wang,\njzwang@188.com. ‡ These authors are equal"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nK. Zhu et al.": "The recognition and understanding of\nthe intricate interplay between vari-"
        },
        {
          "2\nK. Zhu et al.": "ous factors within music and their impact on music emotion constitute a central"
        },
        {
          "2\nK. Zhu et al.": "concern in ongoing research on MER.\nInvestigating this matter not only fa-"
        },
        {
          "2\nK. Zhu et al.": "cilitates\nthe advancement of more efficient and nuanced MER techniques but"
        },
        {
          "2\nK. Zhu et al.": "also contributes to the development of comprehensive insights into the complex"
        },
        {
          "2\nK. Zhu et al.": "nature of music emotion. Existing research usually applies disentanglement or"
        },
        {
          "2\nK. Zhu et al.": "multi-domain analysis\nto modeling music\nemotion from multiple aspects. Be-"
        },
        {
          "2\nK. Zhu et al.": "rardinis\net al.\n[1] applies Music Source Separation during pre-processing and"
        },
        {
          "2\nK. Zhu et al.": "analyze the emotional content in vocal, bass, drums, and other parts separately,"
        },
        {
          "2\nK. Zhu et al.": "their proposed method shows promising performance. Zhao et al.\n[39] provide a"
        },
        {
          "2\nK. Zhu et al.": "new perspective by modeling music emotion with both music content and music"
        },
        {
          "2\nK. Zhu et al.": "context, their proposed method applies multi-modal analysis on audio content,"
        },
        {
          "2\nK. Zhu et al.": "lyrics, track name, and artist name of the music."
        },
        {
          "2\nK. Zhu et al.": "To further explore the essence of music emotion,\nresearch was also carried"
        },
        {
          "2\nK. Zhu et al.": "out on instrumental music.\nIn the field of psychology and affective computing,"
        },
        {
          "2\nK. Zhu et al.": "Laukka et al.\n[18] proposed a convincing music emotion perception model for in-"
        },
        {
          "2\nK. Zhu et al.": "strumental music and concluded six factors that affect music emotion: Dynam-"
        },
        {
          "2\nK. Zhu et al.": "ics, Rhythm, Timbre, Register, Tonality, and Structure. Those\nfactors\nreflect"
        },
        {
          "2\nK. Zhu et al.": "both the acoustic characteristics and the structural characteristics of the music."
        },
        {
          "2\nK. Zhu et al.": "Laukka’s model indicates the importance of incorporating both acoustic analysis"
        },
        {
          "2\nK. Zhu et al.": "and symbolic analysis for MER. Acoustic factors such as Dynamics and Timbre"
        },
        {
          "2\nK. Zhu et al.": "are highly related to the Arousal expression of the music but are not included in"
        },
        {
          "2\nK. Zhu et al.": "the symbolic representations of music. Therefore symbolic-only methods\nshow"
        },
        {
          "2\nK. Zhu et al.": "relatively weaker performance on Arousal detection. Structural\nfactors such as"
        },
        {
          "2\nK. Zhu et al.": "Tonality and Structure are highly related to the Valence expression. Although"
        },
        {
          "2\nK. Zhu et al.": "those\nfactors are\nincluded in the acoustic domain,\nexisting acoustic analysis"
        },
        {
          "2\nK. Zhu et al.": "methods can hardly learn the structural\ninformation without extra supervision."
        },
        {
          "2\nK. Zhu et al.": "To incorporate all\nthe important\nfactors, both acoustic analysis and symbolic"
        },
        {
          "2\nK. Zhu et al.": "analysis are needed."
        },
        {
          "2\nK. Zhu et al.": "However, most existing MER methods for instrumental music are uni-domain"
        },
        {
          "2\nK. Zhu et al.": "and fail\nto model music\nemotion from multiple\naspects. Existing\nresearches"
        },
        {
          "2\nK. Zhu et al.": "mainly apply deep-learning-based methods on the acoustic domain or uses sequence-"
        },
        {
          "2\nK. Zhu et al.": "modeling methods on the symbolic domain representations of the music. In their"
        },
        {
          "2\nK. Zhu et al.": "recent publication on emotion recognition in symbolic music, Qiu et al.\n[31]\nin-"
        },
        {
          "2\nK. Zhu et al.": "troduced a pioneering approach utilizing the MIDIBERT model\n[4], a large-scale"
        },
        {
          "2\nK. Zhu et al.": "pre-trained music understanding model. At present, no existing research on Mu-"
        },
        {
          "2\nK. Zhu et al.": "sic Emotion Recognition (MER) for instrumental music integrates both acoustic"
        },
        {
          "2\nK. Zhu et al.": "and symbolic analyses. As a result, we present an innovative method in this study"
        },
        {
          "2\nK. Zhu et al.": "that encompasses music emotion modeling from both acoustic and symbolic per-"
        },
        {
          "2\nK. Zhu et al.": "spectives. Given the representative nature of piano music within the instrumental"
        },
        {
          "2\nK. Zhu et al.": "domain, we implemented and conducted an evaluation of our proposed approach"
        },
        {
          "2\nK. Zhu et al.": "using the publicly available piano emotion dataset EMOPIA [16]."
        },
        {
          "2\nK. Zhu et al.": "Our contribution can be summarized as follows:"
        },
        {
          "2\nK. Zhu et al.": "– Inspired by existing psychology and affective computing research, we pro-"
        },
        {
          "2\nK. Zhu et al.": "posed a multi-domain emotion modeling method for\ninstrumental music,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "which only needs audio input. Our method used a pre-trained transcrip-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "tion model to obtain symbolic representation, therefore can be used on each"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "instrument that can be automatically transcripted."
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "– We designed a refined acoustic model with mixed acoustic\nfeatures\ninput"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "and a transformer-based symbolic model. Both models\nshowed promising"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "performance."
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "– We implemented and evaluated our proposed method on the public piano"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "emotion dataset EMOPIA [16]. Our method achieved state-of-the-art per-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "formance on EMOPIA with better consistency between Valence detection"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n3": "and Arousal detection performance."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and Arousal detection performance.": "2\nRelated Works"
        },
        {
          "and Arousal detection performance.": "There have been many studies in the research field of MER. According to the"
        },
        {
          "and Arousal detection performance.": "different domains of\nfocus,\nthese studies\ninclude MER with acoustic-only and"
        },
        {
          "and Arousal detection performance.": "MER with symbolic-only studies. These works have promoted progress in MER,"
        },
        {
          "and Arousal detection performance.": "and there are also some points that can be improved."
        },
        {
          "and Arousal detection performance.": "2.1\nMER with Acoustic-only"
        },
        {
          "and Arousal detection performance.": "In order to explore which part of the vocal or accompaniment music carries more"
        },
        {
          "and Arousal detection performance.": "emotional\ninformation, Xu et al.\n[38] used the sound source separation technol-"
        },
        {
          "and Arousal detection performance.": "ogy, combined with the 84-dimensional manual\nlow-level\nfeatures (such as Mel"
        },
        {
          "and Arousal detection performance.": "frequency cepstrum coefficient\n(MFCC),\nspectral\ncenter,\nspectral attenuation"
        },
        {
          "and Arousal detection performance.": "point, spectral flux, and other similar measures.), and then used a classifier to"
        },
        {
          "and Arousal detection performance.": "recognize music emotion. Coutinho et al.\n[6] extracted 65 Low-level Descriptors"
        },
        {
          "and Arousal detection performance.": "(LLDs) in a time window of 1 second and calculated their first-order difference to"
        },
        {
          "and Arousal detection performance.": "obtain a total of 130 low-level\nfeatures, then calculated the mean and standard"
        },
        {
          "and Arousal detection performance.": "deviation of each LLD in one second, and finally formed a 260-dimensional\nfea-"
        },
        {
          "and Arousal detection performance.": "ture vector, and then used Long Short-term Memory (LSTM) network to carry"
        },
        {
          "and Arousal detection performance.": "out regression prediction of dynamic V/A (Valence / Arousal) value. Fukayama"
        },
        {
          "and Arousal detection performance.": "et al.\n[8] proposed a method to adapt to aggregation by considering new acous-"
        },
        {
          "and Arousal detection performance.": "tic signal\ninput based on multi-stage regression. At the same time, a method of"
        },
        {
          "and Arousal detection performance.": "adjusting the aggregation weight is introduced to deal with the emotion caused"
        },
        {
          "and Arousal detection performance.": "by the new input that cannot be known in advance, and the deviation observed"
        },
        {
          "and Arousal detection performance.": "in the training data is utilized by using Gaussian process regression. Li et al. [19]"
        },
        {
          "and Arousal detection performance.": "introduced a novel approach to tackle dynamic emotion regression by leverag-"
        },
        {
          "and Arousal detection performance.": "ing Deep Bi-directional Long Short-term Memory (DBiLSTM) in a multi-scale"
        },
        {
          "and Arousal detection performance.": "regression framework . Moreover, the author also examined the influence of dis-"
        },
        {
          "and Arousal detection performance.": "similar sequence lengths between the training and prediction stages on the over-"
        },
        {
          "and Arousal detection performance.": "all performance of DBiLSTM. By investigating this aspect, the study aimed to"
        },
        {
          "and Arousal detection performance.": "gain insights into the effects of such variations on the efficacy of the model.\n[23]"
        },
        {
          "and Arousal detection performance.": "uses the CNN network that can process local\ninformation with fewer parameters"
        },
        {
          "and Arousal detection performance.": "and the RNN network that can process context information, that is, the CRNN"
        },
        {
          "and Arousal detection performance.": "structure, which uses the least parameters than Media Eval 2015."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nK. Zhu et al.": "Other methods have achieved the best results in the dynamic regression pre-"
        },
        {
          "4\nK. Zhu et al.": "diction of emotion at that time. Huang et al. [14] introduced the attention mech-"
        },
        {
          "4\nK. Zhu et al.": "anism into the music emotion classification task, and introduced the attention"
        },
        {
          "4\nK. Zhu et al.": "layer with short-term and short-term memory units\ninto the deep convolution"
        },
        {
          "4\nK. Zhu et al.": "neural network for music emotion classification. Different weights are allocated"
        },
        {
          "4\nK. Zhu et al.": "on different time blocks (chunks), and the song-level emotion classification pre-"
        },
        {
          "4\nK. Zhu et al.": "diction is obtained through fusion. Liu et al.\n[22] regards music emotion recogni-"
        },
        {
          "4\nK. Zhu et al.": "tion as a multi-label classification task, and uses convolutional neural networks"
        },
        {
          "4\nK. Zhu et al.": "and spectrum diagram to complete end-to-end classification. Chen et al.\n[2] con-"
        },
        {
          "4\nK. Zhu et al.": "sidered the\ncomplementarity between CNN with different\nstructures and be-"
        },
        {
          "4\nK. Zhu et al.": "tween CNN and LSTM, and combined multi-channel CNN with different struc-"
        },
        {
          "4\nK. Zhu et al.": "tures and LSTM into a unified structure (Multi-channel Convolutional LSTM,"
        },
        {
          "4\nK. Zhu et al.": "MCCLSTM)\nto extract advanced music descriptors. Choi\net al.\n[3]\nemployed"
        },
        {
          "4\nK. Zhu et al.": "a pre-trained convolutional neural network (CNN)\nfeature, which was\ninitially"
        },
        {
          "4\nK. Zhu et al.": "trained for music auto-tagging purposes. They then successfully transferred this"
        },
        {
          "4\nK. Zhu et al.": "CNN to various music-related classification and regression tasks, showcasing its"
        },
        {
          "4\nK. Zhu et al.": "adaptability and versatility. Similarly, Panda et al.\n[28]\nintroduced a collection"
        },
        {
          "4\nK. Zhu et al.": "of\ninnovative affective audio features to enhance emotional classification in au-"
        },
        {
          "4\nK. Zhu et al.": "dio music. The authors observed that conventional\nfeature extractors primarily"
        },
        {
          "4\nK. Zhu et al.": "focus on low-level timbre-related aspects, neglecting essential elements like mu-"
        },
        {
          "4\nK. Zhu et al.": "sical\nform, texture, and expressive skills. To address this limitation, the authors"
        },
        {
          "4\nK. Zhu et al.": "devised a novel set of algorithms specifically designed to capture information re-"
        },
        {
          "4\nK. Zhu et al.": "lated to music texture and expression, effectively compensating for the significant"
        },
        {
          "4\nK. Zhu et al.": "gaps in music emotion recognition research."
        },
        {
          "4\nK. Zhu et al.": "2.2\nMER with Symbolic-only"
        },
        {
          "4\nK. Zhu et al.": "Previous research employed manual extraction of statistical musical characteris-"
        },
        {
          "4\nK. Zhu et al.": "tics, which were subsequently inputted into machine learning classifiers to fore-"
        },
        {
          "4\nK. Zhu et al.": "cast\nthe emotional aspects of notated music. Grekow et al.\n[10] conducted an"
        },
        {
          "4\nK. Zhu et al.": "analysis on classical music in MIDI\nformat and extracted 63 distinct\nfeatures."
        },
        {
          "4\nK. Zhu et al.": "In a similar vein, Lin et al.\n[20] conducted a comparative investigation involv-"
        },
        {
          "4\nK. Zhu et al.": "ing multiple features (audio,\nlyrics, and MIDI) extracted from the same music."
        },
        {
          "4\nK. Zhu et al.": "Remarkably, they discovered that MIDI features exhibited superior performance"
        },
        {
          "4\nK. Zhu et al.": "in emotion recognition. Building upon this finding, the researchers utilized the"
        },
        {
          "4\nK. Zhu et al.": "JSymbolic library [25] to extract 112 advanced music features from MIDI files."
        },
        {
          "4\nK. Zhu et al.": "Subsequently, Support Vector Machine (SVM) was employed to classify the data."
        },
        {
          "4\nK. Zhu et al.": "Similarly, Panda et al. [29] employed various tools to extract features from MIDI"
        },
        {
          "4\nK. Zhu et al.": "files and utilized SVM for classification purposes."
        },
        {
          "4\nK. Zhu et al.": "More\nrecent\nstudies demonstrate a growing adoption of a symbolic music"
        },
        {
          "4\nK. Zhu et al.": "encoding technique similar to MIDI [26], which is gaining popularity among re-"
        },
        {
          "4\nK. Zhu et al.": "searchers. Additionally, deep learning models have emerged as the prominent ap-"
        },
        {
          "4\nK. Zhu et al.": "proach in this field. Ferreira [7] devised a method to encode MIDI files into MIDI-"
        },
        {
          "4\nK. Zhu et al.": "like sequences, leveraging LSTM and GPT2 for sentiment classification purposes."
        },
        {
          "4\nK. Zhu et al.": "This approach offers simplicity and efficiency. Drawing inspiration from the re-"
        },
        {
          "4\nK. Zhu et al.": "markable achievements of BERT, Chou et al.\n[5]\nintroduced MidiBERTPiano, a"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "large-scale pre-trained model utilizing CP representation. The proposed model"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "showcases promising outcomes\nin various domains,\nincluding symbolic music"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "emotion recognition. Highlighting the paramount\nimportance of emotional ex-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "pression in music’s intrinsic structure, Liu et al.\n[32] proposed a straightforward"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "multi-task framework for the symbolic MER task. Notably, this approach ben-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "efits\nfrom readily available labels\nfor auxiliary tasks, eliminating the need for"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n5": "manual annotation of\nlabels beyond emotion classification."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Acoustic domain analysis shows better performance on Arousal detection",
      "data": [
        {
          "6\nK. Zhu et al.": "Coefficient (MFCC), Mel-spectrogram, Spectral Centroid (SC), and Root Mean"
        },
        {
          "6\nK. Zhu et al.": "Square Energy (RMSE) of\nthe audio input. SC and RMSE reflect\nthe energy"
        },
        {
          "6\nK. Zhu et al.": "distribution and changes of\nthe audio, which is\nstrongly correlated to music"
        },
        {
          "6\nK. Zhu et al.": "emotion expression. We use mel-spectrogram instead of STFT spectrogram be-"
        },
        {
          "6\nK. Zhu et al.": "cause it better fits the human auditory perception process. We also calculate a"
        },
        {
          "6\nK. Zhu et al.": "20-dimensional MFCC with librosa [24]. After\nthese features are obtained, we"
        },
        {
          "6\nK. Zhu et al.": "resize and align them in the time dimension. The mixed feature can be obtained"
        },
        {
          "6\nK. Zhu et al.": "by splicing these features."
        },
        {
          "6\nK. Zhu et al.": "The processing flow of the acoustic domain branch is shown at the top of Fig-"
        },
        {
          "6\nK. Zhu et al.": "ure 1. We use a 2D-ConvNet module as the acoustic encoder for its great ability"
        },
        {
          "6\nK. Zhu et al.": "to encode temporal and frequency domain information simultaneously. After the"
        },
        {
          "6\nK. Zhu et al.": "feature extraction process, the extracted features are flattened and combined in"
        },
        {
          "6\nK. Zhu et al.": "the channel dimension to form the acoustic domain output. A comprehensive"
        },
        {
          "6\nK. Zhu et al.": "summary of the settings used in the experiment can be found in Table 1."
        },
        {
          "6\nK. Zhu et al.": "Acoustic domain analysis\nshows better performance on Arousal detection"
        },
        {
          "6\nK. Zhu et al.": "than symbolic domain analysis. Arousal\nis mainly decided by acoustic attributes"
        },
        {
          "6\nK. Zhu et al.": "such as Dynamics, Energy, and Timbre, which are not\nincluded in symbolic"
        },
        {
          "6\nK. Zhu et al.": "domain representation. Therefore we\ncalculate an extra arousal\nclassification"
        },
        {
          "6\nK. Zhu et al.": "loss function using Binary Cross Entropy (BCE) on the acoustic domain analysis"
        },
        {
          "6\nK. Zhu et al.": "branch during the training process."
        },
        {
          "6\nK. Zhu et al.": "3.2\nSymbolic Domain Analysis for Valence Modeling"
        },
        {
          "6\nK. Zhu et al.": "As mentioned above, our proposed method is designed to perform both acoustic"
        },
        {
          "6\nK. Zhu et al.": "and symbolic domain analysis with only audio input. That is to say, our symbolic"
        },
        {
          "6\nK. Zhu et al.": "part uses the automatic piano transcription module to form the symbolic domain"
        },
        {
          "6\nK. Zhu et al.": "representation instead of directly using the MIDI files in the EMOPIA dataset."
        },
        {
          "6\nK. Zhu et al.": "This provides a common paradigm for other transcribable musical\ninstruments."
        },
        {
          "6\nK. Zhu et al.": "Therefore for the symbolic domain analysis branch, we use a pre-trained auto-"
        },
        {
          "6\nK. Zhu et al.": "matic transcription model\nto perform piano transcription. Specifically, we use"
        },
        {
          "6\nK. Zhu et al.": "the refined version of Onsets and Frames [11, 12] proposed by Zhao et al.\n[40],"
        },
        {
          "6\nK. Zhu et al.": "which shows better generalizability and costs fewer computation resources. The"
        },
        {
          "6\nK. Zhu et al.": "transcripted piano score is converted into MIDI format, which includes the onset,"
        },
        {
          "6\nK. Zhu et al.": "offset, duration, and velocity of each note."
        },
        {
          "6\nK. Zhu et al.": "The music score is the \"language\" of the music and is a semantic sequence"
        },
        {
          "6\nK. Zhu et al.": "similar to natural\nlanguage. Therefore the symbolic representation of the music"
        },
        {
          "6\nK. Zhu et al.": "score is similar to that of the natural\nlanguage."
        },
        {
          "6\nK. Zhu et al.": "In this work, we use a refined MIDI-like representation for note embedding,"
        },
        {
          "6\nK. Zhu et al.": "which is\nshown in Figure 2. Unlike the original MIDI-like [27]\nrepresentation,"
        },
        {
          "6\nK. Zhu et al.": "we add an attribute named \"harmonic\" which explicitly denotes the number of"
        },
        {
          "6\nK. Zhu et al.": "sounding notes at the onset of a note. Since harmonic is an important part of"
        },
        {
          "6\nK. Zhu et al.": "musical performance, we decide to add extra information about\nit. Therefore,"
        },
        {
          "6\nK. Zhu et al.": "the symbolic domain representation for a single note consists of the onset time,"
        },
        {
          "6\nK. Zhu et al.": "harmonic, velocity, time shift, and offset time of the note."
        },
        {
          "6\nK. Zhu et al.": "The structure of the symbolic domain analysis branch is shown at the bottom"
        },
        {
          "6\nK. Zhu et al.": "of Figure 1. After the note embeddings are obtained, we input them into a Trans-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "former encoder module [36] to extract the emotional representation of the piano"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "score. The Transformer\nencoder module\nconsists of\nfour original Transformer"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "encoder layers adopted in [36]. We pre-trained the encoder with the MIDI data"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "from the MAESTRO dataset,\nfor there are not enough samples in EMOPIA to"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "train our Transformer encoder module."
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "Symbolic domain analysis mainly focuses on the high-level semantics of the"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "note sequences, which leads to better Valence detection accuracy than acoustic"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "analysis. As we want to make use of its advantage, we calculate an extra valence"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "classification loss on the symbolic domain analysis branch during the training"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "process."
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "3.3\nCombining Symbolic and Acoustic Analysis"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "The final purpose of our method is\nto perform 4-Quadrant\n(4Q) classification"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "concerning both Arousal and Valence, therefore the cross-domain feature fusion"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "method is important. When combining extracted acoustic domain features and"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "symbolic domain features, the Cross-domain Attention (CDA) module is used for"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "cross-domain feature fusion. CDA has a similar mechanism to multi-head cross-"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "modal attention [35].\nIn CDA module, Query and Key-Value pairs come from"
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "two different domains\ninstead of different modalities\nin cross-modal attention."
        },
        {
          "Fig. 2. The refined MIDI-like symbolic representation we used.": "Each attention head can be calculated separately:"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Acoustic Encoder Settings.",
      "data": [
        {
          "8\nK. Zhu et al.": "attention heads, this mechanism effectively highlights the significant aspects of"
        },
        {
          "8\nK. Zhu et al.": "each domain, which cannot be achieved through simple concatenation alone."
        },
        {
          "8\nK. Zhu et al.": "As\nshown in Figure 1,\nin each processing procedure, our model calculates"
        },
        {
          "8\nK. Zhu et al.": "the CDA mechanism twice. We\ncalculate an acoustic\ncross-domain attention"
        },
        {
          "8\nK. Zhu et al.": "mechanism and a symbolic cross-domain attention mechanism separately. This"
        },
        {
          "8\nK. Zhu et al.": "bidirectional CDA fusion strategy brings higher fusing efficiency. The output of"
        },
        {
          "8\nK. Zhu et al.": "acoustic CDA and symbolic CDA are concatenated and input into a classifier for"
        },
        {
          "8\nK. Zhu et al.": "4Q emotion classification. During the training process, we calculate a 4Q Label"
        },
        {
          "8\nK. Zhu et al.": "loss on this classifier using Cross Entropy (CE) loss function."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Comparison with symbolic-domain methods on EMOPIA.",
      "data": [
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "Method"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "LSTM-Attn [21]+MIDI-like [27]"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "LSTM-Attn [21]+REMI [15]"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "symbolic-LR [16]"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "MIDIBERT [4]"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "MT-MIDIBERT [4, 31]"
        },
        {
          "Table 2. Comparison with symbolic-domain methods on EMOPIA.": "proposed model"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: shows the comparison between our method and the other",
      "data": [
        {
          "10\nK. Zhu et al.": "We also compared our model with two existing acoustic-domain models, one"
        },
        {
          "10\nK. Zhu et al.": "uses\nlinear\nregression on hand-crafted features and the other uses a ResNet-"
        },
        {
          "10\nK. Zhu et al.": "like network. Table 3 shows the comparison between our method and the other"
        },
        {
          "10\nK. Zhu et al.": "two acoustic-domain methods. All acoustic-domain methods show strong perfor-"
        },
        {
          "10\nK. Zhu et al.": "mance on Arousal detection as well. This is in line with common sense, because"
        },
        {
          "10\nK. Zhu et al.": "Arousal\nis greatly affected by energy, velocity, and dynamics, and this informa-"
        },
        {
          "10\nK. Zhu et al.": "tion is evident in acoustic information. Though our method is slightly weaker on"
        },
        {
          "10\nK. Zhu et al.": "Arousal detection,\nit still outperforms the Short-chunk ResNet model by 3.1%"
        },
        {
          "10\nK. Zhu et al.": "on the 4Q metrics."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: Ablation studies trained and evaluated on the EMOPIA dataset.",
      "data": [
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ""
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": "4Q"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ".651"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ".630"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ".689"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ".683"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music": ".708"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nK. Zhu et al.": "3. Choi, K., Fazekas, G., Sandler, M., Cho, K.: Transfer\nlearning for music classifi-"
        },
        {
          "12\nK. Zhu et al.": "cation and regression tasks.\nIn: 18th International Society for Music Information"
        },
        {
          "12\nK. Zhu et al.": "Retrieval Conference. pp. 141–149.\nInternational Society for Music\nInformation"
        },
        {
          "12\nK. Zhu et al.": "Retrieval (2017)"
        },
        {
          "12\nK. Zhu et al.": "4. Chou, Y.H., Chen, I., Chang, C.J., Ching, J., Yang, Y.H., et al.: Midibert-piano:"
        },
        {
          "12\nK. Zhu et al.": "large-scale pre-training for symbolic music understanding. arXiv:2107.05223 (2021)"
        },
        {
          "12\nK. Zhu et al.": "5. Chou, Y.H., Chen, I., Chang, C.J., Ching, J., Yang, Y.H., et al.: Midibert-piano:"
        },
        {
          "12\nK. Zhu et al.": "Large-scale\npre-training\nfor\nsymbolic music\nunderstanding.\narXiv:2107.05223"
        },
        {
          "12\nK. Zhu et al.": "(2021)"
        },
        {
          "12\nK. Zhu et al.": "6. Coutinho, E., Trigeorgis, G., Zafeiriou, S., Schuller, B.: Automatically estimating"
        },
        {
          "12\nK. Zhu et al.": "emotion in music with deep long-short term memory recurrent neural networks. In:"
        },
        {
          "12\nK. Zhu et al.": "Working Notes Proceedings of the MediaEval 2015 Workshop. vol. 1436, pp. 1–3"
        },
        {
          "12\nK. Zhu et al.": "(2015)"
        },
        {
          "12\nK. Zhu et al.": "7. Ferreira, L., Whitehead, J.: Learning to generate music with sentiment.\nIn: Pro-"
        },
        {
          "12\nK. Zhu et al.": "ceedings of the 20th International Society for Music Information Retrieval Confer-"
        },
        {
          "12\nK. Zhu et al.": "ence. pp. 384–390 (2019)"
        },
        {
          "12\nK. Zhu et al.": "8. Fukayama, S., Goto, M.: Music emotion recognition with adaptive aggregation of"
        },
        {
          "12\nK. Zhu et al.": "gaussian process regressors.\nIn: 2016 IEEE international conference on acoustics,"
        },
        {
          "12\nK. Zhu et al.": "speech and signal processing. pp. 71–75. IEEE (2016)"
        },
        {
          "12\nK. Zhu et al.": "9. Gómez-Cañón, J.S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.H., Gómez,"
        },
        {
          "12\nK. Zhu et al.": "E.: Music emotion recognition: Toward new, robust standards in personalized and"
        },
        {
          "12\nK. Zhu et al.": "context-sensitive applications.\nIEEE Signal Processing Magazine 38(6), 106–114"
        },
        {
          "12\nK. Zhu et al.": "(2021)"
        },
        {
          "12\nK. Zhu et al.": "10. Grekow, J., Raś, Z.W.: Detecting emotions in classical music from midi files.\nIn:"
        },
        {
          "12\nK. Zhu et al.": "Foundations of\nIntelligent Systems: 18th International Symposium. pp. 261–270."
        },
        {
          "12\nK. Zhu et al.": "Springer (2009)"
        },
        {
          "12\nK. Zhu et al.": "11. Hawthorne, C., Elsen, E., Song, J., Roberts, A., Simon, I., Raffel, C., Engel, J.H.,"
        },
        {
          "12\nK. Zhu et al.": "Oore, S., Eck, D.: Onsets and frames: Dual-objective piano transcription. In: Pro-"
        },
        {
          "12\nK. Zhu et al.": "ceedings of the 19th International Society for Music Information Retrieval Confer-"
        },
        {
          "12\nK. Zhu et al.": "ence. pp. 50–57 (2018)"
        },
        {
          "12\nK. Zhu et al.": "12. Hawthorne, C., Stasyuk, A., Roberts, A., Simon,\nI., Huang, C.A., Dieleman, S.,"
        },
        {
          "12\nK. Zhu et al.": "Elsen, E., Engel, J.H., Eck, D.: Enabling factorized piano music modeling and gen-"
        },
        {
          "12\nK. Zhu et al.": "eration with the MAESTRO dataset. In: 7th International Conference on Learning"
        },
        {
          "12\nK. Zhu et al.": "Representations (2019)"
        },
        {
          "12\nK. Zhu et al.": "13. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:"
        },
        {
          "12\nK. Zhu et al.": "Proceedings of the IEEE conference on computer vision and pattern recognition."
        },
        {
          "12\nK. Zhu et al.": "pp. 770–778 (2016)"
        },
        {
          "12\nK. Zhu et al.": "14. Huang, Y.S., Chou, S.Y., Yang, Y.H.: Music\nthumbnailing via neural attention"
        },
        {
          "12\nK. Zhu et al.": "modeling of music emotion. In: 2017 Asia-Pacific Signal and Information Processing"
        },
        {
          "12\nK. Zhu et al.": "Association Annual Summit and Conference. pp. 347–350. IEEE (2017)"
        },
        {
          "12\nK. Zhu et al.": "15. Huang, Y.S., Yang, Y.H.: Pop music transformer: Beat-based modeling and gen-"
        },
        {
          "12\nK. Zhu et al.": "eration of expressive pop piano compositions.\nIn: Proceedings of\nthe 28th ACM"
        },
        {
          "12\nK. Zhu et al.": "International Conference on Multimedia. pp. 1180–1188 (2020)"
        },
        {
          "12\nK. Zhu et al.": "16. Hung, H., Ching, J., Doh, S., Kim, N., Nam, J., Yang, Y.: EMOPIA: A multi-modal"
        },
        {
          "12\nK. Zhu et al.": "pop piano dataset\nfor emotion recognition and emotion-based music generation."
        },
        {
          "12\nK. Zhu et al.": "In: Proceedings of the 22nd International Society for Music Information Retrieval"
        },
        {
          "12\nK. Zhu et al.": "Conference. pp. 318–325 (2021)"
        },
        {
          "12\nK. Zhu et al.": "17. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: 3rd Inter-"
        },
        {
          "12\nK. Zhu et al.": "national Conference on Learning Representations (2015)"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "18. Laukka, P., Eerola, T., Thingujam, N.S., Yamasaki, T., Beller, G.: Universal and"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "culture-specific factors in the recognition and performance of musical affect expres-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "434 (2013)\nsions. Emotion 13(3),"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "19. Li, X., Tian, J., Xu, M., Ning, Y., Cai, L.: Dblstm-based multi-scale fusion for"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "dynamic emotion prediction in music. In: 2016 IEEE International Conference on"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "Multimedia and Expo. pp. 1–6. IEEE (2016)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "20. Lin, Y., Chen, X., Yang, D.: Exploration of music emotion recognition based on"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "midi. In: Proceedings of the 14th International Society for Music Information Re-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "trieval Conference. pp. 221–226 (2013)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "21. Lin, Z., Feng, M., dos Santos, C.N., Yu, M., Xiang, B., Zhou, B., Bengio, Y.: A"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "structured self-attentive sentence embedding. In: 5th International Conference on"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "Learning Representations (2017)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "22. Liu, X., Chen, Q., Wu, X., Liu, Y., Liu, Y.: Cnn based music emotion classification."
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "arXiv:1704.05665 (2017)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "23. Malik, M., Adavanne,\nS., Drossos, K., Virtanen, T., Ticha, D.,\nJarina, R.:"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "Stacked convolutional and recurrent neural networks for music emotion recognition."
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "arXiv:1706.02292 (2017)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "24. McFee, B., Raffel, C., Liang, D., Ellis, D.P., McVicar, M., Battenberg, E., Nieto,"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "O.:\nlibrosa: Audio and music signal analysis in python. In: Proceedings of the 14th"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "python in science conference. vol. 8, pp. 18–25. Citeseer (2015)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "25. McKay, C., Fujinaga, I.:\njsymbolic: A feature extractor for midi files. In: Proceed-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "ings of the 2006 International Computer Music Conference (2006)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "26. Oore, S., Simon,\nI., Dieleman, S., Eck, D., Simonyan, K.: This time with feeling:"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "Learning expressive musical performance. Neural Computing and Applications 32,"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "955–967 (2020)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "27. Oore, S., Simon,\nI., Dieleman, S., Eck, D., Simonyan, K.: This\ntime with feel-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "ing: Learning expressive musical performance. Neural Computing and Applications"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "32(4), 955–967 (2020)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "28. Panda, R., Malheiro, R., Paiva, R.P.: Musical texture and expressivity features for"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "music emotion recognition.\nIn: 19th International Society for Music Information"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "Retrieval Conference. pp. 383–391 (2018)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "29. Panda, R., Malheiro, R., Rocha, B., Oliveira, A., Paiva, R.P.: Multi-modal music"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "emotion recognition: A new dataset, methodology and comparative analysis.\nIn:"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "International symposium on computer music multidisciplinary research (2013)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "30. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "performance deep learning library. Advances in neural\ninformation processing sys-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "tems 32 (2019)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "31. Qiu, J., Chen, C., Zhang, T.: A novel multi-task learning method for\nsymbolic"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "music emotion recognition. arXiv:2201.05782 (2022)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "32. Qiu, J., Chen, C., Zhang, T.: A novel multi-task learning method for\nsymbolic"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "music emotion recognition. arXiv:2201.05782 (2022)"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "33. Ru, G., Zhang, X., Wang, J., Cheng, N., Xiao, J.: Improving music genre classifi-"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "cation from multi-modal properties of music and genre correlations perspective."
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "In:\nICASSP 2023\n-\n2023\nIEEE International Conference\non Acoustics, Speech"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "and\nSignal Processing\n(ICASSP).\npp.\n1–5\n(2023).\nhttps://doi.org/10.1109/"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "ICASSP49357.2023.10097241"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "34. Tang, H., Zhang, X., Wang, J., Cheng, N., Xiao, J.: Emomix: Emotion mixing via"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "diffusion models for emotional speech synthesis. In: 24th Annual Conference of the"
        },
        {
          "Multi-domain Music Emotion Modeling for Instrumental Music\n13": "International Speech Communication Association (2023)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "35. Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov, R.:",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "Multimodal\ntransformer\nfor unaligned multimodal\nlanguage"
        },
        {
          "14": "",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "vol. 2019, p. 6558. NIH Public Access (2019)"
        },
        {
          "14": "36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "Ł., Polosukhin, I.: Attention is all you need. Advances in neural"
        },
        {
          "14": "",
          "K. Zhu et al.": "cessing systems 30 (2017)"
        },
        {
          "14": "37. Won, M., Ferraro, A., Bogdanov, D., Serra, X.: Evaluation of cnn-based automatic",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "music tagging models. arXiv:2006.00751 (2020)"
        },
        {
          "14": "38. Xu, J., Li, X., Hao, Y., Yang, G.: Source separation improves music emotion recog-",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "nition.\nIn: Proceedings of\ninternational"
        },
        {
          "14": "",
          "K. Zhu et al.": "423–426 (2014)"
        },
        {
          "14": "39. Zhao, J., Ru, G., Yu, Y., Wu, Y., Li, D., Li, W.: Multimodal music emotion recogni-",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "Conference on Multimedia and Expo. pp. 1–6. IEEE (2022)"
        },
        {
          "14": "40. Zhao, J., Wu, Y., Wen, L., Ma, L., Ruan, L., Wang, W., Li, W.: Improving auto-",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": ""
        },
        {
          "14": "",
          "K. Zhu et al.": "(2023)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The multiple voices of musical emotions: Source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "J De Berardinis",
        "A Cangelosi",
        "E Coutinho"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21st international society for music information retrieval conference"
    },
    {
      "citation_id": "2",
      "title": "High-level music descriptor extraction algorithm based on combination of multi-channel cnns and lstm",
      "authors": [
        "N Chen",
        "S Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "3",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler",
        "K Cho"
      ],
      "year": "2017",
      "venue": "18th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "4",
      "title": "Midibert-piano: large-scale pre-training for symbolic music understanding",
      "authors": [
        "Y Chou",
        "I Chen",
        "C Chang",
        "J Ching",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Midibert-piano: large-scale pre-training for symbolic music understanding",
      "arxiv": "arXiv:2107.05223"
    },
    {
      "citation_id": "5",
      "title": "Midibert-piano: Large-scale pre-training for symbolic music understanding",
      "authors": [
        "Y Chou",
        "I Chen",
        "C Chang",
        "J Ching",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Midibert-piano: Large-scale pre-training for symbolic music understanding",
      "arxiv": "arXiv:2107.05223"
    },
    {
      "citation_id": "6",
      "title": "Automatically estimating emotion in music with deep long-short term memory recurrent neural networks",
      "authors": [
        "E Coutinho",
        "G Trigeorgis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Working Notes Proceedings of the MediaEval 2015 Workshop"
    },
    {
      "citation_id": "7",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "L Ferreira",
        "J Whitehead"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "8",
      "title": "Music emotion recognition with adaptive aggregation of gaussian process regressors",
      "authors": [
        "S Fukayama",
        "M Goto"
      ],
      "year": "2016",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "9",
      "title": "Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "T Eerola",
        "P Herrera",
        "X Hu",
        "Y Yang",
        "E Gómez"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "10",
      "title": "Detecting emotions in classical music from midi files",
      "authors": [
        "J Grekow",
        "Z Raś"
      ],
      "year": "2009",
      "venue": "Foundations of Intelligent Systems: 18th International Symposium"
    },
    {
      "citation_id": "11",
      "title": "Onsets and frames: Dual-objective piano transcription",
      "authors": [
        "C Hawthorne",
        "E Elsen",
        "J Song",
        "A Roberts",
        "I Simon",
        "C Raffel",
        "J Engel",
        "S Oore",
        "D Eck"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "12",
      "title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset",
      "authors": [
        "C Hawthorne",
        "A Stasyuk",
        "A Roberts",
        "I Simon",
        "C Huang",
        "S Dieleman",
        "E Elsen",
        "J Engel",
        "D Eck"
      ],
      "year": "2019",
      "venue": "th International Conference on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Music thumbnailing via neural attention modeling of music emotion",
      "authors": [
        "Y Huang",
        "S Chou",
        "Y Yang"
      ],
      "year": "2017",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "15",
      "title": "Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions",
      "authors": [
        "Y Huang",
        "Y Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "H Hung",
        "J Ching",
        "S Doh",
        "N Kim",
        "J Nam",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 22nd International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "17",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Universal and culture-specific factors in the recognition and performance of musical affect expressions",
      "authors": [
        "P Laukka",
        "T Eerola",
        "N Thingujam",
        "T Yamasaki",
        "G Beller"
      ],
      "year": "2013",
      "venue": "Emotion"
    },
    {
      "citation_id": "19",
      "title": "Dblstm-based multi-scale fusion for dynamic emotion prediction in music",
      "authors": [
        "X Li",
        "J Tian",
        "M Xu",
        "Y Ning",
        "L Cai"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "20",
      "title": "Exploration of music emotion recognition based on midi",
      "authors": [
        "Y Lin",
        "X Chen",
        "D Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "21",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Dos Santos",
        "M Yu",
        "B Xiang",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Cnn based music emotion classification",
      "authors": [
        "X Liu",
        "Q Chen",
        "X Wu",
        "Y Liu",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Cnn based music emotion classification",
      "arxiv": "arXiv:1704.05665"
    },
    {
      "citation_id": "23",
      "title": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "authors": [
        "M Malik",
        "S Adavanne",
        "K Drossos",
        "T Virtanen",
        "D Ticha",
        "R Jarina"
      ],
      "year": "2017",
      "venue": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "arxiv": "arXiv:1706.02292"
    },
    {
      "citation_id": "24",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "25",
      "title": "jsymbolic: A feature extractor for midi files",
      "authors": [
        "C Mckay",
        "I Fujinaga"
      ],
      "year": "2006",
      "venue": "Proceedings of the 2006 International Computer Music Conference"
    },
    {
      "citation_id": "26",
      "title": "This time with feeling: Learning expressive musical performance",
      "authors": [
        "S Oore",
        "I Simon",
        "S Dieleman",
        "D Eck",
        "K Simonyan"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "27",
      "title": "This time with feeling: Learning expressive musical performance",
      "authors": [
        "S Oore",
        "I Simon",
        "S Dieleman",
        "D Eck",
        "K Simonyan"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "28",
      "title": "Musical texture and expressivity features for music emotion recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "19th International Society for Music Information Retrieval Conference"
    },
    {
      "citation_id": "29",
      "title": "Multi-modal music emotion recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "R Panda",
        "R Malheiro",
        "B Rocha",
        "A Oliveira",
        "R Paiva"
      ],
      "year": "2013",
      "venue": "International symposium on computer music multidisciplinary research"
    },
    {
      "citation_id": "30",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "A novel multi-task learning method for symbolic music emotion recognition",
      "authors": [
        "J Qiu",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "A novel multi-task learning method for symbolic music emotion recognition",
      "arxiv": "arXiv:2201.05782"
    },
    {
      "citation_id": "32",
      "title": "A novel multi-task learning method for symbolic music emotion recognition",
      "authors": [
        "J Qiu",
        "C Chen",
        "T Zhang"
      ],
      "year": "2022",
      "venue": "A novel multi-task learning method for symbolic music emotion recognition",
      "arxiv": "arXiv:2201.05782"
    },
    {
      "citation_id": "33",
      "title": "Improving music genre classification from multi-modal properties of music and genre correlations perspective",
      "authors": [
        "G Ru",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP49357.2023.10097241"
    },
    {
      "citation_id": "34",
      "title": "Emomix: Emotion mixing via diffusion models for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "35",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Evaluation of cnn-based automatic music tagging models",
      "authors": [
        "M Won",
        "A Ferraro",
        "D Bogdanov",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Evaluation of cnn-based automatic music tagging models",
      "arxiv": "arXiv:2006.00751"
    },
    {
      "citation_id": "38",
      "title": "Source separation improves music emotion recognition",
      "authors": [
        "J Xu",
        "X Li",
        "Y Hao",
        "G Yang"
      ],
      "year": "2014",
      "venue": "Proceedings of international conference on multimedia retrieval"
    },
    {
      "citation_id": "39",
      "title": "Multimodal music emotion recognition with hierarchical cross-modal attention network",
      "authors": [
        "J Zhao",
        "G Ru",
        "Y Yu",
        "Y Wu",
        "D Li",
        "W Li"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "40",
      "title": "Improving automatic piano transcription by refined feature fusion and weighted loss",
      "authors": [
        "J Zhao",
        "Y Wu",
        "L Wen",
        "L Ma",
        "L Ruan",
        "W Wang",
        "W Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 9th Conference on Sound and Music Technology"
    }
  ]
}