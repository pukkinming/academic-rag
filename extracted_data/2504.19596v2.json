{
  "paper_id": "2504.19596v2",
  "title": "Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities",
  "published": "2025-04-28T09:00:04Z",
  "authors": [
    "Wei-Bang Jiang",
    "Xi Fu",
    "Yi Ding",
    "Cuntai Guan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial for healthcare and brain-computer interfaces. While existing methods rely on specialized architectures and dataset-specific fusion strategies, they struggle to learn universal representations that generalize across datasets and handle missing modalities at inference time. To address these issues, we propose PhysioOmni, a foundation model for multimodal physiological signal analysis that models both homogeneous and heterogeneous features to decouple multimodal signals and extract generic representations while maintaining compatibility with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal tokenizer, enabling masked signal pre-training via modality-invariant and modality-specific objectives. To ensure adaptability to diverse and incomplete modality combinations, the pre-trained encoders undergo resilient fine-tuning with prototype alignment on downstream datasets. Extensive experiments on four downstream tasks, emotion recognition, sleep stage classification, motor prediction, and mental workload detection, demonstrate that PhysioOmni achieves state-of-the-art performance while maintaining strong robustness to missing modalities. Our code and model weights will be released.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal physiological signals, such as electroencephalography (EEG), electrocardiography (ECG), electrooculography (EOG), and electromyography (EMG), have garnered increasing interest in braincomputer interfaces (BCI) due to their ability to capture diverse physiological and cognitive states  [29] . EEG reflects neural activity, ECG monitors cardiac rhythms, EOG tracks eye movements, and EMG records muscle activations, collectively offering a comprehensive representation of human physiological responses. These modalities have been widely applied across various domains, including cognitive load assessment  [1] , emotion recognition  [20] , motor imagery  [5] , and sleep stage classification  [22] , driving advances in both medical diagnostics and BCI. To enhance performance, numerous algorithms have been proposed to effectively integrate multiple modalities, leveraging their complementary information to improve accuracies across applications  [32, 4, 9, 33, 17] .\n\nOver the past two years, several EEG foundation models, including LaBraM, have emerged, demonstrating the feasibility of using masked EEG models for pre-training on large-scale EEG datasets  [18, 38, 39] . These models have notably improved performance and generalization, laying the groundwork for general unsupervised representation learning. However, a gap persists in the development of general pre-trained foundation models for multimodal physiological signals. Although some studies have explored the integration of multiple physiological signals to boost performance, they are often limited by two key factors: either using multiple modalities during training but relying on a single modality for testing, or being tailored to specific downstream tasks, which restricts their generalizability across diverse datasets. For instance, Fang et al. introduced multimodal foundation models designed for sleep stage classification using masked autoencoders  [10] , while Brant-X employed contrastive learning for multi-level alignment between EEG and EXG but relied solely on EEG for downstream tasks  [44] . Developing a universal multimodal physiological foundation model capable of extracting semantic representations while handling arbitrary missing modalities presents several key challenges:\n\n1) Decoupling homogeneous and heterogeneous features: Multimodal physiological signals encompass both shared and unique patterns. Effectively disentangling modality-invariant (homogeneous) and modality-specific (heterogeneous) features is crucial for robust multimodal learning.\n\n2) Unified multimodal representation learning: Beyond feature decoupling, integrating physiological signals with diverse characteristics to derive effective and generalizable representations remains a fundamental challenge in multimodal representation learning.\n\n3) Handling arbitrary missing modalities: While leveraging all available modalities during training, ensuring robust performance under incomplete modalities at inference time is highly challenging, requiring strategies that maximize adaptability while minimizing performance degradation.\n\nWith regard to above challenges, we propose PhysioOmni, a universal multimodal physiological foundation model pre-trained on diverse multimodal datasets, including EEG, ECG, EOG, and EMG signals. Our approach begins with training a decoupled multimodal tokenizer, where one shared codebook and four private codebooks disentangle multimodal embeddings into modality-invariant and modality-specific codes. These discrete codes serve as the foundation for masked signal modeling, enabling the encoders to learn universal representations across modalities. During fine-tuning, we introduce homogeneous representation mapping, which projects features from different modalities into a common space. To handle arbitrary missing modalities, we incorporate prototype alignment and modality-specific prediction, ensuring robust adaptation across different modality combinations. We comprehensively evaluate PhysioOmni on four popular BCI tasks: emotion recognition, sleep stage classification, motor prediction, and workload detection. Our approach achieves state-of-the-art (SOTA) performance across both unimodal and multimodal settings, underscoring the effectiveness of decoupled multimodal learning and resilient fine-tuning. Our key contributions are threefold:\n\n1) Decoupled multimodal tokenizer: We design a multimodal tokenizer that disentangles modalityinvariant and modality-specific features using a shared codebook for common patterns and private codebooks for unique characteristics, enhancing multimodal fusion.\n\n2) Masked signal pre-training: We extend masked signal modeling to multimodal physiological signals, enabling the model to learn both generic and semantic representations. Leveraging the decoupled tokenizer, we design modality-invariant and modality-specific code prediction, ensuring the model captures both shared and unique characteristics of each signal type.\n\n3) Resilient fine-tuning with prototype alignment: To handle arbitrary missing modalities, we introduce homogeneous representation mapping, prototype alignment, and modality-specific prediction, enabling the model to dynamically adapt to varying modality combinations during inference while maintaining robust performance despite incomplete input signals.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Foundation Models For Eeg",
      "text": "EEG foundation models, designed to handle arbitrary configurations while learning robust and versatile representations, have gained significant attention in recent years. BIOT  [42]  introduces a Biosignal Transformer that tokenizes channels into patches, enabling cross-data learning across heterogeneous biosignal formats (EEG and ECG). LaBraM  [18]  leverages vector-quantized neural spectrum prediction and masked EEG modeling to facilitate cross-dataset learning, significantly enhancing performance across diverse EEG tasks. EEGPT  [38]  pre-trains a 10-million-parameter Transformer using mask-based dual self-supervised learning and spatio-temporal representation alignment to improve EEG representation learning. CBraMod  [39]  employs a criss-cross Transformer to separately model spatial and temporal dependencies while incorporating an asymmetric conditional positional encoding scheme for greater adaptability.  The masked signal modeling process includes two projectors for private and shared code prediction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Models In Bci",
      "text": "While many studies incorporate multimodal signals to enhance BCI performance, a universal foundation model that generalizes across modalities and datasets while extracting generic representations remains lacking. VBH-GNN  [26]  utilizes multimodal physiological signals and variational Bayesian heterogeneous graphs for cross-subject emotion recognition. Brant-X  [44]  aligns physiological signals by leveraging an EEG foundation model for knowledge transfer but is limited to EEG-based downstream tasks. CIMSleepNet  [31]  addresses arbitrary modality missing in sleep staging through a modal imagination module and contrastive learning, alongside temporal attention for better context representation. However, these models either rely on multimodal inputs during training but evaluate on a single modality or are tailored to specific tasks, limiting their generalizability across datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In essence, PhysioOmni consists of three training stages: 1) Joint learning of a shared codebook and four modality-specific private codebooks through decoupled multimodal tokenizer training; 2) Masked signal modeling to learn generic representations by predicting private and shared codes from masked inputs; 3) Resilient fine-tuning of pre-trained encoders with self-alignment to handle arbitrary missing modalities in downstream tasks. Figure  5  shows the first two stages.\n\nGiven paired multimodal samples\n\n, where x j i ∈ R Cj ×Tj , with C j and T j representing the number of channels and time points, we segment the signals into patches x j i ∈ R Nj ×Pj before passing them into the networks, where N j = Cj Tj Pj represents the number of patches and P j is the patch size for modality j. We denote the sampling rate for modality j as S j .\n\nModel Architecture. We adopt the encoder architecture of LaBraM  [18]  for all modalities due to its effectiveness and simplicity. A lightweight temporal encoder, consisting of several 1-D convolutional layers followed by GroupNorm  [40]  and GELU  [15]  activation, is used to extract temporal features within each patch. To incorporate temporal and channel-specific information for each modality, learnable temporal and and spatial embeddings are added to the extracted features before feeding them into the Transformer blocks. Unlike LaBraM, we incorporate RMSNorm  [43]  and SwiGLU  [30]  into the Transformer blocks, as done in LLaMA  [36] . The same backbone is used for both multimodal tokenizer training and masked signal modeling.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Decoupled Multimodal Tokenizer Training",
      "text": "At this stage, we aim to extract decoupled, compact, and semantically meaningful representations from multimodal physiological signals to facilitate subsequent masked signal modeling in pre-training.\n\nTo achieve this, we first define four modality-specific encoders E e , E c , E o , E m for EEG, ECG, EOG, and EMG, respectively, to extract both private and shared latent embeddings from the input signals:\n\nwhere z jp i and z js i represent the private and shared embeddings for modality j, obtained by splitting the encoder outputs. Unlike prior studies  [13, 25, 41]  that rely on separate encoders for decoupled embeddings, we employ a single encoder per modality to reduce computational overhead.\n\nCodebook Optimization. We design five learnable codebooks: one shared codebook (V s ∈ R K×D ) and four private codebooks (V j ∈ R K×D for modality j), where K is the codebook size and D is the code dimension. These codebooks extract both modality-invariant and modality-specific features from multiple modalities. Notably, the shared codebook operates at the largest temporal scale (EEG). The shared and private embeddings from the encoders retrieve the closest codebook entry by looking up their respective codebooks and replacing themselves with the nearest code:\n\nwhere\n\nTo enhance stability, we apply ℓ 2 normalization to the embeddings, which is equivalent to selecting codes based on cosine similarity. Since the temporal scale (patch size) differs across modalities, the Temporal Alignment module (TA) is crucial for aligning smaller-scale modalities with EEG. It is implemented by a cross-attention layer, where a query aggregates",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Pj",
      "text": "Pe patches into a single embedding:\n\nwhere q ∈ R 1×D is a learnable query, and W K j and W V j are the key and value projection weights. Given the unique characteristics of each modality, EEG and EMG signals are crucial in the frequency domain  [8, 24] , while EOG reflects eye movements, and ECG exhibits periodic patterns. We propose reconstructing the Fourier amplitude for EEG and EMG signals, while preserving the original signals for EOG and ECG. The codes from the private and shared codebooks are first ℓ 2 normalized, concatenated, and then fed into the decoders D e , D c , D o , D m to reconstruct the target signals:\n\nwhere o i denotes the reconstructed signals, and ∥ represents concatenation operator. The training loss for codebook optimization is then formulated as:\n\nwhere j ∈ {e, c, o, m} and l ∈ {e, c, o, m, s}. The stop-gradient operation is denoted as sg. In this framework, the decoders are optimized via the reconstruction loss, the codebooks are updated using the VQ loss, and the encoders are refined through the commitment loss. We apply z-score normalization to the reconstruction target in each sample to enhance the stability of convergence. Additionally, we employ an exponential moving average strategy to ensure stable codebook updates  [37] .\n\nCross-modal Reconstruction. EEG signals encapsulate rich cognitive and physiological information, as they directly reflect neural activities  [6] . To encourage the shared codebook to capture common patterns across modalities, we designate EEG as an anchor and leverage its shared embeddings to reconstruct the other modalities with their private codes:  Outputs from all modality encoders undergo Homogeneous Representation Mapping, followed by a Feature Fuser, which consists of a Transformer Block and a Token Aggregator for multimodal feature integration. Prototype alignment ensures robustness to missing modalities in downstream tasks, while modality-specific prediction preserves the performance of individual modalities.\n\nEEG codes by a factor of PeSj Pj Se to match the patch count of each modality j ∈ {c, o, m}:\n\nPe S j P j Se ×D is the learnable query matrix. The cross-modal reconstruction loss is then computed as:\n\nDisentangling Loss. Beyond ensuring that the shared codebook captures modality-invariant information, it is equally crucial to distinguish shared and private embeddings, ensuring they encode distinct aspects of the input. To mitigate redundancy between embeddings, we introduce a disentangling loss that enforces soft orthogonality:\n\nwhere cosine similarity is used to encourage orthogonality.\n\nUltimately, the total loss for decoupled multimodal tokenizer training integrates these constraints:\n\nwhere α 1 and α 2 are weighting factors balancing the loss terms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Masked Signal Pre-Training",
      "text": "Masked signal modeling, an effective representation learning paradigm across various domains  [7, 14, 18] , is extended here to learn generic and semantic representations for multimodal signals.\n\nFor each modality j, we randomly generate a mask M = {0, 1} Nj with a mask ratio r j . All masked patches are replaced by a learnable mask token, producing a corrupted sample x j i . The encoder is then trained to predict the corresponding codebook indices for each masked patch. Since the embeddings are decoupled into shared and private codebooks, we employ two projection heads to recover the masked patches. Due to varying temporal scales, shared codes may span multiple patches. To ensure alignment, we duplicate the shared codes PeSj Pj Se times. The training objective is formulated as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Resilient Fine-Tuning With Prototype Alignment",
      "text": "In this stage, we aim to maintain the representational capacity of individual modalities while addressing the challenge of missing modalities. Figure  2  provides an overview of this process.\n\nHomogeneous Representation Mapping. Each modality-specific input x j i is first encoded using its respective pre-trained encoder E j , producing feature representations z j i ∈ R n j ×d j , where n j and d j denote the sequence length and dimension, respectively. Since different modalities yield features of varying lengths and dimensions, we introduce Homogeneous Representation Mapping to project them into a unified feature space R n×d  [45] .\n\nFollowing encoding, a modality-specific Length Mapping module ℓ m standardizes feature lengths by transforming the extracted representations into a common-length format f j i ∈ R n×d j :\n\nHere, the encoded representations z j i are first transformed by a modality-specific head H j , which consists of linear and normalization layers, to produce z j i , a probability distribution over spatial locations obtained via a softmax operation. To facilitate cross-modality alignment, a Reorganization Function R restructures spatial distributions based on the encoded feature representations. Finally, a modality-specific Embedding Mapping module e m projects the reorganized embeddings into a common feature space using a linear transformation:\n\nGiven the encoded feature f j i , the Feature Fuser module integrates information from multiple tokens using a shared Transformer block, followed by a Token Aggregator, which applies a 1D convolution layer per modality to aggregate sequential information:\n\nTo enhance robustness against missing modalities while ensuring effective modality fusion, we introduce three losses in the Alignment & Prediction module. First, to align unimodal features within a common space, we propose Prototype Alignment, where a set of learnable prototypes\n\nis shared across all modalities. Specifically, for an n-class classification task, each sample is encouraged to be close to the unique prototype corresponding to its class, setting |U| = n. For regression tasks, |U| is a hyperparameter, and each sample is encouraged to align with the nearest prototype. Therefore, the alignment loss is defined as:\n\nwhere\n\n. This loss ensures that the feature representations h j i align with their respective prototypes. The main prediction loss L main is computed by averaging the features h j i across all modalities, reinforcing the effectiveness of multimodal fusion, which is also used as the final prediction at test stage. Additionnaly, a modality-specific loss L j spec is introduced to preserve the independent representational capacity of each modality. For L main and L j spec , we employ cross-entropy loss with label smoothing for multi-class classification, binary cross-entropy (BCE) loss for binary classification, and mean squared error (MSE) loss for regression tasks. The total fine-tuning loss is a weighted sum of all components:\n\nwhere γ values control the trade-off between different loss terms.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Downstream Datasets",
      "text": "We consider 4 popular downstream tasks in BCI for evaluating PhysioOmni, where EEGMAT is presented in Appendix E. The detailed information is presented in Table  1 :\n\n• SEED-VII  [16]  (emotion recognition): SEED-VII features seven emotions (happiness, sadness, neutral, fear, disgust, surprise, and anger) of 20 subjects who underwent 4 sessions on different days. Each session contains 20 trials of video clips lasting 2-5 minutes. For data partitioning, all subjects' data are combined, with the first 10 trials designated as the training set, the middle 5 trials as the validation set, and the final 5 trials as the test set.\n\n• HMC  [2]  (sleep stage classification): HMC was developed for automatic sleep scoring, focusing on five sleep stages: wake, NREM-1, NREM-2, NREM-3, and REM. The dataset consists of randomly selected patient recordings from a diverse population undergoing polysomnographic (PSG) examinations for various sleep disorders. It includes full-night polysomnographic recordings from 151 subjects, with the first 100 used for training, the next 25 for validation, and the remaining 26 for testing.\n\n• FBM  [3]  (motor prediction): This dataset includes full-body motion capture (66 targets) from approximately 10 walking trials conducted by 10 able-bodied individuals during gait tasks on level ground, ramps, and stairs, with the advantage of unconstrained movement using a motion capture system with wireless IMUs. We use a data stride of 50 ms. For model evaluation, data from all subjects are combined, with the last trial used for testing, the second-to-last trial for validation, and the remaining trials for training. Training Data & Environment Settings. To enhance the model's generalization ability, we extensively gather diverse multimodal physiological data from multiple datasets, mostly containing three or more modalities, as detailed in Appendix B. Our experiments are conducted on four NVIDIA A100-80G GPUs with Python 3.12.8 and PyTorch 2.5.1 + CUDA 12.4. The best models are selected based on their performance on the validation set and subsequently evaluated on the test set with full modalities. To ensure reliability and comparability, we report the average and standard deviation across three random seeds.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "The results are presented in Table  2 , 3, and 4, with baseline descriptions provided in Appendix C. Overall, PhysioOmni achieves competitive performance compared to a wide range of unimodal, multimodal, and cross-modal methods. Notably, there is a trade-off in selecting the best model for PhysioOmni, as different modality combinations reach optimal performance at different training epochs, placing our approach at a disadvantage. Despite this, PhysioOmni matches or surpasses the performance of the leading baselines in most cases. In addition to achieving best results under the single EEG condition, PhysioOmni also outperforms all baselines on other unimodal (EOG, ECG, EMG) and multimodal settings. Specifically, it delivers superior performance for EOG, ECG, and multimodal scenarios, and remains competitive with LaBraM. On HMC, PhysioOmni consistently achieves the best results across all modalities except EOG. While it does not outperform the top baselines in terms of correlation and R 2 , it achieves the lowest RMSE on the FBM dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "To evaluate the contribution of key components, we conduct ablation studies on modality-specific prediction, disentangling loss, prototype alignment, cross-modal reconstruction, and the shared codebook. The results are presented in Figure  3 , where lower RMSE values on FBM indicate better performance. Overall, the removal of any component generally leads to performance degradation. Notably, prototype alignment proves especially effective on SEED-VII and HMC, while modalityspecific prediction is crucial across all datasets. Removing cross-modal reconstruction, disentangling loss, or the shared codebook consistently results in performance drops, underscoring their importance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We propose PhysioOmni, a universal multimodal physiological foundation model that learns robust representations through decoupled tokenization and masked signal modeling, and effectively handles arbitrary missing modalities via resilient fine-tuning. Through comprehensive experiments on four BCI tasks, PhysioOmni consistently achieves SOTA across both unimodal and multimodal settings. Loss ratio γ SEED-VII: 1 (align), 0.5 (main), 0.5 (EEG), 0.5 (EOG), 0.5 (ECG) HMC: 1 (align), 0.1 (main), 0.01 (EEG), 4 (EOG), 0.5 (EMG) FBM: 0.01 (align), 0.1 (main), 2 (EEG), 0.5 (EOG), 1 (EMG) EEGMAT: 1 (align), 0.5 (main), 0.5 (EEG), 0.5 (EOG), 0.",
      "page_start": 9,
      "page_end": 14
    },
    {
      "section_name": "A Hyperparameter Settings",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B Pre-Training Datasets",
      "text": "We utilize a diverse collection of multimodal physiological datasets for various tasks:\n\n• TUEG  [28] : The TUH EEG Corpus (TUEG) is an extensive archive comprising 26,846 clinical EEG recordings collected at Temple University Hospital. Some recordings include EOG (horizontal), ECG, and EMG signals alongside EEG (21-23 channels), with sampling frequencies ranging from 250 to 1024 Hz. For pre-training, we select recordings that include at least three modalities.\n\n• DEAP  [23] : DEAP provides a multimodal dataset for analyzing human affective states, recorded from 32 participants as they watched 40 one-minute music video excerpts. EEG (32 channels), EOG (horizontal and vertical), and EMG signals were captured at a sampling rate of 512 Hz. indicates no agreement beyond chance, with negative values indicating worse than random agreement. • Weighted F1: The F1 score is the harmonic mean of precision and recall, balancing the trade-off between false positives and false negatives. The weighted F1 takes into account the support (the number of true instances) of each class, so it gives more importance to classes with more data. • RMSE: RMSE is the square root of the average squared differences between predicted and actual values, providing a measure of the model's prediction error. Larger errors are penalized more due to the squaring of differences while a lower RMSE indicates a better fit between the model and the data. • R 2 Score: R 2 measures the proportion of the variance in the dependent variable that is predictable from the independent variables. A value of 1 indicates perfect prediction, 0 indicates that the model does not improve on simply predicting the mean of the data, and negative values suggest that the model is performing worse than a simple mean predictor. • Pearson Correlation: Pearson correlation quantifies the linear relationship between two variables, with values ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation). A value of 0 indicates no linear relationship.\n\nFor the monitor metric, AUROC is used for binary classification, Cohen's Kappa for multi-class classification, and R 2 Score for regression.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "E More Experiments On Eegmat",
      "text": "This dataset  [46]",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "F Ablation On Frozen Encoders",
      "text": "We further investigate the impact of freezing the pre-trained encoders during fine-tuning, with results presented in Figure  4 . As expected, freezing the encoders leads to a significant drop in downstream performance across all tasks and modality combinations. This highlights the importance of full end-to-end fine-tuning, which allows the model to adapt pre-trained representations to task-specific nuances and optimize the fusion of multimodal features. The results emphasize that while pre-training provides a strong initialization, fine-tuning remains crucial for maximizing performance in real-world downstream applications.\n\nprovide a strong initialization, capturing essential modality-specific and modality-invariant patterns that transfer well across downstream tasks. These findings confirm that our masked signal modeling and codebook-based design are effective in learning transferable representations that significantly enhance overall model performance.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "H Ablation On Homogeneous Representation Mapping & Feature Fuser",
      "text": "Homogeneous Representation Mapping (HRM) is designed to project representations from different modalities into a common space with uniform embedding sizes, ensuring better compatibility across modalities. Meanwhile, the Feature Fuser (FF) integrates these projected representations to generate comprehensive multimodal features, enhancing the model's ability to leverage complementary information across different signals.\n\nTo assess the contribution of HRM and FF, we conduct an ablation study and present the results in Table  9 . The findings demonstrate that removing the two modules generally lead to a degradation in performance on SEED-VII and HMC, underscoring their crucial role in effective modality fusion. However, on FBM, while eliminating HRM and FF leads to significant performance degration for EEG, it results in a performance boost in certain cases, particularly for EEG in combination with another modality. A possible explanation is that FBM exhibits a different modality interaction pattern compared to SEED-VII and HMC, where the added complexity of fusion modules may lead to overfitting. These results suggest that for FBM, a simpler fusion mechanism could be more advantageous than enforcing strict homogeneous representation constraints. This highlights the importance of dataset-specific tuning when designing multimodal fusion strategies.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "I Limitations",
      "text": "While PhysioOmni demonstrates strong performance and generalizability across diverse physiological signals and downstream tasks, several limitations remain: 1) Although PhysioOmni handles arbitrary missing modalities during inference, it still relies on a fixed set of known modalities during training. Adapting to entirely new modalities not seen during pre-training remains an open challenge. 2) Our method pre-trains individual encoders for each modality. While effective, this increases model complexity and limits parameter sharing. Developing a unified encoder architecture that can handle all physiological signals jointly is a promising direction for future work.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "J Broader Impacts",
      "text": "PhysioOmni has the potential to advance multimodal physiological computing and BCI systems by providing a scalable and adaptable foundation model. Its ability to generalize across datasets and tasks while remaining resilient to missing modalities makes it well-suited for real-world scenarios where data completeness and consistency cannot be guaranteed. Applications include assistive healthcare systems, emotion-aware computing, cognitive load monitoring, and sleep analysis. However, care must be taken to address potential privacy concerns when deploying models trained on sensitive physiological data. Future efforts should also explore fairness, energy efficiency, and interpretability to ensure the responsible use of such foundation models in clinical and everyday applications.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the decoupled multimodal tokenizer learning framework and masked signal",
      "page": 3
    },
    {
      "caption": "Figure 5: shows the first two stages.",
      "page": 3
    },
    {
      "caption": "Figure 2: Schematic of the resilient fine-tuning process, illustrated with EEG and EOG as examples.",
      "page": 5
    },
    {
      "caption": "Figure 2: provides an overview of this process.",
      "page": 5
    },
    {
      "caption": "Figure 3: , where lower RMSE values on FBM indicate better",
      "page": 8
    },
    {
      "caption": "Figure 3: Ablation study of pivotal components on downstream datasets.",
      "page": 9
    },
    {
      "caption": "Figure 4: As expected, freezing the encoders leads to a significant drop in downstream",
      "page": 16
    },
    {
      "caption": "Figure 4: Ablation study on frozen encoders.",
      "page": 17
    },
    {
      "caption": "Figure 5: , demonstrate a consistent",
      "page": 17
    },
    {
      "caption": "Figure 5: Ablation study on pre-training.",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial"
        },
        {
          "Abstract": "for healthcare and brain-computer\ninterfaces. While existing methods rely on"
        },
        {
          "Abstract": "specialized architectures and dataset-specific fusion strategies,\nthey struggle to"
        },
        {
          "Abstract": "learn universal representations that generalize across datasets and handle missing"
        },
        {
          "Abstract": "modalities at inference time. To address these issues, we propose PhysioOmni, a"
        },
        {
          "Abstract": "foundation model for multimodal physiological signal analysis that models both"
        },
        {
          "Abstract": "homogeneous and heterogeneous features to decouple multimodal signals and ex-"
        },
        {
          "Abstract": "tract generic representations while maintaining compatibility with arbitrary missing"
        },
        {
          "Abstract": "modalities. PhysioOmni trains a decoupled multimodal tokenizer, enabling masked"
        },
        {
          "Abstract": "signal pre-training via modality-invariant and modality-specific objectives. To en-"
        },
        {
          "Abstract": "sure adaptability to diverse and incomplete modality combinations, the pre-trained"
        },
        {
          "Abstract": "encoders undergo resilient fine-tuning with prototype alignment on downstream"
        },
        {
          "Abstract": "datasets. Extensive experiments on four downstream tasks, emotion recognition,"
        },
        {
          "Abstract": "sleep stage classification, motor prediction, and mental workload detection, demon-"
        },
        {
          "Abstract": "strate that PhysioOmni achieves state-of-the-art performance while maintaining"
        },
        {
          "Abstract": "strong robustness to missing modalities. Our code and model weights will be"
        },
        {
          "Abstract": "released."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "released.": "1\nIntroduction"
        },
        {
          "released.": "Multimodal physiological signals, such as electroencephalography (EEG), electrocardiography (ECG),"
        },
        {
          "released.": ""
        },
        {
          "released.": "computer interfaces (BCI) due to their ability to capture diverse physiological and cognitive states"
        },
        {
          "released.": "[29]. EEG reflects neural activity, ECG monitors cardiac rhythms, EOG tracks eye movements, and"
        },
        {
          "released.": "EMG records muscle activations, collectively offering a comprehensive representation of human"
        },
        {
          "released.": "physiological responses. These modalities have been widely applied across various domains,"
        },
        {
          "released.": "cluding cognitive load assessment [1], emotion recognition [20], motor imagery [5], and sleep stage"
        },
        {
          "released.": "classification [22], driving advances in both medical diagnostics and BCI. To enhance performance,"
        },
        {
          "released.": "numerous algorithms have been proposed to effectively integrate multiple modalities, leveraging their"
        },
        {
          "released.": "complementary information to improve accuracies across applications [32, 4, 9, 33, 17]."
        },
        {
          "released.": ""
        },
        {
          "released.": "strating the feasibility of using masked EEG models for pre-training on large-scale EEG datasets"
        },
        {
          "released.": ""
        },
        {
          "released.": "work for general unsupervised representation learning. However, a gap persists in the development of"
        },
        {
          "released.": "general pre-trained foundation models for multimodal physiological signals. Although some studies"
        },
        {
          "released.": "have explored the integration of multiple physiological signals to boost performance, they are often"
        },
        {
          "released.": "limited by two key factors: either using multiple modalities during training but relying on a single"
        },
        {
          "released.": "∗Equal contribution"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "ability across diverse datasets. For instance, Fang et al.\nintroduced multimodal foundation models"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "designed for sleep stage classification using masked autoencoders [10], while Brant-X employed"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "contrastive learning for multi-level alignment between EEG and EXG but relied solely on EEG for"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "downstream tasks [44]. Developing a universal multimodal physiological foundation model capable"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "of extracting semantic representations while handling arbitrary missing modalities presents several"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "key challenges:"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "1) Decoupling homogeneous and heterogeneous features: Multimodal physiological signals encom-"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "pass both shared and unique patterns. Effectively disentangling modality-invariant (homogeneous)"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "and modality-specific (heterogeneous) features is crucial for robust multimodal learning."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "2) Unified multimodal representation learning: Beyond feature decoupling, integrating physiologi-"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "cal signals with diverse characteristics to derive effective and generalizable representations remains a"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "fundamental challenge in multimodal representation learning."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "3) Handling arbitrary missing modalities: While leveraging all available modalities during training,"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "ensuring robust performance under incomplete modalities at inference time is highly challenging,"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "requiring strategies that maximize adaptability while minimizing performance degradation."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "With regard to above challenges, we propose PhysioOmni, a universal multimodal physiological"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "foundation model pre-trained on diverse multimodal datasets, including EEG, ECG, EOG, and EMG"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "signals. Our approach begins with training a decoupled multimodal\ntokenizer, where one shared"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "codebook and four private codebooks disentangle multimodal embeddings into modality-invariant and"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "modality-specific codes. These discrete codes serve as the foundation for masked signal modeling,"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "enabling the encoders to learn universal representations across modalities. During fine-tuning, we"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "introduce homogeneous representation mapping, which projects features from different modalities"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "into a common space. To handle arbitrary missing modalities, we incorporate prototype alignment"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "and modality-specific prediction, ensuring robust adaptation across different modality combinations."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "We comprehensively evaluate PhysioOmni on four popular BCI tasks: emotion recognition, sleep"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "stage classification, motor prediction, and workload detection. Our approach achieves state-of-the-art"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "(SOTA) performance across both unimodal and multimodal settings, underscoring the effectiveness"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "of decoupled multimodal learning and resilient fine-tuning. Our key contributions are threefold:"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "1) Decoupled multimodal tokenizer: We design a multimodal tokenizer that disentangles modality-"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "invariant and modality-specific features using a shared codebook for common patterns and private"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "codebooks for unique characteristics, enhancing multimodal fusion."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "2) Masked signal pre-training: We extend masked signal modeling to multimodal physiological"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "signals, enabling the model\nto learn both generic and semantic representations. Leveraging the"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "decoupled tokenizer, we design modality-invariant and modality-specific code prediction, ensuring"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "the model captures both shared and unique characteristics of each signal type."
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "3) Resilient fine-tuning with prototype alignment: To handle arbitrary missing modalities, we in-"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "troduce homogeneous representation mapping, prototype alignment, and modality-specific prediction,"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "enabling the model to dynamically adapt to varying modality combinations during inference while"
        },
        {
          "modality for testing, or being tailored to specific downstream tasks, which restricts their generaliz-": "maintaining robust performance despite incomplete input signals."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG signals\nECG signals": "Figure 1: Overview of the decoupled multimodal tokenizer learning framework and masked signal",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "modeling. Left: The tokenizer learning framework consists of a shared codebook, private codebooks,",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "encoders, decoders, temporal alignment modules, and cross-modal alignment modules. Right: The",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "masked signal modeling process includes two projectors for private and shared code prediction.",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "Multimodal Models in BCI",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "tion model that generalizes across modalities and datasets while extracting generic representations",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "remains lacking. VBH-GNN [26] utilizes multimodal physiological signals and variational Bayesian",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "heterogeneous graphs for cross-subject emotion recognition. Brant-X [44] aligns physiological",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "signals by leveraging an EEG foundation model for knowledge transfer but is limited to EEG-based",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "downstream tasks. CIMSleepNet [31] addresses arbitrary modality missing in sleep staging through",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "a modal imagination module and contrastive learning, alongside temporal attention for better context",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "representation. However, these models either rely on multimodal inputs during training but evaluate",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        },
        {
          "EEG signals\nECG signals": "on a single modality or are tailored to specific tasks, limiting their generalizability across datasets.",
          "EOG signals": "",
          "EMG signals": "",
          "Taking EEG as an example": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "multimodal tokenizer training and masked signal modeling."
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "3.1\nDecoupled Multimodal Tokenizer Training"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "At this stage, we aim to extract decoupled, compact, and semantically meaningful representations"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "from multimodal physiological signals to facilitate subsequent masked signal modeling in pre-training."
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "To achieve this, we first define four modality-specific encoders E e, E c, E o, E m for EEG, ECG, EOG,"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "and EMG, respectively, to extract both private and shared latent embeddings from the input signals:"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "zep\n, zes\n= E e(xe\nzcp\n, zcs\n= E c(xc\nzop\n, zos\n= E o(xo\nzmp\n, zms\n= E m(xm\n),\n(1)\ni\ni )\ni\ni )\ni\ni )\ni\ni"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "i\ni\ni\ni"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "where zjp\nand zjs\nrepresent the private and shared embeddings for modality j, obtained by splitting\ni\ni"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "the encoder outputs. Unlike prior studies [13, 25, 41] that rely on separate encoders for decoupled"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "embeddings, we employ a single encoder per modality to reduce computational overhead."
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "Codebook Optimization. We design five learnable codebooks: one shared codebook (V s ∈ RK×D)"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "and four private codebooks (V j ∈ RK×D for modality j), where K is the codebook size and D is"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "the code dimension. These codebooks extract both modality-invariant and modality-specific features"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "from multiple modalities. Notably, the shared codebook operates at the largest temporal scale (EEG)."
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "The shared and private embeddings from the encoders retrieve the closest codebook entry by looking"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "up their respective codebooks and replacing themselves with the nearest code:"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "zjp\n= V j[arg min\nzjs\n= V s[arg min\n(2)\n∥ℓ2(zjp\n) − ℓ2(vj\n∥ℓ2(TA(zjs\n)) − ℓ2(vs"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "i\ni\nk)∥2]"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "k\nk"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "where vi ∈ V is the code and k ∈ [1, K]. To enhance stability, we apply ℓ2 normalization to the"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "embeddings, which is equivalent to selecting codes based on cosine similarity. Since the temporal"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "scale (patch size) differs across modalities,\nthe Temporal Alignment module (TA) is crucial for"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "aligning smaller-scale modalities with EEG. It is implemented by a cross-attention layer, where a"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "query aggregates Pj\npatches into a single embedding:"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "Pe"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "zjs\n, W V\nzjs\n).\nTA(zjs\n) = CrossAttention(q, W K\n(3)\nj\nj"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "i\ni\ni"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "where q ∈ R1×D is a learnable query, and W K\nand W V\nare the key and value projection weights.\nj\nj"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "Given the unique characteristics of each modality, EEG and EMG signals are crucial in the frequency"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "domain [8, 24], while EOG reflects eye movements, and ECG exhibits periodic patterns. We propose"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "reconstructing the Fourier amplitude for EEG and EMG signals, while preserving the original"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "signals for EOG and ECG. The codes from the private and shared codebooks are first ℓ2 normalized,"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "concatenated, and then fed into the decoders De, Dc, Do, Dm to reconstruct the target signals:"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "oe\n∥ˆzes\n)\noc\n∥ˆzcs\n)\noo\n∥ˆzos\n)\nom\n∥zms\n),\n(4)\ni = De(ˆzep\ni\ni = Dc(ˆzcp\ni\ni = Do(ˆzop\ni\ni = Dm(ˆzmp\ni"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "where oi denotes the reconstructed signals, and ∥ represents concatenation operator. The training loss"
        },
        {
          "[30] into the Transformer blocks, as done in LLaMA [36]. The same backbone is used for both": "for codebook optimization is then formulated as:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Feature Fuser"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "𝐿𝑒\nspec"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Modality-specific"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Prediction\n𝐿𝑜"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "spec"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Encoder\nMapping\nLength\nMapping\nEmbedding \nEEG Signals"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Me"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Mean Pooling"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "𝐿main"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Transformer Block\nToken Aggregator\n& Prediction"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Prototype Alignment"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "𝐿e\nalign"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Encoder\nMapping\nLength\nMapping\nEmbedding \nEOG Signals"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "…\nM𝑜\n𝐿𝑜"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "align"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Figure 2: Schematic of the resilient fine-tuning process, illustrated with EEG and EOG as examples."
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Outputs from all modality encoders undergo Homogeneous Representation Mapping, followed by a"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Feature Fuser, which consists of a Transformer Block and a Token Aggregator for multimodal feature"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "integration. Prototype alignment ensures robustness to missing modalities in downstream tasks, while"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "modality-specific prediction preserves the performance of individual modalities."
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "PeSj"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "∈\nEEG codes by a\nfactor of\nto match the patch count of\neach modality j\n{c, o, m}:"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Pj Se"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Pe Sj"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "×D"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Pj Se\nzjs\n, W V\nzjs\nCrossAttention(Q, W K\n), where Q ∈ R\nis the learnable query matrix.\nThe"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "j\nj\ni\ni"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "cross-modal reconstruction loss is then computed as:"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "∥oj"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "(cid:88) i\n(cid:88) j\n(7)\nLCR =\n2.\ni − xj\ni ∥2"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Disentangling Loss. Beyond ensuring that the shared codebook captures modality-invariant informa-"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "tion, it is equally crucial to distinguish shared and private embeddings, ensuring they encode distinct"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "aspects of the input. To mitigate redundancy between embeddings, we introduce a disentangling loss"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "that enforces soft orthogonality:"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "(cid:88) i\n(cid:88) j\n, zjs\n),\nsim(zjp\n(8)\nLD ="
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "i\ni"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "where cosine similarity is used to encourage orthogonality."
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Ultimately, the total loss for decoupled multimodal tokenizer training integrates these constraints:"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "(9)\nLT = LCB + α1LCR + α2LD,"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "where α1 and α2 are weighting factors balancing the loss terms."
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "3.2\nMasked Signal Pre-training"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Masked signal modeling, an effective representation learning paradigm across various domains"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "[7, 14, 18], is extended here to learn generic and semantic representations for multimodal signals."
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "For each modality j, we randomly generate a mask M = {0, 1}Nj with a mask ratio rj. All masked"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "i . The encoder is then\npatches are replaced by a learnable mask token, producing a corrupted sample (cid:101)xj"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "trained to predict the corresponding codebook indices for each masked patch. Since the embeddings"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "are decoupled into shared and private codebooks, we employ two projection heads to recover the"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "masked patches. Due to varying temporal scales, shared codes may span multiple patches. To ensure"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "alignment, we duplicate the shared codes PeSj\ntimes. The training objective is formulated as:"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "Pj Se"
        },
        {
          "Homogeneous Representation Mapping\nAlignment & Prediction": "(cid:88)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "of varying lengths and dimensions, we introduce Homogeneous Representation Mapping to project"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "them into a unified feature space Rn×d [45]."
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "Following encoding, a modality-specific Length Mapping module ℓm standardizes feature lengths by"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": ":\ntransforming the extracted representations into a common-length format f j\ni ∈ Rn×dj"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "R(a, b) = bT a.\nzj\nf j\n(11)\ni )) ∈ Rnj ×n,\ni = softmax(Hj(zj\ni ),\ni ) = R(zj\ni = ℓm(zj\n(cid:101)\ni , (cid:101)zj"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "Here, the encoded representations zj\ni are first transformed by a modality-specific head Hj, which"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "consists of linear and normalization layers,\ni , a probability distribution over spatial\nto produce (cid:101)zj"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "locations obtained via a softmax operation. To facilitate cross-modality alignment, a Reorganization"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "Function R restructures spatial distributions based on the encoded feature representations. Finally,"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "a modality-specific Embedding Mapping module em projects the reorganized embeddings into a"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "f j"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "common feature space using a linear transformation:\ni = em(f j\ni ) = Linear(f j\ni ) ∈ Rn×d."
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "Fusion & Alignment. Given the encoded feature ˆf j\ni , the Feature Fuser module integrates information"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "from multiple tokens using a shared Transformer block, followed by a Token Aggregator, which"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "applies a 1D convolution layer per modality to aggregate sequential information:"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "hj\n(12)\ni = Aggregator(Transformer( ˆf j\ni )) ∈ Rd."
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "To enhance robustness against missing modalities while ensuring effective modality fusion, we"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "introduce three losses in the Alignment & Prediction module.\nFirst,\nto align unimodal\nfeatures"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "within a common space, we propose Prototype Alignment, where a set of\nlearnable prototypes"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "U = {u1, ..., u|U |} ∈ R|U |×d is shared across all modalities. Specifically, for an n-class classification"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "task, each sample is encouraged to be close to the unique prototype corresponding to its class, setting"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "|U| = n. For regression tasks, |U| is a hyperparameter, and each sample is encouraged to align with"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "the nearest prototype. Therefore, the alignment loss is defined as:"
        },
        {
          "dj denote the sequence length and dimension, respectively. Since different modalities yield features": "2"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": "consists of\nrandomly selected patient"
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": "polysomnographic (PSG) examinations for various sleep disorders."
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": "next 25 for validation, and the remaining 26 for testing."
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": "• FBM [3] (motor prediction): This dataset includes full-body motion capture (66 targets)"
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        },
        {
          "• HMC [2] (sleep stage classification): HMC was developed for automatic sleep scoring,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the second-to-last trial for validation, and the remaining trials for training.": ""
        },
        {
          "the second-to-last trial for validation, and the remaining trials for training.": "Modality (#Channel)"
        },
        {
          "the second-to-last trial for validation, and the remaining trials for training.": "EEG (62), EOG, ECG"
        },
        {
          "the second-to-last trial for validation, and the remaining trials for training.": "EEG (4), EOG, EMG"
        },
        {
          "the second-to-last trial for validation, and the remaining trials for training.": "EEG (60), EOG, EMG (12)"
        },
        {
          "the second-to-last trial for validation, and the remaining trials for training.": "EEG (19), ECG"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "Method"
        },
        {
          "across three random seeds.": "EEG-Conformer [34]"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "BIOT [42]"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "LaBraM-Base [18]"
        },
        {
          "across three random seeds.": "CBraMod [39]"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "Fu et al. [11]"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "FeatFusion"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": "PhysioOmni"
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": ""
        },
        {
          "across three random seeds.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: The results of different methods on HMC.": "Test Modality"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EOG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EOG"
        },
        {
          "Table 3: The results of different methods on HMC.": ""
        },
        {
          "Table 3: The results of different methods on HMC.": "EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG+EOG+EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG+EOG+EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EOG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG+EOG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG+EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EOG+EMG"
        },
        {
          "Table 3: The results of different methods on HMC.": "EEG+EOG+EMG"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: The results of different methods on FBM.": "Test Modality"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EOG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EMG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": ""
        },
        {
          "Table 4: The results of different methods on FBM.": "EOG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG+EOG+EMG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EOG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EMG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG+EOG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG+EMG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EOG+EMG"
        },
        {
          "Table 4: The results of different methods on FBM.": "EEG+EOG+EMG"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u0000&\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u0000&\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u0000&\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u0000&\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u0000&\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u0000&\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000+\u00000\u0000&\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000+\u00000\u0000&\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000+\u00000\u0000&\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000)\u0000%\u00000\u0000\u0003\u0000\u000b\u00005\u00000\u00006\u0000(\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000)\u0000%\u00000\u0000\u0003\u0000\u000b\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000)\u0000%\u00000\u0000\u0003\u0000\u000b\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": ""
        },
        {
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000R\u0000W\u0000R\u0000W\u0000\\\u0000S\u0000H\u0000\u0003\u0000D\u0000O\u0000L\u0000J\u0000Q\u0000P\u0000H\u0000Q\u0000W": "\u0000(\u00000\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000F\u0000U\u0000R\u0000V\u0000V\u0000\u0010\u0000P\u0000R\u0000G\u0000D\u0000O\u0000\u0003\u0000U\u0000H\u0000F\u0000R\u0000Q\u0000V\u0000W\u0000U\u0000X\u0000F\u0000W\u0000L\u0000R\u0000Q": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000V\u0000K\u0000D\u0000U\u0000H\u0000G\u0000\u0003\u0000F\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N": "\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1]\nIsabela Albuquerque, Abhishek Tiwari, Mark Parent, Raymundo Cassani, Jean-François Gagnon,"
        },
        {
          "References": "Daniel Lafond, Sébastien Tremblay, and Tiago H Falk. Wauc: a multi-modal database for"
        },
        {
          "References": "mental workload assessment under physical activity. Frontiers in Neuroscience, 14:549524,"
        },
        {
          "References": "2020."
        },
        {
          "References": "[2] Diego Alvarez-Estevez and Roselyne M. Rijsman.\nInter-database validation of a deep learning"
        },
        {
          "References": "approach for automatic sleep scoring. PLOS ONE, 16(8):1–27, 08 2021."
        },
        {
          "References": "Justin A Brantley, Trieu Phat Luu, Sho Nakagome, Fangshi Zhu, and Jose L Contreras-Vidal.\n[3]"
        },
        {
          "References": "Full body mobile brain-body imaging data during unconstrained locomotion on stairs, ramps,"
        },
        {
          "References": "and level ground. Scientific Data, 5(1):1–10, 2018."
        },
        {
          "References": "[4] Stanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and Alexandre Gram-"
        },
        {
          "References": "fort. A deep learning architecture for temporal sleep stage classification using multivariate and"
        },
        {
          "References": "multimodal time series.\nIEEE Transactions on Neural Systems and Rehabilitation Engineering,"
        },
        {
          "References": "26(4):758–769, 2018."
        },
        {
          "References": "[5]\nJeong-Hyun Cho, Ji-Hoon Jeong, and Seong-Whan Lee. NeuroGrasp: Real-Time EEG Classi-"
        },
        {
          "References": "fication of High-Level Motor Imagery Tasks Using a Dual-Stage Deep Learning Framework."
        },
        {
          "References": "IEEE Transactions on Cybernetics, 52(12):13279–13292, 2022."
        },
        {
          "References": "[6] Fernando Lopes da Silva. Neural mechanisms underlying brain waves: from neural membranes"
        },
        {
          "References": "to networks. Electroencephalography and clinical neurophysiology, 79(2):81–93, 1991."
        },
        {
          "References": "[7]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep"
        },
        {
          "References": "bidirectional transformers for language understanding. In Proceedings of the 2019 conference of"
        },
        {
          "References": "the North American chapter of the association for computational linguistics: human language"
        },
        {
          "References": "technologies, volume 1 (long and short papers), pages 4171–4186, 2019."
        },
        {
          "References": "[8] O Dressler, G Schneider, G Stockmanns, and EF Kochs. Awareness and the EEG power"
        },
        {
          "References": "spectrum: analysis of frequencies. British journal of anaesthesia, 93(6):806–809, 2004."
        },
        {
          "References": "[9] Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Kai Wang, and Chin-teng Lin. Dewave: Discrete"
        },
        {
          "References": "encoding of EEG waves for EEG to text translation. Advances in Neural Information Processing"
        },
        {
          "References": "Systems, 36:9907–9918, 2023."
        },
        {
          "References": "[10] Ching Fang, Christopher Michael Sandino, Behrooz Mahasseni, Juri Minxha, Hadi Pouransari,"
        },
        {
          "References": "Erdrin Azemi, Ali Moin, and Ellen L. Zippi. Promoting cross-modal representations to im-"
        },
        {
          "References": "prove multimodal foundation models for physiological signals.\nIn Advancements In Medical"
        },
        {
          "References": "Foundation Models: Explainability, Robustness, Security, and Beyond, 2024."
        },
        {
          "References": "[11] Xi Fu and Cuntai Guan. Gait Pattern Recognition Based on Supervised Contrastive Learning Be-"
        },
        {
          "References": "tween EEG and EMG.\nIn 2023 45th Annual International Conference of the IEEE Engineering"
        },
        {
          "References": "in Medicine & Biology Society (EMBC), pages 1–4, 2023."
        },
        {
          "References": "[12] Nigel Gebodh, Zeinab Esmaeilpour, Abhishek Datta, and Marom Bikson. Dataset of concurrent"
        },
        {
          "References": "EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation. Scientific"
        },
        {
          "References": "Data, 8(1):274, 2021."
        },
        {
          "References": "[13] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. MISA: Modality-Invariant"
        },
        {
          "References": "and -Specific Representations for Multimodal Sentiment Analysis.\nIn Proceedings of the 28th"
        },
        {
          "References": "ACM International Conference on Multimedia, MM ’20, page 1122–1131, New York, NY, USA,"
        },
        {
          "References": "2020. Association for Computing Machinery."
        },
        {
          "References": "[14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked"
        },
        {
          "References": "the IEEE/CVF conference on\nautoencoders are scalable vision learners.\nIn Proceedings of"
        },
        {
          "References": "computer vision and pattern recognition, pages 16000–16009, 2022."
        },
        {
          "References": "arXiv preprint\n[15] Dan Hendrycks and Kevin Gimpel.\nGaussian error\nlinear units\n(gelus)."
        },
        {
          "References": "arXiv:1606.08415, 2016."
        },
        {
          "References": "[16] Wei-Bang Jiang, Xuan-Hao Liu, Wei-Long Zheng, and Bao-Liang Lu. SEED-VII: A Multimodal"
        },
        {
          "References": "IEEE\nDataset of Six Basic Emotions with Continuous Labels for Emotion Recognition."
        },
        {
          "References": "Transactions on Affective Computing, pages 1–16, 2024."
        },
        {
          "References": "[17] Wei-Bang Jiang, Yansen Wang, Bao-Liang Lu, and Dongsheng Li. NeuroLM: A universal"
        },
        {
          "References": "multi-task foundation model for bridging the gap between language and EEG signals.\nIn The"
        },
        {
          "References": "Thirteenth International Conference on Learning Representations, 2025."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Representations with Tremendous EEG Data in BCI.\nIn The Twelfth International Conference"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "on Learning Representations, 2024."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[19]\nJiarui Jin, Haoyu Wang, Hongyan Li, Jun Li, Jiahui Pan, and Shenda Hong. Reading your heart:"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Learning ECG words and sentences via pre-training ECG language model.\nIn The Thirteenth"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "International Conference on Learning Representations, 2025."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[20] Stamos Katsigiannis and Naeem Ramzan. DREAMER: A database for emotion recognition"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "IEEE Journal of\nthrough EEG and ECG signals from wireless low-cost off-the-shelf devices."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Biomedical and Health Informatics, 22(1):98–107, 2017."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[21] B. Kemp, A.H. Zwinderman, B. Tuk, H.A.C. Kamphuisen, and J.J.L. Oberye. Analysis of a"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "IEEE\nsleep-dependent neuronal feedback loop:\nthe slow-wave microcontinuity of the EEG."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Transactions on Biomedical Engineering, 47(9):1185–1194, 2000."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[22] Sirvan Khalighi, Teresa Sousa, José Moutinho Santos, and Urbano Nunes.\nISRUC-Sleep:"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "A comprehensive public dataset for sleep researchers. Computer methods and programs in"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "biomedicine, 124:180–192, 2016."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[23] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani,"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. DEAP: A Database for"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Emotion Analysis ;Using Physiological Signals.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "3(1):18–31, 2012."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[24] Paavo V Komi and Per Tesch. EMG frequency spectrum, muscle structure, and fatigue during"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "European journal of applied physiology and occupational\ndynamic contractions in man."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "physiology, 42:41–50, 1979."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[25] Yong Li, Yuanzhi Wang, and Zhen Cui. Decoupled multimodal distilling for emotion recognition."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition\nIn Proceedings of"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "(CVPR), pages 6631–6640, June 2023."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[26] Chenyu Liu, Xinliang Zhou, Zhengri Zhu, Liming Zhai, Ziyu Jia, and Yang Liu. VBH-GNN:"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Variational bayesian heterogeneous graph neural networks for cross-subject emotion recognition."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "In The Twelfth International Conference on Learning Representations, 2024."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[27] Shuo Ma, Yingwei Zhang, Qiqi Zhang, Yiqiang Chen, Haoran Wang, and Ziyu Jia. SleepMG:"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Multimodal Generalizable Sleep Staging with Inter-modal Balance of Classification and Domain"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Discrimination.\nIn Proceedings of the 32nd ACM International Conference on Multimedia, MM"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "’24, page 4004–4013, New York, NY, USA, 2024. Association for Computing Machinery."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[28]\nIyad Obeid and Joseph Picone. The temple university hospital EEG data corpus. Frontiers in"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "neuroscience, 10:195498, 2016."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[29] R. Sharma, V.I. Pavlovic, and T.S. Huang. Toward multimodal human-computer interface."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Proceedings of the IEEE, 86(5):853–869, 1998."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[30] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[31] Qi Shen, Junchang Xin, Bing Dai, Shudi Zhang, and Zhiqiong Wang. Robust sleep staging over"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "incomplete multimodal physiological signals via contrastive imagination. Advances in Neural"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Information Processing Systems, 37:112025–112049, 2024."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[32] Mohammad Soleymani, Maja Pantic, and Thierry Pun. Multimodal emotion recognition in"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "response to videos.\nIEEE Transactions on Affective Computing, 3(2):211–223, 2011."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[33] Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. Decoding"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "natural images from EEG for object recognition.\nIn The Twelfth International Conference on"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Learning Representations, 2024."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[34] Yonghao Song, Qingqing Zheng, Bingchuan Liu, and Xiaorong Gao. EEG Conformer: Con-"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "IEEE Transactions on Neural\nvolutional Transformer for EEG Decoding and Visualization."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "Systems and Rehabilitation Engineering, 31:710–719, 2023."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[35] Mario Giovanni Terzano and Liborio Parrino. The cyclic alternating pattern (CAP) in human"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "sleep.\nIn Handbook of Clinical Neurophysiology, volume 6, pages 79–93. Elsevier, 2005."
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open"
        },
        {
          "[18] Wei-Bang Jiang, Li-Ming Zhao, and Bao-Liang Lu. Large Brain Model for Learning Generic": "and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Neural Information Processing Systems, 30, 2017."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[38] Guangyu Wang, Wenchao Liu, Yuhong He, Cong Xu, Lin Ma, and Haifeng Li. Eegpt: Pretrained"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Advances in Neural\ntransformer\nfor universal and reliable representation of eeg signals."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Information Processing Systems, 37:39249–39280, 2024."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[39]\nJiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li,"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "and Gang Pan. CBramod: A criss-cross brain foundation model for EEG decoding.\nIn The"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Thirteenth International Conference on Learning Representations, 2025."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[40] Yuxin Wu and Kaiming He. Group normalization.\nIn Proceedings of the European Conference"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "on Computer Vision (ECCV), September 2018."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[41] Yan Xia, Hai Huang, Jieming Zhu, and Zhou Zhao. Achieving cross modal generalization"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "with multimodal unified representation. Advances in Neural Information Processing Systems,"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "36:63529–63541, 2023."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[42] Chaoqi Yang, M Westover, and Jimeng Sun. BIOT: Biosignal Transformer for Cross-data"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Learning in the Wild. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "editors, Advances in Neural Information Processing Systems, volume 36, pages 78240–78260."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Curran Associates, Inc., 2023."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[43] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Information Processing Systems, 32, 2019."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[44] Daoze Zhang, Zhizhang Yuan, Junru Chen, Kerui Chen, and Yang Yang. Brant-X: A Uni-"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "fied Physiological Signal Alignment Framework.\nIn Proceedings of the 30th ACM SIGKDD"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Conference on Knowledge Discovery and Data Mining, pages 4155–4166, 2024."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "[45] Yunhua Zhang, Hazel Doughty, and Cees Snoek. Learning unseen modality interaction. Ad-"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "vances in Neural Information Processing Systems, 36:54716–54726, 2023."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Igor Zyma, Sergii Tukaev, Ivan Seleznov, Ken Kiyono, Anton Popov, Mariia Chernykh, and\n[46]"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "Oleksii Shpenkov. Electroencephalograms during mental arithmetic task performance. Data,"
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "4(1):14, 2019."
        },
        {
          "[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in": "12"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": "EEG Encoder"
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": "200"
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": "800"
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        },
        {
          "Table 5: Hyperparameters for decoupled multimodal tokenizer.": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "EEG Encoder"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "{1,8,8}"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "{16,16,16}"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "{15,3,3}"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "{8,1,1}"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "{7,1,1}"
        },
        {
          "Table 6: Hyperparameters for masked signal pre-training.": "12"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Hyperparameters for fine-tuning.": "Values"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "128×128"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "1"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "4"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "128"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "512"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "10"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "7 (SEED-VII), 5 (HMC), 256 (FBM), 2 (EEGMAT)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "128 (EEGMAT), 512 (SEED-VII, HMC, FBM)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "SEED-VII: 1 (align), 0.5 (main), 0.5 (EEG), 0.5 (EOG), 0.5 (ECG)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "HMC: 1 (align), 0.1 (main), 0.01 (EEG), 4 (EOG), 0.5 (EMG)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": ""
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "FBM: 0.01 (align), 0.1 (main), 2 (EEG), 0.5 (EOG), 1 (EMG)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "EEGMAT: 1 (align), 0.5 (main), 0.5 (EEG), 0.5 (EOG), 0.5 (EMG)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "1e-3 (SEED-VII), 5e-4 (HMC, FBM), 1e-4 (EEGMAT)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "1e-4 (SEED-VII), 5e-5 (HMC, FBM), 1e-5 (EEGMAT)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "Cosine"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "AdamW"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "(0.9,0.999)"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "0.05"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "50"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "5"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "None"
        },
        {
          "Table 7: Hyperparameters for fine-tuning.": "0.1 (multi-class classification)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": "at least three modalities."
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": "rate of 512 Hz."
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": "Rechtschaffen and Kales manual."
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": "sampled at 2000 Hz."
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": ""
        },
        {
          "We utilize a diverse collection of multimodal physiological datasets for various tasks:": "rate of 1000 Hz."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "the-art baselines:"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• EEG-Conformer [34]: A compact Convolutional Transformer that integrates local feature"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "extraction via convolution and global feature modeling via self-attention for EEG classifica-"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "tion. By combining temporal and spatial convolutions with self-attention, EEG-Conformer"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "effectively captures both short-term and long-term dependencies in EEG signals."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• BIOT [42]: A Biosignal Transformer designed for flexible biosignal encoding, BIOT"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "facilitates cross-data learning across diverse signal\nformats such as EEG and ECG.\nIt"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "tokenizes each channel into fixed-length segments while preserving spatio-temporal features"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "through channel and positional embeddings, demonstrating strong generalizability via joint"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "pre-training and fine-tuning on multiple biosignal tasks."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• LaBraM [18]: A unified foundation model for EEG, LaBraM enables cross-dataset learning"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "by segmenting EEG signals into channel patches and encoding them with a vector-quantized"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "neural tokenizer. It employs masked neural code prediction for unsupervised pre-training,"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "leveraging approximately 2,500 hours of EEG data from 20 datasets. We fine-tune its"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "publicly available pre-trained checkpoints on each downstream dataset."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• CBraMod [39]: An EEG foundation model designed to address the heterogeneity of spatial"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "and temporal dependencies in EEG signals, CBraMod utilizes a criss-cross Transformer"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "with separate attention mechanisms. It incorporates an asymmetric conditional positional"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "encoding scheme for enhanced adaptability across diverse EEG formats. We fine-tune its"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "publicly available pre-trained checkpoints on each downstream dataset."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• Fu et al.\n[11]: This method employs a multimodal\ntraining strategy using supervised"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "contrastive learning, leveraging EMG signals during training to enhance EEG-based gait"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "classification and regression. The model learns gait patterns from EEG with EMG guidance"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "but relies solely on EEG during inference. We use multimodal signals to train and single"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "modality to test for this method."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• SleepMG [27]: A multimodal generalizable sleep staging method, SleepMG balances"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "inter-modal differences in PSG by assessing classification and domain discrimination perfor-"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "mances across modalities. It defines modal performance metrics based on variance from the"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "average performance and adaptively adjusts gradient updates to emphasize poorly balanced"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "modalities."
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "• FeatFusion: We leverage our pre-trained encoders from PhysioOmni and fuse the features"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "extracted from these encoders by the Homogeneous Representation Mapping and Feature"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "Fuser. We concatenate the features from all modalities after Homogeneous Representation"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "Mapping and feed them into the Transformer\nlayer. This approach utilizes the generic"
        },
        {
          "To comprehensively evaluate PhysioOmni, we compare it with the following classical and state-of-": "representations of each modality and employs proposed fusion strategy."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "trade-off between false positives and false negatives. The weighted F1 takes into account the"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "support (the number of true instances) of each class, so it gives more importance to classes"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "and actual values, providing a measure of the model’s prediction error. Larger errors are"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "penalized more due to the squaring of differences while a lower RMSE indicates a better fit"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "is"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "predictable from the independent variables. A value of 1 indicates perfect prediction, 0"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "indicates that the model does not improve on simply predicting the mean of the data, and"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": "variables, with values ranging from -1 (perfect negative correlation) to +1 (perfect positive"
        },
        {
          "indicates no agreement beyond chance, with negative values indicating worse than random": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "single-modality and multimodal settings."
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "Method"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "EEG-Conformer [34]"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "BIOT [42]"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "LaBraM-Base [18]"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "CBraMod [39]"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "Fu et al. [11]"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "FeatFusion"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": "PhysioOmni"
        },
        {
          "PhysioOmni in leveraging complementary information from multiple physiological signals, both in": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*"
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": "\u0000+\u00000\u0000&"
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*"
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": "\u0000)\u0000%\u00000"
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": ""
        },
        {
          "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u00006\u0000(\u0000(\u0000'\u0000\u0010\u00009\u0000,\u0000,",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "\u0000(\u0000(\u0000*",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*\u0000\u000e\u0000(\u0000&\u0000*"
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000+\u00000\u0000&",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "\u0000(\u0000(\u0000*",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000)\u0000%\u00000",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": ""
        },
        {
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000%\u0000D\u0000O\u0000D\u0000Q\u0000F\u0000H\u0000G\u0000\u0003\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\u0000\u0012\u00005\u00000\u00006\u0000(\u0000": "\u0000(\u0000(\u0000*",
          "\u00003\u0000K\u0000\\\u0000V\u0000L\u0000R\u00002\u0000P\u0000Q\u0000L\u0000\u0003\u0000\u000b\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000H\u0000G\u0000\u0003\u0000)\u0000\u0014\u0000\u0012\u00005\u0000\u0015\u0000\u0003\u00006\u0000F\u0000R\u0000U\u0000H\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*",
          "\u0000Z\u0000\u0012\u0000R\u0000\u0003\u0000S\u0000U\u0000H\u0000\u0010\u0000W\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000\u000b\u0000&\u0000R\u0000K\u0000H\u0000Q\u0000\n\u0000V\u0000\u0003\u0000.\u0000D\u0000S\u0000S\u0000D\u0000\u0012\u00003\u0000H\u0000D\u0000U\u0000V\u0000R\u0000Q\u0000\u0003\u0000&\u0000R\u0000U\u0000U\u0000H\u0000O\u0000D\u0000W\u0000L\u0000R\u0000Q\u0000": "\u0000(\u0000(\u0000*\u0000\u000e\u0000(\u00002\u0000*\u0000\u000e\u0000(\u00000\u0000*"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "/ w/o HRM & FF)."
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "Test Modality"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "ECG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+ECG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG+ECG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG+ECG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "Test Modality"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG+EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG+EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "Test Modality"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": ""
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EOG+EMG"
        },
        {
          "Table 9: The ablation study on Homogeneous Representation Mapping & Feature Fuser (PhysioOmni": "EEG+EOG+EMG"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "complexity and limits parameter sharing. Developing a unified encoder architecture that can handle"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "all physiological signals jointly is a promising direction for future work."
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "Broader Impacts"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "PhysioOmni has the potential to advance multimodal physiological computing and BCI systems by"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "providing a scalable and adaptable foundation model. Its ability to generalize across datasets and tasks"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": ""
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "data completeness and consistency cannot be guaranteed. Applications include assistive healthcare"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "systems, emotion-aware computing, cognitive load monitoring, and sleep analysis. However, care"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "must be taken to address potential privacy concerns when deploying models trained on sensitive"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "physiological data. Future efforts should also explore fairness, energy efficiency, and interpretability"
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": "to ensure the responsible use of such foundation models in clinical and everyday applications."
        },
        {
          "Our method pre-trains individual encoders for each modality. While effective, this increases model": ""
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Wauc: a multi-modal database for mental workload assessment under physical activity",
      "authors": [
        "Isabela Albuquerque",
        "Abhishek Tiwari",
        "Mark Parent",
        "Raymundo Cassani",
        "Jean-François Gagnon",
        "Daniel Lafond",
        "Sébastien Tremblay",
        "Tiago H Falk"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Inter-database validation of a deep learning approach for automatic sleep scoring",
      "authors": [
        "Diego Alvarez-Estevez",
        "Roselyne Rijsman"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "3",
      "title": "Full body mobile brain-body imaging data during unconstrained locomotion on stairs, ramps, and level ground",
      "authors": [
        "Justin Brantley",
        "Trieu Phat Luu",
        "Sho Nakagome",
        "Fangshi Zhu",
        "Jose L Contreras- Vidal"
      ],
      "year": "2018",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "4",
      "title": "Gilles Wainrib, and Alexandre Gramfort. A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series",
      "authors": [
        "Stanislas Chambon",
        "N Mathieu",
        "Galtier",
        "J Pierrick",
        "Arnal"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "5",
      "title": "NeuroGrasp: Real-Time EEG Classification of High-Level Motor Imagery Tasks Using a Dual-Stage Deep Learning Framework",
      "authors": [
        "Jeong-Hyun Cho",
        "Ji-Hoon Jeong",
        "Seong-Whan Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Neural mechanisms underlying brain waves: from neural membranes to networks",
      "authors": [
        "Fernando Lopes Da Silva"
      ],
      "year": "1991",
      "venue": "Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "7",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "8",
      "title": "Awareness and the EEG power spectrum: analysis of frequencies",
      "authors": [
        "G Dressler",
        "G Schneider",
        "Stockmanns",
        "Kochs"
      ],
      "year": "2004",
      "venue": "British journal of anaesthesia"
    },
    {
      "citation_id": "9",
      "title": "Discrete encoding of EEG waves for EEG to text translation",
      "authors": [
        "Yiqun Duan",
        "Jinzhao Zhou",
        "Zhen Wang",
        "Yu-Kai Wang",
        "Chin-Teng Lin",
        "Dewave"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "Promoting cross-modal representations to improve multimodal foundation models for physiological signals",
      "authors": [
        "Ching Fang",
        "Christopher Michael Sandino",
        "Behrooz Mahasseni",
        "Juri Minxha",
        "Hadi Pouransari",
        "Erdrin Azemi",
        "Ali Moin",
        "Ellen Zippi"
      ],
      "year": "2024",
      "venue": "Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond"
    },
    {
      "citation_id": "11",
      "title": "Gait Pattern Recognition Based on Supervised Contrastive Learning Between EEG and EMG",
      "authors": [
        "Xi Fu",
        "Cuntai Guan"
      ],
      "year": "2023",
      "venue": "2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "12",
      "title": "Dataset of concurrent EEG, ECG, and behavior with multiple doses of transcranial electrical stimulation",
      "authors": [
        "Nigel Gebodh",
        "Zeinab Esmaeilpour",
        "Abhishek Datta",
        "Marom Bikson"
      ],
      "year": "2021",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "13",
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20"
    },
    {
      "citation_id": "14",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "16",
      "title": "SEED-VII: A Multimodal Dataset of Six Basic Emotions with Continuous Labels for Emotion Recognition",
      "authors": [
        "Wei-Bang Jiang",
        "Xuan-Hao Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "NeuroLM: A universal multi-task foundation model for bridging the gap between language and EEG signals",
      "authors": [
        "Wei-Bang Jiang",
        "Yansen Wang",
        "Bao-Liang Lu",
        "Dongsheng Li"
      ],
      "year": "2025",
      "venue": "The Thirteenth International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI",
      "authors": [
        "Wei-Bang Jiang",
        "Li-Ming Zhao",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Reading your heart: Learning ECG words and sentences via pre-training ECG language model",
      "authors": [
        "Jiarui Jin",
        "Haoyu Wang",
        "Hongyan Li",
        "Jun Li",
        "Jiahui Pan",
        "Shenda Hong"
      ],
      "year": "2025",
      "venue": "The Thirteenth International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "21",
      "title": "Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG",
      "authors": [
        "B Kemp",
        "A Zwinderman",
        "B Tuk",
        "H Kamphuisen",
        "J Oberye"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "22",
      "title": "ISRUC-Sleep: A comprehensive public dataset for sleep researchers",
      "authors": [
        "Sirvan Khalighi",
        "Teresa Sousa",
        "José Moutinho Santos",
        "Urbano Nunes"
      ],
      "year": "2016",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "23",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "EMG frequency spectrum, muscle structure, and fatigue during dynamic contractions in man",
      "authors": [
        "V Paavo",
        "Per Komi",
        "Tesch"
      ],
      "year": "1979",
      "venue": "European journal of applied physiology and occupational physiology"
    },
    {
      "citation_id": "25",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Yong Li",
        "Yuanzhi Wang",
        "Zhen Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "VBH-GNN: Variational bayesian heterogeneous graph neural networks for cross-subject emotion recognition",
      "authors": [
        "Chenyu Liu",
        "Xinliang Zhou",
        "Zhengri Zhu",
        "Liming Zhai",
        "Ziyu Jia",
        "Yang Liu"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "SleepMG: Multimodal Generalizable Sleep Staging with Inter-modal Balance of Classification and Domain Discrimination",
      "authors": [
        "Shuo Ma",
        "Yingwei Zhang",
        "Qiqi Zhang",
        "Yiqiang Chen",
        "Haoran Wang",
        "Ziyu Jia"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia, MM '24"
    },
    {
      "citation_id": "28",
      "title": "The temple university hospital EEG data corpus",
      "authors": [
        "Iyad Obeid",
        "Joseph Picone"
      ],
      "year": "2016",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "29",
      "title": "Toward multimodal human-computer interface",
      "authors": [
        "R Sharma",
        "V Pavlovic",
        "T Huang"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "30",
      "title": "Glu variants improve transformer",
      "authors": [
        "Noam Shazeer"
      ],
      "year": "2020",
      "venue": "Glu variants improve transformer",
      "arxiv": "arXiv:2002.05202"
    },
    {
      "citation_id": "31",
      "title": "Robust sleep staging over incomplete multimodal physiological signals via contrastive imagination",
      "authors": [
        "Qi Shen",
        "Junchang Xin",
        "Bing Dai",
        "Shudi Zhang",
        "Zhiqiong Wang"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "Mohammad Soleymani",
        "Maja Pantic",
        "Thierry Pun"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Decoding natural images from EEG for object recognition",
      "authors": [
        "Yonghao Song",
        "Bingchuan Liu",
        "Xiang Li",
        "Nanlin Shi",
        "Yijun Wang",
        "Xiaorong Gao"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization",
      "authors": [
        "Yonghao Song",
        "Qingqing Zheng",
        "Bingchuan Liu",
        "Xiaorong Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "35",
      "title": "The cyclic alternating pattern (CAP) in human sleep",
      "authors": [
        "Mario Giovanni",
        "Liborio Parrino"
      ],
      "year": "2005",
      "venue": "Handbook of Clinical Neurophysiology"
    },
    {
      "citation_id": "36",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "37",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aaron Van Den",
        "Oriol Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Eegpt: Pretrained transformer for universal and reliable representation of eeg signals",
      "authors": [
        "Guangyu Wang",
        "Wenchao Liu",
        "Yuhong He",
        "Cong Xu",
        "Lin Ma",
        "Haifeng Li"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "39",
      "title": "CBramod: A criss-cross brain foundation model for EEG decoding",
      "authors": [
        "Jiquan Wang",
        "Sha Zhao",
        "Zhiling Luo",
        "Yangxuan Zhou",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2025",
      "venue": "The Thirteenth International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "Group normalization",
      "authors": [
        "Yuxin Wu",
        "Kaiming He"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "41",
      "title": "Achieving cross modal generalization with multimodal unified representation",
      "authors": [
        "Yan Xia",
        "Hai Huang",
        "Jieming Zhu",
        "Zhou Zhao"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "BIOT: Biosignal Transformer for Cross-data Learning in the Wild",
      "authors": [
        "Chaoqi Yang",
        "M Westover",
        "Jimeng Sun"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Root mean square layer normalization",
      "authors": [
        "Biao Zhang",
        "Rico Sennrich"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Brant-X: A Unified Physiological Signal Alignment Framework",
      "authors": [
        "Daoze Zhang",
        "Zhizhang Yuan",
        "Junru Chen",
        "Kerui Chen",
        "Yang Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "45",
      "title": "Learning unseen modality interaction",
      "authors": [
        "Yunhua Zhang",
        "Hazel Doughty",
        "Cees Snoek"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Electroencephalograms during mental arithmetic task performance",
      "authors": [
        "Igor Zyma",
        "Sergii Tukaev",
        "Ivan Seleznov",
        "Ken Kiyono",
        "Anton Popov",
        "Mariia Chernykh",
        "Oleksii Shpenkov"
      ],
      "year": "2019",
      "venue": "Data"
    },
    {
      "citation_id": "47",
      "title": "",
      "authors": [
        "Ur]hqhqfrghuv%dodqfhg$ffxudf\\506"
      ],
      "venue": ""
    },
    {
      "citation_id": "48",
      "title": "",
      "authors": [
        "Ur]hqhqfrghuv"
      ],
      "venue": ""
    },
    {
      "citation_id": "49",
      "title": "Ablation on Pre-training To assess the effectiveness of our proposed pre-training paradigm, we conduct an ablation study by randomly initializing all encoder parameters. The results, shown in Figure 5, demonstrate a consistent performance drop across all datasets and modality combinations when pre-training is removed. This highlights the critical role of pre-training in enabling robust and generalizable representations",
      "venue": "Ablation on Pre-training To assess the effectiveness of our proposed pre-training paradigm, we conduct an ablation study by randomly initializing all encoder parameters. The results, shown in Figure 5, demonstrate a consistent performance drop across all datasets and modality combinations when pre-training is removed. This highlights the critical role of pre-training in enabling robust and generalizable representations"
    },
    {
      "citation_id": "50",
      "title": "",
      "authors": [
        "Zrsuhwudlqlqj%dodqfhg$ffxudf\\506"
      ],
      "venue": ""
    }
  ]
}